2020.acl-main.173,P19-1483,0,0.41514,"ata-to-text generation (Lebret et al., 2016; Wiseman et al., 2017) which are not open-ended, require models to be factual and/or faithful to the source text. Despite recent improvements in conditional text generation, most summarization systems are trained to maximize the log-likelihood of the reference summary at the word-level, which does not necessarily reward models for being faithful. Moreover, models are usually agnostic to the noises or artifacts of the training data, such as reference divergence, making them vulnerable to hallucinations (Kryscinski et al., 2019a; Wiseman et al., 2017; Dhingra et al., 2019). Thus, models can generate texts that are not consistent with the input, yet would likely have reasonable model log-likelihood. 2.1 Intrinsic and Extrinsic Hallucinations Given a document D and its abstractive summary S, we try to identify all hallucinations in S with respect to the content of D, regardless of the quality of the summary. In this work, we define a summary as being hallucinated if it has a span(s) wi . . . wi+j , j ≥ i, that is not supported by the input document. To distinguish hallucinations further in the context of a document and a summary, we categorize hallucinations by t"
2020.acl-main.173,P16-1154,0,0.0367126,"ot only in terms of raw metrics, i.e., ROUGE, but also in generating faithful and factual summaries as evaluated by humans. Furthermore, we show that textual entailment measures better correlate with faithfulness than standard metrics, potentially leading the way to automatic evaluation metrics as well as training and decoding criteria.1 1 Introduction Current state of the art conditional text generation models accomplish a high level of fluency and coherence, mostly thanks to advances in sequenceto-sequence architectures with attention and copy (Sutskever et al., 2014; Bahdanau et al., 2015; Gu et al., 2016), fully attention-based Transformer architectures (Vaswani et al., 2017; Dai et al., 2019) and more recently pretrained language modeling for natural language understanding (Devlin et al., 2019; Radford et al., 2018; Yang et al., 2019; Liu et al., 2019). There has been a growing interest in ∗ The first two authors contributed equally. Our human annotated summaries for faithfulness and factuality will be released at https://github.com/google-researchdatasets/xsum hallucination annotations. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead"
2020.acl-main.173,2021.ccl-1.108,0,0.226531,"Missing"
2020.acl-main.173,W01-0100,0,0.502282,"tions. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to generate hallucinated text in conditional text generation, specifically, extreme abstractive document summarization (Narayan et al., 2018a). Document summarization — the task of producing a shorter version of a document while preserving its information content (Mani, 2001; Nenkova and McKeown, 2011) — requires models to generate text that is not only human-like but also faithful and/or factual given the document. The example in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text generators. The article describes an event of “Conservative MP Zac Smith winning the primary for 2016 London mayoral election”, but summaries often forge entities (e.g., “Nigel Goldsmith” or “Zac Goldwin”) or information (e.g., “UKIP leader Nigel Goldsmith”, “Nigel Goldsmith winning the mayoral election”, “Sadiq Khan being the former Lo"
2020.acl-main.173,D18-1206,1,0.740796,"uman annotated summaries for faithfulness and factuality will be released at https://github.com/google-researchdatasets/xsum hallucination annotations. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to generate hallucinated text in conditional text generation, specifically, extreme abstractive document summarization (Narayan et al., 2018a). Document summarization — the task of producing a shorter version of a document while preserving its information content (Mani, 2001; Nenkova and McKeown, 2011) — requires models to generate text that is not only human-like but also faithful and/or factual given the document. The example in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text generators. The article describes an event of “Conservative MP Zac Smith winning the primary for 2016 London mayoral election”, but summaries often forge entities (e.g., “Nigel Goldsmith” or “Zac Goldwin"
2020.acl-main.173,N18-1158,1,0.86231,"uman annotated summaries for faithfulness and factuality will be released at https://github.com/google-researchdatasets/xsum hallucination annotations. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to generate hallucinated text in conditional text generation, specifically, extreme abstractive document summarization (Narayan et al., 2018a). Document summarization — the task of producing a shorter version of a document while preserving its information content (Mani, 2001; Nenkova and McKeown, 2011) — requires models to generate text that is not only human-like but also faithful and/or factual given the document. The example in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text generators. The article describes an event of “Conservative MP Zac Smith winning the primary for 2016 London mayoral election”, but summaries often forge entities (e.g., “Nigel Goldsmith” or “Zac Goldwin"
2020.acl-main.173,N04-1019,0,0.253075,"quality of summaries (Nenkova, 2005). Most popular among them is the automatic metric ROUGE (Lin and Hovy, 2003) that measures the unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a proxy for assessing informativeness and the longest common subsequence (ROUGE-L), for fluency. ROUGE, however, can be misleading when used as the only means to assess the informativeness of summaries (Schluter, 2017). Hence, the ROUGE score is often complemented with subjective human assessment of summaries. More objective measures have been proposed to improve agreement among human annotators. Pyramid method (Nenkova and Passonneau, 2004) requires summaries to be annotated by experts for salient information. Narayan et al. (2018a,b) used a questionanswering based approach where a summary is used as context to answer questions which were written based on its reference summary. Hardy et al. (2019) proposed a reference-less approach where a summary is assessed against the source document, highlighted with its pertinent content. There has not been much work on evaluating faithfulness and truthfulness of abstractive summaries. The automatic evaluation such as ROUGE and the human evaluation of saliency and linguistic quality of summ"
2020.acl-main.173,P19-1459,0,0.0611913,"Missing"
2020.acl-main.173,N18-2102,0,0.222406,"ly sampled 500 articles from the test set to facilitate our study. Using the full test set was unfeasible given its size and the cost of human judgments. We have trained annotators (fluent in English) specifically for our assessment. Our annotators went through two pilot studies to have a better understanding of intrinsic and extrinsic hallucinations, and factuality of summaries. Documents used in the pilot studies were not used in the final annotation. We also report on ROUGE (Lin and Hovy, 2003) scores, BERTScore (Zhang et al., 2020) and semantic inference metric such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019; Falke et al., 2019; Kryscinski et al., 2019b) and question answering (Arumae and Liu, 2019; Wang et al., 2020). 5.1 Automatic Evaluation of Summaries ROUGE (Lin and Hovy, 2003) provides a means to quickly assess a model’s ability to generate summaries closer to human authored summaries. We report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L, for fluency. Like ROUGE, BERTScore (Zhang et al., 2020) computes a similarity score for each token in the candidate summary with each token in the reference summary. However, instead of exact matches, it computes token sim"
2020.acl-main.173,2020.tacl-1.18,1,0.943458,"tion (e.g., “UKIP leader Nigel Goldsmith”, “Nigel Goldsmith winning the mayoral election”, “Sadiq Khan being the former London mayor” or “Zac Goldwin being the Labour’s candidate”) that are not supported by the document or are factually wrong. Interestingly, all summaries are topical and fluent, and perform well in terms of ROUGE scores (Lin and Hovy, 2003). We conducted a large-scale human evaluation of hallucinated content in systems that use Recurrent Neural Network (RNN) (See et al., 2017), Convolutional Neural Network (CNN) (Narayan et al., 2018a), and Transformers (Radford et al., 2019; Rothe et al., 2020), as well as human written summaries for the recently introduced eXtreme S UMmarization task (XS UM, Narayan et al., 2018a). We seek to answer the following questions: (i) How frequently do abstractive summarizers hallucinate content?; (ii) Do models hal1906 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919 c July 5 - 10, 2020. 2020 Association for Computational Linguistics G OLD Zac Goldsmith will contest the 2016 London mayoral election for the Conservatives, it has been announced. D OCUMENT: The Richmond Park and North Kingston MP said"
2020.acl-main.173,E17-2007,0,0.0221606,"e a good balance between ROUGE and better faithfulness. 6 Related Work Following the Document Understanding Conference (DUC; Dang, 2005), a majority of work has focused on evaluating the content and the linguistic quality of summaries (Nenkova, 2005). Most popular among them is the automatic metric ROUGE (Lin and Hovy, 2003) that measures the unigram and bigram overlap (ROUGE-1 and ROUGE-2) as a proxy for assessing informativeness and the longest common subsequence (ROUGE-L), for fluency. ROUGE, however, can be misleading when used as the only means to assess the informativeness of summaries (Schluter, 2017). Hence, the ROUGE score is often complemented with subjective human assessment of summaries. More objective measures have been proposed to improve agreement among human annotators. Pyramid method (Nenkova and Passonneau, 2004) requires summaries to be annotated by experts for salient information. Narayan et al. (2018a,b) used a questionanswering based approach where a summary is used as context to answer questions which were written based on its reference summary. Hardy et al. (2019) proposed a reference-less approach where a summary is assessed against the source document, highlighted with i"
2020.acl-main.173,P17-1099,0,0.649396,"2016 London mayoral election”, but summaries often forge entities (e.g., “Nigel Goldsmith” or “Zac Goldwin”) or information (e.g., “UKIP leader Nigel Goldsmith”, “Nigel Goldsmith winning the mayoral election”, “Sadiq Khan being the former London mayor” or “Zac Goldwin being the Labour’s candidate”) that are not supported by the document or are factually wrong. Interestingly, all summaries are topical and fluent, and perform well in terms of ROUGE scores (Lin and Hovy, 2003). We conducted a large-scale human evaluation of hallucinated content in systems that use Recurrent Neural Network (RNN) (See et al., 2017), Convolutional Neural Network (CNN) (Narayan et al., 2018a), and Transformers (Radford et al., 2019; Rothe et al., 2020), as well as human written summaries for the recently introduced eXtreme S UMmarization task (XS UM, Narayan et al., 2018a). We seek to answer the following questions: (i) How frequently do abstractive summarizers hallucinate content?; (ii) Do models hal1906 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919 c July 5 - 10, 2020. 2020 Association for Computational Linguistics G OLD Zac Goldsmith will contest the 2016 Londo"
2020.acl-main.173,K19-1079,0,0.10635,"al language understanding (Devlin et al., 2019; Radford et al., 2018; Yang et al., 2019; Liu et al., 2019). There has been a growing interest in ∗ The first two authors contributed equally. Our human annotated summaries for faithfulness and factuality will be released at https://github.com/google-researchdatasets/xsum hallucination annotations. 1 understanding how maximum likelihood training and approximate beam-search decoding in these models lead to less human-like text in open-ended text generation such as language modeling and story generation (Holtzman et al., 2020; Welleck et al., 2020; See et al., 2019). In this paper we investigate how these models are prone to generate hallucinated text in conditional text generation, specifically, extreme abstractive document summarization (Narayan et al., 2018a). Document summarization — the task of producing a shorter version of a document while preserving its information content (Mani, 2001; Nenkova and McKeown, 2011) — requires models to generate text that is not only human-like but also faithful and/or factual given the document. The example in Figure 1 illustrates that the faithfulness and factuality are yet to be conquered by conditional text gener"
2020.acl-main.173,P19-1363,0,0.0636683,"m the test set to facilitate our study. Using the full test set was unfeasible given its size and the cost of human judgments. We have trained annotators (fluent in English) specifically for our assessment. Our annotators went through two pilot studies to have a better understanding of intrinsic and extrinsic hallucinations, and factuality of summaries. Documents used in the pilot studies were not used in the final annotation. We also report on ROUGE (Lin and Hovy, 2003) scores, BERTScore (Zhang et al., 2020) and semantic inference metric such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019; Falke et al., 2019; Kryscinski et al., 2019b) and question answering (Arumae and Liu, 2019; Wang et al., 2020). 5.1 Automatic Evaluation of Summaries ROUGE (Lin and Hovy, 2003) provides a means to quickly assess a model’s ability to generate summaries closer to human authored summaries. We report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L, for fluency. Like ROUGE, BERTScore (Zhang et al., 2020) computes a similarity score for each token in the candidate summary with each token in the reference summary. However, instead of exact matches, it computes token similarity using contextu"
2020.acl-main.173,N18-1101,0,0.0295043,"D. For factuality, the differences between P T G EN, TC ONV S2S, and T RAN S2S were insignificant. 5.4 Automatic Measures for Hallucinations Summaries are a proxy for their source documents under the assumption that they highlight the most important content. With this assumption, we further studied the extent to which the hallucinated content can be measured by semantic inference related measures, such as textual entailment and question answering. Textual Entailment. We trained an entailment classifier by finetuning a BERT-Large pretrained model (Devlin et al., 2019) on the Multi-NLI dataset (Williams et al., 2018). We calculated the entailment probability score between the document and its abstractive summaries. Note that this entailment classifier is not optimal for the BBC article-summary pairs; the Multi-NLI dataset contains sentence-sentence pairs. Ideally a summary should entail the document or perhaps be neutral to the document, but never contradict the document. As can be seen in Table 3, the B ERT S2S abstracts showed the least number of 5 See Appendix for full results. Models P T G EN TC ONV S2S T RAN S2S B ERT S2S G OLD Textual Entailment entail. neut. cont. 38.4 34.4 27.2 29.6 37.4 33.0 34.6"
2020.acl-main.173,2020.acl-main.450,0,0.364319,"n judgments. We have trained annotators (fluent in English) specifically for our assessment. Our annotators went through two pilot studies to have a better understanding of intrinsic and extrinsic hallucinations, and factuality of summaries. Documents used in the pilot studies were not used in the final annotation. We also report on ROUGE (Lin and Hovy, 2003) scores, BERTScore (Zhang et al., 2020) and semantic inference metric such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019; Falke et al., 2019; Kryscinski et al., 2019b) and question answering (Arumae and Liu, 2019; Wang et al., 2020). 5.1 Automatic Evaluation of Summaries ROUGE (Lin and Hovy, 2003) provides a means to quickly assess a model’s ability to generate summaries closer to human authored summaries. We report on ROUGE-1 and ROUGE-2 for informativeness and ROUGE-L, for fluency. Like ROUGE, BERTScore (Zhang et al., 2020) computes a similarity score for each token in the candidate summary with each token in the reference summary. However, instead of exact matches, it computes token similarity using contextual embeddings. Results are presented in Table 1. For both cases, the pretrained encoder-decoder architecture B E"
2020.acl-main.173,N03-1020,0,\N,Missing
2020.acl-main.173,D17-1239,0,\N,Missing
2020.acl-main.173,D18-2012,0,\N,Missing
2020.acl-main.173,D18-1443,0,\N,Missing
2020.acl-main.173,Q19-1026,0,\N,Missing
2020.acl-main.173,N19-1264,0,\N,Missing
2020.acl-main.173,P19-1330,1,\N,Missing
2020.acl-main.173,P19-1620,0,\N,Missing
2020.acl-main.173,P19-1213,0,\N,Missing
2020.acl-main.173,N19-1423,0,\N,Missing
2020.acl-main.173,P19-1285,0,\N,Missing
2020.bionlp-1.15,D19-1371,0,0.0398295,"Missing"
2020.bionlp-1.15,W19-5039,0,0.0251323,"names they had to rely more on the text of the passage and question, which they had trouble understanding. By contrast, the agreement of experts was slightly higher in Setting B than Setting A, possibly because without prior knowledge about the entities, which may differ across experts, they had to rely to a larger extent on the particular text of the passage and question. 4 Related work Several biomedical MRC datasets exist, but have orders of magnitude fewer questions than BIOMRC (Ben Abacha and Demner-Fushman, 2019) or are not suitable for a cloze-style MRC task (Pampari et al., 2018; Ben Abacha et al., 2019; Zhang et al., ˇ 2018). The closest dataset to ours is CLICR (Suster and Daelemans, 2018), a biomedical MRC dataset with cloze-type questions created using full-text articles from BMJ case reports.13 CLICR contains 100k passage-question instances, the same number as BIOMRC LITE, but much fewer than the ˇ 812.7k instances of BIOMRC LARGE. Suster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the UMLS Metathesaurus (Lindberg et al., 1993). Cloze-style questions were created from the ‘learning points’ (summaries of important information) of the"
2020.bionlp-1.15,P16-1223,0,0.0479938,"Missing"
2020.bionlp-1.15,P17-1171,0,0.0198668,"with cloze-type questions created using full-text articles from BMJ case reports.13 CLICR contains 100k passage-question instances, the same number as BIOMRC LITE, but much fewer than the ˇ 812.7k instances of BIOMRC LARGE. Suster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the UMLS Metathesaurus (Lindberg et al., 1993). Cloze-style questions were created from the ‘learning points’ (summaries of important information) of the reports, by replacing biomedical entities with ˇ placeholders. Suster et al. experimented with the Stanford Reader (Chen et al., 2017) and the GatedAttention Reader (Dhingra et al., 2017), which perform worse than AOA - READER (Cui et al., 2017). The QA dataset of BIOASQ (Tsatsaronis et al., 2015) contains questions written by biomedical experts. The gold answers comprise multiple relevant documents per question, relevant snippets from the documents, exact answers in the form of entities, as well as reference summaries, written by the ex13 https://casereports.bmj.com/ Kappa 70.23 65.61 72.30 47.22 perts. Creating data of this kind, however, requires significant expertise and time. In the eight years of BIOASQ, only 3,243 que"
2020.bionlp-1.15,P17-1055,0,0.120762,"en n-grams from the passage that include an entity identifier (@entityN ), and all the n-grams from the question that include the placeholder (XXXX).7 Then for each candidate answer (entity identifier), it counts the tokens shared between the n-grams that include the candidate and the n-grams that include the placeholder. The candidate with the most shared tokens is selected. These baselines are used to check that the questions cannot be answered by simplistic heuristics (Chen et al., 2016). Neural baselines: We use the same implementations of AS - READER (Kadlec et al., 2016) and AOA READER (Cui et al., 2017) as Pappas et al. (2018), who also provide short descriptions of these neural models, not provided here to save space. The hyper-parameters of both methods were tuned on the development set of BIOMRC LITE. BERT -based Experiments and Results We experimented only on BIOMRC LITE and TINY, since we did not have the computational resources to train the neural models we considered on the LARGE version of BIOREAD . Pappas et al. (2018) also reported experimental results only on a LITE version of their BIOREAD dataset. We hope that others may be able to experiment on BIOMRC LARGE, and we make our cod"
2020.bionlp-1.15,N19-1423,0,0.171015,"NY, since we did not have the computational resources to train the neural models we considered on the LARGE version of BIOREAD . Pappas et al. (2018) also reported experimental results only on a LITE version of their BIOREAD dataset. We hope that others may be able to experiment on BIOMRC LARGE, and we make our code available, as already noted. 3.1 Methods We experimented with the four basic baselines (BASE 1–4) that Pappas et al. (2018) used in BIOREAD , the two neural MRC models used by the same authors, AS - READER (Kadlec et al., 2016) and AOA - READER (Cui et al., 2017), and a BERTbased (Devlin et al., 2019) model we developed. Basic baselines: BASE 1, 2, 3 return the first, last, and the entity that occurs most frequently in the passage (or randomly one of the entities with the same highest frequency, if multiple exist), respectively. Since in BIOREAD the correct answer is never (by construction) the most frequent entity of the passage, unless there are multiple entities with the same highest frequency, BASE 3 performs poorly. Hence, we also include a variant, BASE 3+, which randomly selects one of the entities of the 143 model: We use SCIBERT (Beltagy et al., 2019), a pre-trained BERT (Devlin e"
2020.bionlp-1.15,P17-1168,0,0.0189158,"articles from BMJ case reports.13 CLICR contains 100k passage-question instances, the same number as BIOMRC LITE, but much fewer than the ˇ 812.7k instances of BIOMRC LARGE. Suster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the UMLS Metathesaurus (Lindberg et al., 1993). Cloze-style questions were created from the ‘learning points’ (summaries of important information) of the reports, by replacing biomedical entities with ˇ placeholders. Suster et al. experimented with the Stanford Reader (Chen et al., 2017) and the GatedAttention Reader (Dhingra et al., 2017), which perform worse than AOA - READER (Cui et al., 2017). The QA dataset of BIOASQ (Tsatsaronis et al., 2015) contains questions written by biomedical experts. The gold answers comprise multiple relevant documents per question, relevant snippets from the documents, exact answers in the form of entities, as well as reference summaries, written by the ex13 https://casereports.bmj.com/ Kappa 70.23 65.61 72.30 47.22 perts. Creating data of this kind, however, requires significant expertise and time. In the eight years of BIOASQ, only 3,243 questions and gold answers have been created. It would b"
2020.bionlp-1.15,P16-1086,0,0.0241129,"requency. BASE 4 extracts all the token n-grams from the passage that include an entity identifier (@entityN ), and all the n-grams from the question that include the placeholder (XXXX).7 Then for each candidate answer (entity identifier), it counts the tokens shared between the n-grams that include the candidate and the n-grams that include the placeholder. The candidate with the most shared tokens is selected. These baselines are used to check that the questions cannot be answered by simplistic heuristics (Chen et al., 2016). Neural baselines: We use the same implementations of AS - READER (Kadlec et al., 2016) and AOA READER (Cui et al., 2017) as Pappas et al. (2018), who also provide short descriptions of these neural models, not provided here to save space. The hyper-parameters of both methods were tuned on the development set of BIOMRC LITE. BERT -based Experiments and Results We experimented only on BIOMRC LITE and TINY, since we did not have the computational resources to train the neural models we considered on the LARGE version of BIOREAD . Pappas et al. (2018) also reported experimental results only on a LITE version of their BIOREAD dataset. We hope that others may be able to experiment on"
2020.bionlp-1.15,D17-1082,0,0.0211979,"used to pre-train models, which could then be fine-tuned for human-generated QA or MRC datasets. Outside the biomedical domain, several clozestyle open-domain MRC datasets have been created automatically (Hill et al., 2016; Hermann et al., 2015; Dunn et al., 2017; Bajgar et al., 2016), but have been criticized of containing questions that can be answered by simple heuristics like our basic baselines (Chen et al., 2016). There are also several large open-domain MRC datasets annotated by humans (Kwiatkowski et al., 2019; Rajpurkar et al., 2016, 2018; Trischler et al., 2017; Nguyen et al., 2016; Lai et al., 2017). To our knowledge the biggest human annotated corpus is Google’s Natural Questions dataset (Kwiatkowski et al., 2019), with approximately 300k human annotated examples. Datasets of this kind require extensive annotation effort, which for open-domain datasets is usually crowd-sourced. Crowd-sourcing, however, is much more difficult for biomedical datasets, because of the required expertise of the annotators. 5 Conclusions and Future Work We introduced BIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to the previous BIOREAD dataset of Pappas et"
2020.bionlp-1.15,D18-1258,0,0.07324,"bly because without entity names they had to rely more on the text of the passage and question, which they had trouble understanding. By contrast, the agreement of experts was slightly higher in Setting B than Setting A, possibly because without prior knowledge about the entities, which may differ across experts, they had to rely to a larger extent on the particular text of the passage and question. 4 Related work Several biomedical MRC datasets exist, but have orders of magnitude fewer questions than BIOMRC (Ben Abacha and Demner-Fushman, 2019) or are not suitable for a cloze-style MRC task (Pampari et al., 2018; Ben Abacha et al., 2019; Zhang et al., ˇ 2018). The closest dataset to ours is CLICR (Suster and Daelemans, 2018), a biomedical MRC dataset with cloze-type questions created using full-text articles from BMJ case reports.13 CLICR contains 100k passage-question instances, the same number as BIOMRC LITE, but much fewer than the ˇ 812.7k instances of BIOMRC LARGE. Suster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the UMLS Metathesaurus (Lindberg et al., 1993). Cloze-style questions were created from the ‘learning points’ (summaries of impo"
2020.bionlp-1.15,L18-1439,1,0.855691,"them (Rajpurkar et al., 2016, 2018; Nguyen et al., 2016). They require no human annotations, however, which is particularly important in biomedical question answering, where employing annotators with appropriate expertise is costly. For example, the BIOASQ QA dataset (Tsatsaronis et al., 2015) currently contains approximately 3k questions, much fewer than the 100k questions of a SQUAD (Rajpurkar et al., 2016), exactly because it relies on expert annotators. To bypass the need for expert annotators and produce a biomedical MRC dataset large enough to train (or pre-train) deep learning models, Pappas et al. (2018) adopted the cloze-style questions approach. They used the full text of unlabeled biomedical articles from PUBMED CENTRAL,1 and METAMAP (Aronson and Lang, 2010) to annotate the biomedical entities of the articles. They extracted sequences of 21 sentences from the articles. The first 20 sentences were used as a passage and the last sentence as a cloze-style question. A biomedical entity of the ‘question’ was replaced by a placeholder, and systems have to guess which biomedical entity of the passage can best fill the placeholder. This allowed Pappas et al. to produce a dataset, called BIOREAD, o"
2020.bionlp-1.15,P18-2124,0,0.0513082,"Missing"
2020.bionlp-1.15,D16-1264,0,0.350538,"ned (and evaluated for their ability) to read the passage and the smaller text, and guess the named entity that was replaced by the placeholder, which is typically one of the named entities of the passage. This kind of question answering (QA) is also known as cloze-type questions (Taylor, 1953). Several datasets have been created following this approach either using books (Hill et al., 2016; Bajgar et al., 2016) or news articles (Hermann et al., 2015). Datasets of this kind are noisier than MRC datasets containing human-authored questions and manually annotated passage spans that answer them (Rajpurkar et al., 2016, 2018; Nguyen et al., 2016). They require no human annotations, however, which is particularly important in biomedical question answering, where employing annotators with appropriate expertise is costly. For example, the BIOASQ QA dataset (Tsatsaronis et al., 2015) currently contains approximately 3k questions, much fewer than the 100k questions of a SQUAD (Rajpurkar et al., 2016), exactly because it relies on expert annotators. To bypass the need for expert annotators and produce a biomedical MRC dataset large enough to train (or pre-train) deep learning models, Pappas et al. (2018) adopted"
2020.bionlp-1.15,N18-1140,0,0.0175582,"ad trouble understanding. By contrast, the agreement of experts was slightly higher in Setting B than Setting A, possibly because without prior knowledge about the entities, which may differ across experts, they had to rely to a larger extent on the particular text of the passage and question. 4 Related work Several biomedical MRC datasets exist, but have orders of magnitude fewer questions than BIOMRC (Ben Abacha and Demner-Fushman, 2019) or are not suitable for a cloze-style MRC task (Pampari et al., 2018; Ben Abacha et al., 2019; Zhang et al., ˇ 2018). The closest dataset to ours is CLICR (Suster and Daelemans, 2018), a biomedical MRC dataset with cloze-type questions created using full-text articles from BMJ case reports.13 CLICR contains 100k passage-question instances, the same number as BIOMRC LITE, but much fewer than the ˇ 812.7k instances of BIOMRC LARGE. Suster et al. used CLAMP (Soysal et al., 2017) to detect biomedical entities and link them to concepts of the UMLS Metathesaurus (Lindberg et al., 1993). Cloze-style questions were created from the ‘learning points’ (summaries of important information) of the reports, by replacing biomedical entities with ˇ placeholders. Suster et al. experimented"
2020.bionlp-1.15,W17-2623,0,0.0355402,"ated datasets like BIOMRC and CLICR could be used to pre-train models, which could then be fine-tuned for human-generated QA or MRC datasets. Outside the biomedical domain, several clozestyle open-domain MRC datasets have been created automatically (Hill et al., 2016; Hermann et al., 2015; Dunn et al., 2017; Bajgar et al., 2016), but have been criticized of containing questions that can be answered by simple heuristics like our basic baselines (Chen et al., 2016). There are also several large open-domain MRC datasets annotated by humans (Kwiatkowski et al., 2019; Rajpurkar et al., 2016, 2018; Trischler et al., 2017; Nguyen et al., 2016; Lai et al., 2017). To our knowledge the biggest human annotated corpus is Google’s Natural Questions dataset (Kwiatkowski et al., 2019), with approximately 300k human annotated examples. Datasets of this kind require extensive annotation effort, which for open-domain datasets is usually crowd-sourced. Crowd-sourcing, however, is much more difficult for biomedical datasets, because of the required expertise of the annotators. 5 Conclusions and Future Work We introduced BIOMRC, a large-scale cloze-style biomedical MRC dataset. Care was taken to reduce noise, compared to th"
2020.bionlp-1.15,P18-1128,0,\N,Missing
2020.bionlp-1.15,Q19-1026,0,\N,Missing
2020.emnlp-main.339,2020.emnlp-main.19,0,0.168895,"is paper, we propose encoder-centric stepwise models for extractive summarization using 4143 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4143–4159, c November 16–20, 2020. 2020 Association for Computational Linguistics structured transformers. Structured transformers are transformer-based architectures that have the flexibility to model some form of structure of the input, e.g., hierarchical document structure. In this paper, we specifically study two such architectures – HiBERT (Zhang et al., 2019) and Extended Transformers Construction (ETC; Ainslie et al., 2020). Details of these are given in Sections 4 and 5. We enable stepwise summarization by injecting the previously planned summary content into the structured transformer as an auxiliary sub-structure. The model then can holistically learn any documentlevel coherence properties, such as saliency, redundancy, and ordering, embodied in the gold summaries. This differs from other methods which are either task specific (e.g., redundancy aware modeling in Bi et al., 2020) or not holistic (e.g., manually curated features in Liu et al., 2019a). An added advantage of structured encoders is that they break"
2020.emnlp-main.339,P16-1046,0,0.370803,"r content selection, planning and ordering, highlighting the strength of stepwise modeling. Amongst the two structured transformers we test, stepwise Extended Transformers provides the best performance across both datasets and sets a new standard for these challenges.1 1 Introduction Extractive document summarization is the task of creating a summary by identifying (and subsequently concatenating) the most important sentences in a document (Erkan and Radev, 2004; Nenkova and McKeown, 2011). In recent years this task has matured significantly, mostly thanks to advances in deep neural networks. Cheng and Lapata (2016) conceptualize extractive summarization as a sequence labeling task in which first a hierarchical long short-term memory network (LSTM; ∗ Equal contribution. The code and data are available at https://github. com/google-research/google-research/ tree/master/etcsum. 1 Hochreiter and Schmidhuber, 1997) is used to encode a document and then another LSTM is used to predict for each sentence whether it should be included in the summary. This architecture was later adopted by Nallapati et al. (2016a), Nallapati et al. (2017), Narayan et al. (2018b), Zhang et al. (2018) and Dong et al. (2018). Follow"
2020.emnlp-main.339,P19-1500,0,0.105669,"ber, 1997) is used to encode a document and then another LSTM is used to predict for each sentence whether it should be included in the summary. This architecture was later adopted by Nallapati et al. (2016a), Nallapati et al. (2017), Narayan et al. (2018b), Zhang et al. (2018) and Dong et al. (2018). Following the success of pre-trained transformerbased architectures for many tasks (Vaswani et al., 2017; Devlin et al., 2019), the current state-of-theart approach to extractive summarization uses transformers to learn sentence representations and to rank sentences by their saliency (Liu, 2019; Liu and Lapata, 2019b; Zhang et al., 2019; Zhong et al., 2019a; Bi et al., 2020). The top scoring sentences are then assembled to produce an extract of the document. Summaries built in this fashion (Cheng and Lapata, 2016; Narayan et al., 2018a; Zhang et al., 2018; Dong et al., 2018) are prone to contain redundant information. Several recent approaches have explored mechanisms to better handle redundancy, such as heuristic-based Trigram Blocking (TriBlk; Liu and Lapata, 2019b; Wang et al., 2020), handcrafted feature-driven models (Ren et al., 2017) and redundancy aware neural sequence models (Zhou et al., 2018; B"
2020.emnlp-main.339,D19-1387,0,0.12856,"ber, 1997) is used to encode a document and then another LSTM is used to predict for each sentence whether it should be included in the summary. This architecture was later adopted by Nallapati et al. (2016a), Nallapati et al. (2017), Narayan et al. (2018b), Zhang et al. (2018) and Dong et al. (2018). Following the success of pre-trained transformerbased architectures for many tasks (Vaswani et al., 2017; Devlin et al., 2019), the current state-of-theart approach to extractive summarization uses transformers to learn sentence representations and to rank sentences by their saliency (Liu, 2019; Liu and Lapata, 2019b; Zhang et al., 2019; Zhong et al., 2019a; Bi et al., 2020). The top scoring sentences are then assembled to produce an extract of the document. Summaries built in this fashion (Cheng and Lapata, 2016; Narayan et al., 2018a; Zhang et al., 2018; Dong et al., 2018) are prone to contain redundant information. Several recent approaches have explored mechanisms to better handle redundancy, such as heuristic-based Trigram Blocking (TriBlk; Liu and Lapata, 2019b; Wang et al., 2020), handcrafted feature-driven models (Ren et al., 2017) and redundancy aware neural sequence models (Zhou et al., 2018; B"
2020.emnlp-main.339,2021.ccl-1.108,0,0.0813545,"Missing"
2020.emnlp-main.339,N19-1397,1,0.942134,"cking (TriBlk; Liu and Lapata, 2019b; Wang et al., 2020), handcrafted feature-driven models (Ren et al., 2017) and redundancy aware neural sequence models (Zhou et al., 2018; Bi et al., 2020). One common problem with these models is that their focus is limited to content overlap and to respecting length budgets. However, these are but a small subset of the dimensions necessary to produce informative and coherent summaries. Ideally, models would utilize enriched document and summary representations in order to implicitly learn better extractive plans for producing summaries (Liu et al., 2019a; Mendes et al., 2019). One such method is stepwise summarization (Liu et al., 2019a), where a summary is constructed incrementally by choosing new content conditioned on previously planned content. In this paper, we propose encoder-centric stepwise models for extractive summarization using 4143 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4143–4159, c November 16–20, 2020. 2020 Association for Computational Linguistics structured transformers. Structured transformers are transformer-based architectures that have the flexibility to model some form of structure of the"
2020.emnlp-main.339,D19-5633,0,0.028386,"2017). Liu and Lapata (2019a) used cross-document attention mechanism to share information as opposed to simply concatenating text spans using hierarchical transformers. Similar to this motivation, we also explore better encoding of long inputs with structured transformers. Table-to-Text Content Planning. Wiseman et al. (2017) introduced the Rotowire dataset, which requires multi-sentence summaries of large tables. Several works found that the key to generate fluent and informative summaries for this task is to have dedicated content planning and realization steps (Puduppully et al., 2019a,c; Miculicich et al., 2019). Miculicich et al. (2019) and Gong et al. (2019b) used a transformer encoder, and, Gong et al. (2019a) used multi-dimensional hierarchical LSTM encoders to compute better table entry representations. Following these lines of work, we evaluate our models to generate long content plans for this task using structured transformers. 3 Problem: Stepwise Content Extraction We define a general paradigm for stepwise content extraction that can be easily tailored to both extractive summarization and table-to-text generation. Given an input D = {s1 , s2 , . . . , sn } with n content units, the goal is t"
2020.emnlp-main.339,D19-5630,0,0.35119,"learn and leverage content plans directly from the data. Moreover, structured transformers form the basis of our model, which are flexible in terms of content type (e.g., text or tables) that can be modeled. We demonstrate this by learning intricate extractive content plan for the Rotowire table-to-text generation task (Wiseman et al., 2017). This task requires the generation of long summaries from large score tables detailing the the specifics of a sports match, which often necessitates dedicated content selection and planning models to generate a high-quality summary (Wiseman et al., 2017; Puduppully et al., 2019a). We show that our stepwise framework achieves higher content selection, planning and ordering scores relative to prior work with task-specific planning mechanisms. The contributions of the paper are as follows: 1) this is first study to use ETC (Ainslie et al., 2020) for summarization for its ability and flexibility to better model long and structured inputs; 2) we propose augmentions of two structured transformers, HiBERT and ETC, in order to enable stepwise models for extractive planning; 3) we demonstrate empirically that our models are general purpose and can be adapted as an extractive"
2020.emnlp-main.339,2020.tacl-1.18,1,0.685354,"ly: content planning, which consists of selecting which records in the table should be mentioned in the summary, in what order, and how they should be organized into sentences; and realization, which uses the content plan to create a human-readable summary. We refer the reader to the supplementary material for an example. Our main focus in this paper is to demonstrate our models’ ability to model long and structured Rotowire input tables, and generate long meaningful content plans. For realization, we simply use a RoBERTa (Liu et al., 2019b) initialized sequence-to-sequence transformer model (Rothe et al., 2020), trained to emit the realization sentence by sentence. We train our stepwise models to take a score table and the partially generated content plan, and predict the next element in the content plan. This can be either one of the entries in the score table, a sentence break or a token marking the end of the plan. Unlike extractive summarization, here an optimal extractive content plan can have repeated entries from the input table (e.g. team names) to better preserve and generate discourse relations among sentences in the target summary (Puduppully et al., 2019b), making it a challenging task f"
2020.emnlp-main.339,2020.acl-main.553,0,0.0917732,"summarization uses transformers to learn sentence representations and to rank sentences by their saliency (Liu, 2019; Liu and Lapata, 2019b; Zhang et al., 2019; Zhong et al., 2019a; Bi et al., 2020). The top scoring sentences are then assembled to produce an extract of the document. Summaries built in this fashion (Cheng and Lapata, 2016; Narayan et al., 2018a; Zhang et al., 2018; Dong et al., 2018) are prone to contain redundant information. Several recent approaches have explored mechanisms to better handle redundancy, such as heuristic-based Trigram Blocking (TriBlk; Liu and Lapata, 2019b; Wang et al., 2020), handcrafted feature-driven models (Ren et al., 2017) and redundancy aware neural sequence models (Zhou et al., 2018; Bi et al., 2020). One common problem with these models is that their focus is limited to content overlap and to respecting length budgets. However, these are but a small subset of the dimensions necessary to produce informative and coherent summaries. Ideally, models would utilize enriched document and summary representations in order to implicitly learn better extractive plans for producing summaries (Liu et al., 2019a; Mendes et al., 2019). One such method is stepwise summar"
2020.emnlp-main.339,K17-1045,0,0.0197473,", our models are first to use summary representations with structured transformers for summarization. Our models learn to make summary-informed next-sentence predictions without any hand-curated features. 4144 Long-form Summarization. It is well known that a better content selection benefits abstractive summarizers to generate summaries that are not only fluent but also informative (Gehrmann et al., 2018; Hsu et al., 2018; Xiao et al., 2020). It can be particularly important when generating long abstractive summaries (Liu et al., 2018; Liu and Lapata, 2019a) or summarizing multiple documents (Yasunaga et al., 2017). Earlier multi-document summarization methods have addressed the issue of long form input by graph-based representations of sentences or passages (Erkan and Radev, 2004; Christensen et al., 2013). Recently, Yasunaga et al. (2017) proposed a neural version of this framework using graph convolutional networks (Kipf and Welling, 2017). Liu and Lapata (2019a) used cross-document attention mechanism to share information as opposed to simply concatenating text spans using hierarchical transformers. Similar to this motivation, we also explore better encoding of long inputs with structured transforme"
2020.emnlp-main.339,D18-1088,0,0.125588,"s in deep neural networks. Cheng and Lapata (2016) conceptualize extractive summarization as a sequence labeling task in which first a hierarchical long short-term memory network (LSTM; ∗ Equal contribution. The code and data are available at https://github. com/google-research/google-research/ tree/master/etcsum. 1 Hochreiter and Schmidhuber, 1997) is used to encode a document and then another LSTM is used to predict for each sentence whether it should be included in the summary. This architecture was later adopted by Nallapati et al. (2016a), Nallapati et al. (2017), Narayan et al. (2018b), Zhang et al. (2018) and Dong et al. (2018). Following the success of pre-trained transformerbased architectures for many tasks (Vaswani et al., 2017; Devlin et al., 2019), the current state-of-theart approach to extractive summarization uses transformers to learn sentence representations and to rank sentences by their saliency (Liu, 2019; Liu and Lapata, 2019b; Zhang et al., 2019; Zhong et al., 2019a; Bi et al., 2020). The top scoring sentences are then assembled to produce an extract of the document. Summaries built in this fashion (Cheng and Lapata, 2016; Narayan et al., 2018a; Zhang et al., 2018; Dong et al.,"
2021.acl-long.120,N19-1423,0,0.192867,"kens) as its definition includes different entity classes and tokens in training and testing. It is possible that words observed as non-entities during training belong to one of the test classes, as seen in Figure 1: both Huaqiao Park, in training, and Shantou Harbour, during testing, are entities of the class Facility, however, Huaqiao Park is labelled as a non-entity in the former. Based on this insight we propose several architectures for NERC based on cross-attention between the sentence and the entity type descriptions using transformers (Vaswani et al., 2017) combined with pre-training (Devlin et al., 2019). We 1516 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1516–1528 August 1–6, 2021. ©2021 Association for Computational Linguistics Dev Test Train sentence str with annotation Shantou Harbour , a natural river seaport , The Shantou Science and Technology Museum O B-ORG I-ORG I-ORG I-ORG I-ORG completed construction in November of O O O B-DATE I-DATE I-DATE O O ORG O O Used to classify a companies, government reference to a date or period ... agencies ... PERSON O O O O O"
2021.acl-long.120,C18-1161,0,0.033749,"Missing"
2021.acl-long.120,W18-5036,0,0.021525,"(Obamuyide and Vlachos, 2018) who pose the task as one of textual entailment. Obeidat et al. (2019) use descriptions for zero-shot NET, however, similar to a previous attempt by Ma et al. (2016), they use the underlying hierarchy to only include unseen classes in the leaves of the hierarchy to reduce the relevant unseen classes to only two or three. The only work on zero-shot word sequence labelling (Rei and Søgaard, 2018) explores the transfer from labels on a sentence level objective (e.g. sentiment analysis) to a token or phrase-based annotation, similar to T¨ackstr¨om and McDonald (2011). Guerini et al. (2018) label their approach zero-shot named entity recognition, however, they focus on recognizing unseen entities not entity classes. Finally, Fritzler et al. (2019) focused on few-shot NERC using prototypical networks (Snell et al., 2017). They tested their model in the zero-shot setting, but concluded that their approach is not suitable for zero-shot learning as the results on OntoNotes were too low. 6 Conclusions & Future work This paper explored the task of zero-shot NERC with entity type descriptions to transfer knowledge from observed to unseen classes. We addressed the zero-shot NERC specifi"
2021.acl-long.120,P19-1356,0,0.0147284,"1,neg , ..., vn,neg = ENC(s), We label this model Sequential Multiclass Crossattention Model (SMXM). Referring to the initial example, cross-attention enables Shantou Harbour to attend to infrastructure in the type description of the class Facility, generating a representation for this token based on the type description in the context of the sentence. Cross-attention Encoder The cross-attention model is based on the pre-trained transformer encoder BERT (Devlin et al., 2019) which allows the model to capture surface-level information as well as semantic aspects between all words of the input (Jawahar et al., 2019). For X-ENC the input tuple (s, dc ) is structured in the form: xX-ENC = [CLS] s [SEP] dc [SEP]. 2.1 Modelling the negative class As discussed in Section 1, the non-entity class creates a challenging setting it is possible that words observed as non-entities during training belong to one of the test classes. We explore three approaches to modelling the negative class: (i) using a (textual) description for the negative class, (ii) modelling the negative class directly, (iii) modelling the negative class using the representations generated for the classes corresponding to types. Description-base"
2021.acl-long.120,N16-1030,0,0.0279013,"An analysis on the classification and recognition task in isolation highlights the importance of the description choice, finding that annotation guidelines result in higher scores than the class name itself or Wikipedia passages. 2 Zero-shot NERC In NERC, given a sentence s = w1 , ..., wn of length n and a description dc for each class c ∈ Cts in the test set, we predict a sequence of labels y ˆ ∈ (Cts )n , with n being the length of the sentence. We model the task as multiclass classification, which despite ignoring the sequential structure of the output, it has been found to be competitive (Lample et al., 2016; Rei, 2017). Thus, we predict the correct class for each token w at position t: arg maxc∈Cts F (s, wt , dc ), using a suitable function F modelling the semantic affinity between wt and dc in the context of s. The parameters of F need to be learned without annotated data for Cts , but with annotated data and descriptions for the training C tr classes. To model F we focus on the use of crossattention (Humeau et al., 2019; Wolf et al., 2019b) in the form of a transformer encoder (Vaswani et al., 2017). For each type description dc , the cross-attention encoder (X-ENC) generates a vector represen"
2021.acl-long.120,D17-1282,0,0.0502386,"Missing"
2021.acl-long.120,2020.acl-main.519,0,0.443333,"ntations generated for the classes corresponding to types. For evaluation we introduce zero-shot adaptations to two real-world NERC datasets with distinct properties: the OntoNotes (Pradhan et al., 2013) as well as the highly domain-specific MedMentions dataset (Mohan and Li, 2019). The adaptations adhere to recommendations to zeroshot evaluation (Xian et al., 2018) by evaluating models on the rarest classes while ensuring that all class sets are disjoint. Our best model achieves a macro F1 of 0.45 on OntoNotes-ZS and 0.38 on MedMentions-ZS, outperforming a state-ofthe-art MRC model for NERC (Li et al., 2020; Sun et al., 2020) and an adapted zero-shot text classification model (Yin et al., 2019). An analysis on the classification and recognition task in isolation highlights the importance of the description choice, finding that annotation guidelines result in higher scores than the class name itself or Wikipedia passages. 2 Zero-shot NERC In NERC, given a sentence s = w1 , ..., wn of length n and a description dc for each class c ∈ Cts in the test set, we predict a sequence of labels y ˆ ∈ (Cts )n , with n being the length of the sentence. We model the task as multiclass classification, which des"
2021.acl-long.120,P19-1335,0,0.128727,"assifying these spans from a set of pre-defined entity classes. A prevalent issue for many real-world applications is that annotated data does not readily exist. This motivates the focus on the zero-shot setting (Xian et al., 2018; Wang et al., 2019), where annotated data is not available for the classes of interest. Instead, information available from observed classes must be transferred to unseen target classes. Recently zero-shot approaches making use of textual representations to represent entity classes ∗ Work done when author was working at Google. were explored for entity linking (EL) (Logeswaran et al., 2019; Wu et al., 2020) and named entity typing (NET) (Obeidat et al., 2019), which are similar to the NERC subtask of named entity classification (NEC). However, no previous work has addressed the task of zero-shot NERC, which additionally requires the detection of which tokens make up an entity in addition to its type, i.e. Named Entity Recognition (NER). This paper is the first to study zero-shot NERC, by leveraging entity type descriptions. The task is illustrated in Figure 1. During testing, the input is a sentence and a set of target entity classes. each accompanied by its description, and th"
2021.acl-long.120,C16-1017,0,0.0142431,"isolation which appears to help the model extract more useful supervision signal, as indicated by the higher validation F1 achieved. When trained with masking, SMXM’s training loss closely follows the trend of the validation F1 , indicating good transfer learning from the model’s training objective to the zero-shot evaluation. 5 Related Work fined class descriptions have also been explored for relation classification (Obamuyide and Vlachos, 2018) who pose the task as one of textual entailment. Obeidat et al. (2019) use descriptions for zero-shot NET, however, similar to a previous attempt by Ma et al. (2016), they use the underlying hierarchy to only include unseen classes in the leaves of the hierarchy to reduce the relevant unseen classes to only two or three. The only work on zero-shot word sequence labelling (Rei and Søgaard, 2018) explores the transfer from labels on a sentence level objective (e.g. sentiment analysis) to a token or phrase-based annotation, similar to T¨ackstr¨om and McDonald (2011). Guerini et al. (2018) label their approach zero-shot named entity recognition, however, they focus on recognizing unseen entities not entity classes. Finally, Fritzler et al. (2019) focused on f"
2021.acl-long.120,W18-5511,1,0.733454,"providing additional implicit supervision to the model: masked tokens cannot be the non-entity class. For these masked tokens the model can focus on the entity classification in isolation which appears to help the model extract more useful supervision signal, as indicated by the higher validation F1 achieved. When trained with masking, SMXM’s training loss closely follows the trend of the validation F1 , indicating good transfer learning from the model’s training objective to the zero-shot evaluation. 5 Related Work fined class descriptions have also been explored for relation classification (Obamuyide and Vlachos, 2018) who pose the task as one of textual entailment. Obeidat et al. (2019) use descriptions for zero-shot NET, however, similar to a previous attempt by Ma et al. (2016), they use the underlying hierarchy to only include unseen classes in the leaves of the hierarchy to reduce the relevant unseen classes to only two or three. The only work on zero-shot word sequence labelling (Rei and Søgaard, 2018) explores the transfer from labels on a sentence level objective (e.g. sentiment analysis) to a token or phrase-based annotation, similar to T¨ackstr¨om and McDonald (2011). Guerini et al. (2018) label t"
2021.acl-long.120,N19-1087,0,0.0447731,"nt issue for many real-world applications is that annotated data does not readily exist. This motivates the focus on the zero-shot setting (Xian et al., 2018; Wang et al., 2019), where annotated data is not available for the classes of interest. Instead, information available from observed classes must be transferred to unseen target classes. Recently zero-shot approaches making use of textual representations to represent entity classes ∗ Work done when author was working at Google. were explored for entity linking (EL) (Logeswaran et al., 2019; Wu et al., 2020) and named entity typing (NET) (Obeidat et al., 2019), which are similar to the NERC subtask of named entity classification (NEC). However, no previous work has addressed the task of zero-shot NERC, which additionally requires the detection of which tokens make up an entity in addition to its type, i.e. Named Entity Recognition (NER). This paper is the first to study zero-shot NERC, by leveraging entity type descriptions. The task is illustrated in Figure 1. During testing, the input is a sentence and a set of target entity classes. each accompanied by its description, and the goal is to recognize and classify entities in these target classes. D"
2021.acl-long.120,W13-3516,0,0.0351582,"Missing"
2021.acl-long.120,P17-1194,0,0.0247211,"assification and recognition task in isolation highlights the importance of the description choice, finding that annotation guidelines result in higher scores than the class name itself or Wikipedia passages. 2 Zero-shot NERC In NERC, given a sentence s = w1 , ..., wn of length n and a description dc for each class c ∈ Cts in the test set, we predict a sequence of labels y ˆ ∈ (Cts )n , with n being the length of the sentence. We model the task as multiclass classification, which despite ignoring the sequential structure of the output, it has been found to be competitive (Lample et al., 2016; Rei, 2017). Thus, we predict the correct class for each token w at position t: arg maxc∈Cts F (s, wt , dc ), using a suitable function F modelling the semantic affinity between wt and dc in the context of s. The parameters of F need to be learned without annotated data for Cts , but with annotated data and descriptions for the training C tr classes. To model F we focus on the use of crossattention (Humeau et al., 2019; Wolf et al., 2019b) in the form of a transformer encoder (Vaswani et al., 2017). For each type description dc , the cross-attention encoder (X-ENC) generates a vector representation vt,c"
2021.acl-long.120,N18-1027,0,0.01483,"F1 , indicating good transfer learning from the model’s training objective to the zero-shot evaluation. 5 Related Work fined class descriptions have also been explored for relation classification (Obamuyide and Vlachos, 2018) who pose the task as one of textual entailment. Obeidat et al. (2019) use descriptions for zero-shot NET, however, similar to a previous attempt by Ma et al. (2016), they use the underlying hierarchy to only include unseen classes in the leaves of the hierarchy to reduce the relevant unseen classes to only two or three. The only work on zero-shot word sequence labelling (Rei and Søgaard, 2018) explores the transfer from labels on a sentence level objective (e.g. sentiment analysis) to a token or phrase-based annotation, similar to T¨ackstr¨om and McDonald (2011). Guerini et al. (2018) label their approach zero-shot named entity recognition, however, they focus on recognizing unseen entities not entity classes. Finally, Fritzler et al. (2019) focused on few-shot NERC using prototypical networks (Snell et al., 2017). They tested their model in the zero-shot setting, but concluded that their approach is not suitable for zero-shot learning as the results on OntoNotes were too low. 6 Co"
2021.acl-long.120,2020.emnlp-main.519,0,0.013732,"m a set of pre-defined entity classes. A prevalent issue for many real-world applications is that annotated data does not readily exist. This motivates the focus on the zero-shot setting (Xian et al., 2018; Wang et al., 2019), where annotated data is not available for the classes of interest. Instead, information available from observed classes must be transferred to unseen target classes. Recently zero-shot approaches making use of textual representations to represent entity classes ∗ Work done when author was working at Google. were explored for entity linking (EL) (Logeswaran et al., 2019; Wu et al., 2020) and named entity typing (NET) (Obeidat et al., 2019), which are similar to the NERC subtask of named entity classification (NEC). However, no previous work has addressed the task of zero-shot NERC, which additionally requires the detection of which tokens make up an entity in addition to its type, i.e. Named Entity Recognition (NER). This paper is the first to study zero-shot NERC, by leveraging entity type descriptions. The task is illustrated in Figure 1. During testing, the input is a sentence and a set of target entity classes. each accompanied by its description, and the goal is to recog"
2021.acl-long.120,D19-1404,0,0.0289075,"Missing"
2021.acl-long.120,N18-1101,0,0.0162435,"set, to classify whether a class description (The text is about X) is entailed by the text. To adapt this model to NERC, we modify the description to The word is of type X with X being the entity class name, and classify each word instead of the entire sentence. Since their model generates a binary output for each class, the negative prediction for all classes predicts the negative class. By treating each sentence-description pair independently, the relationship between classes as well as the complexity of the negative class in zero-shot evaluation is ignored. We fine-tune BERT-Large on MNLI (Williams et al., 2018), as it performed best in the experiments of (Yin et al., 2019), before training BEM on the zero-shot datasets using adjusted class weights, which has been crucial for successful training of the model; not using it resulted in degenerated solutions in preliminary experiments. The proposed entity masking objective is not suitable for BEM’s binary classification approach as it would simply learns to predict the masked token to be an entity during training. MRC for NERC is an approach by Li et al. (2020) who construct queries for entity classes and transform NERC to a machine reading comprehensio"
2021.acl-long.474,N19-1423,0,0.0119301,"ion tasks (Durmus et al., 2020; Kry´sci´nski et al., 2019; Sellam et al., 2020; Rei et al., 2020). We evaluate FAME models on semantic inference metrics such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019b; Falke et al., 2019; Kryscinski et al., 2019) and question answering (Arumae and Liu, 2019; Wang et al., 2020a). In particular, we report the probability of a summary entailing (ent.) its input document (Maynez et al., 2020) and QA-based Feqa scores (Durmus et al., 2020). For ent. scores, we train an entailment classifier by fine-tuning a BERT-Large pretrained model (Devlin et al., 2019) on the MultiNLI dataset (Williams et al., 2018). For Feqa, we use a fine-tuned BART (Lewis et al., 2019) language model for question generation to generate questions from the summaries, and a BERTbase model fine-tuned on SQuAD (Rajpurkar et al., 2018) to answer the generated questions with input document as context.7 In addition to ent. and Feqa, we train a scorer leveraging manually annotated document-summary pairs for faithfulness, as a surrogate for human evaluation and call this metric BERTFaithful.8 In particular, we finetune a BERT-Base classi7 We used the Feqa code available here: http"
2021.acl-long.474,D17-1091,0,0.0242281,"Others avoid text degeneration by truncating the unreliable tail of the probability distribution at each decoding step, either by sampling from the top-k tokens (Top-k Sampling; Fan et al., 2018) or by sampling from a dynamic nucleus of tokens with the bulk of the probability mass (Nucleus Sampling; Holtzman et al., 2020). Others modify the training objective to make the distribution sparse (Martins et al., 2020) or assign lower probability to unlikely generations (Welleck et al., 2019a). For conditional text generation, most work focuses on generating diverse questions (Narayan et al., 2016; Dong et al., 2017; Sultan et al., 2020; Wang et al., 2020b) or paraphrases (Li et al., 2016b; Dai et al., 2017; Xu et al., 2018; Cao and Wan, 2020). Following Gehrmann et al. (2018), Cho et al. (2019) use a mixture of experts to sample different binary masks on the source sequence for diverse content selection for summarization. Our focus sampling is similar to top-k and nucleus sampling methods; in that it truncates the tail of the probability distribution. However, instead of truncating it at each decoding step, it biases the decoder proactively to generate output from a set of tokens which are topically-rel"
2021.acl-long.474,P19-1331,0,0.126886,"es (Vaswani et al., 2017), and large pretrained language models (Devlin et al., ∗ Work done when authors were interning/working at Google. Figure 1: Block A shows the best predictions from P EGASUS and our P EG FAME (P EGASUS with FAME) model, along with the G OLD summary for an XS UM article. Block B presents diverse summaries generated from P EGASUS using top-k and nucleus sampling. Block C shows diverse summaries generated using our P EG FAME model with Focus sampling. The text in orange is not supported by the input article. 2019; Radford et al., 2018; Yang et al., 2019; Liu et al., 2019; Dong et al., 2019a; Song et al., 2019; Lewis et al., 2019; Rothe et al., 2020; Raffel et al., 2019; Zhang et al., 2019). However, in terms of summary quality, many challenges remain. For example, generating summaries that are faithful to the input is an unsolved problem (Kryscinski et al., 2020; Maynez et al., 2020; Gabriel et al., 2020). Furthermore, there can be multiple equally good summaries per source docu6078 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6078–6095 August 1–6, 2021."
2021.acl-long.474,2020.acl-main.454,0,0.0183992,"ary for a document. Focus sampling is not used here. See Section 4.3 for details on the evaluation metrics reported. Best number for each metric is boldfaced. Faithfulness ROUGE and BERTScore do not correlate well with faithfulness of the generated summaries (Maynez et al., 2020). Human evaluation is traditionally considered as the gold standard for measuring faithfulness. But recent research has shown that even human evaluation has shortcomings (Schoch et al., 2020). Moreover, it is prohibitively expensive. This has led to the proposal of meta-evaluation metrics for various generation tasks (Durmus et al., 2020; Kry´sci´nski et al., 2019; Sellam et al., 2020; Rei et al., 2020). We evaluate FAME models on semantic inference metrics such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019b; Falke et al., 2019; Kryscinski et al., 2019) and question answering (Arumae and Liu, 2019; Wang et al., 2020a). In particular, we report the probability of a summary entailing (ent.) its input document (Maynez et al., 2020) and QA-based Feqa scores (Durmus et al., 2020). For ent. scores, we train an entailment classifier by fine-tuning a BERT-Large pretrained model (Devlin et al., 2019) on the Mu"
2021.acl-long.474,W19-4103,0,0.025381,"ndix C). Topic-Aware Generation Models The idea of capturing document-level semantic information has been widely explored in the summarization community. Barzilay and Elhadad (1997) use WordNet (Fellbaum, 1998) to model a text’s content relative to a topic based on lexical chains. Lin and Hovy (2000) propose to learn topic signatures for summarizing documents. Recently, document-level topic information has been used for improving neural language models (Mikolov and Zweig, 2012; Ghosh et al., 2016; Dieng et al., 2017; Karmaker Santu et al., 2019), neural response generators (Xing et al., 2017; Dziri et al., 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c). Both, Narayan et al. (2018) and Ailem et al. (2019), use a pretrained Latent Dirichlet Allocation (LDA; Blei et al., 2003) model, whereas, Wang et al. (2020c) use Poisson factor analysis (Zhou et al., 2012), to synthesize topic vectors for the input. Instead, we dynamically learn a target-induced topic distribution for the input under the assumption that the human-written 6079 summary is a good proxy for the input document. FAME Faithful Generation Models Cao et al. (2017) force faithful"
2021.acl-long.474,P19-1213,0,0.0422724,"Missing"
2021.acl-long.474,P18-1082,0,0.408582,", 2020; Maynez et al., 2020; Gabriel et al., 2020). Furthermore, there can be multiple equally good summaries per source docu6078 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6078–6095 August 1–6, 2021. ©2021 Association for Computational Linguistics ment. Neural generation models fail to account for this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov et al., 2019; Freitag et al., 2020; Choi et al., 2020). Not much attention has been given to generation of diverse, yet faithful summaries – two goals are often challenging to achieve simultaneously (Hashimoto et al., 2019); a model can produce diverse outputs through sampling (Fan et al., 2018; Holtzman et al., 2020), but at the cost of quality. In this paper we introduce a Focus Attention MEchanism (or FAME) to transformer-based seq2seq architectures. FAME is inspired by how humans write summaries. Specifically, FAME aims to perform source-side planning to focus the summary on supp"
2021.acl-long.474,2020.emnlp-main.5,0,0.0248696,"al., 2020). Furthermore, there can be multiple equally good summaries per source docu6078 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6078–6095 August 1–6, 2021. ©2021 Association for Computational Linguistics ment. Neural generation models fail to account for this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov et al., 2019; Freitag et al., 2020; Choi et al., 2020). Not much attention has been given to generation of diverse, yet faithful summaries – two goals are often challenging to achieve simultaneously (Hashimoto et al., 2019); a model can produce diverse outputs through sampling (Fan et al., 2018; Holtzman et al., 2020), but at the cost of quality. In this paper we introduce a Focus Attention MEchanism (or FAME) to transformer-based seq2seq architectures. FAME is inspired by how humans write summaries. Specifically, FAME aims to perform source-side planning to focus the summary on supported and topical content. FAME achieves thi"
2021.acl-long.474,D18-1443,0,0.0827254,"arization. 2 Related Work Task-Specific Architectural Priors Several works enhance seq2seq architectures with taskspecific priors. Pointer-generator style models (See et al., 2017; Xu et al., 2020) can accurately generate mostly extractive summaries by copying words from the source text via pointing. Text editing models (Malmi et al., 2019; Dong et al., 2019b; Mallinson et al., 2020) cast text generation as a sequence tagging problem with carefully selected edit operations required for the task. Others focus on improving content selection to better constrain the model to likely input phrases (Gehrmann et al., 2018) or by improving the representation of relevant input tokens (Zhou et al., 2017). Instead of directly modeling such priors, FAME learns the theme of the document through dynamic vocabulary biasing. Thus, FAME can be seen as a generalization of Pointer-generator or text-editing models via soft vocabulary learning. In fact, our FAME models achieve state-of-the-art on text-editing tasks (Appendix C). Topic-Aware Generation Models The idea of capturing document-level semantic information has been widely explored in the summarization community. Barzilay and Elhadad (1997) use WordNet (Fellbaum, 199"
2021.acl-long.474,N19-1348,0,0.0283256,"te a high level of abstractiveness, and generating them automatically requires document-level inference, abstraction, and paraphrasing. Due to their extreme nature, XS UM summaries are ideal to evaluate FAME models’ ability to capture the theme of the document.4 We use on the original cased version consisting of 204,045/11,332/11,334 training/validation/test document-summary pairs. During training, the input documents are truncated to 512 tokens. The 4 We further experiment with long-form story highlight generation (C NN /D M; Hermann et al., 2015) and two text editing tasks: Sentence Fusion (Geva et al., 2019) and Sentence Splitting (Botha et al., 2018). Their results can be found in Appendix B and C. Our FAME models achieve SOTA on both text-editing tasks. length of the summaries are limited to 64. 4.2 Pretrained Models with FAME We introduce FAME to two popular seq2seq architectures: RoBERTa initialized seq2seq (ROBERTA S2S, Rothe et al., 2020) and P EGASUS (Zhang et al., 2019). We refer ROBERTA S2S models with FAME as ROB FAME and P EGASUS with FAME with P EG FAME. We experiment with ROBERTA S2S-Large with shared encoder and decoder; it has 24 layers, a hidden size of 1024, filter size of 4096,"
2021.acl-long.474,P16-1154,0,0.038209,"diplomatic staff after it said the country was responsible for the use of Australian visas used in the killing of a Palestinian in the Middle East. Introduction Document summarization — producing the shorter version of a document while preserving salient information (Mani, 2001; Nenkova and McKeown, 2011) — is challenging even for humans. Today, systems can generate summaries with a high level of fluency and coherence. This is due to recent advances such as sequence-to-sequence architectures (seq2seq) with attention and copy mechanism (Hochreiter and Schmidhuber, 1997; Bahdanau et al., 2015; Gu et al., 2016), fully attention-based Transformer architectures (Vaswani et al., 2017), and large pretrained language models (Devlin et al., ∗ Work done when authors were interning/working at Google. Figure 1: Block A shows the best predictions from P EGASUS and our P EG FAME (P EGASUS with FAME) model, along with the G OLD summary for an XS UM article. Block B presents diverse summaries generated from P EGASUS using top-k and nucleus sampling. Block C shows diverse summaries generated using our P EG FAME model with Focus sampling. The text in orange is not supported by the input article. 2019; Radford et a"
2021.acl-long.474,N19-1169,0,0.0166244,"11th International Joint Conference on Natural Language Processing, pages 6078–6095 August 1–6, 2021. ©2021 Association for Computational Linguistics ment. Neural generation models fail to account for this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov et al., 2019; Freitag et al., 2020; Choi et al., 2020). Not much attention has been given to generation of diverse, yet faithful summaries – two goals are often challenging to achieve simultaneously (Hashimoto et al., 2019); a model can produce diverse outputs through sampling (Fan et al., 2018; Holtzman et al., 2020), but at the cost of quality. In this paper we introduce a Focus Attention MEchanism (or FAME) to transformer-based seq2seq architectures. FAME is inspired by how humans write summaries. Specifically, FAME aims to perform source-side planning to focus the summary on supported and topical content. FAME achieves this through a novel technique which augments standard contextual representations with a dynamic source-conditioned vocabulary biasing layer. We present the following experimental findings: FA"
2021.acl-long.474,K19-1073,0,0.0265635,"our FAME models achieve state-of-the-art on text-editing tasks (Appendix C). Topic-Aware Generation Models The idea of capturing document-level semantic information has been widely explored in the summarization community. Barzilay and Elhadad (1997) use WordNet (Fellbaum, 1998) to model a text’s content relative to a topic based on lexical chains. Lin and Hovy (2000) propose to learn topic signatures for summarizing documents. Recently, document-level topic information has been used for improving neural language models (Mikolov and Zweig, 2012; Ghosh et al., 2016; Dieng et al., 2017; Karmaker Santu et al., 2019), neural response generators (Xing et al., 2017; Dziri et al., 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c). Both, Narayan et al. (2018) and Ailem et al. (2019), use a pretrained Latent Dirichlet Allocation (LDA; Blei et al., 2003) model, whereas, Wang et al. (2020c) use Poisson factor analysis (Zhou et al., 2012), to synthesize topic vectors for the input. Instead, we dynamically learn a target-induced topic distribution for the input under the assumption that the human-written 6079 summary is a good proxy for the input documen"
2021.acl-long.474,W19-8609,0,0.148748,"al., 2020; Gabriel et al., 2020). Furthermore, there can be multiple equally good summaries per source docu6078 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6078–6095 August 1–6, 2021. ©2021 Association for Computational Linguistics ment. Neural generation models fail to account for this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov et al., 2019; Freitag et al., 2020; Choi et al., 2020). Not much attention has been given to generation of diverse, yet faithful summaries – two goals are often challenging to achieve simultaneously (Hashimoto et al., 2019); a model can produce diverse outputs through sampling (Fan et al., 2018; Holtzman et al., 2020), but at the cost of quality. In this paper we introduce a Focus Attention MEchanism (or FAME) to transformer-based seq2seq architectures. FAME is inspired by how humans write summaries. Specifically, FAME aims to perform source-side planning to focus the summary on supported and topical cont"
2021.acl-long.474,N16-1014,0,0.0329108,"ability distribution at each decoding step, either by sampling from the top-k tokens (Top-k Sampling; Fan et al., 2018) or by sampling from a dynamic nucleus of tokens with the bulk of the probability mass (Nucleus Sampling; Holtzman et al., 2020). Others modify the training objective to make the distribution sparse (Martins et al., 2020) or assign lower probability to unlikely generations (Welleck et al., 2019a). For conditional text generation, most work focuses on generating diverse questions (Narayan et al., 2016; Dong et al., 2017; Sultan et al., 2020; Wang et al., 2020b) or paraphrases (Li et al., 2016b; Dai et al., 2017; Xu et al., 2018; Cao and Wan, 2020). Following Gehrmann et al. (2018), Cho et al. (2019) use a mixture of experts to sample different binary masks on the source sequence for diverse content selection for summarization. Our focus sampling is similar to top-k and nucleus sampling methods; in that it truncates the tail of the probability distribution. However, instead of truncating it at each decoding step, it biases the decoder proactively to generate output from a set of tokens which are topically-relevant to the input. 3 tx1 Dense tx2 tx3 GELU ... tX Dense yt x1 x2 x3 ..."
2021.acl-long.474,C00-1072,0,0.427932,"2017). Instead of directly modeling such priors, FAME learns the theme of the document through dynamic vocabulary biasing. Thus, FAME can be seen as a generalization of Pointer-generator or text-editing models via soft vocabulary learning. In fact, our FAME models achieve state-of-the-art on text-editing tasks (Appendix C). Topic-Aware Generation Models The idea of capturing document-level semantic information has been widely explored in the summarization community. Barzilay and Elhadad (1997) use WordNet (Fellbaum, 1998) to model a text’s content relative to a topic based on lexical chains. Lin and Hovy (2000) propose to learn topic signatures for summarizing documents. Recently, document-level topic information has been used for improving neural language models (Mikolov and Zweig, 2012; Ghosh et al., 2016; Dieng et al., 2017; Karmaker Santu et al., 2019), neural response generators (Xing et al., 2017; Dziri et al., 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c). Both, Narayan et al. (2018) and Ailem et al. (2019), use a pretrained Latent Dirichlet Allocation (LDA; Blei et al., 2003) model, whereas, Wang et al. (2020c) use Poisson fact"
2021.acl-long.474,N03-1020,0,0.657432,"Missing"
2021.acl-long.474,2021.ccl-1.108,0,0.0643904,"Missing"
2021.acl-long.474,2020.emnlp-main.348,0,0.0440949,"Missing"
2021.acl-long.474,2020.acl-main.173,1,0.855929,"ock B presents diverse summaries generated from P EGASUS using top-k and nucleus sampling. Block C shows diverse summaries generated using our P EG FAME model with Focus sampling. The text in orange is not supported by the input article. 2019; Radford et al., 2018; Yang et al., 2019; Liu et al., 2019; Dong et al., 2019a; Song et al., 2019; Lewis et al., 2019; Rothe et al., 2020; Raffel et al., 2019; Zhang et al., 2019). However, in terms of summary quality, many challenges remain. For example, generating summaries that are faithful to the input is an unsolved problem (Kryscinski et al., 2020; Maynez et al., 2020; Gabriel et al., 2020). Furthermore, there can be multiple equally good summaries per source docu6078 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6078–6095 August 1–6, 2021. ©2021 Association for Computational Linguistics ment. Neural generation models fail to account for this and tend to generate outputs with low diversity due to standard likelihood training, approximate decoding objectives, and lack of high quality multireference datasets (Fan et al., 2018; Kulikov e"
2021.acl-long.474,2020.findings-emnlp.111,0,0.0166544,"existing diversity and faithfulness measures. Empirically, we find that optimizing for high diversity often comes at the cost of faithfulness. Thus FAME provides a mechanism for trading-off high faithfulness with better diversity in summarization. 2 Related Work Task-Specific Architectural Priors Several works enhance seq2seq architectures with taskspecific priors. Pointer-generator style models (See et al., 2017; Xu et al., 2020) can accurately generate mostly extractive summaries by copying words from the source text via pointing. Text editing models (Malmi et al., 2019; Dong et al., 2019b; Mallinson et al., 2020) cast text generation as a sequence tagging problem with carefully selected edit operations required for the task. Others focus on improving content selection to better constrain the model to likely input phrases (Gehrmann et al., 2018) or by improving the representation of relevant input tokens (Zhou et al., 2017). Instead of directly modeling such priors, FAME learns the theme of the document through dynamic vocabulary biasing. Thus, FAME can be seen as a generalization of Pointer-generator or text-editing models via soft vocabulary learning. In fact, our FAME models achieve state-of-the-art"
2021.acl-long.474,2020.findings-emnlp.217,0,0.0283636,"Missing"
2021.acl-long.474,D19-1510,1,0.853043,". Sampling technique using a variety of existing diversity and faithfulness measures. Empirically, we find that optimizing for high diversity often comes at the cost of faithfulness. Thus FAME provides a mechanism for trading-off high faithfulness with better diversity in summarization. 2 Related Work Task-Specific Architectural Priors Several works enhance seq2seq architectures with taskspecific priors. Pointer-generator style models (See et al., 2017; Xu et al., 2020) can accurately generate mostly extractive summaries by copying words from the source text via pointing. Text editing models (Malmi et al., 2019; Dong et al., 2019b; Mallinson et al., 2020) cast text generation as a sequence tagging problem with carefully selected edit operations required for the task. Others focus on improving content selection to better constrain the model to likely input phrases (Gehrmann et al., 2018) or by improving the representation of relevant input tokens (Zhou et al., 2017). Instead of directly modeling such priors, FAME learns the theme of the document through dynamic vocabulary biasing. Thus, FAME can be seen as a generalization of Pointer-generator or text-editing models via soft vocabulary learning. In f"
2021.acl-long.474,P17-1099,0,0.552242,"ctiveness of our new Focus 1 In the paper we focus on assessing FAME on XS UM. But other summarization and text editing results can be found in Appendix B and C. Sampling technique using a variety of existing diversity and faithfulness measures. Empirically, we find that optimizing for high diversity often comes at the cost of faithfulness. Thus FAME provides a mechanism for trading-off high faithfulness with better diversity in summarization. 2 Related Work Task-Specific Architectural Priors Several works enhance seq2seq architectures with taskspecific priors. Pointer-generator style models (See et al., 2017; Xu et al., 2020) can accurately generate mostly extractive summaries by copying words from the source text via pointing. Text editing models (Malmi et al., 2019; Dong et al., 2019b; Mallinson et al., 2020) cast text generation as a sequence tagging problem with carefully selected edit operations required for the task. Others focus on improving content selection to better constrain the model to likely input phrases (Gehrmann et al., 2018) or by improving the representation of relevant input tokens (Zhou et al., 2017). Instead of directly modeling such priors, FAME learns the theme of the docu"
2021.acl-long.474,2020.acl-main.704,0,0.0221615,"ere. See Section 4.3 for details on the evaluation metrics reported. Best number for each metric is boldfaced. Faithfulness ROUGE and BERTScore do not correlate well with faithfulness of the generated summaries (Maynez et al., 2020). Human evaluation is traditionally considered as the gold standard for measuring faithfulness. But recent research has shown that even human evaluation has shortcomings (Schoch et al., 2020). Moreover, it is prohibitively expensive. This has led to the proposal of meta-evaluation metrics for various generation tasks (Durmus et al., 2020; Kry´sci´nski et al., 2019; Sellam et al., 2020; Rei et al., 2020). We evaluate FAME models on semantic inference metrics such as textual entailment (Pasunuru and Bansal, 2018; Welleck et al., 2019b; Falke et al., 2019; Kryscinski et al., 2019) and question answering (Arumae and Liu, 2019; Wang et al., 2020a). In particular, we report the probability of a summary entailing (ent.) its input document (Maynez et al., 2020) and QA-based Feqa scores (Durmus et al., 2020). For ent. scores, we train an entailment classifier by fine-tuning a BERT-Large pretrained model (Devlin et al., 2019) on the MultiNLI dataset (Williams et al., 2018). For Feqa"
2021.acl-long.474,2020.acl-main.500,0,0.0381542,"egeneration by truncating the unreliable tail of the probability distribution at each decoding step, either by sampling from the top-k tokens (Top-k Sampling; Fan et al., 2018) or by sampling from a dynamic nucleus of tokens with the bulk of the probability mass (Nucleus Sampling; Holtzman et al., 2020). Others modify the training objective to make the distribution sparse (Martins et al., 2020) or assign lower probability to unlikely generations (Welleck et al., 2019a). For conditional text generation, most work focuses on generating diverse questions (Narayan et al., 2016; Dong et al., 2017; Sultan et al., 2020; Wang et al., 2020b) or paraphrases (Li et al., 2016b; Dai et al., 2017; Xu et al., 2018; Cao and Wan, 2020). Following Gehrmann et al. (2018), Cho et al. (2019) use a mixture of experts to sample different binary masks on the source sequence for diverse content selection for summarization. Our focus sampling is similar to top-k and nucleus sampling methods; in that it truncates the tail of the probability distribution. However, instead of truncating it at each decoding step, it biases the decoder proactively to generate output from a set of tokens which are topically-relevant to the input. 3"
2021.acl-long.474,2020.acl-main.450,0,0.410602,"widely explored in the summarization community. Barzilay and Elhadad (1997) use WordNet (Fellbaum, 1998) to model a text’s content relative to a topic based on lexical chains. Lin and Hovy (2000) propose to learn topic signatures for summarizing documents. Recently, document-level topic information has been used for improving neural language models (Mikolov and Zweig, 2012; Ghosh et al., 2016; Dieng et al., 2017; Karmaker Santu et al., 2019), neural response generators (Xing et al., 2017; Dziri et al., 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c). Both, Narayan et al. (2018) and Ailem et al. (2019), use a pretrained Latent Dirichlet Allocation (LDA; Blei et al., 2003) model, whereas, Wang et al. (2020c) use Poisson factor analysis (Zhou et al., 2012), to synthesize topic vectors for the input. Instead, we dynamically learn a target-induced topic distribution for the input under the assumption that the human-written 6079 summary is a good proxy for the input document. FAME Faithful Generation Models Cao et al. (2017) force faithful generation by conditioning on both source text and extracted fact descriptions from the source text. So"
2021.acl-long.474,2020.findings-emnlp.194,0,0.406061,"widely explored in the summarization community. Barzilay and Elhadad (1997) use WordNet (Fellbaum, 1998) to model a text’s content relative to a topic based on lexical chains. Lin and Hovy (2000) propose to learn topic signatures for summarizing documents. Recently, document-level topic information has been used for improving neural language models (Mikolov and Zweig, 2012; Ghosh et al., 2016; Dieng et al., 2017; Karmaker Santu et al., 2019), neural response generators (Xing et al., 2017; Dziri et al., 2019), and not surprisingly, neural summarizers (Narayan et al., 2018; Ailem et al., 2019; Wang et al., 2020c). Both, Narayan et al. (2018) and Ailem et al. (2019), use a pretrained Latent Dirichlet Allocation (LDA; Blei et al., 2003) model, whereas, Wang et al. (2020c) use Poisson factor analysis (Zhou et al., 2012), to synthesize topic vectors for the input. Instead, we dynamically learn a target-induced topic distribution for the input under the assumption that the human-written 6079 summary is a good proxy for the input document. FAME Faithful Generation Models Cao et al. (2017) force faithful generation by conditioning on both source text and extracted fact descriptions from the source text. So"
2021.eacl-main.92,P17-1171,0,0.10272,"Missing"
2021.eacl-main.92,D19-5819,1,0.84975,"a, 2010). For each thread, we concatenate the title and initial post to generate passages. For BERT-based models we truncate at 350 wordpiece tokens. Unlike the BioASQ data, this data generally does not contain specialist knowledge queries. Thus, compared to the collection of question-answer pairs mined from the web, there is less of a domain shift. NaturalQuestions Aggregated queries issued to Google Search (Kwiatkowski et al., 2019) with relevance judgements. We convert the original format to a passage retrieval task, where the goal is to retrieval the long answer among all wiki paragraphs (Ahmad et al., 2019). We discarded questions whose long answer is either a table or a list. We evaluate retrieval performance on the development set as the test set is not publicly available. The target collection contains all passages from the development set and is augmented with passages from 2016-12-21 dump of Wikipedia (Chen et al., 1079 2017). Each passage is also concatenated with title. For BERT-based models passages are truncated at 350 wordpiece tokens. This data is different from the previous data in two regards. First, there is a single annotated relevant paragraph per query. This is due to the nature"
2021.eacl-main.92,P19-1436,0,0.0585413,"this is approximate as it does not operate over the whole collection. For first-stage retrieval, the most common method is to learn term weights for a standard inverted index in order to make search efficient (Zamani et al., 2018; Dai and Callan, 2019). Here we propose a first-stage retrieval model that incorporates both term-based (sparse) and neural-based (dense) representations in a hybrid model that uses nearest neighbor search for exact inference (Liu et al., 2011; Johnson et al., 2017; Wu et al., 2019). Similar methods using approximate nearest neighbour search have been investigated by Seo et al. (2019). 3 Synthetic Question Generation In this work, we are specifically investigating the zero-shot scenario where there exists neither user issued questions nor domain specific data except the passage collection itself. We propose to address the 2 3 Ubuntu Forums Passage: Every time I get a notification about and begin updating when they become available, the process is interrupted by an error message: error in foomatic-filters. Then I get “error in linux generic package” and a bunch of numbers. This is replaced before I can write it all down with “error in Linux package” Everything seems to go O"
2021.eacl-main.92,P15-2033,0,0.0282759,"training resources with synthetic question-answer pairs can lead to improvements. Nogueira et al. (2019) employed query generation in the context of first-stage retrieval. In that study, the generated queries were used to augment documents to improve BM25 keyword search. Here we focus on using synthetic queries to train the neural retrieval models. Hybrid Models Combining neural and termbased models have been studied, most commonly via linearly interpolating scores in an approximate re-ranking stage (Karpukhin et al., 2020; Luan et al., 2020) or through the final layer of a rescoring network (Severyn et al., 2015; McDonald et al., 2018). Since rescoring can be cast as classification, blending signals is straight-forward. However, this is approximate as it does not operate over the whole collection. For first-stage retrieval, the most common method is to learn term weights for a standard inverted index in order to make search efficient (Zamani et al., 2018; Dai and Callan, 2019). Here we propose a first-stage retrieval model that incorporates both term-based (sparse) and neural-based (dense) representations in a hybrid model that uses nearest neighbor search for exact inference (Liu et al., 2011; Johns"
2021.eacl-main.92,2020.nlpcovid19-acl.1,0,0.0246857,"earn cross-domain representations (Cohen et al., 2018; Tran et al., 2019). Our work is more aligned with methods like Yilmaz et al. (2019) which use general domain resources to build neural models for new domains, though via different techniques – data augmentation vs. model transfer. Our experiments show that data augmentation 1076 compares favorably a model transfer baseline. For specialized domains, recently, there have been a number of studies using cross-domain transfer and other techniques for biomedical passage retrieval via the TREC-COVID challenge2,3 that uses the CORD-19 collection (Wang et al., 2020). Question generation for data augmentation is a common tool, but has not been tested in the pure zero-shot setting nor for neural passage retrieval. Duan et al. (2017) use community QA as a data source, as we do, to train question generators. The generated question-passage pairs are not used to train a neural model, but QA is instead done via question-question similarity. Furthermore, they do not test on specialized domains. Alberti et al. (2019) show that augmenting supervised training resources with synthetic question-answer pairs can lead to improvements. Nogueira et al. (2019) employed qu"
2021.eacl-main.92,N19-4013,0,0.0463843,"Missing"
2021.eacl-main.92,P79-1022,0,0.533628,"Missing"
2021.eacl-main.92,W11-0329,0,0.0262499,"ective is to minimize the negative log-likelihood: − L(q, p+ , p− 1 , ..., pk ) = +i log(ehq,q + k X − ehq,qi i ) − hq, q+ i i=1 This loss function is a special case of ListNet loss (Cao et al., 2007) where all relevance judgements are binary, and only one passage is marked relevant for each training example. − For the set {p− 1 , ..., pk }, we use in-batch negatives. Given a batch of (query, relevant-passage) pairs, negative passages for a query are passages from different pairs in the batch. In-batch negatives has been widely adopted as it enables efficient training via computation sharing (Yih et al., 2011; Gillick et al., 2018; Karpukhin et al., 2020). 4.3 Inference Since the relevance-based model encodes questions and passages independently, we run the encoder 1078 over every passage in a collection offline to create a distributed lookup-table as a backend. At inference, we run the question encoder online and then perform nearest neighbor search to find relevant passages, as illustrated in the bottom half of Figure 3. While there has been extensive work in fast approximate nearest neighbour retrieval for dense representations (Liu et al., 2011; Johnson et al., 2017), we simply use distributed"
2021.eacl-main.92,D19-1352,0,0.265246,"an effective strategy for building neural passage retrieval models in the absence of large training corpora. Depending on the domain, this technique can even approach the accuracy of supervised models. 1 Document Collection Recent advances in neural retrieval have led to advancements on several document, passage and knowledge-base benchmarks (Guo et al., 2016; Pang et al., 2016; Hui et al., 2017; Dai et al., 2018; Gillick et al., 2018; Nogueira and Cho, 2019a; MacAvaney et al., 2019; Yang et al., 2019a,b,c). Most neural passage retrieval systems are, in fact, two stages (Zamani et al., 2018; Yilmaz et al., 2019), illustrated in Figure 1. The first is a true retrieval model (aka first-stage retrieval1 ) that takes a question and retrieves a set of candidate passages from a large collection of documents. This stage itself is rarely a neural model and most commonly is an term-based retrieval model such as BM25 (Robertson et al., 2004; Yang et al., 2017), though there is recent work on neural models (Zamani et al., 2018; Dai and Callan, 2019; Chang et al., Also called open domain retrieval. Rescoring Model Figure 1: End-to-end neural retrieval. A first-stage model over a large collection returns a smalle"
C10-1094,H91-1060,0,0.288184,"Missing"
C10-1094,P06-4020,0,0.0683923,"Missing"
C10-1094,W06-2920,0,0.0649395,"Missing"
C10-1094,P04-1041,0,0.0138708,"Missing"
C10-1094,W08-2102,0,0.0219878,"Missing"
C10-1094,P05-1022,0,0.052948,"state-of-the-art parsers were evaluated for their recall on the goldstandard dependencies. Three of the parsers were based on grammars automatically extracted from the PTB: the C&C CCG parser (Clark and Curran, 2007), the Enju HPSG parser (Miyao and Tsujii, 2005), and the Stanford parser (Klein and Manning, 2003). The two remaining systems were the 834 RASP parser (Briscoe et al., 2006), using a manually constructed grammar and a statistical parse selection component, and the DCU post-processor of PTB parsers (Cahill et al., 2004) using the output of the Charniak and Johnson reranking parser (Charniak and Johnson, 2005). Because of the wide variation in parser output representations, a mostly manual evaluation was performed to ensure that each parser got credit for the constructions it recovered correctly. The parsers were run essentially “out of the box”, meaning that the development set was used to confirm input and output formats, but no real tuning was performed. In addition, since a separate question model is available for C&C, this was also evaluated on ObQ sentences. The best overall performers were C&C and Enju, which is unsurprising since they are deep parsers based on grammar formalisms designed to"
C10-1094,J07-4004,0,0.224176,"Missing"
C10-1094,de-marneffe-etal-2006-generating,0,0.0641288,"Missing"
C10-1094,gimenez-marquez-2004-svmtool,0,0.0865987,"Missing"
C10-1094,E06-1011,1,0.313697,"-based parsers typically rely on global training and inference algorithms, where the goal is to learn models in which the weight/probability of correct trees is higher than that of incorrect trees. At inference time a global search is run to find the 1 highest weighted dependency tree. Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). As a result, graph-based parsers (including MSTParser) often limit the scope of their features to a small number of adjacent arcs (usually two) and/or resort to approximate inference (McDonald and Pereira, 2006). MaltParser2 is a freely available implementation of the parsing models described in Nivre et al. (2006a) and Nivre et al. (2006b). MaltParser is categorized as a transition-based parsing system, characterized by parsing algorithms that produce dependency trees by transitioning through abstract state machines (K¨ubler et al., 2008). Transitionbased parsers learn models that predict the next state given the current state of the system as well as features over the history of parsing decisions and the input sentence. At inference time, the parser starts in an initial state, then greedily moves t"
C10-1094,W07-2216,1,0.608607,"graph-based parsing system in that core parsing algorithms can be equated to finding directed maximum spanning trees (either projective or non-projective) from a dense graph representation of the sentence. Graph-based parsers typically rely on global training and inference algorithms, where the goal is to learn models in which the weight/probability of correct trees is higher than that of incorrect trees. At inference time a global search is run to find the 1 highest weighted dependency tree. Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). As a result, graph-based parsers (including MSTParser) often limit the scope of their features to a small number of adjacent arcs (usually two) and/or resort to approximate inference (McDonald and Pereira, 2006). MaltParser2 is a freely available implementation of the parsing models described in Nivre et al. (2006a) and Nivre et al. (2006b). MaltParser is categorized as a transition-based parsing system, characterized by parsing algorithms that produce dependency trees by transitioning through abstract state machines (K¨ubler et al., 2008). Transitionbased parsers learn models that predict t"
C10-1094,P05-1012,1,0.0924636,"Missing"
C10-1094,W06-2932,1,0.848475,"r the evaluation, including parser training, post-processing, and evaluation.3 4.1 Parser Training One important difference between MSTParser and MaltParser, on the one hand, and the best performing parsers evaluated in Rimell et al. (2009), on the other, is that the former were never developed specifically as parsers for English. Instead, they are best understood as data-driven parser generators, that is, tools for generating a parser given a training set of sentences annotated with dependency structures. Over the years, both systems have been applied to a wide range of languages (see, e.g., McDonald et al. (2006), McDonald (2006), Nivre et al. (2006b), Hall et al. (2007), Nivre et al. (2007)), but they come with no language-specific enhancements and are not equipped specifically to deal with unbounded dependencies. Since the dependency representation used in the evaluation corpus is based on the Stanford typed dependency scheme (de Marneffe et al., 2006), we opted for using the WSJ section of the PTB, converted to Stanford dependencies, as our primary source of training data. Thus, both parsers were trained on section 2–21 of the WSJ data, which we converted to Stanford dependencies using the Stanford"
C10-1094,P05-1011,0,0.0881372,"Missing"
C10-1094,nivre-etal-2006-maltparser,1,0.308576,"hich the weight/probability of correct trees is higher than that of incorrect trees. At inference time a global search is run to find the 1 highest weighted dependency tree. Unfortunately, global inference and learning for graph-based dependency parsing is typically NP-hard (McDonald and Satta, 2007). As a result, graph-based parsers (including MSTParser) often limit the scope of their features to a small number of adjacent arcs (usually two) and/or resort to approximate inference (McDonald and Pereira, 2006). MaltParser2 is a freely available implementation of the parsing models described in Nivre et al. (2006a) and Nivre et al. (2006b). MaltParser is categorized as a transition-based parsing system, characterized by parsing algorithms that produce dependency trees by transitioning through abstract state machines (K¨ubler et al., 2008). Transitionbased parsers learn models that predict the next state given the current state of the system as well as features over the history of parsing decisions and the input sentence. At inference time, the parser starts in an initial state, then greedily moves to subsequent states – based on the predictions of the model – until a termination state is reached. Tran"
C10-1094,W06-2933,1,0.871166,"Missing"
C10-1094,P06-2041,1,0.881979,"Missing"
C10-1094,D07-1097,1,0.623321,"Missing"
C10-1094,P09-1040,1,0.856395,"ntially produces the same kind of dependency structures as output but uses the original phrase structure trees from the PTB as input to training. For our experiments we used MSTParser with the same parsing algorithms and features as reported in McDonald et al. (2006). However, unlike that work we used an atomic maximum entropy model as the second stage arc predictor as opposed to the more time consuming sequence labeler. McDonald et al. (2006) showed that there is negligible accuracy loss when using atomic rather than structured labeling. For MaltParser we used the projective Stack algorithm (Nivre, 2009) with default settings and a slightly enriched feature model. All parsing was projective because the Stanford dependency trees are strictly projective. 4 QB contains 4000 questions, but we removed all questions that also occurred in the test or development set of Rimell et al. (2009), who sampled their questions from the same TREC QA test sets. 836 4.2 Post-Processing All the development and test sets in the corpus of Rimell et al. (2009) were parsed using MSTParser and MaltParser after part-of-speech tagging the input using SVMTool (Gim´enez and M`arquez, 2004) trained on section 2–21 of the"
C10-1094,P08-1067,0,0.062612,"Missing"
C10-1094,D09-1085,1,0.255417,"Sagae and Lavie, 2006; Huang, 2008; Carreras et al., 2008), broad-coverage parsing is still far from being a solved problem. In particular, metrics like attachment score for dependency parsers (Buchholz and Marsi, 2006) and Parseval for constituency parsers (Black et al., 1991) suffer from being an average over a highly skewed distribution of different grammatical constructions. As a result, infrequent yet semantically important construction types could be parsed with accuracies far below what one might expect. This shortcoming of aggregate parsing metrics was highlighted in a recent study by Rimell et al. (2009), introducing a new parser evaluation corpus containing around 700 sentences annotated with unbounded dependencies in seven different grammatical constructions. This corpus was used to evaluate five state-of-the-art parsers cgomezr@udc.es for English, focusing on grammar-based and statistical phrase structure parsers. For example, in the sentence By Monday, they hope to have a sheaf of documents both sides can trust., parsers should recognize that there is a dependency between trust and documents, an instance of object extraction out of a (reduced) relative clause. In the evaluation, the recal"
C10-1094,P06-1063,0,0.0324415,"Missing"
C10-1094,N06-2033,0,0.0764608,"Missing"
C10-1094,P03-1054,0,0.00493055,"d (2006), Nivre et al. (2006b), Hall et al. (2007), Nivre et al. (2007)), but they come with no language-specific enhancements and are not equipped specifically to deal with unbounded dependencies. Since the dependency representation used in the evaluation corpus is based on the Stanford typed dependency scheme (de Marneffe et al., 2006), we opted for using the WSJ section of the PTB, converted to Stanford dependencies, as our primary source of training data. Thus, both parsers were trained on section 2–21 of the WSJ data, which we converted to Stanford dependencies using the Stanford parser (Klein and Manning, 2003). The Stanford scheme comes in several varieties, but because both parsers require the dependency structure for each sentence to be a tree, we had to use the so-called basic variety (de Marneffe et al., 2006). It is well known that questions are very rare in the WSJ data, and Rimell et al. (2009) found that parsers trained only on WSJ data generally performed badly on the questions included in the 3 To ensure replicability, we provide all experimental settings, post-processing scripts and additional information about the evaluation at http://stp.ling.uu.se/∼nivre/exp/. evaluation corpus, while"
C10-1094,D07-1013,1,0.699906,"bounded dependency constructions (a–g). Arcs drawn below each sentence represent the dependencies scored in the evaluation, while the tree above each sentence is the Stanford basic dependency representation, with solid arcs indicating crucial dependencies (cf. Section 4). All examples are from the development sets. (2009) and considerably better than the other statistical parsers in that evaluation. Interestingly, though the two systems have similar accuracies overall, there is a clear distinction between the kinds of errors each system makes, which we argue is consistent with observations by McDonald and Nivre (2007). 2 Unbounded Dependency Evaluation An unbounded dependency involves a word or phrase interpreted at a distance from its surface position, where an unlimited number of clause boundaries may in principle intervene. The unbounded dependency corpus of Rimell et al. (2009) includes seven grammatical constructions: object extraction from a relative clause (ObRC), object extraction from a reduced relative clause (ObRed), subject extraction from a relative clause (SbRC), free relatives (Free), object questions (ObQ), right node raising (RNR), and subject extraction from an embedded clause (SbEm), all"
C14-1168,I13-1041,0,0.0122948,"Missing"
C14-1168,P11-1040,0,0.0233001,"Missing"
C14-1168,D07-1074,0,0.00844979,"tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching of snippets and search queries, giving priority to n-gram matches with larger n. Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less spelling variation than tweets. In NER, it is common to use gazetteers, but also dictionaries as distant supervision (Kazama and Torisawa, 2007; Cucerzan, 2007). R¨ud et al. (2011) consider using search engines for distant supervision of NER of search queries. Their set-up is very similar to Ganchev et al. (2012), except they do not use click-through data. They use the search engine snippets to generate feature representations rather than projections. Want et al. (2013) also use distant supervision for NER, i.e., Wikipedia page view counts, 1790 applying their model to Twitter data, but their results are considerably below the state of the art. Also, their source of supervision is not linked to the individual tweets in the way mentioned websites are."
C14-1168,P11-1061,0,0.03304,", 2013; Derczynski et al., 2013). Strictly supervised approaches to analyzing Twitter has the weakness that labeled data quickly becomes unrepresentative of what people write on Twitter. This paper presents results using no in-domain labeled data that are significantly better than several offthe-shelf systems, as well as results leveraging a mixture of out-of-domain and in-domain labeled data to reach new highs across several data sets. Type-constrained POS tagging using tag dictionaries has been explored in weakly supervised settings (Li et al., 2012), as well as for cross-language learning (Das and Petrov, 2011; T¨ackstr¨om et al., 2013). Our type constraints in POS tagging come from tag dictionaries, but also from linked websites. The idea of using linked websites as distant supervision is similar in spirit to the idea presented in Ganchev et al. (2012) for search query tagging. Ganchev et al. (2012), considering the problem of POS tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching"
C14-1168,W10-2608,0,0.0382342,"Missing"
C14-1168,R13-1026,0,0.0929452,"oriously hard to analyze (Foster et al., 2011; Eisenstein, 2013; Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, fragmented or mixed language, etc. Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and suffer on other (out-of-sample) Twitter data sets. This bias can, at least in theory, be corrected by lea"
C14-1168,N13-1037,0,0.0133621,"Missing"
C14-1168,W10-0713,0,0.0396482,"the universal POS tag set by Petrov et al. (2012). The data sets also differ in a few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong. Instead of making annotations more consistent across data sets, we follow Ritter et al. (2011) in using a few post-processing rules to deterministically assign Twitter-specific symbols to their correct tags. The major difference between the NER data sets is whether Twitter user accounts are annotated as PER. We follow Finin et al. (2010) in doing so. Unlabeled data We downloaded 200k tweet-website pairs from the Twitter search API over a period of one week in August 2013 by searching for tweets that contain the string http and downloading the content of the websites they linked to. We filter out duplicate tweets and restrict ourselves to websites that contain more than one sentence (after removing boilerplate text, scripts, HTML, etc).13 We also require website and tweet to have at least one matching word that is not a stopword (as defined by the NLTK stopword list).14 Finally we restrict ourselves to pairs where the website"
C14-1168,P05-1045,0,0.0126701,"(2011), in particular features for word tokens, a set of features that check for the presence of hyphens, digits, single quotes, upper/lowercase, 3 character prefix and suffix information. Moreover, we add Brown word cluster features that use 2i for i ∈ 1, ..., 4 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013), which is publicly available.5 We use a pool size of 1000 tweets. We experimented with other pool sizes {500,2000} showing similar performance. The number of iterations i is set on the development data. For NER on websites, we use the Stanford NER system (Finkel et al., 2005)6 with POS tags from the L APOS tagger (Tsuruoka et al., 2011).7 For POS we found it to be superior to use the current POS model for re-tagging websites; for NER it was slightly better to use the Stanford NER tagger and thus off-line NER tagging rather than retagging the websites in every iteration. 3.2 Data In our experiments, we consider two scenarios, sometimes referred to as unsupervised and semisupervised domain adaptation (DA), respectively (Daum´e et al., 2010; Plank, 2011). In unsupervised DA, we assume only (labeled) newswire data, in semi-supervised DA, we assume labeled data from bo"
C14-1168,I11-1100,0,0.0448938,"Missing"
C14-1168,fromreide-etal-2014-crowdsourcing,1,0.118987,"red-task 9 LDC2011T03. 10 http://www.clips.ua.ac.be/conll2003/ner/ 11 http://www.isi.edu/publications/licensed-sw/mace/ 5 1786 et al. (2011). For NER, we simply use the parameters from our POS tagging experiments and thus do not assume to have access to further development data. For both POS tagging and NER, we have three test sets. For POS tagging, the ones used in Foster et al. (2011) (F OSTER -T EST) and Ritter et al. (2011) (R ITTER -T EST),12 as well as the one presented in Hovy et al. (2014) (H OVY-T EST). For NER, we use the data set from Ritter et al. (2011) and the two data sets from Fromreide et al. (2014) as test sets. One is a manual correction of a held-out portion of F ININ -T RAIN, named F ININ -T EST; the other one is referred to as F ROMREIDE -T EST. Since the different POS corpora use different tag sets, we map all of them corpora onto the universal POS tag set by Petrov et al. (2012). The data sets also differ in a few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong. Instead of making annotations more consistent across data sets, we follow Rit"
C14-1168,P12-2047,1,0.432581,"that are significantly better than several offthe-shelf systems, as well as results leveraging a mixture of out-of-domain and in-domain labeled data to reach new highs across several data sets. Type-constrained POS tagging using tag dictionaries has been explored in weakly supervised settings (Li et al., 2012), as well as for cross-language learning (Das and Petrov, 2011; T¨ackstr¨om et al., 2013). Our type constraints in POS tagging come from tag dictionaries, but also from linked websites. The idea of using linked websites as distant supervision is similar in spirit to the idea presented in Ganchev et al. (2012) for search query tagging. Ganchev et al. (2012), considering the problem of POS tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching of snippets and search queries, giving priority to n-gram matches with larger n. Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less spelling variation than tweets. In NER, it is common to use gazet"
C14-1168,P11-2008,0,0.147708,"Missing"
C14-1168,N13-1132,1,0.602877,"n our experiments, we use the SANCL shared task8 splits of the OntoNotes 4.0 distribution of the WSJ newswire annotations as newswire training data for POS tagging.9 For NER, we use the CoNLL 2003 data sets of annotated newswire from the Reuters corpus.10 The in-domain training POS data comes from Gimpel et al. (2011), and the in-domain NER data comes from Finin et al. (2010) (F ININ -T RAIN). These data sets are added to the newswire sets when doing semi-supervised DA. Note that for NER, we thus do not rely on expertannotated Twitter data, but rely on crowdsourced annotations. We use MACE11 (Hovy et al., 2013) to resolve inter-annotator conflicts between turkers (50 iterations, 10 restarts, no confidence threshold). We believe relying on crowdsourced annotations makes our set-up more robust across different samples of Twitter data. Development and test data. We use several evaluation sets for both tasks to prevent overfitting to a specific sample. We use the (out-of-sample) development data sets from Ritter et al. (2011) and Foster 4 http://www.chokkan.org/software/crfsuite/ http://www.ark.cs.cmu.edu/TweetNLP/ 6 http://http://nlp.stanford.edu/software/CRF-NER.shtml 7 http://www.logos.ic.i.u-tokyo.a"
C14-1168,hovy-etal-2014-pos,1,0.895368,"Foster et al., 2011; Eisenstein, 2013; Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, fragmented or mixed language, etc. Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and suffer on other (out-of-sample) Twitter data sets. This bias can, at least in theory, be corrected by learning from additiona"
C14-1168,P07-1034,0,0.0634483,"cross different samples of tweets than existing approaches. We consider both the scenario where a small sample of labeled Twitter data is available, and the scenario where only newswire data is available. Training on a mixture of out-of-domain (WSJ) and in-domain (Twitter) data as well as unlabeled data, we get the best reported results in the literature for both POS tagging and NER on Twitter. Our tagging models are publicly available at https://bitbucket.org/lowlands/ttagger-nsd 2 Tagging with not-so-distant supervision We assume that our labeled data is highly biased by domain differences (Jiang and Zhai, 2007), population drift (Hand, 2006), or by our sample size simply being too small. To correct this bias, we want to use unlabeled Twitter data. It is well-known that semi-supervised learning algorithms such as self-training sometimes effectively correct model biases (McClosky et al., 2006; Huang et al., 2009). This paper presents an augmented self-training algorithm that corrects model bias by exploiting unlabeled data and not-so-distant supervision. More specifically, the idea is to use hyperlinks to condition tagging decisions in tweets on a richer linguistic context than what is available in th"
C14-1168,P11-1016,0,0.0902879,"Missing"
C14-1168,D07-1073,0,0.0341914,"sidering the problem of POS tagging search queries, tag search queries and the associated snippets provided by the search engine, projecting tags from the snippets to the queries, guided by click-through data. They do not incorporate tag dictionaries, but consider a slightly more advanced matching of snippets and search queries, giving priority to n-gram matches with larger n. Search queries contain limited contexts, like tweets, but are generally much shorter and exhibit less spelling variation than tweets. In NER, it is common to use gazetteers, but also dictionaries as distant supervision (Kazama and Torisawa, 2007; Cucerzan, 2007). R¨ud et al. (2011) consider using search engines for distant supervision of NER of search queries. Their set-up is very similar to Ganchev et al. (2012), except they do not use click-through data. They use the search engine snippets to generate feature representations rather than projections. Want et al. (2013) also use distant supervision for NER, i.e., Wikipedia page view counts, 1790 applying their model to Twitter data, but their results are considerably below the state of the art. Also, their source of supervision is not linked to the individual tweets in the way mentio"
C14-1168,D12-1127,0,0.0865368,"Missing"
C14-1168,N06-1020,0,0.0511572,"l as unlabeled data, we get the best reported results in the literature for both POS tagging and NER on Twitter. Our tagging models are publicly available at https://bitbucket.org/lowlands/ttagger-nsd 2 Tagging with not-so-distant supervision We assume that our labeled data is highly biased by domain differences (Jiang and Zhai, 2007), population drift (Hand, 2006), or by our sample size simply being too small. To correct this bias, we want to use unlabeled Twitter data. It is well-known that semi-supervised learning algorithms such as self-training sometimes effectively correct model biases (McClosky et al., 2006; Huang et al., 2009). This paper presents an augmented self-training algorithm that corrects model bias by exploiting unlabeled data and not-so-distant supervision. More specifically, the idea is to use hyperlinks to condition tagging decisions in tweets on a richer linguistic context than what is available in the tweets. This semi-supervised approach gives state-of-the-art performance across available Twitter POS and NER data sets. The overall semi-supervised learning algorithm is presented in Figure 1. The aim is to correct model bias by predicting tag sequences on small pools of unlabeled"
C14-1168,P09-1113,0,0.139295,"his is the hypothesis we explore in this paper. We present a semi-supervised learning method that does not require additional labeled in-domain data to correct sample bias, but rather leverages pools of unlabeled Twitter data. However, since taggers trained on newswire perform poorly on Twitter data, we need additional guidance when utilizing the unlabeled data. This paper proposes distant supervision to help our models learn from unlabeled data. Distant supervision is a weakly supervised learning paradigm, where a knowledge resource is exploited to gather (possible noisy) training instances (Mintz et al., 2009). Our basic idea is to can use linguistic analysis of linked websites as a novel kind of distant supervision for learning how to analyze tweets. We explore standard sources of distant supervision, such as Wiktionary for POS tagging, but we also propose to use the linked websites of tweets with URLs as supervision. The intuition is that we can use websites to provide a richer linguistic context for our tagging decisions. We exploit the fact that tweets with URLs provide a one-to-one map between an unlabeled instance and the source of supervision, making this This work is licensed under a Creati"
C14-1168,N13-1039,0,0.158057,"Missing"
C14-1168,petrov-etal-2012-universal,1,0.678346,"gging and NER, we have three test sets. For POS tagging, the ones used in Foster et al. (2011) (F OSTER -T EST) and Ritter et al. (2011) (R ITTER -T EST),12 as well as the one presented in Hovy et al. (2014) (H OVY-T EST). For NER, we use the data set from Ritter et al. (2011) and the two data sets from Fromreide et al. (2014) as test sets. One is a manual correction of a held-out portion of F ININ -T RAIN, named F ININ -T EST; the other one is referred to as F ROMREIDE -T EST. Since the different POS corpora use different tag sets, we map all of them corpora onto the universal POS tag set by Petrov et al. (2012). The data sets also differ in a few annotation conventions, e.g., some annotate URLs as NOUN, some as X. Moreover, our newswire tagger baselines tend to get Twitter-specific symbols such as URLs, hashtags and user accounts wrong. Instead of making annotations more consistent across data sets, we follow Ritter et al. (2011) in using a few post-processing rules to deterministically assign Twitter-specific symbols to their correct tags. The major difference between the NER data sets is whether Twitter user accounts are annotated as PER. We follow Finin et al. (2010) in doing so. Unlabeled data W"
C14-1168,N10-1021,0,0.0862976,"Missing"
C14-1168,D11-1141,0,0.836453,"ze named entities. Tweets, however, are notoriously hard to analyze (Foster et al., 2011; Eisenstein, 2013; Baldwin et al., 2013). The challenges include dealing with variations in spelling, specific conventions for commenting and retweeting, frequent use of abbreviations and emoticons, non-standard syntax, fragmented or mixed language, etc. Gimpel et al. (2011) showed that we can induce POS tagging models with high accuracy on in-sample Twitter data with relatively little annotation effort. Learning taggers for Twitter data from small amounts of labeled data has also been explored by others (Ritter et al., 2011; Owoputi et al., 2013; Derczynski et al., 2013). Hovy et al. (2014), on the other hand, showed that these models overfit their respective samples and suffer severe drops when evaluated on out-of-sample Twitter data, sometimes performing even worse than newswire models. This may be due to drift on Twitter (Eisenstein, 2013) or simply due to the heterogeneous nature of Twitter, which makes small samples biased. So while existing systems perform well on their own (in-sample) data sets, they over-fit the samples they were induced from, and suffer on other (out-of-sample) Twitter data sets. This b"
C14-1168,P11-1097,0,0.0422016,"Missing"
C14-1168,Q13-1001,1,0.89785,"Missing"
C14-1168,W11-0328,0,0.0169234,"tures that check for the presence of hyphens, digits, single quotes, upper/lowercase, 3 character prefix and suffix information. Moreover, we add Brown word cluster features that use 2i for i ∈ 1, ..., 4 bitstring prefixes estimated from a large Twitter corpus (Owoputi et al., 2013), which is publicly available.5 We use a pool size of 1000 tweets. We experimented with other pool sizes {500,2000} showing similar performance. The number of iterations i is set on the development data. For NER on websites, we use the Stanford NER system (Finkel et al., 2005)6 with POS tags from the L APOS tagger (Tsuruoka et al., 2011).7 For POS we found it to be superior to use the current POS model for re-tagging websites; for NER it was slightly better to use the Stanford NER tagger and thus off-line NER tagging rather than retagging the websites in every iteration. 3.2 Data In our experiments, we consider two scenarios, sometimes referred to as unsupervised and semisupervised domain adaptation (DA), respectively (Daum´e et al., 2010; Plank, 2011). In unsupervised DA, we assume only (labeled) newswire data, in semi-supervised DA, we assume labeled data from both domains, besides unlabeled target data, but the amount of l"
C14-1168,W03-0419,0,\N,Missing
C14-1168,D10-1002,0,\N,Missing
D07-1013,W06-2920,0,0.779687,"mputational linguistics community and have been successfully employed for many problems ranging from machine translation (Ding and Palmer, 2004) to ontology construction (Snow et al., 2004). In this work we focus on a common parsing paradigm called datadriven dependency parsing. Unlike grammar-based parsing, data-driven approaches learn to produce dependency graphs for sentences solely from an annotated corpus. The advantage of such models is that they are easily ported to any domain or language in which annotated resources exist. As evident from the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006), there are currently two dominant models for data-driven dependency parsing. The first is what Buchholz and Marsi (2006) call the “all-pairs” approach, where every possible arc is considered in the construction of the optimal parse. The second is the “stepwise” approach, where the optimal parse is built stepwise and where the subset of possible arcs considered depend on previous decisions. Theoretically, these models are extremely different. The all-pairs models are globally trained, use exact (or near exact) inference algorithms, and define features over a limited history of parsing decision"
D07-1013,W04-1513,0,0.0175923,"ough labeled directed arcs, as shown in Figure 1, taken from the Prague Dependency Treebank (B¨ohmov´a et al., 2003). A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, arising from long distance dependencies or free word order, through non-projective dependency arcs, exemplified by the arc from jedna to Z in Figure 1. Syntactic dependency graphs have recently gained a wide interest in the computational linguistics community and have been successfully employed for many problems ranging from machine translation (Ding and Palmer, 2004) to ontology construction (Snow et al., 2004). In this work we focus on a common parsing paradigm called datadriven dependency parsing. Unlike grammar-based parsing, data-driven approaches learn to produce dependency graphs for sentences solely from an annotated corpus. The advantage of such models is that they are easily ported to any domain or language in which annotated resources exist. As evident from the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006), there are currently two dominant models for data-driven dependency parsing. The first is what Buchholz and Marsi (200"
D07-1013,P90-1005,0,0.260126,"tadriven dependency parsing: global, exhaustive, graph-based models, and local, greedy, transition-based models. We show that, in spite of similar performance overall, the two models produce different types of errors, in a way that can be explained by theoretical properties of the two models. This analysis leads to new directions for parser development. 1 Figure 1: Example dependency graph. Introduction Syntactic dependency representations have a long history in descriptive and theoretical linguistics and many formal models have been advanced (Hudson, 1984; Mel’ˇcuk, 1988; Sgall et al., 1986; Maruyama, 1990). A dependency graph of a sentence represents each word and its syntactic modifiers through labeled directed arcs, as shown in Figure 1, taken from the Prague Dependency Treebank (B¨ohmov´a et al., 2003). A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, arising from long distance dependencies or free word order, through non-projective dependency arcs, exemplified by the arc from jedna to Z in Figure 1. Syntactic dependency graphs have recently gained a wide interest in the computational linguistics community a"
D07-1013,P05-1012,1,0.15271,"e, define the score of a graph as the sum of its arc scores, X s(G = (V, A)) = s(i, j, l) (i,j,l)∈A The score of a dependency arc, s(i, j, l) represents the likelihood of creating a dependency from word wi to word wj with the label l. If the arc score function is known a priori, then the parsing problem can be stated as, G = arg max s(G) = arg max G∈D(Gx ) X s(i, j, l) G∈D(Gx ) (i,j,l)∈A This problem is equivalent to finding the highest scoring directed spanning tree in the graph Gx originating out of the root node 0, which can be solved for both the labeled and unlabeled case in O(n2 ) time (McDonald et al., 2005b). In this approach, nonprojective arcs are produced naturally through the inference algorithm that searches over all possible directed trees, whether projective or not. The parsing models of McDonald work primarily in this framework. To learn arc scores, these models use large-margin structured learning algorithms (McDonald et al., 2005a), which optimize the parameters of the model to maximize the score margin between the correct dependency graph and all incorrect dependency graphs for every sentence in a training set. The learning procedure is global since model parameters are set relative"
D07-1013,H05-1066,1,0.202859,"Missing"
D07-1013,W06-2932,1,0.71883,"85.82 91.65 87.60 70.30 81.29 84.58 65.68 80.75 1. V = {0, 1, . . . , n} 2. If (i, j, l) ∈ A, then j 6= 0. 3. If (i, j, l) ∈ A, then for all i0 ∈ V − {i} and l0 ∈ L, (i0 , j, l0 ) ∈ / A. 4. For all j ∈ V − {0}, there is a (possibly empty) sequence of nodes i1 , . . . , im ∈V and labels l1 , . . . , lm , l∈L such that (0, i1 , l1 ),(i1 , i2 , l2 ), . . . , (im , j, l)∈A. Table 1: Labeled parsing accuracy for top scoring systems at CoNLL-X (Buchholz and Marsi, 2006). on a variety of languages, as seen in Table 1, which shows results for the two top performing systems in the CoNLL-X shared task, McDonald et al. (2006) (“all-pairs”) and Nivre et al. (2006) (“stepwise”). Despite the similar performance in terms of overall accuracy, there are indications that the two types of models exhibit different behaviour. For example, Sagae and Lavie (2006) displayed that combining the predictions of both parsing models can lead to significantly improved accuracies. In order to pave the way for new and better methods, a much more detailed error analysis is needed to understand the strengths and weaknesses of different approaches. In this work we set out to do just that, focusing on the two top performing systems from th"
D07-1013,P05-1013,1,0.620556,"systems for data-driven dependency parsing are inspired by shift-reduce parsing, where configurations contain a stack for storing partially processed nodes. Transitions in such systems add arcs to the dependency graph and/or manipulate the stack. One example is the transition system defined by Nivre (2003), which parses a sentence x = w0 , w1 , . . . , wn in O(n) time, producing a projective dependency graph satisfying conditions 1–4 in section 2.1, possibly after adding arcs (0, i, lr ) for every node i 6= 0 that is a root in the output graph (where lr is a special label for root modifiers). Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing. To learn transition scores, these systems use discriminative learning methods, e.g., memory-based learning or support vector machines. The learning procedure is local since only single transitions are scored, not entire transition sequences. The primary advantage of these models is that features are not restricted to a limited number of graph arcs but can take into account the entire"
D07-1013,W06-2933,1,0.80114,"80.75 1. V = {0, 1, . . . , n} 2. If (i, j, l) ∈ A, then j 6= 0. 3. If (i, j, l) ∈ A, then for all i0 ∈ V − {i} and l0 ∈ L, (i0 , j, l0 ) ∈ / A. 4. For all j ∈ V − {0}, there is a (possibly empty) sequence of nodes i1 , . . . , im ∈V and labels l1 , . . . , lm , l∈L such that (0, i1 , l1 ),(i1 , i2 , l2 ), . . . , (im , j, l)∈A. Table 1: Labeled parsing accuracy for top scoring systems at CoNLL-X (Buchholz and Marsi, 2006). on a variety of languages, as seen in Table 1, which shows results for the two top performing systems in the CoNLL-X shared task, McDonald et al. (2006) (“all-pairs”) and Nivre et al. (2006) (“stepwise”). Despite the similar performance in terms of overall accuracy, there are indications that the two types of models exhibit different behaviour. For example, Sagae and Lavie (2006) displayed that combining the predictions of both parsing models can lead to significantly improved accuracies. In order to pave the way for new and better methods, a much more detailed error analysis is needed to understand the strengths and weaknesses of different approaches. In this work we set out to do just that, focusing on the two top performing systems from the CoNLL-X shared task as representativ"
D07-1013,W03-3017,1,0.186877,"x , starting from the initial configuration cx and taking the optimal transition t∗ = arg maxt∈T s(c, t) out of every configuration c. This can be seen as a greedy search for the optimal dependency graph, based on a sequence of locally optimal decisions in terms of the transition system. Many transition systems for data-driven dependency parsing are inspired by shift-reduce parsing, where configurations contain a stack for storing partially processed nodes. Transitions in such systems add arcs to the dependency graph and/or manipulate the stack. One example is the transition system defined by Nivre (2003), which parses a sentence x = w0 , w1 , . . . , wn in O(n) time, producing a projective dependency graph satisfying conditions 1–4 in section 2.1, possibly after adding arcs (0, i, lr ) for every node i 6= 0 that is a root in the output graph (where lr is a special label for root modifiers). Nivre and Nilsson (2005) showed how the restriction to projective dependency graphs could be lifted by using graph transformation techniques to preprocess training data and post-process parser output, so-called pseudo-projective parsing. To learn transition scores, these systems use discriminative learning"
D07-1013,E06-1010,1,0.29887,"ween the parsers on this metric. MSTParser is slightly more precise for arcs that are predicted with more siblings, whereas MaltParser has slightly higher recall on arcs that have more siblings in the gold standard tree. Arcs closer to the root tend to have more siblings, which ties this result to the previous ones. The final graph property we wish to look at is the degree of non-projectivity. The degree of a dependency arc from word w to word u is defined here as the number of words occurring between w and u that are not descendants of w and modify a word that does not occur between w and u (Nivre, 2006). In the example from Figure 1, the arc from jedna to Z has a degree of one, and all other arcs have a degree of zero. Figure 6 plots dependency arc precision and recall relative to arc degree in predicted and gold standard dependency graphs. MSTParser is more 127 precise when predicting arcs with high degree and MaltParser vice-versa. Again, this can be explained by the fact that there is a tight correlation between a high degree of non-projectivity, dependency length, distance to root and number of siblings. 4.3 Linguistic Factors It is important to relate each system’s accuracy to a set of"
D07-1013,N06-2033,0,0.632561,"y empty) sequence of nodes i1 , . . . , im ∈V and labels l1 , . . . , lm , l∈L such that (0, i1 , l1 ),(i1 , i2 , l2 ), . . . , (im , j, l)∈A. Table 1: Labeled parsing accuracy for top scoring systems at CoNLL-X (Buchholz and Marsi, 2006). on a variety of languages, as seen in Table 1, which shows results for the two top performing systems in the CoNLL-X shared task, McDonald et al. (2006) (“all-pairs”) and Nivre et al. (2006) (“stepwise”). Despite the similar performance in terms of overall accuracy, there are indications that the two types of models exhibit different behaviour. For example, Sagae and Lavie (2006) displayed that combining the predictions of both parsing models can lead to significantly improved accuracies. In order to pave the way for new and better methods, a much more detailed error analysis is needed to understand the strengths and weaknesses of different approaches. In this work we set out to do just that, focusing on the two top performing systems from the CoNLL-X shared task as representatives of the two dominant models in data-driven dependency parsing. 2 2.1 Two Models for Dependency Parsing Preliminaries Let L = {l1 , . . . , l|L |} be a set of permissible arc labels. Let x ="
D07-1096,D07-1119,0,0.635846,"Missing"
D07-1096,D07-1120,0,0.0179351,"Missing"
D07-1096,W06-1615,1,0.130888,"d annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target domain. Obtaining adequate annotated syntactic resources for multiple languages is already a challenging problem, which is only exacerbated when these resources must be drawn from multiple and diverse domains. As a result, the only language that could be feasibly tested in the domain adaptation track was English. The setup for the domain adaptation track was as follows. Participants were prov"
D07-1096,W06-2920,0,0.770442,"d provide a first analysis of these results. 1 Introduction Previous shared tasks of the Conference on Computational Natural Language Learning (CoNLL) have been devoted to chunking (1999, 2000), clause identification (2001), named entity recognition (2002, 2003), and semantic role labeling (2004, 2005). In 2006 the shared task was multilingual dependency parsing, where participants had to train a single parser on data from thirteen different languages, which enabled a comparison not only of parsing and learning methods, but also of the performance that can be achieved for different languages (Buchholz and Marsi, 2006). In dependency-based syntactic parsing, the task is to derive a syntactic structure for an input sentence by identifying the syntactic head of each word in the sentence. This defines a dependency graph, where In this year’s shared task, we continue to explore data-driven methods for multilingual dependency parsing, but we add a new dimension by also introducing the problem of domain adaptation. The way this was done was by having two separate tracks: a multilingual track using essentially the same setup as last year, but with partly different languages, and a domain adaptation track, where th"
D07-1096,D07-1121,0,0.0305771,"Missing"
D07-1096,D07-1101,0,0.829681,"Missing"
D07-1096,W01-0521,0,0.119247,"and Turkish. The treebanks from 2 The reason for having an upper bound on the training set size was the fact that, in 2006, some participants could not train on all the data for some languages because of time limitations. Similar considerations also led to the decision to have a smaller number of languages this year (ten, as opposed to thirteen). 917 which the data sets were extracted are described in section 3. 2.3 Domain Adaptation Track One well known characteristic of data-driven parsing systems is that they typically perform much worse on data that does not come from the training domain (Gildea, 2001). Due to the large overhead in annotating text with deep syntactic parse trees, the need to adapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantia"
D07-1096,A00-2018,0,0.0790907,"test data, and to handle multiple languages, possibly by adjusting a number of hyper-parameters. Participants in the multilingual track were expected to submit parsing results for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One"
D07-1096,W04-3237,0,0.0190919,"dapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target doma"
D07-1096,D07-1097,1,0.748997,"Missing"
D07-1096,D07-1122,0,0.0269928,"Missing"
D07-1096,P97-1003,0,0.129198,"neralize to unseen test data, and to handle multiple languages, possibly by adjusting a number of hyper-parameters. Participants in the multilingual track were expected to submit parsing results for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz an"
D07-1096,D07-1112,0,0.583145,"Missing"
D07-1096,D07-1102,0,0.0310581,"Missing"
D07-1096,W07-2416,0,0.767751,"he test data is a small subset of the development test set of PDT. English For English we used the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). In particular, we used sections 2-11 for training and a subset of section 23 for testing. As a preprocessing stage we removed many functions tags from the non-terminals in the phrase structure representation to make the representations more uniform with out-of-domain test sets for the domain adaptation track (see section 3.2). The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a). This work was done by Ryan McDonald. all the approximately 65,000 tokens of the original treebank for training. The rich morphology of Turkish requires the basic tokens in parsing to be inflectional groups (IGs) rather than words. IGs of a single word are connected to each other deterministically using dependency links labeled DERIV, referred to as word-internal dependencies in the following, and the FORM and the LEMMA fields may be empty (they contain underscore characters in the data files). Sentences do not necessarily have a unique root; most internal punctuation and a few foreign word"
D07-1096,D07-1123,0,0.0858787,"he test data is a small subset of the development test set of PDT. English For English we used the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). In particular, we used sections 2-11 for training and a subset of section 23 for testing. As a preprocessing stage we removed many functions tags from the non-terminals in the phrase structure representation to make the representations more uniform with out-of-domain test sets for the domain adaptation track (see section 3.2). The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a). This work was done by Ryan McDonald. all the approximately 65,000 tokens of the original treebank for training. The rich morphology of Turkish requires the basic tokens in parsing to be inflectional groups (IGs) rather than words. IGs of a single word are connected to each other deterministically using dependency links labeled DERIV, referred to as word-internal dependencies in the following, and the FORM and the LEMMA fields may be empty (they contain underscore characters in the data files). Sentences do not necessarily have a unique root; most internal punctuation and a few foreign word"
D07-1096,W02-2016,0,0.338207,"expected to submit parsing results for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors inv"
D07-1096,W04-3111,1,0.307142,"task. A new test set of about 9,000 tokens was provided by G¨uls¸en Eryi˘git (Eryi˘git, 2007), who also handled the conversion to the CoNLL format, which means that we could use 919 Domain Adaptation Track As mentioned previously, the source data is drawn from a corpus of news, specifically the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). This data set is identical to the English training set from the multilingual track (see section 3.1). For the target domains we used three different labeled data sets. The first two were annotated as part of the PennBioIE project (Kulick et al., 2004) and consist of sentences drawn from either biomedical or chemical research abstracts. Like the source WSJ corpus, this data is annotated using the Penn Treebank phrase structure scheme. To convert these sets to dependency structures we used the same procedure as before (Johansson and Nugues, 2007a). Additional care was taken to remove sentences that contained non-WSJ part-of-speech tags or non-terminals (e.g., HYPH part-of-speech tag indicating a hyphen). Furthermore, the annotation scheme for gaps and traces was made consistent with the Penn Treebank wherever possible. As already mentioned,"
D07-1096,D07-1098,0,0.0447748,"Missing"
D07-1096,D07-1124,0,0.0197253,"Missing"
D07-1096,J93-2004,0,0.0597121,"dependency annotation, just as for PADT. It was also used in the shared task 2006, but there are two important changes compared to last year. First, version 2.0 of PDT was used instead of version 1.0, and a conversion script was created by Zdenek Zabokrtsky, using the new XMLbased format of PDT 2.0. Secondly, due to the upper bound on training set size, only sections 1–3 of PDT constitute the training data, which amounts to some 450,000 tokens. The test data is a small subset of the development test set of PDT. English For English we used the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993). In particular, we used sections 2-11 for training and a subset of section 23 for testing. As a preprocessing stage we removed many functions tags from the non-terminals in the phrase structure representation to make the representations more uniform with out-of-domain test sets for the domain adaptation track (see section 3.2). The resulting data set was then converted to dependency structures using the procedure described in Johansson and Nugues (2007a). This work was done by Ryan McDonald. all the approximately 65,000 tokens of the original treebank for training. The rich morphology of Turk"
D07-1096,N04-1001,0,0.244784,"se trees, the need to adapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated res"
D07-1096,D07-1125,0,0.020143,"Missing"
D07-1096,P06-1043,0,0.463102,"scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target domain. Obtaining adequate annotated syntactic resources for multiple languages is already a challenging problem, which is only exacerbated when these resources must be drawn from multiple and diverse domains. As a result, the only language that could be feasibly tested in the domain adaptation track was English. The setup for the domain adaptation track was as follo"
D07-1096,N03-1027,0,0.115089,"text with deep syntactic parse trees, the need to adapt parsers from domains with plentiful resources (e.g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter set"
D07-1096,D07-1013,1,0.545996,"Missing"
D07-1096,N06-2033,0,0.645238,"Missing"
D07-1096,E06-1011,1,0.349109,"the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors involved in this variation was an important problem for future research. In order to provide an extended empirical foundation"
D07-1096,D07-1111,0,0.734777,"Missing"
D07-1096,H05-1066,1,0.508532,"Missing"
D07-1096,D07-1126,0,0.0372423,"Missing"
D07-1096,W06-2934,1,0.278986,"Missing"
D07-1096,D07-1128,0,0.0204553,"Missing"
D07-1096,D07-1129,0,0.0286186,"Missing"
D07-1096,W06-2902,0,0.016467,"g., news) to domains with little resources is an important problem. This problem is commonly referred to as domain adaptation, where the goal is to adapt annotated resources from a source domain to a target domain of interest. Almost all prior work on domain adaptation assumes one of two scenarios. In the first scenario, there are limited annotated resources available in the target domain, and many studies have shown that this may lead to substantial improvements. This includes the work of Roark and Bacchiani (2003), Florian et al. (2004), Chelba and Acero (2004), Daum´e and Marcu (2006), and Titov and Henderson (2006). Of these, Roark and Bacchiani (2003) and Titov and Henderson (2006) deal specifically with syntactic parsing. The second scenario assumes that there are no annotated resources in the target domain. This is a more realistic situation and is considerably more difficult. Recent work by McClosky et al. (2006) and Blitzer et al. (2006) have shown that the existence of a large unlabeled corpus in the new domain can be leveraged in adaptation. For this shared-task, we are assuming the latter setting – no annotated resources in the target domain. Obtaining adequate annotated syntactic resources for"
D07-1096,D07-1099,0,0.322183,"Missing"
D07-1096,D07-1130,0,0.0262275,"Missing"
D07-1096,D07-1131,0,0.0212092,"Missing"
D07-1096,W03-3023,0,0.882665,"ults for all languages involved. 1 http://depparse.uvt.nl/depparse-wiki/SoftwarePage One of the claimed advantages of dependency parsing, as opposed to parsing based on constituent analysis, is that it extends naturally to languages with free or flexible word order. This explains the interest in recent years for multilingual evaluation of dependency parsers. Even before the 2006 shared task, the parsers of Collins (1997) and Charniak (2000), originally developed for English, had been adapted for dependency parsing of Czech, and the parsing methodology proposed by Kudo and Matsumoto (2002) and Yamada and Matsumoto (2003) had been evaluated on both Japanese and English. The parser of McDonald and Pereira (2006) had been applied to English, Czech and Danish, and the parser of Nivre et al. (2007) to ten different languages. But by far the largest evaluation of multilingual dependency parsing systems so far was the 2006 shared task, where nineteen systems were evaluated on data from thirteen languages (Buchholz and Marsi, 2006). One of the conclusions from the 2006 shared task was that parsing accuracy differed greatly between languages and that a deeper analysis of the factors involved in this variation was an i"
D07-1096,P06-1085,0,\N,Missing
D07-1096,D07-1127,0,\N,Missing
D11-1006,P10-1131,0,0.0631278,"e of the languages that we considered: • USR: The weakly supervised system of Naseem et al. (2010), in which manually defined universal syntactic rules (USR) are used to constrain a probabilistic Bayesian model. In addition to their original results, we also report results using the same part-of-speech tagset as the systems described in this paper (USR†). This is useful for two reasons. First, it makes the comparison more direct. Second, we can generate USR results for all eight languages and not just for the languages that they report. • PGI: The phylogenetic grammar induction (PGI) model of Berg-Kirkpatrick and Klein (2010), in which the parameters of completely 69 unsupervised DMV models for multiple languages are coupled via a phylogenetic prior. • PR: The posterior regularization (PR) approach of Ganchev et al. (2009), in which a supervised English parser is used to generate constraints that are projected using a parallel corpus and used to regularize a target language parser. We report results without treebank specific rules. Table 4 gives results comparing the models presented in this work to those three systems. For this comparison we use sentences of length 10 or less after punctuation has been removed in"
D11-1006,D10-1117,0,0.0262077,"ew domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervised systems. Furthermore, they are often trained and evaluated under idealized conditions, e.g., only on short sentences or assuming the existence of gold-standard part-ofspeech (POS) tags.1 The reason for these assumptions is clear. Unsupervised grammar induction is difficult given the complexity of the analysis space. These assumptions help to give the model traction. The study of unsupervised grammar induction has many merits. Most notably, it increases our"
D11-1006,J93-2003,0,0.027115,"ions), P RT (particles), P UNC (punctuation marks) and X (a catch-all tag). Similar tagsets are used by other studies on grammar induction and projection (Naseem et al., 2010; Zeman and Resnik, 2008). For all our experiments we replaced the language specific part-of-speech tags in the treebanks with these universal tags. Like all treebank projection studies we require a corpus of parallel text for each pair of languages we study. For this we used the Europarl corpus version 5 (Koehn, 2005). The corpus was preprocessed in standard ways and word aligned by running six iterations of IBM Model 1 (Brown et al., 1993), followed by six iterations of the HMM model (Vogel et al., 1996) in both directions. We then intersect word alignments to generate one-to-one alignments. 2.2 Parsing Model All of our parsing models are based on the transition-based dependency parsing paradigm (Nivre, 2008). Specifically, all models use an arc-eager transition strategy and are trained using the averaged perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identi"
D11-1006,W06-2920,0,0.843095,"000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervise"
D11-1006,W10-2906,1,0.522648,"vein to Hwa et al. (2005), Ganchev et al. (2009), Smith and Eisner (2009) inter alia. The algorithm is based on the work of Hall et al. (2011) for training extrinsic parser objective functions and borrows heavily from ideas in learning with weak supervision including work on learning with constraints (Chang et al., 2007) and posterior regularization (Ganchev et al., 2010). In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al. (2010). The algorithm is given in Figure 2. It starts by labeling a set of target language sentences with a parser, which in our case is the direct transfer parser from the previous section (line 1). Next, it uses these parsed target sentences to ‘seed’ a new parser by training a parameter vector using the predicted parses as a gold standard via standard perceptron updates for J rounds (lines 3-6). This generates a parser that emulates the direct transfer parser, but Notation: x: input sentence y: dependency tree a: alignment w: parameter vector φ(x, y): feature vector DP : dependency parser, i.e.,"
D11-1006,P07-1036,0,0.0538361,"h does not rely on projecting syntax across aligned parallel corpora (modulo the fact that non-gold tags come from a system that uses parallel corpora). In this section we describe a simple mechanism for projecting from the direct transfer system using large amounts of parallel data in a similar vein to Hwa et al. (2005), Ganchev et al. (2009), Smith and Eisner (2009) inter alia. The algorithm is based on the work of Hall et al. (2011) for training extrinsic parser objective functions and borrows heavily from ideas in learning with weak supervision including work on learning with constraints (Chang et al., 2007) and posterior regularization (Ganchev et al., 2010). In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al. (2010). The algorithm is given in Figure 2. It starts by labeling a set of target language sentences with a parser, which in our case is the direct transfer parser from the previous section (line 1). Next, it uses these parsed target sentences to ‘seed’ a new parser by training a parameter vector using the"
D11-1006,A00-2018,0,0.0218137,"resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buc"
D11-1006,P04-1014,0,0.00519879,"ng parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually"
D11-1006,N09-1009,0,0.0195959,"hers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervised systems. Furthermore, they are often trained and evaluated under idealized conditions, e.g., only on short sentences or assuming the existence of gold-standard part-ofspeech (POS) tags.1 The reason for these assumptions is clear. Unsupervised grammar induction is difficult given the complexity of the analysis space. These assumptions help to give the mode"
D11-1006,D11-1005,0,0.550093,"ti-source direct (multi-dir.) and projected (multi-proj.) transfer systems. best-source is the best source model from the languages in Table 2 (excluding the target language). avg-source is the mean UAS over the source models for the target (excluding target language). multi-source transfer already provides strong performance gains. We expect that more principled techniques will lead to further improvements. For example, recent work by Søgaard (2011) explores data set sub-sampling methods. Unlike our work, Søgaard found that simply concatenating all the data led to degradation in performance. Cohen et al. (2011) explores the idea learning language specific mixture coefficients for models trained independently on the target language treebanks. However, their results show that this method often did not significantly outperform uniform mixing. 5 Comparison Comparing unsupervised and parser projection systems is difficult as many publications use nonoverlapping sets of languages or different evaluation criteria. We compare to the following three systems that do not augment the treebanks and report results for some of the languages that we considered: • USR: The weakly supervised system of Naseem et al. ("
D11-1006,P99-1065,0,0.200331,"Missing"
D11-1006,P97-1003,0,0.0394505,"cting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins e"
D11-1006,W02-1001,0,0.157428,"Missing"
D11-1006,P11-1061,1,0.74025,"Missing"
D11-1006,P09-1042,0,0.766198,"esults in slightly lower accuracies on average. 5 This requires a transition-based parser with a beam greater than 1 to allow for ambiguity to be resolved at later stages. 65 3.2 Projected Transfer Unlike most language transfer systems for parsers, the direct transfer approach does not rely on projecting syntax across aligned parallel corpora (modulo the fact that non-gold tags come from a system that uses parallel corpora). In this section we describe a simple mechanism for projecting from the direct transfer system using large amounts of parallel data in a similar vein to Hwa et al. (2005), Ganchev et al. (2009), Smith and Eisner (2009) inter alia. The algorithm is based on the work of Hall et al. (2011) for training extrinsic parser objective functions and borrows heavily from ideas in learning with weak supervision including work on learning with constraints (Chang et al., 2007) and posterior regularization (Ganchev et al., 2010). In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al. (2010). The algorithm is given in"
D11-1006,W01-0521,0,0.0110295,"uction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010)."
D11-1006,D11-1138,1,0.682786,"beam greater than 1 to allow for ambiguity to be resolved at later stages. 65 3.2 Projected Transfer Unlike most language transfer systems for parsers, the direct transfer approach does not rely on projecting syntax across aligned parallel corpora (modulo the fact that non-gold tags come from a system that uses parallel corpora). In this section we describe a simple mechanism for projecting from the direct transfer system using large amounts of parallel data in a similar vein to Hwa et al. (2005), Ganchev et al. (2009), Smith and Eisner (2009) inter alia. The algorithm is based on the work of Hall et al. (2011) for training extrinsic parser objective functions and borrows heavily from ideas in learning with weak supervision including work on learning with constraints (Chang et al., 2007) and posterior regularization (Ganchev et al., 2010). In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al. (2010). The algorithm is given in Figure 2. It starts by labeling a set of target language sentences with a parser, which in our"
D11-1006,P04-1061,0,0.0621498,"for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervised systems. Furthermore, they are often trained and evaluated under idealized conditions, e.g., only on short sentences or assuming the existence of gold-standard part-ofspeech (POS) tags.1 The reason for these assumptions is clear. Unsupervised grammar induction is difficult given the complexity of the analysis"
D11-1006,2005.mtsummit-papers.11,0,0.023336,"adverbs), P RON (pronouns), D ET (determiners), A DP (prepositions or postpositions), N UM (numerals), C ONJ (conjunctions), P RT (particles), P UNC (punctuation marks) and X (a catch-all tag). Similar tagsets are used by other studies on grammar induction and projection (Naseem et al., 2010; Zeman and Resnik, 2008). For all our experiments we replaced the language specific part-of-speech tags in the treebanks with these universal tags. Like all treebank projection studies we require a corpus of parallel text for each pair of languages we study. For this we used the Europarl corpus version 5 (Koehn, 2005). The corpus was preprocessed in standard ways and word aligned by running six iterations of IBM Model 1 (Brown et al., 1993), followed by six iterations of the HMM model (Vogel et al., 1996) in both directions. We then intersect word alignments to generate one-to-one alignments. 2.2 Parsing Model All of our parsing models are based on the transition-based dependency parsing paradigm (Nivre, 2008). Specifically, all models use an arc-eager transition strategy and are trained using the averaged perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. The features used by all mod"
D11-1006,J93-2004,0,0.0417235,"el corpora to project the final parser. Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al"
D11-1006,P06-1043,0,0.00526051,"Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction syst"
D11-1006,P05-1012,1,0.789242,"ltiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in"
D11-1006,D10-1120,0,0.557983,"urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervised systems. Furthermore, they are often trained and evaluated under idealized conditions, e.g., only on short sentences or assuming the existence of gold-standard part-ofspeech (POS) tags.1 The reason for these assumptions is clear. Unsupervised grammar induction is difficult given the complexity of the analysis space. These assumptions help to give the model traction. The study of unsupervised grammar inductio"
D11-1006,P05-1013,0,0.0178333,"re trained using the averaged perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of 64 the top word on the stack (if available). All feature conjunctions are included. For treebanks with non-projective trees we use the pseudo-projective parsing technique to transform the treebank into projective structures (Nivre and Nilsson, 2005). We focus on using this parsing system for two reasons. First, the parser is near state-of-the-art on English parsing benchmarks and second, and more importantly, the parser is extremely fast to train and run, making it easy to run a large number of experiments. Preliminary experiments using a different dependency parser – MSTParser (McDonald et al., 2005) – resulted in similar empirical observations. 2.3 Evaluation All systems are evaluated using unlabeled attachment score (UAS), which is the percentage of words (ignoring punctuation tokens) in a corpus that modify the correct head (Buchholz"
D11-1006,nivre-etal-2006-maltparser,0,0.0980768,"can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical"
D11-1006,J08-4003,0,0.108689,"the treebanks with these universal tags. Like all treebank projection studies we require a corpus of parallel text for each pair of languages we study. For this we used the Europarl corpus version 5 (Koehn, 2005). The corpus was preprocessed in standard ways and word aligned by running six iterations of IBM Model 1 (Brown et al., 1993), followed by six iterations of the HMM model (Vogel et al., 1996) in both directions. We then intersect word alignments to generate one-to-one alignments. 2.2 Parsing Model All of our parsing models are based on the transition-based dependency parsing paradigm (Nivre, 2008). Specifically, all models use an arc-eager transition strategy and are trained using the averaged perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of 64 the top word on the stack (if available). All feature conjunctions are included. For treebanks with non-projective trees we use the pseudo-projective par"
D11-1006,P06-1055,1,0.504745,"how that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers. The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006;"
D11-1006,D10-1069,1,0.607013,"t al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large ad"
D11-1006,P07-1049,0,0.00664668,"ysis space. These assumptions help to give the model traction. The study of unsupervised grammar induction has many merits. Most notably, it increases our understanding of how computers (and possibly humans) learn in the absence of any explicit feedback. However, the gold POS tag assumption weakens any conclusions that can be drawn, as part-of-speech are also a form of syntactic analysis, only shallower. Furthermore, from a practical standpoint, it is rarely the case that we are completely devoid of resources for most languages. This point has been made by 1 A notable exception is the work of Seginer (2007). 62 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 62–72, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics studies that transfer parsers to new languages by projecting syntax across word alignments extracted from parallel corpora (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009). Although again, most of these studies also assume the existence of POS tags. In this work we present a method for creating dependency parsers for languages for which no labeled training data is available. First, w"
D11-1006,D08-1052,0,0.00824402,"em result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. Thi"
D11-1006,P05-1044,0,0.0095803,"t. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervised systems. Furthermore, they are often trained and evaluated under idealized conditions, e.g., only on short sentences or assuming the existence of gold-standard part-ofspeech (POS) tags.1 The reason for these assumptions is clear. Unsupervised grammar induction is difficult given the complexity of the analysis space. These assumption"
D11-1006,D09-1086,0,0.138219,"actic analysis, only shallower. Furthermore, from a practical standpoint, it is rarely the case that we are completely devoid of resources for most languages. This point has been made by 1 A notable exception is the work of Seginer (2007). 62 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 62–72, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics studies that transfer parsers to new languages by projecting syntax across word alignments extracted from parallel corpora (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009). Although again, most of these studies also assume the existence of POS tags. In this work we present a method for creating dependency parsers for languages for which no labeled training data is available. First, we train a source side English parser that, crucially, is delexicalized so that its predictions rely soley on the part-of-speech tags of the input sentence, in the same vein as Zeman and Resnik (2008). We empirically show that directly transferring delexicalized models (i.e. parsing a foreign language POS sequence with an English parser) already outperforms state-of-the-art unsupervi"
D11-1006,W04-3207,0,0.00665029,"parallel data in a similar vein to Hwa et al. (2005), Ganchev et al. (2009), Smith and Eisner (2009) inter alia. The algorithm is based on the work of Hall et al. (2011) for training extrinsic parser objective functions and borrows heavily from ideas in learning with weak supervision including work on learning with constraints (Chang et al., 2007) and posterior regularization (Ganchev et al., 2010). In our case, the weak signals come from aligned source and target sentences, and the agreement in their corresponding parses, which is similar to posterior regularization or the bilingual view of Smith and Smith (2004) and Burkett et al. (2010). The algorithm is given in Figure 2. It starts by labeling a set of target language sentences with a parser, which in our case is the direct transfer parser from the previous section (line 1). Next, it uses these parsed target sentences to ‘seed’ a new parser by training a parameter vector using the predicted parses as a gold standard via standard perceptron updates for J rounds (lines 3-6). This generates a parser that emulates the direct transfer parser, but Notation: x: input sentence y: dependency tree a: alignment w: parameter vector φ(x, y): feature vector DP :"
D11-1006,N09-1010,0,0.0349353,"showing that these methods are robust to tagging errors. 4 Multi-Source Transfer The previous section focused on transferring an English parser to a new target language. However, there are over 20 treebanks available for a variety of language groups including Indo-European, Altaic (including Japanese), Semitic, and Sino-Tibetan. Many of these are even in standardized formats (Buchholz and Marsi, 2006; Nivre et al., 2007). Past studies have shown that for both part-of-speech tagging and grammar induction, learning with multiple comparable languages leads to improvements (Cohen and Smith, 2009; Snyder et al., 2009; BergKirkpatrick and Klein, 2010). In this section we exTarget Test Language da de el en es it nl pt sv da 79.2 34.3 33.3 34.4 38.1 44.8 38.7 42.5 44.5 de 45.2 83.9 52.5 37.9 49.4 56.7 43.7 52.0 57.0 el 44.0 53.2 77.5 45.7 57.3 66.8 62.1 66.6 57.8 Source Training Language en es it 45.9 45.0 48.6 47.2 45.8 53.4 63.9 41.6 59.3 82.5 28.5 38.6 53.3 79.7 68.4 57.7 64.7 79.3 60.8 40.9 50.4 69.2 68.5 74.7 58.3 46.3 53.4 nl 46.1 55.8 57.3 43.7 51.2 57.6 73.6 67.1 54.5 pt 48.1 55.5 58.6 42.3 66.7 69.1 58.5 84.6 66.8 sv 47.8 46.2 47.5 43.7 41.4 50.9 44.2 52.1 84.8 Table 2: UAS for all source-target lan"
D11-1006,P11-2120,0,0.151016,".3 68.0 61.1 63.8 pred-POS multi-dir. multi-proj. 46.2 47.5 51.7 52.0 58.5 63.0 55.6 56.5 56.8 58.9 54.3 64.4 67.7 70.3 58.3 62.1 56.1 59.3 Table 3: UAS for multi-source direct (multi-dir.) and projected (multi-proj.) transfer systems. best-source is the best source model from the languages in Table 2 (excluding the target language). avg-source is the mean UAS over the source models for the target (excluding target language). multi-source transfer already provides strong performance gains. We expect that more principled techniques will lead to further improvements. For example, recent work by Søgaard (2011) explores data set sub-sampling methods. Unlike our work, Søgaard found that simply concatenating all the data led to degradation in performance. Cohen et al. (2011) explores the idea learning language specific mixture coefficients for models trained independently on the target language treebanks. However, their results show that this method often did not significantly outperform uniform mixing. 5 Comparison Comparing unsupervised and parser projection systems is difficult as many publications use nonoverlapping sets of languages or different evaluation criteria. We compare to the following th"
D11-1006,N10-1116,0,0.0264635,"om ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources available for English. This observation has led to a vast amount of research on unsupervised grammar induction (Carroll and Charniak, 1992; Klein and Manning, 2004; Smith and Eisner, 2005; Cohen and Smith, 2009; BergKirkpatrick and Klein, 2010; Naseem et al., 2010; Spitkovsky et al., 2010; Blunsom and Cohn, 2010). Grammar induction systems have seen large advances in quality, but parsing accuracies still significantly lag behind those of supervised systems. Furthermore, they are often trained and evaluated under idealized conditions, e.g., only on short sentences or assuming the existence of gold-standard part-ofspeech (POS) tags.1 The reason for these assumptions is clear. Unsupervised grammar induction is difficult given the complexity of the analysis space. These assumptions help to give the model traction. The study of unsupervised grammar induction has many merits. Most n"
D11-1006,C96-2141,0,0.0558518,"all tag). Similar tagsets are used by other studies on grammar induction and projection (Naseem et al., 2010; Zeman and Resnik, 2008). For all our experiments we replaced the language specific part-of-speech tags in the treebanks with these universal tags. Like all treebank projection studies we require a corpus of parallel text for each pair of languages we study. For this we used the Europarl corpus version 5 (Koehn, 2005). The corpus was preprocessed in standard ways and word aligned by running six iterations of IBM Model 1 (Brown et al., 1993), followed by six iterations of the HMM model (Vogel et al., 1996) in both directions. We then intersect word alignments to generate one-to-one alignments. 2.2 Parsing Model All of our parsing models are based on the transition-based dependency parsing paradigm (Nivre, 2008). Specifically, all models use an arc-eager transition strategy and are trained using the averaged perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on t"
D11-1006,W04-0307,0,0.0233826,"d parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages. 1 Introduction Statistical parsing has been one of the most active areas of research in the computational linguistics community since the construction of the Penn Treebank (Marcus et al., 1993). This includes work on phrasestructure parsing (Collins, 1997; Charniak, 2000; Petrov et al., 2006), dependency parsing (McDonald et al., 2005; Nivre et al., 2006) as well as a number of other formalisms (Clark and Curran, 2004; Wang and Harper, 2004; Shen and Joshi, 2008). As underlying modeling techniques have improved, these parsers have begun to converge to high levels of accuracy for English newswire text. Subsequently, researchers have begun to look at both portKeith Hall Google Z¨urich kbhall@google.com ing these parsers to new domains (Gildea, 2001; McClosky et al., 2006; Petrov et al., 2010) and constructing parsers for new languages (Collins et al., 1999; Buchholz and Marsi, 2006; Nivre et al., 2007). One major obstacle in building statistical parsers for new languages is that they often lack the manually annotated resources ava"
D11-1006,I08-3008,0,0.794089,"nship. An example of such a tree is given in Figure 1. Dependency tree arcs are often labeled with the role of the syntactic relationship, e.g., is to hearing might be labeled as S UBJECT. However, we focus on unlabeled parsing in order to reduce problems that arise due to different treebank annotation schemes. Of course, even for unlabeled dependencies, significant variations in the annotation schemes remain. For example, in the Danish treebank determiners govern adjectives and nouns in noun phrases, while in most other treebanks the noun is the head of the noun phrase. Unlike previous work (Zeman and Resnik, 2008; Smith and Eisner, 2009), we do not apply any transformations to the treebanks, which makes our results easier to reproduce, but systematically underestimates accuracy. 2.1 Data Sets The treebank data in our experiments are from the CoNLL shared-tasks on dependency parsing (Buchholz and Marsi, 2006; Nivre et al., 2007). We use English (en) only as a source language throughout the paper. Additionally, we use the following eight languages as both source and target languages: Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es) and Swedish (sv). For langu"
D11-1006,D08-1059,0,0.0486712,"ges we study. For this we used the Europarl corpus version 5 (Koehn, 2005). The corpus was preprocessed in standard ways and word aligned by running six iterations of IBM Model 1 (Brown et al., 1993), followed by six iterations of the HMM model (Vogel et al., 1996) in both directions. We then intersect word alignments to generate one-to-one alignments. 2.2 Parsing Model All of our parsing models are based on the transition-based dependency parsing paradigm (Nivre, 2008). Specifically, all models use an arc-eager transition strategy and are trained using the averaged perceptron algorithm as in Zhang and Clark (2008) with a beam size of 8. The features used by all models are: the part-of-speech tags of the first four words on the buffer and of the top two words on the stack; the word identities of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of 64 the top word on the stack (if available). All feature conjunctions are included. For treebanks with non-projective trees we use the pseudo-projective parsing technique to transform the treebank into projective structures (Nivre and Nilsson, 2005). We focus on using this parsing system for two reasons"
D11-1006,P07-1002,0,\N,Missing
D11-1006,P01-1067,0,\N,Missing
D11-1006,P99-1010,0,\N,Missing
D11-1006,D07-1096,1,\N,Missing
D11-1017,P07-1036,0,0.00547072,"to additional features. Targeted self-training is also similar to the retraining of Burkett et al. (2010) in which they jointly parse unannotated bilingual text using a multiview learning objective, then retrain the monolingual parser models to include each side of the jointly parsed bitext as monolingual training data. Our approach is different in that it doesn’t use a second parser and bitext to guide the creation of new training data, and instead relies on n-best lists and an extrinsic metric. Our method can be considered an instance of weakly or distantly supervised structured prediction (Chang et al., 2007; Chang et al., 2010; Clarke et al., 2010; Ganchev et al., 2010). Those methods attempt to learn structure models from related external signals or aggregate data statistics. This work differs in two respects. First, we use the external signals not as explicit constraints, but to compute an oracle score used to re-rank a set of parses. As such, there are no requirements that it factor by the structure of the parse tree and can in fact be any arbitrary metric. Second, our final objective is different. In weakly/distantly supervised learning, the objective is to use external knowledge to build be"
D11-1017,A00-2018,0,0.0628137,"ween those extrinsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 20"
D11-1017,W10-2903,0,0.016302,"aining is also similar to the retraining of Burkett et al. (2010) in which they jointly parse unannotated bilingual text using a multiview learning objective, then retrain the monolingual parser models to include each side of the jointly parsed bitext as monolingual training data. Our approach is different in that it doesn’t use a second parser and bitext to guide the creation of new training data, and instead relies on n-best lists and an extrinsic metric. Our method can be considered an instance of weakly or distantly supervised structured prediction (Chang et al., 2007; Chang et al., 2010; Clarke et al., 2010; Ganchev et al., 2010). Those methods attempt to learn structure models from related external signals or aggregate data statistics. This work differs in two respects. First, we use the external signals not as explicit constraints, but to compute an oracle score used to re-rank a set of parses. As such, there are no requirements that it factor by the structure of the parse tree and can in fact be any arbitrary metric. Second, our final objective is different. In weakly/distantly supervised learning, the objective is to use external knowledge to build better structured predictors. In our case t"
D11-1017,P05-1066,0,0.596748,"Missing"
D11-1017,P97-1003,0,0.0555975,"correlation between those extrinsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petr"
D11-1017,de-marneffe-etal-2006-generating,0,0.0173365,"Missing"
D11-1017,N10-1060,0,0.0202747,"ng has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment with Japanese, Korean and Turkish, but never meant as a stand-alone task, but is rather a there is nothing language specific in our approach. 183 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183–192, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics which we refer to as targeted self-training (Section 2). Similar to self-training,"
D11-1017,N04-1035,0,0.046446,"ence reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight, 2001; Galley et al., 2004; Xu et al., 2009). While it has been repeatedly shown that using a parser can bring net gains on downstream application quality, it is often unclear how much intrinsic parsing accuracy actually matters. In this paper we try to shed some light on this issue by comparing different parsers in the context of machine translation (MT). We present experiments on translation from English to three Subject-ObjectVerb (SOV) languages,1 because those require extensive syntactic reordering to produce grammatical translations. We evaluate parse quality on a number of extrinsic metrics, including word reord"
D11-1017,C10-1043,0,0.387001,"ne way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into a set C of contiguous spans"
D11-1017,W01-0521,0,0.021726,"regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment with Japanese, Korean and Turkish, but never meant as a stand-alone task, but is rather a there is nothing language specific in our approach. 183 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183–192, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics which we refer to as targeted self-training (S"
D11-1017,2007.mtsummit-papers.29,0,0.312625,"le component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into a set C of co"
D11-1017,D11-1138,1,0.521563,"e parse ranked most highly in the n-best list. The motivation of this selection step is that good performance on the downstream external task, measured by the extrinsic metric, should be predictive of an intrinsically good parse. At the very least, even if the selected parse is not syntactically correct, or even if it goes against the original treebanking guidelines, it results in a higher extrinsic score and should therefore be preferred. One could imagine extending this framework by repeatedly running self-training on successively improving parsers in an EM-style algorithm. A recent work by Hall et al. (2011) on training a parser with multiple objective functions investigates a similar idea in the context of online learning. In this paper we focus our attention on machine translation as the final application, but one could envision applying our techniques to other applications such as information extraction or question answering. In particular, we explore one application of targeted self-training, where computing the extrinsic metric involves plugging the parse into an MT system’s reordering component and computing the accuracy of the reordering compared to a reference word order. We now direct ou"
D11-1017,D10-1092,0,0.0712737,"omputing the accuracy of the reordering compared to a reference word order. We now direct our attention to the details of this application. 3 The MT Reordering Task Determining appropriate target language word order for a translation is a fundamental problem in MT. When translating between languages with significantly different word order such as English and Japanese, it has been shown that metrics which explicitly account for word-order are much better correlated with human judgments of translation quality than those that give more weight to word choice, like BLEU (Lavie and Denkowski, 2009; Isozaki et al., 2010a; Birch and Osborne, 2010). This demonstrates the importance of getting reordering right. 3.1 Reordering as a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transforma"
D11-1017,W10-1736,0,0.160736,"Missing"
D11-1017,P06-1063,0,0.0599599,"ysis and is extremely sensitive to parser errors. 4 4.1 Experimental Setup Good parse Treebank data In our experiments the baseline training corpus is the Wall Street Journal (WSJ) section of the Penn Treebank (Marcus et al., 1993) using standard training/development/testing splits. We converted the treebank to match the tokenization expected by our MT system. In particular, we split tokens containing hyphens into multiple tokens and, somewhat simplistically, gave the original token’s part-of-speech tag to all newly created tokens. In Section 6 we make also use of the Question Treebank (QTB) (Judge et al., 2006), as a source of syntactically annotated out-of-domain data. Though we experiment with both dependency parsers and phrase structure parsers, our MT system assumes dependency parses as input. We use the Stanford converter (de Marneffe et al., 2006) to convert phrase structure parse trees to dependency parse trees (for both treebank trees and predicted trees). Reordered: 15 or greater of an SPF has that sunscreen Wear Reordering score: 1.0 (matches reference) Bad parse Reordered: 15 or greater of an SPF has that Wear sunscreen Reordering score: 0.78 (“Wear” is out of place) Figure 1: Examples of"
D11-1017,P06-1096,0,0.0282082,"Missing"
D11-1017,J93-2004,0,0.0436852,"hile there is a good correlation between those extrinsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to t"
D11-1017,N06-1020,0,0.0229852,"Missing"
D11-1017,P05-1012,1,0.122844,"nsic metrics, parsing quality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsi"
D11-1017,P08-1006,0,0.0120471,"ems between the WSJ-only shift-reduce parser and the QTB-Train targeted self-training 10x shift-reduce parser. ceptron model for an end-to-end MT system where the alignment parameters are updated based on selecting an alignment from a n-best list that leads to highest BLEU score. As mentioned earlier, this also makes our work similar to Hall et al. (2011) who train a perceptron algorithm on multiple objective functions with the goal of producing parsers that are optimized for extrinsic metrics. It has previously been observed that parsers often perform differently for downstream applications. Miyao et al. (2008) compared parser quality in the biomedical domain using a protein-protein interaction (PPI) identification accuracy metric. This allowed them to compare the utility of extant dependency parsers, phrase structure parsers, and deep structure parsers for the PPI identification task. One could apply the targeted self-training technique we describe to optimize any of these parsers for the PPI task, similar to how we have optimized our parser for the MT reordering task. 8 Conclusion reordering component of a machine translation system. This significantly improves the subjective quality of the system"
D11-1017,J08-4003,0,0.104441,"the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment with Japanese,"
D11-1017,P03-1021,1,0.0248517,"mework. 4.5 MT system We carried out all our translation experiments on a state-of-the-art phrase-based statistical MT system. During both training and testing, the system reorders source-language sentences in a preprocessing step using the above-mentioned rules. During decoding, we used an allowed jump width of 4 words. In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. Overall for decoding, we use between 20 to 30 features, whose weights are optimized using MERT (Och, 2003). All experiments for a given lan187 guage pair use the same set of MERT weights tuned on a system using a separate parser (that is neither the baseline nor the experiment parser). This potentially underestimates the improvements that can be obtained, but also eliminates MERT as a possible source of improvement, allowing us to trace back improvements in translation quality directly to parser changes.2 For parallel training data, we use a custom collection of parallel documents. They come from various sources with a substantial portion coming from the web after using simple heuristics to identi"
D11-1017,P02-1040,0,0.108652,"th metrics.4 One explanation for the drops in LAS is that some parts of the parse tree are important for downstream reordering quality while others are not (or only to a lesser extent). Some distinctions between labels become less important; for example, arcs labeled “amod” and “advmod” are transformed identically by the reordering rules. Some semantic distinctions also become less important; for example, any sane interpretation of “red hot car” would be reordered the same, that is, not at all. 5.2 Translation quality improvement To put the improvement of the MT system in terms of BLEU score (Papineni et al., 2002), a widely used metric for automatic MT evaluation, we took 5000 sentences from Web-Test and had humans generate reference translations into Japanese, Korean, and 4 We did not attempt this experiment for the BerkeleyParser since training was too slow. 188 Turkish. We then trained MT systems varying only the parser used for reordering in training and decoding. Table 2 shows that targeted self-training data increases BLEU score for translation into all three languages. In addition to BLEU increase, a side-by-side human evaluation on 500 sentences (sampled from the 5000 used to compute BLEU score"
D11-1017,N07-1051,1,0.0360042,"1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment with Japanese, Korean and Turkish, but never meant as a stand-alone task, but is rather a there is nothing language specific in our approach. 183 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 183–192, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics which we refer to as targeted self-training (Section 2). Similar to self-training, a baseline model is used to produce predictions on an unlabeled data set. However, rather than directly training on the output of the b"
D11-1017,P06-1055,1,0.0472315,"uality as measured on the Penn Treebank is not a good indicator of the final downstream application quality. Since the word reordering metric can be computed efficiently offline (i.e. without the use of the final MT system), we then propose to tune parsers specifically for that metric, with the goal of improving the performance of the overall system. To this end we propose a simple training regime The field of syntactic parsing has received a great deal of attention and progress since the creation of the Penn Treebank (Marcus et al., 1993; Collins, 1997; Charniak, 2000; McDonald et al., 2005; Petrov et al., 2006; Nivre, 2008). A common— and valid—criticism, however, is that parsers typically get evaluated only on Section 23 of the Wall Street Journal portion of the Penn Treebank. This is problematic for many reasons. As previously observed, this test set comes from a very narrow domain that does not necessarily reflect parser performance on text coming from more varied domains (Gildea, 2001), especially web text (Foster, 2010). There is also evidence that after so much repeated testing, parsers are indirectly over-fitting to this set 1 (Petrov and Klein, 2007). Furthermore, parsing was We experiment"
D11-1017,D10-1069,1,0.571776,"to produce predictions on an unlabeled data set. However, rather than directly training on the output of the baseline model, we generate a list of hypotheses and use an external signal to select the best candidate. The selected parse trees are added to the training data and the model is then retrained. The experiments in Section 5 show that this simple procedure noticeably improves our parsers for the task at hand, resulting in significant improvements in downstream translation quality, as measured in a human evaluation on web text. This idea is similar in vein to McClosky. et al. (2006) and Petrov et al. (2010), except that we use an extrinsic quality metric instead of a second parsing model for making the selection. It is also similar to Burkett and Klein (2008) and Burkett et al. (2010), but again avoiding the added complexity introduced by the use of additional (bilingual) models for candidate selection. It should be noted that our extrinsic metric is computed from data that has been manually annotated with reference word reorderings. Details of the reordering metric and the annotated data we used are given in Sections 3 and 4. While this annotation requires some effort, such annotations are much"
D11-1017,W11-2102,1,0.704327,"s into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into a set C of contiguous spans whose content appears contiguously in the same order in the reference. The reordering score is then computed as ρ(esys , eref ) = 1 − |C |− 1 . |e |− 1 This metric assigns a score between 0"
D11-1017,D07-1077,0,0.168629,"mportance of getting reordering right. 3.1 Reordering as a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the"
D11-1017,C04-1073,0,0.448999,"s a separately evaluable component One way to break down the problem of translating between languages with different word order is to handle reordering and translation separately: first reorder source-language sentences into targetlanguage word order in a preprocessing step, and then translate the reordered sentences. It has been shown that good results can be achieved by reordering each input sentence using a series of tree transformations on its parse tree. The rules for tree transformation can be manually written (Collins et al., 2005; Wang, 2007; Xu et al., 2009) or automatically learned (Xia and McCord, 2004; Habash, 2007; Genzel, 2010). Doing reordering as a preprocessing step, separately from translation, makes it easy to evaluate reordering performance independently from the MT system. Accordingly, Talbot et al. (2011) present a framework for evaluating the quality of reordering separately from the lexical choice involved in translation. They propose a simple reordering metric based on METEOR’s reordering penalty (Lavie and Denkowski, 2009). This metric is computed solely on the source language side. To compute it, one takes the candidate reordering of the input sentence and partitions it into"
D11-1017,N09-1028,1,0.710453,"uide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight, 2001; Galley et al., 2004; Xu et al., 2009). While it has been repeatedly shown that using a parser can bring net gains on downstream application quality, it is often unclear how much intrinsic parsing accuracy actually matters. In this paper we try to shed some light on this issue by comparing different parsers in the context of machine translation (MT). We present experiments on translation from English to three Subject-ObjectVerb (SOV) languages,1 because those require extensive syntactic reordering to produce grammatical translations. We evaluate parse quality on a number of extrinsic metrics, including word reordering accuracy, BL"
D11-1017,P01-1067,0,0.0915201,"s of weakly-labeled reference reorderings to guide parser training. Our best parsers contribute significant improvements in subjective translation quality while their intrinsic attachment scores typically regress. 1 Introduction means to an end, towards the goal of building systems that can process natural language input. This is not to say that parsers are not used in larger systems. All to the contrary, as parsing technology has become more mature, parsers have become efficient and accurate enough to be useful in many natural language processing systems, most notably in machine translation (Yamada and Knight, 2001; Galley et al., 2004; Xu et al., 2009). While it has been repeatedly shown that using a parser can bring net gains on downstream application quality, it is often unclear how much intrinsic parsing accuracy actually matters. In this paper we try to shed some light on this issue by comparing different parsers in the context of machine translation (MT). We present experiments on translation from English to three Subject-ObjectVerb (SOV) languages,1 because those require extensive syntactic reordering to produce grammatical translations. We evaluate parse quality on a number of extrinsic metrics,"
D11-1017,W06-3108,0,0.0368459,"icable to any language pair. We chose to use manually written rules to eliminate the variance induced by the automatic reordering-rule learning framework. 4.5 MT system We carried out all our translation experiments on a state-of-the-art phrase-based statistical MT system. During both training and testing, the system reorders source-language sentences in a preprocessing step using the above-mentioned rules. During decoding, we used an allowed jump width of 4 words. In addition to the regular distance distortion model, we incorporate a maximum entropy based lexicalized phrase reordering model (Zens and Ney, 2006) as a feature used in decoding. Overall for decoding, we use between 20 to 30 features, whose weights are optimized using MERT (Och, 2003). All experiments for a given lan187 guage pair use the same set of MERT weights tuned on a system using a separate parser (that is neither the baseline nor the experiment parser). This potentially underestimates the improvements that can be obtained, but also eliminates MERT as a possible source of improvement, allowing us to trace back improvements in translation quality directly to parser changes.2 For parallel training data, we use a custom collection of"
D11-1017,D08-1059,0,0.0478733,"derings is straightforward because annotators need little special background or training, as long as they can speak both the source and target languages. We chose Japanese as the target language through which to create the English reference reorderings because we had access to bilingual annotators fluent in English and Japanese. 186 Parsers The core dependency parser we use is an implementation of a transition-based dependency parser using an arc-eager transition strategy (Nivre, 2008). The parser is trained using the averaged perceptron algorithm with an early update strategy as described in Zhang and Clark (2008). The parser uses the following features: word identity of the first two words on the buffer, the top word on the stack and the head of the top word on the stack (if available); part-ofspeech identities of the first four words on the buffer and top two words on the stack; dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the leftmost modifier of the first word in the buffer. We also include conjunctions over all nonlexical features. We also give results for the latent variable parser (a.k.a. BerkeleyParser) of Petro"
D11-1017,W10-2906,1,\N,Missing
D11-1017,A00-1031,0,\N,Missing
D11-1017,W10-1749,0,\N,Missing
D11-1017,D08-1092,0,\N,Missing
D11-1138,W05-0909,0,0.0430593,"ation system. In particular, we use a system of source-side reordering rules which, given a parse of the source sentence, will reorder the sentence into a target-side order (Collins et al., 2005). In our experiments we work with a set of EnglishJapanese reordering rules1 and gold reorderings based on human generated correct reordering of an aligned target sentences. We use a reordering score based on the reordering penalty from the METEOR scoring metric. Though we could have used a further downstream measure like BLEU, METEOR has also been shown to directly correlate with translation quality (Banerjee and Lavie, 2005) and is simpler to measure. # chunks − 1 # unigrams matched − 1 reorder-cost = 1 − reorder-score reorder-score = 1 − All reordering augmented-loss experiments are run with the same treebank data as the baseline (the training portions of PTB, Brown, and QTB). The extrinsic reordering training data consists of 10930 examples of English sentences and their correct Japanese word-order. We evaluate our results on an evaluation set of 6338 examples of similarly created reordering data. The reordering cost, evaluation 1 Our rules are similar to those from Xu et al. (2009). 1494 trans–PTB + Brown + QT"
D11-1138,P10-1124,0,0.0148682,"ns–PTB trans–unlabeled aug.-loss trans–labeled aug.-loss graph–PTB graph–unlabeled aug.-loss graph–labeled aug.-loss LAS 88.64 88.74 88.84 85.75 85.80 85.85 UAS 91.64 91.91 91.91 88.70 88.81 88.93 ALS 82.96 83.65 83.46 73.88 74.26 74.40 Table 3: Results for both parsers on the development set of the PTB. When training with ALS (labeled and unlabeled), we see an improvement in UAS, LAS, and ALS. Furthermore, if we use a labeled-ALS as the metric for augmented-loss training, we also see a considerable increase in LAS. like information extraction (Yates and Etzioni, 2009) and textual entailment (Berant et al., 2010). In Table 3 we show results for parsing with the ALS augmented-loss objective. For each parser, we consider two different ALS objective functions; one based on unlabeled-ALS and the other on labeledALS. The arc-length score penalizes incorrect longdistance dependencies more than local dependencies; long-distance dependencies are often more destructive in preserving sentence meaning and can be more difficult to predict correctly due to the larger context on which they depend. Combining this with the standard attachment scores biases training to focus on the difficult head dependencies. For bot"
D11-1138,W06-1615,1,0.385824,"eased performance on multiple target tasks including reordering for machine translation and parser adaptation. 1 Introduction The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001; McClosky et al., 2006; Blitzer et al., 2006; Petrov et al., 2010). But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means 1489 that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser tr"
D11-1138,W06-2920,0,0.00757661,"present a set of scoring functions that can be used in the inline reranker loss framework, resulting in a new augmented-loss for each one. Augmented-loss learning is then applied to target a downstream task using the loss functions to measure gains. We show empirical results for two extrinsic loss-functions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic loss-function: an arclength parsing score. For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score (UAS) and labeled attachment score (LAS) (Buchholz and Marsi, 2006). In terms of treebank data, the primary training corpus is the Penn Wall Street Journal Treebank (PTB) (Marcus et al., 1993). We also make use of the Brown corpus, and the Question Treebank (QTB) (Judge et al., 2006). For PTB and Brown we use standard training/development/testing splits of the data. For the QTB we split the data into three sections: 2000 training, 1000 development, and 1000 test. All treebanks are converted to dependency format using the Stanford converter v1.6 (de Marneffe et al., 2006). 4 Experiments 4.1 Machine Translation Reordering Score As alluded to in Section 2.2, we"
D11-1138,N10-1015,0,0.00907495,"s mentioned earlier, CODL is one example of learning algorithms that use weak supervision, others include Mann and McCallum (2010) and Ganchev et al. (2010). Again, these works are typically interested in using the extrinsic metric – or, in general, extrinsic information – to optimize the intrinsic metric in the absence of any labeled intrinsic data. Our goal is to optimize both simultaneously. The idea of jointly training parsers to optimize multiple objectives is related to joint learning and inference for tasks like information extraction (Finkel and Manning, 2009) and machine translation (Burkett et al., 2010). In such works, a large search space that covers both the space of parse structures and the space of task-specific structures is defined and parameterized so that standard learning and inference algorithms can be applied. What sets our work apart is that there is still just a single parameter set that is being optimized – the parser parameters. Our method only uses feedback from task specific objectives in order to update the parser parameters, guiding it towards better downstream performance. This is advantageous for two reasons. First, it decouples the tasks, making inference and learning m"
D11-1138,P07-1036,0,0.135597,"e function associated with the instance (either intrinsic or extrinsic), thus jointly optimizing multiple objectives. An update schedule trades-off the relative importance of each objective function. We call our algorithm augmented-loss training as it optimizes multiple losses to augment the traditional supervised parser loss. There have been a number of efforts to exploit weak or external signals of quality to train better prediction models. This includes work on generalized expectation (Mann and McCallum, 2010), posterior regularization (Ganchev et al., 2010) and constraint driven learning (Chang et al., 2007; Chang et al., 2010). The work of Chang et al. (2007) on constraint driven learning is perhaps the closest to our framework and we draw connections to it in Section 5. In these studies the typical goal is to use the weak signal to improve the structured prediction models on the intrinsic evaluation metrics. For our setting this would mean using weak application specific signals to improve dependency parsing. Though we explore such ideas in our experiments, in particular for semi-supervised domain adaptation, we are primarily interested in the case where the weak signal is precisely what we wi"
D11-1138,P05-1066,0,0.0454649,"Missing"
D11-1138,W02-1001,0,0.322807,"en1490 Algorithm 1 Structured Perceptron {Input data sets: D = {d1 = (x1 , y1 ) . . . dN = (xN , yN )}} {Input 0/1 loss: L(Fθ (x), y) = [Fθ (x) 6= y ? 1 : 0]} {Let: Fθ (x) = arg maxy∈Y θ · Φ(y)} {Initialize model parameters: θ = ~0} repeat for i = 1 . . . N do {Compute structured loss} yˆi = Fθ (xi ) if L(ˆ yi , yi ) &gt; 0 then {Update model Parameters} θ = θ + Φ(yi ) − Φ(ˆ yi ) end if end for until converged {Return model θ} tence xi and an output yi ; and 2) a loss-function, L(ˆ y , y), that measures the cost of predicting output yˆ relative to the gold standard y and is usually the 0/1 loss (Collins, 2002). For dependency parser training, this set-up consists of input sentences x and the corresponding gold dependency tree y ∈ Yx , where Yx is the space of possible parse trees for sentence x. In the perceptron setting, Fθ (x) = arg maxy∈Yx θ · Φ(y) where Φ is mapping from a parse tree y for sentence x to a high dimensional feature space. Learning proceeds by predicting a structured output given the current model, and if that structure is incorrect, updating the model: rewarding features that fire in the gold-standard Φ(yi ), and discounting features that fire in the predicted output, Φ(ˆ yi ). T"
D11-1138,de-marneffe-etal-2006-generating,0,0.0179881,"Missing"
D11-1138,N09-1037,0,0.0107955,"ate the updates in our online learning algorithm. As mentioned earlier, CODL is one example of learning algorithms that use weak supervision, others include Mann and McCallum (2010) and Ganchev et al. (2010). Again, these works are typically interested in using the extrinsic metric – or, in general, extrinsic information – to optimize the intrinsic metric in the absence of any labeled intrinsic data. Our goal is to optimize both simultaneously. The idea of jointly training parsers to optimize multiple objectives is related to joint learning and inference for tasks like information extraction (Finkel and Manning, 2009) and machine translation (Burkett et al., 2010). In such works, a large search space that covers both the space of parse structures and the space of task-specific structures is defined and parameterized so that standard learning and inference algorithms can be applied. What sets our work apart is that there is still just a single parameter set that is being optimized – the parser parameters. Our method only uses feedback from task specific objectives in order to update the parser parameters, guiding it towards better downstream performance. This is advantageous for two reasons. First, it decou"
D11-1138,W01-0521,0,0.0234404,"sed parsing) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation. 1 Introduction The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001; McClosky et al., 2006; Blitzer et al., 2006; Petrov et al., 2010). But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means 1489 that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our s"
D11-1138,P07-1050,1,0.0513251,"es of the first two words on the buffer and of the top word on the stack; the word identity of the syntactic head of the top word on the stack (if available); dependency arc label identities for the top word on the stack, the left and rightmost modifier of the top word on the stack, and the left most modifier of the first word in the buffer (if available). All feature conjunctions are included. • Graph-based: An implementation of graphbased parsing algorithms with an arc-factored parameterization (McDonald et al., 2005). We use the non-projective k-best MST algorithm to generate k-best lists (Hall, 2007), where k = 8 for the experiments in this paper. The graphbased parser features used in the experiments in this paper are defined over a word, wi at position i; the head of this word wρ(i) where ρ(i) provides the index of the head word; and partof-speech tags of these words ti . We use the following set of features similar to McDonald et al. (2005): isolated features: wi , ti , wρ(i) , tρ(i) word-tag pairs: (wi , ti ); (wρ(i) , tρ(i) ) word-head pairs: (wi , wρ(i) ), (ti , tρ(i) ) word-head-tag triples: (tρ(i) , wi , ti ) (wρ(i) , wi , ti ) (wρ(i) , tρ(i) , ti ) (wρ(i) , tρ(i) , wi ) tag-neigh"
D11-1138,P06-1063,0,0.0103196,"Missing"
D11-1138,D11-1017,1,0.509057,"d found very similar improvements: for example, the transition parser’s LAS for the labeled loss is 88.68 and 88.84, respectively). We note that ALS can be decomposed locally and could be used as the primary objective function for 1496 parsing. A parse with perfect scores under ALS and LAS will match the gold-standard training tree. However, if we were to order incorrect parses of a sentence, ALS and LAS will suggest different orderings. Our results show that by optimizing for losses based on a combination of these metrics we train a more robust parsing model. 5 Related Work A recent study by Katz-Brown et al. (2011) also investigates the task of training parsers to improve MT reordering. In that work, a parser is used to first parse a set of manually reordered sentences to produce k-best lists. The parse with the best reordering score is then fixed and added back to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training (McClosky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings. Our method differs as it does not statically fix a new parse, but dynamicall"
D11-1138,P06-1096,0,0.0216677,"ck to the training set and a new parser is trained on resulting data. The method is called targeted self-training as it is similar in vein to self-training (McClosky et al., 2006), with the exception that the new parse data is targeted to produce accurate word reorderings. Our method differs as it does not statically fix a new parse, but dynamically updates the parameters and parse selection by incorporating the additional loss in the inner loop of online learning. This allows us to give guarantees of convergence. Furthermore, we also evaluate the method on alternate extrinsic loss functions. Liang et al. (2006) presented a perceptron-based algorithm for learning the phrase-translation parameters in a statistical machine translation system. Similar to the inline-ranker loss function presented here, they use a k-best lists of hypotheses in order to identify parameters which can improve a global objective function: BLEU score. In their work, they are interested in learning a parameterization over translation phrases (including the underlying wordalignment) which optimizes the BLEU score. Their goal is considerably different; they want to incorporate additional features into their model and define an ob"
D11-1138,J93-2004,0,0.0370346,"each one. Augmented-loss learning is then applied to target a downstream task using the loss functions to measure gains. We show empirical results for two extrinsic loss-functions (optimizing for the downstream task): machine translation and domain adaptation; and for one intrinsic loss-function: an arclength parsing score. For some experiments we also measure the standard intrinsic parser metrics unlabeled attachment score (UAS) and labeled attachment score (LAS) (Buchholz and Marsi, 2006). In terms of treebank data, the primary training corpus is the Penn Wall Street Journal Treebank (PTB) (Marcus et al., 1993). We also make use of the Brown corpus, and the Question Treebank (QTB) (Judge et al., 2006). For PTB and Brown we use standard training/development/testing splits of the data. For the QTB we split the data into three sections: 2000 training, 1000 development, and 1000 test. All treebanks are converted to dependency format using the Stanford converter v1.6 (de Marneffe et al., 2006). 4 Experiments 4.1 Machine Translation Reordering Score As alluded to in Section 2.2, we use a reorderingbased loss function to improve word order in a machine translation system. In particular, we use a system of"
D11-1138,P06-1043,0,0.0597095,"nd how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation. 1 Introduction The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001; McClosky et al., 2006; Blitzer et al., 2006; Petrov et al., 2010). But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means 1489 that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal param"
D11-1138,D07-1013,1,0.109425,"ρi )(i − ρi ) ALS = i P i (i − ρi ) For each word of the sentence we compute the distance between the word’s position i and the position of the words head ρi . The arc-length score is the summed length of all those with correct head assignments (δ(ˆ ρi , ρi ) is 1 if the predicted head and the correct head match, 0 otherwise). The score is normalized by the summed arc lengths for the sentence. The labeled version of this score requires that the labels of the arc are also correct. Optimizing for dependency arc length is particularly important as parsers tend to do worse on longer dependencies (McDonald and Nivre, 2007) and these dependencies are typically the most meaningful for downstream tasks, e.g., main verb dependencies for tasks To test whether such weak information can significantly improve the parsing of questions, we trained an augmented-loss parser using the training set of the QTB stripped of all dependencies except the dependency from the root to the main verb of the sentence. In other words, for each sentence, the parser may only observe a single dependency at training from the QTB – the dependency to the main verb. Our augmented-loss function in this case is a simple binary function: 0 if a pa"
D11-1138,P05-1012,1,0.614055,"tandard Φ(yi ), and discounting features that fire in the predicted output, Φ(ˆ yi ). The structured perceptron, as given in Algorithm 1, only updates when there is a positive loss, meaning that there was a prediction mistake. For the moment we will abstract away from details such as the precise definition of F (x) and Φ(y). We will show in the next section that our augmentedloss method is general and can be applied to any dependency parsing framework that can be trained by the perceptron algorithm, such as transition-based parsers (Nivre, 2008; Zhang and Clark, 2008) and graph-based parsers (McDonald et al., 2005). 2.1 Augmented-Loss Training The augmented-loss training algorithm that we propose is based on the structured perceptron; however, the augmented-loss training framework is a general mechanism to incorporate multiple loss functions in online learner training. Algorithm 2 is the pseudocode for the augmented-loss structured perceptron algorithm. The algorithm is an extension to Algorithm 1 where there are 1) multiple loss functions being evaluated L1 , . . . , LM ; 2) there are multiple datasets associated with each of these loss functions D1 , . . . , DM ; and 3) there is a schedule for process"
D11-1138,N10-1120,0,0.0149717,"cific extrinsic measures of quality. Our empirical results show how this approach performs for two dependency parsing algorithms (graph-based and transition-based parsing) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation. 1 Introduction The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001; McClosky et al., 2006; Blitzer et al., 2006; Petrov et al., 2010). But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means 1489 that we are not properly modeling the do"
D11-1138,J08-4003,0,0.16755,"pdating the model: rewarding features that fire in the gold-standard Φ(yi ), and discounting features that fire in the predicted output, Φ(ˆ yi ). The structured perceptron, as given in Algorithm 1, only updates when there is a positive loss, meaning that there was a prediction mistake. For the moment we will abstract away from details such as the precise definition of F (x) and Φ(y). We will show in the next section that our augmentedloss method is general and can be applied to any dependency parsing framework that can be trained by the perceptron algorithm, such as transition-based parsers (Nivre, 2008; Zhang and Clark, 2008) and graph-based parsers (McDonald et al., 2005). 2.1 Augmented-Loss Training The augmented-loss training algorithm that we propose is based on the structured perceptron; however, the augmented-loss training framework is a general mechanism to incorporate multiple loss functions in online learner training. Algorithm 2 is the pseudocode for the augmented-loss structured perceptron algorithm. The algorithm is an extension to Algorithm 1 where there are 1) multiple loss functions being evaluated L1 , . . . , LM ; 2) there are multiple datasets associated with each of these"
D11-1138,D10-1069,1,0.366832,"ultiple target tasks including reordering for machine translation and parser adaptation. 1 Introduction The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001; McClosky et al., 2006; Blitzer et al., 2006; Petrov et al., 2010). But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means 1489 that we are not properly modeling the downstream task in the parsers, it also means that there is some information from small task or domain-specific data sets which could help direct our search for optimal parameters during parser training. The goal being"
D11-1138,W11-2102,1,0.532668,"der 35.29 76.49 38.71 78.19 39.02 78.39 39.58 78.67 25.71 69.84 28.99 72.23 29.99 72.88 30.03 73.15 Table 1: Reordering scores for parser-based reordering (English-to-Japanese). Exact is the number of correctly reordered sentences. All models use the same treebankdata (PTB, QTB, and the Brown corpus). Results for three augmented-loss schedules are shown: 0.5 where for every two treebank updates we make one augmented-loss update, 1 is a 1-to-1 mix, and 2 is where we make twice as many augmented-loss updates as treebank updates. criteria and data used in our experiments are based on the work of Talbot et al. (2011). Table 1 shows the results of using the reordering cost as an augmented-loss to the standard treebank objective function. Results are presented as measured by the reordering score as well as a coarse exact-match score (the number of sentences which would have correct word-order given the parse and the fixed reordering rules). We see continued improvements as we adjust the schedule to process the extrinsic loss more frequently, the best result being when we make two augmented-loss updates for every one treebank-based loss update. 4.2 Semi-supervised domain adaptation Another application of the"
D11-1138,D07-1003,0,0.0463539,"on intrinsic parsing quality or task-specific extrinsic measures of quality. Our empirical results show how this approach performs for two dependency parsing algorithms (graph-based and transition-based parsing) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation. 1 Introduction The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001; McClosky et al., 2006; Blitzer et al., 2006; Petrov et al., 2010). But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means 1"
D11-1138,N09-1028,1,0.130574,"r empirical results show how this approach performs for two dependency parsing algorithms (graph-based and transition-based parsing) and how it achieves increased performance on multiple target tasks including reordering for machine translation and parser adaptation. 1 Introduction The accuracy and speed of state-of-the-art dependency parsers has motivated a resumed interest in utilizing the output of parsing as an input to many downstream natural language processing tasks. This includes work on question answering (Wang et al., 2007), sentiment analysis (Nakagawa et al., 2010), MT reordering (Xu et al., 2009), and many other tasks. In most cases, the accuracy of parsers degrades when run on out-of-domain data (Gildea, 2001; McClosky et al., 2006; Blitzer et al., 2006; Petrov et al., 2010). But these accuracies are measured with respect to gold-standard out-of-domain parse trees. There are few tasks that actually depend on the complete parse tree. Furthermore, when evaluated on a downstream task, often the optimal parse output has a model score lower than the best parse as predicted by the parsing model. While this means 1489 that we are not properly modeling the downstream task in the parsers, it"
D11-1138,D08-1059,0,0.395731,"odel: rewarding features that fire in the gold-standard Φ(yi ), and discounting features that fire in the predicted output, Φ(ˆ yi ). The structured perceptron, as given in Algorithm 1, only updates when there is a positive loss, meaning that there was a prediction mistake. For the moment we will abstract away from details such as the precise definition of F (x) and Φ(y). We will show in the next section that our augmentedloss method is general and can be applied to any dependency parsing framework that can be trained by the perceptron algorithm, such as transition-based parsers (Nivre, 2008; Zhang and Clark, 2008) and graph-based parsers (McDonald et al., 2005). 2.1 Augmented-Loss Training The augmented-loss training algorithm that we propose is based on the structured perceptron; however, the augmented-loss training framework is a general mechanism to incorporate multiple loss functions in online learner training. Algorithm 2 is the pseudocode for the augmented-loss structured perceptron algorithm. The algorithm is an extension to Algorithm 1 where there are 1) multiple loss functions being evaluated L1 , . . . , LM ; 2) there are multiple datasets associated with each of these loss functions D1 , . ."
D12-1030,P11-1045,0,0.0108961,"in machine translation. This also distinguishes it from previous work on dependency parse re-ranking (Hall, 2007) as we are not re-ranking/re-scoring the output of a base model but using a single decoding algorithm and learned model at training and testing. This work is largely orthogonal to other attempts to speed up chart parsing algorithms. This includes work on coarse-to-fine parsing (Charniak and Johnson, 2005; Petrov and Klein, 2007; Rush and Petrov, 2012), chart-cell closing and pruning (Roark and Hollingshead, 2008; Roark and Hollingshead, 329 2009), and dynamic beam-width prediction (Bodenstab et al., 2011). Of particular note, Rush and Petrov (2012) report run-times far better than our cube pruning system. At the heart of their system is a linear time vine-parsing stage that prunes most of the search space before higher-order parsing. This effectively makes their final system linear time in practice as the higher order models have far fewer parts to consider. One could easily use the same first-pass pruner in our cube-pruning framework. In our study we use cube pruning only for decoding and rely on inference-based learning algorithms to train model parameters. Gimpel and Smith (2009) extended c"
D12-1030,W06-2920,0,0.462257,"Missing"
D12-1030,D07-1101,0,0.738168,"constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requires significant changes to the underlying decoding algorithm. This is in stark co"
D12-1030,P05-1022,0,0.199893,"a generative base model from a following discriminative re-ranking model. Hence, our formulation is more akin to the one pass decoding algorithm of Chiang (2007) for integrated decoding with a language model in machine translation. This also distinguishes it from previous work on dependency parse re-ranking (Hall, 2007) as we are not re-ranking/re-scoring the output of a base model but using a single decoding algorithm and learned model at training and testing. This work is largely orthogonal to other attempts to speed up chart parsing algorithms. This includes work on coarse-to-fine parsing (Charniak and Johnson, 2005; Petrov and Klein, 2007; Rush and Petrov, 2012), chart-cell closing and pruning (Roark and Hollingshead, 2008; Roark and Hollingshead, 329 2009), and dynamic beam-width prediction (Bodenstab et al., 2011). Of particular note, Rush and Petrov (2012) report run-times far better than our cube pruning system. At the heart of their system is a linear time vine-parsing stage that prunes most of the search space before higher-order parsing. This effectively makes their final system linear time in practice as the higher order models have far fewer parts to consider. One could easily use the same firs"
D12-1030,J07-2003,0,0.323506,"higher-order features only involves changing the scoring function of 320 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 320–331, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics potential parses in each chart cell by expanding the signature of each chart item to include all the nonlocal context required to compute features. The core chart-parsing algorithm remains the same regardless of which features are incorporated. To control complexity we use cube pruning (Chiang, 2007) with the beam size k in each cell. Furthermore, dynamic programming in the style of Huang and Sagae (2010) can be done by merging k-best items that are equivalent in scoring. Thus, our method is an application of integrated decoding with a language model in MT (Chiang, 2007) to dependency parsing, which has previously been applied to constituent parsing (Huang, 2008). However, unlike Huang, we only have one decoding pass and a single trained model, while Huang’s constituent parser maintains a separate generative base model from a following discriminative re-ranking model. We draw connections"
D12-1030,de-marneffe-etal-2006-generating,0,0.0123389,"Missing"
D12-1030,D07-1098,0,0.0197449,"well documented (McDonald and Nivre, 2007; Nivre and McDonald, 2008). Graph-based parsers typically tradeoff rich feature scope for exact (or near exact) decoding, whereas transition-based parsers make the opposite trade-off. Recent research on both parsing paradigms has attempted to address this. In the transition-based parsing literature, the focus has been on increasing the search space of the system at decoding time, as expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear progr"
D12-1030,C96-1058,0,0.561994,"s expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requi"
D12-1030,E09-1037,0,0.0167381,"rediction (Bodenstab et al., 2011). Of particular note, Rush and Petrov (2012) report run-times far better than our cube pruning system. At the heart of their system is a linear time vine-parsing stage that prunes most of the search space before higher-order parsing. This effectively makes their final system linear time in practice as the higher order models have far fewer parts to consider. One could easily use the same first-pass pruner in our cube-pruning framework. In our study we use cube pruning only for decoding and rely on inference-based learning algorithms to train model parameters. Gimpel and Smith (2009) extended cube pruning concepts to partitionfunction and marginal calculations, which would enable the training of probabilistic graphical models. Finally, due to its use of the Eisner chart-parsing algorithm as a backbone, our model is fundamentally limited to predicting projective dependency structures. Investigating extensions of this work to the non-projective case is an area of future study. Work on defining bottom-up chart-parsing algorithms for non-projective dependency trees could potentially serve as a mechanism to solving this problem (G´omez-Rodr´ıguez et al., 2009; Kuhlmann and Sat"
D12-1030,N09-1061,0,0.0383786,"Missing"
D12-1030,N10-1035,0,0.036726,"Missing"
D12-1030,P07-1050,0,0.0551976,"the extension of such ideas to dependency parsing, also giving state-ofthe-art results. An important difference between our formulation and forest rescoring is that we only have one decoding pass and a single trained model, while forest rescoring, as formulated by Huang (2008), separates a generative base model from a following discriminative re-ranking model. Hence, our formulation is more akin to the one pass decoding algorithm of Chiang (2007) for integrated decoding with a language model in machine translation. This also distinguishes it from previous work on dependency parse re-ranking (Hall, 2007) as we are not re-ranking/re-scoring the output of a base model but using a single decoding algorithm and learned model at training and testing. This work is largely orthogonal to other attempts to speed up chart parsing algorithms. This includes work on coarse-to-fine parsing (Charniak and Johnson, 2005; Petrov and Klein, 2007; Rush and Petrov, 2012), chart-cell closing and pruning (Roark and Hollingshead, 2008; Roark and Hollingshead, 329 2009), and dynamic beam-width prediction (Bodenstab et al., 2011). Of particular note, Rush and Petrov (2012) report run-times far better than our cube pru"
D12-1030,N12-1015,0,0.017798,"Missing"
D12-1030,P10-1110,0,0.481055,"rsers make the opposite trade-off. Recent research on both parsing paradigms has attempted to address this. In the transition-based parsing literature, the focus has been on increasing the search space of the system at decoding time, as expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a"
D12-1030,P08-1067,0,0.742354,"ure of each chart item to include all the nonlocal context required to compute features. The core chart-parsing algorithm remains the same regardless of which features are incorporated. To control complexity we use cube pruning (Chiang, 2007) with the beam size k in each cell. Furthermore, dynamic programming in the style of Huang and Sagae (2010) can be done by merging k-best items that are equivalent in scoring. Thus, our method is an application of integrated decoding with a language model in MT (Chiang, 2007) to dependency parsing, which has previously been applied to constituent parsing (Huang, 2008). However, unlike Huang, we only have one decoding pass and a single trained model, while Huang’s constituent parser maintains a separate generative base model from a following discriminative re-ranking model. We draw connections to related work in Section 6. Our chart-based approximate search algorithm allows for features on dependencies of an arbitrary order — as well as over non-local structural properties of the parse trees — to be scored at will. In this paper, we use first to third-order features of greater varieties than Koo and Collins (2010). Additionally, we look at higher-order depe"
D12-1030,D07-1123,0,0.034191,"Donald and Nivre, 2007; Nivre and McDonald, 2008). Graph-based parsers typically tradeoff rich feature scope for exact (or near exact) decoding, whereas transition-based parsers make the opposite trade-off. Recent research on both parsing paradigms has attempted to address this. In the transition-based parsing literature, the focus has been on increasing the search space of the system at decoding time, as expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of"
D12-1030,P10-1001,0,0.0902925,"ncrease in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requires significant changes to the underlying decoding algorithm. This is in stark contrast to transition-bas"
D12-1030,D10-1125,0,0.25314,"007; Koo and Collins, 2010), but at a high computational cost as increasing the order of a model typically results in an asymptotic increase in running time. ILP formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010) also allow for exact inference with higherorder features, but again at a high computational cost as ILP’s have, in the worst-case, exponential run-time with respect to the sentence length. Studies that have abandoned exact inference have focused on sampling (Nakagawa, 2007), belief propagation (Smith and Eisner, 2008), Lagrangian relaxation (Koo et al., 2010; Martins et al., 2011), and more recently structured prediction cascades (Weiss and Taskar, 2010; Rush and Petrov, 2012). However, these approximations themselves are often computationally expensive, requiring multiple decoding/sampling stages in order to produce an output. All the methods above, both exact and approximate, require specialized algorithms for every new feature that is beyond the scope of the previous factorization. In our method, the same parsing algorithm can be utilized (Eisner’s + cube pruning) just with slight different feature signatures. Our proposed parsing model draws"
D12-1030,E09-1055,0,0.0109773,"and Smith (2009) extended cube pruning concepts to partitionfunction and marginal calculations, which would enable the training of probabilistic graphical models. Finally, due to its use of the Eisner chart-parsing algorithm as a backbone, our model is fundamentally limited to predicting projective dependency structures. Investigating extensions of this work to the non-projective case is an area of future study. Work on defining bottom-up chart-parsing algorithms for non-projective dependency trees could potentially serve as a mechanism to solving this problem (G´omez-Rodr´ıguez et al., 2009; Kuhlmann and Satta, 2009; G´omez-Rodr´ıguez et al., 2010). 7 Conclusion In this paper we presented a method for generalized higher-order dependency parsing. The method works by augmenting the dynamic programming signatures of the Eisner chart-parsing algorithm and then controlling complexity via cube pruning. The resulting system has the flexibility to incorporate arbitrary feature history while still exploring an exponential search space efficiently. Empirical results show that the system gives state-of-the-art accuracies across numerous data sets while still maintaining practical parsing speeds – as much as 4-5 tim"
D12-1030,P09-1039,0,0.617285,"and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requires significant changes to the underlying decoding algorithm. This is in stark contrast to transition-based systems, which simply require the definition of new feature extractors. In this paper, we abandon exact search in graphbased parsing in favor of freedom"
D12-1030,D10-1004,0,0.322372,"and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requires significant changes to the underlying decoding algorithm. This is in stark contrast to transition-based systems, which simply require the definition of new feature extractors. In this paper, we abandon exact search in graphbased parsing in favor of freedom in feature scope. We pr"
D12-1030,D11-1022,0,0.0527376,"ns, 2010), but at a high computational cost as increasing the order of a model typically results in an asymptotic increase in running time. ILP formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010) also allow for exact inference with higherorder features, but again at a high computational cost as ILP’s have, in the worst-case, exponential run-time with respect to the sentence length. Studies that have abandoned exact inference have focused on sampling (Nakagawa, 2007), belief propagation (Smith and Eisner, 2008), Lagrangian relaxation (Koo et al., 2010; Martins et al., 2011), and more recently structured prediction cascades (Weiss and Taskar, 2010; Rush and Petrov, 2012). However, these approximations themselves are often computationally expensive, requiring multiple decoding/sampling stages in order to produce an output. All the methods above, both exact and approximate, require specialized algorithms for every new feature that is beyond the scope of the previous factorization. In our method, the same parsing algorithm can be utilized (Eisner’s + cube pruning) just with slight different feature signatures. Our proposed parsing model draws heavily on the work of"
D12-1030,D07-1013,1,0.778257,"Missing"
D12-1030,E06-1011,1,0.911376,"n most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requires significant changes to the underlying decoding algorithm. Thi"
D12-1030,P05-1012,1,0.569981,"of head-modifier dependency arcs y ∗ such that y ∗ = argmaxy∈Y(x) f (x, y), where f is a scoring function. As mentioned before, y ∗ must represent a directed tree. |Y(x) |is then the set of valid dependency trees for x and grows exponentially with respect to its length |x|. We further define L as the set of possible arc labels and use l the notation (i − → j) ∈ y to indicate that there is a dependency from head word xi to modifier xj with label l in dependency tree y. In practice, f (x, y) is factorized into scoring functions on parts of (x, y). For example, in firstorder dependency parsing (McDonald et al., 2005), f (x, y) is factored by the individual arcs: X l f (i − → j) y ∗ = argmax f (x, y) = argmax y∈Y(x) y∈Y(x) l (i− →j)∈y The factorization of dependency structures into arcs enables an efficient dynamic programming algorithm with running time O(|x|3 ) (Eisner, 1996), for the large family of projective dependency structures. Figure 2 shows the parsing logic for the Eisner algorithm. It has two types of dynamic programming states: complete items and incomplete items. Complete items correspond to half-constituents, and are represented as triangles graphically. Incomplete items correspond to depend"
D12-1030,D07-1100,0,0.0337083,"order features have been studied extensively (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), but at a high computational cost as increasing the order of a model typically results in an asymptotic increase in running time. ILP formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010) also allow for exact inference with higherorder features, but again at a high computational cost as ILP’s have, in the worst-case, exponential run-time with respect to the sentence length. Studies that have abandoned exact inference have focused on sampling (Nakagawa, 2007), belief propagation (Smith and Eisner, 2008), Lagrangian relaxation (Koo et al., 2010; Martins et al., 2011), and more recently structured prediction cascades (Weiss and Taskar, 2010; Rush and Petrov, 2012). However, these approximations themselves are often computationally expensive, requiring multiple decoding/sampling stages in order to produce an output. All the methods above, both exact and approximate, require specialized algorithms for every new feature that is beyond the scope of the previous factorization. In our method, the same parsing algorithm can be utilized (Eisner’s + cube pru"
D12-1030,P08-1108,1,0.937438,"Missing"
D12-1030,N07-1051,0,0.0301882,"a following discriminative re-ranking model. Hence, our formulation is more akin to the one pass decoding algorithm of Chiang (2007) for integrated decoding with a language model in machine translation. This also distinguishes it from previous work on dependency parse re-ranking (Hall, 2007) as we are not re-ranking/re-scoring the output of a base model but using a single decoding algorithm and learned model at training and testing. This work is largely orthogonal to other attempts to speed up chart parsing algorithms. This includes work on coarse-to-fine parsing (Charniak and Johnson, 2005; Petrov and Klein, 2007; Rush and Petrov, 2012), chart-cell closing and pruning (Roark and Hollingshead, 2008; Roark and Hollingshead, 329 2009), and dynamic beam-width prediction (Bodenstab et al., 2011). Of particular note, Rush and Petrov (2012) report run-times far better than our cube pruning system. At the heart of their system is a linear time vine-parsing stage that prunes most of the search space before higher-order parsing. This effectively makes their final system linear time in practice as the higher order models have far fewer parts to consider. One could easily use the same first-pass pruner in our cub"
D12-1030,W06-1616,0,0.222765,"d Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010). Both solutions allow for exact inference with higher-order features, but typically at a high cost in terms of efficiency. Furthermore, specialized algorithms are required that deeply exploit the structural properties of the given model. Upgrading a parser to score new types of higher-order dependencies thus requires significant changes to the underlying decoding algorithm. This is in stark contrast to transition-based systems, which simply require the definition of new feature extractors. In this paper, we abandon exact search in graphbased parsin"
D12-1030,C08-1094,0,0.0239384,"n to the one pass decoding algorithm of Chiang (2007) for integrated decoding with a language model in machine translation. This also distinguishes it from previous work on dependency parse re-ranking (Hall, 2007) as we are not re-ranking/re-scoring the output of a base model but using a single decoding algorithm and learned model at training and testing. This work is largely orthogonal to other attempts to speed up chart parsing algorithms. This includes work on coarse-to-fine parsing (Charniak and Johnson, 2005; Petrov and Klein, 2007; Rush and Petrov, 2012), chart-cell closing and pruning (Roark and Hollingshead, 2008; Roark and Hollingshead, 329 2009), and dynamic beam-width prediction (Bodenstab et al., 2011). Of particular note, Rush and Petrov (2012) report run-times far better than our cube pruning system. At the heart of their system is a linear time vine-parsing stage that prunes most of the search space before higher-order parsing. This effectively makes their final system linear time in practice as the higher order models have far fewer parts to consider. One could easily use the same first-pass pruner in our cube-pruning framework. In our study we use cube pruning only for decoding and rely on in"
D12-1030,N09-1073,0,0.0582924,"Missing"
D12-1030,N12-1054,0,0.771249,".66 93.20 / 91.25 91.36 / 87.22 90.50 / 83.01 86.63 / 84.95 3rd -order exact (reimpl.) 92.96 / 94.07 / 91.29 / 87.26 / 86.49 / 93.36 / 91.66 / 90.32 / 86.77 / - this paper 93.08 / 88.23 94.00 / 88.08 91.35 / 88.42 87.48 / 84.05 86.54 / 82.15 93.24 / 91.45 91.69 / 87.70 91.44 / 84.58 86.87 / 85.19 89.05 / 84.74 90.14 / 85.89 90.46 / - 90.63 / 86.65 Table 2: UAS/LAS for experiments on non-English treebanks. Numbers in bold are the highest scoring system. Zhang and Nivre is a reimplementation of Zhang and Nivre (2011) with beams of size 64 and 256. Rush and Petrov are the UAS results reported in Rush and Petrov (2012). Nth -order exact are implementations of exact 1st-3rd order dependency parsing. † For reference, Zhang and Nivre (2011) report 86.0/84.4, which is previously the best result reported on this data set. ‡ It should be noted that Rush and Petrov (2012) do not jointly optimize labeled and unlabeled dependency structure, which we found to often help. This, plus extra features, accounts for the differences in UAS. bels in the Penn2Malt label set, which results in little non-structural ambiguity. In contrast, Stanfordstyle dependencies contain a much larger set of labels (50) with more fine-grained"
D12-1030,D08-1016,0,0.0429328,"nsively (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010), but at a high computational cost as increasing the order of a model typically results in an asymptotic increase in running time. ILP formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2010) also allow for exact inference with higherorder features, but again at a high computational cost as ILP’s have, in the worst-case, exponential run-time with respect to the sentence length. Studies that have abandoned exact inference have focused on sampling (Nakagawa, 2007), belief propagation (Smith and Eisner, 2008), Lagrangian relaxation (Koo et al., 2010; Martins et al., 2011), and more recently structured prediction cascades (Weiss and Taskar, 2010; Rush and Petrov, 2012). However, these approximations themselves are often computationally expensive, requiring multiple decoding/sampling stages in order to produce an output. All the methods above, both exact and approximate, require specialized algorithms for every new feature that is beyond the scope of the previous factorization. In our method, the same parsing algorithm can be utilized (Eisner’s + cube pruning) just with slight different feature sign"
D12-1030,D07-1099,0,0.0217787,"e and McDonald, 2008). Graph-based parsers typically tradeoff rich feature scope for exact (or near exact) decoding, whereas transition-based parsers make the opposite trade-off. Recent research on both parsing paradigms has attempted to address this. In the transition-based parsing literature, the focus has been on increasing the search space of the system at decoding time, as expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke"
D12-1030,D08-1059,0,0.442873,"h-based parsers typically tradeoff rich feature scope for exact (or near exact) decoding, whereas transition-based parsers make the opposite trade-off. Recent research on both parsing paradigms has attempted to address this. In the transition-based parsing literature, the focus has been on increasing the search space of the system at decoding time, as expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al.,"
D12-1030,P11-2033,0,0.739432,"ly tradeoff rich feature scope for exact (or near exact) decoding, whereas transition-based parsers make the opposite trade-off. Recent research on both parsing paradigms has attempted to address this. In the transition-based parsing literature, the focus has been on increasing the search space of the system at decoding time, as expanding the feature scope is often trivial and in most cases only leads to a constant-time increase in parser complexity. The most common approach is to use beam search (Duan et al., 2007; Johansson and Nugues, 2007; Titov and Henderson, 2007; Zhang and Clark, 2008; Zhang and Nivre, 2011), but more principled dynamic programming solutions have been proposed (Huang and Sagae, 2010). In all cases inference remains approximate, though a larger search space is explored. In the graph-based parsing literature, the main thrust of research has been on extending the Eisner chart-parsing algorithm (Eisner, 1996) to incorporate higher-order features (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). A similar line of research investigated the use of integer linear programming (ILP) formulations of parsing (Riedel and Clarke, 2006; Martins et al., 2009; Martins et al., 2"
D12-1030,D07-1096,1,\N,Missing
D13-1093,E12-1009,0,0.153822,"onald (2012). 3 Experiments We ran a number of experiments on the cubepruning dependency parser of Zhang and McDonald (2012), whose search space can be represented as a hypergraph in which the nodes are the complete and incomplete states and the hyperedges are the instantiations of the two parsing rules in the Eisner algorithm (Eisner, 1996). The feature templates we used are a superset of Zhang and McDonald (2012). These features include first-, second-, and third-order features and their labeled counterparts, as well as valency features. In addition, we also included a feature template from Bohnet and Kuhn (2012). This template examines the leftmost child and the rightmost child of a modifier simultaneously. All other highorder features of Zhang and McDonald (2012) only look at arcs on the same side of their head. We trained the parser with hamming-loss-augmented MIRA (Crammer et al., 2006), following Martins et al. (2010). Based on results on the English validation data, in all the experiments, we trained MIRA with 8 epochs and used a beam of size 6 per node. To speed up the parser, we used an unlabeled first-order model to prune unlikely dependency arcs at both training and testing time (Koo and Col"
D13-1093,W06-2920,0,0.0316564,"1.49 93.80 89.65 87.79 83.59 91.62 85.00 80.60 70.12 76.86 66.56 92.00 87.07 92.19 88.40 86.46 78.55 85.77 76.62 88.48 82.38 79.61 71.65 86.49 81.67 91.79 89.28 83.35 80.09 87.80 82.14 Best Published† UAS LAS 87.48 84.05 94.07 89.09 93.72 91.793.50 88.23 87.47 83.50 91.44 85.42 81.12 66.977.55 65.791.86 84.893.03 87.70 86.05 77.87 86.95 73.490.32 80.280.23 73.18 86.81 81.86 92.41 88.42 86.19 79.2- Table 2: Parsing Results for languages from CoNLL 2006/2007 shared tasks. When a language is in both years, we use the 2006 data set. The best results with † are the maximum in the following papers: Buchholz and Marsi (2006), Nivre et al. (2007), Zhang and McDonald (2012), Bohnet and Kuhn (2012), and Martins et al. (2013), For consistency, we scored the CoNLL 2007 best systems with the CoNLL 2006 evaluation script. ZN 2011 (reimpl.) is our reimplementation of Zhang and Nivre (2011), with a beam of 64. Results in bold are the best among ZN 2011 reimplementation and different update strategies from this paper. 3.3 CoNLL Results We also report parsing results for 17 languages from the CoNLL 2006/2007 shared-task (Buchholz and Marsi, 2006; Nivre et al., 2007). The parser in our experiments can only produce projective"
D13-1093,D08-1024,0,0.190379,"Missing"
D13-1093,J07-2003,0,0.0261665,"Missing"
D13-1093,P04-1015,0,0.10193,"this, they generalized the original update rule to select an output y ′ within the pruned search space that scores higher than yˆ, but is not necessarily the highest among all possibilities, which represents a true violation of the model on that training instance. This violation fixing perceptron thus relaxes the argmax function to accommodate inexact search and becomes provably convergent as a result. In the sequential cases where yˆ has a linear structure such as tagging and incremental parsing, the violation fixing perceptron boils down to finding and updating along a certain prefix of yˆ. Collins and Roark (2004) locate the earliest position in a ′ chain structure where yˆpref is worse than ypref by a margin large enough to cause yˆ to be dropped from the beam. Huang et al. (2012) locate the position where the violation is largest among all prefixes of yˆ, where size of a violation is defined as ′ w · f (x, ypref ) − w · f (x, yˆpref ). For hypergraphs, the notion of prefix must be generalized to subtrees. Figure 1 shows the packedforest representation of the union of gold subtrees and highest-scoring (Viterbi) subtrees at every gold node for an input. At each gold node, there are two incoming hypered"
D13-1093,W02-1001,0,0.0711707,"ollins and Roark, 2004; Daum´e and Marcu, 2005; Zhang and Clark, 2008; Huang et al., 2012). However, sequential search algorithms, and in particular left-to-right beam search (Collins and Roark, 2004; Zhang and Clark, 2008), squeeze inference into a very narrow space. To address this, Huang (2008) formulated constituency parsing as approximate bottom-up inference in order to compactly represent an exponential number of outputs while scoring features of arbitrary scope. This idea was adapted to graph-based Structured Perceptron for Inexact Hypergraph Search The structured perceptron algorithm (Collins, 2002) is a general learning algorithm. Given training instances (x, yˆ), the algorithm first solves the decoding problem y ′ = argmaxy∈Y(x) w · f (x, y) given the weight vector w for the high-dimensional feature representation f of the mapping (x, y), where y ′ is the prediction under the current model, yˆ is the gold output and Y(x) is the space of all valid outputs for input x. The perceptron update rule is simply: w′ = w + f (x, yˆ) − f (x, y ′ ). The convergence of original perceptron algorithm relies on the argmax function being exact so that the condition w · f (x, y ′ ) &gt; w · f (x, yˆ) (modu"
D13-1093,de-marneffe-etal-2006-generating,0,0.00528498,"Missing"
D13-1093,C96-1058,0,0.144046,"Missing"
D13-1093,N12-1015,1,0.222192,"ecific instances of inexact hypergraph search. Typically, the approximation is accomplished by cube-pruning throughout the hypergraph (Chiang, 2007). Unfortunately, as the scope of features at each node increases, the inexactness of search and its negative impact on learning can potentially be exacerbated. Unlike sequential search, the impact on learning of approximate hypergraph search – as well as methods to mitigate any ill effects – has not been studied. Motivated by this, we develop online learning algorithms for inexact hypergraph search by generalizing the violation-fixing percepron of Huang et al. (2012). We empirically validate the benefit of this approach within the cube-pruning dependency parser of Zhang and McDonald (2012). Online learning algorithms like the perceptron are widely used for structured prediction tasks. For sequential search problems, like left-to-right tagging and parsing, beam search has been successfully combined with perceptron variants that accommodate search errors (Collins and Roark, 2004; Huang et al., 2012). However, perceptron training with inexact search is less studied for bottom-up parsing and, more generally, inference over hypergraphs. In this paper, we gener"
D13-1093,P08-1067,1,0.864538,"prediction problems generally deal with exponentially many outputs, often making exact search infeasible. For sequential search problems, such as tagging and incremental parsing, beam search coupled with perceptron algorithms that account for potential search errors have been shown to be a powerful combination (Collins and Roark, 2004; Daum´e and Marcu, 2005; Zhang and Clark, 2008; Huang et al., 2012). However, sequential search algorithms, and in particular left-to-right beam search (Collins and Roark, 2004; Zhang and Clark, 2008), squeeze inference into a very narrow space. To address this, Huang (2008) formulated constituency parsing as approximate bottom-up inference in order to compactly represent an exponential number of outputs while scoring features of arbitrary scope. This idea was adapted to graph-based Structured Perceptron for Inexact Hypergraph Search The structured perceptron algorithm (Collins, 2002) is a general learning algorithm. Given training instances (x, yˆ), the algorithm first solves the decoding problem y ′ = argmaxy∈Y(x) w · f (x, y) given the weight vector w for the high-dimensional feature representation f of the mapping (x, y), where y ′ is the prediction under the"
D13-1093,P10-1001,0,0.158246,"Missing"
D13-1093,C12-2077,0,0.171635,"Missing"
D13-1093,D10-1004,0,0.150365,"Missing"
D13-1093,P13-2109,0,0.225319,"ported by the fourth-order unlabeled dependency parser of Ma and Zhao (2012), although we did not utilize fourth-order features. The LAS score on Penn-YM is on par with the best reported by Bohnet and Kuhn (2012). On Penn-S, there are not many existing results to compare with, due to the tradition of reporting results on Penn-YM in the past. Nevertheless, our result is higher than the second best by a large margin. Our Chinese parsing scores are the highest reported results. 1 http://stp.lingfil.uu.se//∼nivre/research/Penn2Malt.html The data was prepared by Andr´e F. T. Martins as was done in Martins et al. (2013). 2 Parser Zhang and Nivre (2011) Zhang and Nivre (reimpl.) (beam=64) Zhang and Nivre (reimpl.) (beam=128) Koo and Collins (2010) Zhang and McDonald (2012) Rush and Petrov (2012) Martins et al. (2013) Qian and Liu (2013) Bohnet and Kuhn (2012) Ma and Zhao (2012) cube-pruning w/ skip w/ s-max w/ p-max UAS 92.993.00 92.94 93.04 93.06 93.07 93.17 93.39 93.493.21 93.50 93.44 Penn-YM LAS Toks/Sec † 680 91.891.98 800 91.91 400 91.86 220 740 180 † 120 92.38 92.07 300 92.41 300 92.33 300 UAS 92.96 93.11 92.792.82 92.92 93.59 93.64 Penn-S LAS Toks/Sec 90.74 500 90.84 250 4460 600 90.35 200 91.17 200 91"
D13-1093,Q13-1004,0,0.0970471,"Missing"
D13-1093,N12-1054,0,0.429959,"Missing"
D13-1093,D08-1059,0,0.0931437,"Missing"
D13-1093,D12-1030,1,0.913328,"the hypergraph (Chiang, 2007). Unfortunately, as the scope of features at each node increases, the inexactness of search and its negative impact on learning can potentially be exacerbated. Unlike sequential search, the impact on learning of approximate hypergraph search – as well as methods to mitigate any ill effects – has not been studied. Motivated by this, we develop online learning algorithms for inexact hypergraph search by generalizing the violation-fixing percepron of Huang et al. (2012). We empirically validate the benefit of this approach within the cube-pruning dependency parser of Zhang and McDonald (2012). Online learning algorithms like the perceptron are widely used for structured prediction tasks. For sequential search problems, like left-to-right tagging and parsing, beam search has been successfully combined with perceptron variants that accommodate search errors (Collins and Roark, 2004; Huang et al., 2012). However, perceptron training with inexact search is less studied for bottom-up parsing and, more generally, inference over hypergraphs. In this paper, we generalize the violation-fixing perceptron of Huang et al. (2012) to hypergraphs and apply it to the cube-pruning parser of Zhang"
D13-1093,P11-2033,0,0.103421,"eled dependency parser of Ma and Zhao (2012), although we did not utilize fourth-order features. The LAS score on Penn-YM is on par with the best reported by Bohnet and Kuhn (2012). On Penn-S, there are not many existing results to compare with, due to the tradition of reporting results on Penn-YM in the past. Nevertheless, our result is higher than the second best by a large margin. Our Chinese parsing scores are the highest reported results. 1 http://stp.lingfil.uu.se//∼nivre/research/Penn2Malt.html The data was prepared by Andr´e F. T. Martins as was done in Martins et al. (2013). 2 Parser Zhang and Nivre (2011) Zhang and Nivre (reimpl.) (beam=64) Zhang and Nivre (reimpl.) (beam=128) Koo and Collins (2010) Zhang and McDonald (2012) Rush and Petrov (2012) Martins et al. (2013) Qian and Liu (2013) Bohnet and Kuhn (2012) Ma and Zhao (2012) cube-pruning w/ skip w/ s-max w/ p-max UAS 92.993.00 92.94 93.04 93.06 93.07 93.17 93.39 93.493.21 93.50 93.44 Penn-YM LAS Toks/Sec † 680 91.891.98 800 91.91 400 91.86 220 740 180 † 120 92.38 92.07 300 92.41 300 92.33 300 UAS 92.96 93.11 92.792.82 92.92 93.59 93.64 Penn-S LAS Toks/Sec 90.74 500 90.84 250 4460 600 90.35 200 91.17 200 91.28 200 UAS 86.085.93 86.05 86.87"
D13-1093,D07-1096,1,\N,Missing
D17-1309,P16-1039,0,0.0399199,"Missing"
D17-1309,D14-1082,0,0.251491,"ults at or near the state-of-the-art on a variety of natural language processing tasks, with an order of magnitude speedup over an LSTM-based approach. We begin by introducing the network model structure and the character-based representations we use throughout all tasks (§2). The four tasks that we address are: language identification (LangID), part-of-speech (POS) tagging, word segmentation, and preordering for translation. In order to use feed-forward networks for structured prediction tasks, we use transition systems (Titov and Henderson, 2007, 2010) with feature embeddings as proposed by Chen and Manning (2014), and introduce two novel transition systems for the last two tasks. We focus on budgeted models and ablate four techniques (one on each task) for improving accuracy for a given memory budget: 1. Quantization: Using more dimensions and less precision (Lang-ID: §3.1). 2. Word clusters: Reducing the network size to allow for word clusters and derived features (POS tagging: §3.2). 3. Selected features: Adding explicit feature conjunctions (segmentation: §3.3). 4. Pipelines: Introducing another task in a pipeline and allocating parameters to the auxiliary task instead (preordering: §3.4). We achie"
D17-1309,P05-1066,0,0.0288102,"Missing"
D17-1309,W08-0804,0,0.125718,"s applications of this network structure used (pretrained) word embeddings to represent words (Chen and Manning, 2014; Weiss et al., 2015). However, for word embeddings to be effective, they usually need to cover large vocabularies (100,000+) and dimensions (50+). Inspired by the success of character-based representations (Ling et al., 2015), we use features defined over character n-grams instead of relying on word embeddings, and learn their embeddings from scratch. We use a distinct feature group g for each ngram length N , and control the size Vg directly by applying random feature mixing (Ganchev and Dredze, 2008). That is, we define the feature value v for an n-gram string x as v = H(x) mod Vg , where H is a well-behaved hash function. Typical values for Vg are in the 100-5000 range, which is far smaller than the exponential number of unique raw n-grams. A consequence of these small feature vocabularies is that we can also use small feature embeddings, typically Dg =16. ⨁ Memory needs P are dominated by the embedding matrix sizes ( g Vg Dg , where Vg and Dg are the vocabulary sizes and dimensions respectively for each feature group g), while runtime is strongly influenced by the hidden layer dimension"
D17-1309,N16-1155,0,0.124158,"translation models have been able to attain impressive accuracies, with models that use hundreds of millions (Bahdanau et al., 2014; Wu et al., 2016) or billions (Shazeer et al., 2017) of parameters. These models, however, may not be feasible in all computational settings. In particular, models running on mobile devices are often constrained in terms of memory and computation. Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997) have achieved good results with small memory footprints by using character-based input representations: e.g., the part-of-speech tagging models of Gillick et al. (2016) have only roughly 900,000 parameters. Latency, however, can still be an issue with LSTMs, due to the large number of matrix multiplications they require (eight per LSTM cell): Kim and Rush (2016) report speeds of only 8.8 words/second when running a two-layer LSTM translation system on an Android phone. Feed-forward neural networks have the potential to be much faster. In this paper, we show that small feed-forward networks can achieve results at or near the state-of-the-art on a variety of natural language processing tasks, with an order of magnitude speedup over an LSTM-based approach. We b"
D17-1309,N15-1105,0,0.123253,"Missing"
D17-1309,D16-1139,0,0.0228959,"s. These models, however, may not be feasible in all computational settings. In particular, models running on mobile devices are often constrained in terms of memory and computation. Long Short-Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997) have achieved good results with small memory footprints by using character-based input representations: e.g., the part-of-speech tagging models of Gillick et al. (2016) have only roughly 900,000 parameters. Latency, however, can still be an issue with LSTMs, due to the large number of matrix multiplications they require (eight per LSTM cell): Kim and Rush (2016) report speeds of only 8.8 words/second when running a two-layer LSTM translation system on an Android phone. Feed-forward neural networks have the potential to be much faster. In this paper, we show that small feed-forward networks can achieve results at or near the state-of-the-art on a variety of natural language processing tasks, with an order of magnitude speedup over an LSTM-based approach. We begin by introducing the network model structure and the character-based representations we use throughout all tasks (§2). The four tasks that we address are: language identification (LangID), part"
D17-1309,P08-1068,0,0.150947,"Missing"
D17-1309,D15-1176,0,0.100219,"Missing"
D17-1309,W16-4801,0,0.0687806,"Missing"
D17-1309,W16-5805,0,0.0415107,"rk given an embedding vector h0 . This cost is dominated by the matrix multiplications to compute (unscaled) activation unit values, hence our metric excludes the non-linearities and softmax normal2880 Model Baldwin and Lui (2010): NN Baldwin and Lui (2010): NP Small FF, 6 dim Small FF, 16 dim Small FF, 16 dim, quantized ization, but still accounts for the final layer logits. To ground this metric, we also provide indicative absolute speeds for each task, as measured on a modern workstation CPU (3.50GHz Intel Xeon E5-1650 v3). 3.1 Language Identification Recent shared tasks on code-switching (Molina et al., 2016) and dialects (Malmasi et al., 2016) have generated renewed interest in language identification. We restrict our focus to single language identification across diverse languages, and compare to the work of Baldwin and Lui (2010) on predicting the language of Wikipedia text in 66 languages. For this task, we obtain the input h0 by separately averaging the embeddings for each ngram length (N = [1, 4]), as summation did not produce good results. Table 1 shows that we outperform the lowmemory nearest-prototype model of Baldwin and Lui (2010). Their nearest neighbor model is the most accurate but i"
D17-1309,P15-1021,0,0.048914,"al. (2016)-combo’ in Table 4). However, such embeddings could easily lead to a model size explosion and thus are not considered in this work. The results in Table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size. Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second. 3.4 Preordering Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015). We propose a novel transition system for this task (Table 5), so that we can repeatedly apply a small network to produce these permutations. Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans. The system is sound for permutations: any derivation will end with all of the input words in a permuted order, and complete: all permutations are reachable (use SHIFT and SWAP operations to perform a bubble sort, then APPEND n − 1 times to form a single span). For training and evaluation, we use the English-Japa"
D17-1309,P09-1040,0,0.0229685,"fective than on larger character embeddings, in terms of both accuracy and model size. Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second. 3.4 Preordering Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015). We propose a novel transition system for this task (Table 5), so that we can repeatedly apply a small network to produce these permutations. Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans. The system is sound for permutations: any derivation will end with all of the input words in a permuted order, and complete: all permutations are reachable (use SHIFT and SWAP operations to perform a bubble sort, then APPEND n − 1 times to form a single span). For training and evaluation, we use the English-Japanese manual word alignments from Nakagawa (2015). 2882 Model Nakagawa (2015) Small FF Small FF + POS tags Small FF + Tagger input fts. FRS 81.6 75.2 81.3 76.6 Size 0.5MB 1.3MB 3.7MB References Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua"
D17-1309,P14-1028,0,0.025929,"pplementary material. Accuracy 95.01 95.95 94.24 94.16 95.18 Size − − 846KB 3.2MB 2.0MB Table 4: Segmentation results. Explicit bigrams are useful. Transition Precondition A PPEND ([σ|i|j], [β]) → ([σ|[ij]], [β]) S HIFT ([σ], [i|β]) → ([σ|i], [β]) S WAP ([σ|i|j], [β]) → [σ|j], [i|β]); i &lt; j Table 5: Preordering Transition system. Initially all words are part of singleton spans on the buffer: ([], [[w1 ][w2 ]...[wn ]]). In the final state the buffer is empty and the stack contains a single span. shown that explicitly using character bigram features leads to better accuracy (Zhang et al., 2016; Pei et al., 2014). Zhang et al. (2016) suggests that embedding manually specified feature conjunctions further improves accuracy (‘Zhang et al. (2016)-combo’ in Table 4). However, such embeddings could easily lead to a model size explosion and thus are not considered in this work. The results in Table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size. Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second. 3.4 Preordering Preordering source-side words into the target-side"
D17-1309,W11-2102,0,0.0300358,"Missing"
D17-1309,D07-1099,0,0.13293,"In this paper, we show that small feed-forward networks can achieve results at or near the state-of-the-art on a variety of natural language processing tasks, with an order of magnitude speedup over an LSTM-based approach. We begin by introducing the network model structure and the character-based representations we use throughout all tasks (§2). The four tasks that we address are: language identification (LangID), part-of-speech (POS) tagging, word segmentation, and preordering for translation. In order to use feed-forward networks for structured prediction tasks, we use transition systems (Titov and Henderson, 2007, 2010) with feature embeddings as proposed by Chen and Manning (2014), and introduce two novel transition systems for the last two tasks. We focus on budgeted models and ablate four techniques (one on each task) for improving accuracy for a given memory budget: 1. Quantization: Using more dimensions and less precision (Lang-ID: §3.1). 2. Word clusters: Reducing the network size to allow for word clusters and derived features (POS tagging: §3.2). 3. Selected features: Adding explicit feature conjunctions (segmentation: §3.3). 4. Pipelines: Introducing another task in a pipeline and allocating"
D17-1309,P10-1040,0,0.188532,"Missing"
D17-1309,P15-1032,1,0.867077,"rams ), with one embedding matrix Eg ∈ RVg ×Dg per group. h1 h0 ⨁ Quantization A commonly used strategy for compressing neural networks is quantization, using less precision to store parameters (Han et al., 2015). We compress the embedding weights (the vast majority of the parameters for these shallow models) by storing scale factors for each embedding (details in the supplementary material). In §3.1, we contrast devoting model size to higher ⨁ Hashed Character n-grams Previous applications of this network structure used (pretrained) word embeddings to represent words (Chen and Manning, 2014; Weiss et al., 2015). However, for word embeddings to be effective, they usually need to cover large vocabularies (100,000+) and dimensions (50+). Inspired by the success of character-based representations (Ling et al., 2015), we use features defined over character n-grams instead of relying on word embeddings, and learn their embeddings from scratch. We use a distinct feature group g for each ngram length N , and control the size Vg directly by applying random feature mixing (Ganchev and Dredze, 2008). That is, we define the feature value v for an n-gram string x as v = H(x) mod Vg , where H is a well-behaved ha"
D17-1309,C04-1073,0,0.0181722,"ctions further improves accuracy (‘Zhang et al. (2016)-combo’ in Table 4). However, such embeddings could easily lead to a model size explosion and thus are not considered in this work. The results in Table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size. Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second. 3.4 Preordering Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015). We propose a novel transition system for this task (Table 5), so that we can repeatedly apply a small network to produce these permutations. Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans. The system is sound for permutations: any derivation will end with all of the input words in a permuted order, and complete: all permutations are reachable (use SHIFT and SWAP operations to perform a bubble sort, then APPEND n − 1 times to form a single span). For training a"
D17-1309,P17-1078,0,0.0172196,"emory perspective, one multilingual BTS model will take less space than separate FF models. However, from a runtime perspective, a pipeline of our models doing language identification, word segmentation, and then POS tagging would still be faster than a single instance of the deep LSTM BTS model, by about 12x in our FLOPs estimate.4 3.3 Segmentation Word segmentation is critical for processing Asian languages where words are not explicitly separated by spaces. Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015). We use a structured model based on the transition system in Table 3, and similar to the one proposed by Zhang and Clark (2007). We conduct the segmentation experiments on the Chinese Treebank 6.0 with the recommended data splits. No external resources or pretrained embeddings are used. Hashing was detrimental to quality in our preliminary experiments, hence we do not use it for this task. To learn an embedding for unknown characters, we cast characters occurring only once in the training set to a special symbol. Selected Features Because we are not using hashing here, we"
D17-1309,P16-1040,0,0.0148556,"s detailed in the supplementary material. Accuracy 95.01 95.95 94.24 94.16 95.18 Size − − 846KB 3.2MB 2.0MB Table 4: Segmentation results. Explicit bigrams are useful. Transition Precondition A PPEND ([σ|i|j], [β]) → ([σ|[ij]], [β]) S HIFT ([σ], [i|β]) → ([σ|i], [β]) S WAP ([σ|i|j], [β]) → [σ|j], [i|β]); i &lt; j Table 5: Preordering Transition system. Initially all words are part of singleton spans on the buffer: ([], [[w1 ][w2 ]...[wn ]]). In the final state the buffer is empty and the stack contains a single span. shown that explicitly using character bigram features leads to better accuracy (Zhang et al., 2016; Pei et al., 2014). Zhang et al. (2016) suggests that embedding manually specified feature conjunctions further improves accuracy (‘Zhang et al. (2016)-combo’ in Table 4). However, such embeddings could easily lead to a model size explosion and thus are not considered in this work. The results in Table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size. Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second. 3.4 Preordering Preordering source-side words in"
D17-1309,P07-1106,0,0.0152845,"our models doing language identification, word segmentation, and then POS tagging would still be faster than a single instance of the deep LSTM BTS model, by about 12x in our FLOPs estimate.4 3.3 Segmentation Word segmentation is critical for processing Asian languages where words are not explicitly separated by spaces. Recently, neural networks have significantly improved segmentation accuracy (Zhang et al., 2016; Cai and Zhao, 2016; Liu et al., 2016; Yang et al., 2017; Kong et al., 2015). We use a structured model based on the transition system in Table 3, and similar to the one proposed by Zhang and Clark (2007). We conduct the segmentation experiments on the Chinese Treebank 6.0 with the recommended data splits. No external resources or pretrained embeddings are used. Hashing was detrimental to quality in our preliminary experiments, hence we do not use it for this task. To learn an embedding for unknown characters, we cast characters occurring only once in the training set to a special symbol. Selected Features Because we are not using hashing here, we need to be careful about the size of the input vocabulary. The neural network with its non-linearity is in theory able to learn bigrams by conjoinin"
D17-1309,N12-1052,1,\N,Missing
D17-1309,W14-3907,0,\N,Missing
D17-1309,W07-2218,0,\N,Missing
D18-1211,P18-1128,0,0.0166753,"using the TREC ad-hoc retrieval evaluation script10 focusing on MAP, Precision@20 and nDCG@20 (Manning et al., 2008). We trained each model five times with different random seeds and report the mean and standard deviation for each metric on test data; in each run, the model selected had the highest MAP on the development data. We also report results for an oracle, which re-ranks the N documents returned by BM 25 placing all human-annotated relevant documents at the top. To test for statistical significance between two systems, we employed twotailed stratified shuffling (Smucker et al., 2007; Dror et al., 2018) using the model with the highest development MAP over the five runs per method. 4.2 BioASQ Experiments Our first experiment used the dataset of the document ranking task of BIOASQ (Tsatsaronis et al., 2015), years 1–5.11 It contains 2,251 English biomedical questions, each formulated by a biomedical expert, who searched (via PubMed12 ) for, and annotated relevant documents. Not all relevant documents were necessarily annotated, but the data includes additional expert relevance judgments made during the official evaluation.13 The document collection consists of approx. 28M ‘articles’ (titles a"
D18-1211,D17-1110,0,0.174069,"Missing"
D18-1211,N16-1030,0,0.00546428,"the terms, here their IDFs, to upper layers) applied before scoring the q-terms. By contrast, in DRMM (Fig. 1) term-gating is applied after q-term scoring, and operates on [e(qi ); idf(qi )]. 3.2 Context-sensitive Term Encodings In their original incarnations, DRMM and PACRR use pre-trained word embeddings that are insensitive to the context of a particular query or document where a term occurs. This contrasts with the plethora of systems that use context-sensitive word encodings (for each particular occurrence of a word) in virtually all NLP tasks (Bahdanau et al., 2014; Plank et al., 2016; Lample et al., 2016). In general, this is achieved via RNNs, e.g., LSTMs (Gers et al., 2000), or CNNs (Bai et al., 2018). In the IR literature, context-sensitivity is typically viewed through two lenses: term proximity (B¨uttcher et al., 2006) and term dependency (Metzler and Croft, 2005). The former assumes that the context around a term match is also relevant, whereas the latter aims to capture when multiple terms (e.g., an n-gram) must be matched together. An advantage of neural network architectures like RNN s and CNN s is that they can capture both. In the models below (§§3.3–3.4), an encoder produces the co"
D18-1211,W17-2328,0,0.0279019,"that the interaction-based DRMM outperforms previous representation-based methods. On the other hand, interaction-based models are less efficient, since one cannot index a document representation independently of the query. This is less important, though, when relevance ranking methods rerank the top documents returned by a conventional IR engine, which is the scenario we consider here. One set of our experiments ranks biomedical texts. Several methods have been proposed for the BIOASQ challenge (Tsatsaronis et al., 2015), mostly based on traditional IR techniques. The most related work is of Mohan et al. (2017), who use a deep learning architecture. Unlike our work, they focus on user click data as a supervised signal, and they use context-insensitive representations of document-query interactions. The other dataset we experiment with, TREC ROBUST 2004 (Voorhees, 2005), has been used extensively to evaluate traditional and deep learning IR methods. Document relevance ranking is also related to other NLP tasks. Passage scoring for question answering (Surdeanu et al., 2008) ranks passages by their relevance to the question; several deep networks have been proposed, e.g., Tan et al. (2015). Short-text"
D18-1211,P08-1082,0,0.0298003,"proposed for the BIOASQ challenge (Tsatsaronis et al., 2015), mostly based on traditional IR techniques. The most related work is of Mohan et al. (2017), who use a deep learning architecture. Unlike our work, they focus on user click data as a supervised signal, and they use context-insensitive representations of document-query interactions. The other dataset we experiment with, TREC ROBUST 2004 (Voorhees, 2005), has been used extensively to evaluate traditional and deep learning IR methods. Document relevance ranking is also related to other NLP tasks. Passage scoring for question answering (Surdeanu et al., 2008) ranks passages by their relevance to the question; several deep networks have been proposed, e.g., Tan et al. (2015). Short-text matching/ranking is also related and has seen recent deep learning solutions (Lu and Li, 2013; Hu et al., 2014; Severyn and Moschitti, 2015). In document relevance ranking, though, documents are typically much longer than queries, which makes methods from other tasks that consider pairs of short texts not directly applicable. Our starting point is DRMM, to which we add richer representations inspired by PACRR. Hence, we first discuss DRMM and PACRR further. 2.1 DRMM"
D18-1211,N18-1202,0,0.0170916,"t, whereas the latter aims to capture when multiple terms (e.g., an n-gram) must be matched together. An advantage of neural network architectures like RNN s and CNN s is that they can capture both. In the models below (§§3.3–3.4), an encoder produces the context-sensitive encoding of each qterm or d-term from the pre-trained embeddings. To compute this we use a standard BILSTM encoding scheme and set the context-sentence encoding as the concatenation of the last layer’s hidden states of the forward and backward LSTMs at each position. As is common for CNNs and even recent RNN term encodings (Peters et al., 2018), we use the original term embedding e(ti ) as a residual and combine it with the BILSTM encodings. → − ← − Specifically, if h (ti ) and h (ti ) are the last layer’s hidden states of the left-to-right and right-to-left LSTM s for term ti , respectively, then we set the context-sensitive term encoding as: → − ← − c(ti ) = [ h (ti ) + e(ti ); h (ti ) + e(ti )] (1) Since we are adding the original term embedding to each LSTM hidden state, we require the dimensionality of the hidden layers to be equal to that of the original embedding. Other methods were tried, including passing all representation"
D18-1211,P16-2067,0,0.0169478,"on information about the terms, here their IDFs, to upper layers) applied before scoring the q-terms. By contrast, in DRMM (Fig. 1) term-gating is applied after q-term scoring, and operates on [e(qi ); idf(qi )]. 3.2 Context-sensitive Term Encodings In their original incarnations, DRMM and PACRR use pre-trained word embeddings that are insensitive to the context of a particular query or document where a term occurs. This contrasts with the plethora of systems that use context-sensitive word encodings (for each particular occurrence of a word) in virtually all NLP tasks (Bahdanau et al., 2014; Plank et al., 2016; Lample et al., 2016). In general, this is achieved via RNNs, e.g., LSTMs (Gers et al., 2000), or CNNs (Bai et al., 2018). In the IR literature, context-sensitivity is typically viewed through two lenses: term proximity (B¨uttcher et al., 2006) and term dependency (Metzler and Croft, 2005). The former assumes that the context around a term match is also relevant, whereas the latter aims to capture when multiple terms (e.g., an n-gram) must be matched together. An advantage of neural network architectures like RNN s and CNN s is that they can capture both. In the models below (§§3.3–3.4), an e"
E06-1011,P05-1012,1,0.361592,"example of a projective (or nested) tree representation, in which all edges can be drawn in the plane with none crossing. Sometimes a non-projective representations are preferred, as in the sentence in Figure 2.1 In particular, for freer-word order languages, non-projectivity is a common phenomenon since the relative positional constraints on dependents is much less rigid. The dependency structures in Figures 1 and 2 satisfy the tree constraint: they are weakly connected graphs with a unique root node, and each non-root node has a exactly one parent. Though trees are 1 Examples are drawn from McDonald et al. (2005c). more common, some formalisms allow for words to modify multiple parents (Hudson, 1984). Recently, McDonald et al. (2005c) have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree (MST) in a graph yields efficient algorithms for both projective and non-projective trees. When combined with a discriminative online learning algorithm and a rich feature set, these models provide state-of-the-art performance across multiple languages. However, the parsing algorithms require that the score of a dependency tree factors as a sum of the scores of its ed"
E06-1011,H05-1066,1,0.625225,"Missing"
E06-1011,A00-2018,0,0.242316,"projective and non-projective trees. When combined with a discriminative online learning algorithm and a rich feature set, these models provide state-of-the-art performance across multiple languages. However, the parsing algorithms require that the score of a dependency tree factors as a sum of the scores of its edges. This first-order factorization is very restrictive since it only allows for features to be defined over single attachment decisions. Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy (Yamada and Matsumoto, 2003; Charniak, 2000). In this paper we extend the MST parsing framework to incorporate higher-order feature representations of bounded-size connected subgraphs. We also present an algorithm for acyclic dependency graphs, that is, dependency graphs in which a word may depend on multiple heads. In both cases parsing is in general intractable and we provide novel approximate algorithms to make these cases tractable. We evaluate these algorithms within an online learning framework, which has been shown to be robust with respect approximate inference, and describe experiments displaying that these new models lead to s"
E06-1011,H05-1011,0,0.0128157,"we have found that a single margin constraint per example leads to much faster training with a negligible degradation in performance. Furthermore, this formulation relates learning directly to inference, which is important, since we want the model to set weights relative to the errors made by an approximate inference algorithm. This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs Collins (2002). Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment (Moore, 2005), sequence analysis (Daum´e and Marcu, 2005; McDonald et al., 2005a) and phrase-structure parsing (Collins and Roark, 2004). This robustness to approximations comes from the fact that the online framework sets weights with respect to inference. In other words, the learning method sees common errors due to 85 Training data: T = {(xt , yt )}Tt=1 1. w (0) = 0; v = 0; i = 0 1st-order-projective 2nd-order-projective 2. for n : 1..N 3. 4. for t : 1..T ‚ ‚ ‚ ‚ min ‚w(i+1) − w(i) ‚ s.t. s(xt , yt ; w (i+1) ) 0 (i) where y = arg maxy 0 s(xt , y ; w ) 5. 6. Complete 36.7 42.1 Table 1: Dependency parsing"
E06-1011,P05-1013,0,0.490079,"ective MST parsing is NP-hard, as shown in appendix A. To circumvent this, we designed an approximate algorithm based on the exact O(n3 ) second-order projective Eisner algorithm. The approximation works by first finding the highest scoring projective parse. It then rearranges edges in the tree, one at a time, as long as such rearrangements increase the overall score and do not violate the tree constraint. We can easily motivate this approximation by observing that even in non-projective languages like Czech and Danish, most trees are primarily projective with just a few non-projective edges (Nivre and Nilsson, 2005). Thus, by starting with the highest scoring projective tree, we are typically only a small number of transformations away from the highest scoring non-projective tree. The algorithm is shown in Figure 4. The expression y[i → j] denotes the dependency graph identical to y except that xi ’s parent is xi instead 83 FIRST-ORDER h1 h1 h3 h3 ⇒ h1 r r+1 h3 h1 (A) SECOND-ORDER h3 (B) h1 h1 h2 h2 h3 h1 h2 h2 h3 ⇒ h1 (A) h2 h2 r r+1 h3 h3 ⇒ h1 h2 h2 (B) h3 h1 h3 (C) Figure 3: A O(n3 ) extension of the Eisner algorithm to second-order dependency parsing. This figure shows how h1 creates a dependency to"
E06-1011,P04-1015,0,0.184333,"adation in performance. Furthermore, this formulation relates learning directly to inference, which is important, since we want the model to set weights relative to the errors made by an approximate inference algorithm. This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs Collins (2002). Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment (Moore, 2005), sequence analysis (Daum´e and Marcu, 2005; McDonald et al., 2005a) and phrase-structure parsing (Collins and Roark, 2004). This robustness to approximations comes from the fact that the online framework sets weights with respect to inference. In other words, the learning method sees common errors due to 85 Training data: T = {(xt , yt )}Tt=1 1. w (0) = 0; v = 0; i = 0 1st-order-projective 2nd-order-projective 2. for n : 1..N 3. 4. for t : 1..T ‚ ‚ ‚ ‚ min ‚w(i+1) − w(i) ‚ s.t. s(xt , yt ; w (i+1) ) 0 (i) where y = arg maxy 0 s(xt , y ; w ) 5. 6. Complete 36.7 42.1 Table 1: Dependency parsing results for English. −s(xt , y 0 ; w(i+1) ) ≥ L(yt , y 0 ) 0 English Accuracy 90.7 91.5 1st-order-projective 2nd-order-pro"
E06-1011,P99-1065,0,0.0624493,"Missing"
E06-1011,W02-1001,0,0.771205,"ed learning, we assume a training set T = {(xt , yt )}Tt=1 , consisting of pairs of a sentence xt and its correct dependency representation yt . The algorithm is an extension of the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) to learning with structured outputs, in the present case dependency structures. Figure 6 gives pseudo-code for the algorithm. An online learning algorithm considers a single training instance for each update to the weight vector w. We use the common method of setting the final weight vector as the average of the weight vectors after each iteration (Collins, 2002), which has been shown to alleviate overfitting. On each iteration, the algorithm considers a single training instance. We parse this instance to obtain a predicted dependency graph, and find the smallest-norm update to the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph, which is the number of words with incorrect parents in the predicted tree (McDonald et al., 2005b). Note that we only impose margin constraints between the single highest-scoring graph and the correct"
E06-1011,C96-1058,0,0.974845,"lish and the best accuracy we know of for Czech and Danish. 2 Maximum Spanning Tree Parsing Dependency-tree parsing as the search for the maximum spanning tree (MST) in a graph was 81 root John saw a dog yesterday which was a Yorkshire Terrier Figure 2: An example non-projective dependency structure. root hit John ball with the bat the root0 John1 hit2 the3 ball4 with5 the6 bat7 Figure 1: An example dependency structure. proposed by McDonald et al. (2005c). This formulation leads to efficient parsing algorithms for both projective and non-projective dependency trees with the Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) respectively. The formulation works by defining the score of a dependency tree to be the sum of edge scores, X s(x, y) = s(i, j) (i,j)∈y where x = x1 · · · xn is an input sentence and y a dependency tree for x. We can view y as a set of tree edges and write (i, j) ∈ y to indicate an edge in y from word xi to word xj . Consider the example from Figure 1, where the subscripts index the nodes of the tree. The score of this tree would then be, s(0, 2) + s(2, 1) + s(2, 4) + s(2, 5) + s(4, 3) + s(5, 7) + s(7, 6) We call this first"
E06-1011,W05-1505,0,0.0916131,"Missing"
E06-1011,H05-1124,1,0.22089,"example of a projective (or nested) tree representation, in which all edges can be drawn in the plane with none crossing. Sometimes a non-projective representations are preferred, as in the sentence in Figure 2.1 In particular, for freer-word order languages, non-projectivity is a common phenomenon since the relative positional constraints on dependents is much less rigid. The dependency structures in Figures 1 and 2 satisfy the tree constraint: they are weakly connected graphs with a unique root node, and each non-root node has a exactly one parent. Though trees are 1 Examples are drawn from McDonald et al. (2005c). more common, some formalisms allow for words to modify multiple parents (Hudson, 1984). Recently, McDonald et al. (2005c) have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree (MST) in a graph yields efficient algorithms for both projective and non-projective trees. When combined with a discriminative online learning algorithm and a rich feature set, these models provide state-of-the-art performance across multiple languages. However, the parsing algorithms require that the score of a dependency tree factors as a sum of the scores of its ed"
E06-1011,W96-0213,0,0.292784,"eature space. We evaluate dependencies on per word accuracy, which is the percentage of words in the sentence with the correct parent in the tree, and on complete dependency analysis. In our evaluation we exclude punctuation for English and include it for Czech and Danish, which is the standard. 5.1 English Results To create data sets for English, we used the Yamada and Matsumoto (2003) head rules to extract dependency trees from the WSJ, setting sections 2-21 as training, section 22 for development and section 23 for evaluation. The models rely on part-of-speech tags as input and we used the Ratnaparkhi (1996) tagger to provide these for the development and evaluation set. These data sets are exclusively projective so we only compare the projective parsers using the exact projective parsing algorithms. The purpose of these experiments is to gauge the overall benefit from including second-order features with exact parsing algorithms, which can be attained in the projective setting. Results are shown in Table 1. We can see that there is clearly an advantage in introducing second-order features. In particular, the complete tree metric is improved considerably. 5.2 Czech Results ith where xi -pos is th"
E06-1011,W04-3201,0,0.0050333,"the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph, which is the number of words with incorrect parents in the predicted tree (McDonald et al., 2005b). Note that we only impose margin constraints between the single highest-scoring graph and the correct graph relative to the current weight setting. Past work on tree-structured outputs has used constraints for the k-best scoring tree (McDonald et al., 2005b) or even all possible trees by using factored representations (Taskar et al., 2004; McDonald et al., 2005c). However, we have found that a single margin constraint per example leads to much faster training with a negligible degradation in performance. Furthermore, this formulation relates learning directly to inference, which is important, since we want the model to set weights relative to the errors made by an approximate inference algorithm. This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs Collins (2002). Online learning algorithms have been shown to be robust even with approximate rather than exact inference i"
E06-1011,W03-3023,0,0.91412,"fficient algorithms for both projective and non-projective trees. When combined with a discriminative online learning algorithm and a rich feature set, these models provide state-of-the-art performance across multiple languages. However, the parsing algorithms require that the score of a dependency tree factors as a sum of the scores of its edges. This first-order factorization is very restrictive since it only allows for features to be defined over single attachment decisions. Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy (Yamada and Matsumoto, 2003; Charniak, 2000). In this paper we extend the MST parsing framework to incorporate higher-order feature representations of bounded-size connected subgraphs. We also present an algorithm for acyclic dependency graphs, that is, dependency graphs in which a word may depend on multiple heads. In both cases parsing is in general intractable and we provide novel approximate algorithms to make these cases tractable. We evaluate these algorithms within an online learning framework, which has been shown to be robust with respect approximate inference, and describe experiments displaying that these new"
E06-1038,A00-2018,0,0.0211328,"r, they do so without a larger picture of the function of each word in the sentence. For instance, dropping verbs is not that uncommon - a relative clause for instance may be dropped during compression. However, dropping the main verb in the sentence is uncommon, since that verb and its arguments typically encode most of the information being conveyed. An obvious solution to this problem is to include features over a deep syntactic analysis of the sentence. To do this we parse every sentence twice, once with a dependency parser (McDonald et al., 2005b) and once with a phrase-structure parser (Charniak, 2000). These parsers have been trained out-of-domain on the Penn WSJ Treebank and as a result contain noise. However, we are merely going to use them as an additional source of features. We call this soft syntactic evidence since the deep trees are not used as a strict goldstandard in our model but just as more evidence for 300 S root0 VP saw2 PP on4 Mary1 Ralph3 NP after6 Tuesday5 lunch7 NP PP NP NP NNP VBD NNP IN NNP IN NN Mary1 saw2 Ralph3 on4 Tuesday5 after6 lunch7 Figure 2: An example dependency tree from the McDonald et al. (2005b) parser and phrase structure tree from the Charniak (2000) par"
E06-1038,W02-1001,0,0.0339184,"Tt=1 1. w0 = 0; v = 0; i = 0 2. for n : 1..N 3. 4. for t : 1..T ‚ ‚ ‚ ‚ min ‚w(i+1) − w(i) ‚ s.t. s(xt , yt ) − s(xt , y 0 ) ≥ L(yt , y 0 ) where y 0 ∈ bestk (x; w(i) ) 5. 6. v = v + w(i+1) i =i+1 7. w = v/(N ∗ T ) Figure 3: MIRA learning algorithm as presented by McDonald et al. (2005b). best performance, though varying k did not have a major impact overall. Furthermore we found that after only 3-5 training epochs performance on the development data was maximized. The final weight vector is the average of all weight vectors throughout training. Averaging has been shown to reduce overfitting (Collins, 2002) as well as reliance on the order of the examples during training. We found it to be particularly important for this data set. 4 Experiments We use the same experimental methodology as Knight and Marcu (2000). We provide every compression to four judges and ask them to evaluate each one for grammaticality and importance on a scale from 1 to 5. For each of the 32 sentences in our test set we ask the judges to evaluate three systems: human annotated, the decision tree model of Knight and Marcu (2000) and our system. The judges were told all three compressions were automatically generated and the"
E06-1038,H05-1124,1,0.811144,"ssion that ends at word xj , for all j < i, then C[i] must also be the highest scoring compression ending at word xi since it represents the max combination over all high scoring shorter compressions plus the score of extending the compression to the current word. Thus, since xn is by definition in every compressed version of x (see above), then it must be the case that C[n] stores the score of the best compression. This table can be filled in O(n 2 ). This algorithm is really an extension of Viterbi to the case when scores factor over dynamic substrings of the text (Sarawagi and Cohen, 2004; McDonald et al., 2005a). As such, we can use back-pointers to reconstruct the highest scoring compression as well as k-best decoding algorithms. This decoding algorithm is dynamic with respect to compression rate. That is, the algorithm will return the highest scoring compression regardless of length. This may seem problematic since longer compressions might contribute more to the score (since they contain more bigrams) and thus be preferred. However, in Section 3.2 we define a rich feature set, including features on words dropped from the compression that will help disfavor compressions that drop very few words s"
E06-1038,P05-1012,1,0.844199,"ssion that ends at word xj , for all j < i, then C[i] must also be the highest scoring compression ending at word xi since it represents the max combination over all high scoring shorter compressions plus the score of extending the compression to the current word. Thus, since xn is by definition in every compressed version of x (see above), then it must be the case that C[n] stores the score of the best compression. This table can be filled in O(n 2 ). This algorithm is really an extension of Viterbi to the case when scores factor over dynamic substrings of the text (Sarawagi and Cohen, 2004; McDonald et al., 2005a). As such, we can use back-pointers to reconstruct the highest scoring compression as well as k-best decoding algorithms. This decoding algorithm is dynamic with respect to compression rate. That is, the algorithm will return the highest scoring compression regardless of length. This may seem problematic since longer compressions might contribute more to the score (since they contain more bigrams) and thus be preferred. However, in Section 3.2 we define a rich feature set, including features on words dropped from the compression that will help disfavor compressions that drop very few words s"
E06-1038,N03-1026,0,0.184859,"s an important problem in text summarization. Most summarization systems are evaluated on the amount of relevant information retained as well as their compression rate. Thus, returning highly compressed, yet informative, sentences allows summarization systems to return larger sets of sentences and increase the overall amount of information extracted. We focus on the particular instantiation of sentence compression when the goal is to produce the compressed version solely by removing words or phrases from the original, which is the most common setting in the literature (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005). In this framework, the goal is to find the shortest substring of the original sentence that conveys the most important aspects of the meaning. We will work in a supervised learning setting and |T | assume as input a training set T =(xt , yt )t=1 of original sentences xt and their compressions yt . We use the Ziff-Davis corpus, which is a set of 1087 pairs of sentence/compression pairs. Furthermore, we use the same 32 testing examples from Knight and Marcu (2000) and the rest for training, except that we hold out 20 sentences for the purpose of development. A handf"
E06-1038,N03-1028,0,0.015517,"use these syntactic constraints as soft evidence in our model. That is, they represent just another layer of evidence to be considered during training when setting parameters. Thus, if the parses have too much noise, the learning algorithm can lower the weight of the parse features since they are unlikely to be useful discriminators on the training data. This differs from the models of Knight and Marcu (2000), which treat the noisy parses as gold-standard when 301 calculating probability estimates. An important distinction we should make is the notion of supported versus unsupported features (Sha and Pereira, 2003). Supported features are those that are on for the gold standard compressions in the training. For instance, the bigram feature “NN&VB” will be supported since there is most likely a compression that contains a adjacent noun and verb. However, the feature “JJ&VB” will not be supported since an adjacent adjective and verb most likely will not be observed in any valid compression. Our model includes all features, including those that are unsupported. The advantage of this is that the model can learn negative weights for features that are indicative of bad compressions. This is not difficult to d"
E06-1038,P05-1036,0,0.482617,"in text summarization. Most summarization systems are evaluated on the amount of relevant information retained as well as their compression rate. Thus, returning highly compressed, yet informative, sentences allows summarization systems to return larger sets of sentences and increase the overall amount of information extracted. We focus on the particular instantiation of sentence compression when the goal is to produce the compressed version solely by removing words or phrases from the original, which is the most common setting in the literature (Knight and Marcu, 2000; Riezler et al., 2003; Turner and Charniak, 2005). In this framework, the goal is to find the shortest substring of the original sentence that conveys the most important aspects of the meaning. We will work in a supervised learning setting and |T | assume as input a training set T =(xt , yt )t=1 of original sentences xt and their compressions yt . We use the Ziff-Davis corpus, which is a set of 1087 pairs of sentence/compression pairs. Furthermore, we use the same 32 testing examples from Knight and Marcu (2000) and the rest for training, except that we hold out 20 sentences for the purpose of development. A handful of sentences occur twice"
E09-1059,C04-1200,0,0.0750622,"the sentiment polarity function that maps a lexical item t, e.g., word or short phrase, to a real-valued score, LEX - SENT(t) http://www.nist.gov/tac/ 515 ∈ [−1, 1] The LEX - SENT function maps items with positive polarity to higher values and items with negative polarity to lower values. To build this function we constructed large sentiment lexicons by seeding a semantic word graph induced from WordNet with positive and negative examples and then propagating this score out across the graph with a decaying confidence. This method is common among sentiment analysis systems (Hu and Liu, 2004a; Kim and Hovy, 2004; Blair-Goldensohn et al., 2008). In particular, we use the lexicons that were created and evaluated by Blair-Goldensohn et al. (2008). Next we define sentiment intensity, X |LEX - SENT(t)| INTENSITY (s) = Another key input many sentiment summarizers assume is a list of salient entity aspects, which are specific properties of an entity that people tend to rate when expressing their opinion. For example, aspects of a digital camera could include picture quality, battery life, size, color, value, etc. Finding such aspects is a challenging research problem that has been addressed in a number of w"
E09-1059,P08-1031,0,0.009056,"nd evaluated by Blair-Goldensohn et al. (2008). Next we define sentiment intensity, X |LEX - SENT(t)| INTENSITY (s) = Another key input many sentiment summarizers assume is a list of salient entity aspects, which are specific properties of an entity that people tend to rate when expressing their opinion. For example, aspects of a digital camera could include picture quality, battery life, size, color, value, etc. Finding such aspects is a challenging research problem that has been addressed in a number of ways (Hu and Liu, 2004b; Gamon et al., 2005; Carenini et al., 2005; Zhuang et al., 2006; Branavan et al., 2008; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008b; Titov and McDonald, 2008a). We denote the set of aspects for an entity as A and each aspect as a ∈ A. Furthermore, we assume that given A it is possible to determine whether some sentence s ∈ D mentions an aspect in A. For our experiments we use a hybrid supervisedunsupervised method for finding aspects as described and evaluated in Blair-Goldensohn et al. (2008). Having defined what an aspect is, we next define a summary diversity function over aspects, X DIVERSITY (S) = COVERAGE (a) t∈s which simply measures the magnitude of sentime"
E09-1059,N03-1020,0,0.478909,"Missing"
E09-1059,W08-1106,0,0.0423387,"rize answers to opinion questions about entities mentioned in blogs. Our work most closely resembles the evaluations in Carenini et al. (2006, 2008). Carenini et al. (2006) had raters evaluate extractive and abstractive summarization systems. Mirroring our results, they show that both extractive and abstractive summarization outperform a baseline, but that overall, humans have no preference between the two. Again mirroring our results, their analysis indicates that even though there is no overall difference, there are situations where one system generally outperforms the other. In particular, Carenini and Cheung (2008) show that an entity’s controversiality, e.g., mid-range star rating, is correlated with which summary has highest value. The study presented here differs from Carenini et al. in many respects: First, our evaluation is over different extractive summarization systems in an attempt to understand what model properties are correlated with human preference irrespective of presentation; Secondly, our evaluation is on a larger scale including hundreds of judgments by hundreds of raters; Finally, we take a major next step and show that it is possible to automatically learn significantly improved model"
E09-1059,E06-1039,0,0.811885,"versity New York, NY Sasha Blair-Goldensohn Google, Inc. New York, NY Ryan McDonald Google, Inc. New York, NY klerman@cs.columbia.edu sasha@google.com ryanmcd@google.com Abstract pieces that can be evaluated easily and automatically (Blair-Goldensohn et al., 2008). While this technique produces meaningful evaluations of the selected components, other components remain untested, and the overall effectiveness of the entire system as a whole remains unknown. When systems are evaluated end-to-end by human judges, the studies are often small, consisting of only a handful of judges and data points (Carenini et al., 2006). Furthermore, automated summarization metrics like ROUGE (Lin and Hovy, 2003) are non-trivial to adapt to this domain as they require human curated outputs. We present the results of a large-scale, end-toend human evaluation of three sentiment summarization models applied to user reviews of consumer products. The evaluation shows that there is no significant difference in rater preference between any of the sentiment summarizers, but that raters do prefer sentiment summarizers over nonsentiment baselines. This indicates that even simple sentiment summarizers provide users utility. An analysis"
E09-1059,H05-1043,0,0.134924,"izer. 1 Introduction The growth of the Internet as a commerce medium, and particularly the Web 2.0 phenomenon of user-generated content, have resulted in the proliferation of massive numbers of product, service and merchant reviews. While this means that users have plenty of information on which to base their purchasing decisions, in practice this is often too much information for a user to absorb. To alleviate this information overload, research on systems that automatically aggregate and summarize opinions have been gaining interest (Hu and Liu, 2004a; Hu and Liu, 2004b; Gamon et al., 2005; Popescu and Etzioni, 2005; Carenini et al., 2005; Carenini et al., 2006; Zhuang et al., 2006; Blair-Goldensohn et al., 2008). Evaluating these systems has been a challenge, however, due to the number of human judgments required to draw meaningful conclusions. Often systems are evaluated piecemeal, selecting 1 http://duc.nist.gov/ Proceedings of the 12th Conference of the European Chapter of the ACL, pages 514–522, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 514 iPod Shuffle: 4/5 stars (Jindal and Liu, 2006; Stoyanov and Cardie, 2008) In this study, we look at an extractive"
E09-1059,H05-1045,0,0.0267915,"approximate hill climbing technique. First we randomly initialize the summary S to length ∼K. Then we greedily insert/delete/swap sentences in and out of the summary to maximize L(S) while maintaining the bound on length. We run this procedure until no operation leads to a higher scoring summary. In all our experiments convergence was quick, even when employing random restarts. Alternate formulations of sentiment summarization are possible, including aspect-based summarization (Hu and Liu, 2004a), abstractive summarization (Carenini et al., 2006) or related tasks such as opinion attribution (Choi et al., 2005). We choose a purely extractive formulation as it makes it easier to develop baselines and allows raters to compare summaries with a simple, consistent presentation format. Sentiment Summarization A standard setting for sentiment summarization assumes a set of documents D = {d1 , . . . , dm } that contain opinions about some entity of interest. The goal of the system is to generate a summary S of that entity that is representative of the average opinion and speaks to its important aspects. An example summary is given in figure 1. For simplicity we assume that all opinions in D are about the en"
E09-1059,C08-1103,0,0.0128818,"; Hu and Liu, 2004b; Gamon et al., 2005; Popescu and Etzioni, 2005; Carenini et al., 2005; Carenini et al., 2006; Zhuang et al., 2006; Blair-Goldensohn et al., 2008). Evaluating these systems has been a challenge, however, due to the number of human judgments required to draw meaningful conclusions. Often systems are evaluated piecemeal, selecting 1 http://duc.nist.gov/ Proceedings of the 12th Conference of the European Chapter of the ACL, pages 514–522, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 514 iPod Shuffle: 4/5 stars (Jindal and Liu, 2006; Stoyanov and Cardie, 2008) In this study, we look at an extractive summarization setting where S is built by extracting representative bits of text from the set D, subject to pre-specified length constraints. Specifically, assume each document di is segmented into candidate text excerpts. For ease of discussion we will assume all excerpts are sentences, but in practice they can be phrases or multi-sentence groups. Viewed this way, D is a set of candidate sentences for our summary, D = {s1 , . . . , sn }, and summarization becomes the following optimization: “In final analysis the iPod Shuffle is a decent player that of"
E09-1059,C04-1057,0,0.013267,"it is not difficult to estimate these models even from small sets of data. Having constructed this model, one logical approach to summarization would be to select sentences for the summary that have highest probability under the model trained on D. We found, L(S) = α · INTENSITY(S) − β · MISMATCH(S) +γ · DIVERSITY(S) This score function rewards summaries for being highly subjective (INTENSITY), reflecting the overall product rating (MISMATCH), and covering a variety of product aspects (DIVERSITY). The coefficients were set by inspection. This system has its roots in event-based summarization (Filatova and Hatzivassiloglou, 2004) for the news domain. In that work an optimization problem was developed that attempted to maximize summary informativeness while covering as many (weighted) sub-events as possible. 2.2.3 Sentiment-Aspect Match (SAM) Because the SMAC model only utilizes an entity’s overall sentiment when calculating MISMATCH, it 517 3 however, that this produced very redundant summaries – if one aspect is particularly prevalent in a product’s reviews, this approach will select all sentences about that aspect, and discuss nothing else. To combat this we developed a technique that scores the summary as a whole,"
E09-1059,P08-1036,1,0.399422,"e define sentiment intensity, X |LEX - SENT(t)| INTENSITY (s) = Another key input many sentiment summarizers assume is a list of salient entity aspects, which are specific properties of an entity that people tend to rate when expressing their opinion. For example, aspects of a digital camera could include picture quality, battery life, size, color, value, etc. Finding such aspects is a challenging research problem that has been addressed in a number of ways (Hu and Liu, 2004b; Gamon et al., 2005; Carenini et al., 2005; Zhuang et al., 2006; Branavan et al., 2008; Blair-Goldensohn et al., 2008; Titov and McDonald, 2008b; Titov and McDonald, 2008a). We denote the set of aspects for an entity as A and each aspect as a ∈ A. Furthermore, we assume that given A it is possible to determine whether some sentence s ∈ D mentions an aspect in A. For our experiments we use a hybrid supervisedunsupervised method for finding aspects as described and evaluated in Blair-Goldensohn et al. (2008). Having defined what an aspect is, we next define a summary diversity function over aspects, X DIVERSITY (S) = COVERAGE (a) t∈s which simply measures the magnitude of sentiment in a sentence. INTENSITY can be viewed as a measure of"
E09-1059,W00-0405,0,0.341385,"tiveness irrespective of polarity. A central function in all our systems is a sentences normalized sentiment, P t∈s LEX - SENT (t) SENT(s) = α + INTENSITY(s) a∈A where COVERAGE(a) ∈ R is a function that weights how well the aspect is covered in the summary and is proportional to the importance of the aspect as some aspects are more important to cover than others, e.g., “picture quality” versus “strap” for digital cameras. The diversity function rewards summaries that cover many important aspects and plays the redundancy reducing role that is common in most extractive summarization frameworks (Goldstein et al., 2000). This function measures the (signed) ratio of lexical sentiment to intensity in a sentence. Sentences that only contain lexical items of the same polarity will have high absolute normalized sentiment, whereas sentences with mixed polarity items or no polarity items will have a normalized sentiment near zero. We include the constant α in the denominator so that SENT gives higher absolute scores to sentences containing many strong sentiment items of the same polarity over sentences with a small number of weak items of the same polarity. Most sentiment summarizers assume that as input, a system"
E09-1059,H05-2017,0,\N,Missing
H05-1066,P99-1065,0,0.0348557,"Missing"
H05-1066,J93-2004,0,0.0564691,"ta and Sorensen, 2004), machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), and lexical resource augmentation (Snow et al., 2004). The primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse while still encoding much of the predicate-argument information needed in applications. In English, projective trees are sufficient to analyze most sentence types. In fact, the largest source of English dependency trees is automatically generated from the Penn Treebank (Marcus et al., 1993) and is by convention exclusively projective. However, there are certain examples in which a nonprojective tree is preferable. Consider the sentence John saw a dog yesterday which was a Yorkshire Terrier. Here the relative clause which was a Yorkshire Terrier and the object it modifies (the dog) are separated by an adverb. There is no way to draw the dependency tree for this sentence in the plane with no crossing edges, as illustrated in Figure 2. In languages with more flexible word order than English, such as German, Dutch and Czech, non-projective dependencies are more frequent. Rich inflec"
H05-1066,W02-1001,0,0.419449,"he highest score, s(x, y). The resulting online update (to be inserted in Figure 4, line 4) would then be: min w(i+1) − w(i) s.t. s(xt , yt ) − s(xt , y 0 ) ≥ L(yt , y 0 ) where y 0 = arg maxy0 s(xt , y 0 ) McDonald et al. (2005) used a similar update with k constraints for the k highest-scoring trees, and showed that small values of k are sufficient to achieve the best accuracy for these methods. However, here we stay with a single best tree because kbest extensions to the Chu-Liu-Edmonds algorithm are too inefficient (Hou, 1996). This model is related to the averaged perceptron algorithm of Collins (2002). In that algorithm, the single highest scoring tree (or structure) is used to update the weight vector. However, MIRA aggressively updates w to maximize the margin between the correct tree and the highest scoring tree, which has been shown to lead to increased accuracy. 3.2 Factored MIRA It is also possible to exploit the structure of the output space and factor the exponential number of margin constraints into a polynomial number of local constraints (Taskar et al., 2003; Taskar et al., 2004). For the directed maximum spanning tree problem, we can factor the output by edges to obtain the fol"
H05-1066,P05-1012,1,0.17838,".ms.mff.cuni.cz Abstract root We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n 3 ) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2 ) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al., 2003; McDonald et al., 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies. John hit the ball with the bat Figure 1: An example dependency tree. Dependency representations, which link words to their arguments, have a long history (Hudson, 1984). Figure 1 shows a dependency tree for the sentence John hit the ball with the bat. We restrict ourselves to dependency tree analyses, in which each word depends on exactly one parent, either another word or a dummy root symbol as shown in the figure. The tree in Figure 1 is projective, meaning that if we put the words in"
H05-1066,P05-1013,0,0.802738,"es, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser. They test this system on Czech and show improved accuracy relative to a projective parser. Our approach differs from those earlier efforts in searching optimally and efficiently the full space of non-projective trees. The main idea of our method is that dependency parsing can be formalized as the search for a maximum spanning tree in a directed graph. This formalization generalizes standard projective parsing models bas"
H05-1066,P04-1054,0,0.385345,"tree for the sentence John hit the ball with the bat. We restrict ourselves to dependency tree analyses, in which each word depends on exactly one parent, either another word or a dummy root symbol as shown in the figure. The tree in Figure 1 is projective, meaning that if we put the words in their linear order, preceded by the root, the edges can be drawn above the words without crossings, or, equivalently, a word and its descendants form a contiguous substring of the sentence. 1 Introduction Dependency parsing has seen a surge of interest lately for applications such as relation extraction (Culotta and Sorensen, 2004), machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), and lexical resource augmentation (Snow et al., 2004). The primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse while still encoding much of the predicate-argument information needed in applications. In English, projective trees are sufficient to analyze most sentence types. In fact, the largest source of English dependency trees is automatically generated from the Penn Treebank (Marcus et al., 1993) a"
H05-1066,P05-1067,0,0.184843,"at. We restrict ourselves to dependency tree analyses, in which each word depends on exactly one parent, either another word or a dummy root symbol as shown in the figure. The tree in Figure 1 is projective, meaning that if we put the words in their linear order, preceded by the root, the edges can be drawn above the words without crossings, or, equivalently, a word and its descendants form a contiguous substring of the sentence. 1 Introduction Dependency parsing has seen a surge of interest lately for applications such as relation extraction (Culotta and Sorensen, 2004), machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), and lexical resource augmentation (Snow et al., 2004). The primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse while still encoding much of the predicate-argument information needed in applications. In English, projective trees are sufficient to analyze most sentence types. In fact, the largest source of English dependency trees is automatically generated from the Penn Treebank (Marcus et al., 1993) and is by convention exclusively projective. H"
H05-1066,C96-1058,0,0.827257,"w vˇetˇsinou a dog nem´a yesterday ani z´ajem which a taky was na a Yorkshire to vˇetˇsinou Terrier nem´a pen´ıze He is mostly not even interested in the new things and in most cases, he has no money for it either. Figure 2: Non-projective dependency trees in English and Czech. grammatical relations, allowing non-projective dependencies that we need to represent and parse efficiently. A non-projective example from the Czech Prague Dependency Treebank (Hajiˇc et al., 2001) is also shown in Figure 2. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a pa"
H05-1066,C04-1010,0,0.0300593,"Yorkshire to vˇetˇsinou Terrier nem´a pen´ıze He is mostly not even interested in the new things and in most cases, he has no money for it either. Figure 2: Non-projective dependency trees in English and Czech. grammatical relations, allowing non-projective dependencies that we need to represent and parse efficiently. A non-projective example from the Czech Prague Dependency Treebank (Hajiˇc et al., 2001) is also shown in Figure 2. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-projective edges into dep"
H05-1066,W04-3201,0,0.00698393,"s algorithm are too inefficient (Hou, 1996). This model is related to the averaged perceptron algorithm of Collins (2002). In that algorithm, the single highest scoring tree (or structure) is used to update the weight vector. However, MIRA aggressively updates w to maximize the margin between the correct tree and the highest scoring tree, which has been shown to lead to increased accuracy. 3.2 Factored MIRA It is also possible to exploit the structure of the output space and factor the exponential number of margin constraints into a polynomial number of local constraints (Taskar et al., 2003; Taskar et al., 2004). For the directed maximum spanning tree problem, we can factor the output by edges to obtain the following constraints: min w(i+1) − w(i) s.t. s(l, j) − s(k, j) ≥ 1 ∀(l, j) ∈ yt , (k, j) ∈ / yt This states that the weight of the correct incoming edge to the word xj and the weight of all other incoming edges must be separated by a margin of 1. It is easy to show that when all these constraints are satisfied, the correct spanning tree and all incorrect spanning trees are separated by a score at least as large as the number of incorrect incoming edges. This is because the scores for all the corr"
H05-1066,W04-0307,0,0.0166533,"non-projective example from the Czech Prague Dependency Treebank (Hajiˇc et al., 2001) is also shown in Figure 2. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser. They test this system on Czech and show improved accuracy relative to a projective parser. Our approach differs from those earlier efforts in searching optimally and efficiently the full space of non-projective trees. The main idea of ou"
H05-1066,W03-3023,0,0.934387,"z´ajem which a taky was na a Yorkshire to vˇetˇsinou Terrier nem´a pen´ıze He is mostly not even interested in the new things and in most cases, he has no money for it either. Figure 2: Non-projective dependency trees in English and Czech. grammatical relations, allowing non-projective dependencies that we need to represent and parse efficiently. A non-projective example from the Czech Prague Dependency Treebank (Hajiˇc et al., 2001) is also shown in Figure 2. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-"
H05-1124,W96-0213,0,0.122148,"stems rely on such segmentations to accurately process the data. Depending on the application, segments may be tokens, phrases, or sentences. However, in this paper we primarily focus on segmenting sentences into tokens. The most common approach to text segmentation is to use finite-state sequence tagging models, in which each atomic text element (character or token) is labeled with a tag representing its role in a segmentation. Models of that form include hidden Markov models (Rabiner, 1989; Bikel et al., 1999) as well as discriminative tagging models based on maximum entropy classification (Ratnaparkhi, 1996; McCallum et al., 2000), conditional random fields (Lafferty et al., 2001; Sha and Pereira, 2003), and large-margin techniques (Kudo and Matsumoto, 2001; Taskar et al., 2003). Tagging models are the best previous methods for text segmentation. However, their purely sequential form limits their ability to naturally handle overlapping or noncontiguous segments. We present here an alternative view of segmentation as structured multilabel classification. In this view, a segmentation of a text is a set of segments, each of which is defined by the set of text positions that belong to the segment. T"
H05-1124,W02-1001,0,0.106215,"pproach can be seen as multilabel variant of the work of McDonald et al. (2004), which creates a set of constraints to separate the score of the single correct output from the k highest scoring outputs with an appropriate large margin. 4 Experiments We now describe a set of experiments on named entity and base NP segmentation. For these experiments, we set k = n, where n is the length of the sentence. This represents a reasonable upper bound on the number of entities or chunks in a sentence and results in a time complexity of O(n3 T ). We compare our methods with both the averaged perceptron (Collins, 2002) and conditional random fields (Lafferty et al., 2001) using identical predicate sets. Though all systems use identical predicates, the actual features of the systems are different due to the fundamental differences between the multilabel classification and sequential tagging models. 4.1 Standard data sets Our first experiments are standard named entity and base NP data sets with no overlapping, embedded or non-contiguous segments. These experiments will show that, for simple segmentations, our model is competitive with sequential tagging models. For the named entity experiments we used the Co"
H05-1124,P05-1040,0,0.0113443,"abeling techniques using the BIO label set will encounter problems. Figure 2 shows two simple examples of segmentations involving overlapping, non-contiguous segments. In both cases, it is difficult to see how a sequential tagger could extract the segments correctly. It would be possible to grow the tag set to represent a bounded number of overlapping, noncontiguous segments by representing all possible combinations of segment membership over k overlapping segments, but this would require an arbitrary upper bound on k and would lead to models that generalize poorly and are expensive to train. Dickinson and Meurers (2005) point out that, as language processing begins to tackle problems in free-word order languages and discourse analysis, annotating and extracting non-contiguous segmentations of text will become increasingly important. Though we focus primarily on entity extraction and NP chunking in this paper, there is no reason why ideas presented here could not be extended to managing other non-contiguous phenomena. 3 Structured Multilabel Classification As outlined in Section 1, we represent segmentation as multilabel classification, assigning to each text the set of segments it contains. Figure 3 shows th"
H05-1124,N03-1028,1,0.921751,", segments may be tokens, phrases, or sentences. However, in this paper we primarily focus on segmenting sentences into tokens. The most common approach to text segmentation is to use finite-state sequence tagging models, in which each atomic text element (character or token) is labeled with a tag representing its role in a segmentation. Models of that form include hidden Markov models (Rabiner, 1989; Bikel et al., 1999) as well as discriminative tagging models based on maximum entropy classification (Ratnaparkhi, 1996; McCallum et al., 2000), conditional random fields (Lafferty et al., 2001; Sha and Pereira, 2003), and large-margin techniques (Kudo and Matsumoto, 2001; Taskar et al., 2003). Tagging models are the best previous methods for text segmentation. However, their purely sequential form limits their ability to naturally handle overlapping or noncontiguous segments. We present here an alternative view of segmentation as structured multilabel classification. In this view, a segmentation of a text is a set of segments, each of which is defined by the set of text positions that belong to the segment. Thus, a particular segment may not be a set of consecutive positions in the text, and segments may"
H05-1124,W03-0419,0,0.0313345,"Missing"
H05-1124,N01-1025,0,\N,Missing
J11-1007,W06-2922,0,0.137103,"state history. Inference is local, in that systems start in a ﬁxed initial state and greedily construct the graph by taking the highest scoring transitions at each state entered until a termination condition is met. We call such systems transition-based parsing models to reﬂect the fact that parameterization is over possible state transitions. Transition-based models have been promoted by the groups of Matsumoto (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003; Cheng, Asahara, and Matsumoto 2006), Nivre (Nivre, Hall, and Nilsson 2004; Nivre and Nilsson 2005; Nivre et al. 2006), and others (Attardi 2006; Attardi and Ciaramita 2007; Johansson and Nugues 2007; Duan, Zhao, and Xu 2007; Titov and Henderson 2007a, 2007b). It is important to note that there is no a priori reason why a graph-based parameterization should require global learning and inference, and a transition-based parameterization would necessitate local learning and greedy inference. Nevertheless, as observed by Buchholz and Marsi (2006), it is striking that recent work on data-driven dependency parsing has been dominated by global, exhaustive, graph-based models, on the one hand, and local, greedy, transition-based models, on th"
J11-1007,N07-1049,0,0.0121055,"Inference is local, in that systems start in a ﬁxed initial state and greedily construct the graph by taking the highest scoring transitions at each state entered until a termination condition is met. We call such systems transition-based parsing models to reﬂect the fact that parameterization is over possible state transitions. Transition-based models have been promoted by the groups of Matsumoto (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003; Cheng, Asahara, and Matsumoto 2006), Nivre (Nivre, Hall, and Nilsson 2004; Nivre and Nilsson 2005; Nivre et al. 2006), and others (Attardi 2006; Attardi and Ciaramita 2007; Johansson and Nugues 2007; Duan, Zhao, and Xu 2007; Titov and Henderson 2007a, 2007b). It is important to note that there is no a priori reason why a graph-based parameterization should require global learning and inference, and a transition-based parameterization would necessitate local learning and greedy inference. Nevertheless, as observed by Buchholz and Marsi (2006), it is striking that recent work on data-driven dependency parsing has been dominated by global, exhaustive, graph-based models, on the one hand, and local, greedy, transition-based models, on the other. Therefore, a carefu"
J11-1007,J96-1002,0,0.058999,"Missing"
J11-1007,W06-2920,0,0.797532,"in the computational linguistics community and have been successfully employed for many problems ranging from machine translation (Ding and Palmer 2004) to ontology construction (Snow, Jurafsky, and Ng 2005). A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, which arise due to long-distance dependencies or in languages where grammatical relations are often signaled by morphology instead of word order. This is undoubtedly one of the reasons for the emergence of dependency parsers for a wide range of languages (Buchholz and Marsi 2006; Nivre et al. 2007). Thus, the example in Figure 1 contains an instance of a discontinuous construction through the subgraph rooted at the word hearing. Speciﬁcally, the dependency arc from hearing to on spans the words is and scheduled, which are not nodes in this subgraph. An arc of this kind is said to be non-projective. In this article we focus on a common paradigm called data-driven dependency parsing, which encompasses parsing systems that learn to produce dependency graphs for sentences from a corpus of sentences annotated with dependency graphs. The advantage of such models is that th"
J11-1007,D07-1101,0,0.20937,"learns these parameters to globally score correct graphs above incorrect ones. Inference is also global, in that systems attempt to ﬁnd the highest scoring graph among the set of all graphs. We call such systems graph-based parsing models to reﬂect the fact that parameterization is over the graph. Graph-based models are mainly associated with the pioneering work of Eisner (Eisner 1996), as well as McDonald and colleagues (McDonald, Crammer, and Pereira 2005; McDonald et al. 2005; McDonald and Pereira 2006; McDonald, Lerman, and Pereira 2006) and others (Riedel, C ¸ akıcı, and Meza-Ruiz 2006; Carreras 2007; Koo et al. 2007; Nakagawa 2007; Smith and Smith 2007). The second category of parsing systems parameterizes models over transitions from one state to another in an abstract state-machine. Parameters in these models are typically learned using standard classiﬁcation techniques that learn to predict one transition from a set of permissible transitions given a state history. Inference is local, in that systems start in a ﬁxed initial state and greedily construct the graph by taking the highest scoring transitions at each state entered until a termination condition is met. We call such systems t"
J11-1007,A00-2018,0,0.0385922,"emble systems and voting schemes, which only perform the integration at parsing time. However, given that we are dealing with datadriven models, it should be possible to integrate at learning time, so that the two complementary models can learn from one another. In this article, we propose to do this by letting one model generate features for the other in a stacked learning framework. Feature-based integration in this sense has previously been exploited for dependency parsing by McDonald (2006), who trained an instance of MSTParser using features generated by the parsers of Collins (1999) and Charniak (2000), which improved unlabeled accuracy by 1.7 percentage points on data from the Penn Treebank. In other NLP domains, feature-based integration has been used by Taskar, Lacoste-Julien, and Klein (2005), who trained a discriminative word alignment model using features derived from the IBM models, by Florian et al. (2004), who trained classiﬁers on auxiliary data to guide named entity classiﬁers, and by others. Feature-based integration also has points in common with co-training, which has been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003), among others. The difference, o"
J11-1007,W06-2927,0,0.10435,"Missing"
J11-1007,W04-1513,0,0.0105139,"yanmcd@google.com. ∗∗ Department of Linguistics and Philology, Box 635, SE-75126 Uppsala, Sweden. E-mail: joakim.nivre@lingfil.uu.se. Submission received: 25 August 2009; revised submission received: 20 August 2010; accepted for publication: 7 October 2010. © 2011 Association for Computational Linguistics Computational Linguistics Volume 37, Number 1 Figure 1 Dependency graph for an English sentence. Syntactic dependency graphs have recently gained a wide interest in the computational linguistics community and have been successfully employed for many problems ranging from machine translation (Ding and Palmer 2004) to ontology construction (Snow, Jurafsky, and Ng 2005). A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions, which arise due to long-distance dependencies or in languages where grammatical relations are often signaled by morphology instead of word order. This is undoubtedly one of the reasons for the emergence of dependency parsers for a wide range of languages (Buchholz and Marsi 2006; Nivre et al. 2007). Thus, the example in Figure 1 contains an instance of a discontinuous construction through the subgraph root"
J11-1007,D07-1098,0,0.0223986,"Missing"
J11-1007,C96-1058,0,0.0656505,"t that there are currently two dominant approaches for data-driven 198 McDonald and Nivre Analyzing and Integrating Dependency Parsers dependency parsing. The ﬁrst category parameterizes models over dependency subgraphs and learns these parameters to globally score correct graphs above incorrect ones. Inference is also global, in that systems attempt to ﬁnd the highest scoring graph among the set of all graphs. We call such systems graph-based parsing models to reﬂect the fact that parameterization is over the graph. Graph-based models are mainly associated with the pioneering work of Eisner (Eisner 1996), as well as McDonald and colleagues (McDonald, Crammer, and Pereira 2005; McDonald et al. 2005; McDonald and Pereira 2006; McDonald, Lerman, and Pereira 2006) and others (Riedel, C ¸ akıcı, and Meza-Ruiz 2006; Carreras 2007; Koo et al. 2007; Nakagawa 2007; Smith and Smith 2007). The second category of parsing systems parameterizes models over transitions from one state to another in an abstract state-machine. Parameters in these models are typically learned using standard classiﬁcation techniques that learn to predict one transition from a set of permissible transitions given a state history."
J11-1007,N04-1001,0,0.0204938,"one model generate features for the other in a stacked learning framework. Feature-based integration in this sense has previously been exploited for dependency parsing by McDonald (2006), who trained an instance of MSTParser using features generated by the parsers of Collins (1999) and Charniak (2000), which improved unlabeled accuracy by 1.7 percentage points on data from the Penn Treebank. In other NLP domains, feature-based integration has been used by Taskar, Lacoste-Julien, and Klein (2005), who trained a discriminative word alignment model using features derived from the IBM models, by Florian et al. (2004), who trained classiﬁers on auxiliary data to guide named entity classiﬁers, and by others. Feature-based integration also has points in common with co-training, which has been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003), among others. The difference, of course, is that standard co-training is a weakly supervised method, where the ﬁrst-stage parser’s predictions replace, rather than complement, the gold standard annotation during training. Feature-based integration is also similar to parse reranking (Collins 2000), where one parser produces a set of candidate parse"
J11-1007,P07-1050,0,0.116397,"Dependency Parsers and transition-based features, that is, features over both sub-graphs and transitions. Huang and Sagae (2010) go even further and show how transition-based parsing can be tabularized to allow for dynamic programming, which in turn permits an exponentially larger search space. Martins et al. (2008) present a method for integrating graph-based and transition-based parsers based on stacking, which is similar to the approach taken in this work. Other studies have tried to overcome the weaknesses of parsing models by changing the underlying model structure directly. For example, Hall (2007), Riedel, C ¸ akıcı, and Meza-Ruiz (2006), Nakagawa (2007), Smith and Eisner (2008), and Martins, Smith, and Xing (2009) attempt to overcome local restrictions in feature scope for graphbased parsers through both approximations and exact solutions with integer linear programming. Our work differs from past studies in that we attempt to quantify exactly the types of errors these parsers make, tie them to their theoretical expectations, and show that integrating graph-based and transition-based parsers not only increases overall accuracy, but does so directly exploiting the strengths of each sys"
J11-1007,D07-1097,1,0.705355,"a system that combines multiple transition-based parsers with a single graph-based parser by weighting each potential dependency relation by the number of parsers that predicted it. A ﬁnal dependency graph is predicted by using spanning tree inference algorithms from the graph-based parsing literature (McDonald et al. 2005). Sagae and Lavie report improvements of up to 1.7 percentage points over the best single parser when combining three transition-based models and one graph-based model for unlabeled dependency parsing, evaluated on data from the Penn Treebank. The same technique was used by Hall et al. (2007) to combine six transition-based parsers in the best performing system in the CoNLL 2007 shared task. Zhang and Clark (2008) propose a parsing system that uses global learning coupled with beam search over a transition-based backbone incorporating both graph-based 200 McDonald and Nivre Analyzing and Integrating Dependency Parsers and transition-based features, that is, features over both sub-graphs and transitions. Huang and Sagae (2010) go even further and show how transition-based parsing can be tabularized to allow for dynamic programming, which in turn permits an exponentially larger sear"
J11-1007,W99-0623,0,0.0431494,"ich is commonly called “classiﬁer stacking.” This method is simple—requiring only the deﬁnition of new features—and robust by allowing a model to learn relative to the predictions of the other. More importantly, we rerun the error analysis and show that the integrated models do indeed take advantage of the complementary strengths of both the graph-based and transition-based parsing systems. Combining the strengths of different machine learning systems, and even parsing systems, is by no means new as there are a number of previous studies that have looked at combining phrase-structure parsers (Henderson and Brill 1999), dependency parsers ˇ (Zeman and Zabokrtsk y` 2005), or both (McDonald 2006). Of particular note is past work on combining graph-based and transition-based dependency parsers. Sagae and Lavie (2006) present a system that combines multiple transition-based parsers with a single graph-based parser by weighting each potential dependency relation by the number of parsers that predicted it. A ﬁnal dependency graph is predicted by using spanning tree inference algorithms from the graph-based parsing literature (McDonald et al. 2005). Sagae and Lavie report improvements of up to 1.7 percentage point"
J11-1007,P07-1120,0,0.0186921,"for all languages except Czech, German, Portuguese, and Slovene. Finally, given that the two base models had the best performance for these data sets at the CoNLL-X shared task, the guided models achieve a substantial improvement of the state of the art.12 Although there is no statistically signiﬁcant difference between the two base models, they are both outperformed by MaltMST (p < 0.0001), which in turn has signiﬁcantly lower accuracy than MSTMalt (p < 0.0005). An extension to the models described so far would be to iteratively integrate the two parsers in the spirit of pipeline iteration (Hollingshead and Roark 2007). For example, one could start with a Malt model, use it to train a guided MSTMalt model, then use that as the guide to train a MaltMSTMalt model, and so forth. We ran such experiments, but found that accuracy did not increase signiﬁcantly and in some cases decreased slightly. This was true regardless of which parser began the iterative process. In retrospect, this result is not surprising. Because the initial integration effectively incorporates knowledge from both parsing systems, there is little to be gained by adding additional parsers in the chain. 6.2 Error Analysis The experimental resu"
J11-1007,W05-1506,0,0.0150967,"her than complement, the gold standard annotation during training. Feature-based integration is also similar to parse reranking (Collins 2000), where one parser produces a set of candidate parses and a second-stage classiﬁer chooses the most likely one. However, feature-based integration is not explicitly constrained to any parse decisions that the ﬁrst-stage parser might make. Furthermore, as only the single most likely parse is used from the ﬁrststage model, it is signiﬁcantly more efﬁcient than reranking, which requires both computationally and conceptually more complex parsing algorithms (Huang and Chiang 2005). 5.1 Parser Stacking with Rich Features As explained in Section 2, both models essentially learn a scoring function s : X → R, where the domain X is different for the two models. For the graph-based model, X is the set of possible dependency arcs (i, j, l); for the transition-based model, X is the set of possible conﬁguration-transition pairs (c, t). But in both cases, the input is represented by a k-dimensional feature vector f : X → Rk . In a stacked parsing system we simply extend the feature vector for one model, called the base model, with a certain number of features generated by the ot"
J11-1007,P10-1110,0,0.0699308,"Missing"
J11-1007,D07-1123,0,0.0625986,"Missing"
J11-1007,P10-1001,0,0.0428804,"the right is the predicted dependency tree based on Equation (1). 203 Computational Linguistics Volume 37, Number 1 over anything larger than arcs (McDonald and Satta 2007). Thus, graph-based parsing systems cannot easily condition on any extended scope of the dependency graph beyond a single arc, which is their primary shortcoming relative to transition-based systems. McDonald, Crammer, and Pereira (2005) show that a rich feature set over the input space, including lexical and surface syntactic features of neighboring words, can partially alleviate this problem, and both Carreras (2007) and Koo et al. (2010) explore higher-order models for projective trees. Additionally, work has been done on approximate non-factored parsing systems (McDonald and Pereira 2006; Hall 2007; Nakagawa 2007; Smith and Eisner 2008) as well as exact solutions through integer linear programming (Riedel, C ¸ akıcı, and Meza-Ruiz 2006; Martins, Smith, and Xing 2009). The speciﬁc graph-based system studied in this work is that presented by McDonald, Lerman, and Pereira (2006), which uses pairwise arc scoring and approximate exhaustive search for unlabeled parsing. A separate arc label classiﬁer is then used to label each arc"
J11-1007,D07-1015,0,0.0355011,"arameters to globally score correct graphs above incorrect ones. Inference is also global, in that systems attempt to ﬁnd the highest scoring graph among the set of all graphs. We call such systems graph-based parsing models to reﬂect the fact that parameterization is over the graph. Graph-based models are mainly associated with the pioneering work of Eisner (Eisner 1996), as well as McDonald and colleagues (McDonald, Crammer, and Pereira 2005; McDonald et al. 2005; McDonald and Pereira 2006; McDonald, Lerman, and Pereira 2006) and others (Riedel, C ¸ akıcı, and Meza-Ruiz 2006; Carreras 2007; Koo et al. 2007; Nakagawa 2007; Smith and Smith 2007). The second category of parsing systems parameterizes models over transitions from one state to another in an abstract state-machine. Parameters in these models are typically learned using standard classiﬁcation techniques that learn to predict one transition from a set of permissible transitions given a state history. Inference is local, in that systems start in a ﬁxed initial state and greedily construct the graph by taking the highest scoring transitions at each state entered until a termination condition is met. We call such systems transition-based p"
J11-1007,W02-2016,0,0.026009,"e-machine. Parameters in these models are typically learned using standard classiﬁcation techniques that learn to predict one transition from a set of permissible transitions given a state history. Inference is local, in that systems start in a ﬁxed initial state and greedily construct the graph by taking the highest scoring transitions at each state entered until a termination condition is met. We call such systems transition-based parsing models to reﬂect the fact that parameterization is over possible state transitions. Transition-based models have been promoted by the groups of Matsumoto (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003; Cheng, Asahara, and Matsumoto 2006), Nivre (Nivre, Hall, and Nilsson 2004; Nivre and Nilsson 2005; Nivre et al. 2006), and others (Attardi 2006; Attardi and Ciaramita 2007; Johansson and Nugues 2007; Duan, Zhao, and Xu 2007; Titov and Henderson 2007a, 2007b). It is important to note that there is no a priori reason why a graph-based parameterization should require global learning and inference, and a transition-based parameterization would necessitate local learning and greedy inference. Nevertheless, as observed by Buchholz and Marsi (2006), it is striking that re"
J11-1007,J93-2004,0,0.0383994,"Missing"
J11-1007,D08-1017,0,0.0807279,"e six transition-based parsers in the best performing system in the CoNLL 2007 shared task. Zhang and Clark (2008) propose a parsing system that uses global learning coupled with beam search over a transition-based backbone incorporating both graph-based 200 McDonald and Nivre Analyzing and Integrating Dependency Parsers and transition-based features, that is, features over both sub-graphs and transitions. Huang and Sagae (2010) go even further and show how transition-based parsing can be tabularized to allow for dynamic programming, which in turn permits an exponentially larger search space. Martins et al. (2008) present a method for integrating graph-based and transition-based parsers based on stacking, which is similar to the approach taken in this work. Other studies have tried to overcome the weaknesses of parsing models by changing the underlying model structure directly. For example, Hall (2007), Riedel, C ¸ akıcı, and Meza-Ruiz (2006), Nakagawa (2007), Smith and Eisner (2008), and Martins, Smith, and Xing (2009) attempt to overcome local restrictions in feature scope for graphbased parsers through both approximations and exact solutions with integer linear programming. Our work differs from pas"
J11-1007,P09-1039,0,0.0517431,"Missing"
J11-1007,P90-1005,0,0.0773187,"er makes and how they relate to theoretical expectations. Using these observations, we present an integrated system based on a stacking learning framework and show that such a system can learn to overcome the shortcomings of each non-integrated system. 1. Introduction Syntactic dependency representations have a long history in descriptive and theoretical linguistics and many formal models have been advanced, most notably Word Grammar (Hudson 1984), Meaning-Text Theory (Mel’ˇcuk 1988), Functional Generative Description (Sgall, Hajiˇcov´a, and Panevov´a 1986), and Constraint Dependency Grammar (Maruyama 1990). Common to all theories is the notion of directed syntactic dependencies between the words of a sentence, an example of which is given in Figure 1 for the sentence A hearing is scheduled on the issue today, which has been extracted from the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993). A dependency graph of a sentence represents each word and its syntactic modiﬁers through labeled directed arcs, where each arc label comes from some ﬁnite set representing possible syntactic roles. Returning to our example in Figure 1, we can see multiple instances of labeled dependency relations s"
J11-1007,P05-1012,1,0.950348,"re Analyzing and Integrating Dependency Parsers dependency parsing. The ﬁrst category parameterizes models over dependency subgraphs and learns these parameters to globally score correct graphs above incorrect ones. Inference is also global, in that systems attempt to ﬁnd the highest scoring graph among the set of all graphs. We call such systems graph-based parsing models to reﬂect the fact that parameterization is over the graph. Graph-based models are mainly associated with the pioneering work of Eisner (Eisner 1996), as well as McDonald and colleagues (McDonald, Crammer, and Pereira 2005; McDonald et al. 2005; McDonald and Pereira 2006; McDonald, Lerman, and Pereira 2006) and others (Riedel, C ¸ akıcı, and Meza-Ruiz 2006; Carreras 2007; Koo et al. 2007; Nakagawa 2007; Smith and Smith 2007). The second category of parsing systems parameterizes models over transitions from one state to another in an abstract state-machine. Parameters in these models are typically learned using standard classiﬁcation techniques that learn to predict one transition from a set of permissible transitions given a state history. Inference is local, in that systems start in a ﬁxed initial state and greedily construct the g"
J11-1007,W06-2932,1,0.926364,"Missing"
J11-1007,D07-1013,1,0.937957,"Missing"
J11-1007,E06-1011,1,0.837671,"rating Dependency Parsers dependency parsing. The ﬁrst category parameterizes models over dependency subgraphs and learns these parameters to globally score correct graphs above incorrect ones. Inference is also global, in that systems attempt to ﬁnd the highest scoring graph among the set of all graphs. We call such systems graph-based parsing models to reﬂect the fact that parameterization is over the graph. Graph-based models are mainly associated with the pioneering work of Eisner (Eisner 1996), as well as McDonald and colleagues (McDonald, Crammer, and Pereira 2005; McDonald et al. 2005; McDonald and Pereira 2006; McDonald, Lerman, and Pereira 2006) and others (Riedel, C ¸ akıcı, and Meza-Ruiz 2006; Carreras 2007; Koo et al. 2007; Nakagawa 2007; Smith and Smith 2007). The second category of parsing systems parameterizes models over transitions from one state to another in an abstract state-machine. Parameters in these models are typically learned using standard classiﬁcation techniques that learn to predict one transition from a set of permissible transitions given a state history. Inference is local, in that systems start in a ﬁxed initial state and greedily construct the graph by taking the highest"
J11-1007,H05-1066,1,0.85104,"Missing"
J11-1007,W07-2216,1,0.519115,"to exist, but non-projective trees required approximate inference that used an exhaustive projective algorithm followed by transformations to the graph that incrementally introduce non-projectivity. In general, inference and learning for graph-based dependency parsing is NP-hard when the score is factored Figure 2 A graph-based parsing example. A dense graph Gx is shown on the left (arcs into the root are omitted) with corresponding arc scores. On the right is the predicted dependency tree based on Equation (1). 203 Computational Linguistics Volume 37, Number 1 over anything larger than arcs (McDonald and Satta 2007). Thus, graph-based parsing systems cannot easily condition on any extended scope of the dependency graph beyond a single arc, which is their primary shortcoming relative to transition-based systems. McDonald, Crammer, and Pereira (2005) show that a rich feature set over the input space, including lexical and surface syntactic features of neighboring words, can partially alleviate this problem, and both Carreras (2007) and Koo et al. (2010) explore higher-order models for projective trees. Additionally, work has been done on approximate non-factored parsing systems (McDonald and Pereira 2006;"
J11-1007,D07-1100,0,0.205097,"ally score correct graphs above incorrect ones. Inference is also global, in that systems attempt to ﬁnd the highest scoring graph among the set of all graphs. We call such systems graph-based parsing models to reﬂect the fact that parameterization is over the graph. Graph-based models are mainly associated with the pioneering work of Eisner (Eisner 1996), as well as McDonald and colleagues (McDonald, Crammer, and Pereira 2005; McDonald et al. 2005; McDonald and Pereira 2006; McDonald, Lerman, and Pereira 2006) and others (Riedel, C ¸ akıcı, and Meza-Ruiz 2006; Carreras 2007; Koo et al. 2007; Nakagawa 2007; Smith and Smith 2007). The second category of parsing systems parameterizes models over transitions from one state to another in an abstract state-machine. Parameters in these models are typically learned using standard classiﬁcation techniques that learn to predict one transition from a set of permissible transitions given a state history. Inference is local, in that systems start in a ﬁxed initial state and greedily construct the graph by taking the highest scoring transitions at each state entered until a termination condition is met. We call such systems transition-based parsing models t"
J11-1007,W03-3017,1,0.908431,"tion, s(c, t) = g(f(c), t). Given a transition scoring function, the parsing problem consists in ﬁnding a terminal conﬁguration cm ∈ Cx , starting from the initial conﬁguration cx and taking the optimal transition t∗ out of every conﬁguration c: t∗ = arg max s(c, t) t∈T 2 http://mstparser.sourceforge.net. 204 McDonald and Nivre Analyzing and Integrating Dependency Parsers This can be seen as a greedy search for the optimal dependency graph, based on a sequence of locally optimal decisions in terms of the transition system. By way of example, we consider the transition system ﬁrst presented in Nivre (2003), where a parser conﬁguration is a triple c = (σ, β, A), consisting of a stack σ of partially processed nodes, a buffer β of remaining input nodes, and a set A of labeled dependency arcs. The initial conﬁguration for a sentence x = w0 , w1 , . . . , wn is cx = ([0], [1, . . . , n], ∅ ) and the set of terminal conﬁgurations Cx contains all conﬁgurations of the form c = (σ, [ ], A) (that is, all conﬁgurations with an empty buffer and with arbitrary σ and A). The set T of transitions for this system is speciﬁed in Figure 3. The transitions L EFT-A RCl and R IGHT-A RCl extend the arc set A with an"
J11-1007,N07-1050,1,0.884713,"Missing"
J11-1007,P09-1040,1,0.791945,"Missing"
J11-1007,W04-2407,1,0.801564,"Missing"
J11-1007,W06-2933,1,0.809035,"Missing"
J11-1007,P08-1108,1,0.456224,"G = (V, A) consisting of a set of nodes V and a set of labeled directed arcs A ⊆ V × V × L; that is, if (i, j, l) ∈ A for i, j ∈ V and l ∈ L, then there is an arc from node i to node j with label l in the graph. In terms of standard linguistic dependency theory nomenclature, we say that (i, j, l) ∈ A if there is a dependency with head wi , dependent wj , and syntactic role l. A dependency graph G for sentence x must satisfy the following properties: 1. V = {0, 1, . . . , n}. 2. If (i, j, l) ∈ A, then j = 0. 1 This work has previously been published partially in McDonald and Nivre (2007) and Nivre and McDonald (2008). 201 Computational Linguistics Volume 37, Number 1 3. If (i, j, l) ∈ A, then for all arcs (i , j, l ) ∈ A, i = i and l = l . 4. For all j ∈ V − {0}, either (0, j, l) for some l ∈ L or there is a non-empty sequence of nodes i1 , . . . , im ∈ V and labels l1 , . . . , lm+1 ∈ L such that (0, i1 , l1 ),(i1 , i2 , l2 ), . . . , (im , j, lm+1 )∈A. The ﬁrst constraint states that the dependency graph spans the entire input. The second constraint states that the node 0 is a root. The third constraint states that each node has at most one incoming arc in the graph. The ﬁnal constraint states that"
J11-1007,P05-1013,1,0.940866,"ransition from a set of permissible transitions given a state history. Inference is local, in that systems start in a ﬁxed initial state and greedily construct the graph by taking the highest scoring transitions at each state entered until a termination condition is met. We call such systems transition-based parsing models to reﬂect the fact that parameterization is over possible state transitions. Transition-based models have been promoted by the groups of Matsumoto (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003; Cheng, Asahara, and Matsumoto 2006), Nivre (Nivre, Hall, and Nilsson 2004; Nivre and Nilsson 2005; Nivre et al. 2006), and others (Attardi 2006; Attardi and Ciaramita 2007; Johansson and Nugues 2007; Duan, Zhao, and Xu 2007; Titov and Henderson 2007a, 2007b). It is important to note that there is no a priori reason why a graph-based parameterization should require global learning and inference, and a transition-based parameterization would necessitate local learning and greedy inference. Nevertheless, as observed by Buchholz and Marsi (2006), it is striking that recent work on data-driven dependency parsing has been dominated by global, exhaustive, graph-based models, on the one hand, and"
J11-1007,W06-1616,0,0.0475874,"approach could potentially result in substantial improvements. 3. Novel approaches: The theoretical analysis presented in this article reveals that the two dominant approaches are each based on a particular combination of training and inference methods, which raises the question of which other combinations can fruitfully be explored. For example, can we construct globally trained, greedy, transition-based parsers? Or graph-based parsers with global features? To some extent the former characterization ﬁts the approach of Zhang and Clark (2008) and Huang and Sagae (2010), and the latter that of Riedel and Clarke (2006), Nakagawa (2007), and others. The analysis presented in this section explains the relative success of such approaches. In the next two sections we explore a model that falls into category 2. The system we propose uses a two-stage stacking framework, where a second-stage parser conditions on the predictions of a ﬁrst-stage parser during inference. The second-stage parser is also learned with access to the ﬁrst-stage parser’s decisions and thus learns when to trust the ﬁrst-stage parser’s predictions and when to trust its own. The method is not a traditional ensemble, because the parsers are no"
J11-1007,W06-2934,0,0.0234066,"Missing"
J11-1007,N06-2033,0,0.569808,"ortantly, we rerun the error analysis and show that the integrated models do indeed take advantage of the complementary strengths of both the graph-based and transition-based parsing systems. Combining the strengths of different machine learning systems, and even parsing systems, is by no means new as there are a number of previous studies that have looked at combining phrase-structure parsers (Henderson and Brill 1999), dependency parsers ˇ (Zeman and Zabokrtsk y` 2005), or both (McDonald 2006). Of particular note is past work on combining graph-based and transition-based dependency parsers. Sagae and Lavie (2006) present a system that combines multiple transition-based parsers with a single graph-based parser by weighting each potential dependency relation by the number of parsers that predicted it. A ﬁnal dependency graph is predicted by using spanning tree inference algorithms from the graph-based parsing literature (McDonald et al. 2005). Sagae and Lavie report improvements of up to 1.7 percentage points over the best single parser when combining three transition-based models and one graph-based model for unlabeled dependency parsing, evaluated on data from the Penn Treebank. The same technique was"
J11-1007,N01-1023,0,0.0151753,"g features generated by the parsers of Collins (1999) and Charniak (2000), which improved unlabeled accuracy by 1.7 percentage points on data from the Penn Treebank. In other NLP domains, feature-based integration has been used by Taskar, Lacoste-Julien, and Klein (2005), who trained a discriminative word alignment model using features derived from the IBM models, by Florian et al. (2004), who trained classiﬁers on auxiliary data to guide named entity classiﬁers, and by others. Feature-based integration also has points in common with co-training, which has been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003), among others. The difference, of course, is that standard co-training is a weakly supervised method, where the ﬁrst-stage parser’s predictions replace, rather than complement, the gold standard annotation during training. Feature-based integration is also similar to parse reranking (Collins 2000), where one parser produces a set of candidate parses and a second-stage classiﬁer chooses the most likely one. However, feature-based integration is not explicitly constrained to any parse decisions that the ﬁrst-stage parser might make. Furthermore, as only the single mos"
J11-1007,D08-1016,0,0.0834247,"r both sub-graphs and transitions. Huang and Sagae (2010) go even further and show how transition-based parsing can be tabularized to allow for dynamic programming, which in turn permits an exponentially larger search space. Martins et al. (2008) present a method for integrating graph-based and transition-based parsers based on stacking, which is similar to the approach taken in this work. Other studies have tried to overcome the weaknesses of parsing models by changing the underlying model structure directly. For example, Hall (2007), Riedel, C ¸ akıcı, and Meza-Ruiz (2006), Nakagawa (2007), Smith and Eisner (2008), and Martins, Smith, and Xing (2009) attempt to overcome local restrictions in feature scope for graphbased parsers through both approximations and exact solutions with integer linear programming. Our work differs from past studies in that we attempt to quantify exactly the types of errors these parsers make, tie them to their theoretical expectations, and show that integrating graph-based and transition-based parsers not only increases overall accuracy, but does so directly exploiting the strengths of each system. Thus, this is the ﬁrst large-scale error analysis of modern data-driven depend"
J11-1007,D07-1014,0,0.0152176,"Missing"
J11-1007,H05-1010,0,0.0185543,"Missing"
J11-1007,D07-1099,0,0.0360841,"Missing"
J11-1007,W07-2218,0,0.156608,"construct the graph by taking the highest scoring transitions at each state entered until a termination condition is met. We call such systems transition-based parsing models to reﬂect the fact that parameterization is over possible state transitions. Transition-based models have been promoted by the groups of Matsumoto (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003; Cheng, Asahara, and Matsumoto 2006), Nivre (Nivre, Hall, and Nilsson 2004; Nivre and Nilsson 2005; Nivre et al. 2006), and others (Attardi 2006; Attardi and Ciaramita 2007; Johansson and Nugues 2007; Duan, Zhao, and Xu 2007; Titov and Henderson 2007a, 2007b). It is important to note that there is no a priori reason why a graph-based parameterization should require global learning and inference, and a transition-based parameterization would necessitate local learning and greedy inference. Nevertheless, as observed by Buchholz and Marsi (2006), it is striking that recent work on data-driven dependency parsing has been dominated by global, exhaustive, graph-based models, on the one hand, and local, greedy, transition-based models, on the other. Therefore, a careful comparative analysis of these model types appears highly relevant, and this"
J11-1007,W03-3023,0,0.17081,"these models are typically learned using standard classiﬁcation techniques that learn to predict one transition from a set of permissible transitions given a state history. Inference is local, in that systems start in a ﬁxed initial state and greedily construct the graph by taking the highest scoring transitions at each state entered until a termination condition is met. We call such systems transition-based parsing models to reﬂect the fact that parameterization is over possible state transitions. Transition-based models have been promoted by the groups of Matsumoto (Kudo and Matsumoto 2002; Yamada and Matsumoto 2003; Cheng, Asahara, and Matsumoto 2006), Nivre (Nivre, Hall, and Nilsson 2004; Nivre and Nilsson 2005; Nivre et al. 2006), and others (Attardi 2006; Attardi and Ciaramita 2007; Johansson and Nugues 2007; Duan, Zhao, and Xu 2007; Titov and Henderson 2007a, 2007b). It is important to note that there is no a priori reason why a graph-based parameterization should require global learning and inference, and a transition-based parameterization would necessitate local learning and greedy inference. Nevertheless, as observed by Buchholz and Marsi (2006), it is striking that recent work on data-driven de"
J11-1007,W05-1518,0,0.0840469,"odels, on the other. Therefore, a careful comparative analysis of these model types appears highly relevant, and this is what we will try to provide in this article. For convenience, we will use the shorthand terms “graph-based” and “transition-based” for these models, although both graph-based and transition-based parameterizations can be (and have been) combined with different types of learning and inference. For example, the system described by Zhang and Clark (2008) could be characterized as a transition-based model with global learning, and the ˇ ensemble system of Zeman and Zabokrtsk y` (2005) as a graph-based model with greedy inference. Perhaps the most interesting reason to study the canonical graph-based and transition-based models is that even though they appear to be quite different theoretically (see Section 2), recent empirical studies show that both obtain similar parsing accuracies on a variety of languages. For example, Table 1 shows the results of the two top performing systems in the CoNLL-X shared task, those of McDonald, Lerman, and Pereira (2006) (graph-based) and Nivre et al. (2006) (transition-based), which exhibit no statistically signiﬁcant difference in accurac"
J11-1007,D08-1059,0,0.597677,"data-driven dependency parsing has been dominated by global, exhaustive, graph-based models, on the one hand, and local, greedy, transition-based models, on the other. Therefore, a careful comparative analysis of these model types appears highly relevant, and this is what we will try to provide in this article. For convenience, we will use the shorthand terms “graph-based” and “transition-based” for these models, although both graph-based and transition-based parameterizations can be (and have been) combined with different types of learning and inference. For example, the system described by Zhang and Clark (2008) could be characterized as a transition-based model with global learning, and the ˇ ensemble system of Zeman and Zabokrtsk y` (2005) as a graph-based model with greedy inference. Perhaps the most interesting reason to study the canonical graph-based and transition-based models is that even though they appear to be quite different theoretically (see Section 2), recent empirical studies show that both obtain similar parsing accuracies on a variety of languages. For example, Table 1 shows the results of the two top performing systems in the CoNLL-X shared task, those of McDonald, Lerman, and Pere"
J11-1007,J03-4003,0,\N,Missing
J11-1007,D07-1096,1,\N,Missing
J14-2001,P99-1059,0,0.0228125,"and wj inclusive, (ii) G[i,j] is a directed tree, and (iii) it holds for every arc (i, l, j) ∈ G[i,j] that there is a directed path 1 Although span and arc constraints can easily be added to other dependency parsing frameworks, this often affects parsing complexity. For example, in graph-based parsing (McDonald, Crammer, and Pereira 2005) arc constraints can be enforced within the O(n3 ) Eisner algorithm (Eisner 1996) by pruning out inconsistent chart cells, but span constraints require the parser to keep track of full subtree end points, which would necessitate the use of O(n4 ) algorithms (Eisner and Satta 1999). 250 Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing from i to every node k such that min(i, j) &lt; k &lt; max(i, j) (projectivity). We now define two constraints on a dependency graph G for a sentence x: r G is a projective dependency tree (PDT) if and only if it is a projective spanning tree over the interval [1, n] rooted at node n. r G is a projective dependency graph (PDG) if and only if it can be extended to a projective dependency tree simply by adding arcs. It is clear from the definitions that every PDT is also a PDG, but not the other way around. Every PDG can be c"
J14-2001,C96-1058,0,0.0699697,"ay that a subgraph G[i,j] = (V[i,j] , A[i,j] ) of G is a projective spanning tree over the interval [i, j] (1 ≤ i ≤ j ≤ n) iff (i) G[i,j] contains all nodes corresponding to words between wi and wj inclusive, (ii) G[i,j] is a directed tree, and (iii) it holds for every arc (i, l, j) ∈ G[i,j] that there is a directed path 1 Although span and arc constraints can easily be added to other dependency parsing frameworks, this often affects parsing complexity. For example, in graph-based parsing (McDonald, Crammer, and Pereira 2005) arc constraints can be enforced within the O(n3 ) Eisner algorithm (Eisner 1996) by pruning out inconsistent chart cells, but span constraints require the parser to keep track of full subtree end points, which would necessitate the use of O(n4 ) algorithms (Eisner and Satta 1999). 250 Nivre, Goldberg, and McDonald Constrained Arc-Eager Dependency Parsing from i to every node k such that min(i, j) &lt; k &lt; max(i, j) (projectivity). We now define two constraints on a dependency graph G for a sentence x: r G is a projective dependency tree (PDT) if and only if it is a projective spanning tree over the interval [1, n] rooted at node n. r G is a projective dependency graph (PDG)"
J14-2001,C12-1059,1,0.847816,"configuration without having to scan the stack and buffer linearly. Because there are at most O(n) arcs in the arc constraint set, the preprocessing will not take more than O(n) time but guarantees that all permissibility checks can be performed in O(1) time. Finally, we note that the arc-constrained system is sound and complete in the sense that it derives all and only PDTs compatible with a given arc constraint set AC for a sentence x. Soundness follows from the fact that, for every arc (i, l, j) ∈ AC , the preconditions 3 For further discussion of reachability in the arc-eager system, see Goldberg and Nivre (2012, 2013). 253 Computational Linguistics Volume 40, Number 2 force the system to reach a configuration of the form (σ |min(i, j), max(i, j)|β, A) in which either L EFT-A RCl (i &gt; j) or R IGHT-A RC l (i &lt; j) will be the only permissible transition. Completeness follows from the observation that every PDT G compatible with AC is also a PDG and can therefore be viewed as a larger constraint set for which every transition sequence (given soundness) derives G exactly. Empirical Case Study: Imperatives. Consider the problem of parsing commands to personal assistants such as Siri or Google Now. In this"
J14-2001,Q13-1033,1,0.843482,"Missing"
J14-2001,I11-1084,0,0.0193491,"transition. Completeness follows from the observation that every PDT G compatible with AC is also a PDG and can therefore be viewed as a larger constraint set for which every transition sequence (given soundness) derives G exactly. Empirical Case Study: Imperatives. Consider the problem of parsing commands to personal assistants such as Siri or Google Now. In this setting, the distribution of utterances is highly skewed towards imperatives making them easy to identify. Unfortunately, parsers trained on treebanks like the Penn Treebank (PTB) typically do a poor job of parsing such utterances (Hara et al. 2011). However, we know that if the first word of a command is a verb, it is likely the root of the sentence. If we take an arc-eager beam search parser (Zhang and Nivre 2011) trained on the PTB, it gets 82.14 labeled attachment score on a set of commands.4 However, if we constrain the same parser so that the first word of the sentence must be the root, accuracy jumps dramatically to 85.56. This is independent of simply knowing that the first word of the sentence is a verb, as both parsers in this experiment had access to gold part-of-speech tags. 4. Parsing with Span Constraints Span Constraints."
J14-2001,P10-1001,0,0.162458,"Missing"
J14-2001,P05-1012,1,0.86983,"Missing"
J14-2001,P13-2017,1,0.886031,"Missing"
J14-2001,W03-3017,1,0.76581,"ce, Ramat-Gan, 5290002, Israel. E-mail: yoav.goldberg@gmail.com. † Google, 76 Buckingham Palace Road, London SW1W9TQ, United Kingdom. E-mail: ryanmcd@google.com. Submission received: 26 June 2013; accepted for publication: 10 October 2013. doi:10.1162/COLI a 00184 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 Figure 1 Span constraint derived from a title assisting parsing. Left: unconstrained. Right: constrained. In this article, we examine the problem of constraining transition-based dependency parsers based on the arc-eager transition system (Nivre 2003, 2008), which perform a single left-to-right pass over the input, eagerly adding dependency arcs at the earliest possible opportunity, resulting in linear time parsing. We consider two types of constraints: span constraints, exemplified earlier, require the output graph to have a single subtree over one or more (non-overlapping) spans of the input; arc constraints instead require specific arcs to be present in the output dependency graph. The main contribution of the article is to show that both span and arc constraints can be implemented as efficiently computed preconditions on parser transi"
J14-2001,J08-4003,1,0.894607,"ad. S HIFT removes the first node in the buffer and pushes it onto the stack, with the precondition that j 6= n. A transition sequence for a sentence x is a sequence C0,m = (c0 , c1 , . . . , cm ) of configurations, such that c0 is the initial configuration cs (x), cm is a terminal configuration, and there is a legal transition t such that ci = t(ci−1 ) for every i, 1 ≤ i ≤ m. The dependency graph derived by C0,m is Gcm = (Vx , Acm ), where Acm is the set of arcs in cm . Complexity and Correctness. For a sentence of length n, the number of transitions in the arc-eager system is bounded by 2n (Nivre 2008). This means that a parser using greedy inference (or constant width beam search) will run in O(n) time provided that transitions plus required precondition checks can be performed in O(1) time. This holds for the arc-eager system and, as we will demonstrate, its constrained variants as well. The arc-eager transition system as presented here is sound and complete for the set of PDTs (Nivre 2008). For a specific sentence x = w1 , . . . , wn , this means that any transition sequence for x produces a PDT (soundness), and that any PDT for x is generated by 251 Computational Linguistics Transition"
J14-2001,W04-2407,1,0.901212,"Missing"
J14-2001,P80-1024,0,0.667437,"Missing"
J14-2001,W03-3023,0,0.309594,"strained to respect two different types of conditions on the output dependency graph: span constraints, which require certain spans to correspond to subtrees of the graph, and arc constraints, which require certain arcs to be present in the graph. The constraints are incorporated into the arc-eager transition system as a set of preconditions for each transition and preserve the linear time complexity of the parser. 1. Introduction Data-driven dependency parsers in general achieve high parsing accuracy without relying on hard constraints to rule out (or prescribe) certain syntactic structures (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira 2005; Zhang and Clark 2008; Koo and Collins 2010). Nevertheless, there are situations where additional information sources, not available at the time of training the parser, may be used to derive hard constraints at parsing time. For example, Figure 1 shows the parse of a greedy arc-eager dependency parser trained on the Wall Street Journal section of the Penn Treebank before (left) and after (right) being constrained to build a single subtree over the span corresponding to the named entity “Cat on a Hot Tin Roof,” which does not"
J14-2001,D08-1059,0,0.0265037,"traints, which require certain spans to correspond to subtrees of the graph, and arc constraints, which require certain arcs to be present in the graph. The constraints are incorporated into the arc-eager transition system as a set of preconditions for each transition and preserve the linear time complexity of the parser. 1. Introduction Data-driven dependency parsers in general achieve high parsing accuracy without relying on hard constraints to rule out (or prescribe) certain syntactic structures (Yamada and Matsumoto 2003; Nivre, Hall, and Nilsson 2004; McDonald, Crammer, and Pereira 2005; Zhang and Clark 2008; Koo and Collins 2010). Nevertheless, there are situations where additional information sources, not available at the time of training the parser, may be used to derive hard constraints at parsing time. For example, Figure 1 shows the parse of a greedy arc-eager dependency parser trained on the Wall Street Journal section of the Penn Treebank before (left) and after (right) being constrained to build a single subtree over the span corresponding to the named entity “Cat on a Hot Tin Roof,” which does not occur in the training set but can easily be found in on-line databases. In this case, addi"
J14-2001,P11-2033,1,0.937551,"ich every transition sequence (given soundness) derives G exactly. Empirical Case Study: Imperatives. Consider the problem of parsing commands to personal assistants such as Siri or Google Now. In this setting, the distribution of utterances is highly skewed towards imperatives making them easy to identify. Unfortunately, parsers trained on treebanks like the Penn Treebank (PTB) typically do a poor job of parsing such utterances (Hara et al. 2011). However, we know that if the first word of a command is a verb, it is likely the root of the sentence. If we take an arc-eager beam search parser (Zhang and Nivre 2011) trained on the PTB, it gets 82.14 labeled attachment score on a set of commands.4 However, if we constrain the same parser so that the first word of the sentence must be the root, accuracy jumps dramatically to 85.56. This is independent of simply knowing that the first word of the sentence is a verb, as both parsers in this experiment had access to gold part-of-speech tags. 4. Parsing with Span Constraints Span Constraints. Given a sentence x = w1 , . . . , wn , we take a span constraint set to be a set SC of non-overlapping spans [i, j] (1 ≤ i &lt; j ≤ n). The task of span-constrained parsing"
J14-2001,J13-1002,1,\N,Missing
L16-1262,W13-2308,0,0.0114723,"g diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Span"
L16-1262,W06-2920,0,0.443151,"clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies in the enhanced representation and miscellaneous information. The format is illustrated in Figure 3, with the French sentence from Figure 2. To support work on treebanks in this format,"
L16-1262,W09-2307,1,0.329071,"standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Ja"
L16-1262,P11-1061,1,0.573621,"UNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapt"
L16-1262,W08-1301,1,0.58058,"Missing"
L16-1262,de-marneffe-etal-2006-generating,1,0.213978,"Missing"
L16-1262,de-marneffe-etal-2014-universal,1,0.831309,"Missing"
L16-1262,E14-4028,0,0.0377832,"Missing"
L16-1262,N15-3011,1,0.696846,"Missing"
L16-1262,D07-1013,1,0.230402,"hocolat . le fille adorer le dessert a` le chocolat . DET NOUN VERB DET NOUN ADP DET NOUN PUNCT Definite=Def Gender=Fem Number=Plur Definite=Def Gender=Masc Definite=Def Gender=Masc Number=Plur Number=Plur Person=3 Number=Plur Number=Plur Number=Sing Number=Sing Tense=Pres Figure 2: UD annotation for a French sentence. (Translation: However, girls love chocolate desserts.) 2. History 3. UD comprises two layers of annotation with diverse origins. The Google universal tag set used in the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged"
L16-1262,P13-2017,1,0.877648,"Missing"
L16-1262,W15-2127,0,0.0113361,"n English, we also obtain parallel representations between prepositional phrases and subordinate clauses, which are in practice often introduced by a preposition, as in (5). nmod case nsubj (5) a. Sue 1662 det left after the rehearsal advcl nsubj b. Sue Language mark nsubj left after we did The choice to make content words the backbone of the syntactic representations may seem to be at odds with the strong tendency in modern syntactic theory to give priority to functional heads, a tendency that is found in both constituency-based and dependency-based approaches to syntax (Brug´e et al., 2012; Osborne and Maxwell, 2015). We believe, however, that this conflict is more apparent than real. The UD view is that we need to recognize both lexical and functional heads, but in order to maximize parallelism across languages, only lexical heads are inferable from the topology of our tree structures. Functional heads are instead represented as specifying features of content words, using dedicated relation labels, features which can alternatively be specified through morphological processes. In the dependency grammar tradition, this is very close to the view of Tesni`ere (1959), according to whom dependencies hold betwe"
L16-1262,petrov-etal-2012-universal,1,0.717175,"exist to build consistent resources for many languages, and the UD project is a merger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we p"
L16-1262,rosa-etal-2014-hamledt,1,0.832586,"Missing"
L16-1262,L16-1376,1,0.208211,"fferent languages. For instance, while the universal UD scheme has a single relation acl for adnominal clauses, several languages make use of the subtype acl:relcl to distinguish relative clauses as an important subtype of adnominal clauses. By design, we can always map back to the core label set by stripping the specific relations that appear after the colon. For a complete list of currently used languagespecific relations, we refer to the UD website. 2 Complete guidelines for the enhanced representations have not been worked out yet, and only one treebank (Finnish) uses them so far, but see Schuster and Manning (2016) for a concrete proposal for English. 3.4. Format and Tools The data is encoded in the CoNLL-U format, which is an evolution of the widely used CoNLL-X format (Buchholz and Marsi, 2006), where each word/token is represented in tab-separated columns on one line and sentence boundaries are marked by blank lines. The 10 columns on a word/token line are used to specify a unique id (integer for words, ranges for multiword tokens), word form, lemma, universal part-of-speech tag, optional language-specific part-ofspeech tag, morphological features, head, dependency relation, additional dependencies i"
L16-1262,E12-2021,1,0.581828,"Missing"
L16-1262,stepanek-pajas-2010-querying,0,0.067769,"Missing"
L16-1262,P13-2103,1,0.625022,"hese resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-speech tags into a universal annotation scheme: treebanks were released for 6 languages in 2013 (English, French, German, Spanish, Swedish and Korean) and for 11 languages in 2014 (Brazilian Portuguese, English, Finnish, French, German, Italian, Indonesian, Japanese, Korean, Spanish and Swedish). The first proposal for incorporating morphology was made by Tsarfaty (2013). The second version of HamleDT (Rosa et al., 2014) provided Stanford/Google annotation for 30 languages by automatically harmonizing treebanks with different native annotations. These efforts were followed by the development of the universal Stanford dependencies (USD), revising Stanford Dependencies for cross-linguistic annotations in light of the Google scheme (de Marneffe et al., 2014). UD is the result of merging all these initiatives into a single coherent framework, based on the universal Stanford dependencies, an extended version of the Google universal tag set, a revised subset of the"
L16-1262,I08-3008,1,0.20904,"the morphological layer grew out of the cross-linguistic error analysis based on the CoNLL-X shared task data by McDonald and Nivre (2007). It was initially used for unsupervised partof-speech tagging by Das and Petrov (2011), and has been adopted as a widely used standard for mapping diverse tag sets to a common standard. The morphological layer also builds on Interset (Zeman, 2008), which started as a tool for conversion between morphosyntactic tag sets of multiple languages. It dates back to 2006 when it was used in the first experiments with cross-lingual delexicalized parser adaptation (Zeman and Resnik, 2008). The Stanford dependencies, used in the syntactic layer, were developed for English in 2005 and eventually emerged as the de facto standard for dependency analysis of English. They have since been adapted to a number of different languages (Chang et al., 2009; Bosco et al., 2013; Haverinen et al., 2013; Seraji et al., 2013; Lipenkova and Souˇcek, 2014). These resources have featured in other attempts at universal standards. The Google Universal Dependency Treebank (UDT) project (McDonald et al., 2013) was the first attempt to combine the Stanford dependencies and the Google universal part-of-"
L16-1262,zeman-etal-2012-hamledt,1,0.729155,"Missing"
L16-1262,zeman-2008-reusable,1,0.897534,"erger of some of the initiatives. It combines the (universal) Stanford dependencies (de Marneffe et al., 2006; de Marneffe and Manning, 2008; de Marneffe et al., 2014), the universal sv: en nsubj katt conj jagar r˚attor conj och m¨oss cc conj nsubj ? da: en dobj kat jager rotter og mus conj det en: a nsubj cat dobj chases cc rats and mice Figure 1: Divergent annotation of parallel structures Google dependency scheme (Universal Dependency Treebanks) (McDonald et al., 2013), the Google universal partof-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tag sets (Zeman, 2008) used in the HamleDT treebanks (a project that transforms existing treebanks under a common annotation scheme, Zeman et al. 2012). UD is thus based on common usage and existing de facto standards, and is intended to replace all the previous versions by a single coherent standard.1 The general philosophy is to provide a universal inventory of categories and guidelines to facilitate consistent annotation of similar constructions across languages, while allowing languagespecific extensions when necessary. In this paper, we present version 1 of the universal guidelines and explain the underlying d"
N09-2029,E09-1029,0,0.0206114,"marization has historically focused on summarizing events, a task embodied in the series of Document Understanding Conferences1 . However, there has also been work on entity-centric summarization, which aims to produce summaries from text collections that are relevant to a particular entity of interest, e.g., product, person, company, etc. A well-known example of this is from the opinion mining community where there has been a number of studies on summarizing the expressed sentiment towards entities (cf. Hu and Liu (2006)). Another recent example of entity-centric summarization is the work of Filippova et al. (2009) to produce company-specific financial report summaries. In this study we investigate a variation of entitycentric summarization where the goal is not to summarize information about a single entity, but pairs of entities. Specifically, our aim is to jointly generate two summaries that highlight differences between the entities – a task we call contrastive summarization. An obvious application comes from the consumer reviews domain, where a person considering a purchase wishes to see the differences in opinion about the top candidates without reading all the reviews for each product. Other appl"
N09-2029,E09-1059,1,0.427758,"ume the existence of standard sentiment analysis tools to provide the information used in the scoring function L. First, we assume the tools can assign a sentiment score from -1 (negative) to 1 (positive) to an arbitrary span of text. Second, we assume that we can extract a set of aspects that the text is discussing (e.g, “The sound was crystal clear” is about the aspect sound quality). We refer the reader to abundance of literature on sentiment analysis for more details on how such tools can be constructed (cf. Pang and Lee (2008)). For this study, we use the tools described and evaluated in Lerman et al. (2009). We note however, that the subject of this discussion is not the tools themselves, but their use. The single product opinion summarizer we consider is the Sentiment Aspect Match model (SAM) described and evaluated in (Lerman et al., 2009). Underlying SAM is the assumption that opinions can be described by a bag-of-aspects generative process where each aspect is generated independently and the sentiment associated with the aspect is generated conditioned on its identity, Y p(t) = p(a)p(SENT(at )|a) a∈At where At is a set of aspects that are mentioned in text excerpt t, p(a) is the probability"
N09-2029,H05-1014,0,0.0256711,"such sentences exist. In contrastive summarization, there is no assumption that two entities have been explicitly compared. The goal is to automatically generate the comparisons based on the data. In the IR community, Sun et al. (2006) explores retrieval systems that align query results to highlight points of commonality and difference. In contrast, we attempt to identify contrasts from the data, and then generate summaries that highlight them. The novelty detection task of determining whether a new text in a collection contains information distinct from that already gathered is also related (Soboroff and Harman, 2005). The primary difference here is that contrastive summarization aims to extract information from one collection not present in the other in addition to information present in both collections that highlights a difference between the entities. This paper describes a contrastive summarization experiment where the goal is to generate contrasting opinion summaries of two products based on consumer reviews of each. We look at model design choices, describe an implementation of a contrastive summarizer, and provide an evaluation demonstrating a significant improvement in the usefulness of contrastiv"
N10-1069,P04-1015,0,0.12033,"e of the most popular training algorithms for structured prediction problems in natural language processing is the perceptron (Rosenblatt, 1958; Collins, 2002). The structured perceptron has many desirable properties, most notably that there is no need to calculate a partition function, which is necessary for other structured prediction paradigms such as CRFs (Lafferty et al., 2001). Furthermore, it is robust to approximate inference, which is often required for problems where the search space is too large and where strong structural independence assumptions are insufficient, such as parsing (Collins and Roark, 2004; McDonald and Pereira, 2006; Zhang and Clark, 2008) and machine translation (Liang et al., 2006). However, like all structured prediction learning frameworks, the structure perceptron can still be cumbersome to train. This is both due to the increasing size of available training sets as well as the fact that training complexity is proportional to inference, which is frequently nonlinear in sequence length, even with strong structural independence assumptions. In this paper we investigate distributed training strategies for the structured perceptron as a means of reducing training times when l"
N10-1069,W02-1001,0,0.984886,"for the structured perceptron as a means to reduce training times when computing clusters are available. We look at two strategies and provide convergence bounds for a particular mode of distributed structured perceptron training based on iterative parameter mixing (or averaging). We present experiments on two structured prediction problems – namedentity recognition and dependency parsing – to highlight the efficiency of this method. 1 Introduction One of the most popular training algorithms for structured prediction problems in natural language processing is the perceptron (Rosenblatt, 1958; Collins, 2002). The structured perceptron has many desirable properties, most notably that there is no need to calculate a partition function, which is necessary for other structured prediction paradigms such as CRFs (Lafferty et al., 2001). Furthermore, it is robust to approximate inference, which is often required for problems where the search space is too large and where strong structural independence assumptions are insufficient, such as parsing (Collins and Roark, 2004; McDonald and Pereira, 2006; Zhang and Clark, 2008) and machine translation (Liang et al., 2006). However, like all structured predicti"
N10-1069,P08-1109,0,0.0195599,"tochastic gradient descent. The asynchronous algorithms in these studies require shared memory between the distributed computations and are less suitable to the more common cluster computing environment, which is what we study here. While we focus on the perceptron algorithm, there is a large body of work on training structured prediction classifiers. For batch training the most common is conditional random fields (CRFs) (Lafferty et al., 2001), which is the structured analog of maximum entropy. As such, its training can easily be distributed through the gradient or sub-gradient computations (Finkel et al., 2008). However, unlike perceptron, CRFs require the computation of a partition function, which is often expensive and sometimes intractable. Other batch learning algorithms include M3 Ns (Taskar et al., 2004) and Structured SVMs (Tsochantaridis et al., 2004). Due to their efficiency, online learning algorithms have gained attention, especially for structured prediction tasks in NLP. In addition to the perceptron (Collins, 2002), others have looked at stochastic gradient descent (Zhang, 2004), passive aggressive algorithms (McDonald et 457 al., 2005; Crammer et al., 2006), the recently introduced co"
N10-1069,P06-1096,0,0.588263,"Missing"
N10-1069,E06-1011,1,0.844903,"ining algorithms for structured prediction problems in natural language processing is the perceptron (Rosenblatt, 1958; Collins, 2002). The structured perceptron has many desirable properties, most notably that there is no need to calculate a partition function, which is necessary for other structured prediction paradigms such as CRFs (Lafferty et al., 2001). Furthermore, it is robust to approximate inference, which is often required for problems where the search space is too large and where strong structural independence assumptions are insufficient, such as parsing (Collins and Roark, 2004; McDonald and Pereira, 2006; Zhang and Clark, 2008) and machine translation (Liang et al., 2006). However, like all structured prediction learning frameworks, the structure perceptron can still be cumbersome to train. This is both due to the increasing size of available training sets as well as the fact that training complexity is proportional to inference, which is frequently nonlinear in sequence length, even with strong structural independence assumptions. In this paper we investigate distributed training strategies for the structured perceptron as a means of reducing training times when large computing clusters are"
N10-1069,P05-1012,1,0.558275,"Missing"
N10-1069,W03-0419,0,0.0326071,"Missing"
N10-1069,I08-5007,0,0.00673141,"that distributed training through parameter mixing (or averaging) for maximum entropy models can be empirically powerful and has strong theoretical guarantees. A parameter mixing strategy, which can be applied to any parameterized learning algorithm, trains separate models in parallel, each on a disjoint subset of the training data, and then takes an average of all the parameters as the final model. In this paper, we provide results which suggest that the perceptron is ill-suited for straight-forward parameter mixing, even though it is commonly used for large-scale structured learning, e.g., Whitelaw et al. (2008) for named-entity recognition. However, a slight mod456 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 456–464, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics ification we call iterative parameter mixing can be shown to: 1) have similar convergence properties to the standard perceptron algorithm, 2) find a separating hyperplane if the training set is separable, 3) reduce training times significantly, and 4) produce models with comparable (or superior) accuracies to those trained serially on all the d"
N10-1069,D08-1059,0,0.0153424,"red prediction problems in natural language processing is the perceptron (Rosenblatt, 1958; Collins, 2002). The structured perceptron has many desirable properties, most notably that there is no need to calculate a partition function, which is necessary for other structured prediction paradigms such as CRFs (Lafferty et al., 2001). Furthermore, it is robust to approximate inference, which is often required for problems where the search space is too large and where strong structural independence assumptions are insufficient, such as parsing (Collins and Roark, 2004; McDonald and Pereira, 2006; Zhang and Clark, 2008) and machine translation (Liang et al., 2006). However, like all structured prediction learning frameworks, the structure perceptron can still be cumbersome to train. This is both due to the increasing size of available training sets as well as the fact that training complexity is proportional to inference, which is frequently nonlinear in sequence length, even with strong structural independence assumptions. In this paper we investigate distributed training strategies for the structured perceptron as a means of reducing training times when large computing clusters are available. Traditional m"
N10-1069,W06-2920,0,\N,Missing
N10-1119,N09-2008,0,0.00893335,"studies on building polarity lexicons have used linguistic resources like WordNet to define the graph through synonym and antonym relations (Kim and Hovy, 2004; Esuli and Sabastiani, 2009; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009). The goal of this study is to examine the size and quality of polarity lexicons when the graph is induced automatically from documents on the web. Constructing a graph from web-computed lexical co-occurrence statistics is a difficult challenge in and of itself and the research and implementation hurdles that arise are beyond the scope of this work (Alfonseca et al., 2009; Pantel et al., 2009). For this study, we used an English graph where the node set V was based on all n-grams up to length 10 extracted from 4 billion web pages. This list was 779 filtered to 20 million candidate phrases using a number of heuristics including frequency and mutual information of word boundaries. A context vector for each candidate phrase was then constructed based on a window of size six aggregated over all mentions of the phrase in the 4 billion documents. The edge set E was constructed by first, for each potential edge (vi , vj ), computing the cosine similarity value betwee"
N10-1119,D08-1014,0,0.0405255,"Missing"
N10-1119,E06-1039,0,0.154186,"l Conference of the North American Chapter of the ACL, pages 777–785, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics lexicon relative to two previously published lexicons – the lexicon used in Wilson et al. (2005) and the lexicon used in Blair-Goldensohn et al. (2008). Our experiments show that a web-derived lexicon is not only significantly larger, but has improved accuracy on a sentence polarity classification task, which is an important problem in many sentiment analysis applications, including sentiment aggregation and summarization (Hu and Liu, 2004; Carenini et al., 2006; Lerman et al., 2009). These results hold true both when the lexicons are used in conjunction with string matching to classify sentences, and when they are included within a contextual classifier framework (Wilson et al., 2005). Extracting polarity lexicons from the web has been investigated previously by Kaji and Kitsuregawa (2007), who study the problem exclusively for Japanese. In that work a set of positive/negative sentences are first extracted from the web using cues from a syntactic parser as well as the document structure. Adjectives phrases are then extracted from these sentences bas"
N10-1119,D09-1062,0,0.304216,"s being either positive, negative or neutral in sentiment. Additionally, the system outputs two rankings, the first a ranking of the sentence by positive polarity and the second a ranking of the sentence by negative polarity. Classifying sentences by their sentiment is a subtask of sentiment aggregation systems (Hu and Liu, 2004; Gamon et al., 2005). Ranking sentences by their polarity is a critical sub-task in extractive sentiment summarization (Carenini et al., 2006; Lerman et al., 2009). To classify sentences as being positive, negative or neutral, we used an augmented vote-flip algorithm (Choi and Cardie, 2009), which is given in Figure 3. This intuition behind this algorithm is sim782 ple. The number of matched positive and negative phrases from the lexicon are counted and whichever has the most votes wins. The algorithm flips the decision if the number of negations is odd. Though this algorithm appears crude, it benefits from not relying on threshold values for neutral classification, which is difficult due to the fact that the polarity scores in the three lexicons are not on the same scale. To rank sentences we defined the purity of a sentence X as the normalized sum of the sentiment scores for e"
N10-1119,P97-1023,0,0.955925,"tems. As a result, the lexicon is not limited to specific word classes – e.g., adjectives that occur in WordNet – and in fact contains slang, misspellings, multiword expressions, etc. We evaluate a lexicon derived from English documents, both qualitatively and quantitatively, and show that it provides superior performance to previously studied lexicons, including one derived from WordNet. 1 Introduction Polarity lexicons are large lists of phrases that encode the polarity of each phrase within it – either positive or negative – often with some score representing the magnitude of the polarity (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Turney, 2002). Though classifiers built with machine learning algorithms have become commonplace in the sentiment analysis literature, e.g., Pang et al. (2002), the core of many academic and commercial sentiment analysis systems remains the polarity lexicon, which can be constructed manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004; Esuli and Sabastiani, 2009) or using machine learning (Turney, 2002; Rao and Ravichandran, 2009). Often lexicons are combined with machine learning for improved results (Wilson et al., 2005). The pervasiveness and sustained use of"
N10-1119,E09-1046,0,0.116862,"Missing"
N10-1119,D07-1115,0,0.858945,"that a web-derived lexicon is not only significantly larger, but has improved accuracy on a sentence polarity classification task, which is an important problem in many sentiment analysis applications, including sentiment aggregation and summarization (Hu and Liu, 2004; Carenini et al., 2006; Lerman et al., 2009). These results hold true both when the lexicons are used in conjunction with string matching to classify sentences, and when they are included within a contextual classifier framework (Wilson et al., 2005). Extracting polarity lexicons from the web has been investigated previously by Kaji and Kitsuregawa (2007), who study the problem exclusively for Japanese. In that work a set of positive/negative sentences are first extracted from the web using cues from a syntactic parser as well as the document structure. Adjectives phrases are then extracted from these sentences based on different statistics of their occurrence in the positive or negative set. Our work, on the other hand, does not rely on syntactic parsers or restrict the set of candidate lexicon entries to specific syntactic classes, i.e., adjective phrases. As a result, the lexicon built in our study is on a different scale than that examined"
N10-1119,C04-1200,0,0.644582,"ordNet. 1 Introduction Polarity lexicons are large lists of phrases that encode the polarity of each phrase within it – either positive or negative – often with some score representing the magnitude of the polarity (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Turney, 2002). Though classifiers built with machine learning algorithms have become commonplace in the sentiment analysis literature, e.g., Pang et al. (2002), the core of many academic and commercial sentiment analysis systems remains the polarity lexicon, which can be constructed manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004; Esuli and Sabastiani, 2009) or using machine learning (Turney, 2002; Rao and Ravichandran, 2009). Often lexicons are combined with machine learning for improved results (Wilson et al., 2005). The pervasiveness and sustained use of lexicons can be ascribed to a number of reasons, including their interpretability in large-scale systems as well as the granularity of their analysis. In this work we investigate the viability of polarity lexicons that are derived solely from unlabeled web documents. We propose a method based on graph propagation algorithms inspired by previous work on constructing"
N10-1119,E09-1059,1,0.471159,"th American Chapter of the ACL, pages 777–785, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics lexicon relative to two previously published lexicons – the lexicon used in Wilson et al. (2005) and the lexicon used in Blair-Goldensohn et al. (2008). Our experiments show that a web-derived lexicon is not only significantly larger, but has improved accuracy on a sentence polarity classification task, which is an important problem in many sentiment analysis applications, including sentiment aggregation and summarization (Hu and Liu, 2004; Carenini et al., 2006; Lerman et al., 2009). These results hold true both when the lexicons are used in conjunction with string matching to classify sentences, and when they are included within a contextual classifier framework (Wilson et al., 2005). Extracting polarity lexicons from the web has been investigated previously by Kaji and Kitsuregawa (2007), who study the problem exclusively for Japanese. In that work a set of positive/negative sentences are first extracted from the web using cues from a syntactic parser as well as the document structure. Adjectives phrases are then extracted from these sentences based on different statis"
N10-1119,P07-1055,1,0.719523,"Missing"
N10-1119,P07-1123,0,0.0365032,"Missing"
N10-1119,D09-1063,0,0.744662,"., 2008; Rao and Ravichandran, 2009). Whereas past efforts have used linguistic resources – e.g., WordNet – to construct the lexical graph over which propagation runs, our lexicons are constructed using a graph built from co-occurrence statistics from the entire web. Thus, the method we investigate can be seen as a combination of methods for propagating sentiment across lexical graphs and methods for building sentiment lexicons based on distributional characteristics of phrases in raw data (Turney, 2002). The advantage of breaking the dependence on WordNet (or related resources like thesauri (Mohammad et al., 2009)) is that it allows the lexicons to include non-standard entries, most notably spelling mistakes and variations, slang, and multiword expressions. The primary goal of our study is to understand the characteristics and practical usefulness of such a lexicon. Towards this end, we provide both a qualitative and quantitative analysis for a web-derived English 777 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 777–785, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics lexicon relative to two previously publi"
N10-1119,W02-1011,0,0.0175787,"luate a lexicon derived from English documents, both qualitatively and quantitatively, and show that it provides superior performance to previously studied lexicons, including one derived from WordNet. 1 Introduction Polarity lexicons are large lists of phrases that encode the polarity of each phrase within it – either positive or negative – often with some score representing the magnitude of the polarity (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Turney, 2002). Though classifiers built with machine learning algorithms have become commonplace in the sentiment analysis literature, e.g., Pang et al. (2002), the core of many academic and commercial sentiment analysis systems remains the polarity lexicon, which can be constructed manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004; Esuli and Sabastiani, 2009) or using machine learning (Turney, 2002; Rao and Ravichandran, 2009). Often lexicons are combined with machine learning for improved results (Wilson et al., 2005). The pervasiveness and sustained use of lexicons can be ascribed to a number of reasons, including their interpretability in large-scale systems as well as the granularity of their analysis. In this work we invest"
N10-1119,D09-1098,0,0.0411993,"arity lexicons have used linguistic resources like WordNet to define the graph through synonym and antonym relations (Kim and Hovy, 2004; Esuli and Sabastiani, 2009; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009). The goal of this study is to examine the size and quality of polarity lexicons when the graph is induced automatically from documents on the web. Constructing a graph from web-computed lexical co-occurrence statistics is a difficult challenge in and of itself and the research and implementation hurdles that arise are beyond the scope of this work (Alfonseca et al., 2009; Pantel et al., 2009). For this study, we used an English graph where the node set V was based on all n-grams up to length 10 extracted from 4 billion web pages. This list was 779 filtered to 20 million candidate phrases using a number of heuristics including frequency and mutual information of word boundaries. A context vector for each candidate phrase was then constructed based on a window of size six aggregated over all mentions of the phrase in the 4 billion documents. The edge set E was constructed by first, for each potential edge (vi , vj ), computing the cosine similarity value between context vectors. All"
N10-1119,E09-1077,0,0.802874,"ity of each phrase within it – either positive or negative – often with some score representing the magnitude of the polarity (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Turney, 2002). Though classifiers built with machine learning algorithms have become commonplace in the sentiment analysis literature, e.g., Pang et al. (2002), the core of many academic and commercial sentiment analysis systems remains the polarity lexicon, which can be constructed manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004; Esuli and Sabastiani, 2009) or using machine learning (Turney, 2002; Rao and Ravichandran, 2009). Often lexicons are combined with machine learning for improved results (Wilson et al., 2005). The pervasiveness and sustained use of lexicons can be ascribed to a number of reasons, including their interpretability in large-scale systems as well as the granularity of their analysis. In this work we investigate the viability of polarity lexicons that are derived solely from unlabeled web documents. We propose a method based on graph propagation algorithms inspired by previous work on constructing polarity lexicons from lexical graphs (Kim and Hovy, 2004; Hu and Liu, 2004; Esuli and Sabastiani"
N10-1119,W03-1014,0,0.0521372,"ntained 178,104 entries. Depending on the threshold γ (see Figure 1), this lexicon could be larger or smaller. As stated earlier, our selection of γ and all hyperparameters was based on manual inspection of the resulting lexicons and performance on held-out data. In the rest of this section we investigate the properties of this lexicon to understand both its general characteristics as well as its possible utility in sentiment applications. To this end we compare three different lexicons: 1. Wilson et al.: Described in Wilson et al. (2005). Lexicon constructed by combining the lexicon built in Riloff and Wiebe (2003) with other sources1 . Entries are are coarsely rated – strong/weak positive/negative – which we weighted as 1.0, 0.5, -0.5, and -1.0 for our experiments. 2. WordNet LP: Described in Blair-Goldensohn et al. (2008). Constructed using label propagation over a graph derived from WordNet synonym and antonym links. Note that label propagation is not prone to the kinds of errors discussed in Section 2.3 since the lexical graph is derived from a high quality source. 3. Web GP: The web-derived lexicon described in Section 2.1 and Section 2.2. 1 See http://www.cs.pitt.edu/mpqa/ 3.1 Qualitative Evaluati"
N10-1119,P02-1053,0,0.0432157,"specific word classes – e.g., adjectives that occur in WordNet – and in fact contains slang, misspellings, multiword expressions, etc. We evaluate a lexicon derived from English documents, both qualitatively and quantitatively, and show that it provides superior performance to previously studied lexicons, including one derived from WordNet. 1 Introduction Polarity lexicons are large lists of phrases that encode the polarity of each phrase within it – either positive or negative – often with some score representing the magnitude of the polarity (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Turney, 2002). Though classifiers built with machine learning algorithms have become commonplace in the sentiment analysis literature, e.g., Pang et al. (2002), the core of many academic and commercial sentiment analysis systems remains the polarity lexicon, which can be constructed manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004; Esuli and Sabastiani, 2009) or using machine learning (Turney, 2002; Rao and Ravichandran, 2009). Often lexicons are combined with machine learning for improved results (Wilson et al., 2005). The pervasiveness and sustained use of lexicons can be ascribed to"
N10-1119,H05-1044,0,0.898368,"agnitude of the polarity (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Turney, 2002). Though classifiers built with machine learning algorithms have become commonplace in the sentiment analysis literature, e.g., Pang et al. (2002), the core of many academic and commercial sentiment analysis systems remains the polarity lexicon, which can be constructed manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004; Esuli and Sabastiani, 2009) or using machine learning (Turney, 2002; Rao and Ravichandran, 2009). Often lexicons are combined with machine learning for improved results (Wilson et al., 2005). The pervasiveness and sustained use of lexicons can be ascribed to a number of reasons, including their interpretability in large-scale systems as well as the granularity of their analysis. In this work we investigate the viability of polarity lexicons that are derived solely from unlabeled web documents. We propose a method based on graph propagation algorithms inspired by previous work on constructing polarity lexicons from lexical graphs (Kim and Hovy, 2004; Hu and Liu, 2004; Esuli and Sabastiani, 2009; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009). Whereas past efforts have"
N10-1119,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
N12-1052,abeille-barrier-2004-enriching,0,0.0101402,"Missing"
N12-1052,apresjan-etal-2006-syntactically,0,0.0188364,"Missing"
N12-1052,P10-1131,0,0.0123682,"rian et al. (2010). 4 Cross-lingual Word Cluster Features All results of the previous section rely on the availability of large quantities of language specific annotations for each task. Cross-lingual transfer methods and unsupervised methods have recently been shown to hold promise as a way to at least partially sidestep the demand for labeled data. Unsupervised methods attempt to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al., 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). The aim of transfer methods is instead to use knowledge induced from labeled resources in one or more source languages to construct systems for target languages in which no or few such resources are available (Hwa et al., 2005). Currently, the performance of even the most simple direct transfer systems far exceeds that of unsupervised systems (Cohen et al., 2011; McDonald et al., 2011; Søgaard, 2011). Figure 1: Cross-lingual word cluster features for parsing. Top-left: Cross-lingual (EN-ES) word clustering model. Top-right: Samples of some of the induced cross-lingual word clusters. Bottom-l"
N12-1052,C00-2143,0,0.0543358,"Missing"
N12-1052,J92-4003,0,0.166511,"al., 2004; Turian et al., 2010; Faruqui and Pad´o, 2010). Intuitively, the reason for the effectiveness of cluster features lie in their ability to aggregate local distributional information from large unlabeled corpora, which aid in conquering data sparsity in supervised training regimes as well as in mitigating cross-domain generalization issues. 478 In line with much previous work on word clusters for tasks such as dependency parsing and NER, for which local syntactic and semantic constraints are of importance, we induce word clusters by means of a probabilistic class-based language model (Brown et al., 1992; Clark, 2003). However, rather than the more commonly used model of Brown et al. (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). The two models are very similar, but whereas the former takes classto-class transitions into account, the latter directly models word-to-class transitions. By ignoring classto-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). This is a useful property, as we later develop an algorithm for inducing cross-lingual word"
N12-1052,W06-2920,0,0.201684,"translation (Chiang, ∗ The majority of this work was performed while the author was an intern at Google, New York, NY. Jakob Uszkoreit Google Mountain View, CA uszkoreit@google.com 2005; Collins et al., 2005). Currently, supervised data-driven methods dominate the literature on linguistic structure prediction (Smith, 2011). Regrettably, the majority of studies on these methods have focused on evaluations specific to English, since it is the language with the most annotated resources. Notable exceptions include the CoNLL shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003; Buchholz and Marsi, 2006; Nivre et al., 2007) and subsequent studies on this data, as well as a number of focused studies on one or two specific languages, as discussed by Bender (2011). While annotated resources for parsing and several other tasks are available in a number of languages, we cannot expect to have access to labeled resources for all tasks in all languages. This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al., 2008) and transfer (Hwa et al., 2005) systems for prediction of linguistic structure. These methods all attempt to benefit fr"
N12-1052,candito-etal-2010-statistical,0,0.0101679,"Missing"
N12-1052,P05-1033,0,0.0592324,"Missing"
N12-1052,E03-1009,0,0.0650348,"al., 2010; Faruqui and Pad´o, 2010). Intuitively, the reason for the effectiveness of cluster features lie in their ability to aggregate local distributional information from large unlabeled corpora, which aid in conquering data sparsity in supervised training regimes as well as in mitigating cross-domain generalization issues. 478 In line with much previous work on word clusters for tasks such as dependency parsing and NER, for which local syntactic and semantic constraints are of importance, we induce word clusters by means of a probabilistic class-based language model (Brown et al., 1992; Clark, 2003). However, rather than the more commonly used model of Brown et al. (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). The two models are very similar, but whereas the former takes classto-class transitions into account, the latter directly models word-to-class transitions. By ignoring classto-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). This is a useful property, as we later develop an algorithm for inducing cross-lingual word clusters that"
N12-1052,N09-1009,0,0.0150512,"s in all languages. This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al., 2008) and transfer (Hwa et al., 2005) systems for prediction of linguistic structure. These methods all attempt to benefit from the plethora of unlabeled monolingual and/or cross-lingual data that has become available in the digital age. Unsupervised methods are appealing in that they are often inherently language independent. This is borne out by the many recent studies on unsupervised parsing that include evaluations covering a number of languages (Cohen and Smith, 2009; Gillenwater et al., 2010; Naseem et al., 2010; Spitkovsky et al., 2011). However, the performance for most languages is still well below that of supervised systems and recent work has established that the performance is also below simple methods of linguistic transfer (McDonald et al., 2011). In this study we focus on semi-supervised and linguistic-transfer methods for multilingual structure prediction. In particular, we pursue two lines of research around the use of word cluster features in discriminative models for structure prediction: 477 2012 Conference of the North American Chapter of"
N12-1052,D11-1005,0,0.0786587,"to infer linguistic structure without using any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al., 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). The aim of transfer methods is instead to use knowledge induced from labeled resources in one or more source languages to construct systems for target languages in which no or few such resources are available (Hwa et al., 2005). Currently, the performance of even the most simple direct transfer systems far exceeds that of unsupervised systems (Cohen et al., 2011; McDonald et al., 2011; Søgaard, 2011). Figure 1: Cross-lingual word cluster features for parsing. Top-left: Cross-lingual (EN-ES) word clustering model. Top-right: Samples of some of the induced cross-lingual word clusters. Bottom-left: Delexicalized cluster-augmented source (EN) treebank for training transfer parser. Bottom-right: Parsing of target (ES) sentence using the transfer parser. 4.1 Direct Transfer of Discriminative Models Our starting point is the delexicalized direct transfer method proposed by McDonald et al. (2011) based on work by Zeman and Resnik (2008). This method was show"
N12-1052,P05-1066,0,0.141151,"Missing"
N12-1052,P97-1003,0,0.107857,"Missing"
N12-1052,P11-1061,0,0.71549,"ace of fine-grained statistics such as lexical word identities. Second, that the parameters of features over coarsegrained statistics are in some sense language inde481 pendent, e.g., a feature that indicates that adjectives modify their closest noun is useful in all languages. Third, that these coarse-grained statistics are robustly available across languages. The approach proposed by McDonald et al. (2011) relies on these three assumptions. Specifically, by replacing fine-grained language specific part-of-speech tags with universal part-of-speech tags, generated with the method described by Das and Petrov (2011), a universal parser is achieved that can be applied to any language for which universal part-of-speech tags are available. Below, we extend this approach to universal parsing by adding cross-lingual word cluster features. A cross-lingual word clustering is a clustering of words in two languages, in which the clusters correspond to some meaningful cross-lingual property. For example, prepositions from both languages should be in the same cluster, proper names from both languages in another cluster and so on. By adding features defined over these clusters, we can, to some degree, re-lexicalize"
N12-1052,de-marneffe-etal-2006-generating,0,0.0212917,"Missing"
N12-1052,P11-1043,0,0.0156369,"Missing"
N12-1052,W04-3234,0,0.0952298,"In particular, we pursue two lines of research around the use of word cluster features in discriminative models for structure prediction: 477 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477–487, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics 1. Monolingual word cluster features induced from large corpora of text for semi-supervised learning (SSL) of linguistic structure. Previous studies on this approach have typically focused only on a small set of languages and tasks (Freitag, 2004; Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; Haffari et al., 2011; Tratz and Hovy, 2011). Here we show that this method is robust across 13 languages for dependency parsing and 4 languages for named-entity recognition (NER). This is the first study with such a broad view on this subject, in terms of language diversity. 2. Cross-lingual word cluster features for transferring linguistic structure from English to other languages. We develop an algorithm that generates cross-lingual word clusters; that is clusters of words that are consistent across langua"
N12-1052,P10-2036,0,0.0134119,"Missing"
N12-1052,P11-2125,0,0.00794262,"ative models for structure prediction: 477 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477–487, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics 1. Monolingual word cluster features induced from large corpora of text for semi-supervised learning (SSL) of linguistic structure. Previous studies on this approach have typically focused only on a small set of languages and tasks (Freitag, 2004; Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; Haffari et al., 2011; Tratz and Hovy, 2011). Here we show that this method is robust across 13 languages for dependency parsing and 4 languages for named-entity recognition (NER). This is the first study with such a broad view on this subject, in terms of language diversity. 2. Cross-lingual word cluster features for transferring linguistic structure from English to other languages. We develop an algorithm that generates cross-lingual word clusters; that is clusters of words that are consistent across languages. This is achieved by means of a probabilistic model over large amounts of monolingual data in two langu"
N12-1052,han-etal-2002-development,0,0.0257318,"Missing"
N12-1052,P04-1061,0,0.2441,"e language with the most annotated resources. Notable exceptions include the CoNLL shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003; Buchholz and Marsi, 2006; Nivre et al., 2007) and subsequent studies on this data, as well as a number of focused studies on one or two specific languages, as discussed by Bender (2011). While annotated resources for parsing and several other tasks are available in a number of languages, we cannot expect to have access to labeled resources for all tasks in all languages. This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al., 2008) and transfer (Hwa et al., 2005) systems for prediction of linguistic structure. These methods all attempt to benefit from the plethora of unlabeled monolingual and/or cross-lingual data that has become available in the digital age. Unsupervised methods are appealing in that they are often inherently language independent. This is borne out by the many recent studies on unsupervised parsing that include evaluations covering a number of languages (Cohen and Smith, 2009; Gillenwater et al., 2010; Naseem et al., 2010; Spitkovsky et al., 2011). However, the perfo"
N12-1052,P08-1068,0,0.762357,"s. Notable exceptions include the CoNLL shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003; Buchholz and Marsi, 2006; Nivre et al., 2007) and subsequent studies on this data, as well as a number of focused studies on one or two specific languages, as discussed by Bender (2011). While annotated resources for parsing and several other tasks are available in a number of languages, we cannot expect to have access to labeled resources for all tasks in all languages. This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al., 2008) and transfer (Hwa et al., 2005) systems for prediction of linguistic structure. These methods all attempt to benefit from the plethora of unlabeled monolingual and/or cross-lingual data that has become available in the digital age. Unsupervised methods are appealing in that they are often inherently language independent. This is borne out by the many recent studies on unsupervised parsing that include evaluations covering a number of languages (Cohen and Smith, 2009; Gillenwater et al., 2010; Naseem et al., 2010; Spitkovsky et al., 2011). However, the performance for most languages is still w"
N12-1052,P09-1116,0,0.0419587,"talized. →/f - Transition from previous to current label conjoined with feature f . w:j : j-character prefix of w. w−j: : j-character suffix of w. fi : Feature f at relative position i. fi,j : Union of features at positions i and j. fi:j : Conjoined feature sequence between relative positions i and j (inclusive). b: Bias. languages in which our unlabeled data did not have at least 1 million types, we considered all types. 3.1 embedding method based on canonical correlation analysis that provides state-of-the art results for wordbased SSL for English NER. As an alternative to clustering words, Lin and Wu (2009) proposed a phrase clustering approach that obtained the state-of-the-art result for English NER. 3 Monolingual Cluster Experiments Before moving on to the multilingual setting, we conduct a set of monolingual experiments where we evaluate the use of the monolingual word clusters just described as features for dependency parsing and NER. In the parsing experiments, we study the following thirteen languages:1 Danish (DA), German (DE), Greek (EL), English (EN), Spanish (ES), French (FR), Italian (IT), Korean (KO), Dutch (NL), Portugese (PT), Russian (RU), Swedish (SV) and Chinese (ZH) – represen"
N12-1052,J93-2004,0,0.0470271,"Missing"
N12-1052,D11-1006,1,0.664557,"beled monolingual and/or cross-lingual data that has become available in the digital age. Unsupervised methods are appealing in that they are often inherently language independent. This is borne out by the many recent studies on unsupervised parsing that include evaluations covering a number of languages (Cohen and Smith, 2009; Gillenwater et al., 2010; Naseem et al., 2010; Spitkovsky et al., 2011). However, the performance for most languages is still well below that of supervised systems and recent work has established that the performance is also below simple methods of linguistic transfer (McDonald et al., 2011). In this study we focus on semi-supervised and linguistic-transfer methods for multilingual structure prediction. In particular, we pursue two lines of research around the use of word cluster features in discriminative models for structure prediction: 477 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477–487, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics 1. Monolingual word cluster features induced from large corpora of text for semi-supervised learning (SSL) of linguisti"
N12-1052,N04-1043,0,0.288367,"we pursue two lines of research around the use of word cluster features in discriminative models for structure prediction: 477 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477–487, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics 1. Monolingual word cluster features induced from large corpora of text for semi-supervised learning (SSL) of linguistic structure. Previous studies on this approach have typically focused only on a small set of languages and tasks (Freitag, 2004; Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; Haffari et al., 2011; Tratz and Hovy, 2011). Here we show that this method is robust across 13 languages for dependency parsing and 4 languages for named-entity recognition (NER). This is the first study with such a broad view on this subject, in terms of language diversity. 2. Cross-lingual word cluster features for transferring linguistic structure from English to other languages. We develop an algorithm that generates cross-lingual word clusters; that is clusters of words that are consistent across languages. This is achieved"
N12-1052,D10-1120,0,0.169107,"large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al., 2008) and transfer (Hwa et al., 2005) systems for prediction of linguistic structure. These methods all attempt to benefit from the plethora of unlabeled monolingual and/or cross-lingual data that has become available in the digital age. Unsupervised methods are appealing in that they are often inherently language independent. This is borne out by the many recent studies on unsupervised parsing that include evaluations covering a number of languages (Cohen and Smith, 2009; Gillenwater et al., 2010; Naseem et al., 2010; Spitkovsky et al., 2011). However, the performance for most languages is still well below that of supervised systems and recent work has established that the performance is also below simple methods of linguistic transfer (McDonald et al., 2011). In this study we focus on semi-supervised and linguistic-transfer methods for multilingual structure prediction. In particular, we pursue two lines of research around the use of word cluster features in discriminative models for structure prediction: 477 2012 Conference of the North American Chapter of the Association for Computational Linguistics:"
N12-1052,J08-4003,0,0.0108857,"pes in each language for both efficiency and sparsity reasons. For 1 The particular choice of languages was made purely based on data availability and institution licensing. 479 w−1,0,1 , w−1:0 , w0:1 , w−1:1 , b :1,:2,:3,:4,:5 −5:,−4:,−3:,−2:,−1: w−1,0,1 , w−1,0,1 Hyp−1,0,1 , Cap−1,0,1 , Cap−1:0 , Cap0:1 , Cap−1:1 p−1,0,1 , p−1:0 , p0:1 , p−1:1 , p−2:1 , p−1:2 c−1,0,1 , c−1:0 , c0:1 , c−1:1 , c−2:1 , c−1:2 →/p−1,0,1 ,→/c−1,0,1 ,→/Cap−1,0,1 ,→/b Cluster Augmented Feature Models All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). For all languages and settings, we use an arc-eager decoding strategy, with a beam of eight hypotheses, and perform ten epochs of the averaged structured perceptron algorithm (Zhang and Clark, 2008). We extend the state-of-the-art feature model recently introduced by Zhang and Nivre (2011) by adding an additional word cluster based feature template for each word based template. Additionally, we add templates where one or more partof-speech feature is replaced with the corresponding cluster feature. The resulting set of additional feature templates are shown in Table 1. The expanded feature m"
N12-1052,E99-1010,0,0.0785594,"Missing"
N12-1052,W96-0213,0,0.383863,"Missing"
N12-1052,P11-2120,0,0.0621116,"g any annotated data (Klein and Manning, 2004) or possibly by using a set of linguistically motivated rules (Naseem et al., 2010) or a linguistically informed model structure (Berg-Kirkpatrick and Klein, 2010). The aim of transfer methods is instead to use knowledge induced from labeled resources in one or more source languages to construct systems for target languages in which no or few such resources are available (Hwa et al., 2005). Currently, the performance of even the most simple direct transfer systems far exceeds that of unsupervised systems (Cohen et al., 2011; McDonald et al., 2011; Søgaard, 2011). Figure 1: Cross-lingual word cluster features for parsing. Top-left: Cross-lingual (EN-ES) word clustering model. Top-right: Samples of some of the induced cross-lingual word clusters. Bottom-left: Delexicalized cluster-augmented source (EN) treebank for training transfer parser. Bottom-right: Parsing of target (ES) sentence using the transfer parser. 4.1 Direct Transfer of Discriminative Models Our starting point is the delexicalized direct transfer method proposed by McDonald et al. (2011) based on work by Zeman and Resnik (2008). This method was shown to outperform a number of state-of-th"
N12-1052,D11-1117,0,0.0138253,"ch on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al., 2008) and transfer (Hwa et al., 2005) systems for prediction of linguistic structure. These methods all attempt to benefit from the plethora of unlabeled monolingual and/or cross-lingual data that has become available in the digital age. Unsupervised methods are appealing in that they are often inherently language independent. This is borne out by the many recent studies on unsupervised parsing that include evaluations covering a number of languages (Cohen and Smith, 2009; Gillenwater et al., 2010; Naseem et al., 2010; Spitkovsky et al., 2011). However, the performance for most languages is still well below that of supervised systems and recent work has established that the performance is also below simple methods of linguistic transfer (McDonald et al., 2011). In this study we focus on semi-supervised and linguistic-transfer methods for multilingual structure prediction. In particular, we pursue two lines of research around the use of word cluster features in discriminative models for structure prediction: 477 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologie"
N12-1052,W03-0419,0,0.121472,"Missing"
N12-1052,W02-2024,0,0.15029,"r many important downstream tasks such as machine translation (Chiang, ∗ The majority of this work was performed while the author was an intern at Google, New York, NY. Jakob Uszkoreit Google Mountain View, CA uszkoreit@google.com 2005; Collins et al., 2005). Currently, supervised data-driven methods dominate the literature on linguistic structure prediction (Smith, 2011). Regrettably, the majority of studies on these methods have focused on evaluations specific to English, since it is the language with the most annotated resources. Notable exceptions include the CoNLL shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003; Buchholz and Marsi, 2006; Nivre et al., 2007) and subsequent studies on this data, as well as a number of focused studies on one or two specific languages, as discussed by Bender (2011). While annotated resources for parsing and several other tasks are available in a number of languages, we cannot expect to have access to labeled resources for all tasks in all languages. This fact has given rise to a large body of research on unsupervised (Klein and Manning, 2004), semi-supervised (Koo et al., 2008) and transfer (Hwa et al., 2005) systems for prediction o"
N12-1052,D11-1116,0,0.0215533,"ture prediction: 477 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477–487, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics 1. Monolingual word cluster features induced from large corpora of text for semi-supervised learning (SSL) of linguistic structure. Previous studies on this approach have typically focused only on a small set of languages and tasks (Freitag, 2004; Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; Haffari et al., 2011; Tratz and Hovy, 2011). Here we show that this method is robust across 13 languages for dependency parsing and 4 languages for named-entity recognition (NER). This is the first study with such a broad view on this subject, in terms of language diversity. 2. Cross-lingual word cluster features for transferring linguistic structure from English to other languages. We develop an algorithm that generates cross-lingual word clusters; that is clusters of words that are consistent across languages. This is achieved by means of a probabilistic model over large amounts of monolingual data in two languages, coupled with para"
N12-1052,P10-1040,0,0.692482,"the use of word cluster features in discriminative models for structure prediction: 477 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 477–487, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics 1. Monolingual word cluster features induced from large corpora of text for semi-supervised learning (SSL) of linguistic structure. Previous studies on this approach have typically focused only on a small set of languages and tasks (Freitag, 2004; Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; Haffari et al., 2011; Tratz and Hovy, 2011). Here we show that this method is robust across 13 languages for dependency parsing and 4 languages for named-entity recognition (NER). This is the first study with such a broad view on this subject, in terms of language diversity. 2. Cross-lingual word cluster features for transferring linguistic structure from English to other languages. We develop an algorithm that generates cross-lingual word clusters; that is clusters of words that are consistent across languages. This is achieved by means of a probabilistic model over"
N12-1052,P08-1086,1,0.909461,"distributional information from large unlabeled corpora, which aid in conquering data sparsity in supervised training regimes as well as in mitigating cross-domain generalization issues. 478 In line with much previous work on word clusters for tasks such as dependency parsing and NER, for which local syntactic and semantic constraints are of importance, we induce word clusters by means of a probabilistic class-based language model (Brown et al., 1992; Clark, 2003). However, rather than the more commonly used model of Brown et al. (1992), we use the predictive class bigram model introduced by Uszkoreit and Brants (2008). The two models are very similar, but whereas the former takes classto-class transitions into account, the latter directly models word-to-class transitions. By ignoring classto-class transitions, an approximate maximum likelihood clustering can be found efficiently with the distributed exchange algorithm (Uszkoreit and Brants, 2008). This is a useful property, as we later develop an algorithm for inducing cross-lingual word clusters that calls this monolingual algorithm as a subroutine. More formally, let C : V 7→ 1, . . . , K be a (hard) clustering function that maps each word type from the"
N12-1052,I08-3008,0,0.627021,"Missing"
N12-1052,D08-1059,0,0.012038,"1:0 , w0:1 , w−1:1 , b :1,:2,:3,:4,:5 −5:,−4:,−3:,−2:,−1: w−1,0,1 , w−1,0,1 Hyp−1,0,1 , Cap−1,0,1 , Cap−1:0 , Cap0:1 , Cap−1:1 p−1,0,1 , p−1:0 , p0:1 , p−1:1 , p−2:1 , p−1:2 c−1,0,1 , c−1:0 , c0:1 , c−1:1 , c−2:1 , c−1:2 →/p−1,0,1 ,→/c−1,0,1 ,→/Cap−1,0,1 ,→/b Cluster Augmented Feature Models All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). For all languages and settings, we use an arc-eager decoding strategy, with a beam of eight hypotheses, and perform ten epochs of the averaged structured perceptron algorithm (Zhang and Clark, 2008). We extend the state-of-the-art feature model recently introduced by Zhang and Nivre (2011) by adding an additional word cluster based feature template for each word based template. Additionally, we add templates where one or more partof-speech feature is replaced with the corresponding cluster feature. The resulting set of additional feature templates are shown in Table 1. The expanded feature model includes all of the feature templates defined by Zhang and Nivre (2011), which we also use as the baseline model, whereas Table 1 only shows our new templates due to space limitations. For all NE"
N12-1052,P11-2033,0,0.0240295,"1,0,1 , Cap−1:0 , Cap0:1 , Cap−1:1 p−1,0,1 , p−1:0 , p0:1 , p−1:1 , p−2:1 , p−1:2 c−1,0,1 , c−1:0 , c0:1 , c−1:1 , c−2:1 , c−1:2 →/p−1,0,1 ,→/c−1,0,1 ,→/Cap−1,0,1 ,→/b Cluster Augmented Feature Models All of the parsing experiments reported in this study are based on the transition-based dependency parsing paradigm (Nivre, 2008). For all languages and settings, we use an arc-eager decoding strategy, with a beam of eight hypotheses, and perform ten epochs of the averaged structured perceptron algorithm (Zhang and Clark, 2008). We extend the state-of-the-art feature model recently introduced by Zhang and Nivre (2011) by adding an additional word cluster based feature template for each word based template. Additionally, we add templates where one or more partof-speech feature is replaced with the corresponding cluster feature. The resulting set of additional feature templates are shown in Table 1. The expanded feature model includes all of the feature templates defined by Zhang and Nivre (2011), which we also use as the baseline model, whereas Table 1 only shows our new templates due to space limitations. For all NER experiments, we use a sequential firstorder conditional random field (CRF) with a unit var"
N12-1052,petrov-etal-2012-universal,1,\N,Missing
N12-1052,D07-1096,1,\N,Missing
N13-1126,P05-1022,0,0.20208,"Missing"
N13-1126,N09-1009,0,0.0242072,"k, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., ∗ Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010). A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by Naseem et al. (2012) for multisource cross-lingual transfer. In particular, Naseem et al. showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of parameters for all languages. However, these methods all employ generative models with strong independence assumptions and weak feature representations, which upper bounds their accuracy far below that of feature-rich discriminative parsers (McDonald et al., 2005; Ni"
N13-1126,D11-1005,0,0.216775,"OUN P Figure 1: A Greek sentence which is correctly parsed by a delexicalized English parser, provided that part-of-speech tags are available in both the source and target language. 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). Annotations in multiple languages can be combined in delexicalized transfer as well, as long as the parser features are available across the involved languages. This idea was explored by McDonald et al. (2011), who showed that target language accuracy can be improved by simply concatenating delexicalized treebanks in multiple languages. In similar work, Cohen et al. (2011) proposed a mixture model in which the parameters of a generative target language parser is expressed as a linear interpolation of source language parameters, whereas Søgaard (2011) showed that target side language models can be used to selectively subsample training sentences to improve accuracy. Recently, inspired by the phylogenetic prior of Berg-Kirkpatrick and Klein (2010), Søgaard and Wulff (2012) proposed — among other ideas — a typologically informed weighting heuristic for linearly interpolating source language parameters. However, this weighting did not provide significant improvemen"
N13-1126,P11-1061,0,0.119063,"e transfer parsers in an ensemble-training algorithm. Our final model outperforms the state of the art in multi-source transfer parsing on 15 out of 16 evaluated languages. 1 Introduction Many languages still lack access to core NLP tools, such as part-of-speech taggers and syntactic parsers. This is largely due to the reliance on fully supervised learning methods, which require large quantities of manually annotated training data. Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011) and syntactic dependency parsing (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., ∗ Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010). A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related lang"
N13-1126,P07-1033,0,0.0106786,"Missing"
N13-1126,D12-1001,0,0.440814,"t al., 2011; Søgaard, 2011). In contrast to annotation projection approaches (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009; Spreyer and Kuhn, 2009), delexicalized transfer methods do not rely on any bitext. Instead, a parser is trained on annotations in a source language, relying solely on features that are available in both the source and the target language, such as “universal” part-ofspeech tags (Zeman and Resnik, 2008; Naseem et al., 2010; Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012) or type-level features derived from bilingual dictionaries (Durrett et al., 2012).1 This parser is then directly used to parse the target language. For languages with similar typology, this method can be quite accurate, especially when compared to purely unsupervised methods. For instance, a parser trained on English with only part-of-speech features can correctly parse the Greek sentence in Figure 1, even without knowledge of the lexical items since the sequence of part-of-speech tags determines the syntactic structure almost unambiguously. Learning with multiple languages has been shown to benefit unsupervised learning (Cohen and Smith, 1 Note that Täckström et al. (2012"
N13-1126,C96-1058,0,0.260825,"Missing"
N13-1126,N09-1068,0,0.0477374,"Missing"
N13-1126,P09-1042,0,0.364312,"art in multi-source transfer parsing on 15 out of 16 evaluated languages. 1 Introduction Many languages still lack access to core NLP tools, such as part-of-speech taggers and syntactic parsers. This is largely due to the reliance on fully supervised learning methods, which require large quantities of manually annotated training data. Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011) and syntactic dependency parsing (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., ∗ Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010). A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder"
N13-1126,P04-1061,0,0.0287204,"ed learning methods, which require large quantities of manually annotated training data. Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011) and syntactic dependency parsing (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., ∗ Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010). A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by Naseem et al. (2012) for multisource cross-lingual transfer. In particular, Naseem et al. showed that by selectively sharing parameters based on typological features of each language, subs"
N13-1126,C12-1089,0,0.0235319,"ate, especially when compared to purely unsupervised methods. For instance, a parser trained on English with only part-of-speech features can correctly parse the Greek sentence in Figure 1, even without knowledge of the lexical items since the sequence of part-of-speech tags determines the syntactic structure almost unambiguously. Learning with multiple languages has been shown to benefit unsupervised learning (Cohen and Smith, 1 Note that Täckström et al. (2012) and Durrett et al. (2012) do require bitext or a bilingual dictionary. The same holds for most cross-lingual representations, e.g., Klementiev et al. (2012). 1062 ROOT PUNC DOBJ DET Ο NSUBJ POBJ PREP DET Τζόν έδωσε στην Μαρία το βιβλίο . (The) (John) (gave) (to-the) (Maria) (the) (book) . DET NOUN VERB ADP NOUN DET NOUN P Figure 1: A Greek sentence which is correctly parsed by a delexicalized English parser, provided that part-of-speech tags are available in both the source and target language. 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). Annotations in multiple languages can be combined in delexicalized transfer as well, as long as the parser features are available across the involved languages. This idea was explored by McDonal"
N13-1126,P06-1043,0,0.0598052,"Missing"
N13-1126,P05-1012,1,0.614302,"ng (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by Naseem et al. (2012) for multisource cross-lingual transfer. In particular, Naseem et al. showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of parameters for all languages. However, these methods all employ generative models with strong independence assumptions and weak feature representations, which upper bounds their accuracy far below that of feature-rich discriminative parsers (McDonald et al., 2005; Nivre, 2008). In this paper, we improve upon the state of the art in cross-lingual transfer of dependency parsers from multiple source languages by adapting feature-rich discriminatively trained parsers to a specific target language. First, in §4 we show how selective sharing of model parameters based on typological traits can be incorporated into a delexicalized discriminative graph-based parsing model. This requires a careful decomposition of features into language-generic and language-specific sets in order to tie specific target language parameters to their relevant source language count"
N13-1126,D11-1006,1,0.696635,"ransfer parsing on 15 out of 16 evaluated languages. 1 Introduction Many languages still lack access to core NLP tools, such as part-of-speech taggers and syntactic parsers. This is largely due to the reliance on fully supervised learning methods, which require large quantities of manually annotated training data. Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011) and syntactic dependency parsing (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., ∗ Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010). A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kir"
N13-1126,D10-1120,0,0.204747,"anually annotated training data. Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011) and syntactic dependency parsing (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., ∗ Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010). A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by Naseem et al. (2012) for multisource cross-lingual transfer. In particular, Naseem et al. showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to u"
N13-1126,P12-1066,0,0.744312,"out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010). A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by Naseem et al. (2012) for multisource cross-lingual transfer. In particular, Naseem et al. showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of parameters for all languages. However, these methods all employ generative models with strong independence assumptions and weak feature representations, which upper bounds their accuracy far below that of feature-rich discriminative parsers (McDonald et al., 2005; Nivre, 2008). In this paper, we improve upon the state of the art in cross-lingual transfer of"
N13-1126,J08-4003,1,0.328548,"09; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by Naseem et al. (2012) for multisource cross-lingual transfer. In particular, Naseem et al. showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of parameters for all languages. However, these methods all employ generative models with strong independence assumptions and weak feature representations, which upper bounds their accuracy far below that of feature-rich discriminative parsers (McDonald et al., 2005; Nivre, 2008). In this paper, we improve upon the state of the art in cross-lingual transfer of dependency parsers from multiple source languages by adapting feature-rich discriminatively trained parsers to a specific target language. First, in §4 we show how selective sharing of model parameters based on typological traits can be incorporated into a delexicalized discriminative graph-based parsing model. This requires a careful decomposition of features into language-generic and language-specific sets in order to tie specific target language parameters to their relevant source language counterparts. The r"
N13-1126,petrov-etal-2012-universal,1,0.100225,"fall into the delexicalized transfer approach to multilingual syntactic parsing (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Søgaard, 2011). In contrast to annotation projection approaches (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009; Spreyer and Kuhn, 2009), delexicalized transfer methods do not rely on any bitext. Instead, a parser is trained on annotations in a source language, relying solely on features that are available in both the source and the target language, such as “universal” part-ofspeech tags (Zeman and Resnik, 2008; Naseem et al., 2010; Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012) or type-level features derived from bilingual dictionaries (Durrett et al., 2012).1 This parser is then directly used to parse the target language. For languages with similar typology, this method can be quite accurate, especially when compared to purely unsupervised methods. For instance, a parser trained on English with only part-of-speech features can correctly parse the Greek sentence in Figure 1, even without knowledge of the lexical items since the sequence of part-of-speech tags determines the syntactic structure almost unambiguousl"
N13-1126,P02-1035,0,0.0630198,"Missing"
N13-1126,N06-2033,0,0.225801,"Missing"
N13-1126,D07-1111,0,0.0551291,"Missing"
N13-1126,N09-1010,0,0.0669165,", 2009; McDonald et al., 2011; Naseem et al., ∗ Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010). A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen and Smith, 2009; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010) and recently by Naseem et al. (2012) for multisource cross-lingual transfer. In particular, Naseem et al. showed that by selectively sharing parameters based on typological features of each language, substantial improvements can be achieved, compared to using a single set of parameters for all languages. However, these methods all employ generative models with strong independence assumptions and weak feature representations, which upper bounds their accuracy far below that of feature-rich discriminative parsers (McDonald et al., 2005; Nivre, 2008). In this p"
N13-1126,C12-2115,0,0.434966,"es. This idea was explored by McDonald et al. (2011), who showed that target language accuracy can be improved by simply concatenating delexicalized treebanks in multiple languages. In similar work, Cohen et al. (2011) proposed a mixture model in which the parameters of a generative target language parser is expressed as a linear interpolation of source language parameters, whereas Søgaard (2011) showed that target side language models can be used to selectively subsample training sentences to improve accuracy. Recently, inspired by the phylogenetic prior of Berg-Kirkpatrick and Klein (2010), Søgaard and Wulff (2012) proposed — among other ideas — a typologically informed weighting heuristic for linearly interpolating source language parameters. However, this weighting did not provide significant improvements over uniform weighting. The aforementioned approaches work well for transfer between similar languages. However, their assumptions cease to hold for typologically divergent languages; a target language can rarely be described as a linear combination of data or model parameters from a set of source languages, as languages tend to share varied typological traits; this critical insight is discussed furt"
N13-1126,P11-2120,0,0.254226,"; Snyder et al., 2009; Berg-Kirkpatrick and Klein, 2010). Annotations in multiple languages can be combined in delexicalized transfer as well, as long as the parser features are available across the involved languages. This idea was explored by McDonald et al. (2011), who showed that target language accuracy can be improved by simply concatenating delexicalized treebanks in multiple languages. In similar work, Cohen et al. (2011) proposed a mixture model in which the parameters of a generative target language parser is expressed as a linear interpolation of source language parameters, whereas Søgaard (2011) showed that target side language models can be used to selectively subsample training sentences to improve accuracy. Recently, inspired by the phylogenetic prior of Berg-Kirkpatrick and Klein (2010), Søgaard and Wulff (2012) proposed — among other ideas — a typologically informed weighting heuristic for linearly interpolating source language parameters. However, this weighting did not provide significant improvements over uniform weighting. The aforementioned approaches work well for transfer between similar languages. However, their assumptions cease to hold for typologically divergent langu"
N13-1126,W09-1104,0,0.425983,"Missing"
N13-1126,N12-1052,1,0.41114,"ltilingual syntactic parsing (Zeman and Resnik, 2008; McDonald et al., 2011; Cohen et al., 2011; Søgaard, 2011). In contrast to annotation projection approaches (Yarowsky et al., 2001; Hwa et al., 2005; Ganchev et al., 2009; Spreyer and Kuhn, 2009), delexicalized transfer methods do not rely on any bitext. Instead, a parser is trained on annotations in a source language, relying solely on features that are available in both the source and the target language, such as “universal” part-ofspeech tags (Zeman and Resnik, 2008; Naseem et al., 2010; Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012) or type-level features derived from bilingual dictionaries (Durrett et al., 2012).1 This parser is then directly used to parse the target language. For languages with similar typology, this method can be quite accurate, especially when compared to purely unsupervised methods. For instance, a parser trained on English with only part-of-speech features can correctly parse the Greek sentence in Figure 1, even without knowledge of the lexical items since the sequence of part-of-speech tags determines the syntactic structure almost unambiguously. Learning with multiple languages has been shown to"
N13-1126,W12-1908,1,0.889289,"Missing"
N13-1126,H01-1035,0,0.0353175,"redictions from multiple transfer parsers in an ensemble-training algorithm. Our final model outperforms the state of the art in multi-source transfer parsing on 15 out of 16 evaluated languages. 1 Introduction Many languages still lack access to core NLP tools, such as part-of-speech taggers and syntactic parsers. This is largely due to the reliance on fully supervised learning methods, which require large quantities of manually annotated training data. Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011) and syntactic dependency parsing (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., ∗ Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010). A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model paramet"
N13-1126,I08-3008,0,0.646019,"erforms the state of the art in multi-source transfer parsing on 15 out of 16 evaluated languages. 1 Introduction Many languages still lack access to core NLP tools, such as part-of-speech taggers and syntactic parsers. This is largely due to the reliance on fully supervised learning methods, which require large quantities of manually annotated training data. Recently, methods for cross-lingual transfer have appeared as a promising avenue for overcoming this hurdle for both part-of-speech tagging (Yarowsky et al., 2001; Das and Petrov, 2011) and syntactic dependency parsing (Hwa et al., 2005; Zeman and Resnik, 2008; Ganchev et al., 2009; McDonald et al., 2011; Naseem et al., ∗ Work primarily carried out while at Google, NY. 2012). While these methods do not yet compete with fully supervised approaches, they can drastically outperform both unsupervised methods (Klein and Manning, 2004) and weakly supervised methods (Naseem et al., 2010; Berg-Kirkpatrick and Klein, 2010). A promising approach to cross-lingual transfer of syntactic dependency parsers is to use multiple source languages and to tie model parameters across related languages. This idea was first explored for weakly supervised learning (Cohen a"
N13-1126,P10-1131,0,\N,Missing
N15-1068,W06-2922,0,0.874176,"eration has a clear interpretation in terms of constraints on the final output tree (Goldberg and Nivre, 2012), which allows for more robust learning procedures (Goldberg and Nivre, 2012). The arc-eager system, however, cannot produce trees with crossing arcs. Alternative systems can produce crossing dependencies, but at the cost of taking O(n2 ) transitions in the worst case (Nivre, 2008; Nivre, 2009; Choi and McCallum, 2013), requiring more transitions than arc-eager to produce projective trees (Nivre, 2008; G´omez-Rodr´ıguez and Nivre, 2010), or producing trees in an unknown output class1 (Attardi, 2006). Graph-based non-projective parsing algorithms, on the other hand, have been able to preserve many of the attractive properties of their corresponding projective parsing algorithms by restricting search to classes of mildly non-projective trees (Kuhlmann and Nivre, 2006). Mildly non-projective classes of trees are characterizable subsets of directed trees. Classes of particular interest are those that both have high empirical coverage and that can be parsed efficiently. With appropriate definitions of feature functions and output spaces, exact higher-order graph-based non-projective parsers c"
N15-1068,Q13-1034,0,0.0150442,"educed, and since σ2 does not have a parent to its right, it is not a descendant of σ1 , and so after Hans becomes the new σ1 , the system makes the update that R1, R2 9A σ1 . 668 7 Experiments The experiments compare the two-registers transition system for mildly non-projective trees proposed here with two other transition systems: the arceager system for projective trees (Nivre, 2003) and the swap-based system for all non-projective trees (Nivre, 2009). We choose the swap-based system as our non-projective baseline as it currently represents the state-of-the-art in transition-based parsing (Bohnet et al., 2013), with higher empirical performance than the Attardi system or pseudo-projective parsing (Kuhlmann and Nivre, 2010). The arc-eager system is a reimplementation of Zhang and Nivre (2011), using their rich feature set and beam search. The features for the two other transition systems are based on the same set, but with slight modifications to account for the different relevant domains of locality. In particular, for the swap transition system, we updated the features to account for the fact that this transition system is based on the arc-standard model and so the most relevant positions are the"
N15-1068,W06-2920,0,0.256534,"xperiments use beam search with a beam of size 32 and are trained with ten iterations of averaged structured perceptron training. Training set trees that are outside of the reachable class (projective for arc-eager, 2-Crossing Intervals for two-registers) are transformed by lifting arcs (Nivre and Nilsson, 2005) until the tree is within the class. The test sets are left unchanged. We use the standard technique of parameterizing arc creating actions with dependency labels to produce labeled dependency trees. Experiments use the ten datasets in Table 1 from the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). We report numbers using both gold and automatically predicted part-of-speech tags and morphological attribute-values as features. For the latter, the part of speech tagger is a first-order CRF model and the morphological tagger uses a greedy SVM perattribute classifier. Evaluation uses CoNLL-X scoring conventions (Buchholz and Marsi, 2006) and we report both labeled and unlabeled attachment scores. Language Basque Czech Dutch English German Greek Hungarian Portuguese Slovene Turkish Average eager 70.50 (78.06) 79.60 (85.55) 78.69 (81.41) 90.00 (91.18) 88.34 (91.01) 77.34"
N15-1068,P13-1104,0,0.313727,"attractive property of the arc-eager system is the close connection between the parameterization of the parsing problem and the final predicted output structure. In the arc-eager model, each operation has a clear interpretation in terms of constraints on the final output tree (Goldberg and Nivre, 2012), which allows for more robust learning procedures (Goldberg and Nivre, 2012). The arc-eager system, however, cannot produce trees with crossing arcs. Alternative systems can produce crossing dependencies, but at the cost of taking O(n2 ) transitions in the worst case (Nivre, 2008; Nivre, 2009; Choi and McCallum, 2013), requiring more transitions than arc-eager to produce projective trees (Nivre, 2008; G´omez-Rodr´ıguez and Nivre, 2010), or producing trees in an unknown output class1 (Attardi, 2006). Graph-based non-projective parsing algorithms, on the other hand, have been able to preserve many of the attractive properties of their corresponding projective parsing algorithms by restricting search to classes of mildly non-projective trees (Kuhlmann and Nivre, 2006). Mildly non-projective classes of trees are characterizable subsets of directed trees. Classes of particular interest are those that both have"
N15-1068,C12-1059,0,0.177555,"of Nivre (2003) is one of the most popular for a variety of reasons. The arc-eager system has a well-defined output space: it can produce all projective trees and only projective trees. For an input sentence with n words, the arc-eager system always performs 2n operations and each operation takes constant time. Another attractive property of the arc-eager system is the close connection between the parameterization of the parsing problem and the final predicted output structure. In the arc-eager model, each operation has a clear interpretation in terms of constraints on the final output tree (Goldberg and Nivre, 2012), which allows for more robust learning procedures (Goldberg and Nivre, 2012). The arc-eager system, however, cannot produce trees with crossing arcs. Alternative systems can produce crossing dependencies, but at the cost of taking O(n2 ) transitions in the worst case (Nivre, 2008; Nivre, 2009; Choi and McCallum, 2013), requiring more transitions than arc-eager to produce projective trees (Nivre, 2008; G´omez-Rodr´ıguez and Nivre, 2010), or producing trees in an unknown output class1 (Attardi, 2006). Graph-based non-projective parsing algorithms, on the other hand, have been able to preserve m"
N15-1068,Q13-1033,0,0.025589,"l necessarily lead to an asymptotic increase in run-time as k approaches n. With larger values of k, the system would need additional transitions to add arcs between the registers (extending the Store transition to consider all subsets of arcs with the existing registers would become exponential in k). If k were to increase all the way to n, such a system would probably look very similar to list-based systems that consider all pairs of arcs (Covington, 2001; Nivre, 2008). Another direction would be to define dynamic oracles around the two-registers transition system (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013). The additional transitions here have interpretations in terms of which trees are still reachable (Register-Stack(·) adds an arc; Store and Clear indicate that particular vertices should be incident to crossed arcs or are finished with crossed arcs, respectively). The two-registers system is not quite arc-decomposable (Goldberg and Nivre, 2013): if the wrong vertex is stored in a register then a later pair of crossed arcs might both be individually reachable but not jointly reachable. However, there may be a “crossing-sensitive” variant of arcdecomposability that takes into account the vertic"
N15-1068,P10-1151,0,0.166694,"Missing"
N15-1068,J13-4002,0,0.203582,"Missing"
N15-1068,D10-1125,0,0.181687,"s paper) ⊂ 2-planar trees ⊂ all directed trees (Choi). The Choi system uses a quadratic number of transitions in the worst case, while arc-eager, 2-registers, and 2-planar all use at most O(n) transitions. Checking for cycles does not need to be done at all in the arc-eager system, can be with a few constant operations in the 2-registers system, and can be done in amortized constant time for the other systems (G´omez-Rodr´ıguez and Nivre, 2013). In the graph-based parsing literature, there has also been a plethora of work on non-projective parsing (McDonald et al., 2005; Martins et al., 2009; Koo et al., 2010). Recent work by Pitler and colleagues is the most relevant to the work described here (Pitler et al., 2012, 2013, 2014). Like this work, Pitler et al. define a restricted class of non-projective trees and then a graph-based parsing algorithm that parses exactly that set. The register mechanism in two-registers transition parsing bears a resemblance to registers in Augmented Transition Networks (ATNs) (Woods, 1970). In ATNs, global registers are introduced to account for a wide range of natural language phenomena. This includes long-distance dependencies, which is a common source of non-projec"
N15-1068,P06-2066,0,0.0316629,"Alternative systems can produce crossing dependencies, but at the cost of taking O(n2 ) transitions in the worst case (Nivre, 2008; Nivre, 2009; Choi and McCallum, 2013), requiring more transitions than arc-eager to produce projective trees (Nivre, 2008; G´omez-Rodr´ıguez and Nivre, 2010), or producing trees in an unknown output class1 (Attardi, 2006). Graph-based non-projective parsing algorithms, on the other hand, have been able to preserve many of the attractive properties of their corresponding projective parsing algorithms by restricting search to classes of mildly non-projective trees (Kuhlmann and Nivre, 2006). Mildly non-projective classes of trees are characterizable subsets of directed trees. Classes of particular interest are those that both have high empirical coverage and that can be parsed efficiently. With appropriate definitions of feature functions and output spaces, exact higher-order graph-based non-projective parsers can match the asymptotic time and space of higher-order projective parsers (Pitler, 2014). In this paper, we propose a class of mildly nonprojective trees (§3) and a transition system (§4) that is sound and complete with respect to this class (§5) while preserving desirabl"
N15-1068,J13-2004,0,0.018001,"es (even a single crossing implies two vertexdisjoint crossed edges). 2-Crossing Interval trees are a subset of the linguistically motivated 1-EndpointCrossing trees (Pitler et al., 2013) (each crossed edge is incident to one of the two vertices for the 664 interval, so all edges that cross it are incident to the other vertex for the interval); all of the examples from the linguistics literature provided in Pitler (2013, p.132-136) for 1-Endpoint-Crossing trees are 2-Crossing Interval trees as well. 2-Crossing Interval trees are not necessarily well-nested and can have unbounded block degree (Kuhlmann, 2013). Figure 2 shows an example of a 2-Crossing Interval tree (all crossed edges are incident to either a or b; no children are on the far side of their parent) in which the subtrees rooted at a and b are ill-nested and each has a block degree of n + 1. 4 Two-Registers Transition System A transition system for dependency parsing comprises: 1) an initial configuration for an input sentence; 2) a set of final configurations after which the parsing derivation terminates; and 3) a set of deterministic transitions for transitioning from one configuration to another (Nivre, 2008). Our transition system"
N15-1068,P09-1039,0,0.0424515,"ng Interval trees (this paper) ⊂ 2-planar trees ⊂ all directed trees (Choi). The Choi system uses a quadratic number of transitions in the worst case, while arc-eager, 2-registers, and 2-planar all use at most O(n) transitions. Checking for cycles does not need to be done at all in the arc-eager system, can be with a few constant operations in the 2-registers system, and can be done in amortized constant time for the other systems (G´omez-Rodr´ıguez and Nivre, 2013). In the graph-based parsing literature, there has also been a plethora of work on non-projective parsing (McDonald et al., 2005; Martins et al., 2009; Koo et al., 2010). Recent work by Pitler and colleagues is the most relevant to the work described here (Pitler et al., 2012, 2013, 2014). Like this work, Pitler et al. define a restricted class of non-projective trees and then a graph-based parsing algorithm that parses exactly that set. The register mechanism in two-registers transition parsing bears a resemblance to registers in Augmented Transition Networks (ATNs) (Woods, 1970). In ATNs, global registers are introduced to account for a wide range of natural language phenomena. This includes long-distance dependencies, which is a common s"
N15-1068,H05-1066,1,0.900319,"Missing"
N15-1068,P05-1013,0,0.293483,"tion system, we updated the features to account for the fact that this transition system is based on the arc-standard model and so the most relevant positions are the top two tokens on the stack. For the two-register system, we added features over properties of the tokens stored in each of the registers. All experiments use beam search with a beam of size 32 and are trained with ten iterations of averaged structured perceptron training. Training set trees that are outside of the reachable class (projective for arc-eager, 2-Crossing Intervals for two-registers) are transformed by lifting arcs (Nivre and Nilsson, 2005) until the tree is within the class. The test sets are left unchanged. We use the standard technique of parameterizing arc creating actions with dependency labels to produce labeled dependency trees. Experiments use the ten datasets in Table 1 from the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). We report numbers using both gold and automatically predicted part-of-speech tags and morphological attribute-values as features. For the latter, the part of speech tagger is a first-order CRF model and the morphological tagger uses a greedy SVM perattribute classif"
N15-1068,W03-3017,0,0.525753,"ser’s total run-time linear in the worst case. In empirical experiments, our proposed transition-based parser is more accurate on average than both the arc-eager system or the swap-based system, an unconstrained nonprojective transition system with a worst-case quadratic runtime (Nivre, 2009). 1 Introduction Linear-time transition-based parsers that use either greedy inference or beam search are widely used today due to their speed and accuracy (Nivre, 2008; Zhang and Clark, 2008; Zhang and Nivre, 2011). Of the many proposed transition systems (Nivre, 2008), the arc-eager transition system of Nivre (2003) is one of the most popular for a variety of reasons. The arc-eager system has a well-defined output space: it can produce all projective trees and only projective trees. For an input sentence with n words, the arc-eager system always performs 2n operations and each operation takes constant time. Another attractive property of the arc-eager system is the close connection between the parameterization of the parsing problem and the final predicted output structure. In the arc-eager model, each operation has a clear interpretation in terms of constraints on the final output tree (Goldberg and Niv"
N15-1068,J08-4003,0,0.81958,"arc-eager system for projective trees (Nivre, 2003). Crucially, this generalization only adds constant overhead in run-time and space keeping the parser’s total run-time linear in the worst case. In empirical experiments, our proposed transition-based parser is more accurate on average than both the arc-eager system or the swap-based system, an unconstrained nonprojective transition system with a worst-case quadratic runtime (Nivre, 2009). 1 Introduction Linear-time transition-based parsers that use either greedy inference or beam search are widely used today due to their speed and accuracy (Nivre, 2008; Zhang and Clark, 2008; Zhang and Nivre, 2011). Of the many proposed transition systems (Nivre, 2008), the arc-eager transition system of Nivre (2003) is one of the most popular for a variety of reasons. The arc-eager system has a well-defined output space: it can produce all projective trees and only projective trees. For an input sentence with n words, the arc-eager system always performs 2n operations and each operation takes constant time. Another attractive property of the arc-eager system is the close connection between the parameterization of the parsing problem and the final predicted"
N15-1068,P09-1040,0,0.767487,"ct We define a restricted class of non-projective trees that 1) covers many natural language sentences; and 2) can be parsed exactly with a generalization of the popular arc-eager system for projective trees (Nivre, 2003). Crucially, this generalization only adds constant overhead in run-time and space keeping the parser’s total run-time linear in the worst case. In empirical experiments, our proposed transition-based parser is more accurate on average than both the arc-eager system or the swap-based system, an unconstrained nonprojective transition system with a worst-case quadratic runtime (Nivre, 2009). 1 Introduction Linear-time transition-based parsers that use either greedy inference or beam search are widely used today due to their speed and accuracy (Nivre, 2008; Zhang and Clark, 2008; Zhang and Nivre, 2011). Of the many proposed transition systems (Nivre, 2008), the arc-eager transition system of Nivre (2003) is one of the most popular for a variety of reasons. The arc-eager system has a well-defined output space: it can produce all projective trees and only projective trees. For an input sentence with n words, the arc-eager system always performs 2n operations and each operation take"
N15-1068,D12-1044,1,0.844613,"ions in the worst case, while arc-eager, 2-registers, and 2-planar all use at most O(n) transitions. Checking for cycles does not need to be done at all in the arc-eager system, can be with a few constant operations in the 2-registers system, and can be done in amortized constant time for the other systems (G´omez-Rodr´ıguez and Nivre, 2013). In the graph-based parsing literature, there has also been a plethora of work on non-projective parsing (McDonald et al., 2005; Martins et al., 2009; Koo et al., 2010). Recent work by Pitler and colleagues is the most relevant to the work described here (Pitler et al., 2012, 2013, 2014). Like this work, Pitler et al. define a restricted class of non-projective trees and then a graph-based parsing algorithm that parses exactly that set. The register mechanism in two-registers transition parsing bears a resemblance to registers in Augmented Transition Networks (ATNs) (Woods, 1970). In ATNs, global registers are introduced to account for a wide range of natural language phenomena. This includes long-distance dependencies, which is a common source of non-projective trees. While transition-based parsing and ATNs use quite different control and data structures, this o"
N15-1068,Q13-1002,1,0.914099,"Missing"
N15-1068,Q14-1004,1,0.84176,"en able to preserve many of the attractive properties of their corresponding projective parsing algorithms by restricting search to classes of mildly non-projective trees (Kuhlmann and Nivre, 2006). Mildly non-projective classes of trees are characterizable subsets of directed trees. Classes of particular interest are those that both have high empirical coverage and that can be parsed efficiently. With appropriate definitions of feature functions and output spaces, exact higher-order graph-based non-projective parsers can match the asymptotic time and space of higher-order projective parsers (Pitler, 2014). In this paper, we propose a class of mildly nonprojective trees (§3) and a transition system (§4) that is sound and complete with respect to this class (§5) while preserving desirable properties of arc-eager: it runs in O(n) time in the worst case (§6), and each operation can be interpreted as a prediction about 1 A characterization independent of the transition system is unknown. 662 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 662–671, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics the fin"
N15-1068,D08-1059,0,0.0574429,"stem for projective trees (Nivre, 2003). Crucially, this generalization only adds constant overhead in run-time and space keeping the parser’s total run-time linear in the worst case. In empirical experiments, our proposed transition-based parser is more accurate on average than both the arc-eager system or the swap-based system, an unconstrained nonprojective transition system with a worst-case quadratic runtime (Nivre, 2009). 1 Introduction Linear-time transition-based parsers that use either greedy inference or beam search are widely used today due to their speed and accuracy (Nivre, 2008; Zhang and Clark, 2008; Zhang and Nivre, 2011). Of the many proposed transition systems (Nivre, 2008), the arc-eager transition system of Nivre (2003) is one of the most popular for a variety of reasons. The arc-eager system has a well-defined output space: it can produce all projective trees and only projective trees. For an input sentence with n words, the arc-eager system always performs 2n operations and each operation takes constant time. Another attractive property of the arc-eager system is the close connection between the parameterization of the parsing problem and the final predicted output structure. In t"
N15-1068,P11-2033,0,0.544997,"es (Nivre, 2003). Crucially, this generalization only adds constant overhead in run-time and space keeping the parser’s total run-time linear in the worst case. In empirical experiments, our proposed transition-based parser is more accurate on average than both the arc-eager system or the swap-based system, an unconstrained nonprojective transition system with a worst-case quadratic runtime (Nivre, 2009). 1 Introduction Linear-time transition-based parsers that use either greedy inference or beam search are widely used today due to their speed and accuracy (Nivre, 2008; Zhang and Clark, 2008; Zhang and Nivre, 2011). Of the many proposed transition systems (Nivre, 2008), the arc-eager transition system of Nivre (2003) is one of the most popular for a variety of reasons. The arc-eager system has a well-defined output space: it can produce all projective trees and only projective trees. For an input sentence with n words, the arc-eager system always performs 2n operations and each operation takes constant time. Another attractive property of the arc-eager system is the close connection between the parameterization of the parsing problem and the final predicted output structure. In the arc-eager model, each"
N15-1068,D07-1096,1,\N,Missing
P05-1012,P99-1059,0,0.387427,"rithm and requires only two additional binary variables that specify the direction of the item (either gathering left dependents or gathering right dependents) and whether an item is complete (available to gather more dependents). Figure 2 shows the algorithm schematically. As with normal CKY parsing, larger elements are created bottom-up from pairs of smaller elements. Eisner showed that his algorithm is sufficient for both searching the space of dependency parses and, with slight modification, finding the highest scoring tree y for a given sentence x under the edge factorization assumption. Eisner and Satta (1999) give a cubic algorithm for lexicalized phrase structures. However, it only works for a limited class of languages in which tree spines are regular. Furthermore, there is a large grammar constant, which is typically in the thousands for treebank parsers. 2.3 Online Learning Figure 3 gives pseudo-code for the generic online learning setting. A single training instance is considered on each iteration, and parameters updated by applying an algorithm-specific update rule to the instance under consideration. The algorithm in Figure 3 returns an averaged weight vector: an auxiliary weight vector v i"
P05-1012,W05-1506,0,0.0536885,"(xt , yt ) − s(xt , y 0 ) ≥ L(yt , y 0 ) ∀y 0 ∈ bestk (xt ; w(i) ) reducing the number of constraints to the constant k. We tested various values of k on a development data set and found that small values of k are sufficient to achieve close to best performance, justifying our assumption. In fact, as k grew we began to observe a slight degradation of performance, indicating some 94 overfitting to the training data. All the experiments presented here use k = 5. The Eisner (1996) algorithm can be modified to find the k-best trees while only adding an additional O(k log k) factor to the runtime (Huang and Chiang, 2005). A more common approach is to factor the structure of the output space to yield a polynomial set of local constraints (Taskar et al., 2003; Taskar et al., 2004). One such factorization for dependency trees is min w(i+1) − w(i) s.t. s(l, j) − s(k, j) ≥ 1 ∀(l, j) ∈ yt , (k, j) ∈ / yt It is trivial to show that if these O(n2 ) constraints are satisfied, then so are those in (1). We implemented this model, but found that the required training time was much larger than the k-best formulation and typically did not improve performance. Furthermore, the k-best formulation is more flexible with respec"
P05-1012,A00-2018,0,0.037919,"online largemargin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996). The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements. 1 Introduction Research on training parsers from annotated data has for the most part focused on models and training algorithms for phrase structure parsing. The best phrase-structure parsing models represent generatively the joint probability P (x, y) of sentence x having the structure y (Collins, 1999; Charniak, 2000). Generative parsing models are very convenient because training consists of computing probability estimates from counts of parsing events in the training set. However, generative models make complicated and poorly justified independence assumptions and estimations, so we might expect better performance from discriminatively trained models, as has been shown for other tasks like document classification (Joachims, 2002) and shallow parsing (Sha and Pereira, 2003). Ratnaparkhi’s conditional maximum entropy model (Ratnaparkhi, 1999), trained to maximize conditional likelihood P (y|x) of the train"
P05-1012,J93-2004,0,0.059141,".1 and show a significant improvement in parsing times on the testing data. The major limiting factor of the system is its restriction to features over single dependency attachments. Often, when determining the next depen95 dent for a word, it would be useful to know previous attachment decisions and incorporate these into the features. It is fairly straightforward to modify the parsing algorithm to store previous attachments. However, any modification would result in an asymptotic increase in parsing complexity. 3 Experiments We tested our methods experimentally on the English Penn Treebank (Marcus et al., 1993) and on the Czech Prague Dependency Treebank (Hajiˇc, 1998). All experiments were run on a dual 64-bit AMD Opteron 2.4GHz processor. To create dependency structures from the Penn Treebank, we used the extraction rules of Yamada and Matsumoto (2003), which are an approximation to the lexicalization rules of Collins (1999). We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for evaluation. Currently the system has 6, 998, 447 features. Each instance only uses a tiny fraction of these features making sparse vector calculations possible. Our"
P05-1012,P04-1014,0,0.0387577,"Missing"
P05-1012,C04-1010,0,0.281245,"ohn hit the ball with the 2 System Description bat 2.1 Figure 1: An example dependency tree. case, all algorithms are easily extendible to typed structures. The following work on dependency parsing is most relevant to our research. Eisner (1996) gave a generative model with a cubic parsing algorithm based on an edge factorization of trees. Yamada and Matsumoto (2003) trained support vector machines (SVM) to make parsing decisions in a shift-reduce dependency parser. As in Ratnaparkhi’s parser, the classifiers are trained on individual decisions rather than on the overall quality of the parse. Nivre and Scholz (2004) developed a history-based learning model. Their parser uses a hybrid bottom-up/topdown linear-time heuristic parser and the ability to label edges with semantic types. The accuracy of their parser is lower than that of Yamada and Matsumoto (2003). We present a new approach to training dependency parsers, based on the online large-margin learning algorithms of Crammer and Singer (2003) and Crammer et al. (2003). Unlike the SVM parser of Yamada and Matsumoto (2003) and Ratnaparkhi’s parser, our parsers are trained to maximize the accuracy of the overall tree. Our approach is related to those of"
P05-1012,P04-1015,0,0.507943,"developed a history-based learning model. Their parser uses a hybrid bottom-up/topdown linear-time heuristic parser and the ability to label edges with semantic types. The accuracy of their parser is lower than that of Yamada and Matsumoto (2003). We present a new approach to training dependency parsers, based on the online large-margin learning algorithms of Crammer and Singer (2003) and Crammer et al. (2003). Unlike the SVM parser of Yamada and Matsumoto (2003) and Ratnaparkhi’s parser, our parsers are trained to maximize the accuracy of the overall tree. Our approach is related to those of Collins and Roark (2004) and Taskar et al. (2004) for phrase structure parsing. Collins and Roark (2004) presented a linear parsing model trained with an averaged perceptron algorithm. However, to use parse features with sufficient history, their parsing algorithm must prune heuristically most of the possible parses. Taskar et al. (2004) formulate the parsing problem in the large-margin structured classification setting (Taskar et al., 2003), but are limited to parsing sentences of 15 words or less due to computation time. Though these approaches represent good first steps towards discriminatively-trained parsers, th"
P05-1012,W96-0213,0,0.442753,"98). All experiments were run on a dual 64-bit AMD Opteron 2.4GHz processor. To create dependency structures from the Penn Treebank, we used the extraction rules of Yamada and Matsumoto (2003), which are an approximation to the lexicalization rules of Collins (1999). We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for evaluation. Currently the system has 6, 998, 447 features. Each instance only uses a tiny fraction of these features making sparse vector calculations possible. Our system assumes POS tags as input and uses the tagger of Ratnaparkhi (1996) to provide tags for the development and evaluation sets. Table 2 shows the performance of the systems that were compared. Y&M2003 is the SVM-shiftreduce parsing model of Yamada and Matsumoto (2003), N&S2004 is the memory-based learner of Nivre and Scholz (2004) and MIRA is the the system we have described. We also implemented an averaged perceptron system (Collins, 2002) (another online learning algorithm) for comparison. This table compares only pure dependency parsers that do Y&M2003 N&S2004 Avg. Perceptron MIRA English Accuracy 90.3 87.3 90.6 90.9 Root 91.6 84.3 94.0 94.2 Complete 38.4 30."
P05-1012,P99-1065,0,0.0331032,"Missing"
P05-1012,W02-1001,0,0.820528,"ic update rule to the instance under consideration. The algorithm in Figure 3 returns an averaged weight vector: an auxiliary weight vector v is maintained that accumulates 93 Training data: T = {(xt , yt )}Tt=1 1. w0 = 0; v = 0; i = 0 2. for n : 1..N 3. for t : 1..T 4. w(i+1) = update w(i) according to instance (xt , yt ) 5. v = v + w(i+1) 6. i= i+1 7. w = v/(N ∗ T ) Figure 3: Generic online learning algorithm. the values of w after each iteration, and the returned weight vector is the average of all the weight vectors throughout training. Averaging has been shown to help reduce overfitting (Collins, 2002). 2.3.1 MIRA Crammer and Singer (2001) developed a natural method for large-margin multi-class classification, which was later extended by Taskar et al. (2003) to structured classification: min kwk s.t. s(x, y) − s(x, y 0 ) ≥ L(y, y 0 ) ∀(x, y) ∈ T , y 0 ∈ dt(x) where L(y, y 0 ) is a real-valued loss for the tree y 0 relative to the correct tree y. We define the loss of a dependency tree as the number of words that have the incorrect parent. Thus, the largest loss a dependency tree can have is the length of the sentence. Informally, this update looks to create a margin between the correct depe"
P05-1012,P02-1035,0,0.00976901,"Missing"
P05-1012,N03-1028,1,0.261004,"t phrase-structure parsing models represent generatively the joint probability P (x, y) of sentence x having the structure y (Collins, 1999; Charniak, 2000). Generative parsing models are very convenient because training consists of computing probability estimates from counts of parsing events in the training set. However, generative models make complicated and poorly justified independence assumptions and estimations, so we might expect better performance from discriminatively trained models, as has been shown for other tasks like document classification (Joachims, 2002) and shallow parsing (Sha and Pereira, 2003). Ratnaparkhi’s conditional maximum entropy model (Ratnaparkhi, 1999), trained to maximize conditional likelihood P (y|x) of the training data, performed nearly as well as generative Dependency trees are an alternative syntactic representation with a long history (Hudson, 1984). Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction (Culotta and Sorensen, 2004), paraphrase acquisition (Shinyama et al., 2002) and machine translation (Ding and Palmer, 2005). Yet, they can be parsed i"
P05-1012,W04-3201,0,0.0652711,"rning model. Their parser uses a hybrid bottom-up/topdown linear-time heuristic parser and the ability to label edges with semantic types. The accuracy of their parser is lower than that of Yamada and Matsumoto (2003). We present a new approach to training dependency parsers, based on the online large-margin learning algorithms of Crammer and Singer (2003) and Crammer et al. (2003). Unlike the SVM parser of Yamada and Matsumoto (2003) and Ratnaparkhi’s parser, our parsers are trained to maximize the accuracy of the overall tree. Our approach is related to those of Collins and Roark (2004) and Taskar et al. (2004) for phrase structure parsing. Collins and Roark (2004) presented a linear parsing model trained with an averaged perceptron algorithm. However, to use parse features with sufficient history, their parsing algorithm must prune heuristically most of the possible parses. Taskar et al. (2004) formulate the parsing problem in the large-margin structured classification setting (Taskar et al., 2003), but are limited to parsing sentences of 15 words or less due to computation time. Though these approaches represent good first steps towards discriminatively-trained parsers, they have not yet been able"
P05-1012,P04-1054,0,0.375314,"minatively trained models, as has been shown for other tasks like document classification (Joachims, 2002) and shallow parsing (Sha and Pereira, 2003). Ratnaparkhi’s conditional maximum entropy model (Ratnaparkhi, 1999), trained to maximize conditional likelihood P (y|x) of the training data, performed nearly as well as generative Dependency trees are an alternative syntactic representation with a long history (Hudson, 1984). Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction (Culotta and Sorensen, 2004), paraphrase acquisition (Shinyama et al., 2002) and machine translation (Ding and Palmer, 2005). Yet, they can be parsed in O(n3 ) time (Eisner, 1996). Therefore, dependency parsing is a potential “sweet spot” that deserves investigation. We focus here on projective dependency trees in which a word is the parent of all of its arguments, and dependencies are non-crossing with respect to word order (see Figure 1). However, there are cases where crossing dependencies may occur, as is the case for Czech (Hajiˇc, 1998). Edges in a dependency tree may be typed (for instance to indicate grammatical"
P05-1012,W03-3023,0,0.944924,"for instance to indicate grammatical function). Though we focus on the simpler non-typed 91 Proceedings of the 43rd Annual Meeting of the ACL, pages 91–98, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics and Czech treebank data. root John hit the ball with the 2 System Description bat 2.1 Figure 1: An example dependency tree. case, all algorithms are easily extendible to typed structures. The following work on dependency parsing is most relevant to our research. Eisner (1996) gave a generative model with a cubic parsing algorithm based on an edge factorization of trees. Yamada and Matsumoto (2003) trained support vector machines (SVM) to make parsing decisions in a shift-reduce dependency parser. As in Ratnaparkhi’s parser, the classifiers are trained on individual decisions rather than on the overall quality of the parse. Nivre and Scholz (2004) developed a history-based learning model. Their parser uses a hybrid bottom-up/topdown linear-time heuristic parser and the ability to label edges with semantic types. The accuracy of their parser is lower than that of Yamada and Matsumoto (2003). We present a new approach to training dependency parsers, based on the online large-margin learni"
P05-1012,P05-1067,0,0.371058,"2002) and shallow parsing (Sha and Pereira, 2003). Ratnaparkhi’s conditional maximum entropy model (Ratnaparkhi, 1999), trained to maximize conditional likelihood P (y|x) of the training data, performed nearly as well as generative Dependency trees are an alternative syntactic representation with a long history (Hudson, 1984). Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction (Culotta and Sorensen, 2004), paraphrase acquisition (Shinyama et al., 2002) and machine translation (Ding and Palmer, 2005). Yet, they can be parsed in O(n3 ) time (Eisner, 1996). Therefore, dependency parsing is a potential “sweet spot” that deserves investigation. We focus here on projective dependency trees in which a word is the parent of all of its arguments, and dependencies are non-crossing with respect to word order (see Figure 1). However, there are cases where crossing dependencies may occur, as is the case for Czech (Hajiˇc, 1998). Edges in a dependency tree may be typed (for instance to indicate grammatical function). Though we focus on the simpler non-typed 91 Proceedings of the 43rd Annual Meeting of"
P05-1012,J04-4004,0,\N,Missing
P05-1012,C96-1058,0,\N,Missing
P05-1012,J03-4003,0,\N,Missing
P05-1061,C00-1030,0,0.0121031,"(IE) has focused on accurate tagging of named entities. Successful early named-entity taggers were based on finite-state generative models (Bikel et al., 1999). More recently, discriminatively-trained models have been shown to be more accurate than generative models (McCallum et al., 2000; Lafferty et al., 2001; Kudo and Matsumoto, 2001). Both kinds of models have been developed for tagging entities such as people, places and organizations in news material. However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes (Collier et al., 2000) and genomic variations (McDonald et al., 2004b) from biomedical literature. The next logical step for IE is to begin to develop methods for extracting meaningful relations involvMost relation extraction systems focus on the specific problem of extracting binary relations, such as the employee of relation or protein-protein interaction relation. Very little work has been done in recognizing and extracting more complex relations. We define a complex relation as any n-ary relation among n typed entities. The relation is defined by the schema (t1 , . . . , tn ) where ti ∈ T are entity types. An i"
P05-1061,J02-3001,0,0.0155543,"ere has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume at most one relation per sentence, ruling out cases where entities participate in multiple relations, which is a common occurrence in our data. McDonald et al. (2004a) use a rulebased parser combined with a rule-based relation identifier to extract generic binary relations between biological entities. As in predicate-argument extraction (Gildea and Jurafsky, 2002), each relation is always associated with a verb in the sentence that specifies the relation type. Though this system is very general, it is limited by the fact that the design ignores relations not expressed by a verb, as the employee of relation in“John Smith, CEO of Inc. Corp., announced he will resign”. Most relation extraction systems work primarily on a sentential level and never consider relations that cross sentences or paragraphs. Since current data sets typically only annotate intra-sentence relations, this has not yet proven to be a problem. Complex Relations Recall that a complex n"
P05-1061,N01-1025,0,0.012856,"lated, and the second stage scores maximal cliques in that graph as potential complex relation instances. We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text. 1 Introduction Most research on text information extraction (IE) has focused on accurate tagging of named entities. Successful early named-entity taggers were based on finite-state generative models (Bikel et al., 1999). More recently, discriminatively-trained models have been shown to be more accurate than generative models (McCallum et al., 2000; Lafferty et al., 2001; Kudo and Matsumoto, 2001). Both kinds of models have been developed for tagging entities such as people, places and organizations in news material. However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes (Collier et al., 2000) and genomic variations (McDonald et al., 2004b) from biomedical literature. The next logical step for IE is to begin to develop methods for extracting meaningful relations involvMost relation extraction systems focus on the specific problem of extracting binary relations, such as the employee of relation or protein-p"
P05-1061,A00-2030,0,0.00792299,"hallow parse representation. This approach — enumerating all possible entity pairs and classifying each as positive or negative — is the standard method in relation extraction. The main differences among systems are the choice 492 of trainable classifier and the representation for instances. For binary relations, this approach is quite tractable: if the relation schema is (t 1 , t2 ), the number of potential instances is O(|t1 ||t2 |), where |t |is the number of entity mentions of type t in the text under consideration. One interesting system that does not belong to the above class is that of Miller et al. (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations. Once this augmentation is made, any standard parser can be trained and then run on new sentences to extract new relations. Miller et al. show such an approach can yield good results. However, it can be argued that this method will encounter problems when considering anything but binary relations. Complex relations would require a large amount of tree augmentation and most likely result in extremely sparse probability estimates. Furthermore, by integrat"
P05-1061,P04-1055,0,0.0134126,"rsers. The higher the arity of a relation, the more likely it is that entities will be spread out within a piece of text, making long range dependencies especially important. Roth and Yih (2004) present a model in which entity types and relations are classified jointly using a set of global constraints over locally trained classifiers. This joint classification is shown to improve accuracy of both the entities and relations returned by the system. However, the system is based on constraints for binary relations only. Recently, there has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume at most one relation per sentence, ruling out cases where entities participate in multiple relations, which is a common occurrence in our data. McDonald et al. (2004a) use a rulebased parser combined with a rule-based relation identifier to extract generic binary relations between biological entities. As in predicate-argument extraction (Gildea and Jurafsky, 2002), each relation is always associated with a verb in the sentence"
P05-1061,W04-2401,0,0.0245476,"owever, it can be argued that this method will encounter problems when considering anything but binary relations. Complex relations would require a large amount of tree augmentation and most likely result in extremely sparse probability estimates. Furthermore, by integrating relation extraction with parsing, the system cannot consider long-range dependencies due to the local parsing constraints of current probabilistic parsers. The higher the arity of a relation, the more likely it is that entities will be spread out within a piece of text, making long range dependencies especially important. Roth and Yih (2004) present a model in which entity types and relations are classified jointly using a set of global constraints over locally trained classifiers. This joint classification is shown to improve accuracy of both the entities and relations returned by the system. However, the system is based on constraints for binary relations only. Recently, there has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume"
P05-1061,J96-1002,0,0.0349033,"Missing"
P07-1055,P04-1035,0,0.697891,"Missing"
P07-1055,W02-1011,0,0.0355573,"nents, whether at the paragraph, sentence, phrase or word level, fine-to-coarse sentiment analysis. The simplest approach to fine-to-coarse sentiment analysis would be to create a separate system for each level of granularity. There are, however, obvious advantages to building a single model that classifies each level in tandem. Consider the sentence, Extracting sentiment from text is a challenging problem with applications throughout Natural Language Processing and Information Retrieval. Previous work on sentiment analysis has covered a wide range of tasks, including polarity classification (Pang et al., 2002; Turney, 2002), opinion extraction (Pang and Lee, 2004), and opinion source assignment (Choi et al., 2005; Choi et al., 2006). Furthermore, these systems have tackled the problem at different levels of granularity, from the document level (Pang et al., 2002), sentence level (Pang and Lee, 2004; Mao and Lebanon, 2006), phrase level (Turney, 2002; Choi et al., 2005), as well as the speaker My 11 year old daughter has also been using it and level in debates (Thomas et al., 2006). The abilit is a lot harder than it looks. ity to classify sentiment on multiple levels is important since different a"
P07-1055,H05-1043,0,0.021947,"then input into a standard document level polarity classifier with improved results. The current work differs from that in Pang and Lee through the use of a single joint structured model for both sentence and document level analysis. Many problems in natural language processing can be improved by learning and/or predicting multiple outputs jointly. This includes parsing and relation extraction (Miller et al., 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al., 2004). One interesting work on sentiment analysis is that of Popescu and Etzioni (2005) which attempts to classify the sentiment of phrases with respect to possible product features. To do this an iterative algorithm is used that attempts to globally maximize the classification of all phrases while satisfying local consistency constraints. 2 Structured Model In this section we present a structured model for fine-to-coarse sentiment analysis. We start by examining the simple case with two-levels of granularity – the sentence and document – and show that the problem can be reduced to sequential classification with constrained inference. We then discuss the feature space and give a"
P07-1055,H05-1045,0,0.0671813,"mplest approach to fine-to-coarse sentiment analysis would be to create a separate system for each level of granularity. There are, however, obvious advantages to building a single model that classifies each level in tandem. Consider the sentence, Extracting sentiment from text is a challenging problem with applications throughout Natural Language Processing and Information Retrieval. Previous work on sentiment analysis has covered a wide range of tasks, including polarity classification (Pang et al., 2002; Turney, 2002), opinion extraction (Pang and Lee, 2004), and opinion source assignment (Choi et al., 2005; Choi et al., 2006). Furthermore, these systems have tackled the problem at different levels of granularity, from the document level (Pang et al., 2002), sentence level (Pang and Lee, 2004; Mao and Lebanon, 2006), phrase level (Turney, 2002; Choi et al., 2005), as well as the speaker My 11 year old daughter has also been using it and level in debates (Thomas et al., 2006). The abilit is a lot harder than it looks. ity to classify sentiment on multiple levels is important since different applications have different needs. In isolation, this sentence appears to convey negative For example, a su"
P07-1055,W06-1651,0,0.0191901,"fine-to-coarse sentiment analysis would be to create a separate system for each level of granularity. There are, however, obvious advantages to building a single model that classifies each level in tandem. Consider the sentence, Extracting sentiment from text is a challenging problem with applications throughout Natural Language Processing and Information Retrieval. Previous work on sentiment analysis has covered a wide range of tasks, including polarity classification (Pang et al., 2002; Turney, 2002), opinion extraction (Pang and Lee, 2004), and opinion source assignment (Choi et al., 2005; Choi et al., 2006). Furthermore, these systems have tackled the problem at different levels of granularity, from the document level (Pang et al., 2002), sentence level (Pang and Lee, 2004; Mao and Lebanon, 2006), phrase level (Turney, 2002; Choi et al., 2005), as well as the speaker My 11 year old daughter has also been using it and level in debates (Thomas et al., 2006). The abilit is a lot harder than it looks. ity to classify sentiment on multiple levels is important since different applications have different needs. In isolation, this sentence appears to convey negative For example, a summarization system f"
P07-1055,W02-1001,0,0.585217,"Section 3 an empirical evaluation of the model is given that shows significant gains in accuracy over both single level classifiers and cascaded systems. 1.1 Related Work The models in this work fall into the broad class of global structured models, which are typically trained with structured learning algorithms. Hidden Markov models (Rabiner, 1989) are one of the earliest structured learning algorithms, which have recently been followed by discriminative learning approaches such as conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006), the structured perceptron (Collins, 2002) and its large-margin variants (Taskar et al., 2003; Tsochantaridis et al., 2004; McDonald et al., 2005; Daum´e III et al., 2006). These algorithms are usually applied to sequential 433 labeling or chunking, but have also been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), machine translation (Liang et al., 2006) and summarization (Daum´e III et al., 2006). Structured models have previously been used for sentiment analysis. Choi et al. (2005, 2006) use CRFs to learn a global sequence model to classify and assign sources to opinions. Mao and Lebanon (2006) used a sequential CR"
P07-1055,W04-2401,0,0.0309932,"tive or objective using a global mincut inference algorithm that considered local labeling consistencies. The top subjective sentences are then input into a standard document level polarity classifier with improved results. The current work differs from that in Pang and Lee through the use of a single joint structured model for both sentence and document level analysis. Many problems in natural language processing can be improved by learning and/or predicting multiple outputs jointly. This includes parsing and relation extraction (Miller et al., 2000), entity labeling and relation extraction (Roth and Yih, 2004), and part-of-speech tagging and chunking (Sutton et al., 2004). One interesting work on sentiment analysis is that of Popescu and Etzioni (2005) which attempts to classify the sentiment of phrases with respect to possible product features. To do this an iterative algorithm is used that attempts to globally maximize the classification of all phrases while satisfying local consistency constraints. 2 Structured Model In this section we present a structured model for fine-to-coarse sentiment analysis. We start by examining the simple case with two-levels of granularity – the sentence and document"
P07-1055,P06-1096,0,0.104059,"v models (Rabiner, 1989) are one of the earliest structured learning algorithms, which have recently been followed by discriminative learning approaches such as conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006), the structured perceptron (Collins, 2002) and its large-margin variants (Taskar et al., 2003; Tsochantaridis et al., 2004; McDonald et al., 2005; Daum´e III et al., 2006). These algorithms are usually applied to sequential 433 labeling or chunking, but have also been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), machine translation (Liang et al., 2006) and summarization (Daum´e III et al., 2006). Structured models have previously been used for sentiment analysis. Choi et al. (2005, 2006) use CRFs to learn a global sequence model to classify and assign sources to opinions. Mao and Lebanon (2006) used a sequential CRF regression model to measure polarity on the sentence level in order to determine the sentiment flow of authors in reviews. Here we show that fine-to-coarse models of sentiment can often be reduced to the sequential case. Cascaded models for fine-to-coarse sentiment analysis were studied by Pang and Lee (2004). In that work an in"
P07-1055,P05-1012,1,0.364424,"over both single level classifiers and cascaded systems. 1.1 Related Work The models in this work fall into the broad class of global structured models, which are typically trained with structured learning algorithms. Hidden Markov models (Rabiner, 1989) are one of the earliest structured learning algorithms, which have recently been followed by discriminative learning approaches such as conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006), the structured perceptron (Collins, 2002) and its large-margin variants (Taskar et al., 2003; Tsochantaridis et al., 2004; McDonald et al., 2005; Daum´e III et al., 2006). These algorithms are usually applied to sequential 433 labeling or chunking, but have also been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), machine translation (Liang et al., 2006) and summarization (Daum´e III et al., 2006). Structured models have previously been used for sentiment analysis. Choi et al. (2005, 2006) use CRFs to learn a global sequence model to classify and assign sources to opinions. Mao and Lebanon (2006) used a sequential CRF regression model to measure polarity on the sentence level in order to determine the sentiment flow o"
P07-1055,W04-3201,0,0.0101315,"ypically trained with structured learning algorithms. Hidden Markov models (Rabiner, 1989) are one of the earliest structured learning algorithms, which have recently been followed by discriminative learning approaches such as conditional random fields (CRFs) (Lafferty et al., 2001; Sutton and McCallum, 2006), the structured perceptron (Collins, 2002) and its large-margin variants (Taskar et al., 2003; Tsochantaridis et al., 2004; McDonald et al., 2005; Daum´e III et al., 2006). These algorithms are usually applied to sequential 433 labeling or chunking, but have also been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), machine translation (Liang et al., 2006) and summarization (Daum´e III et al., 2006). Structured models have previously been used for sentiment analysis. Choi et al. (2005, 2006) use CRFs to learn a global sequence model to classify and assign sources to opinions. Mao and Lebanon (2006) used a sequential CRF regression model to measure polarity on the sentence level in order to determine the sentiment flow of authors in reviews. Here we show that fine-to-coarse models of sentiment can often be reduced to the sequential case. Cascaded models for fine-to-coarse sentimen"
P07-1055,W06-1639,0,0.0137727,"tion Retrieval. Previous work on sentiment analysis has covered a wide range of tasks, including polarity classification (Pang et al., 2002; Turney, 2002), opinion extraction (Pang and Lee, 2004), and opinion source assignment (Choi et al., 2005; Choi et al., 2006). Furthermore, these systems have tackled the problem at different levels of granularity, from the document level (Pang et al., 2002), sentence level (Pang and Lee, 2004; Mao and Lebanon, 2006), phrase level (Turney, 2002; Choi et al., 2005), as well as the speaker My 11 year old daughter has also been using it and level in debates (Thomas et al., 2006). The abilit is a lot harder than it looks. ity to classify sentiment on multiple levels is important since different applications have different needs. In isolation, this sentence appears to convey negative For example, a summarization system for product sentiment. However, it is part of a favorable review 432 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 432–439, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics for a piece of fitness equipment, where hard essentially means good workout. In this domain, hard’s"
P07-1055,P02-1053,0,0.0231337,"he paragraph, sentence, phrase or word level, fine-to-coarse sentiment analysis. The simplest approach to fine-to-coarse sentiment analysis would be to create a separate system for each level of granularity. There are, however, obvious advantages to building a single model that classifies each level in tandem. Consider the sentence, Extracting sentiment from text is a challenging problem with applications throughout Natural Language Processing and Information Retrieval. Previous work on sentiment analysis has covered a wide range of tasks, including polarity classification (Pang et al., 2002; Turney, 2002), opinion extraction (Pang and Lee, 2004), and opinion source assignment (Choi et al., 2005; Choi et al., 2006). Furthermore, these systems have tackled the problem at different levels of granularity, from the document level (Pang et al., 2002), sentence level (Pang and Lee, 2004; Mao and Lebanon, 2006), phrase level (Turney, 2002; Choi et al., 2005), as well as the speaker My 11 year old daughter has also been using it and level in debates (Thomas et al., 2006). The abilit is a lot harder than it looks. ity to classify sentiment on multiple levels is important since different applications hav"
P07-1055,A00-2030,0,\N,Missing
P07-1055,H05-2017,0,\N,Missing
P08-1036,P08-1031,0,0.0513063,"Missing"
P08-1036,E06-1039,0,0.101719,".g., topics in blogs, numerical product and service ratings in user reviews, and helpfulness rankings in online discussion forums. Many previous studies on user generated content have attempted to predict these labels automatically from the associated text. However, these labels are often present in the data already, which opens another interesting line of research: designing models leveraging these labelings to improve a wide variety of applications. In this study, we look at the problem of aspectbased sentiment summarization (Hu and Liu, 2004a; Popescu and Etzioni, 2005; Gamon et al., 2005; Carenini et al., 2006; Zhuang et al., 2006).1 An aspect-based summarization system takes as input a set of user reviews for a specific product or service and produces a set of relevant aspects, the aggregated sentiment for each aspect, and supporting textual evidence. For example, figure 1 summarizes a restaurant using aspects food, decor, service, and value plus a numeric rating out of 5. Standard aspect-based summarization consists of two problems. The first is aspect identification and mention extraction. Here the goal is to find the set of relevant aspects for a rated entity and extract all textual mentions th"
P08-1036,W02-1011,0,0.0441681,"set of relevant aspects for a rated entity and extract all textual mentions that are associated with each. Aspects can be fine-grained, e.g., fish, lamb, calamari, or coarse-grained, e.g., food, decor, service. Similarly, extracted text can range from a single word to phrases and sentences. The second problem is sentiment classification. Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating. Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1 We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007). Other studies use the term feature (Hu and Liu, 2004b). 308 Proceedings of ACL-08: HLT, pages 308–316, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great. On top of that our service was excellent and the price was right. Can’t wait to go back! Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our anniversary. My soup was col"
P08-1036,H05-1043,0,0.956418,"creation of an abundance of labeled content, e.g., topics in blogs, numerical product and service ratings in user reviews, and helpfulness rankings in online discussion forums. Many previous studies on user generated content have attempted to predict these labels automatically from the associated text. However, these labels are often present in the data already, which opens another interesting line of research: designing models leveraging these labelings to improve a wide variety of applications. In this study, we look at the problem of aspectbased sentiment summarization (Hu and Liu, 2004a; Popescu and Etzioni, 2005; Gamon et al., 2005; Carenini et al., 2006; Zhuang et al., 2006).1 An aspect-based summarization system takes as input a set of user reviews for a specific product or service and produces a set of relevant aspects, the aggregated sentiment for each aspect, and supporting textual evidence. For example, figure 1 summarizes a restaurant using aspects food, decor, service, and value plus a numeric rating out of 5. Standard aspect-based summarization consists of two problems. The first is aspect identification and mention extraction. Here the goal is to find the set of relevant aspects for a rated"
P08-1036,N07-1038,0,0.942091,"or coarse-grained, e.g., food, decor, service. Similarly, extracted text can range from a single word to phrases and sentences. The second problem is sentiment classification. Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating. Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1 We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007). Other studies use the term feature (Hu and Liu, 2004b). 308 Proceedings of ACL-08: HLT, pages 308–316, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great. On top of that our service was excellent and the price was right. Can’t wait to go back! Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our anniversary. My soup was cold and expensive plus it felt like they hadn’t painted since 1980. Food: 3; Decor: 5; Service: 4; Value: 5 The food is only mediocre, but well worth the cost. Wait staff was"
P08-1036,P02-1053,0,0.0327733,"ects for a rated entity and extract all textual mentions that are associated with each. Aspects can be fine-grained, e.g., fish, lamb, calamari, or coarse-grained, e.g., food, decor, service. Similarly, extracted text can range from a single word to phrases and sentences. The second problem is sentiment classification. Once all the relevant aspects and associated pieces of texts are extracted, the system should aggregate sentiment over each aspect to provide the user with an average numeric or symbolic rating. Sentiment classification is a well studied problem (Wiebe, 2000; Pang et al., 2002; Turney, 2002) and in many domains users explicitly 1 We use the term aspect to denote properties of an object that can be rated by a user as in Snyder and Barzilay (2007). Other studies use the term feature (Hu and Liu, 2004b). 308 Proceedings of ACL-08: HLT, pages 308–316, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics Food: 5; Decor: 5; Service: 5; Value: 5 The chicken was great. On top of that our service was excellent and the price was right. Can’t wait to go back! Food: 2; Decor: 1; Service: 3; Value: 2 We went there for our anniversary. My soup was cold and expensive"
P08-1036,H05-2017,0,\N,Missing
P08-1108,W06-2922,0,0.042532,"component arcs, and perform parsing by searching for the highest-scoring graph. This type of model has been used by, among others, Eisner (1996), McDonald et al. (2005a), and Nakagawa (2007). In transition-based parsing, we instead learn a model for scoring transitions from one parser state to the next, conditioned on the parse history, and perform parsing by greedily taking the highest-scoring transition out of every parser state until we have derived a complete dependency graph. This approach is represented, for example, by the models of Yamada and Matsumoto (2003), Nivre et al. (2004), and Attardi (2006). Theoretically, these approaches are very different. The graph-based models are globally trained and use exact inference algorithms, but define features over a limited history of parsing decisions. The transitionbased models are essentially the opposite. They use local training and greedy inference algorithms, but 950 Proceedings of ACL-08: HLT, pages 950–958, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics define features over a rich history of parsing decisions. This is a fundamental trade-off that is hard to overcome by tractable means. Both models have bee"
P08-1108,A00-2018,0,0.0608921,"ike the models presented here, integration takes place only at parsing time, not at learning time, and requires at least three different base parsers. The same technique was used by Hall et al. (2007) to combine six transition-based parsers in the best performing system in the CoNLL 2007 shared task. Feature-based integration in the sense of letting a subset of the features for one model be derived from the output of a different model has been exploited for dependency parsing by McDonald (2006), who trained an instance of MSTParser using features generated by the parsers of Collins (1999) and Charniak (2000), which improved unlabeled accuracy by 1.7 percentage points, again on data from the Penn Treebank. In addition, feature-based integration has been used by Taskar et al. (2005), who trained a discriminative word alignment model using features derived from the IBM models, and by Florian et al. (2004), who trained classifiers on auxiliary data to guide named entity classifiers. Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003), among others. The difference, of course, is that standard co-"
P08-1108,W04-1513,0,0.0099063,"show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model. By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task. 1 Figure 1: Dependency graph for an English sentence. Introduction Syntactic dependency graphs have recently gained a wide interest in the natural language processing community and have been used for many problems ranging from machine translation (Ding and Palmer, 2004) to ontology construction (Snow et al., 2005). A dependency graph for a sentence represents each word and its syntactic dependents through labeled directed arcs, as shown in figure 1. One advantage of this representation is that it extends naturally to discontinuous constructions, which arise due to long distance dependencies or in languages where syntactic structure is encoded in morphology rather than in word order. This is undoubtedly one of the reasons for the emergence of dependency parsers for a wide range of languages. Many of these parsers are based on data-driven parsing models, which"
P08-1108,D07-1098,0,0.0151148,"out integrating the two models. Thus, Nakagawa (2007) and Hall (2007) both try to overcome the limited feature scope of graph-based models by adding global features, in the former case using Gibbs sampling to deal with the intractable inference problem, in the latter case using a re-ranking scheme. For transition-based models, the trend is to alleviate error propagation by abandoning greedy, deterministic inference in favor of beam search with globally normalized models for scoring transition sequences, either generative (Titov and Henderson, 2007a; Titov and Henderson, 2007b) or conditional (Duan et al., 2007; Johansson and Nugues, 2007). 6 Conclusion In this paper, we have demonstrated how the two dominant approaches to data-driven dependency parsing, graph-based models and transition-based models, can be integrated by letting one model learn from features generated by the other. Our experimental results show that both models consistently improve their accuracy when given access to features generated by the other model, which leads to a significant advancement of the state of the art in data-driven dependency parsing. Moreover, a comparative error analysis reveals that the improvements are largel"
P08-1108,C96-1058,0,0.141901,"aphs for sentences solely from an annotated corpus and can be easily ported to any language or domain in which annotated resources exist. Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transitionbased (McDonald and Nivre, 2007). In graph-based parsing, we learn a model for scoring possible dependency graphs for a given sentence, typically by factoring the graphs into their component arcs, and perform parsing by searching for the highest-scoring graph. This type of model has been used by, among others, Eisner (1996), McDonald et al. (2005a), and Nakagawa (2007). In transition-based parsing, we instead learn a model for scoring transitions from one parser state to the next, conditioned on the parse history, and perform parsing by greedily taking the highest-scoring transition out of every parser state until we have derived a complete dependency graph. This approach is represented, for example, by the models of Yamada and Matsumoto (2003), Nivre et al. (2004), and Attardi (2006). Theoretically, these approaches are very different. The graph-based models are globally trained and use exact inference algorith"
P08-1108,N04-1001,0,0.0290764,"d task. Feature-based integration in the sense of letting a subset of the features for one model be derived from the output of a different model has been exploited for dependency parsing by McDonald (2006), who trained an instance of MSTParser using features generated by the parsers of Collins (1999) and Charniak (2000), which improved unlabeled accuracy by 1.7 percentage points, again on data from the Penn Treebank. In addition, feature-based integration has been used by Taskar et al. (2005), who trained a discriminative word alignment model using features derived from the IBM models, and by Florian et al. (2004), who trained classifiers on auxiliary data to guide named entity classifiers. Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003), among others. The difference, of course, is that standard co-training is a weakly supervised method, where guide features replace, rather than complement, the gold standard annotation during 957 training. Feature-based integration is also similar to parse re-ranking (Collins, 2000), where one parser produces a set of candidate parses and a secondstage classif"
P08-1108,D07-1097,1,0.729656,"Missing"
P08-1108,P07-1050,0,0.0814548,"roduces a set of candidate parses and a secondstage classifier chooses the most likely one. However, feature-based integration is not explicitly constrained to any parse decisions that the guide model might make and only the single most likely parse is used from the guide model, making it significantly more efficient than re-ranking. Finally, there are several recent developments in data-driven dependency parsing, which can be seen as targeting the specific weaknesses of graph-based and transition-based models, respectively, though without integrating the two models. Thus, Nakagawa (2007) and Hall (2007) both try to overcome the limited feature scope of graph-based models by adding global features, in the former case using Gibbs sampling to deal with the intractable inference problem, in the latter case using a re-ranking scheme. For transition-based models, the trend is to alleviate error propagation by abandoning greedy, deterministic inference in favor of beam search with globally normalized models for scoring transition sequences, either generative (Titov and Henderson, 2007a; Titov and Henderson, 2007b) or conditional (Duan et al., 2007; Johansson and Nugues, 2007). 6 Conclusion In this"
P08-1108,P07-1120,0,0.0182811,"for (a) dependency length and (b) distance to root. German, Portuguese and Slovene. Finally, given that the two base models had the previously best performance for these data sets, the guided models achieve a substantial improvement of the state of the art. While there is no statistically significant difference between the two base models, they are both outperformed by MaltMST (p &lt; 0.0001), which in turn has significantly lower accuracy than MSTMalt (p &lt; 0.0005). An extension to the models described so far would be to iteratively integrate the two parsers in the spirit of pipeline iteration (Hollingshead and Roark, 2007). For example, one could start with a Malt model, use it to train a guided MSTMalt model, then use that as the guide to train a MaltMSTMalt model, etc. We ran such experiments, but found that accuracy did not increase significantly and in some cases decreased slightly. This was true regardless of which parser began the iterative process. In retrospect, this result is not surprising. Since the initial integration effectively incorporates knowledge from both parsing systems, there is little to be gained by adding additional parsers in the chain. 4.2 Analysis The experimental results presented so"
P08-1108,D07-1123,0,0.0809681,"two models. Thus, Nakagawa (2007) and Hall (2007) both try to overcome the limited feature scope of graph-based models by adding global features, in the former case using Gibbs sampling to deal with the intractable inference problem, in the latter case using a re-ranking scheme. For transition-based models, the trend is to alleviate error propagation by abandoning greedy, deterministic inference in favor of beam search with globally normalized models for scoring transition sequences, either generative (Titov and Henderson, 2007a; Titov and Henderson, 2007b) or conditional (Duan et al., 2007; Johansson and Nugues, 2007). 6 Conclusion In this paper, we have demonstrated how the two dominant approaches to data-driven dependency parsing, graph-based models and transition-based models, can be integrated by letting one model learn from features generated by the other. Our experimental results show that both models consistently improve their accuracy when given access to features generated by the other model, which leads to a significant advancement of the state of the art in data-driven dependency parsing. Moreover, a comparative error analysis reveals that the improvements are largely predictable from theoretica"
P08-1108,D07-1013,1,0.629031,"s or in languages where syntactic structure is encoded in morphology rather than in word order. This is undoubtedly one of the reasons for the emergence of dependency parsers for a wide range of languages. Many of these parsers are based on data-driven parsing models, which learn to produce dependency graphs for sentences solely from an annotated corpus and can be easily ported to any language or domain in which annotated resources exist. Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transitionbased (McDonald and Nivre, 2007). In graph-based parsing, we learn a model for scoring possible dependency graphs for a given sentence, typically by factoring the graphs into their component arcs, and perform parsing by searching for the highest-scoring graph. This type of model has been used by, among others, Eisner (1996), McDonald et al. (2005a), and Nakagawa (2007). In transition-based parsing, we instead learn a model for scoring transitions from one parser state to the next, conditioned on the parse history, and perform parsing by greedily taking the highest-scoring transition out of every parser state until we have de"
P08-1108,W07-2216,1,0.134744,"rther defines the score of a dependency graph as the sum of the 1 We use the common convention of representing words by their index in the sentence. 951 score of all the arcs it contains. As a result, the dependency parsing problem is written: X G = arg max s(i, j, l) G=(V,A) (i,j,l)∈A This problem is equivalent to finding the highest scoring directed spanning tree in the complete graph over the input sentence, which can be solved in O(n2 ) time (McDonald et al., 2005b). Additional parameterizations are possible that take more than one arc into account, but have varying effects on complexity (McDonald and Satta, 2007). An advantage of graph-based methods is that tractable inference enables the use of standard structured learning techniques that globally set parameters to maximize parsing performance on the training set (McDonald et al., 2005a). The primary disadvantage of these models is that scores – and as a result any feature representations – are restricted to a single arc or a small number of arcs in the graph. The specific graph-based model studied in this work is that presented by McDonald et al. (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive s"
P08-1108,P05-1012,1,0.648377,"ces solely from an annotated corpus and can be easily ported to any language or domain in which annotated resources exist. Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transitionbased (McDonald and Nivre, 2007). In graph-based parsing, we learn a model for scoring possible dependency graphs for a given sentence, typically by factoring the graphs into their component arcs, and perform parsing by searching for the highest-scoring graph. This type of model has been used by, among others, Eisner (1996), McDonald et al. (2005a), and Nakagawa (2007). In transition-based parsing, we instead learn a model for scoring transitions from one parser state to the next, conditioned on the parse history, and perform parsing by greedily taking the highest-scoring transition out of every parser state until we have derived a complete dependency graph. This approach is represented, for example, by the models of Yamada and Matsumoto (2003), Nivre et al. (2004), and Attardi (2006). Theoretically, these approaches are very different. The graph-based models are globally trained and use exact inference algorithms, but define features"
P08-1108,H05-1066,1,0.332049,"Missing"
P08-1108,W06-2932,1,0.684549,"are possible that take more than one arc into account, but have varying effects on complexity (McDonald and Satta, 2007). An advantage of graph-based methods is that tractable inference enables the use of standard structured learning techniques that globally set parameters to maximize parsing performance on the training set (McDonald et al., 2005a). The primary disadvantage of these models is that scores – and as a result any feature representations – are restricted to a single arc or a small number of arcs in the graph. The specific graph-based model studied in this work is that presented by McDonald et al. (2006), which factors scores over pairs of arcs (instead of just single arcs) and uses near exhaustive search for unlabeled parsing coupled with a separate classifier to label each arc. We call this system MSTParser, or simply MST for short, which is also the name of the freely available implementation.2 2.3 Transition-Based Models Transition-based dependency parsing systems use a model parameterized over transitions of an abstract machine for deriving dependency graphs, such that every transition sequence from the designated initial configuration to some terminal configuration derives a valid depen"
P08-1108,D07-1100,0,0.135669,"corpus and can be easily ported to any language or domain in which annotated resources exist. Practically all data-driven models that have been proposed for dependency parsing in recent years can be described as either graph-based or transitionbased (McDonald and Nivre, 2007). In graph-based parsing, we learn a model for scoring possible dependency graphs for a given sentence, typically by factoring the graphs into their component arcs, and perform parsing by searching for the highest-scoring graph. This type of model has been used by, among others, Eisner (1996), McDonald et al. (2005a), and Nakagawa (2007). In transition-based parsing, we instead learn a model for scoring transitions from one parser state to the next, conditioned on the parse history, and perform parsing by greedily taking the highest-scoring transition out of every parser state until we have derived a complete dependency graph. This approach is represented, for example, by the models of Yamada and Matsumoto (2003), Nivre et al. (2004), and Attardi (2006). Theoretically, these approaches are very different. The graph-based models are globally trained and use exact inference algorithms, but define features over a limited history"
P08-1108,W04-2407,1,0.288215,"ng the graphs into their component arcs, and perform parsing by searching for the highest-scoring graph. This type of model has been used by, among others, Eisner (1996), McDonald et al. (2005a), and Nakagawa (2007). In transition-based parsing, we instead learn a model for scoring transitions from one parser state to the next, conditioned on the parse history, and perform parsing by greedily taking the highest-scoring transition out of every parser state until we have derived a complete dependency graph. This approach is represented, for example, by the models of Yamada and Matsumoto (2003), Nivre et al. (2004), and Attardi (2006). Theoretically, these approaches are very different. The graph-based models are globally trained and use exact inference algorithms, but define features over a limited history of parsing decisions. The transitionbased models are essentially the opposite. They use local training and greedy inference algorithms, but 950 Proceedings of ACL-08: HLT, pages 950–958, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics define features over a rich history of parsing decisions. This is a fundamental trade-off that is hard to overcome by tractable means."
P08-1108,W06-2933,1,0.872408,"Missing"
P08-1108,W03-3017,1,0.306869,"erminal configuration is reached. This can be seen as a greedy search for the optimal dependency graph, based on a sequence of locally optimal decisions in terms of the transition system. Many transition systems for data-driven dependency parsing are inspired by shift-reduce parsing, 2 http://mstparser.sourceforge.net where each configuration c contains a stack σc for storing partially processed nodes and a buffer βc containing the remaining input. Transitions in such a system add arcs to the dependency graph and manipulate the stack and buffer. One example is the transition system defined by Nivre (2003), which parses a sentence x = w0 , w1 , . . . , wn in O(n) time. To learn a scoring function on transitions, these systems rely on discriminative learning methods, such as memory-based learning or support vector machines, using a strictly local learning procedure where only single transitions are scored (not complete transition sequences). The main advantage of these models is that features are not restricted to a limited number of graph arcs but can take into account the entire dependency graph built so far. The major disadvantage is that the greedy parsing strategy may lead to error propagat"
P08-1108,N06-2033,0,0.554753,"ghts of the guided features to be small (since they are not needed at training time). On the other hand, an online learning algorithm will recognize the guided features as strong indicators early in training and give them a high weight as a result. Features with high weight early in training tend to have the most impact on the final classifier due to both weight regularization and averaging. This is in fact observed when inspecting the weights of MSTMalt . 5 Related Work Combinations of graph-based and transition-based models for data-driven dependency parsing have previously been explored by Sagae and Lavie (2006), who report improvements of up to 1.7 percentage points over the best single parser when combining three transition-based models and one graph-based model for unlabeled dependency parsing, evaluated on data from the Penn Treebank. The combined parsing model is essentially an instance of the graph-based model, where arc scores are derived from the output of the different component parsers. Unlike the models presented here, integration takes place only at parsing time, not at learning time, and requires at least three different base parsers. The same technique was used by Hall et al. (2007) to"
P08-1108,N01-1023,0,0.0547264,"an instance of MSTParser using features generated by the parsers of Collins (1999) and Charniak (2000), which improved unlabeled accuracy by 1.7 percentage points, again on data from the Penn Treebank. In addition, feature-based integration has been used by Taskar et al. (2005), who trained a discriminative word alignment model using features derived from the IBM models, and by Florian et al. (2004), who trained classifiers on auxiliary data to guide named entity classifiers. Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003), among others. The difference, of course, is that standard co-training is a weakly supervised method, where guide features replace, rather than complement, the gold standard annotation during 957 training. Feature-based integration is also similar to parse re-ranking (Collins, 2000), where one parser produces a set of candidate parses and a secondstage classifier chooses the most likely one. However, feature-based integration is not explicitly constrained to any parse decisions that the guide model might make and only the single most likely parse is used from the gu"
P08-1108,H05-1010,0,0.0205901,"as used by Hall et al. (2007) to combine six transition-based parsers in the best performing system in the CoNLL 2007 shared task. Feature-based integration in the sense of letting a subset of the features for one model be derived from the output of a different model has been exploited for dependency parsing by McDonald (2006), who trained an instance of MSTParser using features generated by the parsers of Collins (1999) and Charniak (2000), which improved unlabeled accuracy by 1.7 percentage points, again on data from the Penn Treebank. In addition, feature-based integration has been used by Taskar et al. (2005), who trained a discriminative word alignment model using features derived from the IBM models, and by Florian et al. (2004), who trained classifiers on auxiliary data to guide named entity classifiers. Feature-based integration also has points in common with co-training, which have been applied to syntactic parsing by Sarkar (2001) and Steedman et al. (2003), among others. The difference, of course, is that standard co-training is a weakly supervised method, where guide features replace, rather than complement, the gold standard annotation during 957 training. Feature-based integration is als"
P08-1108,D07-1099,0,0.0840753,"es of graph-based and transition-based models, respectively, though without integrating the two models. Thus, Nakagawa (2007) and Hall (2007) both try to overcome the limited feature scope of graph-based models by adding global features, in the former case using Gibbs sampling to deal with the intractable inference problem, in the latter case using a re-ranking scheme. For transition-based models, the trend is to alleviate error propagation by abandoning greedy, deterministic inference in favor of beam search with globally normalized models for scoring transition sequences, either generative (Titov and Henderson, 2007a; Titov and Henderson, 2007b) or conditional (Duan et al., 2007; Johansson and Nugues, 2007). 6 Conclusion In this paper, we have demonstrated how the two dominant approaches to data-driven dependency parsing, graph-based models and transition-based models, can be integrated by letting one model learn from features generated by the other. Our experimental results show that both models consistently improve their accuracy when given access to features generated by the other model, which leads to a significant advancement of the state of the art in data-driven dependency parsing. Moreover, a com"
P08-1108,W07-2218,0,0.0947249,"es of graph-based and transition-based models, respectively, though without integrating the two models. Thus, Nakagawa (2007) and Hall (2007) both try to overcome the limited feature scope of graph-based models by adding global features, in the former case using Gibbs sampling to deal with the intractable inference problem, in the latter case using a re-ranking scheme. For transition-based models, the trend is to alleviate error propagation by abandoning greedy, deterministic inference in favor of beam search with globally normalized models for scoring transition sequences, either generative (Titov and Henderson, 2007a; Titov and Henderson, 2007b) or conditional (Duan et al., 2007; Johansson and Nugues, 2007). 6 Conclusion In this paper, we have demonstrated how the two dominant approaches to data-driven dependency parsing, graph-based models and transition-based models, can be integrated by letting one model learn from features generated by the other. Our experimental results show that both models consistently improve their accuracy when given access to features generated by the other model, which leads to a significant advancement of the state of the art in data-driven dependency parsing. Moreover, a com"
P08-1108,W03-3023,0,0.726168,"entence, typically by factoring the graphs into their component arcs, and perform parsing by searching for the highest-scoring graph. This type of model has been used by, among others, Eisner (1996), McDonald et al. (2005a), and Nakagawa (2007). In transition-based parsing, we instead learn a model for scoring transitions from one parser state to the next, conditioned on the parse history, and perform parsing by greedily taking the highest-scoring transition out of every parser state until we have derived a complete dependency graph. This approach is represented, for example, by the models of Yamada and Matsumoto (2003), Nivre et al. (2004), and Attardi (2006). Theoretically, these approaches are very different. The graph-based models are globally trained and use exact inference algorithms, but define features over a limited history of parsing decisions. The transitionbased models are essentially the opposite. They use local training and greedy inference algorithms, but 950 Proceedings of ACL-08: HLT, pages 950–958, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics define features over a rich history of parsing decisions. This is a fundamental trade-off that is hard to overcome"
P08-1108,W06-2920,0,\N,Missing
P08-1108,J03-4003,0,\N,Missing
P08-1108,D07-1096,1,\N,Missing
P11-2100,C04-1200,0,0.154137,"is possible to fuse generative document structure models and task specific structured conditional models. While we do model document structure in terms of sentiment transitions, we do not model topical structure. An interesting avenue for future work would be to extend the model of Sauper et al. (2010) to take coarse-grained taskspecific supervision into account, while modeling fine-grained task-specific aspects with latent variables. Note also that the proposed approach is orthogonal to semi-supervised and unsupervised induction of context independent (prior polarity) lexicons (Turney, 2002; Kim and Hovy, 2004; Esuli and Sebastiani, 2009; Rao and Ravichandran, 2009; Velikovich et al., 2010). The output of such models could readily be incorporated as features in the proposed model. 1.1 Preliminaries Let d be a document consisting of n sentences, s = (si )ni=1 , with a document–sentence-sequence pair denoted d = (d, s). Let y d = (y d , y s ) denote random variables1 – the document level sentiment, y d , and the sequence of sentence level sentiment, y s = (yis )ni=1 . 1 We are abusing notation throughout by using the same symbols to refer to random variables and their particular assignments. 570 In w"
P11-2100,P07-1055,1,0.941177,"upervised structured conditional model and its partially supervised counterpart. This allows for highly efficient estimation and inference algorithms with rich feature definitions. We describe the two variants as well as their component models and verify experimentally that both variants give significantly improved results for sentence-level sentiment analysis compared to all baselines. Exploiting document structure for sentiment analysis has attracted research attention since the early work of Pang and Lee (2004), who performed minimal cuts in a sentence graph to select subjective sentences. McDonald et al. (2007) later showed that jointly learning fine-grained (sentence) and coarsegrained (document) sentiment improves predictions at both levels. More recently, Yessenalina et al. (2010) described how sentence-level latent variables can be used to improve document-level prediction and Nakagawa et al. (2010) used latent variables over syntactic dependency trees to improve sentence-level prediction, using only labeled sentences for training. In a similar vein, Sauper et al. (2010) integrated generative content structure models with discriminative models for multi-aspect sentiment summarization and ranking"
P11-2100,N10-1120,0,0.0447991,"ificantly improved results for sentence-level sentiment analysis compared to all baselines. Exploiting document structure for sentiment analysis has attracted research attention since the early work of Pang and Lee (2004), who performed minimal cuts in a sentence graph to select subjective sentences. McDonald et al. (2007) later showed that jointly learning fine-grained (sentence) and coarsegrained (document) sentiment improves predictions at both levels. More recently, Yessenalina et al. (2010) described how sentence-level latent variables can be used to improve document-level prediction and Nakagawa et al. (2010) used latent variables over syntactic dependency trees to improve sentence-level prediction, using only labeled sentences for training. In a similar vein, Sauper et al. (2010) integrated generative content structure models with discriminative models for multi-aspect sentiment summarization and ranking. These approaches all rely on the availability of fine-grained annotations, but T¨ackstr¨om and McDonald (2011) showed that latent variables can be used to learn fine-grained sentiment using only coarse-grained supervision. While this model was shown to beat a set of natural baselines with quite"
P11-2100,P04-1035,0,0.0758885,"e labels, to learn sentence-level sentiment classifiers. The proposed model is a fusion of a fully supervised structured conditional model and its partially supervised counterpart. This allows for highly efficient estimation and inference algorithms with rich feature definitions. We describe the two variants as well as their component models and verify experimentally that both variants give significantly improved results for sentence-level sentiment analysis compared to all baselines. Exploiting document structure for sentiment analysis has attracted research attention since the early work of Pang and Lee (2004), who performed minimal cuts in a sentence graph to select subjective sentences. McDonald et al. (2007) later showed that jointly learning fine-grained (sentence) and coarsegrained (document) sentiment improves predictions at both levels. More recently, Yessenalina et al. (2010) described how sentence-level latent variables can be used to improve document-level prediction and Nakagawa et al. (2010) used latent variables over syntactic dependency trees to improve sentence-level prediction, using only labeled sentences for training. In a similar vein, Sauper et al. (2010) integrated generative c"
P11-2100,E09-1077,0,0.00676659,"e models and task specific structured conditional models. While we do model document structure in terms of sentiment transitions, we do not model topical structure. An interesting avenue for future work would be to extend the model of Sauper et al. (2010) to take coarse-grained taskspecific supervision into account, while modeling fine-grained task-specific aspects with latent variables. Note also that the proposed approach is orthogonal to semi-supervised and unsupervised induction of context independent (prior polarity) lexicons (Turney, 2002; Kim and Hovy, 2004; Esuli and Sebastiani, 2009; Rao and Ravichandran, 2009; Velikovich et al., 2010). The output of such models could readily be incorporated as features in the proposed model. 1.1 Preliminaries Let d be a document consisting of n sentences, s = (si )ni=1 , with a document–sentence-sequence pair denoted d = (d, s). Let y d = (y d , y s ) denote random variables1 – the document level sentiment, y d , and the sequence of sentence level sentiment, y s = (yis )ni=1 . 1 We are abusing notation throughout by using the same symbols to refer to random variables and their particular assignments. 570 In what follows, we assume that we have access to two traini"
P11-2100,D10-1037,0,0.076677,"since the early work of Pang and Lee (2004), who performed minimal cuts in a sentence graph to select subjective sentences. McDonald et al. (2007) later showed that jointly learning fine-grained (sentence) and coarsegrained (document) sentiment improves predictions at both levels. More recently, Yessenalina et al. (2010) described how sentence-level latent variables can be used to improve document-level prediction and Nakagawa et al. (2010) used latent variables over syntactic dependency trees to improve sentence-level prediction, using only labeled sentences for training. In a similar vein, Sauper et al. (2010) integrated generative content structure models with discriminative models for multi-aspect sentiment summarization and ranking. These approaches all rely on the availability of fine-grained annotations, but T¨ackstr¨om and McDonald (2011) showed that latent variables can be used to learn fine-grained sentiment using only coarse-grained supervision. While this model was shown to beat a set of natural baselines with quite a wide margin, it has its shortcomings. Most notably, due to the loose constraints provided by the coarse supervision, it tends to only predict the two dominant fine-grained s"
P11-2100,D07-1083,0,0.0169174,"te η. At each step we select a fully labeled instance, (dj , y dj ) ∈ DF , with probability λ and a coarsely labeled instance, (dj , yjd ) ∈ DC , with probability (1 − λ). We then update the parameters, θI , according to the gradients ∂LF and ∂LC , respectively. In principle we could use different learning rates ηF and ηC as well as different prior variances 2 , but in what follows we set them equal. σF2 and σC Since we are interpolating conditional models, we need at least partial observations of each instance. Methods for blending discriminative and generative models (Lasserre et al., 2006; Suzuki et al., 2007; Agarwal and Daum´e, 2009; Sauper et al., 2010), would enable incorporation of completely unlabeled data as well. It is straightforward to extend the proposed model along these lines, however, in practice coarsely labeled sentiment data is so abundant on the web (e.g., rated consumer reviews) that incorporating completely unlabeled data seems superfluous. Furthermore, using conditional models with shared parameters throughout allows for rich overlapping features, while maintaining simple and efficient inference and estimation. 3 Experiments For the following experiments, we used the same data"
P11-2100,P02-1053,0,0.00462651,"l. (2010), it is possible to fuse generative document structure models and task specific structured conditional models. While we do model document structure in terms of sentiment transitions, we do not model topical structure. An interesting avenue for future work would be to extend the model of Sauper et al. (2010) to take coarse-grained taskspecific supervision into account, while modeling fine-grained task-specific aspects with latent variables. Note also that the proposed approach is orthogonal to semi-supervised and unsupervised induction of context independent (prior polarity) lexicons (Turney, 2002; Kim and Hovy, 2004; Esuli and Sebastiani, 2009; Rao and Ravichandran, 2009; Velikovich et al., 2010). The output of such models could readily be incorporated as features in the proposed model. 1.1 Preliminaries Let d be a document consisting of n sentences, s = (si )ni=1 , with a document–sentence-sequence pair denoted d = (d, s). Let y d = (y d , y s ) denote random variables1 – the document level sentiment, y d , and the sequence of sentence level sentiment, y s = (yis )ni=1 . 1 We are abusing notation throughout by using the same symbols to refer to random variables and their particular a"
P11-2100,N10-1119,1,0.127863,"tructured conditional models. While we do model document structure in terms of sentiment transitions, we do not model topical structure. An interesting avenue for future work would be to extend the model of Sauper et al. (2010) to take coarse-grained taskspecific supervision into account, while modeling fine-grained task-specific aspects with latent variables. Note also that the proposed approach is orthogonal to semi-supervised and unsupervised induction of context independent (prior polarity) lexicons (Turney, 2002; Kim and Hovy, 2004; Esuli and Sebastiani, 2009; Rao and Ravichandran, 2009; Velikovich et al., 2010). The output of such models could readily be incorporated as features in the proposed model. 1.1 Preliminaries Let d be a document consisting of n sentences, s = (si )ni=1 , with a document–sentence-sequence pair denoted d = (d, s). Let y d = (y d , y s ) denote random variables1 – the document level sentiment, y d , and the sequence of sentence level sentiment, y s = (yis )ni=1 . 1 We are abusing notation throughout by using the same symbols to refer to random variables and their particular assignments. 570 In what follows, we assume that we have access to two training sets: a small set of fu"
P11-2100,D10-1102,0,0.0565461,"itions. We describe the two variants as well as their component models and verify experimentally that both variants give significantly improved results for sentence-level sentiment analysis compared to all baselines. Exploiting document structure for sentiment analysis has attracted research attention since the early work of Pang and Lee (2004), who performed minimal cuts in a sentence graph to select subjective sentences. McDonald et al. (2007) later showed that jointly learning fine-grained (sentence) and coarsegrained (document) sentiment improves predictions at both levels. More recently, Yessenalina et al. (2010) described how sentence-level latent variables can be used to improve document-level prediction and Nakagawa et al. (2010) used latent variables over syntactic dependency trees to improve sentence-level prediction, using only labeled sentences for training. In a similar vein, Sauper et al. (2010) integrated generative content structure models with discriminative models for multi-aspect sentiment summarization and ranking. These approaches all rely on the availability of fine-grained annotations, but T¨ackstr¨om and McDonald (2011) showed that latent variables can be used to learn fine-grained"
P11-2100,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
P12-2047,D08-1107,0,0.0259032,"Missing"
P12-2047,W02-1001,0,0.0946452,"Missing"
P12-2047,P06-1063,0,0.061908,"Missing"
P12-2047,P09-1097,0,0.376704,"Missing"
P12-2047,J93-2004,0,0.0413361,"h of two other taggings. Note that snippet 1 and the query get different taggings primarily due to orthographic variations. It would then add “budget/NNP rent/NNP a/NNP car/NNP” to its training set. The interpolation approach of Bendersky et al. (2010) would tag the query as “budget/NNP rent/VB a/DET car/NN”. To see why this is the case, consider the probability for rent/VB vs rent/NNP. For rent/VB we have 0.2 + 0.8 × 23 , while for rent/NNP we have 0 + 0.8 × 31 assuming that π(VB|rent) = 1. 3 Experimental Setup We assume that we have access to labeled English sentences from the PennTreebank (Marcus et al., 1993) and the QuestionBank (Judge et al., 2006), as well as large amounts of unlabeled search queries. Each query is paired with a set of relevant results represented by snippets (sentence fragments containing the search terms), as well as information about the order in which the results were shown to the user and possibly the result the user clicked on. Note that different sets of results are possible for the same query, because of personalization and ranking changes over time. 3.1 Method D IRECT- CLICK D IRECT- ALL D IRECT- TOP -1 Evaluation Data We use two data sets for evaluation. The first is"
P12-2047,petrov-etal-2012-universal,1,0.604169,"Missing"
P12-2047,P11-1097,0,0.0559009,"Missing"
P12-2047,P08-1086,0,0.0889423,"Missing"
P12-2047,P10-1136,0,\N,Missing
P13-2017,W06-2920,0,0.808379,"ebank is made freely available in order to facilitate research on multilingual dependency parsing.1 1 Introduction In recent years, syntactic representations based on head-modifier dependency relations between words have attracted a lot of interest (K¨ubler et al., 2009). Research in dependency parsing – computational methods to predict such representations – has increased dramatically, due in large part to the availability of dependency treebanks in a number of languages. In particular, the CoNLL shared tasks on dependency parsing have provided over twenty data sets in a standardized format (Buchholz and Marsi, 2006; Nivre et al., 2007). While these data sets are standardized in terms of their formal representation, they are still heterogeneous treebanks. That is to say, despite them all being dependency treebanks, which annotate each sentence with a dependency tree, they subscribe to different annotation schemes. This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Mee"
P13-2017,W02-1503,0,0.0535563,"Missing"
P13-2017,W09-2307,0,0.0712427,"Missing"
P13-2017,P11-1061,1,0.243183,"aking fine-grained label distinctions was discouraged. Once these guidelines were fixed, annotators selected roughly an equal amount of sentences to be annotated from each domain in the unlabeled data. As the sentences were already randomly selected from a larger corpus, annotators were told to view the sentences in order and to discard a sentence only if it was 1) fragmented because of a sentence splitting error; 2) not from the language of interest; 3) incomprehensible to a native speaker; or 4) shorter than three words. The selected sentences were pre-processed using cross-lingual taggers (Das and Petrov, 2011) and parsers (McDonald et al., 2011). The annotators modified the pre-parsed trees using the TrEd2 tool. At the beginning of the annotation process, double-blind annotation, followed by manual arbitration and consensus, was used iteratively for small batches of data until the guidelines were finalized. Most of the data was annotated using single-annotation and full review: one annotator annotating the data and another reviewing it, making changes in close collaboration with the original annotator. As a final step, all annotated data was semi-automatically checked for annotation consistency. 2."
P13-2017,W08-1301,0,0.173029,"Missing"
P13-2017,D11-1006,1,0.855635,"icient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to cr"
P13-2017,P07-1122,1,0.763455,"as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In addition, a small number of constructions required structural conversion, notably coordination, which in the Swedish Treebank is given a Prague style analysis (Nilsson et al., 2007). For both English and Swedish, we mapped the language-specific partof-speech tags to universal tags using the mappings of Petrov et al. (2012). Towards A Universal Treebank The Stanford typed dependencies for English (De Marneffe et al., 2006; de Marneffe and Manning, 2008) serve as the point of departure for our ‘universal’ dependency representation, together with the tag set of Petrov et al. (2012) as the underlying part-of-speech representation. The Stanford scheme, partly inspired by the LFG framework, has emerged as a de facto standard for dependency annotation in English and has recentl"
P13-2017,de-marneffe-etal-2006-generating,0,0.333356,"Missing"
P13-2017,P09-1042,1,0.250489,"arsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages. In terms of automatic construction, Zeman et al. (2012) attempt to harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common synWe present a new collection of treebanks with homogeneous syntactic dependency annotation for six languages: German, English,"
P13-2017,W12-1909,0,0.0247816,"Missing"
P13-2017,petrov-etal-2012-universal,1,0.702758,"nal Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011). Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, su"
P13-2017,D09-1086,0,0.0317978,"ross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2005; Smith and Eisner, 2009; Ganchev et al., 2009), but we are only aware of a few systematic attempts to create homogenous syntactic dependency annotation in multiple languages. In terms of automatic construction, Zeman et al. (2012) attempt to harmonize a large number of dependency treebanks by mapping their annotation to a version of the Prague Dependency Treebank scheme (Hajiˇc et al., 2001; B¨ohmov´a et al., 2003). Additionally, there have been efforts to manually or semimanually construct resources with common synWe present a new collection of treebanks with homogeneous syntactic dependency annotation for six lang"
P13-2017,Q13-1001,1,0.0665003,"Missing"
P13-2017,W04-2709,0,0.0429789,"annotation schemes. This can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 201"
P13-2017,P13-2103,0,0.129897,"Missing"
P13-2017,N06-2015,0,0.0347787,"s can include superficial differences, such as the renaming of common relations, as well as true divergences concerning the analysis of linguistic constructions. Common divergences are found in the 1 Downloadable at https://code.google.com/p/uni-dep-tb/. 92 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 92–97, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tactic analyses across multiple languages using alternate syntactic theories as the basis for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do th"
P13-2017,zeman-etal-2012-hamledt,0,0.060315,"Missing"
P13-2017,P03-1054,0,0.0144203,"nch data set is shown in Figure 1. We take two approaches to generating data. The first is traditional manual annotation, as previously used by Helmreich et al. (2004) for multilingual syntactic treebank construction. The second, used only for English and Swedish, is to automatically convert existing treebanks, as in Zeman et al. (2012). 2.1 Automatic Conversion Since the Stanford dependencies for English are taken as the starting point for our universal annotation scheme, we begin by describing the data sets produced by automatic conversion. For English, we used the Stanford parser (v1.6.8) (Klein and Manning, 2003) to convert the Wall Street Journal section of the Penn Treebank (Marcus et al., 1993) to basic dependency trees, including punctuation and with the copula verb as head in copula constructions. For Swedish, we developed a set of deterministic rules for converting the Talbanken part of the Swedish Treebank (Nivre and Megyesi, 2007) to a representation as close as possible to the Stanford dependencies for English. This mainly consisted in relabeling dependency relations and, due to the fine-grained label set used in the Swedish Treebank (Teleman, 1974), this could be done with high precision. In"
P13-2017,P11-2033,1,0.192695,"Missing"
P13-2017,P04-1061,0,0.0673447,"word expressions (Nilsson et al., 2007; K¨ubler et al., 2009; Zeman et al., 2012). These data sets can be sufficient if one’s goal is to build monolingual parsers and evaluate their quality without reference to other languages, as in the original CoNLL shared tasks, but there are many cases where heterogenous treebanks are less than adequate. First, a homogeneous representation is critical for multilingual language technologies that require consistent cross-lingual analysis for downstream components. Second, consistent syntactic representations are desirable in the evaluation of unsupervised (Klein and Manning, 2004) or cross-lingual syntactic parsers (Hwa et al., 2005). In the cross-lingual study of McDonald et al. (2011), where delexicalized parsing models from a number of source languages were evaluated on a set of target languages, it was observed that the best target language was frequently not the closest typologically to the source. In one stunning example, Danish was the worst source language when parsing Swedish, solely due to greatly divergent annotation schemes. In order to overcome these difficulties, some cross-lingual studies have resorted to heuristics to homogenize treebanks (Hwa et al., 2"
P13-2017,D12-1125,0,0.0145332,"for the representation (Butt et al., 2002; Helmreich et al., 2004; Hovy et al., 2006; Erjavec, 2012). In order to facilitate research on multilingual syntactic analysis, we present a collection of data sets with uniformly analyzed sentences for six languages: German, English, French, Korean, Spanish and Swedish. This resource is freely available and we plan to extend it to include more data and languages. In the context of part-of-speech tagging, universal representations, such as that of Petrov et al. (2012), have already spurred numerous examples of improved empirical cross-lingual systems (Zhang et al., 2012; Gelling et al., 2012; T¨ackstr¨om et al., 2013). We aim to do the same for syntactic dependencies and present cross-lingual parsing experiments to highlight some of the benefits of cross-lingually consistent annotation. First, results largely conform to our expectations of which target languages should be useful for which source languages, unlike in the study of McDonald et al. (2011). Second, the evaluation scores in general are significantly higher than previous cross-lingual studies, suggesting that most of these studies underestimate true accuracy. Finally, unlike all previous cross-ling"
P13-2017,J93-2004,0,\N,Missing
P13-2017,W08-1300,0,\N,Missing
P13-2017,D07-1096,1,\N,Missing
P14-2107,P06-1055,0,0.0613812,"Missing"
P14-2107,C10-1007,0,0.0176072,"yntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the lower-order algorithms. Similar to k-best inference, each chart cell maintains a beam of kbest partial dependency structures. Higher-order features are scored when combining beams during inference. Cube-pruning is an approximation, as the highest scoring tree may fall out of the beam before being fully scored with"
P14-2107,N12-1054,0,0.0637066,"o an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the lower-order algorithms. Similar to k-best inference, each chart cell maintains a beam of kbest partial dependency structures. Higher-order features are scored when combining beams during inference. Cube-pruning is an approximation, as the highest scoring tree may fall out of the beam before being fully scored with higher-order features."
P14-2107,D12-1133,0,0.022542,"Missing"
P14-2107,2008.iwslt-papers.8,0,0.0618534,"Missing"
P14-2107,Q13-1034,0,0.039911,"Missing"
P14-2107,W06-2920,0,0.307722,"Missing"
P14-2107,D12-1030,1,0.960738,"ngle arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the lower-order algorithms. Similar to k-best inference, each chart cell maintains a beam of kbest partial dependency structures. Higher-order features are scored when combining beams during inference. Cube-pruning is an approximation, as the highest scoring tree may fall out of the beam before being fully scored with higher-order features. However, Zhang et al. (2013) observe stateof-the-art results when training accounts for errors that arise due to such approximations. 656 Proceedings of the 52nd Annual Meeting of"
P14-2107,D07-1101,0,0.0520694,"identity of unlabeled structure. By limiting the size of the secondary beam, we restrict label ambiguity and enforce structural diversity within the primary beam. The resulting parser consistently improves on the state-of-the-art parser of Zhang et al. (2013). In Introduction Dependency parsers assign a syntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the l"
P14-2107,P11-2033,0,0.0712916,"orces every tree to be different at the dependency level and the second stores the remaining highest scoring options, which can include outputs that differ only at the token level. The present work looks at beam diversity in graph-based dependency parsing, in particular label versus structural diversity. It was shown that by keeping a diverse beam significant improvements could be achieved on standard benchmarks, in particular with respect to difficult attachment decisions. It is worth pointing out that other dependency parsing frameworks (e.g., transitionbased parsing (Zhang and Clark, 2008; Zhang and Nivre, 2011)) could also benefit from modeling structural diversity in search. Discussion Keeping multiple beams in approximate search has been explored in the past. In machine translation, multiple beams are used to prune translation hypotheses at different levels of granularity (Zens and Ney, 2008). However, the focus is improving the speed of translation decoder rather than improving translation quality through enforcement of hypothesis diversity. In parsing, Bohnet and Nivre (2012) and Bohnet et al. (2013) propose a model for joint morphological analysis, part-ofspeech tagging and dependency parsing u"
P14-2107,J07-2003,0,0.24393,"endency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the lower-order algorithms. Similar to k-best inference, each chart cell maintains a beam of kbest partial dependency structures. Higher-order features are scored when combining beams during inference. Cube-pruning is an approximation, as the highest scoring tree may fall out of the beam before being fully scored with higher-order features. However, Zhang et al. (2013) observe stateof-the-art results when training acco"
P14-2107,D13-1093,1,0.908662,"ambiguity around noun objects to the right of the verb (DOBJ vs. IOBJ vs. TMP) could lead one or more of the structural ambiguities falling out of the beam, especially if the beam is small. To combat this, we introduce a secondary beam for each unique unlabeled structure. That is, we partition the primary (entire) beam into disjoint groups according to the identity of unlabeled structure. By limiting the size of the secondary beam, we restrict label ambiguity and enforce structural diversity within the primary beam. The resulting parser consistently improves on the state-of-the-art parser of Zhang et al. (2013). In Introduction Dependency parsers assign a syntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on"
P14-2107,de-marneffe-etal-2006-generating,0,0.357732,"Missing"
P14-2107,C96-1058,0,0.311999,"during inference. Cube-pruning is an approximation, as the highest scoring tree may fall out of the beam before being fully scored with higher-order features. However, Zhang et al. (2013) observe stateof-the-art results when training accounts for errors that arise due to such approximations. 656 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 656–661, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 0: (a) l + l1 + l l (b) l = = l1 + l2 Figure 2: Structures and rules for parsing with the (Eisner, 1996) algorithm. Solid lines show only the construction of right-pointing first-order dependencies. l is the predicted arc label. Dashed lines are the additional sibling modifier signatures in a generalized algorithm, specifically the previous modifier in complete chart items. = 0: l1 1: l2 2: l3 l1 0: + particular, data sets with large label sets (and thus a large number of label confusions) typically see the largest jumps in accuracy. Finally, we show that the same result cannot be achieved by simply increasing the size of the beam, but requires explicit enforcing of beam diversity. 2 1: l = 0: l"
P14-2107,D13-1152,0,0.00944236,"Missing"
P14-2107,P10-1001,0,0.0949339,"e size of the secondary beam, we restrict label ambiguity and enforce structural diversity within the primary beam. The resulting parser consistently improves on the state-of-the-art parser of Zhang et al. (2013). In Introduction Dependency parsers assign a syntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the lower-order algorithms. Similar to k-best inference, e"
P14-2107,C12-2077,0,0.0685642,"beam, we restrict label ambiguity and enforce structural diversity within the primary beam. The resulting parser consistently improves on the state-of-the-art parser of Zhang et al. (2013). In Introduction Dependency parsers assign a syntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart parsing using the lower-order algorithms. Similar to k-best inference, each chart cell maint"
P14-2107,P13-2109,0,0.204142,"Missing"
P14-2107,E06-1011,1,0.174032,"int groups according to the identity of unlabeled structure. By limiting the size of the secondary beam, we restrict label ambiguity and enforce structural diversity within the primary beam. The resulting parser consistently improves on the state-of-the-art parser of Zhang et al. (2013). In Introduction Dependency parsers assign a syntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-pruned dependency parsing runs standard bottom-up chart pa"
P14-2107,P05-1012,1,0.703326,"is, we partition the primary (entire) beam into disjoint groups according to the identity of unlabeled structure. By limiting the size of the secondary beam, we restrict label ambiguity and enforce structural diversity within the primary beam. The resulting parser consistently improves on the state-of-the-art parser of Zhang et al. (2013). In Introduction Dependency parsers assign a syntactic dependency tree to an input sentence (K¨ubler et al., 2009), as exemplified in Figure 1. Graph-based dependency parsers parameterize models directly over substructures of the tree, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012). As the scope of each feature function increases so does parsing complexity, e.g., o(n5 ) for fourth-order dependency parsing (Ma and Zhao, 2012). This has led to work on approximate inference, typically via pruning (Bergsma and Cherry, 2010; Rush and Petrov, 2012; He et al., 2013) Recently, it has been shown that cube-pruning (Chiang, 2007) can efficiently introduce higherorder dependencies in graph-based parsing (Zhang and McDonald, 2012). Cube-p"
P14-2107,D08-1059,0,\N,Missing
P14-2107,D07-1096,1,\N,Missing
P16-1015,D15-1159,0,0.0182363,"alized parser. In terms of empirical accuracy, from the early success of Nivre and colleagues (Nivre et al., 2006b; Hall et al., 2007; Nivre, 2008), there has been an succession of improvements in training and decoding, including structured training with beam search (Zhang and Clark, 2008; Zhang and Nivre, 2011), incorporating graph-based rescoring features (Bohnet and Kuhn, 2012), the aformentioned work on joint parsing and tagging (Bohnet and Nivre, 2012), and more recently the adoption of neural networks and feature embeddings (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Alberti et al., 2015). 3.1 Basic Notation In the following, we use a directed unlabeled dependency tree T = hV, Ai for a sentence x = In terms of abstract generalizations of transi151 Transitions This is an example with two arcs “Adds an arc from j to i, both ∈ ACTIVE(O).” ←i,j (O, U, A) ⇒ (O, U, A ∪ (j → i)) “Adds an arc from i to j, both ∈ ACTIVE(O).” →i,j (O, U, A) ⇒ (O, U, A ∪ (i → j)) “Removes token i ∈ ACTIVE(O) from O.” −i (O...i..., U, A) ⇒ (O, U, A) (a) Arc-standard: is and example are eligible for arcs. This is an example with two arcs (b) Arc-eager: example and with are eligible for arcs. ”Moves the top"
P16-1015,W06-2922,0,0.155799,"he stack). Key to our generalization is the notion of active tokens, which is the set of tokens in which new arcs can be created and/or removed from consideration. A parser instantiation is defined by a set of control parameters, which dictate: the types of transitions that are permitted and their properties; the capacity of the active token set; and the maximum arc distance. We show that a number of different transition systems can be described via this framework. Critically the two most common systems are covered – arc-eager and arc-standard (Nivre, 2008). But also Attardi’s non-projective (Attardi, 2006), Kuhlmann’s hybrid system (Kuhlmann et al., 2011), the directed acyclic graph (DAG) parser of Sagae and Tsujii (2008), and likely others. More interestingly, the easy-first framework of Goldberg and Elhadad (2010) can be described as an arc-standard system with an unbounded active token capacity. We present a number of experiments with an implementation of our generalized framework. One major advantage of our generalization (and its implementation) is that it allows for easy exploration of novel systems not previously studied. In Section 5 we discuss some possibilities and provide experiments"
P16-1015,P12-1110,0,0.0212868,"ncy trees. The work of Attardi (2006), Nivre (2009), G´omez-Rodr´ıguez and Nivre (2010), Choi and Palmer (2011), and Pitler and McDonald (2015) derived transition systems that could parse nonprojective trees. Each of these systems traded-off complexity for empirical coverage. Additionally, Sagae and Tsujii (2008) developed transition systems that could parse DAGs by augmentating the arc-standard and the arc-eager system. Bohnet and Nivre (2012) derived a system that could produce both labeled dependency trees as well as part-ofspeech tags in a joint transition system. Taking this idea further Hatori et al. (2012) defined a transition system that performed joint segmentation, tagging and parsing. 3 Generalized Transition-based Parsing A transition system must define a parser state as well as a set of transitions that move the system from one state to the next. Correct sequences of transitions create valid parse trees. A parser state is typically a tuple of data structures and variables that represent the dependency tree constructed thus far and, implicitly, possible valid transitions to the next state. In order to generalize across the parser states of transition-based parsing systems, we preserve thei"
P16-1015,D14-1082,0,0.0508596,"eters without changing the specific implementation of the generalized parser. In terms of empirical accuracy, from the early success of Nivre and colleagues (Nivre et al., 2006b; Hall et al., 2007; Nivre, 2008), there has been an succession of improvements in training and decoding, including structured training with beam search (Zhang and Clark, 2008; Zhang and Nivre, 2011), incorporating graph-based rescoring features (Bohnet and Kuhn, 2012), the aformentioned work on joint parsing and tagging (Bohnet and Nivre, 2012), and more recently the adoption of neural networks and feature embeddings (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Alberti et al., 2015). 3.1 Basic Notation In the following, we use a directed unlabeled dependency tree T = hV, Ai for a sentence x = In terms of abstract generalizations of transi151 Transitions This is an example with two arcs “Adds an arc from j to i, both ∈ ACTIVE(O).” ←i,j (O, U, A) ⇒ (O, U, A ∪ (j → i)) “Adds an arc from i to j, both ∈ ACTIVE(O).” →i,j (O, U, A) ⇒ (O, U, A ∪ (i → j)) “Removes token i ∈ ACTIVE(O) from O.” −i (O...i..., U, A) ⇒ (O, U, A) (a) Arc-standard: is and example are eligible for arcs. This is an example with two arcs (b) Arc"
P16-1015,P11-2121,0,0.132564,"ms. 1 Introduction Transition-based dependency parsing is perhaps the most successful parsing framework in use today (Nivre, 2008). This is due to the fact that it can process sentences in linear time (Nivre, 2003); is highly accurate (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Weiss et al., 2015); and has elegant mechanisms for parsing non-projective sentences (Nivre, 2009). As a result, there have been numerous studies into different transition systems, each with varying properties and complexities (Nivre, 2003; Attardi, 2006; Nivre, 2008; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010; Choi and Palmer, 2011; Pitler and McDonald, 2015). While connections between these transition systems have been noted, there has been little work on developing frameworks that generalize the phenomena parsed by these diverse systems. Such a framework would be beneficial for many reasons: It would provide a language from which we can theoretically compare known transition systems; it can give rise to new systems that could have favorable empirical properties; and an implementation of the generalization allows for comprehensive empirical studies. In this work we provide such a generalized 2 Related Work Transition-b"
P16-1015,W00-1303,0,0.102026,"een that work and the present study is the distinction between complex actions versus control parameters. In terms of theoretical coverage, the frameworks are not equivalent. For instance, our generalization covers the system of Attardi (2006), whereas GR&N13 cover transition systems where multiple arcs can be created in tandem. In Section 7 we compare the two generalizations. state as well as a finite set of operations that move the system from one state to another (Nivre, 2008). In terms of modern statistical models that dominate the discourse today, the starting point is likely the work of Kudo and Matsumoto (2000) and Yamada and Matsumoto (2003), who adopted the idea of cascaded chunking from Abney (1991) in a greedy dependency parsing framework. From this early work, transition-based parsing quickly grew in scope with the formalization of the arc-eager versus arc-standard paradigms (Nivre, 2003; Nivre, 2008), the latter largely being based on well-known shift-reduce principles in the phrase-structure literature (Ratnaparkhi, 1999). The speed and empirical accuracy of these systems – as evident in the widely used MaltParser software (Nivre et al., 2006a) – led to the study of a number of different tran"
P16-1015,D11-1114,0,0.044509,"Missing"
P16-1015,P11-1068,0,0.0421966,"Missing"
P16-1015,P04-1015,0,0.184462,"Missing"
P16-1015,P13-2020,1,0.903162,"Missing"
P16-1015,de-marneffe-etal-2006-generating,0,0.0234025,"Missing"
P16-1015,P81-1022,0,0.722336,"Missing"
P16-1015,P15-1033,0,0.0120347,"tation of the generalized parser. In terms of empirical accuracy, from the early success of Nivre and colleagues (Nivre et al., 2006b; Hall et al., 2007; Nivre, 2008), there has been an succession of improvements in training and decoding, including structured training with beam search (Zhang and Clark, 2008; Zhang and Nivre, 2011), incorporating graph-based rescoring features (Bohnet and Kuhn, 2012), the aformentioned work on joint parsing and tagging (Bohnet and Nivre, 2012), and more recently the adoption of neural networks and feature embeddings (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Alberti et al., 2015). 3.1 Basic Notation In the following, we use a directed unlabeled dependency tree T = hV, Ai for a sentence x = In terms of abstract generalizations of transi151 Transitions This is an example with two arcs “Adds an arc from j to i, both ∈ ACTIVE(O).” ←i,j (O, U, A) ⇒ (O, U, A ∪ (j → i)) “Adds an arc from i to j, both ∈ ACTIVE(O).” →i,j (O, U, A) ⇒ (O, U, A ∪ (i → j)) “Removes token i ∈ ACTIVE(O) from O.” −i (O...i..., U, A) ⇒ (O, U, A) (a) Arc-standard: is and example are eligible for arcs. This is an example with two arcs (b) Arc-eager: example and with are eligible f"
P16-1015,W06-2933,0,0.063628,"Missing"
P16-1015,N10-1115,0,0.359881,"a set of control parameters, which dictate: the types of transitions that are permitted and their properties; the capacity of the active token set; and the maximum arc distance. We show that a number of different transition systems can be described via this framework. Critically the two most common systems are covered – arc-eager and arc-standard (Nivre, 2008). But also Attardi’s non-projective (Attardi, 2006), Kuhlmann’s hybrid system (Kuhlmann et al., 2011), the directed acyclic graph (DAG) parser of Sagae and Tsujii (2008), and likely others. More interestingly, the easy-first framework of Goldberg and Elhadad (2010) can be described as an arc-standard system with an unbounded active token capacity. We present a number of experiments with an implementation of our generalized framework. One major advantage of our generalization (and its implementation) is that it allows for easy exploration of novel systems not previously studied. In Section 5 we discuss some possibilities and provide experiments for these in Section 6. In this paper, we present a generalized transition-based parsing framework where parsers are instantiated in terms of a set of control parameters that constrain transitions between parser s"
P16-1015,W03-3017,0,0.8129,"ition-based parsing framework where parsers are instantiated in terms of a set of control parameters that constrain transitions between parser states. This generalization provides a unified framework to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective. This includes well-known transition systems, but also previously unstudied systems. 1 Introduction Transition-based dependency parsing is perhaps the most successful parsing framework in use today (Nivre, 2008). This is due to the fact that it can process sentences in linear time (Nivre, 2003); is highly accurate (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Weiss et al., 2015); and has elegant mechanisms for parsing non-projective sentences (Nivre, 2009). As a result, there have been numerous studies into different transition systems, each with varying properties and complexities (Nivre, 2003; Attardi, 2006; Nivre, 2008; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010; Choi and Palmer, 2011; Pitler and McDonald, 2015). While connections between these transition systems have been noted, there has been little work on developing frameworks that generalize the phenomena parsed by the"
P16-1015,P10-1151,0,0.0339731,"Missing"
P16-1015,J08-4003,0,0.278678,"r); and a set of operative tokens (often called the stack). Key to our generalization is the notion of active tokens, which is the set of tokens in which new arcs can be created and/or removed from consideration. A parser instantiation is defined by a set of control parameters, which dictate: the types of transitions that are permitted and their properties; the capacity of the active token set; and the maximum arc distance. We show that a number of different transition systems can be described via this framework. Critically the two most common systems are covered – arc-eager and arc-standard (Nivre, 2008). But also Attardi’s non-projective (Attardi, 2006), Kuhlmann’s hybrid system (Kuhlmann et al., 2011), the directed acyclic graph (DAG) parser of Sagae and Tsujii (2008), and likely others. More interestingly, the easy-first framework of Goldberg and Elhadad (2010) can be described as an arc-standard system with an unbounded active token capacity. We present a number of experiments with an implementation of our generalized framework. One major advantage of our generalization (and its implementation) is that it allows for easy exploration of novel systems not previously studied. In Section 5 we"
P16-1015,J13-4002,0,0.46584,"Missing"
P16-1015,P09-1040,0,0.451661,"ion provides a unified framework to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective. This includes well-known transition systems, but also previously unstudied systems. 1 Introduction Transition-based dependency parsing is perhaps the most successful parsing framework in use today (Nivre, 2008). This is due to the fact that it can process sentences in linear time (Nivre, 2003); is highly accurate (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Weiss et al., 2015); and has elegant mechanisms for parsing non-projective sentences (Nivre, 2009). As a result, there have been numerous studies into different transition systems, each with varying properties and complexities (Nivre, 2003; Attardi, 2006; Nivre, 2008; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010; Choi and Palmer, 2011; Pitler and McDonald, 2015). While connections between these transition systems have been noted, there has been little work on developing frameworks that generalize the phenomena parsed by these diverse systems. Such a framework would be beneficial for many reasons: It would provide a language from which we can theoretically compare known transition systems"
P16-1015,D07-1097,0,0.0791386,"Missing"
P16-1015,N15-1068,1,0.891429,"sition-based dependency parsing is perhaps the most successful parsing framework in use today (Nivre, 2008). This is due to the fact that it can process sentences in linear time (Nivre, 2003); is highly accurate (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Weiss et al., 2015); and has elegant mechanisms for parsing non-projective sentences (Nivre, 2009). As a result, there have been numerous studies into different transition systems, each with varying properties and complexities (Nivre, 2003; Attardi, 2006; Nivre, 2008; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010; Choi and Palmer, 2011; Pitler and McDonald, 2015). While connections between these transition systems have been noted, there has been little work on developing frameworks that generalize the phenomena parsed by these diverse systems. Such a framework would be beneficial for many reasons: It would provide a language from which we can theoretically compare known transition systems; it can give rise to new systems that could have favorable empirical properties; and an implementation of the generalization allows for comprehensive empirical studies. In this work we provide such a generalized 2 Related Work Transition-based dependency parsing can"
P16-1015,C08-1095,0,0.295862,"arcs can be created and/or removed from consideration. A parser instantiation is defined by a set of control parameters, which dictate: the types of transitions that are permitted and their properties; the capacity of the active token set; and the maximum arc distance. We show that a number of different transition systems can be described via this framework. Critically the two most common systems are covered – arc-eager and arc-standard (Nivre, 2008). But also Attardi’s non-projective (Attardi, 2006), Kuhlmann’s hybrid system (Kuhlmann et al., 2011), the directed acyclic graph (DAG) parser of Sagae and Tsujii (2008), and likely others. More interestingly, the easy-first framework of Goldberg and Elhadad (2010) can be described as an arc-standard system with an unbounded active token capacity. We present a number of experiments with an implementation of our generalized framework. One major advantage of our generalization (and its implementation) is that it allows for easy exploration of novel systems not previously studied. In Section 5 we discuss some possibilities and provide experiments for these in Section 6. In this paper, we present a generalized transition-based parsing framework where parsers are"
P16-1015,N03-1033,0,0.121686,"Missing"
P16-1015,P15-1032,0,0.0481564,"control parameters that constrain transitions between parser states. This generalization provides a unified framework to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective. This includes well-known transition systems, but also previously unstudied systems. 1 Introduction Transition-based dependency parsing is perhaps the most successful parsing framework in use today (Nivre, 2008). This is due to the fact that it can process sentences in linear time (Nivre, 2003); is highly accurate (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Weiss et al., 2015); and has elegant mechanisms for parsing non-projective sentences (Nivre, 2009). As a result, there have been numerous studies into different transition systems, each with varying properties and complexities (Nivre, 2003; Attardi, 2006; Nivre, 2008; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010; Choi and Palmer, 2011; Pitler and McDonald, 2015). While connections between these transition systems have been noted, there has been little work on developing frameworks that generalize the phenomena parsed by these diverse systems. Such a framework would be beneficial for many reasons: It would prov"
P16-1015,W03-3023,0,0.387403,"study is the distinction between complex actions versus control parameters. In terms of theoretical coverage, the frameworks are not equivalent. For instance, our generalization covers the system of Attardi (2006), whereas GR&N13 cover transition systems where multiple arcs can be created in tandem. In Section 7 we compare the two generalizations. state as well as a finite set of operations that move the system from one state to another (Nivre, 2008). In terms of modern statistical models that dominate the discourse today, the starting point is likely the work of Kudo and Matsumoto (2000) and Yamada and Matsumoto (2003), who adopted the idea of cascaded chunking from Abney (1991) in a greedy dependency parsing framework. From this early work, transition-based parsing quickly grew in scope with the formalization of the arc-eager versus arc-standard paradigms (Nivre, 2003; Nivre, 2008), the latter largely being based on well-known shift-reduce principles in the phrase-structure literature (Ratnaparkhi, 1999). The speed and empirical accuracy of these systems – as evident in the widely used MaltParser software (Nivre et al., 2006a) – led to the study of a number of different transition systems. Many of these ne"
P16-1015,D08-1059,0,0.0282985,"asic operations, different transition systems can be defined and configured within one single unified system. As a consequence, we obtain a generalized parser that is capable of executing a wide range of different transition systems by setting a number of control parameters without changing the specific implementation of the generalized parser. In terms of empirical accuracy, from the early success of Nivre and colleagues (Nivre et al., 2006b; Hall et al., 2007; Nivre, 2008), there has been an succession of improvements in training and decoding, including structured training with beam search (Zhang and Clark, 2008; Zhang and Nivre, 2011), incorporating graph-based rescoring features (Bohnet and Kuhn, 2012), the aformentioned work on joint parsing and tagging (Bohnet and Nivre, 2012), and more recently the adoption of neural networks and feature embeddings (Chen and Manning, 2014; Weiss et al., 2015; Dyer et al., 2015; Alberti et al., 2015). 3.1 Basic Notation In the following, we use a directed unlabeled dependency tree T = hV, Ai for a sentence x = In terms of abstract generalizations of transi151 Transitions This is an example with two arcs “Adds an arc from j to i, both ∈ ACTIVE(O).” ←i,j (O, U, A)"
P16-1015,P14-2107,1,0.909235,"Missing"
P16-1015,P11-2033,0,0.266001,"e parsers are instantiated in terms of a set of control parameters that constrain transitions between parser states. This generalization provides a unified framework to describe and compare various transitionbased parsing approaches from both a theoretical and empirical perspective. This includes well-known transition systems, but also previously unstudied systems. 1 Introduction Transition-based dependency parsing is perhaps the most successful parsing framework in use today (Nivre, 2008). This is due to the fact that it can process sentences in linear time (Nivre, 2003); is highly accurate (Zhang and Nivre, 2011; Bohnet and Nivre, 2012; Weiss et al., 2015); and has elegant mechanisms for parsing non-projective sentences (Nivre, 2009). As a result, there have been numerous studies into different transition systems, each with varying properties and complexities (Nivre, 2003; Attardi, 2006; Nivre, 2008; Nivre, 2009; G´omez-Rodr´ıguez and Nivre, 2010; Choi and Palmer, 2011; Pitler and McDonald, 2015). While connections between these transition systems have been noted, there has been little work on developing frameworks that generalize the phenomena parsed by these diverse systems. Such a framework would"
P16-1015,nivre-etal-2006-maltparser,0,\N,Missing
P16-1015,E12-1009,1,\N,Missing
P16-1015,J13-1002,0,\N,Missing
P18-1246,D15-1159,0,0.0224111,"twork for tagging. While this first study used only word embeddings, a subsequent model extended the representation to include suffix embeddings (Collobert et al., 2011). The seminal dependency parsing paper of Chen and Manning (2014) led to a number of tagging papers that used their basic architecture of highly featurized (and embedded) feed-forward neural networks. Botha et al. (2017), for example, studied this architecture in a low resource setting using word, sub-word (prefix/suffix) and induced cluster features to obtain competitive accuracy with the state-of-the-art. Zhou et al. (2015), Alberti et al. (2015) and Andor et al. (2016) extended the work of Chen et al. to a structured prediction setting, the later two use again a mix of word and sub-word features. The idea of using a recurrent layer over characters to induce a complementary view of a word has occurred in numerous papers. Perhaps the earliest is Santos and Zadrozny (2014) who compare character-based LSTM encodings to traditional word-based embeddings. Ling et al. (2015) take this a step further and combine the word embeddings with a recurrent character encoding of the word—instead of just relying on one or the other. Alberti et al. (20"
P18-1246,P16-1231,1,0.930794,"his first study used only word embeddings, a subsequent model extended the representation to include suffix embeddings (Collobert et al., 2011). The seminal dependency parsing paper of Chen and Manning (2014) led to a number of tagging papers that used their basic architecture of highly featurized (and embedded) feed-forward neural networks. Botha et al. (2017), for example, studied this architecture in a low resource setting using word, sub-word (prefix/suffix) and induced cluster features to obtain competitive accuracy with the state-of-the-art. Zhou et al. (2015), Alberti et al. (2015) and Andor et al. (2016) extended the work of Chen et al. to a structured prediction setting, the later two use again a mix of word and sub-word features. The idea of using a recurrent layer over characters to induce a complementary view of a word has occurred in numerous papers. Perhaps the earliest is Santos and Zadrozny (2014) who compare character-based LSTM encodings to traditional word-based embeddings. Ling et al. (2015) take this a step further and combine the word embeddings with a recurrent character encoding of the word—instead of just relying on one or the other. Alberti et al. (2017) use characters encod"
P18-1246,K17-3004,0,0.048126,"Missing"
P18-1246,D17-1309,1,0.849286,"the time, they use ngram affix features, which were made context sensitive via manually constructed conjunctions with features from other words in a fixed window. Collobert and Weston (2008) was perhaps the first modern neural network for tagging. While this first study used only word embeddings, a subsequent model extended the representation to include suffix embeddings (Collobert et al., 2011). The seminal dependency parsing paper of Chen and Manning (2014) led to a number of tagging papers that used their basic architecture of highly featurized (and embedded) feed-forward neural networks. Botha et al. (2017), for example, studied this architecture in a low resource setting using word, sub-word (prefix/suffix) and induced cluster features to obtain competitive accuracy with the state-of-the-art. Zhou et al. (2015), Alberti et al. (2015) and Andor et al. (2016) extended the work of Chen et al. to a structured prediction setting, the later two use again a mix of word and sub-word features. The idea of using a recurrent layer over characters to induce a complementary view of a word has occurred in numerous papers. Perhaps the earliest is Santos and Zadrozny (2014) who compare character-based LSTM enc"
P18-1246,W16-1603,0,0.0233447,"ord the tag with highest probability. Table 8 investigates the empirical impact of alternative definitions of gi that concatenate only subsets of {F1st (w), Flast (w), B1st (w), Blast (w)}. 3.2 Word-based Character Model To investigate whether a sentence sensitive character model is better than a character model where the context is restricted to the characters of a word, we reimplemented the word-based character model of Dozat et al. (2017) as shown in Figure 1a. This model uses the final state of a unidirectional LSTM over the characters of the word, combined with the attention mechanism of Cao and Rei (2016) over all characters. We refer the reader to those works for more details. Critically, however, all the information fed to this representation comes from the word itself, and not a wider sentence-level context. 3.3 Sentence-based Word Model We used a similar setup for our context sensitive word encodings as the character encodings. There are a few differences. Obviously, the inputs are the words of the sentence. For each of the words, we use pretrained word embeddings (pword , ..., pword ) n 1 summed with a dynamically learned word embedding for each word in the corpus (eword , ..., eword ): n"
P18-1246,D14-1082,0,0.0362967,"merate, Gim´enez and Marquez (2004) is a good example of an accurate linear model that uses both word and sub-word features. Specifically, like most systems of the time, they use ngram affix features, which were made context sensitive via manually constructed conjunctions with features from other words in a fixed window. Collobert and Weston (2008) was perhaps the first modern neural network for tagging. While this first study used only word embeddings, a subsequent model extended the representation to include suffix embeddings (Collobert et al., 2011). The seminal dependency parsing paper of Chen and Manning (2014) led to a number of tagging papers that used their basic architecture of highly featurized (and embedded) feed-forward neural networks. Botha et al. (2017), for example, studied this architecture in a low resource setting using word, sub-word (prefix/suffix) and induced cluster features to obtain competitive accuracy with the state-of-the-art. Zhou et al. (2015), Alberti et al. (2015) and Andor et al. (2016) extended the work of Chen et al. to a structured prediction setting, the later two use again a mix of word and sub-word features. The idea of using a recurrent layer over characters to ind"
P18-1246,N16-1031,0,0.0806653,"Missing"
P18-1246,K17-3002,0,0.0717431,"ural networks—specifically BiLSTMs (Schuster and Paliwal, 1997; Graves and Schmidhuber, 2005) to create sentence-level context sensitive encodings of words. A successful recipe is to first create an initial context insensitive word representation, which usually has three main parts: 1) A dynamically trained word embedding; 2) a fixed pre-trained word-embedding, induced from a large corpus; and 3) a sub-word character model, which itself is usually the final state of a recurrent model that ingests one character at a time. Such word/sub-word models originated with Plank et al. (2016). Recently, Dozat et al. (2017) used precisely such a context insensitive word representation as input to a BiLSTM in order to obtain context sensitive word encodings used to predict partof-speech tags. The Dozat et al. model had the highest accuracy of all participating systems in the CoNLL 2017 shared task (Zeman et al., 2017). In such a model, sub-word character-based representations only interact indirectly via subsequent recurrent layers. For example, consider the sentence I had shingles, which is a painful disease. Context insensitive character and word representations may have learned that for unknown or infrequent w"
P18-1246,D15-1176,0,0.0286171,"low resource setting using word, sub-word (prefix/suffix) and induced cluster features to obtain competitive accuracy with the state-of-the-art. Zhou et al. (2015), Alberti et al. (2015) and Andor et al. (2016) extended the work of Chen et al. to a structured prediction setting, the later two use again a mix of word and sub-word features. The idea of using a recurrent layer over characters to induce a complementary view of a word has occurred in numerous papers. Perhaps the earliest is Santos and Zadrozny (2014) who compare character-based LSTM encodings to traditional word-based embeddings. Ling et al. (2015) take this a step further and combine the word embeddings with a recurrent character encoding of the word—instead of just relying on one or the other. Alberti et al. (2017) use characters encodings for parsing. Peters et al. (2018) show that contextual embeddings using character convolutions improve accuracy for number of NLP tasks. Plank et al. (2016) is probably the jumping-off point for most current architectures for tagging models with recurrent neural networks. Specifically, they used a combined word embedding and recurrent character encoding as the initial input to a BiLSTM that generate"
P18-1246,P16-2067,0,0.369836,"gh the adoption of recurrent neural networks—specifically BiLSTMs (Schuster and Paliwal, 1997; Graves and Schmidhuber, 2005) to create sentence-level context sensitive encodings of words. A successful recipe is to first create an initial context insensitive word representation, which usually has three main parts: 1) A dynamically trained word embedding; 2) a fixed pre-trained word-embedding, induced from a large corpus; and 3) a sub-word character model, which itself is usually the final state of a recurrent model that ingests one character at a time. Such word/sub-word models originated with Plank et al. (2016). Recently, Dozat et al. (2017) used precisely such a context insensitive word representation as input to a BiLSTM in order to obtain context sensitive word encodings used to predict partof-speech tags. The Dozat et al. model had the highest accuracy of all participating systems in the CoNLL 2017 shared task (Zeman et al., 2017). In such a model, sub-word character-based representations only interact indirectly via subsequent recurrent layers. For example, consider the sentence I had shingles, which is a painful disease. Context insensitive character and word representations may have learned t"
P18-1246,P11-2009,0,0.0677303,"Missing"
P18-1246,K17-3009,0,0.0452285,"Missing"
P18-1246,P16-1147,0,0.0567865,"Missing"
P18-1246,P15-1117,0,0.0196013,"rst modern neural network for tagging. While this first study used only word embeddings, a subsequent model extended the representation to include suffix embeddings (Collobert et al., 2011). The seminal dependency parsing paper of Chen and Manning (2014) led to a number of tagging papers that used their basic architecture of highly featurized (and embedded) feed-forward neural networks. Botha et al. (2017), for example, studied this architecture in a low resource setting using word, sub-word (prefix/suffix) and induced cluster features to obtain competitive accuracy with the state-of-the-art. Zhou et al. (2015), Alberti et al. (2015) and Andor et al. (2016) extended the work of Chen et al. to a structured prediction setting, the later two use again a mix of word and sub-word features. The idea of using a recurrent layer over characters to induce a complementary view of a word has occurred in numerous papers. Perhaps the earliest is Santos and Zadrozny (2014) who compare character-based LSTM encodings to traditional word-based embeddings. Ling et al. (2015) take this a step further and combine the word embeddings with a recurrent character encoding of the word—instead of just relying on one or the ot"
petrov-etal-2012-universal,D07-1013,1,\N,Missing
petrov-etal-2012-universal,zeman-2008-reusable,0,\N,Missing
petrov-etal-2012-universal,dickinson-jochim-2008-simple,0,\N,Missing
petrov-etal-2012-universal,nivre-etal-2006-talbanken05,0,\N,Missing
petrov-etal-2012-universal,J93-2004,0,\N,Missing
petrov-etal-2012-universal,D11-1018,0,\N,Missing
petrov-etal-2012-universal,boguslavsky-etal-2002-development,0,\N,Missing
petrov-etal-2012-universal,D10-1120,0,\N,Missing
petrov-etal-2012-universal,A97-1014,0,\N,Missing
petrov-etal-2012-universal,N03-1033,0,\N,Missing
petrov-etal-2012-universal,H05-1107,0,\N,Missing
petrov-etal-2012-universal,W06-2920,0,\N,Missing
petrov-etal-2012-universal,N01-1026,0,\N,Missing
petrov-etal-2012-universal,D10-1056,0,\N,Missing
petrov-etal-2012-universal,P10-1131,0,\N,Missing
petrov-etal-2012-universal,N09-1010,0,\N,Missing
petrov-etal-2012-universal,A00-1031,0,\N,Missing
petrov-etal-2012-universal,P11-1061,1,\N,Missing
petrov-etal-2012-universal,I08-3008,0,\N,Missing
petrov-etal-2012-universal,P04-1061,0,\N,Missing
petrov-etal-2012-universal,P11-2008,1,\N,Missing
petrov-etal-2012-universal,P95-1039,0,\N,Missing
petrov-etal-2012-universal,D11-1005,1,\N,Missing
petrov-etal-2012-universal,W00-1906,0,\N,Missing
petrov-etal-2012-universal,dzeroski-etal-2006-towards,0,\N,Missing
petrov-etal-2012-universal,simov-etal-2002-building,0,\N,Missing
petrov-etal-2012-universal,D07-1096,1,\N,Missing
petrov-etal-2012-universal,rambow-etal-2006-parallel,0,\N,Missing
petrov-etal-2012-universal,erjavec-2004-multext,0,\N,Missing
petrov-etal-2012-universal,afonso-etal-2002-floresta,0,\N,Missing
petrov-etal-2012-universal,P05-1044,0,\N,Missing
petrov-etal-2012-universal,P07-1096,0,\N,Missing
Q13-1001,I05-1075,0,\N,Missing
Q13-1001,D12-1127,0,\N,Missing
Q13-1001,J93-2004,0,\N,Missing
Q13-1001,N12-1052,1,\N,Missing
Q13-1001,N10-1083,0,\N,Missing
Q13-1001,H05-1107,0,\N,Missing
Q13-1001,W06-2920,0,\N,Missing
Q13-1001,N01-1026,0,\N,Missing
Q13-1001,C10-1124,0,\N,Missing
Q13-1001,D10-1056,0,\N,Missing
Q13-1001,P08-1085,0,\N,Missing
Q13-1001,D12-1075,0,\N,Missing
Q13-1001,P08-1086,0,\N,Missing
Q13-1001,P09-1057,0,\N,Missing
Q13-1001,P02-1035,0,\N,Missing
Q13-1001,petrov-etal-2012-universal,1,\N,Missing
Q13-1001,P10-1040,0,\N,Missing
Q13-1001,2005.mtsummit-papers.11,0,\N,Missing
Q13-1001,D07-1096,1,\N,Missing
Q13-1001,P05-1044,0,\N,Missing
Q16-1001,E14-1060,0,0.0554739,"Missing"
Q16-1001,N15-1107,0,0.0870818,"Missing"
Q16-1001,banea-etal-2008-bootstrapping,0,0.0765869,"Missing"
Q16-1001,D12-1133,0,0.0746248,"Missing"
Q16-1001,W10-0701,0,0.100991,"Missing"
Q16-1001,E03-1009,0,0.309131,"onship between the attributes of the two nodes, which we intend to learn. As we want to keep our model language independent, we use edge features that can be induced between words without using any language specific tools. To this end, we describe three features in this section that can be obtained using unlabeled corpora for any given language.1 Fig. 1 shows a subgraph of the full graph constructed for English. Word Clusters. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; T¨ackstr¨om et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters."
Q16-1001,P11-1061,0,0.0856553,"Missing"
Q16-1001,N12-1086,0,0.0512554,"Missing"
Q16-1001,de-marneffe-etal-2014-universal,0,0.0615903,"Missing"
Q16-1001,P07-1116,0,0.0877182,"Missing"
Q16-1001,Y09-1013,0,0.0251906,"ges; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes) for a large number of words of a given language. We model the problem of morpho-synt"
Q16-1001,C12-2026,0,0.0604854,"Missing"
Q16-1001,D11-1057,0,0.196897,"Missing"
Q16-1001,dukes-habash-2010-morphological,0,0.0178039,"we show that we can expand a 1000 word seed lexicon to more than 100 times its size with high quality for 11 languages. In addition, the automatically created lexicons provide features that improve performance in two downstream tasks: morphological tagging and dependency parsing. playing awesome Radu Soricut Google Inc. rsoricut@google.com POS:V ERB, T ENSE :PAST, VF ORM :F IN, . . . POS:V ERB, T ENSE :P RES, VF ORM :G ER, . . . POS:A DJ, D EGREE :P OS Table 1: A sample English morpho-syntactic lexicon. They are often constructed manually and are expensive to obtain (Kokkinakis et al., 2000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green a"
Q16-1001,N13-1138,0,0.0977876,"Missing"
Q16-1001,dzeroski-etal-2000-morphosyntactic,0,0.143082,"Missing"
Q16-1001,erjavec-2004-multext,0,0.140248,"Missing"
Q16-1001,D13-1105,0,0.0544315,"Missing"
Q16-1001,N16-1077,1,0.886147,"Missing"
Q16-1001,P13-1057,0,0.036681,"Missing"
Q16-1001,gimenez-marquez-2004-svmtool,0,0.125427,"Missing"
Q16-1001,E09-1038,0,0.0329114,"l. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes) for a large number of words of a given language. We model the problem of morpho-syntactic lexicon generation as a graph-based semi-supervised"
Q16-1001,P12-1016,0,0.0160868,", 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes) for a large number of w"
Q16-1001,P98-1080,0,0.316876,"Missing"
Q16-1001,A00-2013,0,0.230514,"Missing"
Q16-1001,huet-etal-2008-morphosyntactic,0,0.0707153,"Missing"
Q16-1001,W10-0717,0,0.0783632,"Missing"
Q16-1001,kokkinakis-etal-2000-annotating,0,0.00855555,"language-independent, and we show that we can expand a 1000 word seed lexicon to more than 100 times its size with high quality for 11 languages. In addition, the automatically created lexicons provide features that improve performance in two downstream tasks: morphological tagging and dependency parsing. playing awesome Radu Soricut Google Inc. rsoricut@google.com POS:V ERB, T ENSE :PAST, VF ORM :F IN, . . . POS:V ERB, T ENSE :P RES, VF ORM :G ER, . . . POS:A DJ, D EGREE :P OS Table 1: A sample English morpho-syntactic lexicon. They are often constructed manually and are expensive to obtain (Kokkinakis et al., 2000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Min"
Q16-1001,P08-1068,0,0.0642064,"n the attributes of the two nodes, which we intend to learn. As we want to keep our model language independent, we use edge features that can be induced between words without using any language specific tools. To this end, we describe three features in this section that can be obtained using unlabeled corpora for any given language.1 Fig. 1 shows a subgraph of the full graph constructed for English. Word Clusters. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; T¨ackstr¨om et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters. An edge was intro"
Q16-1001,P13-2017,1,0.875506,"Missing"
Q16-1001,N13-1090,0,0.0160739,"Missing"
Q16-1001,P07-1017,0,0.0315038,"000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes"
Q16-1001,D09-1063,0,0.090751,"Missing"
Q16-1001,D15-1151,0,0.0159257,"ailable lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth referred to as attributes) for a large number of words of a given language. We model the problem of morpho-syntactic lexicon"
Q16-1001,N15-1055,0,0.0503508,"Missing"
Q16-1001,D13-1032,0,0.0729096,"Missing"
Q16-1001,W11-1107,0,0.0735884,"Missing"
Q16-1001,Q15-1012,0,0.0930734,"Missing"
Q16-1001,N15-1093,0,0.0487045,"Missing"
Q16-1001,J04-2003,0,0.0247049,"(Kokkinakis et al., 2000; Dukes and Habash, 2010); (2) They are currently available for only a few languages; and (3) Size of available lexicons is generally small. Introduction Morpho-syntactic lexicons contain information about the morphological attributes and syntactic roles of words in a given language. A typical lexicon contains all possible attributes that can be displayed by a word. Table 1 shows some entries in a sample English morpho-syntactic lexicon. As these lexicons contain rich linguistic information, they are useful as features in downstream NLP tasks like machine translation (Nießen and Ney, 2004; Minkov et al., 2007; Green and DeNero, 2012), part of speech tagging (Schmid, 1994; Denis and Sagot, 2009; Moore, 2015), dependency parsing (Goldberg et al., 2009), language modeling (Arisoy et al., 2010) and morphological tagging (M¨uller and Schuetze, 2015) inter alia. There are three major factors that limit the use of such lexicons in real world applications: (1) In this paper, we present a method that takes as input a small seed lexicon, containing a few thousand annotated words, and outputs an automatically constructed lexicon which contains morpho-syntactic attributes (henceforth refe"
Q16-1001,A94-1024,0,0.492659,"Missing"
Q16-1001,N13-1039,0,0.0320114,"Missing"
Q16-1001,W11-4644,0,0.0722929,"Missing"
Q16-1001,N09-1024,0,0.219471,"Missing"
Q16-1001,W96-0213,0,0.533254,"nd Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters. An edge was introduced for every word pair sharing the same word cluster and a feature for the cluster is fired. Thus, there are 256 possible cluster features on an edge, though in our case only a single one can fire. Suffix & Prefix. Suffixes are often strong indicators of the morpho-syntactic attributes of a word (Ratnaparkhi, 1996; Clark, 2003). For example, in English, -ing denotes gerund verb forms like, studying, playing and -ed denotes past tense like studied, played etc. Prefixes like un-, in- often denote adjectives. Thus we include both 2-gram and 3-gram 1 Some of these features can cause the graph to become very dense making label propagation prohibitive. We keep the size of the graph in check by only allowing a word node to be connected to at most 100 other (randomly selected) word nodes sharing one particular feature. This reduces edges while still keeping the graph connected. suffix and prefix as edge featur"
Q16-1001,W13-5005,0,0.0554608,"Missing"
Q16-1001,simov-etal-2004-language,0,0.109415,"Missing"
Q16-1001,H05-1060,0,0.0500174,"e), with seed lexicon (Seed), with propagated lexicon (Propagation). (Oflazer and Kuru¨oz, 1994; Hajiˇc and Hladk´a, 1998). The model we use is a standard atomic sequence classifier, that classifies the morphological bundle for each word independent of the others (with the exception of features derived from these words). Specifically, we use a linear SVM model classifier with hand tuned features. This is similar to commonly used analyzers like SVMTagger (Gim´enez and Marquez, 2004) and MateTagger (Bohnet and Nivre, 2012). Our taggers are trained in a language independent manner (Hajiˇc, 2000; Smith et al., 2005; M¨uller et al., 2013). The list of features used in training the tagger are listed in Table 6. In addition to the standard features, we use the morpho-syntactic attributes present in the lexicon for every word as features in the tagger. As shown in M¨uller and Schuetze (2015), this is typically the most important feature for morphological tagging, even more useful than clusters or word embeddings. While predicting the contextual morphological tags for a given word, the morphological attributes present in the lexicon for the current word, the previous word and the next word are used as featur"
Q16-1001,P08-1084,0,0.263349,"Missing"
Q16-1001,N15-1186,1,0.766624,"like studied, played etc. Prefixes like un-, in- often denote adjectives. Thus we include both 2-gram and 3-gram 1 Some of these features can cause the graph to become very dense making label propagation prohibitive. We keep the size of the graph in check by only allowing a word node to be connected to at most 100 other (randomly selected) word nodes sharing one particular feature. This reduces edges while still keeping the graph connected. suffix and prefix as edge features.2 We introduce an edge between two words sharing a particular suffix or prefix feature. Morphological Transformations. Soricut and Och (2015) presented an unsupervised method of inducing prefix- and suffix-based morphological transformations between words using word embeddings. In their method, statistically, most of the transformations are induced between words with the same lemma (without using any prior information about the word lemma). For example, their method induces the transformation between played and playing as suffix:ed:ing. This feature indicates T ENSE :PAST to turn off and T ENSE :P RES to turn on.3 We train the morphological transformation prediction tool of Soricut and Och (2015) on the news corpus (same as the one"
Q16-1001,D08-1114,0,0.279809,"Missing"
Q16-1001,D10-1017,0,0.0938659,"Missing"
Q16-1001,N12-1052,1,0.899187,"Missing"
Q16-1001,N07-1037,0,0.0803348,"Missing"
Q16-1001,P10-1149,0,0.0821283,"Missing"
Q16-1001,W02-1028,0,0.213563,"Missing"
Q16-1001,tron-etal-2006-morphdb,0,0.086576,"Missing"
Q16-1001,P10-1040,0,0.0756671,"f the two nodes, which we intend to learn. As we want to keep our model language independent, we use edge features that can be induced between words without using any language specific tools. To this end, we describe three features in this section that can be obtained using unlabeled corpora for any given language.1 Fig. 1 shows a subgraph of the full graph constructed for English. Word Clusters. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; T¨ackstr¨om et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters. An edge was introduced for every word"
Q16-1001,P08-1086,0,0.0458253,"ubgraph of the full graph constructed for English. Word Clusters. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many NLP tasks in different languages (Clark, 2003; Koo et al., 2008; Turian et al., 2010; Faruqui and Pad´o, 2010; T¨ackstr¨om et al., 2012; Owoputi et al., 2013). Word clusters capture semantic and syntactic similarities between words, for example, play and run are present in the same cluster. We obtain word clusters by using Exchange clustering algorithm (Kneser and Ney, 1993; Martin et al., 1998; Uszkoreit and Brants, 2008) on large unlabeled corpus of every language. As in T¨ackstr¨om et al. (2012), we use one year of news articles scrapped from a variety of sources and cluster only the most frequent 1M words into 256 different clusters. An edge was introduced for every word pair sharing the same word cluster and a feature for the cluster is fired. Thus, there are 256 possible cluster features on an edge, though in our case only a single one can fire. Suffix & Prefix. Suffixes are often strong indicators of the morpho-syntactic attributes of a word (Ratnaparkhi, 1996; Clark, 2003). For example, in English, -ing"
Q16-1001,N10-1119,1,0.867886,"Missing"
Q16-1001,W04-0109,0,0.0792441,"Missing"
Q16-1001,P00-1027,0,0.070682,"Missing"
Q16-1001,P11-2033,0,0.119186,"Missing"
Q16-1001,C98-1077,0,\N,Missing
W04-3111,kingsbury-palmer-2002-treebank,1,\N,Missing
W04-3111,P03-1002,0,\N,Missing
W04-3111,N03-1028,0,\N,Missing
W04-3111,P96-1008,0,\N,Missing
W04-3111,tateisi-tsujii-2004-part,0,\N,Missing
W06-1615,I05-1006,0,0.0338683,"model can be easily combined with all other domain adaptation techniques (Section 7.2). We are simply inducing a feature representation that generalizes well across domains. This feature representation can then be used in all the techniques described above. The key difference between the previous four pieces of work and our own is the use of unlabeled data. We do not require labeled training data in the new domain to demonstrate an improvement over our baseline models. We believe this is essential, since many domains of application in natural language processing have no labeled training data. Lease and Charniak (2005) adapt a WSJ parser to biomedical text without any biomedical treebanked data. However, they assume other labeled resources in the target domain. In Section 7.3 we give similar parsing results, but we adapt a source domain tagger to obtain the PoS resources. 9 Conclusion Structural correspondence learning is a marriage of ideas from single domain semi-supervised learning and domain adaptation. It uses unlabeled data and frequently-occurring pivot features from both source and target domains to find correspondences among features from these domains. Finding correspondences involves estimating t"
W06-1615,J93-2004,0,0.0648391,"Missing"
W06-1615,H05-1124,1,0.0923218,"ments we use a version of the discriminative online large-margin learning algorithm MIRA (Crammer et al., 2006). MIRA learns and outputs a linear classification score, s(x, y; w) = w · f (x, y), where the feature representation f can contain arbitrary features of the input, including the correspondence features described earlier. In particular, MIRA aims to learn weights so that the score of correct output, yt , for input xt is separated from the highest scoring incorrect outputs2 , with a margin proportional to their Hamming losses. MIRA has been used successfully for both sequence analysis (McDonald et al., 2005a) and dependency parsing (McDonald et al., 2005b). As with any structured predictor, we need to factor the output space to make inference tractable. We use a first-order Markov factorization, allowing for an efficient Viterbi inference procedure. 2 7 Empirical Results All the results we present in this section use the MIRA tagger from Section 5.3. The ASO and structural correspondence results also use projection features learned using ASO and SCL. Section 7.1 presents results comparing structural correspondence learning with the supervised baseline and ASO in the case where we have no labeled"
W06-1615,P05-1012,1,0.17897,"ments we use a version of the discriminative online large-margin learning algorithm MIRA (Crammer et al., 2006). MIRA learns and outputs a linear classification score, s(x, y; w) = w · f (x, y), where the feature representation f can contain arbitrary features of the input, including the correspondence features described earlier. In particular, MIRA aims to learn weights so that the score of correct output, yt , for input xt is separated from the highest scoring incorrect outputs2 , with a margin proportional to their Hamming losses. MIRA has been used successfully for both sequence analysis (McDonald et al., 2005a) and dependency parsing (McDonald et al., 2005b). As with any structured predictor, we need to factor the output space to make inference tractable. We use a first-order Markov factorization, allowing for an efficient Viterbi inference procedure. 2 7 Empirical Results All the results we present in this section use the MIRA tagger from Section 5.3. The ASO and structural correspondence results also use projection features learned using ASO and SCL. Section 7.1 presents results comparing structural correspondence learning with the supervised baseline and ASO in the case where we have no labeled"
W06-1615,N04-1043,0,0.193212,"more direct ways of applying structural correspondence To the best of our knowledge, SCL is the first method to use unlabeled data from both domains for domain adaptation. By using just the unlabeled data from the target domain, however, we can view domain adaptation as a standard semisupervised learning problem. There are many possible approaches for semisupservised learning in natural language processing, and it is beyond the scope of this paper to address them all. We chose to compare with ASO because it consistently outperforms cotraining (Blum and Mitchell, 1998) and clustering methods (Miller et al., 2004). We did run experiments with the top-k version of ASO (Ando and Zhang, 2005a), which is inspired by cotraining but consistently outperforms it. This did not outperform the supervised method for domain adaptation. We speculate that this is because biomedical and financial data are quite different. In such a situation, bootstrapping techniques are likely to introduce too much noise from the source domain to be useful. Structural correspondence learning is most similar to that of Ando (2004), who analyzed a 127 learning when we have labeled data from both source and target domains. In particular"
W06-1615,P03-1021,0,0.0658229,"Missing"
W06-1615,P05-1001,0,0.125762,"which behave in the same way for discriminative learning in both domains. Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, and we treat them similarly in a discriminative learner. Even on the unlabeled data, the co-occurrence statistics of pivot and non-pivot features are likely to be sparse, and we must model them in a compact way. There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003). In this work we choose to use the technique of structural learning (Ando and Zhang, 2005a; Ando and Zhang, 2005b). Structural learning models the correlations which are most useful for semi-supervised learning. We demonstrate how to adapt it for transfer learning, and consequently the structural part of structural correspondence learning is borrowed from it.1 SCL is a general technique, which one can apply to feature based classifiers for any task. Here, Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with n"
W06-1615,P04-3013,0,0.0814238,"Missing"
W06-1615,P93-1024,1,0.0933909,"eatures from different domains by modeling their correlations with pivot features. Pivot features are features which behave in the same way for discriminative learning in both domains. Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, and we treat them similarly in a discriminative learner. Even on the unlabeled data, the co-occurrence statistics of pivot and non-pivot features are likely to be sparse, and we must model them in a compact way. There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003). In this work we choose to use the technique of structural learning (Ando and Zhang, 2005a; Ando and Zhang, 2005b). Structural learning models the correlations which are most useful for semi-supervised learning. We demonstrate how to adapt it for transfer learning, and consequently the structural part of structural correspondence learning is borrowed from it.1 SCL is a general technique, which one can apply to feature based classifiers for any task. Here, Discriminative learning methods are widely used in natural language processing. These methods work best when their trai"
W06-1615,W96-0213,0,0.0962513,"els from a resourcerich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger. 1 Introduction Discriminative learning methods are ubiquitous in natural language processing. Discriminative taggers and chunkers have been the state-of-the-art for more than a decade (Ratnaparkhi, 1996; Sha and Pereira, 2003). Furthermore, end-to-end systems like speech recognizers (Roark et al., 2004) and automatic translators (Och, 2003) use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data. However, in many situations we may have a source domain with plentiful labeled training data, but we need to process material from a target domain with a different distribution from the source domain and no labeled data. In such cases, we must take steps to adapt a model trained on the source domain for use"
W06-1615,N03-1027,0,0.0505862,"). Furthermore, end-to-end systems like speech recognizers (Roark et al., 2004) and automatic translators (Och, 2003) use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data. However, in many situations we may have a source domain with plentiful labeled training data, but we need to process material from a target domain with a different distribution from the source domain and no labeled data. In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (Roark and Bacchiani, 2003; Florian et al., 2004; Chelba 1 Structural learning is different from learning with structured outputs, a common paradigm for discriminative natural language processing models. To avoid terminological confusion, we refer throughout the paper to a specific structural learning method, alternating structural optimization (ASO) (Ando and Zhang, 2005a). 120 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 120–128, c Sydney, July 2006. 2006 Association for Computational Linguistics (a) Wall Street Journal DT JJ VBZ The clash is CC NN IN and"
W06-1615,P04-1007,0,0.0262199,"espondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger. 1 Introduction Discriminative learning methods are ubiquitous in natural language processing. Discriminative taggers and chunkers have been the state-of-the-art for more than a decade (Ratnaparkhi, 1996; Sha and Pereira, 2003). Furthermore, end-to-end systems like speech recognizers (Roark et al., 2004) and automatic translators (Och, 2003) use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data. However, in many situations we may have a source domain with plentiful labeled training data, but we need to process material from a target domain with a different distribution from the source domain and no labeled data. In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (Roark and Bacchiani, 2003; Florian et al., 2004; Chelba 1 Structural learning i"
W06-1615,J92-4003,0,0.146823,"respondences among features from different domains by modeling their correlations with pivot features. Pivot features are features which behave in the same way for discriminative learning in both domains. Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, and we treat them similarly in a discriminative learner. Even on the unlabeled data, the co-occurrence statistics of pivot and non-pivot features are likely to be sparse, and we must model them in a compact way. There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003). In this work we choose to use the technique of structural learning (Ando and Zhang, 2005a; Ando and Zhang, 2005b). Structural learning models the correlations which are most useful for semi-supervised learning. We demonstrate how to adapt it for transfer learning, and consequently the structural part of structural correspondence learning is borrowed from it.1 SCL is a general technique, which one can apply to feature based classifiers for any task. Here, Discriminative learning methods are widely used in natural language processing. These methods wor"
W06-1615,N03-1028,1,0.0833164,"rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger. 1 Introduction Discriminative learning methods are ubiquitous in natural language processing. Discriminative taggers and chunkers have been the state-of-the-art for more than a decade (Ratnaparkhi, 1996; Sha and Pereira, 2003). Furthermore, end-to-end systems like speech recognizers (Roark et al., 2004) and automatic translators (Och, 2003) use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data. However, in many situations we may have a source domain with plentiful labeled training data, but we need to process material from a target domain with a different distribution from the source domain and no labeled data. In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (R"
W06-1615,W04-3237,0,0.0875466,"incorporating SCL features into a discriminative parser to improve its adaptation properties. 8 Related Work Domain adaptation is an important and wellstudied area in natural language processing. Here we outline a few recent advances. Roark and Bacchiani (2003) use a Dirichlet prior on the multinomial parameters of a generative parsing model to combine a large amount of training data from a source corpus (WSJ), and small amount of training data from a target corpus (Brown). Aside from Florian et al. (2004), several authors have also given techniques for adapting classification to new domains. Chelba and Acero (2004) first train a classifier on the source data. Then they use maximum a posteriori estimation of the weights of a Improving Parser Performance We emphasize the importance of PoS tagging in a pipelined NLP system by incorporating our SCL 126 maximum entropy target domain classifier. The prior is Gaussian with mean equal to the weights of the source domain classifier. Daum´e III and Marcu (2006) use an empirical Bayes model to estimate a latent variable model grouping instances into domain-specific or common across both domains. They also jointly estimate the parameters of the common classificatio"
W06-1615,N03-1033,0,0.15532,"c Sydney, July 2006. 2006 Association for Computational Linguistics (a) Wall Street Journal DT JJ VBZ The clash is CC NN IN and divisiveness in (b) MEDLINE DT JJ The oncogenic RB JJ constitutively active DT a NNP Japan VBN mutated CC and NN sign POS ’s NNS forms VBP interfere IN of JJ once-cozy IN of IN with DT a JJ financial DT the JJ normal JJ new NN circles NN ras NN signal NN toughness . . NNS proteins NN transduction VBP are . . Figure 1: Part of speech-tagged sentences from both corpora (a) An ambiguous instance we investigate its use in part of speech (PoS) tagging (Ratnaparkhi, 1996; Toutanova et al., 2003). While PoS tagging has been heavily studied, many domains lack appropriate training corpora for PoS tagging. Nevertheless, PoS tagging is an important stage in pipelined language processing systems, from information extractors to speech synthesizers. We show how to use SCL to transfer a PoS tagger from the Wall Street Journal (financial news) to MEDLINE (biomedical abstracts), which use very different vocabularies, and we demonstrate not only improved PoS accuracy but also improved end-to-end parsing accuracy while using the improved tagger. An important but rarely-explored setting in domain"
W06-1615,N04-1001,0,\N,Missing
W06-2932,W06-2920,0,0.886664,"ania Philadelphia, PA {ryantm,klerman,pereira}@cis.upenn.edu Abstract We present a two-stage multilingual dependency parser and evaluate it on 13 diverse languages. The first stage is based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages. The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph. We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis. 1 Introduction Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing. With the availability of resources such as the Penn WSJ Treebank, much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure. However, recently their has been a revived interest in parsing models that produce dependency graph representations of sentences, which model words and their arguments through directed edges (Hudson, 1984; Mel0 cˇ uk, 1988). This inte"
W06-2932,P05-1067,0,0.0246308,"Hudson, 1984; Mel0 cˇ uk, 1988). This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages. Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies. Dependency graphs also encode much of the deep syntactic information needed for further processing. This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al., 2005). In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler. We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al., 2006; Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2"
W06-2932,H05-1049,0,0.0119017,"putationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages. Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies. Dependency graphs also encode much of the deep syntactic information needed for further processing. This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al., 2005). In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler. We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al., 2006; Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and Mart´ı Anton´ın, 2002; Nilsson et al., 2005; Oflazer et al.,"
W06-2932,E06-1011,1,0.394749,"nd xi is the corresponding head. Each edge can be assigned a label l(i,j) from a finite set L of predefined labels. We 216 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), c pages 216–220, New York City, June 2006. 2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective, both of which are true in the data sets we use. 2 Stage 1: Unlabeled Parsing The first stage of our system creates an unlabeled parse y for an input sentence x. This system is primarily based on the parsing models described by McDonald and Pereira (2006). That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph. An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge. That system uses MIRA, an online large-margin learning algorithm, to compute model parameters. Its power lies in the ability to define a rich set of features over parsing decisions, as well as surfac"
W06-2932,P05-1012,1,0.431684,"ined labels. We 216 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), c pages 216–220, New York City, June 2006. 2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective, both of which are true in the data sets we use. 2 Stage 1: Unlabeled Parsing The first stage of our system creates an unlabeled parse y for an input sentence x. This system is primarily based on the parsing models described by McDonald and Pereira (2006). That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph. An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge. That system uses MIRA, an online large-margin learning algorithm, to compute model parameters. Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions. For instance, the system of McDonald et al. (2005a)"
W06-2932,H05-1066,1,0.313302,"Missing"
W06-2932,E06-1038,1,0.676313,"has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages. Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies. Dependency graphs also encode much of the deep syntactic information needed for further processing. This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al., 2005). In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler. We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al., 2006; Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and Mart´ı Anton´ı"
W06-2932,W03-2403,0,\N,Missing
W06-2932,dzeroski-etal-2006-towards,0,\N,Missing
W06-2932,W03-2405,0,\N,Missing
W06-2932,afonso-etal-2002-floresta,0,\N,Missing
W07-2216,C92-2092,0,0.0423826,"ve parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing. Both studies constructed implementations that compare favorably with the state-of-the-art. The work of Meil˘a and Jaakkola (2000) is also of note. In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here. 2 Preliminaries Let L"
W07-2216,W06-2920,0,0.698112,"Under the assumption that the root of the graph is the left most word of the sentence, a projective graph is one where the edges can be drawn in the plane above the sentence with no two edges crossing. Conversely, a non-projective dependency graph does not satisfy this property. Figure 2 gives an example of a nonprojective graph for a sentence that has also been extracted from the Penn Treebank. Non-projectivity arises due to long distance dependencies or in languages with flexible word order. For many languages, a significant portion of sentences require a non-projective dependency analysis (Buchholz et al., 2006). Thus, the ability to learn and infer nonprojective dependency graphs is an important problem in multilingual language processing. Syntactic dependency parsing has seen a number of new learning and inference algorithms which have raised state-of-the-art parsing accuracies for many languages. In this work we focus on datadriven models of dependency parsing. These models are not driven by any underlying grammar, but instead learn to predict dependency graphs based on a set of parameters learned solely from a labeled corpus. The advantage of these models is that they negate the need for the deve"
W07-2216,W02-1001,0,0.119719,"re for both the projective and non-projective case. We see that the non-projective case compares favorably for all three problems. 4 Applications To motivate the algorithms from Section 3, we present some important situations where each calculation is required. 4.1 (i,j)k ∈ET This is a common definition of risk between two graphs as it corresponds directly to labeled dependency parsing accuracy (McDonald et al., 2005a; Buchholz et al., 2006). Some algebra reveals, T = = Inference Based Learning = Many learning paradigms can be defined as inference-based learning. These include the perceptron (Collins, 2002) and its large-margin variants (Crammer and Singer, 2003; McDonald et al., 2005a). In these settings, a models parameters are iteratively updated based on the argmax calculation for a single or set of training instances under the current parameter settings. The work of McDonald et al. (2005b) showed that it is possible to learn a highly accurate non-projective dependency parser for multiple languages using the Chu-Liu-Edmonds algorithm for unlabeled parsing. 4.2 studied for both phrase-structure parsing and dependency parsing (Titov and Henderson, 2006). In that work, as is common with many mi"
W07-2216,A00-2030,0,0.0179914,"Missing"
W07-2216,C96-1058,0,0.984165,"Hamiltonian graph problem suggesting that the parsing problem is intractable in this case. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong"
W07-2216,P97-1043,0,0.23732,"Missing"
W07-2216,W05-1505,0,0.0831088,"Missing"
W07-2216,P06-2047,0,0.0130804,"orithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b). Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006). For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for ins"
W07-2216,P98-1106,0,0.418447,"ustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006). For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing. Both studies constructed implementations that compare favorably with the state-of-th"
W07-2216,P05-1013,0,0.0557336,"work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b). Recently there have a"
W07-2216,C04-1010,0,0.00795222,"roblem is intractable in this case. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models"
W07-2216,P03-1054,0,0.0190043,"grammar-driven literature given by Neuhaus and B¨oker (1997). In that work, an arity constraint is included in their minimal grammar. 5.2 Vertical and Horizontal Markovization In general, we would like to say that every dependency decision is dependent on every other edge in a graph. However, modeling dependency parsing in such a manner would be a computational nightmare. Instead, we would like to make a Markov assumption over the edges of the tree, in a similar way that a Markov assumption can be made for sequential classification problems in order to ensure tractable learning and inference. Klein and Manning (2003) distinguish between two kinds of Markovization for unlexicalized CFG parsing. The first is vertical Markovization, which makes the generation of a non-terminal dependent on other non-terminals that have been generated at different levels in the phrase-structure tree. The second is horizontal Markovization, which makes the generation of a non-terminal dependent on other non-terminals that have been generated at the same level in the tree. For dependency parsing there are analogous notions of vertical and horizontal Markovization for a given edge (i, j)k . First, let us define the vertical and"
W07-2216,W06-1616,0,0.189447,"y through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b). Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006). For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined"
W07-2216,P04-1061,0,0.0197833,"recursive breadthfirst manner. Thus, pkx,y represents the probability of the word x generating its modifier y with label lk . This distribution is usually smoothed and is often conditioned on more information including the orientation of x relative to y (i.e., to the left/right) and distance between the two words. In the supervised setting this model can be trained with maximum likelihood estimation, which amounts to simple counts over the data. Learning in the unsupervised setting requires EM and is discussed in Section 4.4.2. Another generative dependency model of interest is that given by Klein and Manning (2004). In this model the sentence and tree are generated jointly, which allows one to drop the assumption that p(T |n) is uniform. This requires the addition to the model of parameters px,STOP for each xP∈ Σ, with the normalization condition px,STOP + y,k pkx,y = 1. It is possible to extend the model of Klein and Manning (2004) to the non-projective case. However, the resulting distribution will be over multisets of words from the alphabet instead of strings. The discussion in this section is stated for the model in Paskin (2001); a similar treatment can be developed for the model in Klein and Mann"
W07-2216,D07-1015,0,0.704066,"clude the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing. Both studies constructed implementations that compare favorably with the state-of-the-art. The work of Meil˘a and Jaakkola (2000) is also of note. In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here. 2 Preliminaries Let L = {l1 , . . . , l|L |} be a set of permissible"
W07-2216,J93-2004,0,0.0304382,"tractable for any model richer than the edge-factored model. 1 Introduction Dependency representations of natural language are a simple yet flexible mechanism for encoding words and their syntactic dependencies through directed graphs. These representations have been thoroughly studied in descriptive linguistics (Tesni`ere, 1959; Hudson, 1984; Sgall et al., 1986; Me´lˇcuk, 1988) and have been applied in numerous language processing tasks. Figure 1 gives an example dependency graph for the sentence Mr. Tomash will remain as a director emeritus, which has been extracted from the Penn Treebank (Marcus et al., 1993). Each edge in this graph represents a single syntactic dependency directed from a word to its modifier. In this representation all edges are labeled with the specific syntactic function of the dependency, e.g., SBJ for subject and NMOD for modifier of a noun. To simplify computation and some important definitions, an artificial token is inserted into the sentence as the left most word and will always represent the root of the dependency graph. We assume all dependency graphs are directed trees originating out of a single node, which is a common constraint (Nivre, 2005). The dependency graph i"
W07-2216,E06-1011,1,0.572749,"s are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al., 2005a). Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time (McDonald et al., 2005b). The primary problem in treating each dependency as independent is that it is not a realistic assumption. Non-local information, such as arity (or valency) and neighbouring dependencies, can be crucial to obtaining high parsing accuracies (Klein and Manning, 2002; McDonald and Pereira, 2006). However, in the data-driven parsing setting this can be partially adverted by incorporating rich feature representations over the input (McDonald et al., 2005a). The goal of this work is to further our current understanding of the computational nature of nonprojective parsing algorithms for both learning and inference within the data-driven setting. We start by investigating and extending the edge-factored model of McDonald et al. (2005b). In particular, we appeal to the Matrix Tree Theorem for multi-digraphs to design polynomial-time algorithms for calculating both the partition function an"
W07-2216,P05-1012,1,0.665191,"the model to new languages. One interesting class of data-driven models are 121 Proceedings of the 10th Conference on Parsing Technologies, pages 121–132, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics Figure 1: A projective dependency graph. Figure 2: Non-projective dependency graph. those that assume each dependency decision is independent modulo the global structural constraint that dependency graphs must be trees. Such models are commonly referred to as edge-factored since their parameters factor relative to individual edges of the graph (Paskin, 2001; McDonald et al., 2005a). Edge-factored models have many computational benefits, most notably that inference for nonprojective dependency graphs can be achieved in polynomial time (McDonald et al., 2005b). The primary problem in treating each dependency as independent is that it is not a realistic assumption. Non-local information, such as arity (or valency) and neighbouring dependencies, can be crucial to obtaining high parsing accuracies (Klein and Manning, 2002; McDonald and Pereira, 2006). However, in the data-driven parsing setting this can be partially adverted by incorporating rich feature representations ov"
W07-2216,H05-1066,1,0.749014,"Missing"
W07-2216,D07-1014,0,0.608651,"and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2007) showed that the MatrixTree Theorem can be used to train edge-factored log-linear models of dependency parsing. Both studies constructed implementations that compare favorably with the state-of-the-art. The work of Meil˘a and Jaakkola (2000) is also of note. In that study they use the Matrix Tree Theorem to develop a tractable bayesian learning algorithms for tree belief networks, which in many ways are closely related to probabilistic dependency parsing formalisms and the problems we address here. 2 Preliminaries Let L = {l1 , . . . , l|L |} be a set of permissible syntactic edge labels and x"
W07-2216,W04-0307,0,0.01824,"sually by making strong independence assumptions, as is the case for edge-factored models (Paskin, 2001; McDonald et al., 2005a; McDonald et al., 2005b). Recently there have also been proposals for exhaustive methods that weaken the edge-factored assumption, including both approximate methods (McDonald and Pereira, 2006) and exact methods through integer linear programming (Riedel and Clarke, 2006) or branch-and-bound algorithms (Hirakawa, 2006). For grammar based models there has been limited work on empirical systems for non-projective parsing systems, notable exceptions include the work of Wang and Harper (2004). Theoretical studies of note include the work of Neuhaus and B¨oker (1997) showing that the recognition problem for a minimal dependency grammar is hard. In addition, the work of Kahane et al. (1998) provides a polynomial parsing algorithm for a constrained class of nonprojective structures. Non-projective dependency parsing can be related to certain parsing problems defined for phrase structure representations, as for instance immediate dominance CFG parsing (Barton et al., 1987) and shake-and-bake translation (Brew, 1992). Independently of this work, Koo et al. (2007) and Smith and Smith (2"
W07-2216,P96-1021,0,0.100479,"Missing"
W07-2216,W03-3023,0,0.123858,"uggesting that the parsing problem is intractable in this case. For neighbouring parse decisions, we extend the work of McDonald and Pereira (2006) and show that modeling vertical neighbourhoods makes parsing intractable in addition to modeling horizontal neighbourhoods. A consequence of these results is that it is unlikely that exact non-projective dependency parsing is tractable for any model assumptions weaker than those made by the edge-factored models. 1.1 Related Work There has been extensive work on data-driven dependency parsing for both projective parsing (Eisner, 1996; Paskin, 2001; Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; McDonald et al., 2005a) and non-projective parsing systems (Nivre and Nilsson, 2005; Hall and N´ov´ak, 2005; McDonald et al., 2005b). These approaches can often be classified into two broad categories. In the first category are those methods that employ approximate inference, typically through the use of linear time shift-reduce parsing algorithms (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004; Nivre and Nilsson, 2005). In the second category are those that employ exhaustive inference algorithms, usually by making strong independence assumptions, as is the case f"
W07-2216,C98-1102,0,\N,Missing
W10-3110,N07-1009,0,0.0095951,"xical. Thus, this work is complimentary to those mentioned above in that we are measuring not only whether negation detection is useful for sentiment, but to what extent we can determine its exact scope in the text. Towards this end in we describe both an annotated negation span corpus as well as a negation span detector that is trained on the corpus. The span detector is based on conditional random fields (CRFs) (Lafferty, McCallum, and Pereira, 2001), which is a structured prediction learning framework common in sub-sentential natural language processing tasks, including sentiment analysis (Choi and Cardie, 2007; McDonald et al., 2007) The approach presented here resembles work by Morante and Daelemans (2009), who used IGTree to predict negation cues and a CRF metalearner that combined input from k-nearest neighbor classification, a support vector machine, and another underlying CRF to predict the scope of negations within the BioScope corpus. However, our work represents a simplified approach that replaces machine-learned cue prediction with a lexicon of explicit negation cues, and uses only a single CRF to predict negation scopes, with a more comprehensive model that includes features from a depend"
W10-3110,D08-1083,0,0.328317,"§4 describes the proposed negation scope detection sys52 tem. The novel system is evaluated in §5 in terms of raw results on the annotated negation corpora as well as the performance improvement on sentiment classification achieved by incorporating the negation system in a state-of-the-art sentiment analysis pipeline. Lessons learned and future directions are discussed in §6. 2 Related work Negation and its scope in the context of sentiment analysis has been studied in the past (Moilanen and Pulman, 2007). In this work we focus on explicit negation mentions, also called functional negation by Choi and Cardie (2008). However, others have studied various forms of negation within the domain of sentiment analysis, including work on content negators, which typically are verbs such as “hampered”, “lacked”, “denied”, etc. (Moilanen and Pulman, 2007; Choi and Cardie, 2008). A recent study by DanescuNiculescu-Mizil et al. (2009) looked at the problem of finding downward-entailing operators that include a wider range of lexical items, including soft negators such as the adverbs “rarely” and “hardly”. With the absence of a general purpose corpus annotating the precise scope of negation in sentiment corpora, many s"
W10-3110,N10-1119,1,0.476681,"on system within the context of a larger sentiment analysis system. The negation system was built into a sentiment analysis pipeline consisting of the following stages: 1. Sentence boundary detection. 2. Sentiment detection. 3. Negation scope detection, applying the system described in §4. 4. Sentence sentiment scoring. The sentiment detection system in stage 2 finds and scores mentions of n-grams found in a large lexicon of sentiment terms and phrases. The sentiment lexicon is based on recent work using label propagation over a very large distributional similarity graph derived from the web (Velikovich et al., 2010), and applies positive or negative scores to terms such as “good”, “bad”, or “just what the doctor ordered”. The sentence scoring system in stage 4 then determines whether any scored sentiment terms fall within the scope of a negation, and flips the sign of the sentiment score for all negated sentiment terms. The scoring system then sums all sentiment scores within each sentence and computes overall sentence sentiment scores. A sample of English-language online reviews was collected, containing a total of 1135 sentences. Human raters were presented with consecutive sentences and asked to class"
W10-3110,W08-0606,0,0.317463,"Missing"
W10-3110,P07-1055,1,0.692881,"is complimentary to those mentioned above in that we are measuring not only whether negation detection is useful for sentiment, but to what extent we can determine its exact scope in the text. Towards this end in we describe both an annotated negation span corpus as well as a negation span detector that is trained on the corpus. The span detector is based on conditional random fields (CRFs) (Lafferty, McCallum, and Pereira, 2001), which is a structured prediction learning framework common in sub-sentential natural language processing tasks, including sentiment analysis (Choi and Cardie, 2007; McDonald et al., 2007) The approach presented here resembles work by Morante and Daelemans (2009), who used IGTree to predict negation cues and a CRF metalearner that combined input from k-nearest neighbor classification, a support vector machine, and another underlying CRF to predict the scope of negations within the BioScope corpus. However, our work represents a simplified approach that replaces machine-learned cue prediction with a lexicon of explicit negation cues, and uses only a single CRF to predict negation scopes, with a more comprehensive model that includes features from a dependency parser. 3 Data sets"
W10-3110,N10-1120,0,0.0293055,"In the work of Wilson et al. (2005), a supervised polarity classifier is trained with a set of negation features derived from a list of cue words and a small window around them in the text. Choi and Cardie (2008) combine different kinds of negators with lexical polarity items through various compositional semantic models, both heuristic and machine learned, to improve phrasal sentiment analysis. In that work the scope of negation was either left undefined or determined through surface level syntactic patterns similar to the syntactic patterns from Moilanen and Pulman (2007). A recent study by Nakagawa et al. (2010) developed an semi-supervised model for sub-sentential sentiment analysis that predicts polarity based on the interactions between nodes in dependency graphs, which potentially can induce the scope of negation. As mentioned earlier, the goal of this work is to define a system that can identify exactly the scope of negation in free text, which requires a robustness to the wide variation of negation expression, both syntactic and lexical. Thus, this work is complimentary to those mentioned above in that we are measuring not only whether negation detection is useful for sentiment, but to what ext"
W10-3110,C04-1010,0,0.0155452,"sence of explicit negations, but provides no means of inferring the scope of negation. For scope detection, additional signals derived from surface and dependency level syntactic structure are employed. The negation scope detection system is built as an individual annotator within a larger annotation pipeline. The negation annotator relies on two distinct upstream annotators for 1) sentence boundary annotations, derived from a rule-based sentence boundary extractor and 2) token annotations from a dependency parser. The dependency parser is an implementation of the parsing systems described in Nivre and Scholz (2004) and Nivre et al. (2007). Each annotator marks the character offsets for the begin and end positions of individual annotation ranges within documents, and makes the annotations available to downstream processes. The dependency annotator controls multiple lower-level NLP routines, including tokenization and part of speech (POS) tagging in addition to parsing sentence level dependency structure. The output that is kept for downstream use includes only POS and dependency relations for each token. The tokenization performed at this stage is recycled when learning to identify negation scopes. The f"
W10-3110,H05-1044,0,0.160444,"k on content negators, which typically are verbs such as “hampered”, “lacked”, “denied”, etc. (Moilanen and Pulman, 2007; Choi and Cardie, 2008). A recent study by DanescuNiculescu-Mizil et al. (2009) looked at the problem of finding downward-entailing operators that include a wider range of lexical items, including soft negators such as the adverbs “rarely” and “hardly”. With the absence of a general purpose corpus annotating the precise scope of negation in sentiment corpora, many studies incorporate negation terms through heuristics or soft-constraints in statistical models. In the work of Wilson et al. (2005), a supervised polarity classifier is trained with a set of negation features derived from a list of cue words and a small window around them in the text. Choi and Cardie (2008) combine different kinds of negators with lexical polarity items through various compositional semantic models, both heuristic and machine learned, to improve phrasal sentiment analysis. In that work the scope of negation was either left undefined or determined through surface level syntactic patterns similar to the syntactic patterns from Moilanen and Pulman (2007). A recent study by Nakagawa et al. (2010) developed an"
W10-3110,W09-1105,0,\N,Missing
W18-5304,D18-1211,1,0.799843,"RMM uses pretrained word embeddings for q-terms and d-terms, and (bucketed) cosine similarity histograms (outputs of ⊗ nodes in Fig. 2). Each histogram captures the similarity of a q-term to all the d-terms of a particular document. The histograms, which in this model are the document-aware q-term encodings, are fed to an MLP (dense layers of Fig. 2) that produces the (document-aware) score of each q-term. Each q-term score is then weighted using 5 Hui et al. (2017) used an additional LSTM, which was later replaced by the final concatenation (Hui et al., 2018). 6 In the related publication of McDonald et al. (2018) TERM - PACRR is identical to the PACRR - DRMM model. 31 Document Terms Query Term ... ContextSensitive Term Encodings Residual sions of the pre-trained embedding, for residuals to be summed without transformation. Specifically, let e(ti ) be the pre-trained embedding for a q-term or d-term term ti . We compute the context-sensitive encoding of ti as:    (1)  φH (qi ) = dqi c(qi ) X ai,j c(dj ) (e.g., Hadamard Product) (4) The ⊗ nodes and lower parts of the DRMM network of Fig. 2 are now replaced by (multiple copies of) the sub-network of Fig. 4 (one copy per q-term), with the nodes replac"
W18-5304,P16-1177,0,0.0428002,"Missing"
W18-5304,W17-2328,0,0.118904,"the linear layer that combines the q-term scores (Fig. 1). In ABEL - DRMM, an additional linear layer is used that concatenates the deep learning document relevance score with the traditional IR features. In BCNN, the additional features are included in the final linear layer (Fig. 5). The additional features we used were the BM 25 score of the document (the document the snippet came from, in snippet retrieval), word overlap (binary and IDF weighted) between the query and the document or snippet; bigram overlap between the query and the document or snippet. The latter features were taken from Mohan et al. (2017). The additional features improved the performance of all models. A technique that seems to improve our results in snippet retrieval is to retain only the top Ks snippets with the best BCNN scores for each query, and then re-rank the Ks snippets by the relevance scores of the documents they came from; if two snippets came from the same document, they are subsequently ranked by their BCNN score. This is a proxy for more sophisticated models that would jointly consider document and snippet retrieval. This is important as the snippet retrieval model is trained under the condition that it only see"
W18-5304,D17-1110,0,0.0671104,"Missing"
W18-5304,W17-2348,0,0.112541,"Missing"
W18-5304,P15-2116,0,0.0717408,"Missing"
W18-5304,Q16-1019,0,0.335347,"Missing"
W18-5304,D14-1179,0,\N,Missing
W18-5304,N16-1174,0,\N,Missing
W18-5304,N18-1202,0,\N,Missing
W19-5032,D14-1179,0,0.0078856,"Missing"
W19-5032,D17-1070,0,0.0136659,". . . , wn i, a bidirectional GRU (BIGRU) computes two sets of n hidden state vectors, one for each direction. These two sets are then added to form the output H of the BIGRU: Hf = GRUf (e(w1 ), . . . , e(wn )) (4) Hb = GRUb (e(w1 ), . . . , e(wn )) (5) H = Hf + Hb (6) where f , b denote the forward and backward directions, and + indicates component-wise addition. We add residual connections (He et al., 2015) from each word embedding e(wt ) to the corresponding hidden state ht of H. Instead of using the final forward and backward states of H, we apply max-pooling (Collobert and Weston, 2008; Conneau et al., 2017) over the state vectors ht of H. The output of the max pooling is the node embedding f (v). Figure 3 illustrates this method. Additional experiments were conducted with several variants of the last encoder. A unidirectional GRU instead of a BIGRU, and a BIGRU with 301 Node Embedding f(v): X X ..... X ..... X Max-Pooling X X X X + + + + h1 h2 h3 h4 h1 h2 h3 h4 e1 e2 e3 e4 lumen of arterial trunk Node: Statistics Nodes Edges Training true positive edges Training true negative edges Test true positive edges Test true negative edges Avg. descriptor length Max. descriptor length 16,894 19,436 16,89"
W19-5032,D18-1211,1,0.879754,"Missing"
W19-5032,N19-1221,0,0.0218948,"embeddings from texts and network structure; and unlike WANE, we do not align the descriptors of different nodes. We generate the embedding of each node from the word embeddings of its descriptor via the RNN (Fig. 1), but the parameters of the RNN, the word embeddings, hence also the node embeddings are updated during training to predict NODE 2 VEC’s neighborhoods. Although we use NODE 2 VEC to incorporate network context in the node embeddings, other neighborhood embedding methods, such as GCNs, could easily be used too. Similarly, text encoders other than RNNs could be applied. For example, Mishra et al. (2019) try to detect abusive language in tweets with a semi-supervised learning approach based on GCNs. They exploit the network structure and also the labels associated with the tweets, taking into account the linguistic behavior of the authors. 3 Proposed Node Embedding Approach Consider a network (graph) G = hV, E, Si, where V is the set of nodes (vertices); E ⊆ V × V is the set of edges (links) between nodes; and S is a function that maps each node v ∈ V to its textual descriptor S(v) = hw1 , w2 , . . . , wn i, where n is the word length of the descriptor, and each word wi comes from a vocabular"
W19-5032,D18-1209,0,0.0655153,"inoma acute lymphocytic leukemia Figure 1: Example network with nodes associated with textual descriptors. a) A model where each node is represented by a vector (node embedding) from a look-up table. b) A model where each node embedding is generated compositionally from the word embeddings of its descriptor via an RNN. The latter model can learn node embeddings from both the network structure and the word sequences of the textual descriptors. as textual descriptors (labels) or other meta-data associated with the nodes. More recent NE methods, e.g., CANE (Tu et al., 1 Introduction 2017), WANE (Shen et al., 2018), produce embeddings by combining the network structure and the Network Embedding (NE) methods map each text associated with the nodes. These contentnode of a network to an embedding, meaning a oriented methods embed networks whose nodes low-dimensional feature vector. They are highly are rich textual objects (often whole documents). effective in network analysis tasks involving preThey aim to capture the compositionality and sedictions over nodes and edges, for example link mantic similarities in the text, encoding them with prediction (Lu and Zhou, 2010), and node classideep learning methods"
W19-5032,P17-1158,0,0.0516818,"al. (2003) strengthen Content-Aware-N2V 299 the argument of compositionality by observing that many GO terms contain other GO terms. Also, they argue that substrings that are not GO terms appear frequently and often indicate semantic relationships. Ogren et al. (2004) use finite state automata to represent GO terms and demonstrate how small conceptual changes can create biologically meaningful candidate terms. In other work on NE methods, CENE (Sun et al., 2016) treats textual descriptors as a special kind of node, and uses bidirectional recurrent neural networks (RNNs) to encode them. CANE (Tu et al., 2017) learns two embeddings per node, a textbased one and an embedding based on network structure. The text-based one changes when interacting with different neighbors, using a mutual attention mechanism. WANE (Shen et al., 2018) also uses two types of node embeddings, text-based and structure-based. For the text-based embeddings, it matches important words across the textual descriptors of different nodes, and aggregates the resulting alignment features. In spite of performance improvements over structure-oriented approaches, these content-aware methods do not thoroughly explore the network struct"
