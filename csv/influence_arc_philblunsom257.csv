2020.acl-main.231,P08-1090,0,0.103392,"Missing"
2020.acl-main.231,W02-1002,0,\N,Missing
2020.acl-main.231,P08-1100,0,\N,Missing
2020.acl-main.382,D15-1075,0,0.0498638,"ly be automated, for example, by training a neural network to generate task-specific inconsistencies after crowd-sourcing a dataset of inconsistent explanations for a task at hand — we leave this as future work. Finally, to execute step (2d), our framework currently checks for an exact string match between a reverse explanation and any of the inconsistent explanations created at step (2a). Alternatively, one can train a model to identify if a pair of explanations forms an inconsistency, which we also leave as future work. 3 Experiments We consider the task of natural language inference (NLI) (Bowman et al., 2015), which consists of detecting whether a pair of sentences, called premise and hypothesis, are in a relation of: entailment, if the premise entails the hypothesis; contradiction, if the premise contradicts the hypothesis; or neutral, if neither entailment nor contradiction holds. For example, a pair with premise “Two doctors perform surgery on patient.” and hypothesis “Two doctors are performing surgery on a man.” constitutes a neutral pair. The SNLI corpus (Bowman et al., 2015) of ∼570K such human-written instances enabled a plethora of works on this task (Rockt¨aschel et al., 2015; Munkhdalai"
2020.acl-main.382,N18-1179,0,0.0474311,"Missing"
2020.acl-main.382,K18-1007,1,0.855825,"Missing"
2020.acl-main.382,P17-1015,1,0.834672,"on a state-of-the-art neural natural language inference model that provides natural language explanations for its predictions. Our framework shows that this model is capable of generating a significant number of inconsistent explanations. 1 Introduction In order to explain the predictions produced by accurate yet black-box neural models, a growing number of works propose extending these models with natural language explanation generation modules, thus obtaining models that explain themselves in human language (Hendricks et al., 2016; Camburu et al., 2018; Park et al., 2018; Kim et al., 2018; Ling et al., 2017). In this work, we first draw attention to the fact that such models, while appealing, are nonetheless prone to generating inconsistent explanations. We define two explanations to be inconsistent if they provide contradictory arguments about the instances and predictions that they aim to explain. For example, consider a visual question answering (VQA) task (Park et al., 2018) and two instances where the image is the same but the questions are different, say “Is there an animal in the image?” and “Can you see a Husky in the image?”. If for the first instance a model predicts “Yes.” and generate"
2020.findings-emnlp.106,N19-1423,0,0.140809,"er labeled dataset (Figure 1). 1183 Unsupervised Pre-training with CPC Semi-supervised Speech Recognition T H E _ D O G _ I S _ …. CTC …. ( 100 Hz ) …. : Causal conv …. ( 16kHz / 160 = 100 Hz ) : Causal strided conv …. ( 16kHz ) Pre-trained parameters are ﬁxed. Figure 1: Left, unsupervised representation learning with forward contrastive predictive coding. The learned representations are fixed and used as inputs to a speech recognition model (Right). 3.1 Unsupervised learning with bi-directional CPC Following the success of bidirectional models in representation learning (Peters et al., 2018; Devlin et al., 2019), we extend the original CPC method explained above with bidirectional context networks. The encoder function genc is shared for both directions, but there are two autoregresfwd bwd ) which read encoded sive models (gar and gar observations (z) from the forward and backward contexts, respectively. The forward and backward bwd are learned with context representations cfwd t , ct separate InfoNCE losses. When they are used for downstream tasks, a concatenation of two reprebwd sentations ct = [cfwd t ; ct ] is used. A similar technique has been used in image representation learning where represen"
2020.findings-emnlp.106,L16-1611,0,0.0209422,"h recognition systems on that task (Kuchaiev et al., 2018). For the other datasets, transcriptions are lowercased 1185 and unpronounced symbols (e.g., punctuation, silence markers) are removed. We also remove utterances containing numbers as they are transcribed inconsistently across and within datasets. Transcribed multilingual speech In order to evaluate the transferability of the representations, we use speech recognition datasets in 4 African languages collected by the ALFFA project,3 Amharic (Tachbelie et al., 2014), Fongbe (A. A Laleye et al., 2016), Swahili (Gelas et al., 2012), Wolof (Gauthier et al., 2016), for evaluation. These languages have unique phonological properties (e.g. height harmony) and phonetic inventories, making them a good contrast to English. These African languages are low-resource, each with 20 hours or less of transcribed speech. We also use 21 phonetically diverse languages from OpenSLR.4 See Appendix A for more detail. 4.2 Unsupervised Representation Learning We train the model described above (§3.1) using the datasets described in the previous section (§4.1). Similarly to Schneider et al. (2019)), audio signals are randomly cropped with a window size 149,600 observations"
2020.findings-emnlp.106,N18-1202,0,0.0239965,"a different or smaller labeled dataset (Figure 1). 1183 Unsupervised Pre-training with CPC Semi-supervised Speech Recognition T H E _ D O G _ I S _ …. CTC …. ( 100 Hz ) …. : Causal conv …. ( 16kHz / 160 = 100 Hz ) : Causal strided conv …. ( 16kHz ) Pre-trained parameters are ﬁxed. Figure 1: Left, unsupervised representation learning with forward contrastive predictive coding. The learned representations are fixed and used as inputs to a speech recognition model (Right). 3.1 Unsupervised learning with bi-directional CPC Following the success of bidirectional models in representation learning (Peters et al., 2018; Devlin et al., 2019), we extend the original CPC method explained above with bidirectional context networks. The encoder function genc is shared for both directions, but there are two autoregresfwd bwd ) which read encoded sive models (gar and gar observations (z) from the forward and backward contexts, respectively. The forward and backward bwd are learned with context representations cfwd t , ct separate InfoNCE losses. When they are used for downstream tasks, a concatenation of two reprebwd sentations ct = [cfwd t ; ct ] is used. A similar technique has been used in image representation l"
2020.findings-emnlp.106,H92-1073,0,0.63971,"English Datasets. tions. Finally, we include the audio (again ignoring transcriptions) from the standard training splits of the evaluation datasets below. This collection spans a range of recording conditions, noise levels, speaking styles, and languages and amounts to about 8000 hours of audio. Transcribed read English For evaluation, we look at the performance of our representations on a variety of standard English recognition tasks, as well as their ability to be trained on one and tested on another. For read English, we use LibriSpeech (Panayotov et al., 2015) and the Wall Street Journal (Paul and Baker, 1992). Transcribed spoken English To explore more extreme domain shifts, we additionally used conversational speech and public speaking datasets. We used Switchboard (Godfrey et al., 1992), a standard conversational speech recognition dataset consisting of two-sided telephone conversations (test only). Since the data was recorded more than 10 years ago and at a lower sampling rate than the other corpora, it presents a noisy and challenging recognition problem. Finally, we also use the Tedlium-3 (Hernandez et al., 2018) corpus, a large spoken English dataset containing 450 hours of speech extracted"
2020.findings-emnlp.106,W18-2507,0,\N,Missing
2020.tacl-1.23,D18-1045,0,0.339892,"age model X is trained on dataset Y. A bigger language model improves the doc-reranker but does not help the sent-reranker. Architecture Data PPL transformer-XL transformer-XL NIST sent NIST + GW sent 83.3 96.5 LSTM transformer-XL transformer-XL NIST doc NIST doc NIST + GW doc 71.6 43.8 43.4 Table 3: Perplexity per word of language models on NIST dev set. GW refers to Gigaword. Figure 3: Effect of n-best list. about the reliability of using BLEU at assessing cross-sentential consistency (Voita et al., 2019b). To compare the effectiveness of leveraging monolingual data between backtranslation (Edunov et al., 2018; Sennrich et al., 2016a) and our model, we train the document transformer (Zhang et al., 2018) using additional synthetic parallel documents generated by backtranslation (q ′ ). For fair comparison we use the same monolingual data for both models. As shown in Table 1, although both techniques improve translation, backtranslation is less effective than our model. Because we have a new model q ′ , we can use it as a proposal model for our doc-reranker—effectively using the monolingual data twice. We find that this improves results even further, indicating that the effect of both approaches is a"
2020.tacl-1.23,P19-1019,0,0.0128348,"ed must be carefully selected; the ratio of backtranslated data and original data must be balanced carefully. While techniques for doing this are fairly well established for single sentence models, no such established techniques exist for documents. More generally, strategies for using monolingual data in nueral MT systems is an active research area (G¨ulc¸ehre et al., 2015; Cheng et al., 2016, inter alia). Backtranslation (Edunov et al., 2018; Sennrich et al., 2016a), originally invented for semi-supervised MT, has been used as a standard approach for unsupervised MT (Lample et al., 2018a,b; Artetxe et al., 2019, 2018). Noisy channel decompositions have been a standard approach in statistical machine translation (Brown et al., 1993; Koehn et al., 2007) and recently have been applied to neural models (Yu et al., 2017; Yee et al., 2019; A Appendix Ng et al., 2019). Unlike prior work, we adopt noisy channel models for document-level MT. While the model from Yu et al. (2017) could be used on documents by concatenating their sentences to form a single long sequence, this would not let us use the conditional sentence independence assumptions that gives our model the flexibility to use just parallel sentenc"
2020.tacl-1.23,J93-1004,0,0.777583,"Description We define x = (x1 , x2 , . . . , xI ) as the source document with I sentences, and similarly, y = (y1 , y 2 , . . . , y J ) as the target document with J sentences. During the (human) translation process, translators may split or recombine sentences, but we will assume that I = J .1 Let xi = (xi1 , xi2 , . . . , xiM ) represent the ith sentence in the document, consisting of M words; likewise y i = i (y1i , y2i , . . . , yN ) denote the ith sentence in the target document, containing N words. 1 Size mismatches are addressed by merging sentences using sentence alignment algorithms (Gale and Church, 1993). The translation of a document x is determined ˆ , where p(ˆ y |x) is by finding the document y optimal. ˆ = arg max p(y |x). y (1) y Instead of modeling the probability p(y |x) directly, we factorize it using Bayes’ rule: ˆ = arg max y y p( x |y ) × p( y ) p(x) = arg max p(x |y ) × y |{z } channel model p( y ) |{z} . (2) language model We further assume that sentences are independently translated, and that the sentences are generated by a left-to-right factorization according to the chain rule. Therefore, we have ˆ ≈ arg max y y |x| Y Figure 1: Graphical model showing the factorization of ou"
2020.tacl-1.23,D18-1549,0,0.0430969,"Missing"
2020.tacl-1.23,N18-1118,0,0.420146,"lled a ‘‘noisy channel’’ decomposition, by analogy to an information theoretic conception of Bayes’ rule. Whereas several recent papers have demonstrated that the noisy channel decomposition has benefits when translating sentences one-by-one (Yu et al., 2017; Yee et al., 2019; Ng et al., 2019), in this paper we show that this decomposition is particularly suited to tackling the problem of translating complete documents. Although using crosssentence context and maintaining cross-document consistency has long been recognized as essential to the translation problem (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia), operationalizing this in models has been challenging for several reasons. Most prosaically, parallel documents are not generally available (whereas parallel sentences are much more numerous), making direct estimation of document translation probabilities challenging. More subtly, documents are considerably more diverse than sentences, and models must be carefully biased so as not to pick up spurious correlations. Our Bayes’ rule decomposition (§2) permits several innovations that enable us to solve these problems. Rather than directly modeling the conditional distribution, we re"
2020.tacl-1.23,J93-2003,0,0.192577,"Missing"
2020.tacl-1.23,P18-1118,0,0.0611387,"guage understanding, and semi-supervised machine translation. Recent studies (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia) 355 have shown that exploiting document-level context improves translation performance, and in particular improves lexical consistency and coherence of the translated text. Existing work in the area of context-aware NMT typically adapts the MT system to take additional context as input, either a few previous sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Werlen et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al., 2019). These methods vary in the method of encoding the additional context and the way of integrating the context with the existing sequenceto-sequence models. For example, Werlen et al. (2018) encode the context with a separate transformer encoder (Vaswani et al., 2017) and use a hierarchical attention model to integrate the context into the rest of transformer model. Zhang et al. (2018) introduce an extra self-attention layer in the encoder to attend over the the context. Strategies for exploiting monolingual documentlevel data have been explored in two recent studies (Voita"
2020.tacl-1.23,P16-1185,0,0.0212721,"that 1) the language model is portable across domain and language pairs; 2) our model involves straightforward training procedures. Specifically, for backtranslation to succeed, monolingual data that will be back-translated must be carefully selected; the ratio of backtranslated data and original data must be balanced carefully. While techniques for doing this are fairly well established for single sentence models, no such established techniques exist for documents. More generally, strategies for using monolingual data in nueral MT systems is an active research area (G¨ulc¸ehre et al., 2015; Cheng et al., 2016, inter alia). Backtranslation (Edunov et al., 2018; Sennrich et al., 2016a), originally invented for semi-supervised MT, has been used as a standard approach for unsupervised MT (Lample et al., 2018a,b; Artetxe et al., 2019, 2018). Noisy channel decompositions have been a standard approach in statistical machine translation (Brown et al., 1993; Koehn et al., 2007) and recently have been applied to neural models (Yu et al., 2017; Yee et al., 2019; A Appendix Ng et al., 2019). Unlike prior work, we adopt noisy channel models for document-level MT. While the model from Yu et al. (2017) could be"
2020.tacl-1.23,N19-1213,0,0.100327,"Missing"
2020.tacl-1.23,W19-5321,0,0.0352241,"2019). These methods vary in the method of encoding the additional context and the way of integrating the context with the existing sequenceto-sequence models. For example, Werlen et al. (2018) encode the context with a separate transformer encoder (Vaswani et al., 2017) and use a hierarchical attention model to integrate the context into the rest of transformer model. Zhang et al. (2018) introduce an extra self-attention layer in the encoder to attend over the the context. Strategies for exploiting monolingual documentlevel data have been explored in two recent studies (Voita et al., 2019a; Junczys-Dowmunt, 2019). Both use backtranslation (Edunov et al., 2018; Sennrich et al., 2016a) to create synthetic parallel documents as additional training data. In contrast, we train a large-scale language model and use it to refine the consistency between sentences under a noisy channel framework. Advantages of our model over back-translation are that 1) the language model is portable across domain and language pairs; 2) our model involves straightforward training procedures. Specifically, for backtranslation to succeed, monolingual data that will be back-translated must be carefully selected; the ratio of backt"
2020.tacl-1.23,D16-1139,0,0.0241688,"y 2.5 BLEU. 353 The two best systems submitted to the WMT19 Chinese–English translation task are Microsoft Research Asia’s system (Xia et al., 2019) and Baidu’s system (Sun et al., 2019), both of which use multiple techniques to improve upon the transformer big model. Here, we mainly compare our results with those from Xia et al. (2019) because we use the same evaluation metric SacreBLEU (Post, 2018) and the same validation and test sets. Using extra parallel training data and the techniques of masked sequence-to-sequence pretraining (Song et al., 2019), sequence-level knowledge distillation (Kim and Rush, 2016), and backtranslation (Edunov et al., 2018), the best model from Xia et al. (2019) achieves 30.8, 30.9, and 39.3 on newstest2017, newstest2018, and newstest2019, respectively. Although our best results are lower than this, it is notable that our model achieves comparable results to their model, which was trained on 56M sentences of parallel data—over two times more training data than we use. However, our method is orthogonal to these works and can be combined with other techniques to make further improvement. 5 Analysis In this section, we present the quantitative and qualitative analysis of o"
2020.tacl-1.23,N19-1423,0,0.142685,"oss-sentence context in the language model, and it outperforms existing document translation approaches. 1 Introduction There have been many recent demonstrations that neural language models based on transformers (Vaswani et al., 2017; Dai et al., 2019) are capable of learning to generate remarkably coherent documents with few (Zellers et al., 2019) or no (Radford et al., 2019) conditioning variables. Despite this apparent generation ability, in practical applications, unconditional language models are most often used to provide representations for natural language understanding applications (Devlin et al., 2019; Yang et al., 2019; Peters 346 Transactions of the Association for Computational Linguistics, vol. 8, pp. 346–360, 2020. https://doi.org/10.1162/tacl a 00319 Action Editor: David Chiang. Submission batch: 12/2019; Revision batch: 2/2020; Published 6/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. conditional distribution to learning two different distributions: a language model p(y ), which provides unconditional estimates of the output (in this paper, documents); and p(x |y ), which provides the probability of translating a candidate output y in"
2020.tacl-1.23,P07-2045,1,0.0265342,"log q (y i |x)+ log pLM (y i |y <i )+ λ2 log pTM (xi |y i ) + λ3 |y i |+ O(x, y <i−1 , y i−1 ), (4) where |y |denotes the number of tokens in the sentence y , and where the base case O(x, y <0 , y 0 ) = 0. Note that Eq. 4 is a generalization of Eq. 3 in log space—if we set λ1 = λ3 = 0 and λ2 = 1 and take the log of Equation 3 the two objectives are equivalent. The extra factors—the proposal probability and the length of the output—provide improvements (e.g., by calibrating the expected length of the output), and can be incorporated at no cost in the model; they are widely used in prior work (Koehn et al., 2007; Yu et al., 2017; Yee 2 Our proposal model can optionally use document context on the source (conditioning) side, but sentences are generated independently. which contains 169 documents, 2,001 sentences and 275 documents, 3,981 sentences, respectively. The test set is newstest2019, containing 163 documents and 2,000 sentences. On average, documents in the test set have 12 sentences, and 360 words and 500 words on the Chinese and English sides, respectively. The dataset is preprocessed by segmenting Chinese sentences and normalizing punctuation, tokenizing, and truecasing English sentences. As"
2020.tacl-1.23,N18-1202,0,0.0527799,"Missing"
2020.tacl-1.23,W18-6319,0,0.0719972,"s, 2,001 sentences and 275 documents, 3,981 sentences, respectively. The test set is newstest2019, containing 163 documents and 2,000 sentences. On average, documents in the test set have 12 sentences, and 360 words and 500 words on the Chinese and English sides, respectively. The dataset is preprocessed by segmenting Chinese sentences and normalizing punctuation, tokenizing, and truecasing English sentences. As for NIST, we learn a byte pair encoding (Sennrich et al., 2016b) with 32K merges to segment words into sub-word units for both Chinese and English. The evaluation metric is sacreBLEU (Post, 2018). et al., 2019; Ng et al., 2019). The elements on the beam after considering the ℓth sentence are reranked one final time by adding log pLM (hSTOPi | y ≤ℓ ) to the final score; this accounts for the language model’s assessment that the candidate document has been appropriately ended.3 4 Experiments We evaluate our model on two translation tasks, the NIST Open MT Chinese–English task4 and the WMT19 Chinese–English news translation task.5 On both tasks, we use the standard parallel training data, and compare our model with a strong transformer baseline, as well as related models from prior work."
2020.tacl-1.23,P16-1009,0,0.666657,"l can optionally use document context on the source (conditioning) side, but sentences are generated independently. which contains 169 documents, 2,001 sentences and 275 documents, 3,981 sentences, respectively. The test set is newstest2019, containing 163 documents and 2,000 sentences. On average, documents in the test set have 12 sentences, and 360 words and 500 words on the Chinese and English sides, respectively. The dataset is preprocessed by segmenting Chinese sentences and normalizing punctuation, tokenizing, and truecasing English sentences. As for NIST, we learn a byte pair encoding (Sennrich et al., 2016b) with 32K merges to segment words into sub-word units for both Chinese and English. The evaluation metric is sacreBLEU (Post, 2018). et al., 2019; Ng et al., 2019). The elements on the beam after considering the ℓth sentence are reranked one final time by adding log pLM (hSTOPi | y ≤ℓ ) to the final score; this accounts for the language model’s assessment that the candidate document has been appropriately ended.3 4 Experiments We evaluate our model on two translation tasks, the NIST Open MT Chinese–English task4 and the WMT19 Chinese–English news translation task.5 On both tasks, we use the"
2020.tacl-1.23,P16-1162,0,0.850069,"l can optionally use document context on the source (conditioning) side, but sentences are generated independently. which contains 169 documents, 2,001 sentences and 275 documents, 3,981 sentences, respectively. The test set is newstest2019, containing 163 documents and 2,000 sentences. On average, documents in the test set have 12 sentences, and 360 words and 500 words on the Chinese and English sides, respectively. The dataset is preprocessed by segmenting Chinese sentences and normalizing punctuation, tokenizing, and truecasing English sentences. As for NIST, we learn a byte pair encoding (Sennrich et al., 2016b) with 32K merges to segment words into sub-word units for both Chinese and English. The evaluation metric is sacreBLEU (Post, 2018). et al., 2019; Ng et al., 2019). The elements on the beam after considering the ℓth sentence are reranked one final time by adding log pLM (hSTOPi | y ≤ℓ ) to the final score; this accounts for the language model’s assessment that the candidate document has been appropriately ended.3 4 Experiments We evaluate our model on two translation tasks, the NIST Open MT Chinese–English task4 and the WMT19 Chinese–English news translation task.5 On both tasks, we use the"
2020.tacl-1.23,N19-1313,0,0.376484,"bles, conditional dependencies between all y i ’s and between each y i and all xi ’s are created (Shachter, 1998). The (in)dependencies that are present in the posterior distribution are shown in the bottom of Figure 1. Thus, although modeling p(y |x) or p(x |y ) would appear to be superficially similar, the 348 statistical impact of making a conditional independence assumption is quite different. This is fortunate, as it makes it straightforward to use parallel sentences, rather than assuming we have parallel documents, which are less often available (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia). Finally, because we only need to learn to model the likelihood of sentence translations (rather than document translations), the challenges of guiding the learners to make robust generalizations in direct document translation models (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia) are neatly avoided. 2.2 Learning We can parameterize the channel probability p(xi |y i ) using any sequence-to-sequence model and parameterize the language model p(y i |y <i ) using any language model. It is straightforward to learn our model: We simply optimize the channel mod"
2020.tacl-1.23,2006.amta-papers.25,0,0.0900797,"Missing"
2020.tacl-1.23,W19-5333,0,0.267637,"onal task: machine translation. The application of Bayes’ rule to transform the translation modeling problem p(y |x), where y is the target language, and x is the source language, has a long tradition and was the dominant paradigm in speech and language processing for many years (Brown et al., 1993), where it is often called a ‘‘noisy channel’’ decomposition, by analogy to an information theoretic conception of Bayes’ rule. Whereas several recent papers have demonstrated that the noisy channel decomposition has benefits when translating sentences one-by-one (Yu et al., 2017; Yee et al., 2019; Ng et al., 2019), in this paper we show that this decomposition is particularly suited to tackling the problem of translating complete documents. Although using crosssentence context and maintaining cross-document consistency has long been recognized as essential to the translation problem (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia), operationalizing this in models has been challenging for several reasons. Most prosaically, parallel documents are not generally available (whereas parallel sentences are much more numerous), making direct estimation of document translation probabilities chall"
2020.tacl-1.23,W19-5341,0,0.0121626,"spensable for the doc-reranker, indicating that Bayes’ rule provides reliable estimates of translation probabilities. Table 5 presents the results of our model together with baselines on the WMT19 Chinese– English translation task. We find that the results follow the same pattern as those on NIST: A better language model leads to better translation results and overall the reranker outperforms the transformer-big by approximately 2.5 BLEU. 353 The two best systems submitted to the WMT19 Chinese–English translation task are Microsoft Research Asia’s system (Xia et al., 2019) and Baidu’s system (Sun et al., 2019), both of which use multiple techniques to improve upon the transformer big model. Here, we mainly compare our results with those from Xia et al. (2019) because we use the same evaluation metric SacreBLEU (Post, 2018) and the same validation and test sets. Using extra parallel training data and the techniques of masked sequence-to-sequence pretraining (Song et al., 2019), sequence-level knowledge distillation (Kim and Rush, 2016), and backtranslation (Edunov et al., 2018), the best model from Xia et al. (2019) achieves 30.8, 30.9, and 39.3 on newstest2017, newstest2018, and newstest2019, respe"
2020.tacl-1.23,W17-4811,0,0.298707,"., 1993), where it is often called a ‘‘noisy channel’’ decomposition, by analogy to an information theoretic conception of Bayes’ rule. Whereas several recent papers have demonstrated that the noisy channel decomposition has benefits when translating sentences one-by-one (Yu et al., 2017; Yee et al., 2019; Ng et al., 2019), in this paper we show that this decomposition is particularly suited to tackling the problem of translating complete documents. Although using crosssentence context and maintaining cross-document consistency has long been recognized as essential to the translation problem (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia), operationalizing this in models has been challenging for several reasons. Most prosaically, parallel documents are not generally available (whereas parallel sentences are much more numerous), making direct estimation of document translation probabilities challenging. More subtly, documents are considerably more diverse than sentences, and models must be carefully biased so as not to pick up spurious correlations. Our Bayes’ rule decomposition (§2) permits several innovations that enable us to solve these problems. Rather than directly modeling the conditiona"
2020.tacl-1.23,Q18-1029,0,0.395626,"h closely related, our algorithm is much simpler and faster than that proposed in Yu et al. (2017). Rather than using a specially designed channel model (Yu et al., 2016) which is limited in process347 ing long sequences like documents, our conditional sentence independence assumptions allow us to use any sequence-to-sequence model as the channel model, making it a better option for document-level translation. To explore the performance of our proposed model, we focus on Chinese–English translation, following a series of papers on document translation (Zhang et al., 2018; Werlen et al., 2018; Tu et al., 2018; Xiong et al., 2019). Although in general it is unreasonable to expect that independent translations of sentences would lead to coherent translations of documents, the task of translating Chinese into English poses some particularly acute challenges. As Chinese makes fewer inflectional distinctions than English does, and the relevant clues for predicting, for example, what tense an English verb should be in, or whether an English noun should have singular or plural morphology, may be spread throughout a document, it is crucial that extra-sentential context is used. Our experiments (§4) explor"
2020.tacl-1.23,D19-1571,0,0.48966,"improve a conditional task: machine translation. The application of Bayes’ rule to transform the translation modeling problem p(y |x), where y is the target language, and x is the source language, has a long tradition and was the dominant paradigm in speech and language processing for many years (Brown et al., 1993), where it is often called a ‘‘noisy channel’’ decomposition, by analogy to an information theoretic conception of Bayes’ rule. Whereas several recent papers have demonstrated that the noisy channel decomposition has benefits when translating sentences one-by-one (Yu et al., 2017; Yee et al., 2019; Ng et al., 2019), in this paper we show that this decomposition is particularly suited to tackling the problem of translating complete documents. Although using crosssentence context and maintaining cross-document consistency has long been recognized as essential to the translation problem (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia), operationalizing this in models has been challenging for several reasons. Most prosaically, parallel documents are not generally available (whereas parallel sentences are much more numerous), making direct estimation of document translation p"
2020.tacl-1.23,D19-1081,0,0.749257,"i ’s. By conditioning on the child variables, conditional dependencies between all y i ’s and between each y i and all xi ’s are created (Shachter, 1998). The (in)dependencies that are present in the posterior distribution are shown in the bottom of Figure 1. Thus, although modeling p(y |x) or p(x |y ) would appear to be superficially similar, the 348 statistical impact of making a conditional independence assumption is quite different. This is fortunate, as it makes it straightforward to use parallel sentences, rather than assuming we have parallel documents, which are less often available (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia). Finally, because we only need to learn to model the likelihood of sentence translations (rather than document translations), the challenges of guiding the learners to make robust generalizations in direct document translation models (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia) are neatly avoided. 2.2 Learning We can parameterize the channel probability p(xi |y i ) using any sequence-to-sequence model and parameterize the language model p(y i |y <i ) using any language model. It is straightforward to learn our"
2020.tacl-1.23,P19-1116,0,0.456022,"i ’s. By conditioning on the child variables, conditional dependencies between all y i ’s and between each y i and all xi ’s are created (Shachter, 1998). The (in)dependencies that are present in the posterior distribution are shown in the bottom of Figure 1. Thus, although modeling p(y |x) or p(x |y ) would appear to be superficially similar, the 348 statistical impact of making a conditional independence assumption is quite different. This is fortunate, as it makes it straightforward to use parallel sentences, rather than assuming we have parallel documents, which are less often available (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia). Finally, because we only need to learn to model the likelihood of sentence translations (rather than document translations), the challenges of guiding the learners to make robust generalizations in direct document translation models (Voita et al., 2019b; Zhang et al., 2018; Maruf et al., 2019, inter alia) are neatly avoided. 2.2 Learning We can parameterize the channel probability p(xi |y i ) using any sequence-to-sequence model and parameterize the language model p(y i |y <i ) using any language model. It is straightforward to learn our"
2020.tacl-1.23,D16-1138,1,0.769514,"r reverse translation model treats sentences independently. The search is guided by a proposal distribution that provides candidate continuations of a document prefix, and these are reranked according to the posterior distribution. In particular, we compare two proposal models: one based on estimates of independent sentence translations (Vaswani et al., 2017) and one that conditions on the source document context (Zhang et al., 2018). Although closely related, our algorithm is much simpler and faster than that proposed in Yu et al. (2017). Rather than using a specially designed channel model (Yu et al., 2016) which is limited in process347 ing long sequences like documents, our conditional sentence independence assumptions allow us to use any sequence-to-sequence model as the channel model, making it a better option for document-level translation. To explore the performance of our proposed model, we focus on Chinese–English translation, following a series of papers on document translation (Zhang et al., 2018; Werlen et al., 2018; Tu et al., 2018; Xiong et al., 2019). Although in general it is unreasonable to expect that independent translations of sentences would lead to coherent translations of d"
2020.tacl-1.23,P18-1117,0,0.0741973,"arch: context-aware neural machine translation, large-scale language models for language understanding, and semi-supervised machine translation. Recent studies (Tiedemann and Scherrer, 2017; Bawden et al., 2018, inter alia) 355 have shown that exploiting document-level context improves translation performance, and in particular improves lexical consistency and coherence of the translated text. Existing work in the area of context-aware NMT typically adapts the MT system to take additional context as input, either a few previous sentences (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Werlen et al., 2018) or the full document (Haffari and Maruf, 2018; Maruf et al., 2019). These methods vary in the method of encoding the additional context and the way of integrating the context with the existing sequenceto-sequence models. For example, Werlen et al. (2018) encode the context with a separate transformer encoder (Vaswani et al., 2017) and use a hierarchical attention model to integrate the context into the rest of transformer model. Zhang et al. (2018) introduce an extra self-attention layer in the encoder to attend over the the context. Strategies for ex"
2020.tacl-1.23,D17-1301,0,0.528329,"e-translation-evaluation. 5 http://www.statmt.org/wmt19/translationtask.html. 350 For WMT19, we use the transformer as both the channel and proposal model. The hyperparameters for training the transformer is the same as transformer big (Vaswani et al., 2017), namely, 1,024 hidden size, 4,096 filter size, 16 attention heads, and 6 layers. The model is trained on 8 GPUs with batch size of 4,096. The setup for the language model is the same as that of NIST except that the training data is the English side of the parallel training data and Gigaword. Method Model Proposal MT06 MT03 MT04 MT05 MT08 (Wang et al., 2017) (Kuang et al., 2017) (Zhang et al., 2018) RNNsearch Transformer + cache Doc-transformer – – – 37.76 48.14 49.69 – 48.05 50.21 – 47.91 49.73 36.89 48.53 49.46 27.57 38.38 39.69 Baseline Sent-transformer Doc-transformer (q ) Backtranslation (q ′ ) Sent-reranker – – – q 47.72 49.79 50.77 51.33 47.21 49.29 51.80 52.23 49.08 50.17 51.61 52.36 46.86 48.99 51.81 51.63 40.18 41.70 42.47 43.63 Doc-reranker Doc-reranker q q′ 51.99 53.63 52.77 54.51 52.84 54.23 51.84 54.86 44.17 45.17 This work Table 1: Comparison with prior work on NIST Chinese–English translation task. The evaluation metric is tokeniz"
2020.tacl-1.23,D18-1325,0,0.330585,"t al., 2018). Although closely related, our algorithm is much simpler and faster than that proposed in Yu et al. (2017). Rather than using a specially designed channel model (Yu et al., 2016) which is limited in process347 ing long sequences like documents, our conditional sentence independence assumptions allow us to use any sequence-to-sequence model as the channel model, making it a better option for document-level translation. To explore the performance of our proposed model, we focus on Chinese–English translation, following a series of papers on document translation (Zhang et al., 2018; Werlen et al., 2018; Tu et al., 2018; Xiong et al., 2019). Although in general it is unreasonable to expect that independent translations of sentences would lead to coherent translations of documents, the task of translating Chinese into English poses some particularly acute challenges. As Chinese makes fewer inflectional distinctions than English does, and the relevant clues for predicting, for example, what tense an English verb should be in, or whether an English noun should have singular or plural morphology, may be spread throughout a document, it is crucial that extra-sentential context is used. Our experi"
2020.tacl-1.23,K19-1074,0,0.0859984,"Missing"
2020.tacl-1.23,D18-1049,0,0.0540894,"uch harder decoding problem (§3). To address this problem, we propose a new beam-search algorithm, exploiting the fact that our document language model operates left-to-right, and our reverse translation model treats sentences independently. The search is guided by a proposal distribution that provides candidate continuations of a document prefix, and these are reranked according to the posterior distribution. In particular, we compare two proposal models: one based on estimates of independent sentence translations (Vaswani et al., 2017) and one that conditions on the source document context (Zhang et al., 2018). Although closely related, our algorithm is much simpler and faster than that proposed in Yu et al. (2017). Rather than using a specially designed channel model (Yu et al., 2016) which is limited in process347 ing long sequences like documents, our conditional sentence independence assumptions allow us to use any sequence-to-sequence model as the channel model, making it a better option for document-level translation. To explore the performance of our proposed model, we focus on Chinese–English translation, following a series of papers on document translation (Zhang et al., 2018; Werlen et al"
2020.tacl-1.50,P16-1231,0,0.0149245,"This non-incremental procedure is justified, however, because we aim to design the most informative teacher distributions for the nonincremental BERT student, which also has access to bidirectional context. i∈M (x) w ∈Σ i log pθ (˜ xi = w|c(x1 ), · · · , c(xk )) , 9 780 We use the same setup as Preliminary Experiments. where t˜φ,ω (w|x<i , x>i ) is our approximation of tφ (w|x<i , x>i ), as defined in Eqs. 5 and 6. teacher that we distill into BERT also uses phrasestructure trees, this setup is related to self-training (Yarowsky, 1995; Charniak, 1997; Zhou and Li, 2005; McClosky et al., 2006; Andor et al., 2016, inter alia). Interpolation. The RNNG teacher is an expert on syntax, although in practice it is only feasible to train it on a much smaller dataset. Hence, we not only want the BERT student to learn from the RNNG’s syntactic expertise, but also from the rich common-sense and semantics knowledge contained in large text corpora by virtue of predicting the true identity of the masked token xi ,10 as done in the standard BERT setup. We thus interpolate the KD loss and the original BERT masked LM objective: Phrase-structure Parsing - OOD. Still in the context of phrase-structure parsing, we evalu"
2020.tacl-1.50,N19-1112,0,0.367367,"tillation Pretraining for Bidirectional Encoders Adhiguna Kuncoro∗♠♦ Lingpeng Kong∗♠ Daniel Fried∗♣ Dani Yogatama♠ Laura Rimell♠ Chris Dyer♠ Phil Blunsom♠♦ ♠ DeepMind, London, UK Department of Computer Science, University of Oxford, UK ♣ Computer Science Division, University of California, Berkeley, CA, USA {akuncoro,lingpenk,dyogatama,laurarimell,cdyer,pblunsom}@google.com dfried@cs.berkeley.edu ♦ Abstract dels have also been shown to perform remarkably well at syntactic grammaticality judgment tasks (Goldberg, 2019), and encode substantial amounts of syntax in their learned representations (Liu et al., 2019a; Tenney et al., 2019a,b; Hewitt and Manning, 2019; Jawahar et al., 2019). Intriguingly, success on these syntactic tasks has been achieved by Transformer architectures (Vaswani et al., 2017) that lack explicit notions of hierarchical syntactic structures. Based on such evidence, it would be tempting to conclude that data scale alone is all we need to learn the syntax of natural language. Nevertheless, recent findings that systematically compare the syntactic competence of models trained at varying data scales suggest that model inductive biases are in fact more important than data scale for"
2020.tacl-1.50,E17-1117,1,0.89748,"Missing"
2020.tacl-1.50,P19-1334,0,0.0254666,"our UG-KD model outperforms the baseline on CoLA, but performs slightly worse on the other GLUE tasks in aggregate, leading to a slightly lower overall test set accuracy (80.0 for the UG-KD as opposed to 80.3 for the No-KD baseline). The improvement on the syntax-sensitive CoLA provides additional evidence—beyond the improvement on the syntactic tasks (Table 2)— that our approach indeed yields improved syntactic competence. We conjecture that these improvements do not transfer to the other GLUE tasks because they rely more on lexical and semantic properties, and less on syntactic competence (McCoy et al., 2019). We defer a more thorough investigation of how much syntactic competence is necessary for solving most of the GLUE tasks to future work, but make two remarks. First, the findings on GLUE are consistent with the hypothesis that our approach yields improved structural competence, albeit at the expense of a slightly less rich meaning representation, which we attribute to the smaller dataset used to train the RNNG teacher. Second, 785 human-level natural language understanding includes the ability to predict structured outputs, for example, to decipher ‘‘who did what to whom’’ (SRL). Succeeding i"
2020.tacl-1.50,W12-4501,0,0.0208281,"Missing"
2020.tacl-1.50,P16-1162,0,0.00712367,"composition function,1 which recursively combines smaller units into larger ones. RNNGs attempt to maximize the probability of correct action sequences relative to each gold tree.2 1 Not all syntactic LMs have hierarchical biases; Choe and Charniak (2016) modeled strings and phrase structures sequentially with LSTMs. This model can be understood as a special case of RNNGs without the composition function. 2 Unsupervised RNNGs (Kim et al., 2019) exist, although they perform worse on measures of syntactic competence. Extension to Subwords. Here we extend the RNNG to operate over subword units (Sennrich et al., 2016) to enable compatibility with the BERT student. As each word can be split into an arbitrary-length sequence of subwords, we preprocess the phrase-structure trees to include an additional nonterminal symbol that represents a word sequence, as illustrated by the example ‘‘(S (NP (WORD the) (WORD d ##og)) (VP (WORD ba ##rk ##s)))’’, where tokens prefixed by ‘‘##’’ are subword units.3 Figure 1: An example of the masked LM task, where [MASK] = chase, and window is an attractor (red). We suppress phrase-structure annotations and corruptions on the context tokens for clarity. 3 Approach We begin with"
2020.tacl-1.50,D18-1412,1,0.829908,"ammatical judgment; our work represents a step towards answering this question. Substantial progress has recently been made in improving the performance of BERT and other masked LMs (Lan et al., 2020; Liu et al., 2019b; Raffel et al., 2019; Sun et al., 2020, inter alia). Our structure distillation technique is orthogonal, and can be applied for these approaches. Lastly, our findings on the benefits of syntactic knowledge for structured prediction tasks that are not explicitly syntactic in nature, such as SRL and coreference resolution, are consistent with those of prior work (He et al., 2017; Swayamdipta et al., 2018; He et al., 2018; Strubell et al., 2018, inter alia). 6 Conclusion the BERT student is a bidirectional model that estimates the conditional probabilities of masked words in context, we propose to distill an efficient yet surprisingly effective approximation of the RNNG’s posterior estimate for generating each word conditional on its bidirectional context. Our findings suggest that syntactic inductive biases are beneficial for a diverse range of structured prediction tasks, including for tasks that are not explicitly syntactic in nature. In addition, these biases are particularly helpful for i"
2020.tacl-1.50,N19-1334,0,0.102096,"Missing"
2020.tacl-1.50,P95-1026,0,0.377119,"eparate discriminative parser, which has access to yet unseen words x>i . This non-incremental procedure is justified, however, because we aim to design the most informative teacher distributions for the nonincremental BERT student, which also has access to bidirectional context. i∈M (x) w ∈Σ i log pθ (˜ xi = w|c(x1 ), · · · , c(xk )) , 9 780 We use the same setup as Preliminary Experiments. where t˜φ,ω (w|x<i , x>i ) is our approximation of tφ (w|x<i , x>i ), as defined in Eqs. 5 and 6. teacher that we distill into BERT also uses phrasestructure trees, this setup is related to self-training (Yarowsky, 1995; Charniak, 1997; Zhou and Li, 2005; McClosky et al., 2006; Andor et al., 2016, inter alia). Interpolation. The RNNG teacher is an expert on syntax, although in practice it is only feasible to train it on a much smaller dataset. Hence, we not only want the BERT student to learn from the RNNG’s syntactic expertise, but also from the rich common-sense and semantics knowledge contained in large text corpora by virtue of predicting the true identity of the masked token xi ,10 as done in the standard BERT setup. We thus interpolate the KD loss and the original BERT masked LM objective: Phrase-struc"
2020.tacl-1.50,P19-1452,0,0.0849005,"ntation learners that work well at scale still benefit from explicit syntactic biases? And where exactly would such syntactic biases be helpful in different language understanding tasks? Here we work towards answering these questions by devising a new pretraining strategy that injects syntactic biases into a BERT (Devlin et al., 2019) learner that works well at scale. We hypothesize that this approach can improve the competence of BERT on various tasks, which provides evidence for the benefits of syntactic biases in large-scale models. Our approach is based on the prior work of Kuncoro et al. (2019), who devised an effective knowledge distillation (KD; Bucilˇa et al., 2006; Hinton et al., 2015) procedure for improving the syntactic competence of scalable LMs that lack explicit syntactic biases. More concretely, their KD procedure utilized the predictions of an explicitly hierarchical (albeit hard to scale) syntactic LM, recurrent neural network grammars Textual representation learners trained on large amounts of data have achieved notable success on downstream tasks; intriguingly, they have also performed well on challenging tests of syntactic competence. Hence, it remains an open questi"
2020.tacl-1.50,P19-1230,0,0.0386635,", and the English Web Treebank (Petrov and McDonald, 2012). Following Fried et al. (2019), we test the PTB-trained parser on the test splits11 of these OOD treebanks without any retraining, to simulate the case where no in-domain labeled data are available. We use the same codebase as above. 1 Xh θˆB-KD = arg min αℓKD (x; θ ) + (1 − α) |D | θ x∈D i X − log pθ (xi |c(x1 ), · · · , c(xk )) , Dependency Parsing - PTB. Our third task is PTB dependency parsing with Stanford Dependencies (De Marneffe and Manning, 2008) v3.3.0. We use the BERT-augmented joint phrasestructure and dependency parser of Zhou and Zhao (2019), which is inspired by head-driven phrase-structure grammar (HPSG; Pollard and Sag, 1994). i∈M (x) (7) omitting the next-sentence prediction for brevity. We henceforth set α = 0.5 unless stated otherwise. 4 Experiments Semantic Role Labeling. Our fourth evaluation task is span-based (SRL) on the English CoNLL 2012 (OntoNotes) dataset (Pradhan et al., 2013). We apply our approach on top of the BERTaugmented model of Shi and Lin (2019), as implemented on AllenNLP (Gardner et al., 2018). Here we outline the evaluation setup, present our results, and discuss the implications of our findings. 4.1 E"
2020.wmt-1.36,P19-1425,0,0.021074,"ranslation quality as shown by existing work (Sun et al., 2019; Ng et al., 2019). After training the proposal models with the mix of real and synthetic parallel data, we fine-tuned the models with CWMT and a subset of newstest2017 and newstest2018 which were not used for validation. 4.4 Improving Uncertainty Estimation To improve the robustness of noisy channel reranking, we explore two approaches for improving uncertainty estimation of the seq2seq scoring models. 4.4.1 Adversarially Trained Proposal Models To simulate different wordings and noises in source and candidate sentences, we follow Cheng et al. (2019) to train the models on noisy adversarial inputs and targets. We use bidirectional languagemodels to provide the noisy candidates and select the candidates with highest loss (i.e., adversarial source-target inputs). During the training, we optimize the original loss with clean source-target pairs, the language model losses for source and target sides, and the adversarial loss using adversarial source-target inputs. In the final scoring, we use an ensemble of eight adversarially trained models with few differences from Cheng et al. (2019): (a) We explore training with and without the language m"
2020.wmt-1.36,P19-1285,0,0.0283336,"dunov et al., 2018), distillation (Kim and Rush, 2016; Liu et al., 2016), and forward-translated parallel documents. We further improve these sequence-to-sequence (seq2seq) models by fine-tuning them with in-domain data (§4.3). To improve the robustness of the reranker we apply adversarial training and contrastive learning methods for uncertainty estimation (§4.4). Finally, we include candidate translations generated by Monte-Carlo Tree Search (MCTS) (§B) in order to improve the diversity of the candidate pool for the reranker. Our language models are based on the Transformer-XL architecture (Dai et al., 2019) and optimized with distillation and fine-tuning with in-domain data (§5). During development, we observed weaknesses in our system’s translations for long sentences, largely due to premature truncations. We developed several techniques to mitigate this issue such as sentence segmentation (breaking sentences into logical complete segments) and training specialized models with synthetically constructed long sequences to generate additional proposals for our reranker (§A). Experiments show that the aforementioned techniques are very effective: our system outperforms the Transformer baseline by 9"
2020.wmt-1.36,D18-1045,0,0.17332,"and optimized independently while at inference time, when we reason over the posterior distribution of translations given the source document, conditional dependencies between translations are induced by the language model prior. The core of our document-level translation architecture is the noisy channel reranker. It requires proposal, channel, and language models, each of which is optimized separately using different techniques and approaches. For the proposal and channel models we use Transformer models (Vaswani et al., 2017) (§4.1) with data augmentation (§4.2), such as back translation (Edunov et al., 2018), distillation (Kim and Rush, 2016; Liu et al., 2016), and forward-translated parallel documents. We further improve these sequence-to-sequence (seq2seq) models by fine-tuning them with in-domain data (§4.3). To improve the robustness of the reranker we apply adversarial training and contrastive learning methods for uncertainty estimation (§4.4). Finally, we include candidate translations generated by Monte-Carlo Tree Search (MCTS) (§B) in order to improve the diversity of the candidate pool for the reranker. Our language models are based on the Transformer-XL architecture (Dai et al., 2019) a"
2020.wmt-1.36,D11-1125,0,0.118354,"Missing"
2020.wmt-1.36,D16-1139,0,0.184286,"t inference time, when we reason over the posterior distribution of translations given the source document, conditional dependencies between translations are induced by the language model prior. The core of our document-level translation architecture is the noisy channel reranker. It requires proposal, channel, and language models, each of which is optimized separately using different techniques and approaches. For the proposal and channel models we use Transformer models (Vaswani et al., 2017) (§4.1) with data augmentation (§4.2), such as back translation (Edunov et al., 2018), distillation (Kim and Rush, 2016; Liu et al., 2016), and forward-translated parallel documents. We further improve these sequence-to-sequence (seq2seq) models by fine-tuning them with in-domain data (§4.3). To improve the robustness of the reranker we apply adversarial training and contrastive learning methods for uncertainty estimation (§4.4). Finally, we include candidate translations generated by Monte-Carlo Tree Search (MCTS) (§B) in order to improve the diversity of the candidate pool for the reranker. Our language models are based on the Transformer-XL architecture (Dai et al., 2019) and optimized with distillation and"
2020.wmt-1.36,P18-1007,0,0.0261476,"BLEU, such that: a sentence were replaced with the capitalized variant that occurred most frequently in other positions of the English monolingual training data. Thus, in the previous sentence the initial token would have been words rather than Words. (1 − META)4 = TER× (1 − BLEU)× Subword units To encode text into sub-word units, we used the sentencepiece tool (Kudo and Richardson, 2018). For seq2seq models (i.e., the channel model and proposal models), we trained the segmentation model on the first 10 million sentences of the parallel training corpus,3 using joint source and target unigram (Kudo, 2018) subword segmentation algorithm with a target vocabulary of 32K tokens and minimum character coverage of 0.9995, which resulted in 32,768 word pieces.4 For the language model, we used the English side alone with the same vocabulary size and a character coverage of 1.0. (1 − METEOR)× (1 − q0.1 (BLEU)). (4) When several configurations of hyperparameters achieve values of META very close to the maximum (within 0.02), we pick the one maximizing BLEU and/or minimizing the L2 norm of the λs, considered as a vector. This corresponds to an intuitive prior towards giving more weight to the language mod"
2021.emnlp-main.536,P19-1126,0,0.0384961,"Missing"
2021.emnlp-main.536,2021.eacl-main.233,0,0.127872,"cy. It is non-trivial to find an optimal translation strategy, as there is generally a rivalry between the two objectives, i.e. reading more source words before translating leads to better translation quality, but it in turn results in higher latency due to the longer time for reading. Conventional Wait-k policies (Ma et al., 2019) put a hard limitation over the buffer size k 1 , which guarantees low latency but weakens flexibility and scalability when handling long and complicated language pairs. Alternatively, reinforcement learning (RL) approaches (Gu et al., 2017; Satija and Pineau, 2016; Arthur et al., 2021) learn a dynamic policy using a combined reward of a quality metric like the BLEU score and AL (average lagging)2 . However, the poor sample efficiency make it very difficult to learn a robust SiMT model with RL. In this paper we propose a generative framework with a latent variable that dynamically decides between the actions of read or translate at every time step, enabling the formulation of SiMT as a structural sequence-to-sequence learning task. Figure 1 depicts the examples of possible translation paths of different models. Wait-k only explores one hypothesis, while adaptive wait-k ensem"
2021.emnlp-main.536,2020.emnlp-main.184,1,0.924428,"asets are chosen for testing BLEU (Papineni et al., 2002) and AL (average lagging) (Ma et al., 2019). For GSiMT models, we empirically fix λ = 3 for all the experiments, and use ζ as the free parameter to achieve different AL. For Multi30K (Elliott et al., 2016), we use all three language pairs EN→FR, EN→DE and EN→CZ with the image data from Flickr30k as extra modality and flickr2016 as test dataset. We build multimodal models with the goal of testing the generalisation ability of the generative models with extra modalities. To that end, we concatenate the object detection features applied in Caglayan et al. (2020) into the state representation Si,j and maintain the rest of the neural network the same as the unimodal SiMT. The other models (RL, Wait-k and Adpative Wait-k) incorporate the same features as well. Here, as the size of data is small, we apply a smaller Transformers with 4 layers, 4 heads, 512 model dimension and 1024 for linear connection. 4.2 Translation Quality & Latency Table 1 shows the SiMT performance for the benchmark models and our proposed generative models on the WMT15 DE→EN dataset. RL is our implementation of Gu et al. (2017) with policy gradient method. All the numbers for Wait-"
2021.emnlp-main.536,N13-1073,0,0.238471,"nsducer is not designed for SiMT. Because it is optimised by the crossentropy of target words, it naturally prefers read actions over translate actions in order to see more contexts before translation, which intuitively can result in better translation quality but high latency. Here, we propose to extend the neural transducer framework to modern Transformer-based translation models (Vaswani et al., 2017), and introduce a re-parameterised Poisson distribution to regularise the latency (i.e. how many source words are read before translating a target word). Inspired by the fast-alignment work by Dyer et al. (2013), the translation model generally favors word alignments distributed close to the diagonal. We hypothesise that the optimal sequence of translate actions in SiMT is also located close to the diagonal. Thus the Poisson prior acts as context-independent regularisation on the buffer size proportional to the distance between the current position and the diagonal. This ensures that the number of read source words will not grow indefinitely without translating any target words, while the soft boundary, due to the regularisation, still allows the model to consider complicated/long simultaneous transl"
2021.emnlp-main.536,W16-3210,1,0.887869,"Missing"
2021.emnlp-main.536,E17-1099,0,0.349888,"ween the translation quality and the latency. It is non-trivial to find an optimal translation strategy, as there is generally a rivalry between the two objectives, i.e. reading more source words before translating leads to better translation quality, but it in turn results in higher latency due to the longer time for reading. Conventional Wait-k policies (Ma et al., 2019) put a hard limitation over the buffer size k 1 , which guarantees low latency but weakens flexibility and scalability when handling long and complicated language pairs. Alternatively, reinforcement learning (RL) approaches (Gu et al., 2017; Satija and Pineau, 2016; Arthur et al., 2021) learn a dynamic policy using a combined reward of a quality metric like the BLEU score and AL (average lagging)2 . However, the poor sample efficiency make it very difficult to learn a robust SiMT model with RL. In this paper we propose a generative framework with a latent variable that dynamically decides between the actions of read or translate at every time step, enabling the formulation of SiMT as a structural sequence-to-sequence learning task. Figure 1 depicts the examples of possible translation paths of different models. Wait-k only explo"
2021.emnlp-main.536,D16-1073,0,0.0131231,"ck that hinders the applicability of RL in structural sequence-to-sequence learning. The proposed GSiMT model combines the merits of both the Wait-k policies and RL. Deep learning with structures has been explored in many NLP tasks, especially for sequence-tosequence learning. Kim et al. (2017) implements structural dependencies on attention networks, which gives the ability to attend to partial segmentations or subtrees without changing the sequence-to-sequence structure. Tran et al. (2016) parameterises the transition and emission probabilities of an HMM with explicit neural components, and Jiang et al. (2016) applies deep structural latent variables to implement the dependency model with valence (Klein and Manning, 2004) and integrates out all the structures in end-to-end learning. Our • A Transformer-based neural transducer model GSiMT model is based on neural transducer model. for simultaneous machine translation. Previously, Graves (2012) presents an RNN-based neural transducer for phoneme recognition, and Yu • Poisson prior for effectively balancing the et al. (2016) explores an LSTM-based neural transtranslation quality and latency. ducer for MT. The uni-directional variant model of • State-o"
2021.emnlp-main.536,P04-1061,0,0.0606634,"combines the merits of both the Wait-k policies and RL. Deep learning with structures has been explored in many NLP tasks, especially for sequence-tosequence learning. Kim et al. (2017) implements structural dependencies on attention networks, which gives the ability to attend to partial segmentations or subtrees without changing the sequence-to-sequence structure. Tran et al. (2016) parameterises the transition and emission probabilities of an HMM with explicit neural components, and Jiang et al. (2016) applies deep structural latent variables to implement the dependency model with valence (Klein and Manning, 2004) and integrates out all the structures in end-to-end learning. Our • A Transformer-based neural transducer model GSiMT model is based on neural transducer model. for simultaneous machine translation. Previously, Graves (2012) presents an RNN-based neural transducer for phoneme recognition, and Yu • Poisson prior for effectively balancing the et al. (2016) explores an LSTM-based neural transtranslation quality and latency. ducer for MT. The uni-directional variant model of • State-of-the-art SiMT results (BLEU & AL) Yu et al. (2016) is similar to our proposed GSiMT on benchmark datasets, and th"
2021.emnlp-main.536,P02-1040,0,0.109422,"rs with the same parameters in Vaswani et al. (2017) as the backbone. Instead of updating all the parameters from scratch, we pretrain the encoder and decoder (both are uni-directional Transformers) as consecutive NMT model for 10 epochs. Then we freeze the Transformers parameters, and apply 256 batch size and 1e-4 learning rate for training the generative models. On PyTorch (Paszke et al., 2019) platform, each epoch takes around 40 minutes with Adam (Kingma and Ba, 2014) on single V100 GPU4 . The checkpoints with best performance in 5 runs on development datasets are chosen for testing BLEU (Papineni et al., 2002) and AL (average lagging) (Ma et al., 2019). For GSiMT models, we empirically fix λ = 3 for all the experiments, and use ζ as the free parameter to achieve different AL. For Multi30K (Elliott et al., 2016), we use all three language pairs EN→FR, EN→DE and EN→CZ with the image data from Flickr30k as extra modality and flickr2016 as test dataset. We build multimodal models with the goal of testing the generalisation ability of the generative models with extra modalities. To that end, we concatenate the object detection features applied in Caglayan et al. (2020) into the state representation Si,j"
2021.emnlp-main.536,P16-1162,0,0.0185986,". It is worth mentioning that the Poisson prior distribution is only employed for regularising the training, but it is not required at testing time, as the translate action distributions have implicitly learned to translate the target words with low latency. Hence, the lengths of the sequences m and n are known during training, but they are not used at test time. During test, we simply use the average length ratio of the whole dataset. 4 Experiments 4.1 Datasets & Settings For WMT15 DE→EN, we follow the exactly the same preprocessing procedure as in (Ma et al., 2019; Zheng et al., 2020). BPE (Sennrich et al., 2016) is applied to achieve 35K vocabulary and we process 4.5M parallel corpus for training, 3K sentences of newstest-2013 for validation and 2,169 sentences of newstest-2015 for testing. Following (Ma et al., 2019; Zheng et al., 2020), we apply the base version of Transformers with the same parameters in Vaswani et al. (2017) as the backbone. Instead of updating all the parameters from scratch, we pretrain the encoder and decoder (both are uni-directional Transformers) as consecutive NMT model for 10 epochs. Then we freeze the Transformers parameters, and apply 256 batch size and 1e-4 learning rat"
2021.emnlp-main.536,W16-5907,0,0.0284719,"on word alignments as the oracle to improve the learning. However, the high variance of the estimator is still a bottleneck that hinders the applicability of RL in structural sequence-to-sequence learning. The proposed GSiMT model combines the merits of both the Wait-k policies and RL. Deep learning with structures has been explored in many NLP tasks, especially for sequence-tosequence learning. Kim et al. (2017) implements structural dependencies on attention networks, which gives the ability to attend to partial segmentations or subtrees without changing the sequence-to-sequence structure. Tran et al. (2016) parameterises the transition and emission probabilities of an HMM with explicit neural components, and Jiang et al. (2016) applies deep structural latent variables to implement the dependency model with valence (Klein and Manning, 2004) and integrates out all the structures in end-to-end learning. Our • A Transformer-based neural transducer model GSiMT model is based on neural transducer model. for simultaneous machine translation. Previously, Graves (2012) presents an RNN-based neural transducer for phoneme recognition, and Yu • Poisson prior for effectively balancing the et al. (2016) explo"
2021.emnlp-main.536,2020.acl-main.254,0,0.274926,"erage lagging (AL). Our contributions can be summarised: 2 Related Work Conventional SiMT methods are based on heuristic waiting criteria (Cho and Esipova, 2016) or fixed buffering strategy (Ma et al., 2019) to trade off the translation quality for lower latency. Although the heuristic approaches are simple and straightforward, they lack of scalability and cannot generalise well on longer sequences. There is also a bulk of work attempting to improve the attention mechanism (Arivazhagan et al., 2019) and re-translation strategies (Niehues et al., 2018) for better translation quality. Recently, Zheng et al. (2020) extends the fixed Wait-k policies into adaptive version and ensembles multiple models with lower latency to improve the performance, but one still needs to choose a hard boundary on the maximum value of k. By contrast, our GSiMT model considers all the possible paths with a soft boundary modelled by Poisson distribution, which leads to a more flexible balance between quality and latency. RL has been explored (Gu et al., 2017) to learn an agent that dynamically decides to read or translate conditioned on different translation contexts. Arthur et al. (2021) further applies extra knowledge on wo"
2021.naacl-main.18,N19-1388,0,0.0419831,"Missing"
2021.naacl-main.18,W11-2138,0,0.0341717,"ces are sampled from an adversarial distribution for augmentation. SCDA and AdvAug ignore the alignment information, thereby breaking the symmetry of source and target sequences. While DADA takes both context and alignment into account, it replaces multiple words in source and target sequences simultaneously, which risks generating unnatural sequences. In this paper, we utilize both alignment and contextual information to sequentially replace aligned phrases for better performance. Backtranslation. The idea of backtranslation dates back to statistical machine translation (Goutte et al., 2009; Bojar and Tamchyna, 2011). Senrich et al. (2016) use backtranslation, where monolingual sequences in the target language are translated into the source language, and obtain substantial improvements on the WMT and IWSLT tasks. Currey et al. (2017) apply backtranslation to low-resource languages, finding that even low-quality translations due to limited parallel corpora are beneficial. He et al. (2016) propose a dual learning framework, where the primal task (source-to-target translation) and the dual task (target-to-source translation) teach each other through a reinforcement learning process until convergence. Edunov"
2021.naacl-main.18,J93-2003,0,0.117178,"ce, and computing the counterfactual effect of doing so in the target sequence. We argue that for any such augmentation method for NMT, it is crucial to leverage both contextual and alignment information, for the following reasons. (1) Context: As contextual information is widely used to disambiguate words (Peters et al., 2018) and generate realistic-looking sequences (Zellers et al., 2019), it is critical to utilize contextual information to obtain grammatically-correct and semanticallysound sequences. (2) Alignment: Phrasal alignment plays a critical role in statistical machine translation (Brown et al., 1993; Vogel et al., 1996). As phrasal alignment provides information about which phrase in the source sequence produces a phrase in the target sequence, a data augmentation algorithm which disregards alignment risks breaking the symmetry between source and target sequences. To this end, in Section 3.1, we introduce a technique called Translation-Counterfactual Word Replacement (TCWR) for leveraging both context and alignment to replace phrases in source and target sequences. In Section 3.2, we propose a new data augmentation algorithm based on this replacement technique. In Section 3.3, we describ"
2021.naacl-main.18,P19-1425,0,0.0370589,"Missing"
2021.naacl-main.18,2020.acl-main.529,0,0.0327316,"oft augmentation approach, where 191 Dataset News Commentary News Crawl 2010 # Sequences # Words # Chars Dataset # Train # Dev # Test 0.46M 6.8M 10.05M 0.14B 63.96M 0.83B WMT’18 En-Tr WMT’17 En-De IWSLT’15 En-Vi WMT’19 Robust En-Fr Europarl-v7 En-Fr 206K 5.85M 133K 36,058 2M 3007 2,999 1,553 852 - 3000 3,004 1,268 1,401 - Table 1: The statistics of the monolingual datasets. the one-hot representation of a word is replaced by a soft distribution of words given by a language model. DADA (Cheng et al., 2019) uses gradient information to generate adversarial sequences for more robust NMT. AdvAug (Cheng et al., 2020) extends DADA, where embeddings of virtual sequences are sampled from an adversarial distribution for augmentation. SCDA and AdvAug ignore the alignment information, thereby breaking the symmetry of source and target sequences. While DADA takes both context and alignment into account, it replaces multiple words in source and target sequences simultaneously, which risks generating unnatural sequences. In this paper, we utilize both alignment and contextual information to sequentially replace aligned phrases for better performance. Backtranslation. The idea of backtranslation dates back to stati"
2021.naacl-main.18,W17-4715,0,0.0182129,"o account, it replaces multiple words in source and target sequences simultaneously, which risks generating unnatural sequences. In this paper, we utilize both alignment and contextual information to sequentially replace aligned phrases for better performance. Backtranslation. The idea of backtranslation dates back to statistical machine translation (Goutte et al., 2009; Bojar and Tamchyna, 2011). Senrich et al. (2016) use backtranslation, where monolingual sequences in the target language are translated into the source language, and obtain substantial improvements on the WMT and IWSLT tasks. Currey et al. (2017) apply backtranslation to low-resource languages, finding that even low-quality translations due to limited parallel corpora are beneficial. He et al. (2016) propose a dual learning framework, where the primal task (source-to-target translation) and the dual task (target-to-source translation) teach each other through a reinforcement learning process until convergence. Edunov et al. (2018) scale backtranslation to millions of monolingual data and obtain state-of-the-art performance on WMT’14 English→German. Xia et al. (2019) use a two-step pivoting method for improving backtranslation on low-r"
2021.naacl-main.18,N13-1073,0,0.0414005,"es, pages 187–197 June 6–11, 2021. ©2021 Association for Computational Linguistics model. We then consider the (path-specific) counterfactual target phrase that is aligned to that source phrase (given by an unsupervised phrasal alignment method). The idea is that this augmentation procedure exposes inductive biases in existing language models that enables new translation models to learn more efficiently and exhibit more robust generalisation. Specifically, our augmentation procedure performs the following three steps: 1. We utilize unsupervised phrasal alignment (e.g. Neubig et al. (2011) and Dyer et al. (2013)) to obtain correspondences between source and target phrases. 2. A source phrase is removed and then resampled according to a trained masked language model (Devlin et al., 2018; Raffel et al., 2019). 3. We perform (path-specific) counterfactual inference on the causal model given by a trained translation language model (Lample and Conneau, 2019) to resample only the aligned target phrase, given the changed source phrase. Different from prior work, our approach takes advantage of both source/target context and alignment for data augmentation. Experiments on IWSLT’15 English → Vietnamese, WMT’1"
2021.naacl-main.18,D18-1045,0,0.0195749,", 2011). Senrich et al. (2016) use backtranslation, where monolingual sequences in the target language are translated into the source language, and obtain substantial improvements on the WMT and IWSLT tasks. Currey et al. (2017) apply backtranslation to low-resource languages, finding that even low-quality translations due to limited parallel corpora are beneficial. He et al. (2016) propose a dual learning framework, where the primal task (source-to-target translation) and the dual task (target-to-source translation) teach each other through a reinforcement learning process until convergence. Edunov et al. (2018) scale backtranslation to millions of monolingual data and obtain state-of-the-art performance on WMT’14 English→German. Xia et al. (2019) use a two-step pivoting method for improving backtranslation on low-resource languages. We show that TCWR can be used together with backtranslation and obtain further improvements. 5 Experiments Table 2: The statistics of the parallel corpora. 5.1 Language Model Details We use the monolingual training data, including News Commentary and News Crawl 2010, provided by WMT’18, for Eq. 2, while the training set of each language pair is used for Eq. 3. The statis"
2021.naacl-main.18,P17-2090,0,0.0346428,"Missing"
2021.naacl-main.18,P19-1555,0,0.014732,"supervised alignment model (i.e., the black arrows from X to Y above). become a standard technique in computer vision (Krizhevsky et al., 2012; Huang et al., 2017; Chen et al., 2020), it is non-trivial to apply in machine translation since even a slight modification to a sequence can result in drastic changes in its syntax 1 Introduction and semantics. Indeed there is relatively little work Neural machine translation (NMT) models (Kalch- in this direction due to these difficulties (Sennrich brenner and Blunsom, 2013; Bahdanau et al., 2014; et al., 2016; Fadaee et al., 2017; Wang et al., 2018; Gao et al., 2019; Xia et al., 2019; Kobayashi, 2018). Vaswani et al., 2017) have reached state-of-the-art performance on various benchmarks. However, Further, work based on word replacement either these models frequently rely on large-scale paral- ignores the contexts of replaced words or breaks lel corpora for training, exhibiting degraded per- the alignment between source and target sequences, formance on low-resource languages (Zoph et al., both detrimental for generating high-quality data. 2016). Further, modern NMT systems are often In this paper we observe that a translation lanbrittle, as noises (e.g."
2021.naacl-main.18,D13-1176,1,0.777471,"Missing"
2021.naacl-main.18,N18-2072,0,0.0191675,"e black arrows from X to Y above). become a standard technique in computer vision (Krizhevsky et al., 2012; Huang et al., 2017; Chen et al., 2020), it is non-trivial to apply in machine translation since even a slight modification to a sequence can result in drastic changes in its syntax 1 Introduction and semantics. Indeed there is relatively little work Neural machine translation (NMT) models (Kalch- in this direction due to these difficulties (Sennrich brenner and Blunsom, 2013; Bahdanau et al., 2014; et al., 2016; Fadaee et al., 2017; Wang et al., 2018; Gao et al., 2019; Xia et al., 2019; Kobayashi, 2018). Vaswani et al., 2017) have reached state-of-the-art performance on various benchmarks. However, Further, work based on word replacement either these models frequently rely on large-scale paral- ignores the contexts of replaced words or breaks lel corpora for training, exhibiting degraded per- the alignment between source and target sequences, formance on low-resource languages (Zoph et al., both detrimental for generating high-quality data. 2016). Further, modern NMT systems are often In this paper we observe that a translation lanbrittle, as noises (e.g. grammatical errors) can cause guage"
2021.naacl-main.18,D18-1050,0,0.0196491,"e models frequently rely on large-scale paral- ignores the contexts of replaced words or breaks lel corpora for training, exhibiting degraded per- the alignment between source and target sequences, formance on low-resource languages (Zoph et al., both detrimental for generating high-quality data. 2016). Further, modern NMT systems are often In this paper we observe that a translation lanbrittle, as noises (e.g. grammatical errors) can cause guage model can be interpreted as a causal model, significant mistranslations (Sakaguchi et al., 2017; as described in Figure 1. Doing so allows us to ask Michel and Neubig, 2018). counterfactual questions of the form: Given source Data augmentation is a promising direction to and target sequences, if a phrase in the source seovercome these issues. It works by enlarging the quence is changed, how would the target sequence number of data points for training without manually change? We propose a data augmentation method collecting new data. It is widely used to improve for machine translation that generates counterfacdiversity and robustness and to avoid overfitting tual parallel translation data. To ensure these counon small datasets. Even though data augmentation terfa"
2021.naacl-main.18,P11-1064,0,0.23398,"Human Language Technologies, pages 187–197 June 6–11, 2021. ©2021 Association for Computational Linguistics model. We then consider the (path-specific) counterfactual target phrase that is aligned to that source phrase (given by an unsupervised phrasal alignment method). The idea is that this augmentation procedure exposes inductive biases in existing language models that enables new translation models to learn more efficiently and exhibit more robust generalisation. Specifically, our augmentation procedure performs the following three steps: 1. We utilize unsupervised phrasal alignment (e.g. Neubig et al. (2011) and Dyer et al. (2013)) to obtain correspondences between source and target phrases. 2. A source phrase is removed and then resampled according to a trained masked language model (Devlin et al., 2018; Raffel et al., 2019). 3. We perform (path-specific) counterfactual inference on the causal model given by a trained translation language model (Lample and Conneau, 2019) to resample only the aligned target phrase, given the changed source phrase. Different from prior work, our approach takes advantage of both source/target context and alignment for data augmentation. Experiments on IWSLT’15 Engl"
2021.naacl-main.18,N18-1202,0,0.00823986,"Zh/NQPv6BYUFoVY=</latexit> Figure 2: The three steps of Translation-Counterfactual Word Replacement. See text for details. 3 Method Our goal is to take an input sequence pair (X , Y) and create augmented data from it. We aim to do so by removing phrases, resampling them in the source sequence, and computing the counterfactual effect of doing so in the target sequence. We argue that for any such augmentation method for NMT, it is crucial to leverage both contextual and alignment information, for the following reasons. (1) Context: As contextual information is widely used to disambiguate words (Peters et al., 2018) and generate realistic-looking sequences (Zellers et al., 2019), it is critical to utilize contextual information to obtain grammatically-correct and semanticallysound sequences. (2) Alignment: Phrasal alignment plays a critical role in statistical machine translation (Brown et al., 1993; Vogel et al., 1996). As phrasal alignment provides information about which phrase in the source sequence produces a phrase in the target sequence, a data augmentation algorithm which disregards alignment risks breaking the symmetry between source and target sequences. To this end, in Section 3.1, we introduc"
2021.naacl-main.18,W18-6319,0,0.0178483,"ish task with either the random initialization or the XLM initialization. 5.2 NMT Model Details We use fairseq2 to implement the NMT models. The vocabularty size is 37K. Six encoder and decoder layers are applied. The hidden size is set to 1024. 16 self-attention heads are employed. We use Adam as the optimizer. The learning rate is initially set to 1e-7 and is gradually increased to 5e-4 with 4K warm-up steps, before applying linear decay. Dropout is set to 0.3. Label smoothing with the smoothing factor 0.1 is used. For decoding, we use beam search, and the beam size is set to 12. SacreBLEU (Post, 2018) is used as the metric. 5.3 Sensitivity Study 5.3.1 0.4 0.6 Sampling probability 0.8 1.0 Effect of the XLM Initialization We plot the learning curves of the language model for En-Tr with/without XLM initialization. As shown in Figure 4, the model with the XLM initialization converges faster and better compared to the model with the random initialization. As XLM is trained using the masked language model objective on large-scale monolingual data, we draw the conclusion that large-scale pre-training can improve downstream language model pre-training tasks. We further evaluate the models on the d"
2021.naacl-main.18,2020.acl-main.170,0,0.0154188,"Here D is a monolingual dataset and S is a parallel corpus. 4 pθ1 (Xi |X1 , ..., Xi = [MASK], ..., X|X |) (2) and pθ2 (Yj |X , Y1 , ..., Yj = [MASK], ..., Y|Y |). (3) 4.1 Related Work Data Augmentation for NMT We categorize previous work on data augmentation for NMT into two classes, word replacement and backtranslation. Word replacement. WordDropout (Sennrich Eq. 2 only requires monolingual datasets, which et al., 2016) randomly zeros out word embedare abundant. On the other hand, Eq. 3 requires dings in order to introduce noises. BPEDropout parallel corpora to train. We parameterize Eq. 3 (Provilkov et al., 2020) stochastically corrupts the using a variant of the translation language model segmentation procedure of BPE, leading to differ(Lample and Conneau, 2019). The main difference ent subword segmentations with the same BPE vois that only phrases in target sequences are masked, cabulary. RAML (Norouzi et al., 2016) applies whereas Lample and Conneau (2019) mask both a reward-augmented maximum likelihood objecsource and target tokens, with the goal of learning tive, which essentially augments target sequences bilingual relations. Another difference is that a with sequences sampled based on metrics,"
2021.naacl-main.18,P16-1009,0,0.0991914,"Missing"
2021.naacl-main.18,W16-2323,0,0.049653,"Missing"
2021.naacl-main.18,C96-2141,0,0.848836,"e counterfactual effect of doing so in the target sequence. We argue that for any such augmentation method for NMT, it is crucial to leverage both contextual and alignment information, for the following reasons. (1) Context: As contextual information is widely used to disambiguate words (Peters et al., 2018) and generate realistic-looking sequences (Zellers et al., 2019), it is critical to utilize contextual information to obtain grammatically-correct and semanticallysound sequences. (2) Alignment: Phrasal alignment plays a critical role in statistical machine translation (Brown et al., 1993; Vogel et al., 1996). As phrasal alignment provides information about which phrase in the source sequence produces a phrase in the target sequence, a data augmentation algorithm which disregards alignment risks breaking the symmetry between source and target sequences. To this end, in Section 3.1, we introduce a technique called Translation-Counterfactual Word Replacement (TCWR) for leveraging both context and alignment to replace phrases in source and target sequences. In Section 3.2, we propose a new data augmentation algorithm based on this replacement technique. In Section 3.3, we describe the architectures u"
2021.naacl-main.18,D18-1100,0,0.0829281,"ingled-out by an unsupervised alignment model (i.e., the black arrows from X to Y above). become a standard technique in computer vision (Krizhevsky et al., 2012; Huang et al., 2017; Chen et al., 2020), it is non-trivial to apply in machine translation since even a slight modification to a sequence can result in drastic changes in its syntax 1 Introduction and semantics. Indeed there is relatively little work Neural machine translation (NMT) models (Kalch- in this direction due to these difficulties (Sennrich brenner and Blunsom, 2013; Bahdanau et al., 2014; et al., 2016; Fadaee et al., 2017; Wang et al., 2018; Gao et al., 2019; Xia et al., 2019; Kobayashi, 2018). Vaswani et al., 2017) have reached state-of-the-art performance on various benchmarks. However, Further, work based on word replacement either these models frequently rely on large-scale paral- ignores the contexts of replaced words or breaks lel corpora for training, exhibiting degraded per- the alignment between source and target sequences, formance on low-resource languages (Zoph et al., both detrimental for generating high-quality data. 2016). Further, modern NMT systems are often In this paper we observe that a translation lanbrittle"
2021.naacl-main.18,D16-1163,0,0.0645387,"Missing"
C12-1022,N03-2002,0,0.0898836,"Missing"
C12-1022,P11-1087,1,0.836269,"position step amounts to interpolated back-off. Baroni and Matiasek (2002) proposed basic models of German compounds for use in predictive text input, exploiting the same link between right-headedness and context as we have, although their focus was restricted to compounds with two components. In terms of Bayesian modelling, the PYP has been found to be very useful in a variety of tasks, including word segmentation, speech recognition, domain adaption and unsupervised PoS tagging (Goldwater et al., 2006; Mochihashi et al., 2009; Huang and Renals, 2007; Neubig et al., 2010; Wood and Teh, 2009; Blunsom and Cohn, 2011). In all cases its power-law scaling and ease of extensibility via the base distribution allowed the formulation of interesting models that achieved competitive results. 7 Conclusion We have demonstrated how an existing hierarchical Bayesian model can be used to build an n-gram language model that is informed by intuitions about the specific linguistic phenomenon of closed-form compounds. While our focus was on compounds, we argue that this approach can be useful for other phenomena, such as rich morphology more generally, where data sparsity creates smoothing problems for n-gram language mode"
C12-1022,E12-3008,1,0.185309,"utch and Afrikaans), compounds are written as single orthographic units. NLP systems that rely on whitespace to demarcate their elementary modelling units, e.g. the “grams” in n-gram models, are thus prone to suffer from sparse data effects that can be attributed to compounds specifically. An account of compounds in terms of their components therefore holds the potential of improving the performance of such systems. Examples of compounds • A basic noun-noun compound: Auto + Unfall = Autounfall (car crash) 1 Preliminary work on the approach we follow in this paper was previously reported on by Botha (2012). Here, we expand on the scale and depth of the empirical evaluation and investigate an additional inference technique. 342 • Linking elements can appear between components Küche + Tisch = Küchentisch (kitchen table) • Components can undergo stemming Schule + Hof = Schulhof (schoolyard) • Compounding is recursive (Geburt + Tag) + Kind = Geburtstag + Kind = Geburtstagskind (birthday boy/girl) • Compounding extends beyond noun components Zwei-Euro-Münze (two Euro coin) Fahrzeug (vehicle) A compound is said to consist of a head component and one or more modifier components, with optional linking"
C12-1022,N09-1046,1,0.934021,"Missing"
C12-1022,P10-4002,1,0.797671,"n constrained to this vocabulary. Our test corpus for the monolingual task is the union of all the WMT11 development data for German (news-test2008,9,10, 7065 sentences). For translation experiments, the preprocessed English-German bitext was filtered to exclude sentences longer than 50 tokens, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the Berkeley Aligner (Liang et al., 2006) and used as a basis from which to extract a Hiero-style synchronous CFG (Chiang, 2007). The weights of the linear translation models were tuned towards BLEU using cdec’s (Dyer et al., 2010) implementation of MERT (Och, 2003). For this, the development set news-test2008 (2051 sentences) was used, while final BLEU scores are measured on the official test set newstest2011 (3003 sentences, 171460 tokens), without detokenising or recasing hypotheses. Compound segmentation For this evaluation, we used an a priori segmentation of compounds into parts to build our models. This means we assume a single, fixed analysis of a compound regardless of the context it occurs in, which is necessitated by the fact that our probabilistic model does not specify a step for choosing an analysis. To co"
C12-1022,N06-2013,0,0.0197616,"to construct n-gram models with dependencies among sequences of PoS tags or semantic classes in addition to standard word-based dependencies. It should be possible to encode a model with structure comparable to ours in the FLM framework, but it does not lend itself naturally to 353 having a variable number of features depending on the predicted token in the way our model allows a variable number of parts in a compound. Another common approach for addressing the sparsity effects of compounding (Koehn and Knight, 2003; Koehn et al., 2008; Stymne, 2009; Berton et al., 1996), and rich morphology (Habash and Sadat, 2006; Geutner, 1995), has been to use pre/post-processing with an otherwise unmodified translation system or speech recognition system. This approach views the existing machinery as adequate and shifts the focus to finding a more appropriate segmentation of words into tokens, i.e. compounds into parts or words into morphemes, thus achieving a vocabulary reduction. The downside of such a method is that training a standard n-gram language model on pre-segmented data introduces unwanted effects: in the case of German compounds, the split-off modifiers would take precedence in a split-off head’s n-gra"
C12-1022,W08-0318,0,0.358285,"ted conditioned on some history of preceding feature values. This allows one to construct n-gram models with dependencies among sequences of PoS tags or semantic classes in addition to standard word-based dependencies. It should be possible to encode a model with structure comparable to ours in the FLM framework, but it does not lend itself naturally to 353 having a variable number of features depending on the predicted token in the way our model allows a variable number of parts in a compound. Another common approach for addressing the sparsity effects of compounding (Koehn and Knight, 2003; Koehn et al., 2008; Stymne, 2009; Berton et al., 1996), and rich morphology (Habash and Sadat, 2006; Geutner, 1995), has been to use pre/post-processing with an otherwise unmodified translation system or speech recognition system. This approach views the existing machinery as adequate and shifts the focus to finding a more appropriate segmentation of words into tokens, i.e. compounds into parts or words into morphemes, thus achieving a vocabulary reduction. The downside of such a method is that training a standard n-gram language model on pre-segmented data introduces unwanted effects: in the case of German com"
C12-1022,E03-1076,0,0.107741,"feature value is generated conditioned on some history of preceding feature values. This allows one to construct n-gram models with dependencies among sequences of PoS tags or semantic classes in addition to standard word-based dependencies. It should be possible to encode a model with structure comparable to ours in the FLM framework, but it does not lend itself naturally to 353 having a variable number of features depending on the predicted token in the way our model allows a variable number of parts in a compound. Another common approach for addressing the sparsity effects of compounding (Koehn and Knight, 2003; Koehn et al., 2008; Stymne, 2009; Berton et al., 1996), and rich morphology (Habash and Sadat, 2006; Geutner, 1995), has been to use pre/post-processing with an otherwise unmodified translation system or speech recognition system. This approach views the existing machinery as adequate and shifts the focus to finding a more appropriate segmentation of words into tokens, i.e. compounds into parts or words into morphemes, thus achieving a vocabulary reduction. The downside of such a method is that training a standard n-gram language model on pre-segmented data introduces unwanted effects: in th"
C12-1022,N06-1014,0,0.0371505,"known” token if they do not appear in the target-side of the bitext (see below). The motivation is that the hypotheses to be scored against the language model during decoding are by definition constrained to this vocabulary. Our test corpus for the monolingual task is the union of all the WMT11 development data for German (news-test2008,9,10, 7065 sentences). For translation experiments, the preprocessed English-German bitext was filtered to exclude sentences longer than 50 tokens, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the Berkeley Aligner (Liang et al., 2006) and used as a basis from which to extract a Hiero-style synchronous CFG (Chiang, 2007). The weights of the linear translation models were tuned towards BLEU using cdec’s (Dyer et al., 2010) implementation of MERT (Och, 2003). For this, the development set news-test2008 (2051 sentences) was used, while final BLEU scores are measured on the official test set newstest2011 (3003 sentences, 171460 tokens), without detokenising or recasing hypotheses. Compound segmentation For this evaluation, we used an a priori segmentation of compounds into parts to build our models. This means we assume a singl"
C12-1022,P09-1012,0,0.0190265,"robust since it does retain the original surface form of the word – recall that the decomposition step amounts to interpolated back-off. Baroni and Matiasek (2002) proposed basic models of German compounds for use in predictive text input, exploiting the same link between right-headedness and context as we have, although their focus was restricted to compounds with two components. In terms of Bayesian modelling, the PYP has been found to be very useful in a variety of tasks, including word segmentation, speech recognition, domain adaption and unsupervised PoS tagging (Goldwater et al., 2006; Mochihashi et al., 2009; Huang and Renals, 2007; Neubig et al., 2010; Wood and Teh, 2009; Blunsom and Cohn, 2011). In all cases its power-law scaling and ease of extensibility via the base distribution allowed the formulation of interesting models that achieved competitive results. 7 Conclusion We have demonstrated how an existing hierarchical Bayesian model can be used to build an n-gram language model that is informed by intuitions about the specific linguistic phenomenon of closed-form compounds. While our focus was on compounds, we argue that this approach can be useful for other phenomena, such as rich morpholo"
C12-1022,P03-1021,0,0.0361389,"corpus for the monolingual task is the union of all the WMT11 development data for German (news-test2008,9,10, 7065 sentences). For translation experiments, the preprocessed English-German bitext was filtered to exclude sentences longer than 50 tokens, resulting in 1.7 million parallel sentences; word alignments were inferred from this using the Berkeley Aligner (Liang et al., 2006) and used as a basis from which to extract a Hiero-style synchronous CFG (Chiang, 2007). The weights of the linear translation models were tuned towards BLEU using cdec’s (Dyer et al., 2010) implementation of MERT (Och, 2003). For this, the development set news-test2008 (2051 sentences) was used, while final BLEU scores are measured on the official test set newstest2011 (3003 sentences, 171460 tokens), without detokenising or recasing hypotheses. Compound segmentation For this evaluation, we used an a priori segmentation of compounds into parts to build our models. This means we assume a single, fixed analysis of a compound regardless of the context it occurs in, which is necessitated by the fact that our probabilistic model does not specify a step for choosing an analysis. To construct a segmentation dictionary,"
C12-1022,E09-3008,0,0.149328,"ome history of preceding feature values. This allows one to construct n-gram models with dependencies among sequences of PoS tags or semantic classes in addition to standard word-based dependencies. It should be possible to encode a model with structure comparable to ours in the FLM framework, but it does not lend itself naturally to 353 having a variable number of features depending on the predicted token in the way our model allows a variable number of parts in a compound. Another common approach for addressing the sparsity effects of compounding (Koehn and Knight, 2003; Koehn et al., 2008; Stymne, 2009; Berton et al., 1996), and rich morphology (Habash and Sadat, 2006; Geutner, 1995), has been to use pre/post-processing with an otherwise unmodified translation system or speech recognition system. This approach views the existing machinery as adequate and shifts the focus to finding a more appropriate segmentation of words into tokens, i.e. compounds into parts or words into morphemes, thus achieving a vocabulary reduction. The downside of such a method is that training a standard n-gram language model on pre-segmented data introduces unwanted effects: in the case of German compounds, the sp"
C12-1022,P06-1124,0,0.633892,"hat the family of hierarchical Pitman-Yor language models is an attractive vehicle through which to address the problem, and demonstrate the approach by proposing a model for German compounds. In our empirical evaluation the model outperforms a modified Kneser-Ney n-gram model in test set perplexity. When used as part of a translation system, the proposed language model matches the baseline BLEU score for English→German while improving the precision with which compounds are output. We find that an approximate inference technique inspired by the Bayesian interpretation of Kneser-Ney smoothing (Teh, 2006) offers a way to drastically reduce model training time with negligible impact on translation quality. TITLE AND ABSTRACT IN AFRIKAANS Bayes-modellering van saamgestelde woorde in Duits Hierdie werk neem uitdagings rondom die uitbreiding van n-gramtaalmodelle volgens voorafgaande linguistieke intuïsie onder die loep. Ons voer aan dat die familie van hiërargiese Pitman-Yor taalmodelle ’n wenslike stuk gereedskap is om hierdie probleem mee aan te pak en formuleer ’n model van Duitse saamgestelde woorde om die benadering te demonstreer. Met behulp van ’n empiriese evaluering bevind ons dat die mo"
C12-1022,J07-2003,0,\N,Missing
D08-1023,P08-1024,1,0.699798,"ases in translation performance. 1 Introduction The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention. Discriminative approaches are widely seen as a promising technique, potentially allowing us to further the state-of-the-art. Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (Liang et al., 2006; Watanabe et al., 2006), or rescaling a product of sub-models (Och, 2003; Ittycheriah and Roukos, 2007). Recent work by Blunsom et al. (2008) has shown how translation can be framed as a probabilistic log-linear model, where the distribution over translations is modelled in terms of a latent variable on derivations. Their approach was globally optimised and discriminative trained. However, a language model, an information source known to be Here, we show how language models can be incorporated into large-scale discriminative translation models, without losing the probabilistic interpretation of the model. The key insight is that we can use Monte-Carlo methods to approximate the partition function, thereby allowing us to tackle the"
D08-1023,J07-2003,0,0.837682,"nous context free grammar (SCFG, (Lewis II and Stearns, 1968)) describes the generation of pairs of strings. A string pair is generated by applying a series of paired context-free rewrite rules of the form, X → hα, γ, ∼i, where X is a nonterminal, α and γ are strings of terminals and nonterminals and ∼ specifies a one-to-one alignment between non-terminals in α and γ. In the context of SMT, by assigning the source and target languages to the respective sides of a SCFG it is possible to describe translation as the process of parsing the source sentence, while generating the target translation (Chiang, 2007). In this paper we only consider grammars extracted using the heuristics described for the Hiero SMT system (Chiang, 2007). Note however that our approach is general and could be used with other synchronous grammar transducers (e.g., (Galley et al., 2006)). SCFG productions can specify that the order of the child non-terminals is the same in both languages (a monotone production), or is reversed (a reordering production). Without loss of generality, here we add the restriction that non-terminals on the source and target sides of the grammar must have the same category. Figure 1 shows an exampl"
D08-1023,P06-1121,0,0.0451368,"α and γ are strings of terminals and nonterminals and ∼ specifies a one-to-one alignment between non-terminals in α and γ. In the context of SMT, by assigning the source and target languages to the respective sides of a SCFG it is possible to describe translation as the process of parsing the source sentence, while generating the target translation (Chiang, 2007). In this paper we only consider grammars extracted using the heuristics described for the Hiero SMT system (Chiang, 2007). Note however that our approach is general and could be used with other synchronous grammar transducers (e.g., (Galley et al., 2006)). SCFG productions can specify that the order of the child non-terminals is the same in both languages (a monotone production), or is reversed (a reordering production). Without loss of generality, here we add the restriction that non-terminals on the source and target sides of the grammar must have the same category. Figure 1 shows an example derivation for Chinese to English translation. 216 Using Equation (1), the conditional probability of a target translation given the source is the sum over all of its derivations: X pΛ (e|f ) = pΛ (d, e|f ) d∈∆(e,f ) where ∆(e, f ) is the set of all der"
D08-1023,2006.amta-papers.8,0,0.193237,"g us to tackle the extra computational burden associated with adding the language model. This approach is theoretically justified and means that the model continues to be both probabilistic and globally optimised. As expected, using a language model dramatically increases translation performance. Our second major contribution is an exploitation of syntactic features. By encoding source syntax as features allows the model to use, or ignore, this information as it sees fit, thereby avoiding the problems of coverage and sparsity associated with directly incorporating the syntax into the grammar (Huang et al., 2006; Mi et al., 2008). We report on translation gains using this approach. We begin by introducing the synchronous grammar approach to SMT in Section 2. In Section 3 we define the parametric form of our model and describe techniques for approximating the intractable space of all translations for a given source sentence. In Section 4 we evaluate the ability of our model to effectively estimate the highly dependent weights for the sparse features and realvalued language model. In addition we describe how 215 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pag"
D08-1023,N07-1008,0,0.0544121,"ctured syntactic features, and achieving increases in translation performance. 1 Introduction The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention. Discriminative approaches are widely seen as a promising technique, potentially allowing us to further the state-of-the-art. Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (Liang et al., 2006; Watanabe et al., 2006), or rescaling a product of sub-models (Och, 2003; Ittycheriah and Roukos, 2007). Recent work by Blunsom et al. (2008) has shown how translation can be framed as a probabilistic log-linear model, where the distribution over translations is modelled in terms of a latent variable on derivations. Their approach was globally optimised and discriminative trained. However, a language model, an information source known to be Here, we show how language models can be incorporated into large-scale discriminative translation models, without losing the probabilistic interpretation of the model. The key insight is that we can use Monte-Carlo methods to approximate the partition functi"
D08-1023,N04-1022,0,0.0228719,"rue distribu(e|f ) model we tion over derivations. For the p+LM Λ suffer the same problem as in training and cannot build the full chart. Instead a chart is built using the cube-pruning algorithm with a wide beam and we then draw samples from this chart. Although sampling from a reduced chart will result in biased samples, in Section 4 we show this approach to be effective in practice.2 In Section 4 we compare our sampling approach to the heuristic beam search proposed by Blunsom et al. (2008). It is of interest to compare our proposed decoding algorithms to minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004), a commonly used decoding method. From a theoretical standpoint, the summing of derivations for a given translation is exactly 2 We have experimented with using a Metropolis Hastings sampler, with p−LM (e|f ) as the proposal distribution, to samΛ ple from the true distribution with the language model. Unfortunately the sample rejection rate was very high such that this method proved infeasibly slow. equivalent to performing MBR with a 0/1 loss function over derivations. From a practical perspective, MBR is normally performed with B LEU as the loss and approximated using n-best lists. These n-"
D08-1023,P06-1096,0,0.29552,"Missing"
D08-1023,P08-1023,0,0.0289502,"xtra computational burden associated with adding the language model. This approach is theoretically justified and means that the model continues to be both probabilistic and globally optimised. As expected, using a language model dramatically increases translation performance. Our second major contribution is an exploitation of syntactic features. By encoding source syntax as features allows the model to use, or ignore, this information as it sees fit, thereby avoiding the problems of coverage and sparsity associated with directly incorporating the syntax into the grammar (Huang et al., 2006; Mi et al., 2008). We report on translation gains using this approach. We begin by introducing the synchronous grammar approach to SMT in Section 2. In Section 3 we define the parametric form of our model and describe techniques for approximating the intractable space of all translations for a given source sentence. In Section 4 we evaluate the ability of our model to effectively estimate the highly dependent weights for the sparse features and realvalued language model. In addition we describe how 215 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 215–223, c Hono"
D08-1023,P03-1021,0,0.173238,"acting structured syntactic features, and achieving increases in translation performance. 1 Introduction The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention. Discriminative approaches are widely seen as a promising technique, potentially allowing us to further the state-of-the-art. Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (Liang et al., 2006; Watanabe et al., 2006), or rescaling a product of sub-models (Och, 2003; Ittycheriah and Roukos, 2007). Recent work by Blunsom et al. (2008) has shown how translation can be framed as a probabilistic log-linear model, where the distribution over translations is modelled in terms of a latent variable on derivations. Their approach was globally optimised and discriminative trained. However, a language model, an information source known to be Here, we show how language models can be incorporated into large-scale discriminative translation models, without losing the probabilistic interpretation of the model. The key insight is that we can use Monte-Carlo methods to a"
D08-1023,P02-1040,0,0.103942,"on task (Eck and Hori, 2005), using the 2004 test set as development data for tuning the hyperparameters and MERT training the benchmark systems. The statistics for this data are presented in Table 1.3 The training data made available for this task consisted of 40k pairs of transcribed utterances, drawn from the travel domain. The development and test data for this task are somewhat unusual in that each sentence has a single human translated reference, and fifteen paraphrases of this reference, provided by monolingual annotators. Model performance is evaluated using the standard B LEU metric (Papineni et al., 2002) which measures average n-gram precision, n ≤ 4, and we use the NIST definition of the brevity penalty for multiple reference test sets. We provide evaluation against both the entire multi-reference sets, and the single human translation. Our translation grammar is induced using the standard alignment and rule extraction heuristics used in hierarchical translation models (Chiang, 2007).4 As these heuristics aren’t based on a generative model, and don’t guarantee that the target translation will be reachable from the source, we discard those sentence pairs for which we cannot produce a derivati"
D08-1023,P06-1098,0,0.0178298,"rate the power of the discriminative training paradigm by extracting structured syntactic features, and achieving increases in translation performance. 1 Introduction The goal of creating statistical machine translation (SMT) systems incorporating rich, sparse, features over syntax and morphology has consumed much recent research attention. Discriminative approaches are widely seen as a promising technique, potentially allowing us to further the state-of-the-art. Most work on discriminative training for SMT has focussed on linear models, often with margin based algorithms (Liang et al., 2006; Watanabe et al., 2006), or rescaling a product of sub-models (Och, 2003; Ittycheriah and Roukos, 2007). Recent work by Blunsom et al. (2008) has shown how translation can be framed as a probabilistic log-linear model, where the distribution over translations is modelled in terms of a latent variable on derivations. Their approach was globally optimised and discriminative trained. However, a language model, an information source known to be Here, we show how language models can be incorporated into large-scale discriminative translation models, without losing the probabilistic interpretation of the model. The key in"
D08-1023,2005.iwslt-1.1,0,\N,Missing
D09-1037,P09-1088,1,0.484558,"g-range reorderings between source and target. However if the grammar itself is extracted using wordalignments induced with models that are unable to capture such reorderings, it is unlikely that the grammar will live up to expectations. In this work we draw on recent advances in Bayesian modelling of grammar induction (Johnson et al., 2007; Cohn et al., 2009) to propose a non-parametric model of synchronous tree substitution grammar (STSG), continuing a recent trend in SMT to seek principled probabilistic formulations for heuristic translation models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009b; Blunsom et al., 2009a). This model leverages a hierarchical Bayesian prior to induce a compact translation grammar directly from a parsed parallel corpus, unconstrained by word-alignments. We show that the induced grammars are more plausible and improve translation output. This paper is structured as follows: In Section Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to date has focussed primarily on decoding with s"
D09-1037,W08-0336,0,0.0169488,"rs in this way constitutes a single sample from the Gibbs sampler. {rp = h(NP DT 1 NN 2 ), 1 2 i, rv = h(DT Every), 每i, 5 rw = h(NN corner), 一 个 角落i} , Experiments We evaluate our non-parametric model of grammar induction on a subset of the NIST ChineseEnglish translation evaluation, representing a realistic SMT experiment with millions of words and long sentences. The Chinese-English training data consists of the FBIS corpus (LDC2003E14) and the first 100k sentence pairs from the Sinorama corpus (LDC2005E47). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008), and the English text was parsed using the Stanford parser (Klein and Manning, 2003). As a baseline we implemented the heuristic grammar extraction technique of Galley et al. (2004) (henceforth GHKM). This method finds the minimum sized translation rules which are consistent with a word-aligned sentence pair, as and the right option implies rules {rp = h(NP DT 1 NN 2 ), 2 1 i, rv = h(DT Every), 一 个 角落i, rw = h(NN corner), 每i} . We simply evaluate the probability of both triples of rules under our model, P (rp , rv , rw |r− ) = P (rp |r− )P (rv |r− , rp ) P (rw |r− , rp , rv ), where the addit"
D09-1037,J07-2003,0,0.0967129,"P Foreign) (NNP Ministry) NN 1 (NNP Zhu) (NNP Bangzao)), 外交部 1 朱邦造i h(S S 1 S 2 ), 1 2 i h(S S 1 (NP (PRP We)) VP 2 . 3 ), 1 2 3 i h(NP (DT the) (NNS people) POS 1 ), 人民 1 i phrases with an noun phrase. 5.2 Translation In order to test the translation performance of the grammars induced by our model and the GHKM method6 we report B LEU (Papineni et al., 2002) scores on sentences of up to twenty words in length from the MT03 NIST evaluation. We built a synchronous beam search decoder to find the maximum scoring derivation, based on the CYK+ chart parsing algorithm and the cubepruning method of Chiang (2007). Parse edges for all constituents spanning a given chart cell were cube-pruned together using a beam of width 1000, and only edges from the top ten constituents in each cell were retained. No artificial glue-rules or rule span limits were employed.7 The parameters of the translation system were trained to maximize B LEU on the MT02 test set (Och, 2003). Decoding took roughly 10s per sentence for both grammars, using a 8-core 2.6Ghz Intel Xeon machine. Table 6 shows the B LEU scores for the baseline using the GHKM rule induction algorithm, and our non-parametric Bayesian grammar induction meth"
D09-1037,N09-1062,1,0.116442,"nment points cross constituent structures), resulting in large and implausible translation rules which generalise poorly to unseen data (Fossum et al., 2008). The principal reason for employing a grammar based formalism is to induce rules which capture long-range reorderings between source and target. However if the grammar itself is extracted using wordalignments induced with models that are unable to capture such reorderings, it is unlikely that the grammar will live up to expectations. In this work we draw on recent advances in Bayesian modelling of grammar induction (Johnson et al., 2007; Cohn et al., 2009) to propose a non-parametric model of synchronous tree substitution grammar (STSG), continuing a recent trend in SMT to seek principled probabilistic formulations for heuristic translation models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009b; Blunsom et al., 2009a). This model leverages a hierarchical Bayesian prior to induce a compact translation grammar directly from a parsed parallel corpus, unconstrained by word-alignments. We show that the induced grammars are more plausible and improve translation output. This paper is structured as follows: In Section Tree based transl"
D09-1037,W06-3105,0,0.0772044,"th syntax trees on target side. Their method projects the source strings onto nodes of the target tree using the word alignment, and then extracts the minimal transduction rules as well as rules composed of adjacent minimal units. The production weights are estimated either by heuristic counting (Koehn et al., 2003) or using the EM algorithm. Both estimation techniques are flawed. The heuristic method is inconsistent in the limit (Johnson, 2002) while EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al., 2006). With no limit on rule size this method will learn a single rule for every training instance, and therefore will not generalise to unseen sentences. These problems can be ameliorated by imposing limits on rule size or early stopping of EM training, however neither of these techniques addresses the underlying problems. Background Current tree-to-string translation models are a form of Synchronous Tree Substitution Grammar (STSG; Eisner (2003)). Formally, a STSG is a 5-tuple, G = (T, T 0 , N, S, R), where T and T 0 are sets of terminal symbols in the target and source languages respectively, N"
D09-1037,D08-1033,0,0.193956,"Missing"
D09-1037,P03-2041,0,0.0692023,"le EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al., 2006). With no limit on rule size this method will learn a single rule for every training instance, and therefore will not generalise to unseen sentences. These problems can be ameliorated by imposing limits on rule size or early stopping of EM training, however neither of these techniques addresses the underlying problems. Background Current tree-to-string translation models are a form of Synchronous Tree Substitution Grammar (STSG; Eisner (2003)). Formally, a STSG is a 5-tuple, G = (T, T 0 , N, S, R), where T and T 0 are sets of terminal symbols in the target and source languages respectively, N is a set of nonterminal symbols, S ∈ N is the distinguished root non-terminal and R is a set of productions (a.k.a. rules). Each production is a tuple comprising an elementary tree and a string, the former referring to a tree fragment of depth ≥ 1 where each internal node is labelled with a non-terminal and each leaf is labelled with either a terminal or a non-terminal. The string part of the rule describes the lexical component of the rule i"
D09-1037,W08-0306,0,0.0174814,"om a word-aligned parallel corpus. These heuristics are extensions of those developed for phrase-based models (Koehn et al., 2003), and involve symmetrising two directional word alignments followed by a projection step which uses the alignments to find a mapping between source words and nodes in the target parse trees (Galley et al., 2004). However, such approaches leave much to be desired. Word-alignments rarely factorise cleanly with parse trees (i.e., alignment points cross constituent structures), resulting in large and implausible translation rules which generalise poorly to unseen data (Fossum et al., 2008). The principal reason for employing a grammar based formalism is to induce rules which capture long-range reorderings between source and target. However if the grammar itself is extracted using wordalignments induced with models that are unable to capture such reorderings, it is unlikely that the grammar will live up to expectations. In this work we draw on recent advances in Bayesian modelling of grammar induction (Johnson et al., 2007; Cohn et al., 2009) to propose a non-parametric model of synchronous tree substitution grammar (STSG), continuing a recent trend in SMT to seek principled pro"
D09-1037,N04-1035,0,0.371857,"Missing"
D09-1037,P03-1021,0,0.0391611,"02) scores on sentences of up to twenty words in length from the MT03 NIST evaluation. We built a synchronous beam search decoder to find the maximum scoring derivation, based on the CYK+ chart parsing algorithm and the cubepruning method of Chiang (2007). Parse edges for all constituents spanning a given chart cell were cube-pruned together using a beam of width 1000, and only edges from the top ten constituents in each cell were retained. No artificial glue-rules or rule span limits were employed.7 The parameters of the translation system were trained to maximize B LEU on the MT02 test set (Och, 2003). Decoding took roughly 10s per sentence for both grammars, using a 8-core 2.6Ghz Intel Xeon machine. Table 6 shows the B LEU scores for the baseline using the GHKM rule induction algorithm, and our non-parametric Bayesian grammar induction method. We see a small increase in generalisation performance from our model. Our previous analTable 4: Top ten rules in the GHKM grammar that do not appear in the sampled grammar. These are quite low probability rules: their counts range from 1,137 to 103. For example, every instance of the first rule had the same determiner and target translation, h(PP (I"
D09-1037,P02-1040,0,0.107151,"h(NP NP 1 , 2 NP 3 (, ,) CC 4 NP 5 ), 1 2 3 4 5 i h(NP NP 1 , 2 NP 3 , 4 NP 5 (, ,) (CC and) NP 6 ), 1 2 3 4 5 , 6 i h(S S 1 (NP (PRP They)) VP 2 . 3 ), 1 2 3 i h(S PP 1 , 2 NP 3 VP 4 . 5 “ 6 ), 1 2 3 4 6 5 i h(S PP 1 , 2 NP 3 VP 4 . 5 ), 1 中 2 3 4 5 i h(NP (NNP Foreign) (NNP Ministry) NN 1 (NNP Zhu) (NNP Bangzao)), 外交部 1 朱邦造i h(S S 1 S 2 ), 1 2 i h(S S 1 (NP (PRP We)) VP 2 . 3 ), 1 2 3 i h(NP (DT the) (NNS people) POS 1 ), 人民 1 i phrases with an noun phrase. 5.2 Translation In order to test the translation performance of the grammars induced by our model and the GHKM method6 we report B LEU (Papineni et al., 2002) scores on sentences of up to twenty words in length from the MT03 NIST evaluation. We built a synchronous beam search decoder to find the maximum scoring derivation, based on the CYK+ chart parsing algorithm and the cubepruning method of Chiang (2007). Parse edges for all constituents spanning a given chart cell were cube-pruned together using a beam of width 1000, and only edges from the top ten constituents in each cell were retained. No artificial glue-rules or rule span limits were employed.7 The parameters of the translation system were trained to maximize B LEU on the MT02 test set (Och"
D09-1037,P06-1121,0,0.713577,"anguages. Such models are particularly attractive for translating between languages with divergent word orders, such as Chinese and English, where syntax-inspired translation rules can succinctly describe the requisite reordering operations. In contrast, standard phrase-based models (Koehn et al., 2003) assume a mostly monotone mapping between source and target, and therefore cannot adequately model these phenomena. Currently the most successful paradigm for the use of synchronous grammars in translation is that of stringto-tree transduction (Galley et al., 2004; Zollmann and Venugopal, 2006; Galley et al., 2006; Marcu et al., 2006). In this case a grammar is extracted from a parallel corpus, with strings on its source 352 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 352–361, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP 2 we introduce the STSG formalism and describe current heuristic approaches to grammar induction. We define a principled Bayesian model of string-to-tree translation in Section 3, and describe an inference technique using Gibbs sampling in Section 4. In Section 5 we analyse an induced grammar on a corpus of Chinese→English translati"
D09-1037,J95-2002,0,0.0733489,"miner or a preposition. The grammars differ on the complex rules which combine lexicalisation and frontier non-terminals. The GHKM rules are all very simple depth-1 SCFG rules, containing minimal information. In contrast, the sampled rules are more lexicalised, licensing the insertion of various English tokens and tree substructure. Note particularly the second and forth rule which succinctly describe the reordering of prepositional 6 Our decoder was unable to process unary rules (those which consume nothing in the source). Monolingual parsing with unary productions is fairly straightforward (Stolcke, 1995), however in the transductive setting these rules can licence infinite insertions in the target string. This is further complicated by the language model integration. Therefore we composed each unary rule instance with its descendant rule(s) to create a non-unary rule. 7 Our decoder lacks certain features shown to be beneficial to synchronous grammar decoding, in particular rule binarisation (Zhang et al., 2006). As such the reported results for MT03 lag the state-of-the-art: the Moses phrase-based decoder (Koehn et al., 2007) achieves 26.8. We believe that improvements from a better decoder i"
D09-1037,N06-1033,0,0.0747636,"eordering of prepositional 6 Our decoder was unable to process unary rules (those which consume nothing in the source). Monolingual parsing with unary productions is fairly straightforward (Stolcke, 1995), however in the transductive setting these rules can licence infinite insertions in the target string. This is further complicated by the language model integration. Therefore we composed each unary rule instance with its descendant rule(s) to create a non-unary rule. 7 Our decoder lacks certain features shown to be beneficial to synchronous grammar decoding, in particular rule binarisation (Zhang et al., 2006). As such the reported results for MT03 lag the state-of-the-art: the Moses phrase-based decoder (Koehn et al., 2007) achieves 26.8. We believe that improvements from a better decoder implementation would be orthogonal to the improvements presented here (and would allow us to relax the length restriction on the test set). 359 Model GHKM Our model BLEU score 26.0 26.6 Acknowledgements The authors acknowledge the support of the EPSRC (grants GR/T04557/01 and EP/D074959/1). This work has made use of the resources provided by the Edinburgh Compute and Data Facility (ECDF). The ECDF is partially su"
D09-1037,P08-1012,0,0.107099,"lism is to induce rules which capture long-range reorderings between source and target. However if the grammar itself is extracted using wordalignments induced with models that are unable to capture such reorderings, it is unlikely that the grammar will live up to expectations. In this work we draw on recent advances in Bayesian modelling of grammar induction (Johnson et al., 2007; Cohn et al., 2009) to propose a non-parametric model of synchronous tree substitution grammar (STSG), continuing a recent trend in SMT to seek principled probabilistic formulations for heuristic translation models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009b; Blunsom et al., 2009a). This model leverages a hierarchical Bayesian prior to induce a compact translation grammar directly from a parsed parallel corpus, unconstrained by word-alignments. We show that the induced grammars are more plausible and improve translation output. This paper is structured as follows: In Section Tree based translation models are a compelling means of integrating linguistic information into machine translation. Syntax can inform lexical selection and reordering choices and thereby improve translation quality. Research to dat"
D09-1037,W06-3119,0,0.0936833,"mprove translation quality. Research to date has focussed primarily on decoding with such models, but less on the difficult problem of inducing the bilingual grammar from data. We propose a generative Bayesian model of tree-to-string translation which induces grammars that are both smaller and produce better translations than the previous heuristic two-stage approach which employs a separate word alignment step. 1 Introduction Many recent advances in statistical machine translation (SMT) are a result of the incorporation of syntactic knowledge into the translation process (Marcu et al., 2006; Zollmann and Venugopal, 2006). This has been facilitated by the use of synchronous grammars to model translation as a generative process over pairs of strings in two languages. Such models are particularly attractive for translating between languages with divergent word orders, such as Chinese and English, where syntax-inspired translation rules can succinctly describe the requisite reordering operations. In contrast, standard phrase-based models (Koehn et al., 2003) assume a mostly monotone mapping between source and target, and therefore cannot adequately model these phenomena. Currently the most successful paradigm for"
D09-1037,J02-1005,0,0.0430767,"grammar rules from word aligned data is also nontrivial. Galley et al. (2004) describe an algorithm for inducing a string-to-tree grammar using a parallel corpus with syntax trees on target side. Their method projects the source strings onto nodes of the target tree using the word alignment, and then extracts the minimal transduction rules as well as rules composed of adjacent minimal units. The production weights are estimated either by heuristic counting (Koehn et al., 2003) or using the EM algorithm. Both estimation techniques are flawed. The heuristic method is inconsistent in the limit (Johnson, 2002) while EM is degenerate, placing disproportionate probability mass on the largest rules in order to describe the data with as few a rules as possible (DeNero et al., 2006). With no limit on rule size this method will learn a single rule for every training instance, and therefore will not generalise to unseen sentences. These problems can be ameliorated by imposing limits on rule size or early stopping of EM training, however neither of these techniques addresses the underlying problems. Background Current tree-to-string translation models are a form of Synchronous Tree Substitution Grammar (ST"
D09-1037,N03-1017,0,0.476825,"ax-Directed Tree to String Grammar Induction Trevor Cohn and Phil Blunsom School of Informatics University of Edinburgh 10 Crichton Street, Edinburgh EH8 9AB Scotland, United Kingdom {tcohn,pblunsom}@inf.ed.ac.uk Abstract side and syntax trees on its target side, which is then used to translate novel sentences by performing inference over the space of target syntax trees licensed by the grammar. To date grammar-based translation models have relied on heuristics to extract a grammar from a word-aligned parallel corpus. These heuristics are extensions of those developed for phrase-based models (Koehn et al., 2003), and involve symmetrising two directional word alignments followed by a projection step which uses the alignments to find a mapping between source words and nodes in the target parse trees (Galley et al., 2004). However, such approaches leave much to be desired. Word-alignments rarely factorise cleanly with parse trees (i.e., alignment points cross constituent structures), resulting in large and implausible translation rules which generalise poorly to unseen data (Fossum et al., 2008). The principal reason for employing a grammar based formalism is to induce rules which capture long-range reo"
D09-1037,P07-2045,0,0.00785667,"). Monolingual parsing with unary productions is fairly straightforward (Stolcke, 1995), however in the transductive setting these rules can licence infinite insertions in the target string. This is further complicated by the language model integration. Therefore we composed each unary rule instance with its descendant rule(s) to create a non-unary rule. 7 Our decoder lacks certain features shown to be beneficial to synchronous grammar decoding, in particular rule binarisation (Zhang et al., 2006). As such the reported results for MT03 lag the state-of-the-art: the Moses phrase-based decoder (Koehn et al., 2007) achieves 26.8. We believe that improvements from a better decoder implementation would be orthogonal to the improvements presented here (and would allow us to relax the length restriction on the test set). 359 Model GHKM Our model BLEU score 26.0 26.6 Acknowledgements The authors acknowledge the support of the EPSRC (grants GR/T04557/01 and EP/D074959/1). This work has made use of the resources provided by the Edinburgh Compute and Data Facility (ECDF). The ECDF is partially supported by the eDIKT initiative. Table 6: Translation results on the NIST test set MT03 for sentences of length ≤ 20."
D09-1037,W06-1606,0,0.157756,"Missing"
D09-1037,J03-1002,0,\N,Missing
D10-1117,P06-1109,0,0.0176898,"dard part-of-speech tags after removing punctuation. We use a vague Beta prior for the stopping probabilities in Plcfg , sc ∼ Beta(1, 1). All the hyper-parameters are resampled after every 10th sample of the corpus derivations. 4.3 Parsing Unfortunately finding the maximising parse tree for a string under our TSG-DMV model is intractable due to the inter-rule dependencies created by the PYP formulation. Previous work has used Monte Carlo techniques to sample for one of the maximum probability parse (MPP), maximum probability derivation (MPD) or maximum marginal parse (MMP) (Cohn et al., 2009; Bod, 2006). We take a simpler approach and use the Viterbi algorithm to calculate the MPD under an approximating TSG defined by the last set of derivations sampled for the corpus during training. Our results indicate that this is a reasonable approximation, though the experience of other researchers suggests that calculating the MMP under the approximating TSG may also be beneficial for DMV (Cohen et al., 2008). 5 Experiments We follow the standard evaluation regime for DMV style models by performing experiments on the text of the WSJ section of the Penn. Treebank (Marcus et al., 1993) and reporting hea"
D10-1117,P04-1014,0,0.119639,"Missing"
D10-1117,W01-0713,0,0.00939395,"agments and thereby better modelling the text. We define a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions. This approach significantly improves the state-of-the-art, when measured by head attachment accuracy. 1 Introduction Grammar induction is a central problem in Computational Linguistics, the aim of which is to induce linguistic structures from an unannotated text corpus. Despite considerable research effort this unsupervised problem remains largely unsolved, particularly for traditional phrase-structure parsing approaches (Clark, 2001; Klein and Manning, 2002). Phrase-structure parser induction is made difficult due to two types of ambiguity: the constituent structure and the constituent labels. In particular the constituent labels are highly ambiguous, firstly we don’t know a priori how many there are, and secondly labels that appear high in a tree (e.g., an S category for a clause) rely on the correct inference of all the latent labels below them. However recent work on the induction of dependency grammars has proved Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk more fruitful (K"
D10-1117,N09-1009,0,0.617435,"alue 0 (no attachment in the direction (dir) d) and 1 (one or more attachment). L and R indicates child dependents left or right of the parent; superscripts encode the stopping and valency distributions, X1 indicates that the head will continue to attach more children and X∗ that it has already attached a child. 2 Background The most successful framework for unsupervised dependency induction is the Dependency Model with Valence (DMV) (Klein and Manning, 2004). This model has been adapted and extended by a number of authors and currently represents the stateof-the-art for dependency induction (Cohen and Smith, 2009; Headden III et al., 2009). Eisner (2000) introduced the split-head algorithm which permits efficient O(|w|3 ) parsing complexity by replicating (splitting) each terminal and processing left and right dependents separately. We employ the related fold-unfold representation of Johnson (2007) that defines a CFG equivalent of the splithead parsing algorithm, allowing us to easily adapt CFG-based grammar models to dependency grammar. Table 1 shows the equivalent CFG grammar for the DMV model (CFG-DMV) using the unfold-fold transformation. The key insight to understanding the non-terminals in this"
D10-1117,N09-1062,1,0.908726,"ponent rules. Estimating a PTSG requires learning the sufficient statistics for P (e|c) in (1) based on a training sample. Parsing involves finding the most probable tree for a given string (arg maxt P (t|w)). This is typically approximated by finding the most probable derivation which can be done efficiently using the CYK algorithm. 3.1 Model In this work we propose the Tree Substitution Grammar Dependency Model with Valence (TSG-DMV). We define a hierarchical non-parametric TSG model on the space of parse trees licensed by the CFG grammar in Table 1. Our model is a generalisation of that of Cohn et al. (2009) and Cohn et al. (2011). We extend those works by moving from a single level Dirichlet Process (DP) distribution over rules to a multi-level Pitman-Yor Process (PYP), and including lexicalisation. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006; Goldwater et al., 2006). Teh (2006) used a hierarchical PYP to model backoff in language models, we leverage this same capability to model backoff in TSG rules. This effectively allows smoothing from lexicalised to unlexicalised grammars, and from TSG to CFG rules. Here we describe our deepest"
D10-1117,J03-4003,0,0.172699,"Missing"
D10-1117,N09-1012,0,0.714087,"Missing"
D10-1117,P07-1022,0,0.0282885,"tached a child. 2 Background The most successful framework for unsupervised dependency induction is the Dependency Model with Valence (DMV) (Klein and Manning, 2004). This model has been adapted and extended by a number of authors and currently represents the stateof-the-art for dependency induction (Cohen and Smith, 2009; Headden III et al., 2009). Eisner (2000) introduced the split-head algorithm which permits efficient O(|w|3 ) parsing complexity by replicating (splitting) each terminal and processing left and right dependents separately. We employ the related fold-unfold representation of Johnson (2007) that defines a CFG equivalent of the splithead parsing algorithm, allowing us to easily adapt CFG-based grammar models to dependency grammar. Table 1 shows the equivalent CFG grammar for the DMV model (CFG-DMV) using the unfold-fold transformation. The key insight to understanding the non-terminals in this grammar is that the subscripts encode the terminals at the boundaries of the span of that non-terminal. For example the non-terminal LH encodes that the right most terminal spanned by this constituent is H (and the reverse for H R), while A MB encodes that A and B are the left-most 1205 and"
D10-1117,P02-1017,0,0.124434,"hereby better modelling the text. We define a hierarchical non-parametric Pitman-Yor Process prior which biases towards a small grammar with simple productions. This approach significantly improves the state-of-the-art, when measured by head attachment accuracy. 1 Introduction Grammar induction is a central problem in Computational Linguistics, the aim of which is to induce linguistic structures from an unannotated text corpus. Despite considerable research effort this unsupervised problem remains largely unsolved, particularly for traditional phrase-structure parsing approaches (Clark, 2001; Klein and Manning, 2002). Phrase-structure parser induction is made difficult due to two types of ambiguity: the constituent structure and the constituent labels. In particular the constituent labels are highly ambiguous, firstly we don’t know a priori how many there are, and secondly labels that appear high in a tree (e.g., an S category for a clause) rely on the correct inference of all the latent labels below them. However recent work on the induction of dependency grammars has proved Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk more fruitful (Klein and Manning, 2004). D"
D10-1117,P04-1061,0,0.850032,"1; Klein and Manning, 2002). Phrase-structure parser induction is made difficult due to two types of ambiguity: the constituent structure and the constituent labels. In particular the constituent labels are highly ambiguous, firstly we don’t know a priori how many there are, and secondly labels that appear high in a tree (e.g., an S category for a clause) rely on the correct inference of all the latent labels below them. However recent work on the induction of dependency grammars has proved Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk more fruitful (Klein and Manning, 2004). Dependency grammars (Mel0 cˇ uk, 1988) should be easier to induce from text compared to phrase-structure grammars because the set of labels (heads) are directly observed as the words in the sentence. Approaches to unsupervised grammar induction, both for phrase-structure and dependency grammars, have typically used very simplistic models (Clark, 2001; Klein and Manning, 2004), especially in comparison to supervised parsing models (Collins, 2003; Clark and Curran, 2004; McDonald, 2006). Simple models are attractive for grammar induction because they have a limited capacity to overfit, however"
D10-1117,J93-2004,0,0.0383775,"e (MMP) (Cohn et al., 2009; Bod, 2006). We take a simpler approach and use the Viterbi algorithm to calculate the MPD under an approximating TSG defined by the last set of derivations sampled for the corpus during training. Our results indicate that this is a reasonable approximation, though the experience of other researchers suggests that calculating the MMP under the approximating TSG may also be beneficial for DMV (Cohen et al., 2008). 5 Experiments We follow the standard evaluation regime for DMV style models by performing experiments on the text of the WSJ section of the Penn. Treebank (Marcus et al., 1993) and reporting head attachment accuracy. Like previous work we pre-process the training and test data to remove punctuation, training our unlexicalised models on the gold-standard part-of-speech tags, and including words occurring more than 100 times in our lexicalised models (Headden III et al., 2009). It is very difficult for an unsupervised model to learn from long training sentences as they contain 1209 a great deal of ambiguity, therefore the majority of DMV based models have been trained on sentences restricted in length to ≤ 10 tokens.3 This has the added benefit of decreasing the runti"
D10-1117,N10-1116,0,0.33116,"ation (σ) from forty sampling runs. 5.1 Discussion Table 4 shows the head attachment accuracy results for our TSG-DMV, plus many other significant previously proposed models. The subset of hierarchical priors used by each model is noted in brackets. The performance of our models is extremely encouraging, particularly the fact that it achieves the highest reported accuracy on the full test set by a considerable margin. On the |w |≤ 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009). The L-EVG model extends DMV by adding additional lexicalisation, 3 See Spitkovsky et al. (2010a) for an exception to this rule. icantly better than this model. We can identify a number of differences that may impact these results: Model |w |≤ 10 |w |≤ ∞ the Adaptor Grammar model is trained using variational inference with the space of tree fragments Attach-Right 38.4 31.7 truncated, while we employ a sampler which can EM (Klein and Manning, 2004) 46.1 35.9 nominally explore the full space of tree fragments; Dirichlet (Cohen et al., 2008) 46.1 36.9 and the adapted tree fragments must be complete LN (Cohen et al., 2008) 59.4 40.5 subtrees (i.e. they don’t contain variables), whereas SLN,"
D10-1117,W10-2902,0,0.337904,"ation (σ) from forty sampling runs. 5.1 Discussion Table 4 shows the head attachment accuracy results for our TSG-DMV, plus many other significant previously proposed models. The subset of hierarchical priors used by each model is noted in brackets. The performance of our models is extremely encouraging, particularly the fact that it achieves the highest reported accuracy on the full test set by a considerable margin. On the |w |≤ 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009). The L-EVG model extends DMV by adding additional lexicalisation, 3 See Spitkovsky et al. (2010a) for an exception to this rule. icantly better than this model. We can identify a number of differences that may impact these results: Model |w |≤ 10 |w |≤ ∞ the Adaptor Grammar model is trained using variational inference with the space of tree fragments Attach-Right 38.4 31.7 truncated, while we employ a sampler which can EM (Klein and Manning, 2004) 46.1 35.9 nominally explore the full space of tree fragments; Dirichlet (Cohen et al., 2008) 46.1 36.9 and the adapted tree fragments must be complete LN (Cohen et al., 2008) 59.4 40.5 subtrees (i.e. they don’t contain variables), whereas SLN,"
D10-1117,P10-1130,0,0.201528,"ation (σ) from forty sampling runs. 5.1 Discussion Table 4 shows the head attachment accuracy results for our TSG-DMV, plus many other significant previously proposed models. The subset of hierarchical priors used by each model is noted in brackets. The performance of our models is extremely encouraging, particularly the fact that it achieves the highest reported accuracy on the full test set by a considerable margin. On the |w |≤ 10 test set all the TSG-DMVs are second only to the L-EVG model of Headden III et al. (2009). The L-EVG model extends DMV by adding additional lexicalisation, 3 See Spitkovsky et al. (2010a) for an exception to this rule. icantly better than this model. We can identify a number of differences that may impact these results: Model |w |≤ 10 |w |≤ ∞ the Adaptor Grammar model is trained using variational inference with the space of tree fragments Attach-Right 38.4 31.7 truncated, while we employ a sampler which can EM (Klein and Manning, 2004) 46.1 35.9 nominally explore the full space of tree fragments; Dirichlet (Cohen et al., 2008) 46.1 36.9 and the adapted tree fragments must be complete LN (Cohen et al., 2008) 59.4 40.5 subtrees (i.e. they don’t contain variables), whereas SLN,"
D10-1117,P06-1124,0,0.019717,"sing the CYK algorithm. 3.1 Model In this work we propose the Tree Substitution Grammar Dependency Model with Valence (TSG-DMV). We define a hierarchical non-parametric TSG model on the space of parse trees licensed by the CFG grammar in Table 1. Our model is a generalisation of that of Cohn et al. (2009) and Cohn et al. (2011). We extend those works by moving from a single level Dirichlet Process (DP) distribution over rules to a multi-level Pitman-Yor Process (PYP), and including lexicalisation. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006; Goldwater et al., 2006). Teh (2006) used a hierarchical PYP to model backoff in language models, we leverage this same capability to model backoff in TSG rules. This effectively allows smoothing from lexicalised to unlexicalised grammars, and from TSG to CFG rules. Here we describe our deepest model which has a four level hierarchy, depicted graphically in Table 2. In Section 5 we evaluate different subsets of this hierarchy. The topmost level of our model describes lexicalised elementary elementary fragments (e) as produced by a PYP, ∼ Gc e|c lcfg Gc |ac , bc , P ∼ PYP(ac , bc , Plcfg (·|c)"
D10-1117,N10-1081,0,\N,Missing
D10-1117,P10-2042,1,\N,Missing
D12-1021,P11-1087,1,0.0339292,"guous phrasal translation units. Surprisingly, the freedom that this extra power affords allows the Gibbs sampler we propose to mix more quickly, allowing state-of-the-art results from a simple initialiser. 3 Model We use a nonparametric generative model based on the 2-parameter Pitman-Yor process (PYP) (Pitman and Yor, 1997), a generalisation of the Dirichlet Process, which has been used for various NLP modeling tasks with state-of-the-art results such as language modeling, word segmentation, text compression and part of speech induction (Teh, 2006; Goldwater et al., 2006; Wood et al., 2011; Blunsom and Cohn, 2011). In this section we first provide a brief definition of the SCFG formalism and then describe our PYP prior for them. 3.1 Synchronous Context-Free Grammar An synchronous context-free grammar (SCFG) is a 5-tuple hΣ, ∆, V, S, Ri that generalises context-free grammar to generate strings concurrently in two languages (Lewis and Stearns, 1968). Σ is a finite set of source language terminal symbols, ∆ is a finite set of target language terminal symbols, V is a set of nonterminal symbols, with a designated start symbol S, and R is a set of synchronous rewrite rules. A string pair is generated by star"
D12-1021,P09-1088,1,0.883446,"pairs (Galley et al., 2004; Chiang, 2007). However, while the models used for translation have evolved, the way in which they are learnt has not: na¨ıve word-based models are still used to infer translational correspondences from parallel corpora. Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules. As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments (Wu, 1997; Zhang et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). In contrast to these previous attempts, our SCFG model scales to large datasets (over 1.3M sentence pairs) without imposing restrictions on the form of the grammar rules or otherwise constraining the set of learnable rules (e.g., with a word alignment). We validate our sampler by demonstrating its ability to recover grammars used to generate synthetic datasets. We then evaluate our model by inducing word alignments for SMT experiments in several typologically diverse language pairs and across a range of corpora sizes. Our results attest to our model’s ability to learn s"
D12-1021,J92-1002,0,0.138042,"text. The model employs a Pitman-Yor Process prior which uses a novel base distribution over synchronous grammar rules. Through both synthetic grammar induction and statistical machine translation experiments, we show that our model learns complex translational correspondences— including discontiguous, many-to-many alignments—and produces competitive translation results. Further, inference is efficient and we present results on significantly larger corpora than prior work. 1 Phil Blunsom Dept. of Computer Science University of Oxford pblunsom@cs.ox.ac.uk Introduction In the twenty years since Brown et al. (1992) pioneered the first word-based statistical machine translation (SMT) models substantially more expressive models of translational equivalence have been developed. The prevalence of complex phrasal, discontiguous, and non-monotonic translation phenomena in real-world applications of machine translation has driven the development of hierarchical and syntactic models based on synchronous context-free grammars (SCFGs). Such models are now widely used in translation and represent the state-of-the-art in most language pairs (Galley et al., 2004; Chiang, 2007). However, while the models used for tra"
D12-1021,J93-2003,0,0.0321846,"ed a rule template which indicates how large the rule is, whether each position contains a terminal or nonterminal symbol, and the reordering of the source nonterminals a. To conclude the process we must select the terminal types from the source and target 226 vocabularies. To do so, we use the following distribution: Pterminals (s, t) = PM 1← (s, t) + PM 1→ (s, t) 2 where PM 1← (s, t) (PM 1→ (s, t)) first generates the source (target) terminals from uniform draws from the vocabulary, then generates the string in the other language according to IBM M ODEL 1, marginalizing over the alignments (Brown et al., 1993). 4 Gibbs Sampler In this section we introduce a Gibbs sampler that enables us to perform posterior inference given a corpus of sentence pairs. Our innovation is to represent the synchronous derivation of a sentence pair in a hierarchical 4-dimensional binary alignment grid, with elements z[s,t,u,v] ∈ {0, 1}. The settings of the grid variables completely determine the SCFG rules in the current derivation. A setting of a binary variable z[s,t,u,v] = 1 represents a constituent linking the source span [s, t] and the target span [u, v] in the current derivation; variables with a value of 0 indicat"
D12-1021,W07-0403,0,0.313161,"us phrases. This had the dual benefits of biasing the model towards learning minimal translation units, and integrating out the parameters such that a much smaller set of statistics would suffice for inference with a Gibbs sampler. However this work fell short by not evaluating the model independently, instead only presenting results in which it was combined with a standard word-alignment initialisation, thus leaving open the question of its efficacy. The fact that flat phrasal models lack a structured approach to reordering has led many researchers to pursue SCFG induction instead (Wu, 1997; Cherry and Lin, 2007; Zhang et al., 2008; Blunsom et al., 2009). The asymptotic time complexity of the inside algorithm for even the simplest SCFG models is O(|s|3 |t|3 ), too high to be practical for most real translation data. A popular solution to this problem is to heuristically restrict inference to derivations which agree with an independent alignment model (Cherry and Lin, 2007; Zhang et al., 2008). However this may have the unintended effect of biasing the model back towards the initial alignments that they attempt to improve upon. More recently Neubig et al. (2011) reported a novel Bayesian model for phr"
D12-1021,J07-2003,0,0.563916,"duction In the twenty years since Brown et al. (1992) pioneered the first word-based statistical machine translation (SMT) models substantially more expressive models of translational equivalence have been developed. The prevalence of complex phrasal, discontiguous, and non-monotonic translation phenomena in real-world applications of machine translation has driven the development of hierarchical and syntactic models based on synchronous context-free grammars (SCFGs). Such models are now widely used in translation and represent the state-of-the-art in most language pairs (Galley et al., 2004; Chiang, 2007). However, while the models used for translation have evolved, the way in which they are learnt has not: na¨ıve word-based models are still used to infer translational correspondences from parallel corpora. Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules. As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments (Wu, 1997; Zhang et al., 2008; Blunsom et al., 2009; Neubig et al., 2011"
D12-1021,D08-1033,0,0.639634,"Missing"
D12-1021,P10-4002,1,0.251798,"a medium scale and comes from the FBIS corpus. The DE - EN pair constitutes the largest corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the B LEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running G IZA ++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hundred million token subset 3 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ L ANGUAGE PAIR T EST S ET M ODEL 4 M ODEL 1 BASELINE I NITIALISATION W EAK M1 I NIT. S TRONG HMM I NIT. UR - EN MT 09 ZH - EN MT 03-08 E URO PARL 23.1 29.4 28.4 18.5 19.8 25.5 23.7 28.3 27.8 24.0 29.8 29.2 DE - EN PYP-SCFG Table 3: Results for the SMT experiments in B LEU . The baseline is produced using a full G IZA ++ run. The M ODEL 1 I NITIALISATION col"
D12-1021,N10-1033,1,0.859874,"SCFG, rules are associated with probabilities such that the probabilities of all rewrites of a particular LHS category sum to 1. Translation with SCFGs is carried out by parsing the source language with the monolingual source language projection of the grammar (using standard monolingual parsing algorithms), which induces a parallel tree structure and translation in the target language (Chiang, 2007). Alignment or synchronous parsing is the process of concurrently parsing both the source and target sentences, uncovering the derivation or derivations that give rise to a string pair (Wu, 1997; Dyer, 2010). Our goal is to infer the most probable SCFG derivations that explain a corpus of parallel sentences, given a nonparametric prior over probabilistic SCFGs. In this work we will consider grammars with a single nonterminal category X. 3.2 Pitman-Yor Process SCFG Before training we have no way of knowing how many rules will be needed in our grammar to adequately represent the data. By using the PitmanYor process as a prior on the parameters of a synchronous grammar we can formulate a model which prefers smaller numbers of rules that are reused often, thereby avoiding degenerate grammars consisti"
D12-1021,N04-1035,0,0.0933862,"Missing"
D12-1021,P06-1085,0,0.00679291,"Our model goes further by allowing discontiguous phrasal translation units. Surprisingly, the freedom that this extra power affords allows the Gibbs sampler we propose to mix more quickly, allowing state-of-the-art results from a simple initialiser. 3 Model We use a nonparametric generative model based on the 2-parameter Pitman-Yor process (PYP) (Pitman and Yor, 1997), a generalisation of the Dirichlet Process, which has been used for various NLP modeling tasks with state-of-the-art results such as language modeling, word segmentation, text compression and part of speech induction (Teh, 2006; Goldwater et al., 2006; Wood et al., 2011; Blunsom and Cohn, 2011). In this section we first provide a brief definition of the SCFG formalism and then describe our PYP prior for them. 3.1 Synchronous Context-Free Grammar An synchronous context-free grammar (SCFG) is a 5-tuple hΣ, ∆, V, S, Ri that generalises context-free grammar to generate strings concurrently in two languages (Lewis and Stearns, 1968). Σ is a finite set of source language terminal symbols, ∆ is a finite set of target language terminal symbols, V is a set of nonterminal symbols, with a designated start symbol S, and R is a set of synchronous rewri"
D12-1021,N03-1017,0,0.234232,"tistics (in words). translation evaluation.3 The ZH - EN data is of a medium scale and comes from the FBIS corpus. The DE - EN pair constitutes the largest corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the B LEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running G IZA ++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hundred million token subset 3 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ L ANGUAGE PAIR T EST S ET M ODEL 4 M ODEL 1 BASELINE I NITIALISATION W EAK M1 I NIT. S TRONG HMM I NIT. UR - EN MT 09 ZH - EN MT 03-08 E URO PARL 23.1 29.4 28.4 18.5 19.8 25.5 23.7 28.3 27.8 24.0 29.8 29.2 DE - EN PYP-SCFG Table 3: Results for the SMT experiments in B LEU . The baseline is pr"
D12-1021,W02-1018,0,0.176665,"ents in several typologically diverse language pairs and across a range of corpora sizes. Our results attest to our model’s ability to learn synchronous grammars encoding complex translation phenomena. 223 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 223–232, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics 2 Prior Work The goal of directly inducing phrasal translation models from parallel corpora has received a lot of attention in the NLP and SMT literature. Marcu and Wong (2002) presented an ambitious maximum likelihood model and EM inference algorithm for learning phrasal translation representations. The first issue this model faced was a massive parameter space and intractable inference. However a more subtle issue is that likelihood based models of this form suffer from a degenerate solution, resulting in the model learning whole sentences as phrases rather than minimal units of translation. DeNero et al. (2008) recognised this problem and proposed a nonparametric Bayesian prior for contiguous phrases. This had the dual benefits of biasing the model towards learni"
D12-1021,P11-1064,0,0.396565,"2004; Chiang, 2007). However, while the models used for translation have evolved, the way in which they are learnt has not: na¨ıve word-based models are still used to infer translational correspondences from parallel corpora. Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules. As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments (Wu, 1997; Zhang et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). In contrast to these previous attempts, our SCFG model scales to large datasets (over 1.3M sentence pairs) without imposing restrictions on the form of the grammar rules or otherwise constraining the set of learnable rules (e.g., with a word alignment). We validate our sampler by demonstrating its ability to recover grammars used to generate synthetic datasets. We then evaluate our model by inducing word alignments for SMT experiments in several typologically diverse language pairs and across a range of corpora sizes. Our results attest to our model’s ability to learn synchronous grammars en"
D12-1021,J03-1002,0,0.0262314,"able 2: Corpora statistics (in words). translation evaluation.3 The ZH - EN data is of a medium scale and comes from the FBIS corpus. The DE - EN pair constitutes the largest corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the B LEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running G IZA ++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hundred million token subset 3 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ L ANGUAGE PAIR T EST S ET M ODEL 4 M ODEL 1 BASELINE I NITIALISATION W EAK M1 I NIT. S TRONG HMM I NIT. UR - EN MT 09 ZH - EN MT 03-08 E URO PARL 23.1 29.4 28.4 18.5 19.8 25.5 23.7 28.3 27.8 24.0 29.8 29.2 DE - EN PYP-SCFG Table 3: Results for the SMT experiments in B LEU"
D12-1021,2001.mtsummit-papers.68,0,0.0103018,"generate synthetic data, and the five most probable inferred rules by our model. T RAIN ( SRC ) T RAIN ( TRG ) D EV ( SRC ) D EV ( TRG ) ZH - EN UR - EN DE - EN NIST NIST E UROPARL 8.6M 9.5M 22K 27K 1.2M 1.0M 18K 16K 34M 36M 26K 28K Table 2: Corpora statistics (in words). translation evaluation.3 The ZH - EN data is of a medium scale and comes from the FBIS corpus. The DE - EN pair constitutes the largest corpus and is taken from Europarl, the proceedings of the European Parliament (Koehn, 2003). Statistics for the data are shown in Table 2. We measure translation quality via the B LEU score (Papineni et al., 2001). All translation systems employ a Hiero translation model during decoding. Baseline word alignments were obtained by running G IZA ++ in both directions and symmetrizing using the grow-diag-final-and heuristic (Och and Ney, 2003; Koehn et al., 2003). Decoding was performed with the cdec decoder (Dyer et al., 2010) with the synchronous grammar extracted using the techniques developed by Lopez (2008). All translation systems include a 5-gram language model built from a five hundred million token subset 3 http://www.itl.nist.gov/iad/mig/tests/ mt/2009/ L ANGUAGE PAIR T EST S ET M ODEL 4 M ODEL 1"
D12-1021,P06-1124,0,0.642911,"alisation. Our model goes further by allowing discontiguous phrasal translation units. Surprisingly, the freedom that this extra power affords allows the Gibbs sampler we propose to mix more quickly, allowing state-of-the-art results from a simple initialiser. 3 Model We use a nonparametric generative model based on the 2-parameter Pitman-Yor process (PYP) (Pitman and Yor, 1997), a generalisation of the Dirichlet Process, which has been used for various NLP modeling tasks with state-of-the-art results such as language modeling, word segmentation, text compression and part of speech induction (Teh, 2006; Goldwater et al., 2006; Wood et al., 2011; Blunsom and Cohn, 2011). In this section we first provide a brief definition of the SCFG formalism and then describe our PYP prior for them. 3.1 Synchronous Context-Free Grammar An synchronous context-free grammar (SCFG) is a 5-tuple hΣ, ∆, V, S, Ri that generalises context-free grammar to generate strings concurrently in two languages (Lewis and Stearns, 1968). Σ is a finite set of source language terminal symbols, ∆ is a finite set of target language terminal symbols, V is a set of nonterminal symbols, with a designated start symbol S, and R is a"
D12-1021,C96-2141,0,0.588149,"each of the language pairs and experimental conditions described below. We used the approach of Newman et al. (2007) to distribute the sampler across multiple threads. The strength θ and discount d hyperparameters of the Pitman-Yor Processes, and the terminal penalty φ (Section 3.3), were inferred using slice sampling (Neal, 2000). The Gibbs sampler requires an initial set of derivations from which to commence sampling. In our experiments we investigated both weak and a strong initialisations, the former based on word alignments from IBM Model 1 and the latter on alignments from an HMM model (Vogel et al., 1996). For decoding we used the word alignments implied by the derivations in the final sample to extract a Hiero grammar with the same standard set of relative frequency, length, and language model features used for the baseline. Weak Initialisation Our first translation experiments ascertain the degree to which our proposed Gibbs sampling inference algorithm is able to learn good synchronous derivations for the PYP-SCFG model. A number of prior works on alignment with Gibbs samplers have only evaluated models initialised with the more complex G IZA ++ alignment models (Blunsom et al., 2009; DeNer"
D12-1021,J97-3002,0,0.851103,"e-of-the-art in most language pairs (Galley et al., 2004; Chiang, 2007). However, while the models used for translation have evolved, the way in which they are learnt has not: na¨ıve word-based models are still used to infer translational correspondences from parallel corpora. Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules. As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments (Wu, 1997; Zhang et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). In contrast to these previous attempts, our SCFG model scales to large datasets (over 1.3M sentence pairs) without imposing restrictions on the form of the grammar rules or otherwise constraining the set of learnable rules (e.g., with a word alignment). We validate our sampler by demonstrating its ability to recover grammars used to generate synthetic datasets. We then evaluate our model by inducing word alignments for SMT experiments in several typologically diverse language pairs and across a range of corpora sizes. Our result"
D12-1021,P08-1012,0,0.693183,"rt in most language pairs (Galley et al., 2004; Chiang, 2007). However, while the models used for translation have evolved, the way in which they are learnt has not: na¨ıve word-based models are still used to infer translational correspondences from parallel corpora. Learning synchronous grammars is hard due to the high polynomial complexity of dynamic programming and the exponential space of possible rules. As such most prior work for learning SCFGs has relied on inference algorithms that were heuristically constrained or biased by word-based alignment models and small experiments (Wu, 1997; Zhang et al., 2008; Blunsom et al., 2009; Neubig et al., 2011). In contrast to these previous attempts, our SCFG model scales to large datasets (over 1.3M sentence pairs) without imposing restrictions on the form of the grammar rules or otherwise constraining the set of learnable rules (e.g., with a word alignment). We validate our sampler by demonstrating its ability to recover grammars used to generate synthetic datasets. We then evaluate our model by inducing word alignments for SMT experiments in several typologically diverse language pairs and across a range of corpora sizes. Our results attest to our mode"
D12-1021,P02-1040,0,\N,Missing
D13-1034,altantawy-etal-2010-morphological,0,0.0217975,"c morphology, such as Arabic, Hebrew and Amharic. These Semitic languages derive verb and noun stems by interspersing abstract root morphemes into templatic structures in a nonconcatenative way. For example, the Arabic root k·t·b can combine with the template (i-a) to derive the noun stem kitab (book). Established morphological analysers typically ignore this process and simply view the derived stems as elementary units (Buckwalter, 2002), or their account of it coincides with a requirement for extensive linguistic knowledge and hand-crafting of rules (Finkel and Stump, 2002; Schneider, 2010; Altantawy et al., 2010). The former approach is bound to suffer from vocabulary coverage issues, while the latter clearly does not transfer easily across languages. The practical appeal of unsupervised learning of templatic morphology is that it can overcome these shortcomings. Unsupervised learning of concatenative morphology has received extensive attention, partly driven by the MorphoChallenge (Kurimo et al., 2010) in recent years, but that is not the case for root-templatic morphology (Hammarstr¨om and Borin, 2011). In this paper we present a model-based method that learns concatenative and root-templatic morpho"
D13-1034,J06-1004,0,0.0253945,")+ (1) e.g. English un+accept+able Stem |Pre |Suf → Morph (2) Stem → intercal (Root, Template) (3) e.g. Arabic derivation k·t·b + i·a ⇒ kitab (book) Stem → infix (Stem, Infix) (4) e.g. Tagalog sulat (write) ⇒ sumulat (wrote) Stem → circfix (Stem, Circumfix) (5) e.g. Indonesian percaya (to trust) ⇒ kepercayaan (belief) A powerful grammar for morphology Concatenative morphology lends itself well to an analysis in terms of finite-state transducers (FSTs) (Koskenniemi, 1984). With some additional effort, FSTs can also encode non-concatenative morphology (Kiraz, 2000; Beesley and Karttunen, 2003; Cohen-Sygal and Wintner, 2006; Gasser, 2009). Despite this seeming adequacy of regular languages to describe morphology, we see two main shortcomings that motivate moving further up the Chomsky hierarchy of formal languages: first is the issue of learning. We are not aware of successful attempts at inducing FST-based morphological analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior linguistic intuitions to guide the learning process. Secondly, an FST composed of multiple machines might capture morphological processes well and excel at ana"
D13-1034,W02-0506,0,0.0426705,"ˇtax.at ˇ ˇ x ˇ!aˇl.a ˇ!aˇcˇlx an it ya.!al.u ma Table 4: Top Hebrew roots hypothesised by Tpl+T4. Numbers indicate position when ranked by model probability. (G)ood and (B)ad instances from the corpus are given with morpheme boundaries marked: true positive (.), false negative ( ) and false positive (x ). Hypothesised root characters are boldfaced, while accent (ˇ) marks gold root characters. Previous approaches to Arabic root identification that sought to use little supervision typically constrain the search space of candidate characters within a word, leveraging pre-existing dictionaries (Darwish, 2002; Boudlal et al., 2009) or rule ´ constraints (Elghamry, 2005; Rodrigues and Cavar, 2007; Daya et al., 2008). In contrast to these approaches, our model requires no dictionary, and while our grammar rules effect some constraints on what could be a root, they are specified in a convenient and flexible manner that Pre(l i) ... Stem(danotill) ... Suf(A) Pre(w l) ... Stem ... Suf(k m) Word Pre Stem Suf wl> stAr km T2 > · A X2 s t · r T1 T1 R3 s · t · r > A wl >stAr km X R2 s · t R1 R1 R1 r s Word Pre Stem lida notillA R4 d · n · t · l T4 o · a · i · l R3 d · n · t R1 T3 o · a · i T1 R2 d · n R1 R1"
D13-1034,J08-3005,0,0.140777,"bers indicate position when ranked by model probability. (G)ood and (B)ad instances from the corpus are given with morpheme boundaries marked: true positive (.), false negative ( ) and false positive (x ). Hypothesised root characters are boldfaced, while accent (ˇ) marks gold root characters. Previous approaches to Arabic root identification that sought to use little supervision typically constrain the search space of candidate characters within a word, leveraging pre-existing dictionaries (Darwish, 2002; Boudlal et al., 2009) or rule ´ constraints (Elghamry, 2005; Rodrigues and Cavar, 2007; Daya et al., 2008). In contrast to these approaches, our model requires no dictionary, and while our grammar rules effect some constraints on what could be a root, they are specified in a convenient and flexible manner that Pre(l i) ... Stem(danotill) ... Suf(A) Pre(w l) ... Stem ... Suf(k m) Word Pre Stem Suf wl> stAr km T2 > · A X2 s t · r T1 T1 R3 s · t · r > A wl >stAr km X R2 s · t R1 R1 R1 r s Word Pre Stem lida notillA R4 d · n · t · l T4 o · a · i · l R3 d · n · t R1 T3 o · a · i T1 R2 d · n R1 R1 R1 li danotillA X t d t l T2 o · a T1 T1 T1 o n l i a 0 (a) Concat & Tpl+T4, “wl>stArkm” (B W) (b) Concat &"
D13-1034,D11-1057,0,0.0288601,"a templatic bit-string denoting root and non-root characters (e.g. r-r-r-) along with a root morpheme (e.g. ktb) and a so-called residue morpheme (e.g. aaa). Their nonparametric Bayesian model induces lexicons of these entities and achieves very high performance on templates. The explicit formulation of templates alleviates the labelling ambiguity that hampered our evaluation (§6.4), but we believe their method of analysis can be simulated in our framework using the appropriate SRCG-rules. Learning root-templatic morphology is loosely related to morphological paradigm induction (Clark, 2001; Dreyer and Eisner, 2011; Durrett and DeNero, 2013). Our models do not represent templatic paradigms explicitly, but it is interesting to note that preliminary experiments with German indicate that our adaptor grammars pick up on the past participle forming circumfix in ab+ge+spiel+t (played back). 8 Conclusion and Outlook We presented a new approach to modelling nonconcatenative phenomena in morphology using sim354 ple range concatenating grammars and extended adaptor grammars to this formalism. Our experiments show that this richer model improves morphological segmentation and morpheme lexicon induction on differen"
D13-1034,dukes-habash-2010-morphological,0,0.0149835,"8428 18808 5231 24197 30891 12021 3164 4717 4707 1270 492 2.3 2.3 1.9 2.1 6.4 10.7 9.9 6.7 Table 1: Corpus statistics, including average number of morphemes (m/w) and characters (c/w) per word, and total surface-realised roots of length 3 or 4. Analyser (BAMA).7 This allowed control over the word shapes, which is important to focus the evaluation, while yielding reliable segmentation and root annotations. B W has no vocalisation; we denote the 0 corresponding vocalised dataset as B W . Quranic Arabic We extracted the roughly 18k word types from a morphologically analysed version of the Quran (Dukes and Habash, 2010). As an additional challenge, we left all given diacritics intact 0 for this dataset, Q U . Hebrew We leveraged the Hebrew CHILDES database as an annotated resource (Albert et al., 2013) and were able to extract 5k word types that feature at least one affix to use as dataset H EB. The corrected versions of words marked as non-standard child language were used, diacritics were dropped, and we conflated stressed and unstressed vowels to overcome inconsistencies in the source data. 6.2 Models We consider two classes of models. The first is the strictly context-free adaptor grammar for morphemes a"
D13-1034,N13-1138,0,0.0244087,"enoting root and non-root characters (e.g. r-r-r-) along with a root morpheme (e.g. ktb) and a so-called residue morpheme (e.g. aaa). Their nonparametric Bayesian model induces lexicons of these entities and achieves very high performance on templates. The explicit formulation of templates alleviates the labelling ambiguity that hampered our evaluation (§6.4), but we believe their method of analysis can be simulated in our framework using the appropriate SRCG-rules. Learning root-templatic morphology is loosely related to morphological paradigm induction (Clark, 2001; Dreyer and Eisner, 2011; Durrett and DeNero, 2013). Our models do not represent templatic paradigms explicitly, but it is interesting to note that preliminary experiments with German indicate that our adaptor grammars pick up on the past participle forming circumfix in ab+ge+spiel+t (played back). 8 Conclusion and Outlook We presented a new approach to modelling nonconcatenative phenomena in morphology using sim354 ple range concatenating grammars and extended adaptor grammars to this formalism. Our experiments show that this richer model improves morphological segmentation and morpheme lexicon induction on different languages in the Semitic"
D13-1034,W02-0502,0,0.0399464,"em comes from languages that use templatic morphology, such as Arabic, Hebrew and Amharic. These Semitic languages derive verb and noun stems by interspersing abstract root morphemes into templatic structures in a nonconcatenative way. For example, the Arabic root k·t·b can combine with the template (i-a) to derive the noun stem kitab (book). Established morphological analysers typically ignore this process and simply view the derived stems as elementary units (Buckwalter, 2002), or their account of it coincides with a requirement for extensive linguistic knowledge and hand-crafting of rules (Finkel and Stump, 2002; Schneider, 2010; Altantawy et al., 2010). The former approach is bound to suffer from vocabulary coverage issues, while the latter clearly does not transfer easily across languages. The practical appeal of unsupervised learning of templatic morphology is that it can overcome these shortcomings. Unsupervised learning of concatenative morphology has received extensive attention, partly driven by the MorphoChallenge (Kurimo et al., 2010) in recent years, but that is not the case for root-templatic morphology (Hammarstr¨om and Borin, 2011). In this paper we present a model-based method that lear"
D13-1034,W13-2603,0,0.082184,"Missing"
D13-1034,E09-1036,0,0.020168,"able Stem |Pre |Suf → Morph (2) Stem → intercal (Root, Template) (3) e.g. Arabic derivation k·t·b + i·a ⇒ kitab (book) Stem → infix (Stem, Infix) (4) e.g. Tagalog sulat (write) ⇒ sumulat (wrote) Stem → circfix (Stem, Circumfix) (5) e.g. Indonesian percaya (to trust) ⇒ kepercayaan (belief) A powerful grammar for morphology Concatenative morphology lends itself well to an analysis in terms of finite-state transducers (FSTs) (Koskenniemi, 1984). With some additional effort, FSTs can also encode non-concatenative morphology (Kiraz, 2000; Beesley and Karttunen, 2003; Cohen-Sygal and Wintner, 2006; Gasser, 2009). Despite this seeming adequacy of regular languages to describe morphology, we see two main shortcomings that motivate moving further up the Chomsky hierarchy of formal languages: first is the issue of learning. We are not aware of successful attempts at inducing FST-based morphological analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior linguistic intuitions to guide the learning process. Secondly, an FST composed of multiple machines might capture morphological processes well and excel at analysis, but inte"
D13-1034,N10-1118,0,0.013701,"a particular SRCG iff the start symbol S, instantiated with the exhaustive range (0, wn ), derives . An important distinction with regard to CFGs is that, due to the instantiation mechanism, the ordering of non-terminals on the right-hand side of an SRCG rule is irrelevant, i.e. A(ab) → B(a)C(b) and A(ab) → C(b)B(a) are the same rule.2 Consequently, the isomorphisms of any given SRCG derivation tree all encode the same string, which is uniquely defined through the instantiation process. Stm(kitab) The trade-off between arity and rank with respect to parsing complexity has been characterised (Gildea, 2010), and the appropriate refactoring may bring down the complexity for our grammars too. to set up our application of SRCGs in such a way that this is not too big an obstacle: Firstly, our grammars are defined over the characters that make up a word, and not over words that make up a sentence. As such, the input length n would tend to be shorter than when parsing full sentences from a corpus. Secondly, we do type-based morphological analysis, a view supported by evidence from Goldwater et al. (2006), so each unique word in a dataset is only ever parsed once with a given grammar. The set of word t"
D13-1034,P11-2094,0,0.024737,"tly, both of the other expansions in the tree under construction and of any other trees. To some extent, this flies in the face of the reality of estimating a grammar from text, where one would expect certain sub-trees to be used repeatedly across different input strings. Adaptor grammars weaken this independence assumption by allowing whole subtrees to be reused during expansion. Informally, they act as a cache of tree fragments whose tendency to be reused during expansion is governed by the choice of adaptor function. Following earlier applications of adaptor grammars (Johnson et al., 2007; Huang et al., 2011), we employ the Pitman-Yor process (Pitman, 1995; Pitman and Yor, 1997) as adaptor function. A Pitman-Yor Simple Range Concatenating Adaptor Grammar (PYSRCAG) is a tuple G = (GS , M, a, b, α), where GS is a probabilistic SRCG as defined before and M ⊆ N is a set of adapted non-terminals. The vectors a and b, indexed by the elements of M , are the discount and concentration parameters for each adapted nonterminal, with a ∈ [0, 1], b ≥ 0. α are parameters to Dirichlet priors on the rule probabilities θ. PYSRCAG defines a generative process over a set of trees T . Unadapted non-terminals A0 ∈ N "
D13-1034,N09-1036,0,0.0886779,"cceptreject probability. As proposal grammar, we use the analogous approximation of our G as Johnson et al. used for PCFGs, namely by taking a static snapshot GQ of the adaptor grammar where additional rules rewrite adapted non-terminals as the terminal strings of their cached trees. Drawing a sample from the proposal distribution is then a matter of drawing a random tree from the parse chart of w under GQ . Lastly, the adaptor hyperparameters a and b are modelled by placing flat Beta(1, 1) and vague Gamma(10, 0.1) priors on them, respectively, and inferring their values using slice sampling (Johnson and Goldwater, 2009). 5 Modelling root-templatic morphology We start with a CFG-based adaptor grammar4 that models words as a stem and any number of prefixes and suffixes: Word → Pre∗ Stem Suf∗ Pre |Stem |Suf → Char+ (8) (9) This fragment can be seen as building on the stemand-affix adaptor grammar presented in (Johnson et al., 2007) for morphological analysis of English, of which a later version also covers multiple affixes (Sirts and Goldwater, 2013). In the particular case of Arabic, multiple affixes are required to handle the attachment of particles and proclitics onto base words. To extend this to complex st"
D13-1034,W08-0704,0,0.0707735,"d morphological analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior linguistic intuitions to guide the learning process. Secondly, an FST composed of multiple machines might capture morphological processes well and excel at analysis, but interpretability of its internal operations are limited. These shortcomings are overcome for concatenative morphology by context-free adaptor grammars, which allowed diverse segmentation models to be formulated and investigated within a single framework (Johnson et al., 2007; Johnson, 2008; Sirts and Goldwater, 2013). In principle, that covers a wide range of phenomena (typical example language in parentheses): affixal inflection (Czech) and derivation (English), agglutinative derivation (Turkish, Finnish), compounding (German). Our agenda here is to extend that approach to include non-concatenative processes such as root-templatic 346 where the symbols (excluding Word and Stem) implicitly expand to the relevant terminal strings. The bold-faced “functions” combine the potentially discontiguous yields of the argument symbols into single contiguous strings, e.g. infix(s·ulat, um)"
D13-1034,W13-0808,0,0.0593322,"s should also be rel1 Our formulation is in terms of SRCGs, which are equivalent in power to linear context-free rewrite systems (VijayShanker et al., 1987) and multiple context-free grammars (Seki et al., 1991), all of which are weaker than (non-simple) range concatenating grammars (Boullier, 2000). 345 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 345–356, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics evant to other applications of probabilistic SRCGs, e.g. in parsing (Maier, 2010), translation (Kaeshammer, 2013) and genetics (Kato et al., 2006). In addition to unannotated data, our method requires as input a minimal set of high-level grammar rules that encode basic intuitions of the morphology. This is where there would be room to become very language specific. Our aim, however, is not to obtain a best-published result in a particular language, but rather to create a method that is applicable across a variety of morphological processes. The specific rules used in our empirical evaluation on Arabic and Hebrew therefore contain hardly any explicit linguistic knowledge about the languages and are applic"
D13-1034,W06-1508,0,0.102618,"ation is in terms of SRCGs, which are equivalent in power to linear context-free rewrite systems (VijayShanker et al., 1987) and multiple context-free grammars (Seki et al., 1991), all of which are weaker than (non-simple) range concatenating grammars (Boullier, 2000). 345 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 345–356, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics evant to other applications of probabilistic SRCGs, e.g. in parsing (Maier, 2010), translation (Kaeshammer, 2013) and genetics (Kato et al., 2006). In addition to unannotated data, our method requires as input a minimal set of high-level grammar rules that encode basic intuitions of the morphology. This is where there would be room to become very language specific. Our aim, however, is not to obtain a best-published result in a particular language, but rather to create a method that is applicable across a variety of morphological processes. The specific rules used in our empirical evaluation on Arabic and Hebrew therefore contain hardly any explicit linguistic knowledge about the languages and are applicable across the family of Semitic"
D13-1034,J00-1006,0,0.0644282,"uld look like this: Word → (Pre∗ Stem Suf∗ )+ (1) e.g. English un+accept+able Stem |Pre |Suf → Morph (2) Stem → intercal (Root, Template) (3) e.g. Arabic derivation k·t·b + i·a ⇒ kitab (book) Stem → infix (Stem, Infix) (4) e.g. Tagalog sulat (write) ⇒ sumulat (wrote) Stem → circfix (Stem, Circumfix) (5) e.g. Indonesian percaya (to trust) ⇒ kepercayaan (belief) A powerful grammar for morphology Concatenative morphology lends itself well to an analysis in terms of finite-state transducers (FSTs) (Koskenniemi, 1984). With some additional effort, FSTs can also encode non-concatenative morphology (Kiraz, 2000; Beesley and Karttunen, 2003; Cohen-Sygal and Wintner, 2006; Gasser, 2009). Despite this seeming adequacy of regular languages to describe morphology, we see two main shortcomings that motivate moving further up the Chomsky hierarchy of formal languages: first is the issue of learning. We are not aware of successful attempts at inducing FST-based morphological analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior linguistic intuitions to guide the learning process. Secondly, an FST composed of multiple machines"
D13-1034,P84-1038,0,0.698158,"ree grammars. An idealised generative grammar that would capture all the aforementioned phenomena could look like this: Word → (Pre∗ Stem Suf∗ )+ (1) e.g. English un+accept+able Stem |Pre |Suf → Morph (2) Stem → intercal (Root, Template) (3) e.g. Arabic derivation k·t·b + i·a ⇒ kitab (book) Stem → infix (Stem, Infix) (4) e.g. Tagalog sulat (write) ⇒ sumulat (wrote) Stem → circfix (Stem, Circumfix) (5) e.g. Indonesian percaya (to trust) ⇒ kepercayaan (belief) A powerful grammar for morphology Concatenative morphology lends itself well to an analysis in terms of finite-state transducers (FSTs) (Koskenniemi, 1984). With some additional effort, FSTs can also encode non-concatenative morphology (Kiraz, 2000; Beesley and Karttunen, 2003; Cohen-Sygal and Wintner, 2006; Gasser, 2009). Despite this seeming adequacy of regular languages to describe morphology, we see two main shortcomings that motivate moving further up the Chomsky hierarchy of formal languages: first is the issue of learning. We are not aware of successful attempts at inducing FST-based morphological analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior lingui"
D13-1034,W11-0301,0,0.0259656,"utive affixes and 0 stems, whereas all our experiments (except on Q U ) presupposed single affixes. This biases the evaluation slightly in our favour, but works in Morfessor’s 0 favour on the Q U data which is annotated with multiple affixes. 7 Related work The distinctive feature of our morphological model is that it jointly addresses root identification and morpheme segmentation, and our results demonstrate the mutual benefit of this. In contrast, earlier unsupervised approaches tend to focus on these tasks in isolation. In unsupervised Arabic segmentation, the parametric Bayesian model of (Lee et al., 2011) achieves F1-scores in the high eighties by incorporating sentential context and inferred syntactic categories, both of which our model forgoes, although theirs has no account of discontiguous root morphemes. 353 Root 1. spr X 2. lbs X 3. ptx X 5. !al × Example instances G B G B G B B ˇ paˇ ˇ r.ti te.sa ˇ pˇ ˇ r ye.sa ˇ pˇ ˇ r.u sa ˇ pˇ x aˇr t sipur.im hix sta ˇ ˇ ˇlaba ˇ S.t ˇ Sˇ ti.ˇlbe ˇ S.i li.ˇlbo ˇ ˇ Sˇ ti txˇlabˇ S.i le haxˇlbi ˇ ˇpaˇtax.ti ˇ ˇ ti.ptex.i ˇ li.pˇ ˇtoaxˇ nix pˇ ˇtax.at ˇ ˇ x ˇ!aˇl.a ˇ!aˇcˇlx an it ya.!al.u ma Table 4: Top Hebrew roots hypothesised by Tpl+T4. Numbers indi"
D13-1034,W10-1407,0,0.0704632,"t al., 2007) to SRCGs.1 This should also be rel1 Our formulation is in terms of SRCGs, which are equivalent in power to linear context-free rewrite systems (VijayShanker et al., 1987) and multiple context-free grammars (Seki et al., 1991), all of which are weaker than (non-simple) range concatenating grammars (Boullier, 2000). 345 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 345–356, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics evant to other applications of probabilistic SRCGs, e.g. in parsing (Maier, 2010), translation (Kaeshammer, 2013) and genetics (Kato et al., 2006). In addition to unannotated data, our method requires as input a minimal set of high-level grammar rules that encode basic intuitions of the morphology. This is where there would be room to become very language specific. Our aim, however, is not to obtain a best-published result in a particular language, but rather to create a method that is applicable across a variety of morphological processes. The specific rules used in our empirical evaluation on Arabic and Hebrew therefore contain hardly any explicit linguistic knowledge ab"
D13-1034,Q13-1021,0,0.357013,"analysers in an unsupervised way, and believe the challenge lies in the fact that FSTs do not offer a convenient way of expressing prior linguistic intuitions to guide the learning process. Secondly, an FST composed of multiple machines might capture morphological processes well and excel at analysis, but interpretability of its internal operations are limited. These shortcomings are overcome for concatenative morphology by context-free adaptor grammars, which allowed diverse segmentation models to be formulated and investigated within a single framework (Johnson et al., 2007; Johnson, 2008; Sirts and Goldwater, 2013). In principle, that covers a wide range of phenomena (typical example language in parentheses): affixal inflection (Czech) and derivation (English), agglutinative derivation (Turkish, Finnish), compounding (German). Our agenda here is to extend that approach to include non-concatenative processes such as root-templatic 346 where the symbols (excluding Word and Stem) implicitly expand to the relevant terminal strings. The bold-faced “functions” combine the potentially discontiguous yields of the argument symbols into single contiguous strings, e.g. infix(s·ulat, um) produces stem sumulat. Take"
D13-1034,P87-1015,0,0.694185,"Missing"
D13-1034,W83-0114,0,\N,Missing
D13-1034,J11-2002,0,\N,Missing
D13-1176,J93-2003,0,0.116942,"Missing"
D13-1176,P10-4002,1,0.100195,"Missing"
D13-1176,N13-1073,0,0.718218,"Missing"
D13-1176,W11-0114,0,0.0208062,"so shown a marked sensitivity to conditioning information (Mikolov and Zweig, 2012). Continuous representations for characters have been deployed in character-level language models demonstrating notable language generation capabilities (Sutskever et al., 2011). Continuous representations have also been constructed for phrases and sentences. The representations are able to carry similarity and task dependent information, e.g. sentiment, paraphrase or dialogue labels, significantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013). Phrase-based continuous translation models were first proposed in (Schwenk et al., 2006) and re1700 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics cently further developed in (Schwenk, 2012; Le et al., 2012). The models incorporate a principled way of estimating translation probabilities that robustly extends to rare and unseen phrases. They"
D13-1176,P13-1088,1,0.0535229,"d Zweig, 2012). Continuous representations for characters have been deployed in character-level language models demonstrating notable language generation capabilities (Sutskever et al., 2011). Continuous representations have also been constructed for phrases and sentences. The representations are able to carry similarity and task dependent information, e.g. sentiment, paraphrase or dialogue labels, significantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013). Phrase-based continuous translation models were first proposed in (Schwenk et al., 2006) and re1700 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics cently further developed in (Schwenk, 2012; Le et al., 2012). The models incorporate a principled way of estimating translation probabilities that robustly extends to rare and unseen phrases. They achieve significant Bleu score improvements and yield semantically m"
D13-1176,W13-3214,1,0.669529,"representations for characters have been deployed in character-level language models demonstrating notable language generation capabilities (Sutskever et al., 2011). Continuous representations have also been constructed for phrases and sentences. The representations are able to carry similarity and task dependent information, e.g. sentiment, paraphrase or dialogue labels, significantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013). Phrase-based continuous translation models were first proposed in (Schwenk et al., 2006) and re1700 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics cently further developed in (Schwenk, 2012; Le et al., 2012). The models incorporate a principled way of estimating translation probabilities that robustly extends to rare and unseen phrases. They achieve significant Bleu score improvements and yield semantically more suggestive translations. Alth"
D13-1176,N12-1005,0,0.0450134,"gnificantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013). Phrase-based continuous translation models were first proposed in (Schwenk et al., 2006) and re1700 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics cently further developed in (Schwenk, 2012; Le et al., 2012). The models incorporate a principled way of estimating translation probabilities that robustly extends to rare and unseen phrases. They achieve significant Bleu score improvements and yield semantically more suggestive translations. Although wide-reaching in their scope, these models are limited to fixed-size source and target phrases and simplify the dependencies between the target words taking into account restricted target language modelling information. We describe a class of continuous translation models called Recurrent Continuous Translation Models (RCTM) that map without loss of gener"
D13-1176,P06-2093,0,0.011469,"Missing"
D13-1176,C12-2104,0,0.0254665,"ogue labels, significantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013). Phrase-based continuous translation models were first proposed in (Schwenk et al., 2006) and re1700 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics cently further developed in (Schwenk, 2012; Le et al., 2012). The models incorporate a principled way of estimating translation probabilities that robustly extends to rare and unseen phrases. They achieve significant Bleu score improvements and yield semantically more suggestive translations. Although wide-reaching in their scope, these models are limited to fixed-size source and target phrases and simplify the dependencies between the target words taking into account restricted target language modelling information. We describe a class of continuous translation models called Recurrent Continuous Translation Models (RCTM) that map wit"
D13-1176,D12-1110,0,0.679687,"formation (Mikolov and Zweig, 2012). Continuous representations for characters have been deployed in character-level language models demonstrating notable language generation capabilities (Sutskever et al., 2011). Continuous representations have also been constructed for phrases and sentences. The representations are able to carry similarity and task dependent information, e.g. sentiment, paraphrase or dialogue labels, significantly beyond the word level and to accurately predict labels for a highly diverse range of unseen phrases and sentences (Grefenstette et al., 2011; Socher et al., 2011; Socher et al., 2012; Hermann and Blunsom, 2013; Kalchbrenner and Blunsom, 2013). Phrase-based continuous translation models were first proposed in (Schwenk et al., 2006) and re1700 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics cently further developed in (Schwenk, 2012; Le et al., 2012). The models incorporate a principled way of estimating translation probabilities that robustly extends to rare and unseen phrases. They achieve significant Bleu score improvemen"
D15-1307,P06-4018,0,0.016593,"be secure even when the attacker (us) knows how the system works, introduces an interesting opportunity for linguistic steganalysis. As all linguistic stegosystems rely on automatic paraphrasing, and generally require a source of data, it is conservative to assume that the attacker has access to this data source. As the attacker, we can derive features from this data. Applying this, we extracted features using the same subset of the PPDB that CoverTweet uses. To our knowledge, this is the first work to explicitly apply Kerckhoffs’ principle to linguistic 3 We use the Natural Language Toolkit (Bird, 2006) for the list of 127 stop words. 2565 steganalysis. This set includes: a repeat of the n-gram features, but only taking into account n-grams that contain at least one word in4 the PPDB; the mean and variance of character count for words in the PPDB; for phrase lengths j from 1 to 5, the proportion of j-grams in the tweet that are in the PPDB; likelihood of the most probable paraphrased sentence, extracted by generating sentences in the same way as CoverTweet. Finally, a substitution score: each word or phrase in the PPDB is replaced by its most likely substitute, according to the language mode"
D15-1307,N10-1084,0,0.207556,"hich the cover is a piece of text, and the message is embedded using textual transformations intended to preserve the meaning of the original: synonym substitutions, syntactical transformations, etc. Note that we are not concerned 1 There are other steganographic paradigms, not in scope. Translation based methods hide information in the automatic translation of the cover (e.g. Meng et al. (2011)). Cover generation methods automatically produce text containing the payload (e.g. Chapman et al. (2001)). The field suffers from a number of issues: compared to images, text covers have low capacity (Chang and Clark, 2010); certain methods are weak against human attackers (Grosvald and Orgun, 2011) (most paraphrase systems cannot guarantee perfectly fluent stego objects); finally, authors are generally concerned with the performance of the transformation (whether they produce grammatically/semantically correct transformations), rather than whether the generated stego objects are detectable or not (e.g. Chang and Clark (2010)). However, there is a new challenger in the field. We proposed a new linguistic stegosystem (Wilson et al., 2014) and verified its security against human judges, who were unable to distingu"
D15-1307,C12-1031,0,0.350123,"substitutions. Even with the reduced set of rules, 4 bits can be embedded per tweet, and it was proven secure against human judges (Wilson et al., 2014). Twitter is a realistic setting for steganography. There is precedent for information hiding and mass monitoring on micro-blogging sites, such as the use of code words and government censorship on the Chinese website Sina Weibo (Chen et al., 2013). For this reason, we are attacking this setting. There are many other linguistic stegosystems, using an array of different hiding methods (e.g. adjective deletion, word order, anaphora resolution: (Chang and Clark, 2012a), (Chang and Clark, 2012b), (Vybornova and Macq, 2007)). Approximately 1 bit of payload per cover sentence is usual, making CoverTweet the exception. Unfortunately for steganalysis literature, the vast majority of these require data that is unavailable (and too expensive to reproduce); beyond CoverTweet, the only system that can be evaluated is T-Lex. 3 Related Work To our knowledge, there have been only five prior attempts at linguistic steganalysis on cover modification based systems; of these, four attack TLex, the other attacks an equivalent proprietary system. Taskiran et al. (2006) was"
D15-1307,C12-1032,0,0.0232691,"substitutions. Even with the reduced set of rules, 4 bits can be embedded per tweet, and it was proven secure against human judges (Wilson et al., 2014). Twitter is a realistic setting for steganography. There is precedent for information hiding and mass monitoring on micro-blogging sites, such as the use of code words and government censorship on the Chinese website Sina Weibo (Chen et al., 2013). For this reason, we are attacking this setting. There are many other linguistic stegosystems, using an array of different hiding methods (e.g. adjective deletion, word order, anaphora resolution: (Chang and Clark, 2012a), (Chang and Clark, 2012b), (Vybornova and Macq, 2007)). Approximately 1 bit of payload per cover sentence is usual, making CoverTweet the exception. Unfortunately for steganalysis literature, the vast majority of these require data that is unavailable (and too expensive to reproduce); beyond CoverTweet, the only system that can be evaluated is T-Lex. 3 Related Work To our knowledge, there have been only five prior attempts at linguistic steganalysis on cover modification based systems; of these, four attack TLex, the other attacks an equivalent proprietary system. Taskiran et al. (2006) was"
D15-1307,N13-1092,0,0.0400787,"Missing"
D16-1031,N16-1024,0,0.0260895,"Missing"
D16-1031,P16-1154,0,0.0413445,"Missing"
D16-1031,P16-1014,0,0.0518192,"Missing"
D16-1031,D13-1176,1,0.701611,"Missing"
D16-1031,Q16-1017,0,0.0358211,"Missing"
D16-1031,K16-1028,0,0.147144,"Missing"
D16-1031,D15-1044,0,0.253026,"Missing"
D16-1031,D15-1042,0,\N,Missing
D16-1116,D15-1138,0,0.0747526,"Missing"
D16-1116,P13-2009,0,0.013306,"bes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup proposed here offers an alternative solution"
D16-1116,Q13-1005,0,0.117977,"rsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address dat"
D16-1116,D14-1134,0,0.0245851,"Missing"
D16-1116,P14-1133,0,0.0415207,"onment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup proposed here offers an alternative solution to this issue. Discrete autoencoders Very recently there has been some related work on discrete autoencoders for natural language processing (Suster et al., 2016; Marcheggiani and Titov, 2016, i.a.) This work presents a first appro"
D16-1116,P16-1004,0,0.63439,"e prediction of a query on the G EO corpus which is a frequently used benchmark for semantic parsing. The corpus contains 880 questions about US geography together with executable queries representing those questions. We follow the approach established by Zettlemoyer and Collins (2005) and split the corpus into 600 training and 280 test cases. Following common practice, we augment the dataset by referring to the database during training and test time. In particular, we use the database to identify and anonymise variables (cities, states, countries and rivers) following the method described in Dong and Lapata (2016). Most prior work on the G EO corpus relies on standard semantic parsing methods together with custom heuristics or pipelines for this corpus. The recent paper by Dong and Lapata (2016) is of note, as it uses a sequence-to-sequence model for training which is the unidirectional equivalent to S2S, and also to the decoder part of our S EQ 4 network. 3.2 Open Street Maps The second task we tackle with our model is the NL MAPS dataset by Haas and Riezler (2016). The dataset contains 1,500 training and 880 testing instances of natural language questions with corresponding machine readable queries o"
D16-1116,N04-1035,0,0.0239608,"n the higher percentages then too little unsupervised data to meaningfully improve the model. 6 Related Work Semantic parsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016"
D16-1116,N16-1088,0,0.19687,"etrisation trick of Kingma and Welling (2014). Rather than drawing a discrete symbol in Σx from a softmax, we draw a distribution over symbols from a logistic-normal distribution at each time step. These serve as continuous relaxations of discrete samples, providing a differentiable estimator of the expected reconstruction log likelihood. We demonstrate the effectiveness of our proposed model on three semantic parsing tasks: the G EO Q UERY benchmark (Zelle and Mooney, 1996; Wong and Mooney, 2006), the SAIL maze navigation task (MacMahon et al., 2006) and the Natural Language Querying corpus (Haas and Riezler, 2016) on OpenStreetMap. As part of our evaluation, we introduce simple mechanisms for generating large amounts of unsupervised training data for two of these tasks. In most settings, the semi-supervised model outperforms the supervised model, both when trained on additional generated data as well as on subsets of the existing data. 1078 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1078–1087, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Dataset Example G EO what are the high points of states surrounding mississip"
D16-1116,P16-1002,0,0.406241,"objects, etc). Paths within that maze are created by randomly sampling start and end positions. 4 Model Experiments We evaluate our model on the three tasks in multiple settings. First, we establish a supervised baseline to compare the S2S model with prior work. Next, we 5 Our randomly generated unsupervised datasets can be downloaded from http://deepmind.com/ publications 1082 Accuracy Zettlemoyer and Collins (2005) Zettlemoyer and Collins (2007) Liang et al. (2013) Kwiatkowski et al. (2011) Zhao and Huang (2014) Kwiatkowski et al. (2013) 79.3 86.1 87.9 88.6 88.9 89.0 Dong and Lapata (2016) Jia and Liang (2016)6 84.6 89.3 S2S S EQ 4 86.5 87.3 Table 2: Non-neural and neural model results on G EO Q UERY using the train/test split from (Zettlemoyer and Collins, 2005). train our S EQ 4 model in a semi-supervised setting on the entire dataset with the additional monomodal training data described in the previous section. Finally, we perform an “ablation” study where we discard some of the training data and compare S2S to S EQ 4. S2S is trained solely on the reduced data in a supervised manner, while S EQ 4 is once again trained semi-supervised on the same reduced data plus the machine readable part of the"
D16-1116,C12-1083,1,0.256175,"nsupervised data to meaningfully improve the model. 6 Related Work Semantic parsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free gram"
D16-1116,D12-1040,0,0.0355422,"Missing"
D16-1116,P13-1022,0,0.0405195,"Missing"
D16-1116,D11-1140,0,0.063695,"6 Related Work Semantic parsing The tasks in this paper all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), wh"
D16-1116,D13-1161,0,0.036316,"Missing"
D16-1116,P11-1060,0,0.0545052,"and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup proposed here offers an alternative solution to this issue. Discrete autoencoders Very recently there has been some related work on discrete autoencoders for natural language processing (Suster et al., 2016; Marcheggiani and Titov, 2016, i.a.) This work presents a first approach to using effectively discretised sequential information as the latent representation without resorting to dracon"
D16-1116,J13-2005,0,0.0543262,"Missing"
D16-1116,Q16-1017,0,0.0270309,"from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. The semi-supervised setup proposed here offers an alternative solution to this issue. Discrete autoencoders Very recently there has been some related work on discrete autoencoders for natural language processing (Suster et al., 2016; Marcheggiani and Titov, 2016, i.a.) This work presents a first approach to using effectively discretised sequential information as the latent representation without resorting to draconian assumptions (Ammar et al., 2014) to make marginalisation tractable. While our model is not exactly marginalisable either, the continuous relaxation makes training far more tractable. A related idea was recently presented in G¨ulc¸ehre et al. (2015), who use monolingual data to improve machine translation by fusing a sequence-to-sequence model and a language model. 7 Conclusion We described a method for augmenting a supervised sequence t"
D16-1116,Q14-1030,0,0.0168947,"all broadly belong to the domain of semantic parsing, which describes the process of mapping natural language to a formal representation of its meaning. This is extended in the SAIL navigation task, where the formal representation is a function of both the language instruction and a given environment. Semantic parsing is a well-studied problem with numerous approaches including inductive logic programming (Zelle and Mooney, 1996), stringto-tree (Galley et al., 2004) and string-to-graph (Jones et al., 2012) transducers, grammar induction (Kwiatkowski et al., 2011; Artzi and Zettlemoyer, 2013; Reddy et al., 2014) or machine translation (Wong and Mooney, 2006; Andreas et al., 2013). While a large number of relevant literature fo1085 cuses on defining the grammar of the logical forms (Zettlemoyer and Collins, 2005), other models learn purely from aligned pairs of text and logical form (Berant and Liang, 2014), or from more weakly supervised signals such as question-answer pairs together with a database (Liang et al., 2011). Recent work of Jia and Liang (2016) induces a synchronous context-free grammar and generates additional training examples (x, y), which is one way to address data scarcity issues. Th"
D16-1116,N06-1056,0,0.581285,"inalisation, we introduce a novel differentiable alternative for draws from a softmax which can be used with the reparametrisation trick of Kingma and Welling (2014). Rather than drawing a discrete symbol in Σx from a softmax, we draw a distribution over symbols from a logistic-normal distribution at each time step. These serve as continuous relaxations of discrete samples, providing a differentiable estimator of the expected reconstruction log likelihood. We demonstrate the effectiveness of our proposed model on three semantic parsing tasks: the G EO Q UERY benchmark (Zelle and Mooney, 1996; Wong and Mooney, 2006), the SAIL maze navigation task (MacMahon et al., 2006) and the Natural Language Querying corpus (Haas and Riezler, 2016) on OpenStreetMap. As part of our evaluation, we introduce simple mechanisms for generating large amounts of unsupervised training data for two of these tasks. In most settings, the semi-supervised model outperforms the supervised model, both when trained on additional generated data as well as on subsets of the existing data. 1078 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1078–1087, c Austin, Texas, November 1-5, 2016. 201"
D16-1116,D07-1071,0,0.82963,"Missing"
D16-1116,P15-1109,0,0.0223581,"tive model in which latent sentences generate the unpaired logical forms. We apply this method to a number of semantic parsing tasks focusing on domains with limited access to labelled training data and extend those datasets with synthetically generated logical forms. 1 Introduction Neural approaches, in particular attention-based sequence-to-sequence models, have shown great promise and obtained state-of-the-art performance for sequence transduction tasks including machine translation (Bahdanau et al., 2015), syntactic constituency parsing (Vinyals et al., 2015), and semantic role labelling (Zhou and Xu, 2015). A key requirement for effectively training such models is an abundance of supervised data. In this paper we focus on learning mappings from input sequences x to output sequences y in domains where the latter are easily obtained, but annotation in the form of (x, y) pairs is sparse or expensive to produce, and propose a novel architecture that accommodates semi-supervised training on sequence transduction tasks. To this end, we augment the transduction objective (x 7→ y) with an autoencoding objective where the input sequence is treated as a latent variable (y 7→ x 7→ y), enabling training fr"
D16-1116,N16-1160,0,\N,Missing
D16-1138,N16-1012,0,0.0740074,"ctive sentence summarisation, summaries are generated from the given vocabulary without the constraint of copying words in the input sentence. Rush et al. (2015) compiled a data set for this task from the annotated Gigaword data set (Graff et al., 2003; Napoles et al., 2012), where sentence-summary pairs are obtained by pairing the headline of each article with its first sentence. Rush et al. (2015) use the splits of 3.8m/190k/381k for training, validation and testing. In previous work on this dataset, Rush et al. (2015) proposed an attention-based model with feed-forward neural networks, and Chopra et al. (2016) proposed an attention-based recurrent encoder-decoder, similar to one of our baselines. Due to computational constraints we place the following restrictions on the training and validation set: 1. The maximum lengths for the input sentences Model ROUGE-1 ROUGE-2 ROUGE-L Seq2seq Attention 25.16 29.25 9.09 12.85 23.06 27.32 uniSSNT biSSNT uniSSNT+ biSSNT+ 26.96 27.05 30.15 30.27 10.54 10.62 13.59 13.68 24.59 24.64 27.88 27.91 Table 1: ROUGE F1 scores on the sentence summarisation test set. Seq2seq refers to the vanilla encoder-decoder and attention denotes the attentionbased model. SSNT denotes"
D16-1138,N13-1138,0,0.0540562,"m usually includes concatenating it with a prefix or a suffix and substituting some characters. For example, the inflected form of a German stem abgang is abg¨angen when the case is dative and the number is plural. In our experiments, we use the same dataset as Model Avg. accuracy Seq2Seq Seq2Seq w/ Attention Adapted-seq2seq (FTND16) 79.08 95.64 96.20 uniSSNT+ biSSNT+ 87.85 95.32 Table 3: Average accuracy over all the morphological inflection datasets. The baseline results for Seq2Seq variants are taken from (Faruqui et al., 2016). Faruqui et al. (2016). This dataset was originally created by Durrett and DeNero (2013) from Wiktionary, containing inflections for German nouns (de-N), German verbs (de-V), Spanish verbs (esV), Finnish noun and adjective (fi-NA), and Finnish verbs (fi-V). It was further expanded by Nicolai et al. (2015) by adding Dutch verbs (nl-V) and French verbs (fr-V). The number of inflection types for each language ranges from 8 to 57. The number of base forms, i.e. the number of instances in each dataset, ranges from 2000 to 11200. The predefined split is 200/200 for dev and test sets, and the rest of the data for training. Our model is trained separately for each type of inflection, the"
D16-1138,N16-1077,0,0.132405,"on a morphological attribute. The transformation from a base form to an inflected form usually includes concatenating it with a prefix or a suffix and substituting some characters. For example, the inflected form of a German stem abgang is abg¨angen when the case is dative and the number is plural. In our experiments, we use the same dataset as Model Avg. accuracy Seq2Seq Seq2Seq w/ Attention Adapted-seq2seq (FTND16) 79.08 95.64 96.20 uniSSNT+ biSSNT+ 87.85 95.32 Table 3: Average accuracy over all the morphological inflection datasets. The baseline results for Seq2Seq variants are taken from (Faruqui et al., 2016). Faruqui et al. (2016). This dataset was originally created by Durrett and DeNero (2013) from Wiktionary, containing inflections for German nouns (de-N), German verbs (de-V), Spanish verbs (esV), Finnish noun and adjective (fi-NA), and Finnish verbs (fi-V). It was further expanded by Nicolai et al. (2015) by adding Dutch verbs (nl-V) and French verbs (fr-V). The number of inflection types for each language ranges from 8 to 57. The number of base forms, i.e. the number of instances in each dataset, ranges from 2000 to 11200. The predefined split is 200/200 for dev and test sets, and the rest o"
D16-1138,P16-1014,0,0.0241573,"Missing"
D16-1138,D13-1176,1,0.125852,"Missing"
D16-1138,D15-1166,0,0.0830282,"loyed during decoding. In contrast our model defines a separate latent alignment variable, which adds flexibility to the way the alignment distribution can be defined (as a geometric distribution or parameterised by a neural network) and how the alignments can be constrained, without redefining the dynamic program. In addition to marginalizing during training, our decoding algorithm also makes use of dynamic programming, allowing us to use either no beam or small beam sizes. Our work is also related to the attentionbased models first introduced for machine translation (Bahdanau et al., 2015). Luong et al. (2015) proposed two alternative attention mechanisms: a global method that attends all words in the input sentence, and a local one that points to parts of the input words. Another variation on this theme are pointer networks (Vinyals et al., 2015), where the outputs are pointers to elements of the variable-length input, predicted by the attention distribution. Jaitly et al. (2016) propose an online sequence to sequence model with attention that conditions on fixed-sized blocks of the input sequence and emits output tokens corresponding to each block. The model is trained with alignment information"
D16-1138,W12-3018,0,0.0378777,"Missing"
D16-1138,N15-1093,0,0.0362595,"experiments, we use the same dataset as Model Avg. accuracy Seq2Seq Seq2Seq w/ Attention Adapted-seq2seq (FTND16) 79.08 95.64 96.20 uniSSNT+ biSSNT+ 87.85 95.32 Table 3: Average accuracy over all the morphological inflection datasets. The baseline results for Seq2Seq variants are taken from (Faruqui et al., 2016). Faruqui et al. (2016). This dataset was originally created by Durrett and DeNero (2013) from Wiktionary, containing inflections for German nouns (de-N), German verbs (de-V), Spanish verbs (esV), Finnish noun and adjective (fi-NA), and Finnish verbs (fi-V). It was further expanded by Nicolai et al. (2015) by adding Dutch verbs (nl-V) and French verbs (fr-V). The number of inflection types for each language ranges from 8 to 57. The number of base forms, i.e. the number of instances in each dataset, ranges from 2000 to 11200. The predefined split is 200/200 for dev and test sets, and the rest of the data for training. Our model is trained separately for each type of inflection, the same setting as the factored model described in Faruqui et al. (2016). The model is trained to predict the character sequence of the inflected form given that of the stem. The output is evaluated by accuracies of stri"
D16-1138,N16-1076,0,0.0347547,"ch 1314 iteration, our model is trained with direct gradient descent, marginalizing over all the alignments. Latent variables have been employed in neural network-based models for sequence labelling tasks in the past. Examples include connectionist temporal classification (CTC) (Graves et al., 2006) for speech recognition and the more recent segmental recurrent neural networks (SRNNs) (Kong et al., 2016), with applications on handwriting recognition and part-of-speech tagging. Weighted finitestate transducers (WFSTs) have also been augmented to encode input sequences with bidirectional LSTMs (Rastogi et al., 2016), permitting exact inference over all possible output strings. While these models have been shown to achieve appealing performance on different applications, they have common limitations in terms of modelling dependencies between labels. It is not possible for CTCs to model explicit dependencies. SRNNs and neural WFSTs model fixed-length dependencies, making it is difficult to carry out effective inference as the dependencies become longer. Our model shares the property of the sequence transduction model of Graves (2012) in being able to model unbounded dependencies between output tokens via a"
D16-1138,D15-1044,0,0.081984,"ure is able to outperform the baseline encoder-decoder model by overcoming its encoding bottleneck. We further benchmark our results against an attention model in order to determine whether our alternative alignment strategy is able to provide similar benefits while processing the input online. 4.1 Abstractive Sentence Summarisation Sentence summarisation is the task of generating a condensed version of a sentence while preserving its meaning. In abstractive sentence summarisation, summaries are generated from the given vocabulary without the constraint of copying words in the input sentence. Rush et al. (2015) compiled a data set for this task from the annotated Gigaword data set (Graff et al., 2003; Napoles et al., 2012), where sentence-summary pairs are obtained by pairing the headline of each article with its first sentence. Rush et al. (2015) use the splits of 3.8m/190k/381k for training, validation and testing. In previous work on this dataset, Rush et al. (2015) proposed an attention-based model with feed-forward neural networks, and Chopra et al. (2016) proposed an attention-based recurrent encoder-decoder, similar to one of our baselines. Due to computational constraints we place the follow"
D16-1138,P97-1037,0,0.467908,"ications include machine translation and abstractive sentence summarisation. Traditionally this type of problem has been tackled by a combination of hand-crafted features, alignment models, segmentation heuristics, and language models, all of which are tuned separately. In this paper we propose an architecture to tackle the limitations of the vanilla encoder-decoder model, a segment to segment neural transduction model (SSNT) that learns to generate and align simultaneously. Our model is inspired by the HMM word alignment model proposed for statistical machine translation (Vogel et al., 1996; Tillmann et al., 1997); we impose a monotone restriction on the alignments but incorporate recurrent dependencies on the input which enable rich locally non-monotone alignments to be captured. This is similar to the sequence transduction model of Graves (2012), but we propose alignment distributions which are parameterised separately, making the model more flexible 1307 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1307–1316, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics and allowing online inference. Our model introduces a latent"
D16-1138,C96-2141,0,0.119079,"cessing. Common applications include machine translation and abstractive sentence summarisation. Traditionally this type of problem has been tackled by a combination of hand-crafted features, alignment models, segmentation heuristics, and language models, all of which are tuned separately. In this paper we propose an architecture to tackle the limitations of the vanilla encoder-decoder model, a segment to segment neural transduction model (SSNT) that learns to generate and align simultaneously. Our model is inspired by the HMM word alignment model proposed for statistical machine translation (Vogel et al., 1996; Tillmann et al., 1997); we impose a monotone restriction on the alignments but incorporate recurrent dependencies on the input which enable rich locally non-monotone alignments to be captured. This is similar to the sequence transduction model of Graves (2012), but we propose alignment distributions which are parameterised separately, making the model more flexible 1307 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1307–1316, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics and allowing online inference. Our m"
D17-1197,P16-1154,0,0.231329,"an environment. REs occur frequently and they play a key role in communicating information efficiently. While REs are common in natural language, most previous work does not model them explicitly, either treating REs as ordinary words in the model or replacing them with special tokens that are filled in with a post processing step (Wen et al., 2015; Luong et al., 2015). Here we propose a language modeling framework that explicitly incorporates reference decisions. In part, this is based on the principle of pointer networks in which copies are made from another source (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Ling et al., Work completed at DeepMind. Blend soy milk and … b) reference to a table Introduction ∗ 1) soy milk 2) leaves 3) banana 2016; Vinyals et al., 2015; Ahn et al., 2016; Merity et al., 2016). However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key characteristic of natural language reference. Figure 1 depicts examples of REs in the context of the three tasks that we consider in this work. First, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015). In the task of recipe gener"
D17-1197,P16-1014,0,0.0547892,"Missing"
D17-1197,N10-1061,0,0.034711,"orks on task oriented dialogues embed the seq2seq model in traditional dialogue systems, in which the table query part is not differentiable, while our model queries the database directly. Recipe generation was proposed in (Kiddon et al., 2016). They use attention mechanism over the checklists, whereas our work models explicit references to them. Context dependent language models (Mikolov et al., 2010; Jozefowicz et al., 2016; Mikolov et al., 2010; Ji et al., 2015; Wang and Cho, 2015) are proposed to capture long term dependency of text. There are also lots of works on coreference resolution (Haghighi and Klein, 2010; Wiseman et al., 2016). We are the first to combine coreference with language modeling, to the best of our knowledge. 5 Conclusion We introduce reference-aware language models which explicitly model the decision of from where to generate the token at each step. Our model can also learns the decision by treating it as a latent variable. We demonstrate on three applications, table based dialogue modeling, recipe generation and coref based LM, that our model performs better than attention based model, which does not incorporate this decision explicitly. There are several directions to explore fu"
D17-1197,D16-1032,0,0.126535,"made from another source (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Ling et al., Work completed at DeepMind. Blend soy milk and … b) reference to a table Introduction ∗ 1) soy milk 2) leaves 3) banana 2016; Vinyals et al., 2015; Ahn et al., 2016; Merity et al., 2016). However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key characteristic of natural language reference. Figure 1 depicts examples of REs in the context of the three tasks that we consider in this work. First, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015). In the task of recipe generation from a list of ingredients (Kiddon et al., 2016), the generation of the recipe will frequently refer to these items. As shown in Figure 1, in the recipe “Blend soy milk and . . . ”, soy milk refers to the ingredient summaries. Second, reference to a database is crucial in many applications. One example is in task oriented dialogue where access to a database is necessary to answer a user’s query (Young et al., 2013; Li et al., 2016; Vinyals and Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes and Weston, 2016; Wi"
D17-1197,D16-1127,0,0.0690347,"text of the three tasks that we consider in this work. First, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015). In the task of recipe generation from a list of ingredients (Kiddon et al., 2016), the generation of the recipe will frequently refer to these items. As shown in Figure 1, in the recipe “Blend soy milk and . . . ”, soy milk refers to the ingredient summaries. Second, reference to a database is crucial in many applications. One example is in task oriented dialogue where access to a database is necessary to answer a user’s query (Young et al., 2013; Li et al., 2016; Vinyals and Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes and Weston, 2016; Williams and Zweig, 2016; Shang et al., 2015; Wen et al., 2016). Here we consider the domain of restaurant recommendation where a system refers to restaurants (name) and their attributes (address, phone number etc) in its responses. When the system 1850 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1850–1859 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics says “the nirala is a nice restaurant”,"
D17-1197,P16-1057,1,0.749091,"Missing"
D17-1197,P15-1002,0,0.0118737,"think…Go ahead [Linda]2 … thanks goes to [you]1 … Figure 1: Reference-aware language models. Referring expressions (REs) in natural language are noun phrases (proper nouns, common nouns, and pronouns) that identify objects, entities, and events in an environment. REs occur frequently and they play a key role in communicating information efficiently. While REs are common in natural language, most previous work does not model them explicitly, either treating REs as ordinary words in the model or replacing them with special tokens that are filled in with a post processing step (Wen et al., 2015; Luong et al., 2015). Here we propose a language modeling framework that explicitly incorporates reference decisions. In part, this is based on the principle of pointer networks in which copies are made from another source (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Ling et al., Work completed at DeepMind. Blend soy milk and … b) reference to a table Introduction ∗ 1) soy milk 2) leaves 3) banana 2016; Vinyals et al., 2015; Ahn et al., 2016; Merity et al., 2016). However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key characteristic of na"
D17-1197,P15-1152,0,0.0333048,"Missing"
D17-1197,N15-1020,0,0.0273631,"rst, many models need to refer to a list of items (Kiddon et al., 2016; Wen et al., 2015). In the task of recipe generation from a list of ingredients (Kiddon et al., 2016), the generation of the recipe will frequently refer to these items. As shown in Figure 1, in the recipe “Blend soy milk and . . . ”, soy milk refers to the ingredient summaries. Second, reference to a database is crucial in many applications. One example is in task oriented dialogue where access to a database is necessary to answer a user’s query (Young et al., 2013; Li et al., 2016; Vinyals and Le, 2015; Wen et al., 2015; Sordoni et al., 2015; Serban et al., 2016; Bordes and Weston, 2016; Williams and Zweig, 2016; Shang et al., 2015; Wen et al., 2016). Here we consider the domain of restaurant recommendation where a system refers to restaurants (name) and their attributes (address, phone number etc) in its responses. When the system 1850 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1850–1859 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics says “the nirala is a nice restaurant”, it refers to the restaurant name the nirala from the database."
D17-1197,D15-1199,0,0.0573329,"ontext coref [I]1 think…Go ahead [Linda]2 … thanks goes to [you]1 … Figure 1: Reference-aware language models. Referring expressions (REs) in natural language are noun phrases (proper nouns, common nouns, and pronouns) that identify objects, entities, and events in an environment. REs occur frequently and they play a key role in communicating information efficiently. While REs are common in natural language, most previous work does not model them explicitly, either treating REs as ordinary words in the model or replacing them with special tokens that are filled in with a post processing step (Wen et al., 2015; Luong et al., 2015). Here we propose a language modeling framework that explicitly incorporates reference decisions. In part, this is based on the principle of pointer networks in which copies are made from another source (G¨ulc¸ehre et al., 2016; Gu et al., 2016; Ling et al., Work completed at DeepMind. Blend soy milk and … b) reference to a table Introduction ∗ 1) soy milk 2) leaves 3) banana 2016; Vinyals et al., 2015; Ahn et al., 2016; Merity et al., 2016). However, in the full version of our model, we go beyond simple copying and enable coreferent mentions to have different forms, a key"
D17-1197,N16-1114,0,0.015614,"ogues embed the seq2seq model in traditional dialogue systems, in which the table query part is not differentiable, while our model queries the database directly. Recipe generation was proposed in (Kiddon et al., 2016). They use attention mechanism over the checklists, whereas our work models explicit references to them. Context dependent language models (Mikolov et al., 2010; Jozefowicz et al., 2016; Mikolov et al., 2010; Ji et al., 2015; Wang and Cho, 2015) are proposed to capture long term dependency of text. There are also lots of works on coreference resolution (Haghighi and Klein, 2010; Wiseman et al., 2016). We are the first to combine coreference with language modeling, to the best of our knowledge. 5 Conclusion We introduce reference-aware language models which explicitly model the decision of from where to generate the token at each step. Our model can also learns the decision by treating it as a latent variable. We demonstrate on three applications, table based dialogue modeling, recipe generation and coref based LM, that our model performs better than attention based model, which does not incorporate this decision explicitly. There are several directions to explore further based on our fram"
D19-1439,D13-1203,0,0.0289017,"xamples intentionally contain occupations with strong imbalance in the gender ratio. Participant can be replaced with the neutral “someone”, and three different pronouns (he/she/they) can be used. The aim of this dataset is to measure how the change of the pronoun gender affects the accuracy of the model. Our models mask the pronoun and are thus not affected by the pronoun gender. They exhibit no bias on this dataset by design. We mainly use this dataset to measure the accuracy of different models on the entire dataset. According to Rudinger et al. (2018), the best performance is exhibited by Durrett and Klein (2013) when used on the male subset of the dataset. We use this result as the baseline. W INO B IAS. Similarly to the W INO G ENDER dataset, W INO B IAS (Zhao et al., 2018) is a W SCinspired dataset that measures gender bias in the coreference resolution algorithms. Similarly to W INO G ENDER, it contains instances of occupations with high gender imbalance. It contains 3, 160 examples of Winograd Schemas, equally split into validation and test set. The test set examples are split into 2 types, where examples of type 1 are “harder” and should not be solvable using the analysis of co-occurrence, and e"
D19-1439,P19-1386,0,0.176683,"everal large and commonly used benchmarks for coreference resolution, such as (Pradhan et al., 2012; Sch¨afer et al., 2012; Ghaddar and Langlais, 2016). However, Webster et al. (2018) argue that a high performance on these datasets does not correlate with a high accuracy in practice, because examples where the answer cannot be deduced from the syntax (we refer to them as hard pronoun resolution) are underrepresented. Therefore, several hard pronoun resolution datasets have been introduced (Webster et al., 2018; Rahman and Ng, 2012; Rudinger et al., 2018; Davis et al., 2017; Zhao et al., 2018; Emami et al., 2019). However, they are all relatively small, often created only as a test set. Therefore, most of the pronoun resolution models that address hard pronoun resolution rely on little (Liu et al., 2019) or no training data, via unsupervised pre-training (Trinh and Le, 2018; Radford et al., 2019). Another approach involves using external knowledge bases (Emami et al., 2018; F¨ahndrich et al., 2018), however, the accuracy of these models still lags behind that of the aforementioned pre-trained models. A similar approach to ours for unsupervised data generation and language-model-based evaluation has be"
D19-1439,L16-1021,0,0.0367153,"Missing"
D19-1439,E12-3001,0,0.0328171,"challenging coreference resolution problems, where we match or outperform previous state-of-theart approaches on 6 out of 7 datasets, such as G AP, D PR, W NLI, P DP, W INO B IAS, and W INO G ENDER. We release our model to be used off-the-shelf for solving pronoun disambiguation. 1 Introduction Pronoun resolution, also called coreference or anaphora resolution, is a natural language processing (NLP) task, which aims to link the pronouns with their referents. This task is of crucial importance in various other NLP tasks, such as information extraction (Nakayama, 2019) and machine translation (Guillou, 2012). Due to its importance, pronoun resolution has seen a series of different approaches, such as rule-based systems (Lee et al., 2013) and end-to-end-trained neural models (Lee et al., 2017; Liu et al., 2019). However, the recently released dataset G AP (Webster et al., 2018) shows that most of these solutions perform worse than na¨ıve baselines when the answer cannot be deduced from the syntax. Addressing this drawback is difficult, partially due to the lack of largescale challenging datasets needed to train the datahungry neural models. As observed by Trinh and Le (2018), language models are a"
D19-1439,J13-4004,0,0.038776,"sets, such as G AP, D PR, W NLI, P DP, W INO B IAS, and W INO G ENDER. We release our model to be used off-the-shelf for solving pronoun disambiguation. 1 Introduction Pronoun resolution, also called coreference or anaphora resolution, is a natural language processing (NLP) task, which aims to link the pronouns with their referents. This task is of crucial importance in various other NLP tasks, such as information extraction (Nakayama, 2019) and machine translation (Guillou, 2012). Due to its importance, pronoun resolution has seen a series of different approaches, such as rule-based systems (Lee et al., 2013) and end-to-end-trained neural models (Lee et al., 2017; Liu et al., 2019). However, the recently released dataset G AP (Webster et al., 2018) shows that most of these solutions perform worse than na¨ıve baselines when the answer cannot be deduced from the syntax. Addressing this drawback is difficult, partially due to the lack of largescale challenging datasets needed to train the datahungry neural models. As observed by Trinh and Le (2018), language models are a natural approach to pronoun resolution, by selecting the replacement for a pronoun that forms the sentence with highest probability"
D19-1439,D17-1018,0,0.245604,"W INO G ENDER. We release our model to be used off-the-shelf for solving pronoun disambiguation. 1 Introduction Pronoun resolution, also called coreference or anaphora resolution, is a natural language processing (NLP) task, which aims to link the pronouns with their referents. This task is of crucial importance in various other NLP tasks, such as information extraction (Nakayama, 2019) and machine translation (Guillou, 2012). Due to its importance, pronoun resolution has seen a series of different approaches, such as rule-based systems (Lee et al., 2013) and end-to-end-trained neural models (Lee et al., 2017; Liu et al., 2019). However, the recently released dataset G AP (Webster et al., 2018) shows that most of these solutions perform worse than na¨ıve baselines when the answer cannot be deduced from the syntax. Addressing this drawback is difficult, partially due to the lack of largescale challenging datasets needed to train the datahungry neural models. As observed by Trinh and Le (2018), language models are a natural approach to pronoun resolution, by selecting the replacement for a pronoun that forms the sentence with highest probability. Additionally, language models have the advantage of b"
D19-1439,P19-1593,0,0.123337,"release our model to be used off-the-shelf for solving pronoun disambiguation. 1 Introduction Pronoun resolution, also called coreference or anaphora resolution, is a natural language processing (NLP) task, which aims to link the pronouns with their referents. This task is of crucial importance in various other NLP tasks, such as information extraction (Nakayama, 2019) and machine translation (Guillou, 2012). Due to its importance, pronoun resolution has seen a series of different approaches, such as rule-based systems (Lee et al., 2013) and end-to-end-trained neural models (Lee et al., 2017; Liu et al., 2019). However, the recently released dataset G AP (Webster et al., 2018) shows that most of these solutions perform worse than na¨ıve baselines when the answer cannot be deduced from the syntax. Addressing this drawback is difficult, partially due to the lack of largescale challenging datasets needed to train the datahungry neural models. As observed by Trinh and Le (2018), language models are a natural approach to pronoun resolution, by selecting the replacement for a pronoun that forms the sentence with highest probability. Additionally, language models have the advantage of being pre-trained on"
D19-1439,P14-5010,0,0.0041396,"not overlap with any other dataset. The currently best approach transforms examples back into the Winograd Schemas and solves them as a coreference problem (Kocijan et al., 2019). Following our previous work (Kocijan et al., 2019), we reverse the process of example generation in the same way. We automatically detect which part of the premise has been copied to construct the hypothesis. This locates the pronoun that has to be resolved, and the candidate in question. All other nouns in the premise are treated as alternative candidates. We find nouns in the premise with the Stanford POS tagger (Manning et al., 2014). W INO G ENDER. W INO G ENDER (Rudinger et al., 2018) is a dataset that follows the W SC format and is aimed to measure gender bias. One of the candidates is always an occupation, while the other is a participant, both selected to be gender neutral. Examples intentionally contain occupations with strong imbalance in the gender ratio. Participant can be replaced with the neutral “someone”, and three different pronouns (he/she/they) can be used. The aim of this dataset is to measure how the change of the pronoun gender affects the accuracy of the model. Our models mask the pronoun and are thus"
D19-1439,N15-1082,0,0.184773,"ing and improved hyperparameter search. Fine-tuning on other datasets is performed in the same way as training except for two differences. Firstly, in fine-tuning, the model is trained for 30 epochs due to the smaller size of datasets. Secondly, we do not sub-sample the training set for hyperparameter search. We validate the model 4306 after every epoch, retaining the model that performs best on the W IKI CREM validation set. 5 Evaluation Datasets We now introduce the 7 datasets that were used to evaluate the models. We decide not to use the C O NLL2012 and W INO C OREF (Pradhan et al., 2012; Peng et al., 2015) datasets, because they contain more general coreference examples than just pronouns. We did not evaluate on the K NOW R EF dataset (Emami et al., 2019), since it was not yet publicly available at the time of writing. G AP. G AP (Webster et al., 2018) is a collection of 4, 454 passages from Wikipedia containing ambiguous pronouns. It focuses on the resolution of personal pronouns referring to human names and has a 1 : 1 ratio between masculine and feminine pronouns. In addition to the overall performance on the dataset, each model is evaluated also on its performance on the masculine subset (F"
D19-1439,W12-4501,0,0.100202,"with further fine-tuning and improved hyperparameter search. Fine-tuning on other datasets is performed in the same way as training except for two differences. Firstly, in fine-tuning, the model is trained for 30 epochs due to the smaller size of datasets. Secondly, we do not sub-sample the training set for hyperparameter search. We validate the model 4306 after every epoch, retaining the model that performs best on the W IKI CREM validation set. 5 Evaluation Datasets We now introduce the 7 datasets that were used to evaluate the models. We decide not to use the C O NLL2012 and W INO C OREF (Pradhan et al., 2012; Peng et al., 2015) datasets, because they contain more general coreference examples than just pronouns. We did not evaluate on the K NOW R EF dataset (Emami et al., 2019), since it was not yet publicly available at the time of writing. G AP. G AP (Webster et al., 2018) is a collection of 4, 454 passages from Wikipedia containing ambiguous pronouns. It focuses on the resolution of personal pronouns referring to human names and has a 1 : 1 ratio between masculine and feminine pronouns. In addition to the overall performance on the dataset, each model is evaluated also on its performance on the"
D19-1439,D12-1071,0,0.654462,"l., 2018; F¨ahndrich et al., 2018), however, the accuracy of these models still lags behind that of the aforementioned pre-trained models. A similar approach to ours for unsupervised data generation and language-model-based evaluation has been recently presented in our previous work (Kocijan et al., 2019). We generated M ASKEDW IKI, a large unsupervised dataset created by searching for repeated occurrences of nouns. However, training on M ASKEDW IKI on its own is not always enough and sometimes makes a difference only in combination with additional training on the D PR dataset (called W SCR) (Rahman and Ng, 2012). In contrast, W IKI CREM brings a much more consistent improvement over a wider range of datasets, strongly improving models’ performance even when they are not finetuned on additional data. As opposed to our previous work (Kocijan et al., 2019), we evaluate models on a larger collection of test sets, showing the usefulness of W IKI CREM beyond the Winograd Schema Challenge. Moreover, a manual comparison of W IKI CREM and M ASKEDW IKI (Kocijan et al., 2019) shows a significant difference in the quality of the examples. We annotated 100 random examples from M ASKEDW IKI and W IKI CREM. In M AS"
D19-1439,C12-2103,0,0.0746449,"Missing"
D19-1439,Q18-1042,0,0.467885,"isambiguation. 1 Introduction Pronoun resolution, also called coreference or anaphora resolution, is a natural language processing (NLP) task, which aims to link the pronouns with their referents. This task is of crucial importance in various other NLP tasks, such as information extraction (Nakayama, 2019) and machine translation (Guillou, 2012). Due to its importance, pronoun resolution has seen a series of different approaches, such as rule-based systems (Lee et al., 2013) and end-to-end-trained neural models (Lee et al., 2017; Liu et al., 2019). However, the recently released dataset G AP (Webster et al., 2018) shows that most of these solutions perform worse than na¨ıve baselines when the answer cannot be deduced from the syntax. Addressing this drawback is difficult, partially due to the lack of largescale challenging datasets needed to train the datahungry neural models. As observed by Trinh and Le (2018), language models are a natural approach to pronoun resolution, by selecting the replacement for a pronoun that forms the sentence with highest probability. Additionally, language models have the advantage of being pre-trained on a large collection of unstructured text and then fine-tuned on a sp"
D19-1439,H89-1033,0,0.74666,"Missing"
D19-1439,N18-2003,0,0.08158,"cesses differ. In G AP, passages with pronouns are collected and the pronouns are manually annotated, while W IKI CREM is generated by masking names that appear in the text. Even if the same text is used, different masking process will result in different inputs and outputs, making the examples different under the transductive hypothesis. W IKI CREM statistics. We analyze our dataset for gender bias. We use the Gender guesser library4 to determine the gender of the candidates. To mimic the analysis of pronoun genders performed in the related works (Webster et al., 2018; Rudinger et al., 2018; Zhao et al., 2018), we observe the gender of the correct candidates only. There were 0.8M “male” or “mostly male” names and 0.42M “female” or “mostly female” names, the rest were classified as “unknown”. The ratio between female and male candidates is thus estimated around 0.53 in favour of male candidates. We will see that this gender imbalance does not have any negative impact on bias, as shown in Section 6.2. However, our unsupervised generating procedure sometimes yields examples where the correct answer cannot be deduced given the available information, we refer to these as unsolvable examples. To estimate"
D19-1439,P19-1478,1,\N,Missing
D19-1439,N19-1423,0,\N,Missing
E14-1013,D08-1036,0,0.0173331,"ver tags. A similar approach constrains inference to only explore tag assignments such that all tokens of the same word type are assigned the same tag. These constraints reduce tag assignment ambiguity while also providing a bias towards the natural sparsity of tag distributions in language (Clark, 2003). However they do not provide a model based solution to tag ambiguity. Recent work encodes similar sparsity information with non-parametric priors, relying on Bayesian inference to achieve strong results without any tag dictionaries or constraints (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Liang et al. (2010) propose a typebased approach to this Bayesian inference similar to Brown et al. (1992), suggesting that there are strong dependencies between tokens of the same word-type. Lee et al. (2010) demonstrate strong results with a similar model and the introduction of a one-tag-per-type constraint on inference. Blunsom and Cohn (2011) extend the Bayesian inference approach with a hierarchical nonparametric prior that expands the HMM context to trigrams. However, the hierarchical nonparametric model adds too many long-range dependencies for the type-based inference proposed earli"
E14-1013,N10-1083,0,0.054135,"Missing"
E14-1013,P07-1094,0,0.0333026,"without it on languages with flexible word ordering. These results promise further improvements with more advanced lexicon models. From the early work in the 1990’s, much of the focus on unsupervised PoS induction has been on hidden Markov Models (HMM) (Brown et al., 1992; Kupiec, 1992; Merialdo, 1993). The HMM has proven to be a powerful model of PoS tag assignment. Successful approaches generally build upon the HMM model by expanding its context and smoothing the sparse data. Constraints such as tag dictionaries simplify inference by restricting the number of tags to explore for each word (Goldwater and Griffiths, 2007). Ganchev et al. (2010) used posterior regularization to ensure that word types have a sparse posterior distribution over tags. A similar approach constrains inference to only explore tag assignments such that all tokens of the same word type are assigned the same tag. These constraints reduce tag assignment ambiguity while also providing a bias towards the natural sparsity of tag distributions in language (Clark, 2003). However they do not provide a model based solution to tag ambiguity. Recent work encodes similar sparsity information with non-parametric priors, relying on Bayesian inference"
E14-1013,P11-1087,1,0.166732,"xford United Kingdom United Kingdom Gregory.Dubbin@wolfson.ox.ac.uk Phil.Blunsom@cs.ox.ac.uk Abstract Johnson, 2008; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011), a common thread in many of these works is that models based on a Hidden Markov Model (HMM) graphical structure suffer from a tendency to assign too many different tags to the tokens of a given word type. Models which restrict word types to only occur with a single tag show a significant increase in performance, even though this restriction is clearly at odds with the gold standard labeling (Brown et al., 1992; Clark, 2003; Blunsom and Cohn, 2011). While the empirically observed expectation for the number of tags per word type is close to one, there are many exceptions, e.g. words that occur as both nouns and verbs (opening, increase, related etc.). Automatically inducing the syntactic partof-speech categories for words in text is a fundamental task in Computational Linguistics. While the performance of unsupervised tagging models has been slowly improving, current state-of-the-art systems make the obviously incorrect assumption that all tokens of a given word type must share a single part-of-speech tag. This one-tag-per-type heuristic"
E14-1013,J92-4003,0,0.764181,"versity of Oxford University of Oxford United Kingdom United Kingdom Gregory.Dubbin@wolfson.ox.ac.uk Phil.Blunsom@cs.ox.ac.uk Abstract Johnson, 2008; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011), a common thread in many of these works is that models based on a Hidden Markov Model (HMM) graphical structure suffer from a tendency to assign too many different tags to the tokens of a given word type. Models which restrict word types to only occur with a single tag show a significant increase in performance, even though this restriction is clearly at odds with the gold standard labeling (Brown et al., 1992; Clark, 2003; Blunsom and Cohn, 2011). While the empirically observed expectation for the number of tags per word type is close to one, there are many exceptions, e.g. words that occur as both nouns and verbs (opening, increase, related etc.). Automatically inducing the syntactic partof-speech categories for words in text is a fundamental task in Computational Linguistics. While the performance of unsupervised tagging models has been slowly improving, current state-of-the-art systems make the obviously incorrect assumption that all tokens of a given word type must share a single part-of-speec"
E14-1013,D07-1031,0,0.0236981,"distribution over tags. A similar approach constrains inference to only explore tag assignments such that all tokens of the same word type are assigned the same tag. These constraints reduce tag assignment ambiguity while also providing a bias towards the natural sparsity of tag distributions in language (Clark, 2003). However they do not provide a model based solution to tag ambiguity. Recent work encodes similar sparsity information with non-parametric priors, relying on Bayesian inference to achieve strong results without any tag dictionaries or constraints (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Liang et al. (2010) propose a typebased approach to this Bayesian inference similar to Brown et al. (1992), suggesting that there are strong dependencies between tokens of the same word-type. Lee et al. (2010) demonstrate strong results with a similar model and the introduction of a one-tag-per-type constraint on inference. Blunsom and Cohn (2011) extend the Bayesian inference approach with a hierarchical nonparametric prior that expands the HMM context to trigrams. However, the hierarchical nonparametric model adds too many long-range dependencies for the type-based"
E14-1013,W06-2920,0,0.0737356,"Missing"
E14-1013,D10-1056,0,0.0894994,"xtend the type based Sequential Monte Carlo (SMC) inference algorithm of Dubbin and Blunsom (2012) to incorporate our model of the lexicon, removing the need for the heuristic inference technique of Blunsom and Cohn (2011). Introduction Research on the unsupervised induction of partof-speech (PoS) tags has the potential to improve both our understanding of the plausibility of theories of first language acquisition, and Natural Language Processing applications such as Speech Recognition and Machine Translation. While there has been much prior work on this task (Brown et al., 1992; Clark, 2003; Christodoulopoulos et al., 2010; Toutanova and We start in Section 3 by introducing the original PYP-HMM model and our extended model of the lexicon. Section 4 introduces a Particle Gibbs sampler for this model, a basic SMC method that generates samples from the model’s posterior. We evaluate these algorithms in Section 5, analyzing their behavior in comparisons to previously proposed state-of-the-art approaches. 116 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116–125, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics"
E14-1013,D10-1083,0,0.0214636,"viding a bias towards the natural sparsity of tag distributions in language (Clark, 2003). However they do not provide a model based solution to tag ambiguity. Recent work encodes similar sparsity information with non-parametric priors, relying on Bayesian inference to achieve strong results without any tag dictionaries or constraints (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Liang et al. (2010) propose a typebased approach to this Bayesian inference similar to Brown et al. (1992), suggesting that there are strong dependencies between tokens of the same word-type. Lee et al. (2010) demonstrate strong results with a similar model and the introduction of a one-tag-per-type constraint on inference. Blunsom and Cohn (2011) extend the Bayesian inference approach with a hierarchical nonparametric prior that expands the HMM context to trigrams. However, the hierarchical nonparametric model adds too many long-range dependencies for the type-based inference proposed earlier. The model produces state-of-the art results with a one-tag-per-type constraint, but even with this constraint the tag assignments must be roughly inferred from an approximation of the expectations. Ambiguity"
E14-1013,D12-1127,0,0.0891922,"Missing"
E14-1013,E03-1009,0,0.288456,"iversity of Oxford United Kingdom United Kingdom Gregory.Dubbin@wolfson.ox.ac.uk Phil.Blunsom@cs.ox.ac.uk Abstract Johnson, 2008; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011), a common thread in many of these works is that models based on a Hidden Markov Model (HMM) graphical structure suffer from a tendency to assign too many different tags to the tokens of a given word type. Models which restrict word types to only occur with a single tag show a significant increase in performance, even though this restriction is clearly at odds with the gold standard labeling (Brown et al., 1992; Clark, 2003; Blunsom and Cohn, 2011). While the empirically observed expectation for the number of tags per word type is close to one, there are many exceptions, e.g. words that occur as both nouns and verbs (opening, increase, related etc.). Automatically inducing the syntactic partof-speech categories for words in text is a fundamental task in Computational Linguistics. While the performance of unsupervised tagging models has been slowly improving, current state-of-the-art systems make the obviously incorrect assumption that all tokens of a given word type must share a single part-of-speech tag. This o"
E14-1013,P11-1061,0,0.0979184,"previously proposed state-of-the-art approaches. 116 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116–125, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics 2 Background data. By including ambiguity classes in the model, this approach is able to infer ambiguity classes of unknown words. Many improvements in part-of-speech induction over the last few years have come from the use of semi-supervised approaches in the form of projecting PoS constraints across languages with parallel corpora (Das and Petrov, 2011) or extracting them from the wiktionary (Li et al., 2012). These semi-supervised methods ultimately rely on a strong unsupervised model of PoS as their base. Thus, further improvements in unsupervised models, especially in modelling tag constrains, should lead to improvements in semi-supervised part-ofspeech induction. We find that modelling the lexicon in part-ofspeech inference can lead to more efficient algorithms that match the state-of-the-art unsupervised performance. We also note that the lexicon model relies heavily on morphological information, and suffers without it on languages with"
E14-1013,N10-1082,0,0.0163882,"oach constrains inference to only explore tag assignments such that all tokens of the same word type are assigned the same tag. These constraints reduce tag assignment ambiguity while also providing a bias towards the natural sparsity of tag distributions in language (Clark, 2003). However they do not provide a model based solution to tag ambiguity. Recent work encodes similar sparsity information with non-parametric priors, relying on Bayesian inference to achieve strong results without any tag dictionaries or constraints (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Liang et al. (2010) propose a typebased approach to this Bayesian inference similar to Brown et al. (1992), suggesting that there are strong dependencies between tokens of the same word-type. Lee et al. (2010) demonstrate strong results with a similar model and the introduction of a one-tag-per-type constraint on inference. Blunsom and Cohn (2011) extend the Bayesian inference approach with a hierarchical nonparametric prior that expands the HMM context to trigrams. However, the hierarchical nonparametric model adds too many long-range dependencies for the type-based inference proposed earlier. The model produce"
E14-1013,J93-2004,0,0.0471095,"Missing"
E14-1013,W12-1907,1,0.892432,"ompetitive with and faster than the state-of-the-art without making any unrealistic restrictions. 1 In this paper we extend the Pitman-Yor HMM tagger (Blunsom and Cohn, 2011) to explicitly include a model of the lexicon that encodes from which tags a word type may be generated. For each word type we draw an ambiguity class which is the set of tags that it may occur with, capturing the fact that words are often ambiguous between certain tags (e.g. Noun and Verb), while rarely between others (e.g. Determiner and Verb). We extend the type based Sequential Monte Carlo (SMC) inference algorithm of Dubbin and Blunsom (2012) to incorporate our model of the lexicon, removing the need for the heuristic inference technique of Blunsom and Cohn (2011). Introduction Research on the unsupervised induction of partof-speech (PoS) tags has the potential to improve both our understanding of the plausibility of theories of first language acquisition, and Natural Language Processing applications such as Speech Recognition and Machine Translation. While there has been much prior work on this task (Brown et al., 1992; Clark, 2003; Christodoulopoulos et al., 2010; Toutanova and We start in Section 3 by introducing the original P"
E14-1013,P06-1124,0,0.203073,"or Process prior. The Pitman-Yor prior defines a smooth back off probability from more complex to less complex transition and emission distributions. In the PYP-HMM trigram model, the transition distributions form a hierarchy with trigram transition distributions drawn from a PYP with the bigram transitions as their base distribution, and the bigram transitions similarly backing off to the unigram transitions. The hierarchical prior can be intuitively understood to smooth the trigram transition distributions with bigram and unigram distributions in a similar manner to an ngram language model (Teh, 2006). This back-off structure greatly reduces sparsity in the trigram distributions and is achieved by chaining together the PYPs through their base distributions: ∼ PYP(aT , bT , Bi ) Bi |aB , bB , U ∼ PYP(aB , bB , U ) U |aU , bU ∼ PYP(aU , bU , Uniform). Ei |aE , bE , C ∼ PYP(aE , bE , Ci ), c− k −a n−1+b K − a+b n−1+b 1 ≤ k ≤ K− k = K− + 1 (1) where zn is the index of the table chosen by the nth customer to the restaurant, z1:n−1 is the seating arrangement of the previous n − 1 customers to enter, c− k is the count of the customers at table k, and K − is the total number of tables chosen by th"
E14-1013,J94-2001,0,\N,Missing
E14-1035,P12-2023,0,0.700385,"er translations within the same data set. If these distributions overlap, then we expect topic adaptation to help separate them and yield better translations than an unadapted system. Topics can be of varying granularity and are therefore a flexible means to structure data that is not uniform enough to be modelled in its entirety. In recent years there have been several attempts to integrating topical information into SMT either by learning better word alignments (Zhao and Xing, 2006), by adapting translation features cross-domain (Su et al., 2012), or by dynamically adapting lexical weights (Eidelman et al., 2012) or adding sparse topic features (Hasler et al., 2012). Translating text from diverse sources poses a challenge to current machine translation systems which are rarely adapted to structure beyond corpus level. We explore topic adaptation on a diverse data set and present a new bilingual variant of Latent Dirichlet Allocation to compute topic-adapted, probabilistic phrase translation features. We dynamically infer document-specific translation probabilities for test sets of unknown origin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU"
E14-1035,W07-0717,0,0.0868419,"gin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU over the baseline and 1.04 over a domain adaptation benchmark. We further provide an analysis of the domain-specific data and show additive gains of our model in combination with other types of topic-adapted features. 1 Introduction In statistical machine translation (SMT), there has been a lot of interest in trying to incorporate information about the provenance of training examples in order to improve translations for specific target domains. A popular approach are mixture models (Foster and Kuhn, 2007) where each component contains data from a specific genre or domain. Mixture models can be trained for crossdomain adaption when the target domain is known or for dynamic adaptation when the target domain is inferred from the source text under translation. More recent domain adaptation methods employ corpus or instance weights to promote relevant training examples (Matsoukas et al., 2009; Foster et al., 2010) or do more radical data selection based on language model perplexity (Axelrod et al., 2011). In this work, we are interested in the dynamic adaptation case, which is challenging because w"
E14-1035,D11-1033,0,0.0183881,"er to improve translations for specific target domains. A popular approach are mixture models (Foster and Kuhn, 2007) where each component contains data from a specific genre or domain. Mixture models can be trained for crossdomain adaption when the target domain is known or for dynamic adaptation when the target domain is inferred from the source text under translation. More recent domain adaptation methods employ corpus or instance weights to promote relevant training examples (Matsoukas et al., 2009; Foster et al., 2010) or do more radical data selection based on language model perplexity (Axelrod et al., 2011). In this work, we are interested in the dynamic adaptation case, which is challenging because we cannot tune our model towards any specific domain. In previous literature, domains have often been loosely defined in terms of corpora, for example, news texts would be defined as belonging to We take a new approach to topic adaptation by estimating probabilistic phrase translation features in a completely Bayesian fashion. The motivation is that automatically identifying topics in the training data can help to select the appropriate translation of a source phrase in the context of a document. By"
E14-1035,2012.iwslt-papers.17,1,0.830437,"ibutions overlap, then we expect topic adaptation to help separate them and yield better translations than an unadapted system. Topics can be of varying granularity and are therefore a flexible means to structure data that is not uniform enough to be modelled in its entirety. In recent years there have been several attempts to integrating topical information into SMT either by learning better word alignments (Zhao and Xing, 2006), by adapting translation features cross-domain (Su et al., 2012), or by dynamically adapting lexical weights (Eidelman et al., 2012) or adding sparse topic features (Hasler et al., 2012). Translating text from diverse sources poses a challenge to current machine translation systems which are rarely adapted to structure beyond corpus level. We explore topic adaptation on a diverse data set and present a new bilingual variant of Latent Dirichlet Allocation to compute topic-adapted, probabilistic phrase translation features. We dynamically infer document-specific translation probabilities for test sets of unknown origin, thereby capturing the effects of document context on phrase translations. We show gains of up to 1.26 BLEU over the baseline and 1.04 over a domain adaptation b"
E14-1035,W11-1014,0,0.331415,"r baseline counterparts after tuning, replacing them yielded better results in combination with the other adapted features. We believe the reason could be that fewer phrase table features in total are easier to optimise. |t| 2 f (x) = , 1 + 1x CC 110K 818 1892 Table 1: Number of sentence pairs and documents (in brackets) in the French-English data sets. The training data has 2.7M English words per domain. More topic-adapted features lazy MDI Mixed 354K (6450) 2453 (39) 5664 (112) relevance (4) (5) The third feature is a document similarity feature, similar to the semantic feature described by Banchs and Costa-jussà (2011): docSimt = max(1 − JSD(vtrain doci , vtest doc )) (6) i where vtrain_doci and vtest_doc are document topic vector of training and test documents. Because topic 0 captures phrase pairs that are common to many documents, we exclude it from the topic vectors before computing similarities. 4.1 Feature combination We tried integrating the four topic-adapted features separately and in all possible combinations. As we will see in the results section, while all features improve over the baseline in isolation, the adapted translation feature P(t|s,d) is the strongest feature. For the features that hav"
E14-1035,D11-1125,0,0.0087326,"containing data from all three domains and train one language model on the concatenation of (equally sized) target sides of the training data. Word alignments are trained on the concatenation of all training data and fixed for all models. Our baseline (ALL) is a phrase-based FrenchEnglish system trained on the concatenation of all parallel data. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5gram language model. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute caseinsensitive BLEU. As domain-aware benchmark systems, we use the phrase table fill-up method (FILLUP) of Bisazza et al. (2011) which preserves the translation scores of phrases from the IN model and the linear mixture models (LINTM) of Sennrich (2012b) (both available in the Moses toolkit). For both systems, we build separate phrase tables for each domain and use a wrapper to decode tuning and test sets with domainspecific tables. Both benchmarks have an advanModel IN ALL Mixed 26.77 26.86 CC 18.76 19.61 NC 29.56 29.42 TED 32.47 31.88 Model Ted-"
E14-1035,2011.iwslt-evaluation.18,0,0.0695227,"e concatenation of all training data and fixed for all models. Our baseline (ALL) is a phrase-based FrenchEnglish system trained on the concatenation of all parallel data. It was built with the Moses toolkit (Koehn et al., 2007) using the 14 standard core features including a 5gram language model. Translation quality is evaluated on a large test set, using the average feature weights of three optimisation runs with PRO (Hopkins and May, 2011). We use the mteval-v13a.pl script to compute caseinsensitive BLEU. As domain-aware benchmark systems, we use the phrase table fill-up method (FILLUP) of Bisazza et al. (2011) which preserves the translation scores of phrases from the IN model and the linear mixture models (LINTM) of Sennrich (2012b) (both available in the Moses toolkit). For both systems, we build separate phrase tables for each domain and use a wrapper to decode tuning and test sets with domainspecific tables. Both benchmarks have an advanModel IN ALL Mixed 26.77 26.86 CC 18.76 19.61 NC 29.56 29.42 TED 32.47 31.88 Model Ted-half vs Ted-full CC-half vs CC-full NC-half vs NC-full Table 2: BLEU of in-domain and baseline models. Model Ted-IN vs ALL CC-IN vs ALL NC-IN vs ALL Avg JSD 0.15 0.17 0.13 Avg"
E14-1035,W04-3250,1,0.221354,"The botton row of the table indicates the relative improvement of the best topic-adapted model per domain over the ALL model. Using all four topic-adapted features yields an improvement of 0.81 BLEU on the mixed test set. The highest improvement on a given domain is achieved for TED with an increase of 1.26 BLEU. The smallest improvement is measured on the NC domain. This is in line with the observation that distributions in the NC in-domain table are most similar to the ALL table, therefore we would expect the smallest improvement for domain or topic adaptation. We used bootstrap resampling (Koehn, 2004) to measure significance on the mixed test set and marked all statistically significant results compared to the respective baselines with asterisk (*: p ≤ 0.01). There is quite a clear horizontal separation between documents of different domains, for example, topics 6, 8, 19 occur mostly in Ted, NC and CC documents respectively. The overall structure is very similar between training (top) and test (bottom) documents, which shows that test inference was successful in carrying over the information learned on training documents. There is also some degree of topic sharing across domains, for examp"
E14-1035,D10-1005,0,0.00711491,"en a conversation and training documents in an additional feature. This is similar to the work of Banchs and Costa-jussà (2011), both of which inspired our document similarity feature. Also related is the work of Sennrich (2012a) who explore mixturemodelling on unsupervised clusters for domain adaptation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of multilingual topic models outside the field of MT include BoydGraber and Blei (2009; Boyd-Graber and Resnik (2010) who learn cross-lingual topic correspondences (but do not learn conditional distributions like our model does). In terms of model structure, our model is similar to BiTAM (Zhao and Xing, 2006) which is an LDA-style model to learn topicbased word alignments. The work of Carpuat and Wu (2007) is similar to ours in spirit, but they predict the most probable translation in a context at the token level while our adaptation operates at the type level of a document. many IT-related documents). The last example, démon, has three frequent translations in English: devil, demon and daemon. The last tran"
E14-1035,2012.iwslt-papers.14,0,0.052881,"atures make use of the topic mixtures learned by our bilingual topic model. The first feature is an adapted lexical weight, similar to the features in the work of Eidelman et al. (2012). Our feature is different in that we marginalise over topics to produce a single adapted feature where v[k] is the kth element of a document topic vector for document d and w(t|s,k) is a topic-dependent word translation probability: lex(t¯|s, ¯ d) = 5 5.1 1 ∏ { j|(i, j) ∈ a} ∑ ∑ w(t|s, k) · v[k] (3) i ∀(i, j)∈a k | {z } w(t|s) The second feature is a target unigram feature similar to the lazy MDI adaptation of Ruiz and Federico (2012). It includes an additional term that measures the relevance of a target word wi by comparing its document-specific probability Pdoc to its probability under the asymmetric topic 0: |t| Pdoc (wi ) Pdoc (wi ) )· f ( ) trgUnigramst = ∏ f ( (wi ) Ptopic0 (wi ) i=1 |Pbaseline {z } | {z } x>0 NC 103K 817 1878 TED 140K 818 1894 the log-linear model. We found that while adding the features worked well and yielded close to zero weights for their baseline counterparts after tuning, replacing them yielded better results in combination with the other adapted features. We believe the reason could be that"
E14-1035,2007.tmi-papers.6,0,0.032213,"ation and Chen et al. (2013) who compute phrase pair features from vector space representations that capture domain similarity to a development set. Both are cross-domain adaptation approaches, though. Instances of multilingual topic models outside the field of MT include BoydGraber and Blei (2009; Boyd-Graber and Resnik (2010) who learn cross-lingual topic correspondences (but do not learn conditional distributions like our model does). In terms of model structure, our model is similar to BiTAM (Zhao and Xing, 2006) which is an LDA-style model to learn topicbased word alignments. The work of Carpuat and Wu (2007) is similar to ours in spirit, but they predict the most probable translation in a context at the token level while our adaptation operates at the type level of a document. many IT-related documents). The last example, démon, has three frequent translations in English: devil, demon and daemon. The last translation refers to a computer process and would occur in an IT context. The topic-phrase probabilities reveal that its mostly likely translation as daemon occurs under topic 19 which clusters IT-related phrase pairs and is frequent in the CC corpus. These examples show that our model can disa"
E14-1035,2012.eamt-1.60,0,0.0433237,"opic vectors before computing similarities. 4.1 Feature combination We tried integrating the four topic-adapted features separately and in all possible combinations. As we will see in the results section, while all features improve over the baseline in isolation, the adapted translation feature P(t|s,d) is the strongest feature. For the features that have a counterpart in the baseline model (p(t|s,d) and lex(t|s,d)), we experimented with either adding or replacing them in 331 Experimental setup Data and baselines Our experiments were carried out on a mixed data set, containing the TED corpus (Cettolo et al., 2012), parts of the News Commentary corpus (NC) and parts of the Commoncrawl corpus (CC) from the WMT13 shared task (Bojar et al., 2013) as described in Table 1. We were guided by two constraints in chosing our data set. 1) the data has document boundaries and the content of each document is assumed to be topically related, 2) there is some degree of topical variation within each data set. In order to compare to domain adaptation approaches, we chose a setup with data from different corpora. We want to abstract away from adaptation effects that concern tuning of length penalties and language models"
E14-1035,P06-2124,0,0.183172,"We view topic adaptation as fine-grained domain adaptation with the implicit assumption that there can be multiple distributions over translations within the same data set. If these distributions overlap, then we expect topic adaptation to help separate them and yield better translations than an unadapted system. Topics can be of varying granularity and are therefore a flexible means to structure data that is not uniform enough to be modelled in its entirety. In recent years there have been several attempts to integrating topical information into SMT either by learning better word alignments (Zhao and Xing, 2006), by adapting translation features cross-domain (Su et al., 2012), or by dynamically adapting lexical weights (Eidelman et al., 2012) or adding sparse topic features (Hasler et al., 2012). Translating text from diverse sources poses a challenge to current machine translation systems which are rarely adapted to structure beyond corpus level. We explore topic adaptation on a diverse data set and present a new bilingual variant of Latent Dirichlet Allocation to compute topic-adapted, probabilistic phrase translation features. We dynamically infer document-specific translation probabilities for te"
E14-1035,D10-1044,0,\N,Missing
E14-1035,D09-1074,0,\N,Missing
E14-1035,E12-1055,0,\N,Missing
E14-1035,P13-2122,0,\N,Missing
J09-4011,J07-3002,0,0.0259115,"em of word alignment for morphologically rich languages such as Arabic. To avoid the issue of having to choose a single morphological tokenization, the authors create alignments from a range of tokenizations which are then combined using a binary classiﬁer trained on hand-aligned data. Although of particular interest for those working with Arabic, this chapter fails to go beyond other works on supervised training for word alignment which have consistently shown that it’s easy to achieve large gains in alignment accuracy while much more difﬁcult to impact on end-to-end translation performance (Fraser and Marcu 2007). Part I ﬁnishes with a chapter that applies more-advanced machine learning than those before. In “Linguistically Enriched Word-Sequence Kernels for Discriminative Language Modeling,” Pierre Mah´e and Nicola Cancedda demonstrate the use of string kernels for language modeling, evaluating a number of kernels including one able to integrate a range of factors (surface form, lemma, part-of-speech). This is interesting 638 Book Review work, showing that complex machine learning techniques can be brought to bear on basic NLP tasks, although scaling issues limit the evaluation to small artiﬁcial dat"
J09-4011,P02-1039,0,0.0400341,"he ﬁrst category. Both decompose the translation problem into subproblems, particularly focusing on lexical choice as classiﬁcation. In Chapter 7, “Toward Purely Discriminative Training for Tree-Structured Translation Models,” Benjamin Wellington, Joseph Turian, and I. Dan Melamed seek to transduce source syntax trees into target strings by learning local classiﬁers for the nodes in the trees. Although such an approach allows SMT to be viewed as learning local classiﬁers, the trade-offs made seem to signiﬁcantly limit the model, something encountered in other works on local tree transduction (Yamada and Knight 2002). In Chapter 10, “Statistical Machine Translation through Global Lexical Selection,” Srinivas Bangalore, Stephan Kanthak, and Patrick Haffner take a bag-of-words approach, ignoring ordering information and learning classiﬁers that predict the presence of target lexical items given an entire source sentence. This chapter takes quite a novel ﬁnite–state transducer approach to SMT; however, again the simplifying modeling assumptions seem limiting. Perhaps the most novel and interesting chapter of this book is Chapter 9, “KernelBased Machine Translation,” by Zhuoran Wang and John Shawe-Taylor. Thi"
J09-4011,P09-1088,1,\N,Missing
J09-4011,N09-1025,0,\N,Missing
N09-1062,E03-1005,0,0.326439,", not just immediate children. These large fragments can be used to encode non-local context, such as head-lexicalisation and verb sub-categorisation. Since no annotated data is available providing TSG derivations we must induce the PTSG productions and their probabilities in an unsupervised way from an ordinary treebank. This is the same problem addressed by Data Oriented Parsing (DOP, Bod et al. (2003)), a method which uses as productions all subtrees of the training corpus. However, many of the DOP estimation methods have serious shortcomings (Johnson, 2002), namely inconsistency for DOP1 (Bod, 2003) and overfitting of the maximum likelihood estimate (Prescher et al., 2004). In this paper we develop an alternative means of learning a PTSG from a treebanked corpus, with the twin objectives of a) finding a grammar which accurately models the data and b) keeping the grammar as simple as possible, with few, compact, elementary trees. This is achieved using a prior to encourage sparsity and simplicity in a Bayesian nonparametric formulation. The framework allows us to perform inference over an infinite space of grammar productions in an elegant and efficient manner. The net result is a grammar"
N09-1062,P05-1022,0,0.0061087,"ing latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 1 Introduction Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999)). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998). This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar. We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary 548 trees), not just immediate children. These large fragments can be used to encode non-local c"
N09-1062,A00-2018,0,0.0299597,"rammars, uncovering latent linguistic structures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 1 Introduction Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999)). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998). This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar. We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary 548 trees), not just immediate children. These large fragments can b"
N09-1062,C02-1126,0,0.04097,"component rules, and the probability of a tree is the sum of the probabilities of its derivations. As we mentioned in the introduction, work within the DOP framework seeks to induce PTSGs from treebanks by using all possible subtrees as rules, and one of a variety of methods for estimating rule probabilities.3 Our aim of inducing compact grammars contrasts with that of DOP; moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2 A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3 TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. 549 mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim"
N09-1062,P07-1035,0,0.0165933,"Missing"
N09-1062,P06-1085,1,0.0765468,"bution in more detail below. Rather than representing the distribution Gc explicitly, we integrate over all possible values of Gc . The resulting distribution over ei , conditioned on e&lt;i = e1 . . . ei−1 and the root category c is: (1) Since the sequence of elementary trees can be split into derivations, each of which completely specifies a tree, P (t|e) is either equal to 1 (when t and e are consistent) or 0 (otherwise). Therefore, the work in our model is done by the prior distribution over elementary trees. Note that this is analogous to the Bayesian model of word segmentation presented by Goldwater et al. (2006); indeed, the problem of inferring e from t can be viewed as a segmentation problem, where each full tree must be segmented into one or more elementary trees. As in Goldwater et al. (2006), we wish to favour solutions employing a relatively small number of elementary units (here, elementary trees). This can be done using a Dirichlet process (DP) prior. Specifically, we define the distribution of elementary tree e with root non-terminal symbol c as Gc |αc , P0 ∼ DP(αc , P0 (·|c)) e|c ∼ Gc where P0 (·|c) (the base distribution) is a distribution over the infinite space of trees rooted with c, an"
N09-1062,N07-1018,1,0.524373,"required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs are better suited to learning subcategories with larger membership, such as the terminals for days of the week and noun-adjective agreement. The approaches are orthogonal, and we expect that combining a category refinement model with our TSG model would provide better performance than either approach alone. Our model is similar to the Adaptor Grammar model of Johnson et al. (2007b), which is also a kind of Bayesian nonparametric tree-substitution grammar. However, Adaptor Grammars require that each sub-tree expands completely, with only terminal symbols as leaves, while our own model permits non-terminal frontier nodes. In addition, they disallow recursive containment of adapted non-terminals; we impose no such constraint. 3 Model Recall the nature of our task: we are given a corpus of parse trees t and wish to infer a tree-substitution grammar G that we can use to parse new data. Rather than inferring a grammar directly, we go through an intermediate step of inferrin"
N09-1062,J98-4004,0,0.13043,"ures (e.g., verb subcategorisation), and in doing so far out-performs a standard PCFG. 1 Introduction Many successful models of syntax are based on Probabilistic Context Free Grammars (PCFGs) (e.g., Collins (1999)). However, directly learning a PCFG from a treebank results in poor parsing performance, due largely to the unrealistic independence assumptions imposed by the context-free assumption. Considerable effort is required to coax good results from a PCFG, in the form of grammar engineering, feature selection and clever smoothing (Collins, 1999; Charniak, 2000; Charniak and Johnson, 2005; Johnson, 1998). This effort must be repeated when moving to different languages, grammar formalisms or treebanks. We propose that much of this hand-coded knowledge can be obtained automatically as an emergent property of the treebanked data, thereby reducing the need for human input in crafting the grammar. We present a model for automatically learning a Probabilistic Tree Substitution Grammar (PTSG), an extension to the PCFG in which non-terminals can rewrite as entire tree fragments (elementary 548 trees), not just immediate children. These large fragments can be used to encode non-local context, such as"
N09-1062,J02-1005,0,0.622903,"as entire tree fragments (elementary 548 trees), not just immediate children. These large fragments can be used to encode non-local context, such as head-lexicalisation and verb sub-categorisation. Since no annotated data is available providing TSG derivations we must induce the PTSG productions and their probabilities in an unsupervised way from an ordinary treebank. This is the same problem addressed by Data Oriented Parsing (DOP, Bod et al. (2003)), a method which uses as productions all subtrees of the training corpus. However, many of the DOP estimation methods have serious shortcomings (Johnson, 2002), namely inconsistency for DOP1 (Bod, 2003) and overfitting of the maximum likelihood estimate (Prescher et al., 2004). In this paper we develop an alternative means of learning a PTSG from a treebanked corpus, with the twin objectives of a) finding a grammar which accurately models the data and b) keeping the grammar as simple as possible, with few, compact, elementary trees. This is achieved using a prior to encourage sparsity and simplicity in a Bayesian nonparametric formulation. The framework allows us to perform inference over an infinite space of grammar productions in an elegant and ef"
N09-1062,D07-1072,0,0.0361547,"operator. 3 TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. 549 mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG’s elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs are better suited to"
N09-1062,N07-1051,0,0.0121277,"e 3: TSG used to generate synthetic data. All production probabilities are uniform. found that using the CYK algorithm (Cocke, 1969) to find the Viterbi derivation for p˜ yielded consistently better results. This algorithm maximises an approximated model, as opposed to approximately optimising the true model. We also present results using the tree with the maximum expected count of CFG rules (MER). This uses counts of the CFG rules applied at each span (compiled from the derivation samples) followed by a maximisation step to find the best tree. This is similar to the MAX-RULE-SUM algorithm of Petrov and Klein (2007) and maximum expected recall parsing (Goodman, 2003). 6 Experiments Synthetic data Before applying the model to natural language, we first create a synthetic problem to confirm that the model is capable of recovering a known tree-substitution grammar. We created 50 random trees from the TSG shown in Figure 3. This produces binary trees with A and B internal nodes and ‘a’ and ‘b’ as terminals, such that the terminals correspond to their grand-parent non-terminal (A and a or B and b). These trees cannot be modelled accurately with a CFG because expanding A and B nodes into terminal strings requi"
N09-1062,P06-1055,0,0.780747,"thout the adjunction operator. 3 TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. 549 mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG’s elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be exorbitant. Consequently, our model should be better able to learn specific lexical patterns, such as full noun-phrases and verbs with their sub-categorisation frames, while theirs"
N09-1062,D07-1058,0,0.406805,"moreover, we develop a probabilistic estimator which avoids the shortcomings of DOP1 and the maximum likelihood esti2 A TSG is a Tree Adjoining Grammar (TAG; Joshi (2003)) without the adjunction operator. 3 TAG induction (Chiang and Bikel, 2002; Xia, 2002) also tackles a similar learning problem. 549 mate (Bod, 2000; Bod, 2003; Johnson, 2002). Recent work on DOP estimation also seeks to address these problems, drawing from estimation theory to solve the consistency problem (Prescher et al., 2004; Zollmann and Sima’an, 2005), or incorporating a grammar brevity term into the learning objective (Zuidema, 2007). Our work differs from these previous approaches in that we explicitly model a prior over grammars within a Bayesian framework.4 Models of grammar refinement (Petrov et al., 2006; Liang et al., 2007; Finkel et al., 2007) also aim to automatically learn latent structure underlying treebanked data. These models allow each nonterminal to be split into a number of subcategories. Theoretically the grammar space of our model is a sub-space of theirs (projecting the TSG’s elementary trees into CFG rules). However, the number of nonterminals required to recreate our TSG grammars in a PCFG would be ex"
N09-1062,J03-4003,0,\N,Missing
N10-1028,P09-1088,1,0.900868,"ack of a principled, and scalable, algorithm for learning a synchronous context free grammar (SCFG) from sentence-aligned parallel corpora. The predominant approach for learning phrasebased translation models (both finite state or synchronous grammar based) uses a cascade of heuristics beginning with predicted word alignments and producing a weighted set of translation rules (Koehn et al., 2003). Alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g., (Marcu and Wong, 2002; Cherry and Lin, 2007; DeNero et al., 2008; Blunsom et al., 2009)). Although these models are theoretically attractive, inference is intractable (at least O(|f |3 |e|3 )). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality initialisation, mixing very slowly and being easily caught in modes. Instead, blocked sampling over sentence pairs allows much faster mixing, but done in the"
N10-1028,W07-0403,0,0.522712,"ion system. We concern ourselves with the lack of a principled, and scalable, algorithm for learning a synchronous context free grammar (SCFG) from sentence-aligned parallel corpora. The predominant approach for learning phrasebased translation models (both finite state or synchronous grammar based) uses a cascade of heuristics beginning with predicted word alignments and producing a weighted set of translation rules (Koehn et al., 2003). Alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g., (Marcu and Wong, 2002; Cherry and Lin, 2007; DeNero et al., 2008; Blunsom et al., 2009)). Although these models are theoretically attractive, inference is intractable (at least O(|f |3 |e|3 )). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality initialisation, mixing very slowly and being easily caught in modes. Instead, blocked sampling over sentence pairs"
N10-1028,D08-1033,0,0.14161,"Missing"
N10-1028,N07-1018,0,0.651894,"s are theoretically attractive, inference is intractable (at least O(|f |3 |e|3 )). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality initialisation, mixing very slowly and being easily caught in modes. Instead, blocked sampling over sentence pairs allows much faster mixing, but done in the obvious way (following Johnson et al. (2007)) would incur a O(|f |3 |e|3 ) time complexity. Here we draw inspiration from the work of Van Gael et al. (2008) on inference in infinite hidden Markov models to develop a novel algorithm for efficient sampling from a SCFG. We develop an auxiliary variable ‘slice’ sampler which can dramatically reduce inference complexity, and thereby make blocked sampling practicable on real translation corpora. Our evaluation demonstrates that our algorithm mixes more quickly than the local Gibbs sampler, and produces translation models which achieve state-ofthe-art B LEU scores without using GIZA++ or symme"
N10-1028,N03-1017,0,0.259153,"question answering) will rest on the quality of approximate inference. In this work we tackle this problem in the context of inducing synchronous grammars for a machine translation system. We concern ourselves with the lack of a principled, and scalable, algorithm for learning a synchronous context free grammar (SCFG) from sentence-aligned parallel corpora. The predominant approach for learning phrasebased translation models (both finite state or synchronous grammar based) uses a cascade of heuristics beginning with predicted word alignments and producing a weighted set of translation rules (Koehn et al., 2003). Alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g., (Marcu and Wong, 2002; Cherry and Lin, 2007; DeNero et al., 2008; Blunsom et al., 2009)). Although these models are theoretically attractive, inference is intractable (at least O(|f |3 |e|3 )). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2"
N10-1028,P07-2045,0,0.013919,"LB structure achieves a higher likelihood than the M4 initialised model, although this is not reflected in their relative B LEU scores. In contrast the Gibbs sampler is more significantly affected by its initialisation, only deviating slightly before becoming trapped in a mode, as seen in Fig. 1. With sufficient (infinite) time both sampling strategies will converge on the true posterior regardless of initialisation, however the slice sampler appears to be converging much faster than the Gibbs sampler. Interestingly, the initialisation heuristics (M1 and M4) outperform the default heuristics (Koehn et al., 2007) by a considerable margin. This is most likely because the initialisation heuristics force the alignments to factorise with an ITG, resulting in more aggressive pruning of spurious alignments which in turn allows for more and larger phrase pairs. In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al., 2009), in terms of mixing and translation quality. We measure mixing in terms of training log-likelihood (LLH) after a fixed number of sampling iterations. Translations are produced using Moses (Koehn et al., 2007), initialised with the word alignments fro"
N10-1028,W02-1018,0,0.0924475,"for a machine translation system. We concern ourselves with the lack of a principled, and scalable, algorithm for learning a synchronous context free grammar (SCFG) from sentence-aligned parallel corpora. The predominant approach for learning phrasebased translation models (both finite state or synchronous grammar based) uses a cascade of heuristics beginning with predicted word alignments and producing a weighted set of translation rules (Koehn et al., 2003). Alternative approaches avoid such heuristics, instead learning structured alignment models directly from sentence aligned data (e.g., (Marcu and Wong, 2002; Cherry and Lin, 2007; DeNero et al., 2008; Blunsom et al., 2009)). Although these models are theoretically attractive, inference is intractable (at least O(|f |3 |e|3 )). The efficacy of direct estimation of structured alignment models therefore rests on the approximations used to make inference practicable – typically heuristic constraints or Gibbs sampling. In this work we show that naive Gibbs sampling (specifically, Blunsom et al. (2009)) is ineffectual for inference and reliant on a high quality initialisation, mixing very slowly and being easily caught in modes. Instead, blocked sampli"
N10-1028,2001.mtsummit-papers.68,0,0.00996132,"ause the initialisation heuristics force the alignments to factorise with an ITG, resulting in more aggressive pruning of spurious alignments which in turn allows for more and larger phrase pairs. In the following experiments we compare the slice sampler and the Gibbs sampler (Blunsom et al., 2009), in terms of mixing and translation quality. We measure mixing in terms of training log-likelihood (LLH) after a fixed number of sampling iterations. Translations are produced using Moses (Koehn et al., 2007), initialised with the word alignments from the final sample, and are evaluated using B LEU(Papineni et al., 2001). The slice sampled models are restricted 9 The following beam heuristics are employed: alignments to to learning binary branching one-to-one (or null) null are only permitted on the longer sentence side; words are alignments,8 while no restriction is placed on the only allowed to align to those whose relative sentence position ±3 words. Gibbs sampler (both use the same model, so have is within 10 Words of the longer sentence are randomly assigned to null. 11 Moreover, we only sample values for us as they are visited We limit the maximum training sentence length to 40, resultby the parser, thu"
N10-1028,J97-3002,0,0.861041,"6 Table 1: IWSLT Chinese to English translation. of theQ second product. The last step (4) discards the term us β(us ; a, b) which is constant wrt d. The net result is a formulation which factors with the derivation structure, thereby eliminating the need to consider all O(|e|2 |f |2 ) spans in S. Critically p(d|u) is zero for all spans failing the I (us &lt; θrs ) condition. To exploit the decomposition of Equation 4 we require a parsing algorithm that only explores chart cells whose child cells have not already been pruned by the slice variables. The standard approach of using synchronous CYK (Wu, 1997) doesn’t posses this property: all chart cells would be visited even if they are to be pruned. Instead we use an agenda based parsing algorithm, in particular we extend the algorithm of Klein and Manning (2004) to synchronous parsing.6 Finally, we need a Metropolis-Hastings acceptance step to account for intra-instance dependencies (the ‘rich-get-richer’ effect). We omit the details, save to state that the calculation cancels to the same test as presented in Johnson et al. (2007).7 3 Evaluation comparable LLH). Of particular interest is how the different samplers perform given initialisations"
N10-1028,P02-1040,0,\N,Missing
N13-1117,J93-2003,0,0.114431,"hmarked against Giza++, including significant improvements over IBM model 4. 1 Phil Blunsom Department of Computer Science University of Oxford Oxford, OX1 3QD, United Kingdom Phil.Blunsom@cs.ox.ac.uk To overcome perceived limitations with the word based and non-syntactic nature of the IBM models many alternative approaches to word alignment have been proposed (e.g. (DeNero et al., 2008; Cohn and Blunsom, 2009; Levenberg et al., 2012)). While interesting results have been reported, these alternatives have failed to dislodge the IBM approach. Introduction The IBM and HMM word alignment models (Brown et al., 1993; Vogel et al., 1996) have underpinned the majority of statistical machine translation systems for almost twenty years. The key attraction of these models is their principled probabilistic formulation, and the existence of (mostly) tractable algorithms for their training. In this paper we proposed to retain the original generative stories of the IBM models, while replacing the inflexible categorical distributions with hierarchical Pitman-Yor (PY) processes – a mathematical tool which has been successfully applied to a range of language tasks (Teh, 2006b; Goldwater et al., 2006; Blunsom and Coh"
N13-1117,N09-1013,0,0.0417341,"Missing"
N13-1117,D09-1037,1,0.845539,"hniques. The resulting models are highly extendible, naturally permitting the introduction of phrasal dependencies. We present extensive experimental results showing improvements in both AER and BLEU when benchmarked against Giza++, including significant improvements over IBM model 4. 1 Phil Blunsom Department of Computer Science University of Oxford Oxford, OX1 3QD, United Kingdom Phil.Blunsom@cs.ox.ac.uk To overcome perceived limitations with the word based and non-syntactic nature of the IBM models many alternative approaches to word alignment have been proposed (e.g. (DeNero et al., 2008; Cohn and Blunsom, 2009; Levenberg et al., 2012)). While interesting results have been reported, these alternatives have failed to dislodge the IBM approach. Introduction The IBM and HMM word alignment models (Brown et al., 1993; Vogel et al., 1996) have underpinned the majority of statistical machine translation systems for almost twenty years. The key attraction of these models is their principled probabilistic formulation, and the existence of (mostly) tractable algorithms for their training. In this paper we proposed to retain the original generative stories of the IBM models, while replacing the inflexible cate"
N13-1117,D08-1033,0,0.348643,"Missing"
N13-1117,N09-1036,0,0.059904,"ity assigned by the different models to an alignment sequence, where the restaurants replace the counts of the alignment positions Chinese -> English Pipeline PY-IBM 29.5 Giza++ AER Pipeline English -> Chinese Pipeline PY-IBM Giza++ 14.75 39 29.0 37 28.0 27.5 14.25 AER BLEU BLEU 38 14.50 28.5 34 13.75 26.5 1 1>H 1>H>3 1>H>3>4 Figure 1: BLEU scores of pipelined Giza++ and pipelined PY-IBM translating from Chinese into English 13.50 33 1 1>H 1>H>3 1>H>3>4 Figure 2: BLEU scores of pipelined Giza++ and pipelined PY-IBM translating from English into Chinese in the prior. Maximum marginal decoding (Johnson and Goldwater, 2009) can then be used to get the MAP estimate of the probability distributions over the alignment positions for each sentence from the samples extracted from the Gibbs sampler. In addition to sampling the alignments, we also place a uniform Beta prior on the discount parameters and a vague Gamma prior on the strength parameters, and sample them using slice sampling (Neal, 2003). The end result is that the alignment models have no free parameters to tune. 5 36 35 14.00 27.0 26.0 Giza++ PY-IBM 40 Experimental results In order to assess our PY process alignment models (referred to as PY-IBM hencefort"
N13-1117,P07-2045,0,0.0166046,"Neal, 2003). The end result is that the alignment models have no free parameters to tune. 5 36 35 14.00 27.0 26.0 Giza++ PY-IBM 40 Experimental results In order to assess our PY process alignment models (referred to as PY-IBM henceforth) several experiments were carried out to benchmark them against the original models (as implemented in Giza++). We evaluated the BLEU scores (Papineni et al., 2002) of translations from Chinese into English and from English into Chinese, as well as the alignment error rates (AER) of the induced symmetrised alignments compared to a human aligned dataset. Moses (Koehn et al., 2007) was used for the training of the SMT system and the symmetrisation (using the grow-diag-final procedure), with MERT (Och, 2003) used for tuning of the weights, and SRILM (Stolcke, 2002) to build the language model (5-grams based). The corpus used for training and evaluation was the Chinese 974 32 1 1>H 1>H>3 1>H>3>4 Figure 3: AER of pipelined Giza++ and pipelined PY-IBM aligning Chinese and English FBIS corpus. MT02 was used for tuning, and MT03 was used for evaluation. In each case we used one reference sentence in Chinese and 4 reference sentences in English. Most translation systems rely o"
N13-1117,D12-1021,1,0.867402,"Missing"
N13-1117,J03-1002,0,0.296213,"ach word to fill-in the words in the foreign sentence from left to right. The following probability model is derived for the HMM alignment model (Vogel et al., 1996): P (F,A|E) = p(m|l) × m Y p(ai |ai−1 , m) × p(fi |eai ) i=1 For the HMM alignment model we replace the categorical translation distribution p(fi |eai ) with the same one we used for model 1, and replace the categorical distribution for the transition p(ai |ai−1 , m) with a hierarchical PY process with a longer sequence of alignment positions in the conditional: Using the above notation, the following probability model is derived (Och and Ney, 2003): P (F, A|E) =p(B0 |B1 , ..., Bl ) × l Y p(Bi |Bi−1 , ei ) i=1 ai |ai−1 , m ∼ Gm ai−1 m Gm ai−1 ∼ P Y (G∅ ) × l Y Y p(fj |ei ) i=0 j∈Bi m Gm ∅ ∼ P Y (G0 ) We use a unique distribution for each foreign sentence length, and condition the position on the previous alignment position, backing-off to the HMM’s stationary distribution over alignment positions. For model 3 the dependence on previous alignment sets is ignored and the probability p(Bi |Bi−1 , ei ) is modelled as Y p(Bi |Bi−1 , ei ) = p(φi |ei )φi ! p(j|i, m), j∈Bi 3.2 Models 3 and 4 IBM models 3 and 4 introduce the concept of a word’s f"
N13-1117,P03-1021,0,0.015911,"mental results In order to assess our PY process alignment models (referred to as PY-IBM henceforth) several experiments were carried out to benchmark them against the original models (as implemented in Giza++). We evaluated the BLEU scores (Papineni et al., 2002) of translations from Chinese into English and from English into Chinese, as well as the alignment error rates (AER) of the induced symmetrised alignments compared to a human aligned dataset. Moses (Koehn et al., 2007) was used for the training of the SMT system and the symmetrisation (using the grow-diag-final procedure), with MERT (Och, 2003) used for tuning of the weights, and SRILM (Stolcke, 2002) to build the language model (5-grams based). The corpus used for training and evaluation was the Chinese 974 32 1 1>H 1>H>3 1>H>3>4 Figure 3: AER of pipelined Giza++ and pipelined PY-IBM aligning Chinese and English FBIS corpus. MT02 was used for tuning, and MT03 was used for evaluation. In each case we used one reference sentence in Chinese and 4 reference sentences in English. Most translation systems rely on the Giza++ package in which the implementation of the original models is done by combining them in a pipeline. Model 1 and the"
N13-1117,P02-1040,0,0.0883321,"cted from the Gibbs sampler. In addition to sampling the alignments, we also place a uniform Beta prior on the discount parameters and a vague Gamma prior on the strength parameters, and sample them using slice sampling (Neal, 2003). The end result is that the alignment models have no free parameters to tune. 5 36 35 14.00 27.0 26.0 Giza++ PY-IBM 40 Experimental results In order to assess our PY process alignment models (referred to as PY-IBM henceforth) several experiments were carried out to benchmark them against the original models (as implemented in Giza++). We evaluated the BLEU scores (Papineni et al., 2002) of translations from Chinese into English and from English into Chinese, as well as the alignment error rates (AER) of the induced symmetrised alignments compared to a human aligned dataset. Moses (Koehn et al., 2007) was used for the training of the SMT system and the symmetrisation (using the grow-diag-final procedure), with MERT (Och, 2003) used for tuning of the weights, and SRILM (Stolcke, 2002) to build the language model (5-grams based). The corpus used for training and evaluation was the Chinese 974 32 1 1>H 1>H>3 1>H>3>4 Figure 3: AER of pipelined Giza++ and pipelined PY-IBM aligning"
N13-1117,J10-3001,0,0.014886,"ct The dominant Giza++ implementation of the IBM models (Och and Ney, 2003) employs a variety of exact and approximate EM algorithms to optimise categorical alignment distributions. While effective, this parametric approach results in a significant number of parameters to be tuned and intractable summations over the space of alignments for models 3 and 4. Giza++ hides the hyperparameters with defaults and approximates the intractable expectations using restricted alignment neighbourhoods. However this approach was shown to often return alignments with probabilities well below the true maxima (Ravi and Knight, 2010). The dominant yet ageing IBM and HMM word alignment models underpin most popular Statistical Machine Translation implementations in use today. Though beset by the limitations of implausible independence assumptions, intractable optimisation problems, and an excess of tunable parameters, these models provide a scalable and reliable starting point for inducing translation systems. In this paper we build upon this venerable base by recasting these models in the non-parametric Bayesian framework. By replacing the categorical distributions at their core with hierarchical Pitman-Yor processes, and"
N13-1117,P12-2060,0,0.335269,"n the context of language modelling, the hierarchical PY process was shown to roughly correspond to interpolated Kneser-Ney (Kneser and Ney, 1995; Teh, 2006a). The key contribution of the hierarchical PY formulation is that it provides a principle probabilistic framework that easily extends to latent variable models, such as those used 969 Proceedings of NAACL-HLT 2013, pages 969–977, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics for alignment, for which a Kneser-Ney formulation is unclear. While Bayesian priors have previously been applied to IBM model 1 (Riley and Gildea, 2012), in this work we go considerably further by implementing non-parametric priors for the full Giza++ training pipeline. Inference for the proposed models and their hyper-parameters is done with Gibbs sampling. This eliminates the intractable summations over alignments and the need for tuning hyperparameters. Further, we exploit the highly extendible nature of the hierarchical PY process to implement improvements to the original models such as the introduction of phrasal dependencies. We present extensive experimental results showing improvements in both BLEU scores and AER when compared to Giza"
N13-1117,P06-1124,0,0.315272,"M and HMM word alignment models (Brown et al., 1993; Vogel et al., 1996) have underpinned the majority of statistical machine translation systems for almost twenty years. The key attraction of these models is their principled probabilistic formulation, and the existence of (mostly) tractable algorithms for their training. In this paper we proposed to retain the original generative stories of the IBM models, while replacing the inflexible categorical distributions with hierarchical Pitman-Yor (PY) processes – a mathematical tool which has been successfully applied to a range of language tasks (Teh, 2006b; Goldwater et al., 2006; Blunsom and Cohn, 2011). In the context of language modelling, the hierarchical PY process was shown to roughly correspond to interpolated Kneser-Ney (Kneser and Ney, 1995; Teh, 2006a). The key contribution of the hierarchical PY formulation is that it provides a principle probabilistic framework that easily extends to latent variable models, such as those used 969 Proceedings of NAACL-HLT 2013, pages 969–977, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics for alignment, for which a Kneser-Ney formulation is unclear. While Bayesian"
N13-1117,C96-2141,0,0.957767,"++, including significant improvements over IBM model 4. 1 Phil Blunsom Department of Computer Science University of Oxford Oxford, OX1 3QD, United Kingdom Phil.Blunsom@cs.ox.ac.uk To overcome perceived limitations with the word based and non-syntactic nature of the IBM models many alternative approaches to word alignment have been proposed (e.g. (DeNero et al., 2008; Cohn and Blunsom, 2009; Levenberg et al., 2012)). While interesting results have been reported, these alternatives have failed to dislodge the IBM approach. Introduction The IBM and HMM word alignment models (Brown et al., 1993; Vogel et al., 1996) have underpinned the majority of statistical machine translation systems for almost twenty years. The key attraction of these models is their principled probabilistic formulation, and the existence of (mostly) tractable algorithms for their training. In this paper we proposed to retain the original generative stories of the IBM models, while replacing the inflexible categorical distributions with hierarchical Pitman-Yor (PY) processes – a mathematical tool which has been successfully applied to a range of language tasks (Teh, 2006b; Goldwater et al., 2006; Blunsom and Cohn, 2011). In the cont"
N15-1083,P14-2023,0,0.0386821,"tput translations. The most popular language model implementation is a back-off n-gram model with Kneser-Ney smoothing (Chen and Goodman, 1999). Back-off n-gram models are conceptually simple, very efficient to construct and query, and are regarded as being extremely effective in translation systems. Neural language models are a more recent class of language models (Bengio et al., 2003) that have been It has been shown that neural language models can improve translation quality when used as additional features in a decoder (Vaswani et al., 2013; Botha and Blunsom, 2014; Baltescu et al., 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al., 2013). These results show great promise and in this paper we continue this line of research by investigating the tradeoff between speed and accuracy when integrating neural language models in a decoder. We also focus on how effective these models are when used as the sole language model in a translation system. This is important because our hypothesis is that most of the language modelling is done by the n-gram model, with the neural model only acting as a differentiating factor when the n-gram model cannot provide a decisive p"
N15-1083,D13-1106,0,0.0138249,"ck-off n-gram model with Kneser-Ney smoothing (Chen and Goodman, 1999). Back-off n-gram models are conceptually simple, very efficient to construct and query, and are regarded as being extremely effective in translation systems. Neural language models are a more recent class of language models (Bengio et al., 2003) that have been It has been shown that neural language models can improve translation quality when used as additional features in a decoder (Vaswani et al., 2013; Botha and Blunsom, 2014; Baltescu et al., 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al., 2013). These results show great promise and in this paper we continue this line of research by investigating the tradeoff between speed and accuracy when integrating neural language models in a decoder. We also focus on how effective these models are when used as the sole language model in a translation system. This is important because our hypothesis is that most of the language modelling is done by the n-gram model, with the neural model only acting as a differentiating factor when the n-gram model cannot provide a decisive probability. Furthermore, neural language models are considerably more co"
N15-1083,W14-4012,0,0.122717,"Missing"
N15-1083,P14-1129,0,0.632426,"tored 33.89 33.87 33.69 20.06 19.96 19.52 20.25 20.25 19.87 Table 4: A comparison between standard back-off n-gram models and neural language models. The perplexities for the English language models are shown in parentheses. network. This operation is especially problematic when the neural language model is incorporated as a feature in the decoder, as the language model is queried several hundred thousand times for any sentence of average length. Previous publications on neural language models in machine translation have approached this problem in two different ways. Vaswani et al. (2013) and Devlin et al. (2014) simply ignore normalisation when decoding, albeit Devlin et al. (2014) alter their training objective to learn self-normalised models, i.e. models where the sum of the values in the output layer is (hopefully) close to 1. Vaswani et al. (2013) use noise contrastive estimation to speed up training, while Devlin et al. (2014) train their models with standard gradient descent on a GPU. The second approach is to explicitly normalise the models, but to limit the set of words over which the normalisation is performed, either via class-based factorisation (Botha and Blunsom, 2014; Baltescu et al., 2"
N15-1083,W11-2123,0,0.180399,"of the europarl, news commentary and the news crawl 2007-2013 corpora. The corpora were tokenized and lowercased using the same text processing scripts and the words not occuring the in the target side of the parallel data were replaced with a special <unk&gt; token. Statistics for the monolingual data after the preprocessing step are reported in Table 3. Throughout this paper we report results for 5gram language models, regardless of whether they are back-off n-gram models or neural models. To construct the back-off n-gram models, we used a compact trie-based implementation available in KenLM (Heafield, 2011), because otherwise we would have had difficulties with fitting these models in the main memory of our machines. When training neural language models, we set the size of the distributed representations to 500, we used diagonal context matrices and we used 10 negative samples for noise contrastive estimation, unless otherwise indicated. In cases where we perform experiments on only one language pair, the reader should assume we used French→English data. 4 Normalisation The key challenge with neural language models is scaling the softmax step in the output layer of the 3 We followed the first tw"
N15-1083,P07-2045,0,0.00848919,"decomposition trick resulting in a fast algorithm for both training and testing. In the new training algorithm, when we account for the class conditional probabilities P (ci |hi ), we draw noise samples from the class unigram distribution, and when we account for P (wi |ci , hi ), we sample from the unigram distribution of only the words in the class Cci . 3 Experimental Setup In our experiments, we use data from the 2014 ACL Workshop in Machine Translation.2 We train standard phrase-based translation systems for French → English, English → Czech and English → German using the Moses toolkit (Koehn et al., 2007). We used the europarl and the news commentary corpora as parallel data for training 2 The data is available here: http://www.statmt. org/wmt14/translation-task.html. 823 the translation systems. The parallel corpora were tokenized, lowercased and sentences longer than 80 words were removed using standard text processing tools.3 Table 2 contains statistics about the training corpora after the preprocessing step. We tuned the translation systems on the newstest2013 data using minimum error rate training (Och, 2003) and we used the newstest2014 corpora to report uncased BLEU scores averaged over"
N15-1083,W10-3815,0,0.149847,"ntation is a back-off n-gram model with Kneser-Ney smoothing (Chen and Goodman, 1999). Back-off n-gram models are conceptually simple, very efficient to construct and query, and are regarded as being extremely effective in translation systems. Neural language models are a more recent class of language models (Bengio et al., 2003) that have been It has been shown that neural language models can improve translation quality when used as additional features in a decoder (Vaswani et al., 2013; Botha and Blunsom, 2014; Baltescu et al., 2014; Auli and Gao, 2014) or if used for n-best list rescoring (Schwenk, 2010; Auli et al., 2013). These results show great promise and in this paper we continue this line of research by investigating the tradeoff between speed and accuracy when integrating neural language models in a decoder. We also focus on how effective these models are when used as the sole language model in a translation system. This is important because our hypothesis is that most of the language modelling is done by the n-gram model, with the neural model only acting as a differentiating factor when the n-gram model cannot provide a decisive probability. Furthermore, neural language models are"
N15-1083,D13-1140,0,0.102394,"Missing"
N15-1083,J92-4003,0,\N,Missing
N15-1083,P03-1021,0,\N,Missing
N18-1086,D14-1082,0,0.133864,"Missing"
N18-1086,D11-1114,0,0.035734,"Missing"
N18-1086,P81-1022,0,0.448419,"Missing"
N18-1086,P15-1033,0,0.308273,"coded with an RNN, we extract the inputs to the transition, word and relation prediction models across the batch, and then perform the neural network computations in parallel. The supervised models’ training speed is about 3 minutes per epoch. Experiments We follow the standard setup for English dependency parsing, training on sections 2-21 of the Penn Treebank (PTB) Wall Street Journal corpus, using section 22 for development and section 23 for testing. Dependency trees follow the Stanford dependency (SD) representation (version 3.3.0) used in recent parsing research (Chen and Manning, 2014; Dyer et al., 2015). We also report some results using the older representation 1 http://github.com/slavpetrov/ berkeleyparser 2 http://pytorch.org/ 946 Model Greedy Exact Arc-Hybrid uniRNN Arc-Eager uniRNN Arc-Hybrid biRNN Arc-Eager biRNN 84.12/81.54 79.90/77.67 92.85/90.42 92.82/90.63 84.21/81.61 81.37/79.08 92.89/90.47 92.90/90.68 Arc-Hybrid Gen stack-next Arc-Hybrid Gen buffer-next Arc-Eager Gen buffer-next 56.98/52.22 85.25/82.83 80.79/78.56 82.77/78.01 91.19/88.66 87.34/84.84 ROOT Another $ 20 billion would be raised through treasury bonds Figure 2: Sentence with dependencies induced by the unsupervised mo"
N18-1086,N16-1024,0,0.54287,"t al., 2016) or performing synthetic transduction tasks based on context-free grammars (Grefenstette et al., 2015). We propose generative models, based on transition-based dependency parsing (Nivre, 2008), a widely used framework for incremental syntactic parsing, that are able to capture desirable dependencies. Our generative approach to dependency parsing encodes sentences with an RNN and estimate transition and next word probability distributions by conditioning on a small number of features represented by RNN encoder vectors. In contrast to previous syntactic language models such as RNNG (Dyer et al., 2016), marginal word probabilities can be computed both online and exactly. A GPU implementation which exploits parallelization enables unsupervised learning and fast training and decoding. The price of exact inference is that our models are less expressive than RNNG, as the recurrence is not syntax-dependent. Our generative models are based on the arceager and arc-hybrid transition systems, with O(n3 ) dynamic programs based on Kuhlmann et al. (2011). Previous work on dynamic programming for transition-based parsing either required approximate inference due to a too high polynomial order run-time"
N18-1086,W16-5901,0,0.0307551,"Exact decoding is performed with the Viterbi algorithm: At every item [i, j] the highest scoring arc direction is recorded. After the most likely transition sequence is extracted, arc labels are predicted greedily. 1: I(0, 1) ← 1 2: for j = 2, . . . , n do 3: I(j−1, j) ← 1 4: for i = j − 2, . . . , 0 do 5: for k = i + 1, . . . , j − 1 do 6: T (k) ← ptr (sh|hi , hk−1 )pgen (wk |hi , hk−1 ) 7: end for Pj−1 8: I(i, j) ← k=i+1 I(i, k)I(k, j)ptr (re|hk , hj−1 )T (k) 9: end for 10: end for 11: return I(0, n) + ptr (re|h0 , hn−1 ) is equivalent to computing the gradients with the outside algorithm (Eisner, 2016). For decoding we perform Viterbi search over the dynamic program by maximizing rather than summing over different split positions (values of k when reducing). The buffer-next generative formulation, where shift generates the next word β, can also be computed with the dynamic program. Here w1 is predicted at the initial state in I(0, 1), while the endof-sentence token is generated explicitly when a shift action results in buffer being set to position n, regardless of the state of the stack. 3 3.2 The arc-eager parser has four transitions, as defined in Table 2. Shift and right-arc are shift ac"
N18-1086,P16-1231,0,0.0879633,"Missing"
N18-1086,N16-1181,0,0.0717798,"Missing"
N18-1086,P15-1135,0,0.0218215,"obability. The limitation of our approach is that our models cannot learn arc directionality without supervision, so we interpret shift as adding a (right-arc) dependency between top of the stack and the word being generated. In our experiments the model did not succeed in learning informative, non-trivial tree structures – in most cases it learns to attach words either to the immediate previous word or to the root. However, unsupervised dependency parsers usually require elaborate initialization schemes or biases to produce non-trivial trees (Klein and Manning, 2004; Spitkovsky et al., 2010; Bisk and Hockenmaier, 2015). An example dependency tree predicted by the unsupervised model is given in Figure 2. Table 4: PTB test set parsing results with supervised generative models, on the Stanford (SD) and Yamada and Matsumoto (2003) (YM) dependencies. The models from Buys and Blunsom (2015b) and Titov and Henderson (2007) were retrained to make results directly comparable. 4.1 Parsing In order to benchmark parsing performance, we train discriminative baselines using the same feature space as the generative models. Unidirectional or bidirectional RNNs can be used; we see that the bidirectional encoder is crucial f"
N18-1086,P04-1013,0,0.225518,"models Chelba and Jelinek (2000) and Emami and Jelinek (2005) proposed incremental syntactic language models that predict binarized constituency trees with a shift-reduce model, parameterized by interpolated n-gram smoothing and feed-forward neural networks, respectively. Language modelling probabilities were approximated incrementally using beam-search. Rastrow et al. (2012) applied a transition-based dependency n-gram language model to speech recognition. These models obtained perplexity improvements primarily when interpolated with standard n-gram models, and were not employed as parsers. Henderson (2004) proposed an incremental constituency parser based on recurrent neural networks that have additional connections to previous recurrent states based on the parser configuration at each time step. The generative version of this model was more accurate than the discriminative one. Titov and Henderson (2007) applied a similar approach to dependency parsing. Buys and Blunsom (2015a) and Buys and Blunsom (2015b) proposed generative syntactic models that are applied to both dependency parsing and language modelling, using Bayesian and feed-forward neural networks, respectively. Recurrent Neural Netwo"
N18-1086,W15-2108,1,0.45979,"do 6: for i = 0, . . . , n − gap do 7: j = i + gap 8: for c = 0, 1 do 9: for k = i + 1, . . . , j − 1 do 10: if j &gt; 0 then 11: W (k) ← ptr (sh|hi , hk−1 ) 12: ·pdir (ra|hi , hk−1 )pgen (wk |hi , hk−1 ) 13: T (k) ← I(k1 , j)ptr (re|hk , hj−1 )W (k) 14: end if 15: if j < n then 16: V (k) ← ptr (sh|hi , hk−1 ) 17: ·pdir (la|hi , hk−1 )pgen (wk |hi , hk−1 ) 18: T (k) ← T (k) I(k0 , j)ptr (re|hk , hj−1 ))V (k) 19: end if 20: end for Pj−1 c 21: I(ic , j) ← k=i+1 I(i , k)T (k) 22: end for 23: end for 24: end for 25: return I(0, n) + ptr (re|h0 , hn−1 ) of Yamada and Matsumoto (2003) (YM). We follow Buys and Blunsom (2015b) and Dyer et al. (2016) in replacing training singletons and unknown words in the test set with unknown word class tokens based to their surface forms, following the rules implemented in the Berkeley parser.1 Our models are implemented in PyTorch, which constructs computation graphs dynamically.2 During training, sentences are shuffled at each epoch, and minibatches are constructed of sentences of the same length. We base the hyperparameters of our models primarily on the language models of Zaremba et al. (2014). Models are based on twolayer LSTMs with embedding and hidden state size 650 wit"
N18-1086,P15-2142,1,0.636193,"do 6: for i = 0, . . . , n − gap do 7: j = i + gap 8: for c = 0, 1 do 9: for k = i + 1, . . . , j − 1 do 10: if j &gt; 0 then 11: W (k) ← ptr (sh|hi , hk−1 ) 12: ·pdir (ra|hi , hk−1 )pgen (wk |hi , hk−1 ) 13: T (k) ← I(k1 , j)ptr (re|hk , hj−1 )W (k) 14: end if 15: if j < n then 16: V (k) ← ptr (sh|hi , hk−1 ) 17: ·pdir (la|hi , hk−1 )pgen (wk |hi , hk−1 ) 18: T (k) ← T (k) I(k0 , j)ptr (re|hk , hj−1 ))V (k) 19: end if 20: end for Pj−1 c 21: I(ic , j) ← k=i+1 I(i , k)T (k) 22: end for 23: end for 24: end for 25: return I(0, n) + ptr (re|h0 , hn−1 ) of Yamada and Matsumoto (2003) (YM). We follow Buys and Blunsom (2015b) and Dyer et al. (2016) in replacing training singletons and unknown words in the test set with unknown word class tokens based to their surface forms, following the rules implemented in the Berkeley parser.1 Our models are implemented in PyTorch, which constructs computation graphs dynamically.2 During training, sentences are shuffled at each epoch, and minibatches are constructed of sentences of the same length. We base the hyperparameters of our models primarily on the language models of Zaremba et al. (2014). Models are based on twolayer LSTMs with embedding and hidden state size 650 wit"
N18-1086,P10-1110,0,0.0366811,"ord probabilities can be computed both online and exactly. A GPU implementation which exploits parallelization enables unsupervised learning and fast training and decoding. The price of exact inference is that our models are less expressive than RNNG, as the recurrence is not syntax-dependent. Our generative models are based on the arceager and arc-hybrid transition systems, with O(n3 ) dynamic programs based on Kuhlmann et al. (2011). Previous work on dynamic programming for transition-based parsing either required approximate inference due to a too high polynomial order run-time complexity (Huang and Sagae, 2010), or had too restrictive feature spaces to be used as accurate models (Kuhlmann et al., 2011; Cohen et al., 2011). Recent work showed that bidirectional RNNs enable accurate graphbased and transition-based dependency parsing using minimal feature spaces (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016; Dozat and Manning, 2017). Shi et al. (2017) further showed that under this approach exact decoding We present neural syntactic generative models with exact marginalization that support both dependency parsing and language modeling. Exact marginalization is made tractable through dynamic pr"
N18-1086,Q16-1023,0,0.0282941,"syntax-dependent. Our generative models are based on the arceager and arc-hybrid transition systems, with O(n3 ) dynamic programs based on Kuhlmann et al. (2011). Previous work on dynamic programming for transition-based parsing either required approximate inference due to a too high polynomial order run-time complexity (Huang and Sagae, 2010), or had too restrictive feature spaces to be used as accurate models (Kuhlmann et al., 2011; Cohen et al., 2011). Recent work showed that bidirectional RNNs enable accurate graphbased and transition-based dependency parsing using minimal feature spaces (Kiperwasser and Goldberg, 2016; Cross and Huang, 2016; Dozat and Manning, 2017). Shi et al. (2017) further showed that under this approach exact decoding We present neural syntactic generative models with exact marginalization that support both dependency parsing and language modeling. Exact marginalization is made tractable through dynamic programming over shiftreduce parsing and minimal RNN-based feature sets. Our algorithms complement previous approaches by supporting batched training and enabling online computation of next word probabilities. For supervised dependency parsing, our model achieves a stateof-the-art resul"
N18-1086,P04-1061,0,0.112576,"er by directly optimizing the marginal sentence probability. The limitation of our approach is that our models cannot learn arc directionality without supervision, so we interpret shift as adding a (right-arc) dependency between top of the stack and the word being generated. In our experiments the model did not succeed in learning informative, non-trivial tree structures – in most cases it learns to attach words either to the immediate previous word or to the root. However, unsupervised dependency parsers usually require elaborate initialization schemes or biases to produce non-trivial trees (Klein and Manning, 2004; Spitkovsky et al., 2010; Bisk and Hockenmaier, 2015). An example dependency tree predicted by the unsupervised model is given in Figure 2. Table 4: PTB test set parsing results with supervised generative models, on the Stanford (SD) and Yamada and Matsumoto (2003) (YM) dependencies. The models from Buys and Blunsom (2015b) and Titov and Henderson (2007) were retrained to make results directly comparable. 4.1 Parsing In order to benchmark parsing performance, we train discriminative baselines using the same feature space as the generative models. Unidirectional or bidirectional RNNs can be us"
N18-1086,D17-1002,0,0.0282344,"Missing"
N18-1086,P11-1068,0,0.0611878,"Missing"
N18-1086,E17-1117,0,0.174267,"Missing"
N18-1086,N10-1116,0,0.0203916,"the marginal sentence probability. The limitation of our approach is that our models cannot learn arc directionality without supervision, so we interpret shift as adding a (right-arc) dependency between top of the stack and the word being generated. In our experiments the model did not succeed in learning informative, non-trivial tree structures – in most cases it learns to attach words either to the immediate previous word or to the root. However, unsupervised dependency parsers usually require elaborate initialization schemes or biases to produce non-trivial trees (Klein and Manning, 2004; Spitkovsky et al., 2010; Bisk and Hockenmaier, 2015). An example dependency tree predicted by the unsupervised model is given in Figure 2. Table 4: PTB test set parsing results with supervised generative models, on the Stanford (SD) and Yamada and Matsumoto (2003) (YM) dependencies. The models from Buys and Blunsom (2015b) and Titov and Henderson (2007) were retrained to make results directly comparable. 4.1 Parsing In order to benchmark parsing performance, we train discriminative baselines using the same feature space as the generative models. Unidirectional or bidirectional RNNs can be used; we see that the bidir"
N18-1086,Q16-1037,0,0.0558084,"Missing"
N18-1086,W07-2218,0,0.373989,"92.82/90.63 84.21/81.61 81.37/79.08 92.89/90.47 92.90/90.68 Arc-Hybrid Gen stack-next Arc-Hybrid Gen buffer-next Arc-Eager Gen buffer-next 56.98/52.22 85.25/82.83 80.79/78.56 82.77/78.01 91.19/88.66 87.34/84.84 ROOT Another $ 20 billion would be raised through treasury bonds Figure 2: Sentence with dependencies induced by the unsupervised model. Table 3: PTB development set parsing results (SD dependencies), reporting unlabelled and labelled attachment scores (UAS/LAS). Discriminative models (above the line) use either unidirectional or bidirectional RNNs. Model SD YM Buys and Blunsom (2015b) Titov and Henderson (2007) 90.10/87.74 91.43/89.02 90.16/88.83 90.75/89.29 Arc-eager Arc-hybrid 88.20/85.91 91.01/88.54 87.61/86.36 90.71/88.68 the generative models (Table 3). They are much more accurate than the unidirectional discriminative models, which shows that the word prediction model benefits parsing accuracy. The arc-hybrid model is more accurate than arc-eager, as was the case for the unidirectional discriminative models. This can be explained by arc-eager making attachment decisions earlier in the transition sequence than arc-hybrid, which means that it has access to less context to condition these predict"
N18-1086,W16-5907,0,0.0221359,"al memory, including stacks and other data structures that are operated on with differentiable operations to enable end-to-end learning. Neural Turing machines (Graves et al., 2014) have readwrite memory that is updated at each timestep. Grefenstette et al. (2015) proposed a neural stack that is operated on with differentiable push and pop computations. Another strand of recent work which our models are related to has proposed neural models with structured latent variables: Rastogi et al. (2016) incorporated neural context into weighted finite-state transducers with a bidirectional RNN, while Tran et al. (2016) proposed a neural hidden Markov model for Part-of-Speech (POS) induction. Yu et al. (2016) proposed a neural transduction model with polynomial-time inference where the alignment is a latent variable. Kim et al. (2017) proposed structured attention mechanisms that compute features by taking expectations over latent structure. They define a tree-structured model with a latent variable for head selection, 949 along with projectivity constraints. The soft head selection learned by the model is used as features in an attention-based decoder. Reinforcement learning has been proposed to learn compo"
N18-1086,P15-2084,0,0.0812373,"educe stack-next Unsupervised shift-reduce stack-next 111.53 107.61 125.20 120.09 119.20 350.01 102.28 71.27 169.87 Table 6: Language modelling perplexity analysis on the PTB test set. tive transition-based constituency parser based on stack LSTMs (Dyer et al., 2015), that was also applied as a language model. Recently, Shen et al. (2017) proposed an RNNbased language model that uses a soft gating mechanism to learn structure that can be interpreted as constituency trees, reporting strong language modelling performance. There has also been work on non-incremental syntactic language modelling: Mirowski and Vlachos (2015) proposed a dependency neural language model where each word is conditioned on its ancestors in the dependency tree, and showed that this model achieves strong performance on a sentence completion task. the stack and the partially constructed parse tree, while our model can only make use of the position on top of the stack and otherwise has to rely on the sequentially computed RNN representations. The disadvantage of RNNG is that inference can only be performed over entire sentences, as the proposal distribution for their importance sampling method is a discriminative parser. Exact inference a"
N18-1086,J08-4003,0,0.175029,"1 Department of Computer Science, University of Oxford 2 Paul G. Allen School of Computer Science & Engineering, University of Washington 3 DeepMind jbuys@cs.washington.edu, phil.blunsom@cs.ox.ac.uk Abstract driven strong improvements in intrinsic language modelling performance, they fail at capturing certain long-distance dependencies, such as those required for modelling subject-verb agreement (Linzen et al., 2016) or performing synthetic transduction tasks based on context-free grammars (Grefenstette et al., 2015). We propose generative models, based on transition-based dependency parsing (Nivre, 2008), a widely used framework for incremental syntactic parsing, that are able to capture desirable dependencies. Our generative approach to dependency parsing encodes sentences with an RNN and estimate transition and next word probability distributions by conditioning on a small number of features represented by RNN encoder vectors. In contrast to previous syntactic language models such as RNNG (Dyer et al., 2016), marginal word probabilities can be computed both online and exactly. A GPU implementation which exploits parallelization enables unsupervised learning and fast training and decoding. T"
N18-1086,W03-3023,0,0.581967,"+1) ← 1 4: end for 5: for gap = 2, . . . , n do 6: for i = 0, . . . , n − gap do 7: j = i + gap 8: for c = 0, 1 do 9: for k = i + 1, . . . , j − 1 do 10: if j &gt; 0 then 11: W (k) ← ptr (sh|hi , hk−1 ) 12: ·pdir (ra|hi , hk−1 )pgen (wk |hi , hk−1 ) 13: T (k) ← I(k1 , j)ptr (re|hk , hj−1 )W (k) 14: end if 15: if j < n then 16: V (k) ← ptr (sh|hi , hk−1 ) 17: ·pdir (la|hi , hk−1 )pgen (wk |hi , hk−1 ) 18: T (k) ← T (k) I(k0 , j)ptr (re|hk , hj−1 ))V (k) 19: end if 20: end for Pj−1 c 21: I(ic , j) ← k=i+1 I(i , k)T (k) 22: end for 23: end for 24: end for 25: return I(0, n) + ptr (re|h0 , hn−1 ) of Yamada and Matsumoto (2003) (YM). We follow Buys and Blunsom (2015b) and Dyer et al. (2016) in replacing training singletons and unknown words in the test set with unknown word class tokens based to their surface forms, following the rules implemented in the Berkeley parser.1 Our models are implemented in PyTorch, which constructs computation graphs dynamically.2 During training, sentences are shuffled at each epoch, and minibatches are constructed of sentences of the same length. We base the hyperparameters of our models primarily on the language models of Zaremba et al. (2014). Models are based on twolayer LSTMs with"
N18-1086,D16-1138,1,0.744696,"e operations to enable end-to-end learning. Neural Turing machines (Graves et al., 2014) have readwrite memory that is updated at each timestep. Grefenstette et al. (2015) proposed a neural stack that is operated on with differentiable push and pop computations. Another strand of recent work which our models are related to has proposed neural models with structured latent variables: Rastogi et al. (2016) incorporated neural context into weighted finite-state transducers with a bidirectional RNN, while Tran et al. (2016) proposed a neural hidden Markov model for Part-of-Speech (POS) induction. Yu et al. (2016) proposed a neural transduction model with polynomial-time inference where the alignment is a latent variable. Kim et al. (2017) proposed structured attention mechanisms that compute features by taking expectations over latent structure. They define a tree-structured model with a latent variable for head selection, 949 along with projectivity constraints. The soft head selection learned by the model is used as features in an attention-based decoder. Reinforcement learning has been proposed to learn compositional tree-based representations in the context of an end task (Andreas et al., 2016; Yo"
P06-1009,J93-2003,0,0.0416268,"Missing"
P06-1009,P04-1023,0,0.152922,"recall increased from 89.2 to 92.4. This resulted in an overall decrease in AER to 6.99. We found no benefit from using many-tomany possible alignments as they added a significant amount of noise to the data. 4.2 Table 4. Results using features from Model 4 bidirectional alignments, training with and without the possible (P) alignments. model precision recall f-score AER Fre ↔ Eng 94.6 92.2 93.4 6.47 Fre ↔ Eng (Model 4) 96.1 93.3 94.7 5.19 Table 5. 10-fold cross-validation results, with and without Model 4 features. vised Model 4 did not have access to the wordalignments in our training set. Callison-Burch et al. (2004) demonstrated that the GIZA++ models could be trained in a semi-supervised manner, leading to a slight decrease in error. To our knowledge, our AER of 5.19 is the best reported result, generative or discriminative, on this data set. Model 4 as a feature 5 Previous work (Taskar et al., 2005) has demonstrated that by including the output of Model 4 as a feature, it is possible to achieve a significant decrease in AER. We trained Model 4 in both directions on the two language pairs. We added two indicator features (one for each direction) to our CRF which were active if a given word pair were ali"
P06-1009,H05-1012,0,0.146534,"tup as our work, they achieve an AER of 5.4 with Model 4 features, and 10.7 without (compared to 5.29 and 6.99 for our CRF). One of the strengths of the CRF MAP estimation is the powerful smoothing offered by the prior, which allows us to avoid heuristics such as early stopping and hand weighted loss-functions that were needed for the maximum-margin model. Liu et al. (2005) used a conditional log-linear model with similar features to those we have employed. They formulated a global model, without making a Markovian assumption, leading to the need for a sub-optimal heuristic search strategies. Ittycheriah and Roukos (2005) trained a disCross-validation Using 10-fold cross-validation we are able to generate results on the whole of the Hansards test data which are comparable to previously published results. As the sentences in the test set were randomly chosen from the training corpus we can expect cross-validation to give an unbiased estimate of generalisation performance. These results are displayed in Table 5, using the possible (P) alignments for training. As the training set for each fold is roughly four times as big previous training set, we see a small improvement in AER. The final results of 6.47 and 5.19"
P06-1009,N03-1017,0,0.00803004,"d word-aligned training sentences, our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively. 1 Introduction Modern phrase based statistical machine translation (SMT) systems usually break the translation task into two phases. The first phase induces word alignments over a sentence-aligned bilingual corpus, and the second phase uses statistics over these predicted word alignments to decode (translate) novel sentences. This paper deals with the first of these tasks: word alignment. Most current SMT systems (Och and Ney, 2004; Koehn et al., 2003) use a generative model for word alignment such as the freely available 1 We adopt the standard notation of e and f to denote the target (English) and source (foreign) sentences, respectively. 65 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 65–72, c Sydney, July 2006. 2006 Association for Computational Linguistics rithms are tractable and efficient, thereby avoiding the need for heuristics. The CRF is conditioned on both the source and target sentences, and therefore supports large sets of diverse and overlapping featur"
P06-1009,P05-1057,0,0.367771,"e if a given word pair were aligned in the Model 4 output. Table 4 displays the results on both language pairs when these additional features are used with the refined model. This produces a large increase in performance, and when including the possibles, produces AERs of 5.29 and 25.8, both well below that of Model 4 alone (shown in Tables 1 and 2). 4.3 Related work Recently, a number of discriminative word alignment models have been proposed, however these early models are typically very complicated with many proposing intractable problems which require heuristics for approximate inference (Liu et al., 2005; Moore, 2005). An exception is Taskar et al. (2005) who presented a word matching model for discriminative alignment which they they were able to solve optimally. However, their model is limited to only providing one-to-one alignments. Also, no features were defined on label sequences, which reduced the model’s ability to capture the strong monotonic relationships present between European language pairs. On the French-English Hansards task, using the same training/testing setup as our work, they achieve an AER of 5.4 with Model 4 features, and 10.7 without (compared to 5.29 and 6.99 for our C"
P06-1009,W02-2018,0,0.025776,"et word indexed by at , rather than the index itself, which determines whether the feature is active, and thus the sparsity of the index label set is not an issue. λk hk (t, at−1 , at , e, f ) k − log ZΛ (e, f ) − X λ2 k + const. 2σk2 (2) k 3.1 In order to train the model, we maximize (2). While the log-likelihood cannot be maximised for the parameters, Λ, in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters. We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and Pereira, 2003). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using forward-backward inference, which yields the partition function, ZΛ (e, f ), required for the log-likelihood, and the pair-wise marginals, pΛ (at−1 , at |e, f ), required for its derivatives. The Viterbi algorithm is used to find the maximum posterior probability alignment for test sentences, a∗ = arg maxa pΛ (a|e, f ). Both the forward-backward and Viterbi algorithm are dynamic programs which make use of the Markov assumption to calcula"
P06-1009,W05-0809,0,0.0738309,"iguous or idiomatic alignments. We measure the performance of our model using alignment error rate (AER), which is defined as: Table 1. Results on the Hansard data using all features model precision recall f-score AER Model 4 refined 80.49 64.10 71,37 28.63 Model 4 intersected 95.94 53.56 68.74 31.26 Romanian → English 82.9 61.3 70.5 29.53 English → Romanian 82.8 60.6 70.0 29.98 intersection 94.4 52.5 67.5 32.45 refined 77.1 68.5 72.6 27.41 AER(A, S, P ) = 1 − where A is the set of predicted alignments. The second data set is the Romanian-English parallel corpus from the 2005 ACL shared task (Martin et al., 2005). This consists of approximately 50,000 aligned sentences and 448 wordaligned sentences, which are split into a 248 sentence trial set and a 200 sentence test set. We used these as our training and test sets, respectively. For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003). For this task we have used the same test data as the competition entrants, and therefore can directly compare our results. The word alignments in this corpus were only annotated with sure (S) alignments, and therefore the AER is equiv"
P06-1009,W03-0301,0,0.347465,"nian 82.8 60.6 70.0 29.98 intersection 94.4 52.5 67.5 32.45 refined 77.1 68.5 72.6 27.41 AER(A, S, P ) = 1 − where A is the set of predicted alignments. The second data set is the Romanian-English parallel corpus from the 2005 ACL shared task (Martin et al., 2005). This consists of approximately 50,000 aligned sentences and 448 wordaligned sentences, which are split into a 248 sentence trial set and a 200 sentence test set. We used these as our training and test sets, respectively. For parameter tuning, we used the 17 sentence trial set from the Romanian-English corpus in the 2003 NAACL task (Mihalcea and Pedersen, 2003). For this task we have used the same test data as the competition entrants, and therefore can directly compare our results. The word alignments in this corpus were only annotated with sure (S) alignments, and therefore the AER is equivalent to the F1 score. In the shared task it was found that models which were trained on only the first four letters of each word obtained superior results to those using the full words (Martin et al., 2005). We observed the same result with our model on the trial set and thus have only used the first four letters when training the Dice and Model 1 translation p"
P06-1009,H05-1011,0,0.407777,"pair were aligned in the Model 4 output. Table 4 displays the results on both language pairs when these additional features are used with the refined model. This produces a large increase in performance, and when including the possibles, produces AERs of 5.29 and 25.8, both well below that of Model 4 alone (shown in Tables 1 and 2). 4.3 Related work Recently, a number of discriminative word alignment models have been proposed, however these early models are typically very complicated with many proposing intractable problems which require heuristics for approximate inference (Liu et al., 2005; Moore, 2005). An exception is Taskar et al. (2005) who presented a word matching model for discriminative alignment which they they were able to solve optimally. However, their model is limited to only providing one-to-one alignments. Also, no features were defined on label sequences, which reduced the model’s ability to capture the strong monotonic relationships present between European language pairs. On the French-English Hansards task, using the same training/testing setup as our work, they achieve an AER of 5.4 with Model 4 features, and 10.7 without (compared to 5.29 and 6.99 for our CRF). One of th"
P06-1009,J03-1002,0,0.361359,"erence on Computational Linguistics and 44th Annual Meeting of the ACL, pages 65–72, c Sydney, July 2006. 2006 Association for Computational Linguistics rithms are tractable and efficient, thereby avoiding the need for heuristics. The CRF is conditioned on both the source and target sentences, and therefore supports large sets of diverse and overlapping features. Furthermore, the model allows regularisation using a prior over the parameters, a very effective and simple method for limiting over-fitting. We use a similar graphical structure to the directed hidden Markov model (HMM) from GIZA++ (Och and Ney, 2003). This models one-to-many alignments, where each target word is aligned with zero or more source words. Many-to-many alignments are recoverable using the standard techniques for superimposing predicted alignments in both translation directions. The paper is structured as follows. Section 2 presents CRFs for word alignment, describing their form and their inference techniques. The features of our model are presented in Section 3, and experimental results for word aligning both French-English and Romanian-English sentences are given in Section 4. Section 5 presents related work, and we describe"
P06-1009,J04-4002,0,0.0152164,"h only a few hundred word-aligned training sentences, our model improves over the current state-ofthe-art with alignment error rates of 5.29 and 25.8 for the two tasks respectively. 1 Introduction Modern phrase based statistical machine translation (SMT) systems usually break the translation task into two phases. The first phase induces word alignments over a sentence-aligned bilingual corpus, and the second phase uses statistics over these predicted word alignments to decode (translate) novel sentences. This paper deals with the first of these tasks: word alignment. Most current SMT systems (Och and Ney, 2004; Koehn et al., 2003) use a generative model for word alignment such as the freely available 1 We adopt the standard notation of e and f to denote the target (English) and source (foreign) sentences, respectively. 65 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 65–72, c Sydney, July 2006. 2006 Association for Computational Linguistics rithms are tractable and efficient, thereby avoiding the need for heuristics. The CRF is conditioned on both the source and target sentences, and therefore supports large sets of diverse a"
P06-1009,N03-1028,0,0.028075,"d by at , rather than the index itself, which determines whether the feature is active, and thus the sparsity of the index label set is not an issue. λk hk (t, at−1 , at , e, f ) k − log ZΛ (e, f ) − X λ2 k + const. 2σk2 (2) k 3.1 In order to train the model, we maximize (2). While the log-likelihood cannot be maximised for the parameters, Λ, in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters. We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and Pereira, 2003). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using forward-backward inference, which yields the partition function, ZΛ (e, f ), required for the log-likelihood, and the pair-wise marginals, pΛ (at−1 , at |e, f ), required for its derivatives. The Viterbi algorithm is used to find the maximum posterior probability alignment for test sentences, a∗ = arg maxa pΛ (a|e, f ). Both the forward-backward and Viterbi algorithm are dynamic programs which make use of the Markov assumption to calculate efficiently the exact"
P06-1009,H05-1010,0,0.803732,"a single indicator feature which detects when the source and target words appear in an entry of the dictionary. For the English-French dictionary we used FreeDict,3 which contains 8,799 English words. For Romanian-English we used a dictionary compiled by Rada Mihalcea,4 which contains approximately 38,000 entries. Orthographic features Features based on string overlap allow our model to recognise cognates and orthographically similar translation pairs, which are particularly common between European languages. Here we employ a number of string matching features inspired by similar features in Taskar et al. (2005). We use an indicator feature for every possible source-target word pair in the training data. In addition, we include indicator features for an exact string match, both with and without vowels, and the edit-distance between the source and target words as a realvalued feature. We also used indicator features to test for matching prefixes and suffixes of length three. As stated earlier, the Dice translation score often erroneously rewards alignments with common words. In order to address this problem, we include the absolute difference in word length as a real-valued feature and an indicator fe"
P06-1009,W02-1012,0,0.0864874,"o incorporate non-independent features over the sentence pairs. For instance, as well as detecting that a source word is aligned to a given target word, we would also like to encode syntactic and lexical features of the word pair, such as their partsof-speech, affixes, lemmas, etc. Features such as these would allow for more effective use of sparse data and result in a model which is more robust in the presence of unseen words. Adding these non-independent features to a generative model requires that the features’ inter-dependence be modelled explicitly, which often complicates the model (eg. Toutanova et al. (2002)). Secondly, the later IBM models, such as Model 4, have to resort to heuristic search techniques to approximate forward-backward and Viterbi inference, which sacrifice optimality for tractability. This paper presents an alternative discriminative method for word alignment. We use a conditional random field (CRF) sequence model, which allows for globally optimal training and decoding (Lafferty et al., 2001). The inference algoIn this paper we present a novel approach for inducing word alignments from sentence aligned data. We use a Conditional Random Field (CRF), a discriminative model, which"
P06-1009,C96-2141,0,0.882345,"ent lengths – i.e. aligning rare (long) words with frequent (short) determiners, verbs etc. Markov features Features defined over adjacent aligment labels allow our model to reflect the tendency for monotonic alignments between European languages. We define a real-valued alignment index jump width feature: jump width(t − 1, t) = abs(at − at−1 − 1) this feature has a value of 0 if the alignment labels follow the downward sloping diagonal, and is positive otherwise. This differs from the GIZA++ hidden Markov model which has individual parameters for each different jump width (Och and Ney, 2003; Vogel et al., 1996): we found a single feature (and thus parameter) to be more effective. We also defined three indicator features over null transitions to allow the modelling of the probability of transition between, to and from null labels. Relative sentence postion A feature for the absolute difference in relative sentence position at (abs( |e| − |ft |)) allows the model to learn a preference for aligning words close to the alignment matrix diagonal. We also included two conjunction features for the relative sentence position multiplied by the Dice and Model 1 translation scores. POS tags Part-of-speech tags"
P08-1024,P06-1009,1,0.580917,"Missing"
P08-1024,D07-1007,0,0.0256813,"training sentences Figure 6. Learning curve showing that the model continues to improve as we increase the number of training sentences (development set) Our model avoids the estimation biases associated with heuristic frequency count approaches and uses standard regularisation techniques to avoid degenerate maximum likelihood solutions. Having demonstrated the efficacy of our model with very simple features, the logical next step is to investigate more expressive features. Promising features might include those over source side reordering rules (Wang et al., 2007) or source context features (Carpuat and Wu, 2007). Rule frequency features extracted from large training corpora would help the model to overcome the issue of unreachable reference sentences. Such approaches have been shown to be effective in log-linear wordalignment models where only a small supervised corpus is available (Blunsom and Cohn, 2006). Finally, while in this paper we have focussed on the science of discriminative machine translation, we believe that with suitable engineering this model will advance the state-of-the-art. To do so would require integrating a language model feature into the max-translation decoding algorithm. The u"
P08-1024,P05-1033,0,0.482695,"ns are analogous to the empirical observation of maximum entropy classifiers. Given these two charts we can calculate the loglikelihood of the reference translation as the insidescore from the sentence spanning cell of the reference chart, normalised by the inside-score of the spanning cell from the full chart. The gradient is calculated as the difference of the feature expectations of the two charts. Clark and Curran (2004) provides a more complete discussion of parsing with a loglinear model and latent variables. The full derivation chart is produced using a CYK parser in the same manner as Chiang (2005), and has complexity O(|e|3 ). We produce the reference chart by synchronously parsing the source and reference sentences using a variant of CYK algorithm over two dimensions, with a time complexity of O(|e|3 |f |3 ). This is an instance of the ITG alignment algorithm (Wu, 1997). This step requires the reference translation for each training instance to be contained in the model’s hypothesis space. Achieving full coverage implies inducing a grammar which generates all observed source-target pairs, which is difficult in practise. Instead we discard the unreachable portion of the training sample"
P08-1024,J07-2003,0,0.866734,"rivation. However, doing so exactly is NP-complete. For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely (Ittycheriah and Roukos, 2007; Watanabe et al., 2007), or else ignore the problem and treat derivations as translations (Liang et al., 2006; Tillmann and Zhang, 2007). In this paper we directly address the problem of spurious ambiguity in discriminative models. We use a synchronous context free grammar (SCFG) translation system (Chiang, 2007), a model which has yielded state-of-the-art results on many translation tasks. We present two main contributions. First, we develop a log-linear model of translation which is globally trained on a significant number of parallel sentences. This model maximises the conditional likelihood of the data, p(e|f ), where e and f are the English and foreign sentences, respectively. Our estimation method is theoretically sound, avoiding the biases of the heuristic relative frequency estimates 200 Proceedings of ACL-08: HLT, pages 200–208, c Columbus, Ohio, USA, June 2008. 2008 Association for Computati"
P08-1024,P04-1014,0,0.0239743,"ng sentences, D = {(e, f )}, penalised using a prior, i.e., ΛM AP = arg maxΛ pΛ (D)p(Λ). We use a zero-mean Gaussian prior with the probability  density function p0 (λk ) ∝ exp −λ2k /2σ 2 .2 This results in the following log-likelihood objective and corresponding gradient: X X L= log pΛ (e|f ) + log p0 (λk ) (4) (e,f )∈D k λk ∂L = EpΛ (d|e,f ) [hk ] − EpΛ (e|f ) [hk ] − 2 ∂λk σ (5) In order to train the model, we maximise equation (4) using L-BFGS (Malouf, 2002; Sha and Pereira, 2003). This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using inside-outside inference over the feature forest defined by the SCFG parse chart of f yielding the partition function, ZΛ (f ), required for the log-likelihood, and the marginals, required for its derivatives. Efficiently calculating the objective and its gradient requires two separate packed charts, each representing a derivation forest. The first one is the full chart over the space of possible derivations given the 2 In general, any con"
P08-1024,J07-4004,0,0.0170903,"slation given the source is the sum over all of its derivations: X pΛ (e|f ) = pΛ (d, e|f ) (3) d∈∆(e,f ) where ∆(e, f ) is the set of all derivations of the target sentence e from the source f. Most prior work in SMT, both generative and discriminative, has approximated the sum over derivations by choosing a single ‘best’ derivation using a Viterbi or beam search algorithm. In this work we show that it is both tractable and desirable to directly account for derivational ambiguity. Our findings echo those observed for latent variable log-linear models successfully used in monolingual parsing (Clark and Curran, 2007; Petrov et al., 2007). These models marginalise over derivations leading to a dependency structure and splits of non-terminal categories in a PCFG, respectively. 3.2 Training The parameters of our model are estimated from our training sample using a maximum a posteriori (MAP) estimator. This maximises the likelihood of the parallel training sentences, D = {(e, f )}, penalised using a prior, i.e., ΛM AP = arg maxΛ pΛ (D)p(Λ). We use a zero-mean Gaussian prior with the probability  density function p0 (λk ) ∝ exp −λ2k /2σ 2 .2 This results in the following log-likelihood objective and correspo"
P08-1024,W06-3105,0,0.716213,"odels must also allow for efficient training. Past work on discriminative SMT only address some of these problems. To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007), or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006). These systems all include regularisation, thereby addressing Problem 2. An interesting counterpoint is the work of DeNero et al. (2006), who show that their unregularised model finds degenerate solutions. Some of these discriminative systems have been trained on large training sets (Problem 3); these systems are the local models, for which training is much simpler. Both the global models (Liang et al., 2006; Watanabe et al., 2007) use fairly small training sets, and there is no evidence that their techniques will scale to larger data sets. Our model addresses all three of the above problems within a global model, without resorting to nbest lists or local independence assumptions. Furthermore, our model explicitly accounts for"
P08-1024,N04-1035,0,0.0260784,"Missing"
P08-1024,P06-1121,0,0.0566119,"source and target languages to the respective sides of a SCFG it is possible to describe translation as the process of parsing the source sentence using a CFG, while generating the target translation from 202 the other (Chiang, 2007). All the models we present use the grammar extraction technique described in Chiang (2007), and are bench-marked against our own implementation of this hierarchical model (Hiero). Figure 3 shows a simple instance of a hierarchical grammar with two non-terminals. Note that our approach is general and could be used with other synchronous grammar transducers (e.g., Galley et al. (2006)). 3.1 A global log-linear model Our log-linear translation model defines a conditional probability distribution over the target translations of a given source sentence. A particular sequence of SCFG rule applications which produces a translation from a source sentence is referred to as a derivation, and each translation may be produced by many different derivations. As the training data only provides source and target sentences, the derivations are modelled as a latent variable. The conditional probability of a derivation, d, for a target translation, e, conditioned on the source, f , is give"
P08-1024,N07-1008,0,0.122961,"les, depending on the type of system. Existing discriminative models require a reference derivation to optimise against, however no parallel corpora annotated for derivations exist. Ideally, a model would account for this ambiguity by marginalising out the derivations, thus predicting the best translation rather than the best derivation. However, doing so exactly is NP-complete. For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely (Ittycheriah and Roukos, 2007; Watanabe et al., 2007), or else ignore the problem and treat derivations as translations (Liang et al., 2006; Tillmann and Zhang, 2007). In this paper we directly address the problem of spurious ambiguity in discriminative models. We use a synchronous context free grammar (SCFG) translation system (Chiang, 2007), a model which has yielded state-of-the-art results on many translation tasks. We present two main contributions. First, we develop a log-linear model of translation which is globally trained on a significant number of parallel sentences. This model maximises the conditional likeliho"
P08-1024,N03-1017,0,0.0355966,"uch problems as non-literal translations, poor sentence- and word-alignments. A model which exactly translates the training data will inevitably perform poorly on held-out data. This problem of over-fitting is exacerbated in discriminative models with large, expressive, feature sets. Regularisation is essential for models with more than a handful of features. ● 1e+05 derivations ● ● ● 1e+03 ● ● ● ● 5 7 9 11 13 15 sentence length Figure 1. Exponential relationship between sentence length and the average number of derivations (on a log scale) for each reference sentence in our training corpus. (Koehn et al., 2003). Second, within this framework, we model the derivation, d, as a latent variable, p(e, d|f ), which is marginalised out in training and decoding. We show empirically that this treatment results in significant improvements over a maximum-derivation model. The paper is structured as follows. In Section 2 we list the challenges that discriminative SMT must face above and beyond the current systems. We situate our work, and previous work, on discriminative systems in this context. We present our model in Section 3, including our means of training and decoding. Section 4 reports our experimental s"
P08-1024,P06-1096,0,0.694261,"Missing"
P08-1024,W02-2018,0,0.0254207,"ur model are estimated from our training sample using a maximum a posteriori (MAP) estimator. This maximises the likelihood of the parallel training sentences, D = {(e, f )}, penalised using a prior, i.e., ΛM AP = arg maxΛ pΛ (D)p(Λ). We use a zero-mean Gaussian prior with the probability  density function p0 (λk ) ∝ exp −λ2k /2σ 2 .2 This results in the following log-likelihood objective and corresponding gradient: X X L= log pΛ (e|f ) + log p0 (λk ) (4) (e,f )∈D k λk ∂L = EpΛ (d|e,f ) [hk ] − EpΛ (e|f ) [hk ] − 2 ∂λk σ (5) In order to train the model, we maximise equation (4) using L-BFGS (Malouf, 2002; Sha and Pereira, 2003). This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using inside-outside inference over the feature forest defined by the SCFG parse chart of f yielding the partition function, ZΛ (f ), required for the log-likelihood, and the marginals, required for its derivatives. Efficiently calculating the objective and its gradient requires two separate"
P08-1024,P03-1021,0,0.184772,"x-inspired approaches. Progress within these approaches however has been less dramatic. We believe this is because these frequency count based1 models cannot easily incorporate non-independent and overlapping features, which are extremely useful in describing the translation process. Discriminative models of translation can include such features without making assumptions of independence or explicitly modelling their interdependence. However, while discriminative models promise much, they have not been shown to deliver significant gains 1 We class approaches using minimum error rate training (Och, 2003) frequency count based as these systems re-scale a handful of generative features estimated from frequency counts and do not support large sets of non-independent features. over their simpler cousins. We argue that this is due to a number of inherent problems that discriminative models for SMT must address, in particular the problems of spurious ambiguity and degenerate solutions. These occur when there are many ways to translate a source sentence to the same target sentence by applying a sequence of steps (a derivation) of either phrase translations or synchronous grammar rules, depending on"
P08-1024,N03-1028,0,0.00944121,"stimated from our training sample using a maximum a posteriori (MAP) estimator. This maximises the likelihood of the parallel training sentences, D = {(e, f )}, penalised using a prior, i.e., ΛM AP = arg maxΛ pΛ (D)p(Λ). We use a zero-mean Gaussian prior with the probability  density function p0 (λk ) ∝ exp −λ2k /2σ 2 .2 This results in the following log-likelihood objective and corresponding gradient: X X L= log pΛ (e|f ) + log p0 (λk ) (4) (e,f )∈D k λk ∂L = EpΛ (d|e,f ) [hk ] − EpΛ (e|f ) [hk ] − 2 ∂λk σ (5) In order to train the model, we maximise equation (4) using L-BFGS (Malouf, 2002; Sha and Pereira, 2003). This method has been demonstrated to be effective for (non-convex) log-linear models with latent variables (Clark and Curran, 2004; Petrov et al., 2007). Each L-BFGS iteration requires the objective value and its gradient with respect to the model parameters. These are calculated using inside-outside inference over the feature forest defined by the SCFG parse chart of f yielding the partition function, ZΛ (f ), required for the log-likelihood, and the marginals, required for its derivatives. Efficiently calculating the objective and its gradient requires two separate packed charts, each repr"
P08-1024,D07-1077,0,0.0181604,"Missing"
P08-1024,D07-1080,0,0.724756,"system. Existing discriminative models require a reference derivation to optimise against, however no parallel corpora annotated for derivations exist. Ideally, a model would account for this ambiguity by marginalising out the derivations, thus predicting the best translation rather than the best derivation. However, doing so exactly is NP-complete. For this reason, to our knowledge, all discriminative models proposed to date either side-step the problem by choosing simple model and feature structures, such that spurious ambiguity is lessened or removed entirely (Ittycheriah and Roukos, 2007; Watanabe et al., 2007), or else ignore the problem and treat derivations as translations (Liang et al., 2006; Tillmann and Zhang, 2007). In this paper we directly address the problem of spurious ambiguity in discriminative models. We use a synchronous context free grammar (SCFG) translation system (Chiang, 2007), a model which has yielded state-of-the-art results on many translation tasks. We present two main contributions. First, we develop a log-linear model of translation which is globally trained on a significant number of parallel sentences. This model maximises the conditional likelihood of the data, p(e|f ),"
P08-1024,2006.amta-papers.28,0,0.0147855,"examples and typically many iterations of a solver during training. While current models focus solely on efficient decoding, discriminative models must also allow for efficient training. Past work on discriminative SMT only address some of these problems. To our knowledge no systems directly address Problem 1, instead choosing to ignore the problem by using one or a small handful of reference derivations in an n-best list (Liang et al., 2006; Watanabe et al., 2007), or else making local independence assumptions which side-step the issue (Ittycheriah and Roukos, 2007; Tillmann and Zhang, 2007; Wellington et al., 2006). These systems all include regularisation, thereby addressing Problem 2. An interesting counterpoint is the work of DeNero et al. (2006), who show that their unregularised model finds degenerate solutions. Some of these discriminative systems have been trained on large training sets (Problem 3); these systems are the local models, for which training is much simpler. Both the global models (Liang et al., 2006; Watanabe et al., 2007) use fairly small training sets, and there is no evidence that their techniques will scale to larger data sets. Our model addresses all three of the above problems"
P08-1024,J97-3002,0,0.142806,"Missing"
P09-1088,N06-2013,0,0.00554308,"), and thus produce far more valid phrases/rules. ● ● ● ● ● 476 ● 20 40 60 80 100 120 140 160 180 200 220 240 Number of Sampling Passes Figure 6: The posterior for the single CPU sampler and distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D 2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize B LEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent runs for each scenario. Both sets of runs performed hyperparameter inference every twenty"
P09-1088,N07-1018,0,0.0346801,"× 4.2 A Gibbs sampler for derivations Markov chain Monte Carlo sampling allows us to perform inference for the model described in 4.1 without restricting the infinite space of possible translation rules. To do this we need a method for sampling a derivation for a given sentence pair from p(d|d− ). One possible approach would be to first build a packed chart representation of the derivation forest, calculate the inside probabilities of all cells in this chart, and then sample derivations top-down according to their inside probabilities (analogous to monolingual parse tree sampling described in Johnson et al. (2007)). A problem with this approach is that building the derivation forest would take O(|f |3 |e|3 ) time, which would be impractical for long sentences. Instead we develop a collapsed Gibbs sampler (Teh et al., 2006) which draws new samples by making local changes to the derivations used in a previous sample. After a period of burn in, the derivations produced by the sampler will be drawn from the posterior distribution, p(d|x). The advantage of this algorithm is that we only store the current derivation for each training sentence pair (together these constitute the state of the sampler), but nev"
P09-1088,N03-1017,0,0.592202,"nous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints from a word-alignment model, but instead directly induce a synchrono"
P09-1088,P07-2045,1,0.0237661,"iero grammars is built from every 50th sample after the burn-in, up until the 1500th sample. We evaluate the translation models using IBM B LEU (Papineni et al., 2001). Table 1 lists the statistics of the corpora used in these experiments. 4.5 Extracting a translation model Although we could use our model directly as a decoder to perform translation, its simple hierarchical reordering parameterisation is too weak to be effective in this mode. Instead we use our sampler to sample a distribution over translation models for state-of-the-art phrase based (Moses) and hierarchical (Hiero) decoders (Koehn et al., 2007; Chiang, 2007). Each sample from our model defines a hierarchical alignment on which we can apply the standard extraction heuristics of these models. By extracting from a sequence of samples we can directly infer a distribution over phrase tables or Hiero grammars. 5 Evaluation Our evaluation aims to determine whether the phrase/SCFG rule distributions created by sampling from the model described in Section 4 impact upon the performance of state-of-theart translation systems. We conduct experiments translating both Chinese (high reordering) and Arabic (low reordering) into English. We use the"
P09-1088,J93-2003,0,0.0378426,"lel sentence-aligned corpora. We use a hierarchical Bayesian prior to bias towards compact grammars with small translation units. Inference is performed using a novel Gibbs sampler over synchronous derivations. This sampler side-steps the intractability issues of previous models which required inference over derivation forests. Instead each sampling iteration is highly efficient, allowing the model to be applied to larger translation corpora than previous approaches. 1 Introduction The field of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003). Although very few researchers use word-based models for translation per se, such models are still widely used in the training of phrase-based models. These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced (either finite state (Koehn et al., 2003) or synchronous context free grammar (Chiang, 2007)). Although wide-spread, the disconnect between the translation model and the alignment model is artificial and c"
P09-1088,W08-0336,0,0.019233,", containing fewer spurious off-diagonal alignments, than the heuristic (see Figure 5), and thus produce far more valid phrases/rules. ● ● ● ● ● 476 ● 20 40 60 80 100 120 140 160 180 200 220 240 Number of Sampling Passes Figure 6: The posterior for the single CPU sampler and distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D 2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize B LEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent"
P09-1088,W02-1018,0,0.720719,".ed.ac.uk Trevor Cohn∗ tcohn@inf.ed.ac.uk Chris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted"
P09-1088,W07-0403,0,0.847523,"tcohn@inf.ed.ac.uk Chris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context f"
P09-1088,W06-1606,0,0.0297598,"siderably more efficient for long sentences. Following the broad shift in the field from finite state transducers to grammar transducers (Chiang, 2007), recent approaches to phrase-based alignment have used synchronous grammar formalisms permitting polynomial time inference (Wu, 1997; Related work Most machine translation systems adopt the approach of Koehn et al. (2003) for ‘training’ a phrase-based translation model.2 This method starts with a word-alignment, usually the latent state of an unsupervised word-based aligner such 2 We include grammar based transducers, such as Chiang (2007) and Marcu et al. (2006), in our definition of phrasebased models. 783 Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). However this asymptotic time complexity is of high enough order (O(|f |3 |e|3 )) that inference is impractical for real translation data. Proposed solutions to this problem include imposing sentence length limits, using small training corpora and constraining the search space using a word-alignment model or parse tree. None of these limitations are particularly desirable as they bias inference. As a result phrase-based alignment models are not yet practical for the wider machine tra"
P09-1088,J07-2003,0,0.855119,"eld of machine translation has seen many advances in recent years, most notably the shift from word-based (Brown et al., 1993) to phrasebased models which use token n-grams as translation units (Koehn et al., 2003). Although very few researchers use word-based models for translation per se, such models are still widely used in the training of phrase-based models. These wordbased models are used to find the latent wordalignments between bilingual sentence pairs, from which a weighted string transducer can be induced (either finite state (Koehn et al., 2003) or synchronous context free grammar (Chiang, 2007)). Although wide-spread, the disconnect between the translation model and the alignment model is artificial and clearly undesirable. Word-based models are incapable of learning translational equivalences between non-compositional phrasal units, while the algorithms used for inducing weighted transducers from word-alignments are based on heuristics with little theoretical justification. A 1 f and e are the input and output sentences respectively. 782 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 782–790, c Suntec, Singapore, 2-7 August 2009. 2009 ACL a"
P09-1088,P08-2007,0,0.0824084,"m aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihoo"
P09-1088,J03-1002,0,0.0161441,"that good practical parallel performance can be achieved by having multiple processors independently sample disjoint subsets of the corpus. Each process maintains a set of rule counts for the entire corpus and communicates the changes it has made to its section of the corpus only after sampling every sentence in that section. In this way each process is sampling according to a slightly ‘out-of-date’ distribution. However, as we confirm in Section 5 the performance of this approximation closely follows the exact collapsed Gibbs sampler. GIZA++ implementation of IBM Model 4 (Brown et al., 1993; Och and Ney, 2003) coupled with the phrase extraction heuristics of Koehn et al. (2003) and the SCFG rule extraction heuristics of Chiang (2007) as our benchmark. All the SCFG models employ a single X non-terminal, we leave experiments with multiple non-terminals to future work. Our hypothesis is that our grammar based induction of translation units should benefit language pairs with significant reordering more than those with less. While for mostly monotone translation pairs, such as Arabic-English, the benchmark GIZA++-based system is well suited due to its strong monotone bias (the sequential Markov model an"
P09-1088,W06-3105,0,0.305426,"er polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a novel Gibbs sampler to perform inference over the latent synchronous derivation trees for our training instances. The sampler reasons over the infinite space of possible translation units without recourse to arbitrary restrictions (e.g., constraints drawn from a wordalignment (Cherry and Lin, 2007; Zhang et al., 2008b) or a grammar fixed a priori (Blunsom et al., We present a phrasal synchronous grammar model of translational equivalence. Unlike previous approaches, we do not resort to heuristics or constraints fr"
P09-1088,P03-1021,0,0.0148685,"nd distributed approximation are roughly equivalent over a sampling run. news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). The Chinese text was segmented with a CRF-based Chinese segmenter optimized for MT (Chang et al., 2008). The Arabic text was preprocessed according to the D 2 scheme of Habash and Sadat (2006), which was identified as optimal for corpora this size. The parameters of the NIST systems were tuned using Och’s algorithm to maximize B LEU on the MT02 test set (Och, 2003). To evaluate whether the approximate distributed inference algorithm described in Section 4.4 is effective, we compare the posterior probability of the training corpus when using a single machine, and when the inference is distributed on an eight core machine. Figure 6 plots the mean posterior and standard error for five independent runs for each scenario. Both sets of runs performed hyperparameter inference every twenty passes through the data. It is clear from the training curves that the distributed approximation tracks the corpus probability of the correct sampler sufficiently closely. Th"
P09-1088,D08-1033,0,0.492948,"Missing"
P09-1088,2001.mtsummit-papers.68,0,0.0114687,"ard alignment points until the node factorises correctly. As the alignments contain many such non-factorisable nodes, these trees are of poor quality. However, all samplers used in these experiments are first ‘burnt-in’ for 1000 full passes through the data. This allows the sampler to diverge from its initialisation condition, and thus gives us confidence that subsequent samples will be drawn from the posterior. An expectation over phrase tables and Hiero grammars is built from every 50th sample after the burn-in, up until the 1500th sample. We evaluate the translation models using IBM B LEU (Papineni et al., 2001). Table 1 lists the statistics of the corpora used in these experiments. 4.5 Extracting a translation model Although we could use our model directly as a decoder to perform translation, its simple hierarchical reordering parameterisation is too weak to be effective in this mode. Instead we use our sampler to sample a distribution over translation models for state-of-the-art phrase based (Moses) and hierarchical (Hiero) decoders (Koehn et al., 2007; Chiang, 2007). Each sample from our model defines a hierarchical alignment on which we can apply the standard extraction heuristics of these models"
P09-1088,J97-3002,0,0.890151,", 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 1997). Consequently, for such models both the parameterisation and approximate inference techniques are fundamental to their success. In this paper we present a novel SCFG translation model using a non-parametric Bayesian formulation. The model includes priors to impose a bias towards small grammars with few rules, each of which is as simple as possible (e.g., terminal productions consisting of short phrase pairs). This explicitly avoids the degenerate solutions of maximum likelihood estimation (DeNero et al., 2006), without resort to the heuristic estimator of Koehn et al. (2003). We develop a nov"
P09-1088,N04-1035,0,0.0196258,"thout resorting to heuristic restrictions on the model. Initial experiments suggest that this model performs well on languages for which the monotone bias of existing alignment and heuristic phrase extraction approaches fail. These results open the way for the development of more sophisticated models employing grammars capable of capturing a wide range of translation phenomena. In future we envision it will be possible to use the techniques developed here to directly induce grammars which match state-of-the-art decoders, such as Hiero grammars or tree substitution grammars of the form used by Galley et al. (2004). (2007) who also observed very little empirical difference between the sampler and its distributed approximation. Tables 3 and 4 show the result on the two NIST corpora when running the distributed sampler on a single 8-core machine.5 These scores tally with our initial hypothesis: that the hierarchical structure of our model suits languages that exhibit less monotone reordering. Figure 5 shows the projected alignment of a headline from the thousandth sample on the NIST Chinese data set. The effect of the grammar based alignment can clearly be seen. Where the combination of GIZA++ and the heu"
P09-1088,C08-1136,0,0.571081,"ris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 19"
P09-1088,P07-1094,0,0.0150239,"pler to permute the internal structure of the trees more easily. ... ... ... ... Figure 4: Rule insert/delete sampler. A pair of adjacent nodes in a ternary rule can be re-parented as a binary rule, or vice-versa. 4.3 Hyperparameter Inference Our model is parameterised by a vector of hyperparameters, α = (αR , αN , αP , αPE , αPF , αnull ), which control the sparsity assumption over various model parameters. We could optimise each concentration parameter on the training corpus by hand, however this would be quite an onerous task. Instead we perform inference over the hyperparameters following Goldwater and Griffiths (2007) by defining a vague gamma prior on each concentration parameter, αx ∼ Gamma(10−4 , 104 ). This hyper-prior is relatively benign, allowing the model to consider a wide range of values for the hyperparameter. We sample a new value for each αx using a log-normal distribution with mean αx and variance 0.3, which is then accepted into the distribution p(αx |d, α− ) using the MetropolisHastings algorithm. Unlike the Gibbs updates, this calculation cannot be distributed over a cluster (see Section 4.4) and thus is very costly. Therefore for small corpora we re-sample the hyperparameter after every p"
P09-1088,P08-1012,0,0.818829,"ris Dyer† redpony@umd.edu Miles Osborne∗ miles@inf.ed.ac.uk ∗ † Department of Informatics University of Edinburgh Edinburgh, EH8 9AB, UK Department of Linguistics University of Maryland College Park, MD 20742, USA Abstract model which can fulfil both roles would address both the practical and theoretical short-comings of the machine translation pipeline. The machine translation literature is littered with various attempts to learn a phrase-based string transducer directly from aligned sentence pairs, doing away with the separate word alignment step (Marcu and Wong, 2002; Cherry and Lin, 2007; Zhang et al., 2008b; Blunsom et al., 2008). Unfortunately none of these approaches resulted in an unqualified success, due largely to intractable estimation. Large training sets with hundreds of thousands of sentence pairs are common in machine translation, leading to a parameter space of billions or even trillions of possible bilingual phrase-pairs. Moreover, the inference procedure for each sentence pair is non-trivial, proving NP-complete for learning phrase based models (DeNero and Klein, 2008) or a high order polynomial (O(|f |3 |e|3 ))1 for a sub-class of weighted synchronous context free grammars (Wu, 19"
P09-1088,P06-1085,0,0.00708198,"with a geometric distribution in which a string of length k will be more probable than its segmentations. We define P1null as the string probability of the non-null part of the rule:  1 E null 2 P0 (e) if |f |= 0 P1 (z → he, f i) = 1 F 2 P0 (f ) if |e |= 0 The terminal translation phrase pair distribution is a hierarchical Dirichlet Process in which each phrase are independently distributed according to DPs:4 F P1P (z → he, f i) = φE z (e) × φz (f ) PE φE , P0E ) z ∼ DP(α 4 This prior is similar to one used by DeNero et al. (2008), who used the expected table count approximation presented in Goldwater et al. (2006). However, Goldwater et al. (2006) contains two major errors: omitting P0 , and using the truncated Taylor series expansion (Antoniak, 1974) which fails for small αP0 values common in these models. In this work we track table counts directly. 785 ... ... ... ... ... ... ... ... ... Figure 2: Split/Join sampler applied between a pair of adjacent terminals sharing the same parent. The dashed line indicates the source position being sampled, boxes indicate source and target tokens, while a solid line is a null alignment. ... ... ... ... the Split/Join operator in Figure 3. In order for this opera"
P09-1088,P02-1040,0,\N,Missing
P09-1088,2005.iwslt-1.1,0,\N,Missing
P09-2085,D08-1033,0,0.0317874,"Missing"
P09-2085,P07-1035,0,0.0288854,"present two nonparametric Bayesian language models: a DP unigram model and an HDP bigram model. Under the DP model, words in a corpus w = w1 . . . wn are generated as follows: Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the expectation, and 2) Even after G|α0 , P0 wi |G ∼ DP(α0 , P0 ) ∼G where"
P09-2085,P06-1085,1,0.89977,"we present an efficient method for sampling from the HDP (and related models, such as the hierarchical PitmanYor process) that considerably decreases the memory footprint of such models as compared to the naive implementation. As we have noted, the issues described in this paper apply to models for various kinds of NLP tasks; for concreteness, we will focus on n-gram language modeling for the remainder of the paper, closely following the presentation in GGJ06. The implementation of collapsed Gibbs samplers for non-parametric Bayesian models is non-trivial, requiring considerable book-keeping. Goldwater et al. (2006a) presented an approximation which significantly reduces the storage and computation overhead, but we show here that their formulation was incorrect and, even after correction, is grossly inaccurate. We present an alternative formulation which is exact and can be computed easily. However this approach does not work for hierarchical models, for which case we present an efficient data structure which has a better space complexity than the naive approach. 1 2 Introduction The Chinese Restaurant Process GGJ06 present two nonparametric Bayesian language models: a DP unigram model and an HDP bigram"
P09-2085,D07-1072,0,0.0157343,"aurant Process GGJ06 present two nonparametric Bayesian language models: a DP unigram model and an HDP bigram model. Under the DP model, words in a corpus w = w1 . . . wn are generated as follows: Unsupervised learning of natural language is one of the most challenging areas in NLP. Recently, methods from nonparametric Bayesian statistics have been gaining popularity as a way to approach unsupervised learning for a variety of tasks, including language modeling, word and morpheme segmentation, parsing, and machine translation (Teh et al., 2006; Goldwater et al., 2006a; Goldwater et al., 2006b; Liang et al., 2007; Finkel et al., 2007; DeNero et al., 2008). These models are often based on the Dirichlet process (DP) (Ferguson, 1973) or hierarchical Dirichlet process (HDP) (Teh et al., 2006), with Gibbs sampling as a method of inference. Exact implementation of such sampling methods requires considerable bookkeeping of various counts, which motivated Goldwater et al. (2006a) (henceforth, GGJ06) to develop an approximation using expected counts. However, we show here that their approximation is flawed in two respects: 1) It omits an important factor in the expectation, and 2) Even after G|α0 , P0 wi |G ∼"
P10-2042,N07-1018,0,0.61095,"ocked Metropolis-Hastings sampler in place of the previous method’s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently converges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm. Both algorithms are shown to improve parsing accuracy. 1 In this paper we present a blocked MetropolisHasting sampler for learning a TSG, similar to Johnson et al. (2007). The sampler jointly updates all the substitution variables in a tree, making much larger moves than the local single-variable sampler. A critical issue when developing a Metroplis-Hastings sampler is choosing a suitable proposal distribution, which must have the same support as the true distribution. For our model the natural proposal distribution is a MAP point estimate, however this cannot be represented directly as it is infinitely large. To solve this problem we develop a grammar transformation which can succinctly represent an infinite TSG in an equivalent finite Context Free Grammar (C"
P10-2042,P09-1088,1,0.822732,"ng model, many hierarchical Bayesian models have been proposed which would also permit similar optimised samplers. In particular models which induce segmentations of complex structures stand to benefit from this work; Examples include the word segmentation model of Goldwater et al. (2006) for which it would be trivial to adapt our technique to develop a blocked sampler. Hierarchical Bayesian segmentation models have also become popular in statistical machine translation where there is a need to learn phrasal translation structures that can be decomposed at the word level (DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009). We envisage similar representations being applied to these models to improve their mixing properties. A particularly interesting avenue for further research is to employ our blocked sampler for unsupervised grammar induction. While it is difficult to extend the local Gibbs sampler to the case where the tree is not observed, the dynamic program for our blocked sampler can be easily used for unsupervised inference by omitting the tree matching constraints. Table 2: Development F1 scores using the truncated parsing algorithm and the novel grammar transform algorithm for"
P10-2042,D09-1037,1,0.8459,"hical Bayesian models have been proposed which would also permit similar optimised samplers. In particular models which induce segmentations of complex structures stand to benefit from this work; Examples include the word segmentation model of Goldwater et al. (2006) for which it would be trivial to adapt our technique to develop a blocked sampler. Hierarchical Bayesian segmentation models have also become popular in statistical machine translation where there is a need to learn phrasal translation structures that can be decomposed at the word level (DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009). We envisage similar representations being applied to these models to improve their mixing properties. A particularly interesting avenue for further research is to employ our blocked sampler for unsupervised grammar induction. While it is difficult to extend the local Gibbs sampler to the case where the tree is not observed, the dynamic program for our blocked sampler can be easily used for unsupervised inference by omitting the tree matching constraints. Table 2: Development F1 scores using the truncated parsing algorithm and the novel grammar transform algorithm for four different training"
P10-2042,N09-1062,1,0.678649,"onvergence (a.k.a. poor mixing). The sampler can get easily stuck because many locally improbable decisions are required to escape from a locally optimal solution. This problem manifests itself both locally to a sentence and globally over the training sample. The net result is a sampler that is non-convergent, overly dependent on its initialisation and cannot be said to be sampling from the posterior. Learning a tree substitution grammar is very challenging due to derivational ambiguity. Our recent approach used a Bayesian non-parametric model to induce good derivations from treebanked input (Cohn et al., 2009), biasing towards small grammars composed of small generalisable productions. In this paper we present a novel training method for the model using a blocked Metropolis-Hastings sampler in place of the previous method’s local Gibbs sampler. The blocked sampler makes considerably larger moves than the local sampler and consequently converges in less time. A core component of the algorithm is a grammar transformation which represents an infinite tree substitution grammar in a finite context free grammar. This enables efficient blocked inference for training and also improves the parsing algorithm"
P10-2042,D08-1033,0,0.107486,"Missing"
P10-2042,P06-1085,0,0.0191835,"mation can implicitly represent an exponential space of tree fragments efficiently, allowing us to build a sampler with considerably better mixing properties than a local Gibbs sampler. The same technique was also shown to improve the parsing algorithm. These improvements are in no way limited to our particular choice of a TSG parsing model, many hierarchical Bayesian models have been proposed which would also permit similar optimised samplers. In particular models which induce segmentations of complex structures stand to benefit from this work; Examples include the word segmentation model of Goldwater et al. (2006) for which it would be trivial to adapt our technique to develop a blocked sampler. Hierarchical Bayesian segmentation models have also become popular in statistical machine translation where there is a need to learn phrasal translation structures that can be decomposed at the word level (DeNero et al., 2008; Blunsom et al., 2009; Cohn and Blunsom, 2009). We envisage similar representations being applied to these models to improve their mixing properties. A particularly interesting avenue for further research is to employ our blocked sampler for unsupervised grammar induction. While it is diff"
P10-2042,N09-1036,0,0.0191017,"20000 −325000 log likelihood −310000 −305000 ment over our earlier 84.0 (Cohn et al., 2009) although still well below state-of-the-art parsers. We conjecture that the performance gap is due to the model using an overly simplistic treatment of unknown words, and also a further mixing problems with the sampler. For the full data set the counts are much larger in magnitude which leads to stronger modes. The sampler has difficulty escaping such modes and therefore is slower to mix. One way to solve the mixing problem is for the sampler to make more global moves, e.g., with table label resampling (Johnson and Goldwater, 2009) or split-merge (Jain and Neal, 2000). Another way is to use a variational approximation instead of MCMC sampling (Wainwright and Jordan, 2008). −330000 Block maximal init Block minimal init Local minimal init Local maximal init 0 100 200 300 400 500 iteration Figure 4: Training likelihood vs. iteration. Each sampling method was initialised with both minimal and maximal elementary trees. Training Local minimal init Local maximal init Blocked minimal init Blocked maximal init truncated 77.63 77.19 77.98 77.67 5 transform 77.98 77.71 78.40 78.24 Discussion We have demonstrated how our grammar tr"
P10-4002,W05-1506,0,0.0175505,"mentation of new semirings. Table 1 shows the C++ representation used for semirings. Note that because of our representation, built-in types like double, int, and bool (together with their default operators) are semirings. Beyond these, the type prob t is provided which stores the logarithm of the value it represents, which helps avoid underflow and overflow problems that may otherwise be encountered. A generic first-order expectation semiring is also provided (Li and Eisner, 2009). the tropical semiring, cdec provides a separate derivation extraction framework that makes use of a < operator (Huang and Chiang, 2005). Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Tab"
P10-4002,P07-1019,0,0.10005,"and phrase-based models, these are strictly arranged in a monotone, leftbranching structure. ing models need not be explicitly represented as FSTs—the state space can be inferred. Although intersection using the Chiang algorithm runs in polynomial time and space, the resulting rescored forest may still be too large to represent completely. cdec therefore supports three pruning strategies that can be used during intersection: full unpruned intersection (useful for tagging models to incorporate, e.g., Markov features, but not generally practical for translation), cube pruning, and cube growing (Huang and Chiang, 2007). 4 5 Rescoring with weighted FSTs Semiring framework Semirings are a useful mathematical abstraction for dealing with translation forests since many useful quantities can be computed using a single linear-time algorithm but with different semirings. A semiring is a 5-tuple (K, ⊕, ⊗, 0, 1) that indicates the set from which the values will be drawn, K, a generic addition and multiplication operation, ⊕ and ⊗, and their identities 0 and 1. Multiplication and addition must be associative. Multiplication must distribute over addition, and v ⊗ 0 The design of cdec separates the creation of a transl"
P10-4002,W06-3601,0,0.0172305,"s avoid underflow and overflow problems that may otherwise be encountered. A generic first-order expectation semiring is also provided (Li and Eisner, 2009). the tropical semiring, cdec provides a separate derivation extraction framework that makes use of a < operator (Huang and Chiang, 2005). Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Table 1: Semiring representation. T is a C++ type name. Element C++ representation K T ⊕ T::operator+= T::operator*= ⊗ 0 T() 1 T(1) 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 20"
P10-4002,N03-1017,0,0.465271,", the decoder can extract not only the 1- or k-best translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization"
P10-4002,P07-2045,1,0.0302608,"versity of Edinburgh alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Jonathan Weese Johns Hopkins University jweese@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Hendra Setiawan University of Maryland hendra@umiacs.umd.edu Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms. This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic. In cdec, model-specific code is only required to construct a translation forest (§3). General rescoring (with language models or other models), pruning, inference, and alignment algorithms then"
P10-4002,P09-1019,1,0.765356,"ng et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Table 1: Semiring representation. T is a C++ type name. Element C++ representation K T ⊕ T::operator+= T::operator*= ⊗ 0 T() 1 T(1) 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009). In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the I NSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLE"
P10-4002,D07-1104,1,0.804043,"ms are often expressed as I NSIDE algorithms with 10 6.2 lation grammar and identical pruning settings.4 Figure 4 shows the cdec configuration and weights file used for this test. The workstation used has two 2GHz quad-core Intel Xenon processors, 32GB RAM, is running Linux kernel version 2.6.18 and gcc version 4.1.2. All decoders use SRI’s language model toolkit, version 1.5.9 (Stolcke, 2002). Joshua was run on the Sun HotSpot JVM, version 1.6.0 12. A hierarchical phrase-based translation grammar was extracted for the NIST MT03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007). A non-terminal span limit of 15 was used, and all decoders were configured to use cube pruning with a limit of 30 candidates at each node and no further pruning. All decoders produced a BLEU score between 31.4 and 31.6 (small differences are accounted for by different tie-breaking behavior and OOV handling). Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008). In these models, the translation model is trained to"
P10-4002,D09-1005,0,0.00908811,"oal a 1 shell 100 a 1 2 1 little 101 1 little 1 small Goal ll se hou 1 small small she sma NN little ll JJ 010 110 1 a little 1 house 1 shell Figure 3: Example unrescored translation hypergraphs generated for the German input ein (a) kleines (small/little) Haus (house/shell) using a SCFG-based model (left) and phrase-based model with a distortion limit of 1 (right). must equal 0. Values that can be computed using the semirings include the number of derivations, the expected translation length, the entropy of the translation posterior distribution, and the expected values of feature functions (Li and Eisner, 2009). Since semirings are such a useful abstraction, cdec has been designed to facilitate implementation of new semirings. Table 1 shows the C++ representation used for semirings. Note that because of our representation, built-in types like double, int, and bool (together with their default operators) are semirings. Beyond these, the type prob t is provided which stores the logarithm of the value it represents, which helps avoid underflow and overflow problems that may otherwise be encountered. A generic first-order expectation semiring is also provided (Li and Eisner, 2009). the tropical semiring"
P10-4002,W09-0424,1,0.545644,"alopez@inf.ed.ac.uk Juri Ganitkevitch Johns Hopkins University juri@cs.jhu.edu Jonathan Weese Johns Hopkins University jweese@cs.jhu.edu Ferhan Ture University of Maryland fture@cs.umd.edu Phil Blunsom Oxford University pblunsom@comlab.ox.ac.uk Hendra Setiawan University of Maryland hendra@umiacs.umd.edu Vladimir Eidelman University of Maryland vlad@umiacs.umd.edu Philip Resnik University of Maryland resnik@umiacs.umd.edu classes in a unified way.1 Although open source decoders for both phrasebased and hierarchical translation models have been available for several years (Koehn et al., 2007; Li et al., 2009), their extensibility to new models and algorithms is limited by two significant design flaws that we have avoided with cdec. First, their implementations tightly couple the translation, language model integration (which we call rescoring), and pruning algorithms. This makes it difficult to explore alternative translation models without also re-implementing rescoring and pruning logic. In cdec, model-specific code is only required to construct a translation forest (§3). General rescoring (with language models or other models), pruning, inference, and alignment algorithms then apply to the unif"
P10-4002,D08-1023,1,0.337071,"T03 Chinese-English translation using a suffix array rule extractor (Lopez, 2007). A non-terminal span limit of 15 was used, and all decoders were configured to use cube pruning with a limit of 30 candidates at each node and no further pruning. All decoders produced a BLEU score between 31.4 and 31.6 (small differences are accounted for by different tie-breaking behavior and OOV handling). Large-scale discriminative training In addition to the widely used MERT algorithm, cdec also provides a training pipeline for discriminatively trained probabilistic translation models (Blunsom et al., 2008; Blunsom and Osborne, 2008). In these models, the translation model is trained to maximize conditional log likelihood of the training data under a specified grammar. Since log likelihood is differentiable with respect to the feature weights in an exponential model, it is possible to use gradient-based optimization techniques to train the system, enabling the parameterization of the model using millions of sparse features. While this training approach was originally proposed for SCFG-based translation models, it can be used to train any model type in cdec. When used with sequential tagging models, this pipeline is identi"
P10-4002,P08-1024,1,0.647817,"nd alignment algorithms then apply to the unified data structure (§4). Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.); new models can be more easily prototyped; and controlled comparison of models is made easier. Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009). Since the inference algorithms necessary to compute a training objective (e.g. conditional likelihood or expected BLEU) and its gradient operate on the unified data structure (§5), any model type can be trained using with any of the supported training Abstract We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, th"
P10-4002,E09-1061,1,0.355868,"red forest: 1- or k-best derivations, feature expectations, or intersection with a target language reference (sentence or lattice). The last option generates an alignment forest, from which a word alignment or feature expectations can be extracted. Most of these values are computed in a time complexity that is linear in the number of edges and nodes in the translation hypergraph using cdec’s semiring framework (§5). 2.1 3 Translation hypergraphs Recent research has proposed a unified representation for the various translation and tagging formalisms that is based on weighted logic programming (Lopez, 2009). In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003). Once a forest has been constructed representing the possible translations, general inference algorithms can be applied. In cdec’s translation hypergraph, a node represents a contiguous sequence of target language words. For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source"
P10-4002,J03-1006,0,0.00679634,"a word alignment or feature expectations can be extracted. Most of these values are computed in a time complexity that is linear in the number of edges and nodes in the translation hypergraph using cdec’s semiring framework (§5). 2.1 3 Translation hypergraphs Recent research has proposed a unified representation for the various translation and tagging formalisms that is based on weighted logic programming (Lopez, 2009). In this view, translation (or tagging) deductions have the structure of a context-free forest, or directed hypergraph, where edges have a single head and 0 or more tail nodes (Nederhof, 2003). Once a forest has been constructed representing the possible translations, general inference algorithms can be applied. In cdec’s translation hypergraph, a node represents a contiguous sequence of target language words. For SCFG models and sequential tagging models, a node also corresponds to a source span and non-terminal type, but for word-based and phrase-based models, the relationship to the source string (or lattice) may be more complicated. In a phrase-based translation hypergraph, the node will correspond to a source coverage vector (Koehn et al., 2003). In word-based models, a single"
P10-4002,J93-2003,0,0.0420135,"st translations, but also alignments to a reference, or the quantities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative tra"
P10-4002,P03-1021,0,0.088916,"d Chiang, 2005). Thus, many of the semiring types define not only the elements shown in Table 1 but T::operator< as well. The k-best extraction algorithm is also parameterized by an optional predicate that can filter out derivations at each node, enabling extraction of only derivations that yield different strings as in Huang et al. (2006). 6 Two training pipelines are provided with cdec. The first, called Viterbi envelope semiring training, VEST, implements the minimum error rate training (MERT) algorithm, a gradient-free optimization technique capable of maximizing arbitrary loss functions (Och, 2003). Table 1: Semiring representation. T is a C++ type name. Element C++ representation K T ⊕ T::operator+= T::operator*= ⊗ 0 T() 1 T(1) 6.1 VEST Rather than computing an error surface using kbest approximations of the decoder search space, cdec’s implementation performs inference over the full hypergraph structure (Kumar et al., 2009). In particular, by defining a semiring whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply usin"
P10-4002,P02-1040,0,0.103814,"whose values are sets of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the I NSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006). Three standard algorithms parameterized with semirings are provided: I NSIDE, O UTSIDE, and I NSIDE O UTSIDE, and the semiring is specified using C++ generics (templates). Additionally, each algorithm takes a weight function that maps from hypergraph edges to a value in K, making it possible to use many different semirings without altering the underlying hypergraph. 5.1 Model training Viterbi and k-best extraction Although Viterbi and k-best extraction algorithms are often expressed as I NSIDE algorithms with 10 6.2 lation grammar and identical pruning settings.4 Figure"
P10-4002,N09-1025,0,0.0233546,"s then apply to the unified data structure (§4). Hence all model types benefit immediately from new algorithms (for rescoring, inference, etc.); new models can be more easily prototyped; and controlled comparison of models is made easier. Second, existing open source decoders were designed with the traditional phrase-based parameterization using a very small number of dense features (typically less than 10). cdec has been designed from the ground up to support any parameterization, from those with a handful of dense features up to models with millions of sparse features (Blunsom et al., 2008; Chiang et al., 2009). Since the inference algorithms necessary to compute a training objective (e.g. conditional likelihood or expected BLEU) and its gradient operate on the unified data structure (§5), any model type can be trained using with any of the supported training Abstract We present cdec, an open source framework for decoding, aligning with, and training a number of statistical machine translation models, including word-based models, phrase-based models, and models based on synchronous context-free grammars. Using a single unified internal representation for translation forests, the decoder strictly sep"
P10-4002,J07-2003,0,0.969957,"ation techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative training (§6). These features are implemented without compromising on performance. We show experimentally that cdec uses less memory and time than comparabl"
P10-4002,N03-1028,0,0.299229,"ities necessary to drive discriminative training using gradient-based or gradient-free optimization techniques. Its efficient C++ implementation means that memory use and runtime performance are significantly better than comparable decoders. 1 Introduction The dominant models used in machine translation and sequence tagging are formally based on either weighted finite-state transducers (FSTs) or weighted synchronous context-free grammars (SCFGs) (Lopez, 2008). Phrase-based models (Koehn et al., 2003), lexical translation models (Brown et al., 1993), and finite-state conditional random fields (Sha and Pereira, 2003) exemplify the former, and hierarchical phrase-based models the latter (Chiang, 2007). We introduce a software package called cdec that manipulates both 1 The software is released under the Apache License, version 2.0, and is available from http://cdec-decoder.org/ . 7 Proceedings of the ACL 2010 System Demonstrations, pages 7–12, c Uppsala, Sweden, 13 July 2010. 2010 Association for Computational Linguistics criteria. The software package includes general function optimization utilities that can be used for discriminative training (§6). These features are implemented without compromising on p"
P10-4002,2006.amta-papers.25,0,0.0163455,"of line segments, having an addition operation equivalent to union, and a multiplication operation equivalent to a linear transformation of the line segments, Och’s line search can be computed simply using the I NSIDE algorithm. Since the translation hypergraphs generated by cdec may be quite large making inference expensive, the logic for constructing error surfaces is factored according to the MapReduce programming paradigm (Dean and Ghemawat, 2004), enabling parallelization across a cluster of machines. Implementations of the BLEU and TER loss functions are provided (Papineni et al., 2002; Snover et al., 2006). Three standard algorithms parameterized with semirings are provided: I NSIDE, O UTSIDE, and I NSIDE O UTSIDE, and the semiring is specified using C++ generics (templates). Additionally, each algorithm takes a weight function that maps from hypergraph edges to a value in K, making it possible to use many different semirings without altering the underlying hypergraph. 5.1 Model training Viterbi and k-best extraction Although Viterbi and k-best extraction algorithms are often expressed as I NSIDE algorithms with 10 6.2 lation grammar and identical pruning settings.4 Figure 4 shows the cdec conf"
P10-4002,N10-1128,1,0.770622,"nce pair under the translation model. From this forest, the Viterbi or maximum a posteriori word alignment can be generated. This alignment algorithm is explored in depth by Dyer (2010). Note that if the phase 1 forest has been pruned in some way, or the grammar does not derive the sentence pair, the target intersection parse may fail, meaning that an alignment will not be recoverable. Decoder workflow The decoding pipeline consists of two phases. The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models. In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation. Since the model-specific code need not worry about integration with rescoring models, it can be made quite simple and efficient. Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kinds of models, further simplifying the model-spec"
P10-4002,P08-1115,1,0.407306,"entation of all the derivations of the sentence pair under the translation model. From this forest, the Viterbi or maximum a posteriori word alignment can be generated. This alignment algorithm is explored in depth by Dyer (2010). Note that if the phase 1 forest has been pruned in some way, or the grammar does not derive the sentence pair, the target intersection parse may fail, meaning that an alignment will not be recoverable. Decoder workflow The decoding pipeline consists of two phases. The first (Figure 1) transforms input, which may be represented as a source language sentence, lattice (Dyer et al., 2008), or context-free forest (Dyer and Resnik, 2010), into a translation forest that has been rescored with all applicable models. In cdec, the only model-specific logic is confined to the first step in the process where an input string (or lattice, etc.) is transduced into the unified hypergraph representation. Since the model-specific code need not worry about integration with rescoring models, it can be made quite simple and efficient. Furthermore, prior to language model integration (and distortion model integration, in the case of phrase based translation), pruning is unnecessary for most kin"
P10-4002,N10-1033,1,\N,Missing
P11-1087,N10-1083,0,0.246454,"Missing"
P11-1087,J92-4003,0,0.42141,"based sampling algorithm for hierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages. 1 Introduction Unsupervised part-of-speech (PoS) induction has long been a central challenge in computational linguistics, with applications in human language learning and for developing portable language processing systems. Despite considerable research effort, progress in fully unsupervised PoS induction has been slow and modern systems barely improve over the early Brown et al. (1992) approach (Christodoulopoulos et al., 2010). One popular means of improving tagging performance is to include supervision in the form of a tag dictionary or similar, however this limits portability and also comprimises any cognitive conclusions. In this paper we present a novel approach to fully unsupervised PoS induction which uniformly outperforms the existing state-of-the-art across all our corpora in 10 different languages. Moreover, the performance of our unsupervised model approaches 865 Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk that of many"
P11-1087,W06-2920,0,0.0103196,"hyperparameter inference is that there are no user tunable parameters in the model, an important feature that we believe helps explain its consistently high performance across test settings. 4 Experiments We perform experiments with a range of corpora to both investigate the properties of our proposed models and inference algorithms, as well as to establish their robustness across languages and domains. For our core English experiments we report results on the entire Penn. Treebank (Marcus et al., 1993), while for other languages we use the corpora made available for the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We report results using the manyto-one (M-1) and v-measure (VM) metrics considered best by the evaluation of Christodoulopoulos et al. (2010). M-1 measures the accuracy of the model after mapping each predicted class to its most frequent corresponding tag, while VM is a variant of the F-measure which uses conditional entropy analogies of precision and recall. The log-posterior for the HMM sampler levels off after a few hundred samples, so we report results after five hundred. The 1HMM sampler converges more quickly so we use two hundred samples for these models. All reported results are the"
P11-1087,P96-1041,0,0.0591174,"ons, encoding a backoff path from complex distributions to successsively simpler ones. The use of complex distributions (e.g., over tag trigrams) allows for rich expressivity when sufficient evidence is available, while the hierarchy affords a means of backing off to simpler and more easily estimated distributions otherwise. The PYP has been shown to generate distributions particularly well suited to modelling language (Teh, 2006a; Goldwater et al., 2006b), and has been shown to be a generalisation of Kneser-Ney smoothing, widely recognised as the best smoothing method for language modelling (Chen and Goodman, 1996). The model is depicted in the plate diagram in Figure 1. At its centre is a standard trigram HMM, which generates a sequence of tags and words, tl |tl−1 , tl−2 , T ∼ Ttl−1 ,tl−2 wl |tl , E Tij Bj U ∼ Etl . 867 w1 w2 w3 Figure 1: Plate diagram representation of the trigram HMM. The indexes i and j range over the set of tags and k ranges over the set of characters. Hyper-parameters have been omitted from the figure for clarity. The trigram transition distribution, Tij , is drawn from a hierarchical PYP prior which backs off to a bigram Bj and then a unigram U distribution, Tij |aT , bT , Bj ∼ P"
P11-1087,D10-1056,0,0.825696,"ierarchical Pitman-Yor models in which we track fractional table counts. In an empirical evaluation we show that our model consistently out-performs the current state-of-the-art across 10 languages. 1 Introduction Unsupervised part-of-speech (PoS) induction has long been a central challenge in computational linguistics, with applications in human language learning and for developing portable language processing systems. Despite considerable research effort, progress in fully unsupervised PoS induction has been slow and modern systems barely improve over the early Brown et al. (1992) approach (Christodoulopoulos et al., 2010). One popular means of improving tagging performance is to include supervision in the form of a tag dictionary or similar, however this limits portability and also comprimises any cognitive conclusions. In this paper we present a novel approach to fully unsupervised PoS induction which uniformly outperforms the existing state-of-the-art across all our corpora in 10 different languages. Moreover, the performance of our unsupervised model approaches 865 Trevor Cohn Department of Computer Science University of Sheffield T.Cohn@dcs.shef.ac.uk that of many existing semi-supervised systems, despite"
P11-1087,E03-1009,0,0.746871,"MM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approaches, our model is a principled generative model and does not include any hand tuned language specific features. Inspired by previous successful approaches (Brown et al., 1992), we develo"
P11-1087,D08-1036,0,0.224679,"Ravi and Knight, 2009). These systems achieve 1 Available from http://fjoch.com/mkcls.html. 866 much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag. Conversely, Ganchev et al. (2010) developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags. Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampli"
P11-1087,P07-1094,0,0.748421,"ce University of Sheffield T.Cohn@dcs.shef.ac.uk that of many existing semi-supervised systems, despite our method not receiving any human input. In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approac"
P11-1087,P06-1085,0,0.722485,"emi-supervised systems, despite our method not receiving any human input. In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approaches, our model is a principled generative model and does not inc"
P11-1087,N06-1041,0,0.316653,"incorporating a character language model, allowing the modelling of limited morphology. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1 Available from http://fjoch.com/mkcls.html. 866 much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from"
P11-1087,D07-1031,0,0.827909,"Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1 Available from http://fjoch.com/mkcls.html. 866 much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag. Conversely, Ganchev et al. (2010) developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags. Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence be"
P11-1087,D10-1083,0,0.471822,"in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). These authors took a Bayesian approach using a Dirichlet prior to encourage sparse distributions over the word types emitted from each tag. Conversely, Ganchev et al. (2010) developed a technique to optimize the more desirable reverse property of the word types having a sparse posterior distribution over tags. Recently Lee et al. (2010) combined the one class per word type constraint (Brown et al., 1992) in a HMM with a Dirichlet prior to achieve both forms of sparsity. However this work approximated the derivation of the Gibbs sampler (omitting the interdependence between events when sampling from a collapsed model), resulting in a model which underperformed Brown et al. (1992)’s one-class HMM. Our work also seeks to enforce both forms of sparsity, by developing an algorithm for type-level inference under the one class constraint. This work differs from previous Bayesian models in that we explicitly model a complex backoff"
P11-1087,N10-1082,0,0.0979839,"Missing"
P11-1087,J93-2004,0,0.0390742,"amma(10, 0.1)). All the hyper-parameters are resampled after every 5th sample of the corpus. The result of this hyperparameter inference is that there are no user tunable parameters in the model, an important feature that we believe helps explain its consistently high performance across test settings. 4 Experiments We perform experiments with a range of corpora to both investigate the properties of our proposed models and inference algorithms, as well as to establish their robustness across languages and domains. For our core English experiments we report results on the entire Penn. Treebank (Marcus et al., 1993), while for other languages we use the corpora made available for the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We report results using the manyto-one (M-1) and v-measure (VM) metrics considered best by the evaluation of Christodoulopoulos et al. (2010). M-1 measures the accuracy of the model after mapping each predicted class to its most frequent corresponding tag, while VM is a variant of the F-measure which uses conditional entropy analogies of precision and recall. The log-posterior for the HMM sampler levels off after a few hundred samples, so we report results after five hundred. T"
P11-1087,E99-1010,0,0.854668,"isation in language models. Brown et al. (1992) presented a simple first-order HMM which restricted word types to always be generated from the same class. Though PoS induction was not their aim, this restriction is largely validated by empirical analysis of treebanked data, and moreover conveys the significant advantage that all the tags for a given word type can be updated at the same time, allowing very efficient inference using the exchange algorithm. This model has been popular for language modelling and bilingual word alignment, and an implementation with improved inference called mkcls (Och, 1999)1 has become a standard part of statistical machine translation systems. The HMM ignores orthographic information, which is often highly indicative of a word’s partof-speech, particularly so in morphologically rich languages. For this reason Clark (2003) extended Brown et al. (1992)’s HMM by incorporating a character language model, allowing the modelling of limited morphology. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate"
P11-1087,P09-1057,0,0.0494803,". Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1 Available from http://fjoch.com/mkcls.html. 866 much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical distributions derived from treebank data (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008)."
P11-1087,P05-1044,0,0.108141,"et al. (1992)’s HMM by incorporating a character language model, allowing the modelling of limited morphology. Our work draws from these models, in that we develop a HMM with a one class per tag restriction and include a character level language model. In contrast to these previous works which use the maximum likelihood estimate, we develop a Bayesian model with a rich prior for smoothing the parameter estimates, allowing us to move to a trigram model. A number of researchers have investigated a semisupervised PoS induction task in which a tag dictionary or similar data is supplied a priori (Smith and Eisner, 2005; Haghighi and Klein, 2006; Goldwater and Griffiths, 2007; Toutanova and Johnson, 2008; Ravi and Knight, 2009). These systems achieve 1 Available from http://fjoch.com/mkcls.html. 866 much higher accuracy than fully unsupervised systems, though it is unclear whether the tag dictionary assumption has real world application. We focus solely on the fully unsupervised scenario, which we believe is more practical for text processing in new languages and domains. Recent work on unsupervised PoS induction has focussed on encouraging sparsity in the emission distributions in order to match empirical d"
P11-1087,P06-1124,0,0.738554,"y existing semi-supervised systems, despite our method not receiving any human input. In this paper we present a Bayesian hidden Markov model (HMM) which uses a non-parametric prior to infer a latent tagging for a sequence of words. HMMs have been popular for unsupervised PoS induction from its very beginnings (Brown et al., 1992), and justifiably so, as the most discriminating feature for deciding a word’s PoS is its local syntactic context. Our work brings together several strands of research including Bayesian non-parametric HMMs (Goldwater and Griffiths, 2007), Pitman-Yor language models (Teh, 2006b; Goldwater et al., 2006b), tagging constraints over word types (Brown et al., 1992) and the incorporation of morphological features (Clark, 2003). The result is a non-parametric Bayesian HMM which avoids overfitting, contains no free parameters, and exhibits good scaling properties. Our model uses a hierarchical Pitman-Yor process (PYP) prior to affect sophisicated smoothing over the transition and emission distributions. This allows the modelling of sub-word structure, thereby capturing tag-specific morphological variation. Unlike many existing approaches, our model is a principled generati"
P13-1088,J07-3004,0,0.00819699,"at our CCAE models match or better comparable recursive autoencoder models.1 2 likes tigers N NP (S[dcl]NP)/NP N NP S[dcl]NP S[dcl] > &lt; Figure 1: CCG derivation for Tina likes tigers with forward (>) and backward application (&lt;). tic. It is this property which makes it attractive for our purposes of providing a conditioning structure for semantic operators. A second benefit of the formalism is that it is designed with computational efficiency in mind. While one could debate the relative merits of various linguistic formalisms the existence of mature tools and resources, such as the CCGBank (Hockenmaier and Steedman, 2007), the Groningen Meaning Bank (Basile et al., 2012) and the C&C Tools (Curran et al., 2007) is another big advantage for CCG. CCG’s transparent surface stems from its categorial property: Each point in a derivation corresponds directly to an interpretable category. These categories (or types) associated with each term in a CCG govern how this term can be combined with other terms in a larger structure, implicitly making them semantically expressive. For instance in Figure 1, the word likes has type (S[dcl]NP)/NP, which means that it first looks for a type NP to its right hand side. Subsequentl"
P13-1088,D10-1115,0,0.844018,"mplying that a word’s meaning is a function of the environment it appears in, be that its syntactic role or co-occurrences with other words (Pereira et al., 1993; Sch¨utze, 1998). While distributional semantics is easily applied to single words, sparsity implies that attempts to directly extract distributional representations for larger expressions are doomed to fail. Only in the past few years has it been attempted to extend these representations to semantic composition. Most approaches here use the idea of vector-matrix composition to learn larger representations from single-word encodings (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012b). While these models have proved very promising for compositional semantics, they make minimal use of linguistic information beyond the word level. In this paper we bridge the gap between recent advances in machine learning and more traditional approaches within computational linguistics. We achieve this goal by employing the CCG formalism to consider compositional structures at any point in a parse tree. CCG is attractive both for its transparent interface between syntax and semantics, and a small but powerful set of combinatory operato"
P13-1088,basile-etal-2012-developing,0,0.0118084,"ncoder models.1 2 likes tigers N NP (S[dcl]NP)/NP N NP S[dcl]NP S[dcl] > &lt; Figure 1: CCG derivation for Tina likes tigers with forward (>) and backward application (&lt;). tic. It is this property which makes it attractive for our purposes of providing a conditioning structure for semantic operators. A second benefit of the formalism is that it is designed with computational efficiency in mind. While one could debate the relative merits of various linguistic formalisms the existence of mature tools and resources, such as the CCGBank (Hockenmaier and Steedman, 2007), the Groningen Meaning Bank (Basile et al., 2012) and the C&C Tools (Curran et al., 2007) is another big advantage for CCG. CCG’s transparent surface stems from its categorial property: Each point in a derivation corresponds directly to an interpretable category. These categories (or types) associated with each term in a CCG govern how this term can be combined with other terms in a larger structure, implicitly making them semantically expressive. For instance in Figure 1, the word likes has type (S[dcl]NP)/NP, which means that it first looks for a type NP to its right hand side. Subsequently the expression likes tigers (as type S[dcl]NP)"
P13-1088,D12-1050,0,0.565971,"ributional representations for expressions much larger than single words.2 Vector space models of compositional semantics aim to fill this gap by providing a methodology for deriving the representation of an expression from those of its parts. While distributional representations frequently serve to encode single words in such approaches this is not a strict requirement. There are a number of ideas on how to define composition in such vector spaces. A general framework for semantic vector composition was proposed in Mitchell and Lapata (2008), with Mitchell and Lapata (2010) and more recently Blacoe and Lapata (2012) providing good overviews of this topic. Notable approaches to this issue include Baroni and Zamparelli (2010), who compose nouns and adjectives by representing them as vectors and matrices, respectively, with the compositional representation achieved by multiplication. Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. These two approaches are combined in Grefenstette et al. (2013), producing a tensor-based semantic framework with tensor contraction as composition operation. Another set of models that have very successfully b"
P13-1088,P99-1041,0,0.0433591,"dent nature of meaning according to which one “shall know a word by the company it keeps” (Firth, 1957). Effectively this is usually achieved by considering the co-occurrence with other words in large corpora or the syntactic roles a word performs. Distributional representations are frequently used to encode single words as vectors. Such rep1 A C++ implementation of our models is available at http://www.karlmoritz.com/ 895 resentations have then successfully been applied to a number of tasks including word sense disambiguation (Sch¨utze, 1998) and selectional preference (Pereira et al., 1993; Lin, 1999). While it is theoretically possible to apply the same mechanism to larger expressions, sparsity prevents learning meaningful distributional representations for expressions much larger than single words.2 Vector space models of compositional semantics aim to fill this gap by providing a methodology for deriving the representation of an expression from those of its parts. While distributional representations frequently serve to encode single words in such approaches this is not a strict requirement. There are a number of ideas on how to define composition in such vector spaces. A general framew"
P13-1088,J07-4004,0,0.0147986,"adient of the regularised objective function then becomes: N ∂J 1 X ∂E(n|θ) = + λθ ∂θ N n ∂θ (8) We learn the gradient using backpropagation through structure (Goller and K¨uchler, 1996), and minimize the objective function using L-BFGS. For more details about the partial derivatives used for backpropagation, see the documentation accompanying our model implementation.3 3 898 http://www.karlmoritz.com/ 4.1 Supervised Learning use word-vectors of size 50, initialized using the embeddings provided by Turian et al. (2010) based on the model of Collobert and Weston (2008).4 We use the C&C parser (Clark and Curran, 2007) to generate CCG parse trees for the data used in our experiments. For models CCAE-C and CCAE-D we use the 25 most frequent CCG categories (as extracted from the British National Corpus) with an additional general weight matrix in order to catch all remaining types. The unsupervised method described so far learns a vector representation for each sentence. Such a representation can be useful for some tasks such as paraphrase detection, but is not sufficient for other tasks such as sentiment classification, which we are considering in this paper. In order to extract sentiment from our models, we"
P13-1088,P08-1028,0,0.542317,"me mechanism to larger expressions, sparsity prevents learning meaningful distributional representations for expressions much larger than single words.2 Vector space models of compositional semantics aim to fill this gap by providing a methodology for deriving the representation of an expression from those of its parts. While distributional representations frequently serve to encode single words in such approaches this is not a strict requirement. There are a number of ideas on how to define composition in such vector spaces. A general framework for semantic vector composition was proposed in Mitchell and Lapata (2008), with Mitchell and Lapata (2010) and more recently Blacoe and Lapata (2012) providing good overviews of this topic. Notable approaches to this issue include Baroni and Zamparelli (2010), who compose nouns and adjectives by representing them as vectors and matrices, respectively, with the compositional representation achieved by multiplication. Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. These two approaches are combined in Grefenstette et al. (2013), producing a tensor-based semantic framework with tensor contraction a"
P13-1088,P07-2009,0,0.00670875,"cl]NP)/NP N NP S[dcl]NP S[dcl] > &lt; Figure 1: CCG derivation for Tina likes tigers with forward (>) and backward application (&lt;). tic. It is this property which makes it attractive for our purposes of providing a conditioning structure for semantic operators. A second benefit of the formalism is that it is designed with computational efficiency in mind. While one could debate the relative merits of various linguistic formalisms the existence of mature tools and resources, such as the CCGBank (Hockenmaier and Steedman, 2007), the Groningen Meaning Bank (Basile et al., 2012) and the C&C Tools (Curran et al., 2007) is another big advantage for CCG. CCG’s transparent surface stems from its categorial property: Each point in a derivation corresponds directly to an interpretable category. These categories (or types) associated with each term in a CCG govern how this term can be combined with other terms in a larger structure, implicitly making them semantically expressive. For instance in Figure 1, the word likes has type (S[dcl]NP)/NP, which means that it first looks for a type NP to its right hand side. Subsequently the expression likes tigers (as type S[dcl]NP) requires a second NP on its left. The fi"
P13-1088,N10-1120,0,0.0302433,"for some tasks such as paraphrase detection, but is not sufficient for other tasks such as sentiment classification, which we are considering in this paper. In order to extract sentiment from our models, we extend them by adding a supervised classifier on top, using the learned representations v as input for a binary classification model: 5.1 We evaluate our model on the MPQA opinion corpus (Wiebe et al., 2005), which annotates expressions for sentiment.5 The corpus consists of 10,624 instances with approximately 70 percent describing a negative sentiment. We apply the same pre-processing as (Nakagawa et al., 2010) and (Socher et al., 2011b) by using an additional sentiment lexicon (Wilson et al., 2005) during the model training for this experiment. As a second corpus we make use of the sentence polarity (SP) dataset v1.0 (Pang and Lee, 2005).6 This dataset consists of 10662 sentences extracted from movie reviews which are manually labelled with positive or negative sentiment and equally distributed across sentiment. pred(l=1|v, θ) = sigmoid(Wlabel v + blabel ) (9) Given our corpus of CCG parses with label pairs (N, l), the new objective function becomes: J= 1 X λ E(N, l, θ) + ||θ||2 N 2 (10) (N,l) Assu"
P13-1088,P05-1015,0,0.0346201,"rvised classifier on top, using the learned representations v as input for a binary classification model: 5.1 We evaluate our model on the MPQA opinion corpus (Wiebe et al., 2005), which annotates expressions for sentiment.5 The corpus consists of 10,624 instances with approximately 70 percent describing a negative sentiment. We apply the same pre-processing as (Nakagawa et al., 2010) and (Socher et al., 2011b) by using an additional sentiment lexicon (Wilson et al., 2005) during the model training for this experiment. As a second corpus we make use of the sentence polarity (SP) dataset v1.0 (Pang and Lee, 2005).6 This dataset consists of 10662 sentences extracted from movie reviews which are manually labelled with positive or negative sentiment and equally distributed across sentiment. pred(l=1|v, θ) = sigmoid(Wlabel v + blabel ) (9) Given our corpus of CCG parses with label pairs (N, l), the new objective function becomes: J= 1 X λ E(N, l, θ) + ||θ||2 N 2 (10) (N,l) Assuming each node n ∈ N contains children xn , yn , encoding en and reconstruction rn , so that n = {x, y, e, r} this breaks down into: E(N, l, θ) = (11) X αErec (n, θ) + (1−α)Elbl (en , l, θ) n∈N 2 1 Erec (n, θ) = [xn kyn ] − rn 2 1 E"
P13-1088,P93-1024,0,0.626426,"ford Oxford, OX1 3QD, UK {karl.moritz.hermann,phil.blunsom}@cs.ox.ac.uk Abstract in this field includes the Combinatory Categorial Grammar (CCG), which also places increased emphasis on syntactic coverage (Szabolcsi, 1989). Recently those searching for the right representation for compositional semantics have drawn inspiration from the success of distributional models of lexical semantics. This approach represents single words as distributional vectors, implying that a word’s meaning is a function of the environment it appears in, be that its syntactic role or co-occurrences with other words (Pereira et al., 1993; Sch¨utze, 1998). While distributional semantics is easily applied to single words, sparsity implies that attempts to directly extract distributional representations for larger expressions are doomed to fail. Only in the past few years has it been attempted to extend these representations to semantic composition. Most approaches here use the idea of vector-matrix composition to learn larger representations from single-word encodings (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012b). While these models have proved very promising for compositional semantics,"
P13-1088,D11-1129,0,0.196143,"Missing"
P13-1088,W13-0112,0,0.0559553,"s. A general framework for semantic vector composition was proposed in Mitchell and Lapata (2008), with Mitchell and Lapata (2010) and more recently Blacoe and Lapata (2012) providing good overviews of this topic. Notable approaches to this issue include Baroni and Zamparelli (2010), who compose nouns and adjectives by representing them as vectors and matrices, respectively, with the compositional representation achieved by multiplication. Grefenstette and Sadrzadeh (2011) use a similar approach with matrices for relational words and vectors for arguments. These two approaches are combined in Grefenstette et al. (2013), producing a tensor-based semantic framework with tensor contraction as composition operation. Another set of models that have very successfully been applied in this area are recursive autoencoders (Socher et al., 2011a; Socher et al., 2011b), which are discussed in the next section. 2.3 Figure 2: A simple three-layer autoencoder. The input represented by the vector at the bottom is being encoded in a smaller vector (middle), from which it is then reconstructed (top) into the same dimensionality as the original input vector. used on multiple inputs. By optimizing the two functions to minimize"
P13-1088,J98-1004,0,0.235436,"Missing"
P13-1088,D11-1014,0,0.271275,"of the 51st Annual Meeting of the Association for Computational Linguistics, pages 894–904, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics tions: Can recursive vector space models be reconciled with a more formal notion of compositionality; and is there a role for syntax in guiding semantics in these types of models? CCAEs make use of CCG combinators and types by conditioning each composition function on its equivalent step in a CCG proof. In terms of learning complexity and space requirements, our models strike a balance between simpler greedy approaches (Socher et al., 2011b) and the larger recursive vector-matrix models (Socher et al., 2012b). We show that this combination of state of the art machine learning and an advanced linguistic formalism translates into concise models with competitive performance on a variety of tasks. In both sentiment and compound similarity experiments we show that our CCAE models match or better comparable recursive autoencoder models.1 2 likes tigers N NP (S[dcl]NP)/NP N NP S[dcl]NP S[dcl] > &lt; Figure 1: CCG derivation for Tina likes tigers with forward (>) and backward application (&lt;). tic. It is this property which makes it attr"
P13-1088,D12-1110,0,0.72402,"it appears in, be that its syntactic role or co-occurrences with other words (Pereira et al., 1993; Sch¨utze, 1998). While distributional semantics is easily applied to single words, sparsity implies that attempts to directly extract distributional representations for larger expressions are doomed to fail. Only in the past few years has it been attempted to extend these representations to semantic composition. Most approaches here use the idea of vector-matrix composition to learn larger representations from single-word encodings (Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012b). While these models have proved very promising for compositional semantics, they make minimal use of linguistic information beyond the word level. In this paper we bridge the gap between recent advances in machine learning and more traditional approaches within computational linguistics. We achieve this goal by employing the CCG formalism to consider compositional structures at any point in a parse tree. CCG is attractive both for its transparent interface between syntax and semantics, and a small but powerful set of combinatory operators with which we can parametrise our nonlinear transfor"
P13-1088,P10-1040,0,0.0777042,"parametrised on c and t respectively, adds an additional degree of freedom and also al(7) The gradient of the regularised objective function then becomes: N ∂J 1 X ∂E(n|θ) = + λθ ∂θ N n ∂θ (8) We learn the gradient using backpropagation through structure (Goller and K¨uchler, 1996), and minimize the objective function using L-BFGS. For more details about the partial derivatives used for backpropagation, see the documentation accompanying our model implementation.3 3 898 http://www.karlmoritz.com/ 4.1 Supervised Learning use word-vectors of size 50, initialized using the embeddings provided by Turian et al. (2010) based on the model of Collobert and Weston (2008).4 We use the C&C parser (Clark and Curran, 2007) to generate CCG parse trees for the data used in our experiments. For models CCAE-C and CCAE-D we use the 25 most frequent CCG categories (as extracted from the British National Corpus) with an additional general weight matrix in order to catch all remaining types. The unsupervised method described so far learns a vector representation for each sentence. Such a representation can be useful for some tasks such as paraphrase detection, but is not sufficient for other tasks such as sentiment classi"
P13-1088,P12-2018,0,0.0433834,"Missing"
P13-1088,H05-1044,0,0.00642491,"entiment classification, which we are considering in this paper. In order to extract sentiment from our models, we extend them by adding a supervised classifier on top, using the learned representations v as input for a binary classification model: 5.1 We evaluate our model on the MPQA opinion corpus (Wiebe et al., 2005), which annotates expressions for sentiment.5 The corpus consists of 10,624 instances with approximately 70 percent describing a negative sentiment. We apply the same pre-processing as (Nakagawa et al., 2010) and (Socher et al., 2011b) by using an additional sentiment lexicon (Wilson et al., 2005) during the model training for this experiment. As a second corpus we make use of the sentence polarity (SP) dataset v1.0 (Pang and Lee, 2005).6 This dataset consists of 10662 sentences extracted from movie reviews which are manually labelled with positive or negative sentiment and equally distributed across sentiment. pred(l=1|v, θ) = sigmoid(Wlabel v + blabel ) (9) Given our corpus of CCG parses with label pairs (N, l), the new objective function becomes: J= 1 X λ E(N, l, θ) + ||θ||2 N 2 (10) (N,l) Assuming each node n ∈ N contains children xn , yn , encoding en and reconstruction rn , so th"
P14-1006,D08-1094,0,0.10376,"Missing"
P14-1006,W13-3520,0,0.084317,"Missing"
P14-1006,D10-1115,0,0.0503403,"to capture semantic, syntactic and morphological content (Collobert and Weston, 2008; Turian et al., 2010, inter alia). We describe a multilingual objective function that uses a noise-contrastive update between semantic representations of different languages to learn these word embeddings. As part of this, we use a compositional vector model (CVM, henceforth) to compute semantic representations of sentences and documents. A CVM learns semantic representations of larger syntactic units given the semantic representations of their constituents (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Hermann and Blunsom, 2013, inter alia). A key difference between our approach and those listed above is that we only require sentencealigned parallel data in our otherwise unsupervised learning function. This removes a number of constraints that normally come with CVM models, such as the need for syntactic parse trees, word alignment or annotated data as a training signal. At the same time, by using multiple CVMs to transfer information between languages, we enable our models to capture a broader semantic context than would otherwise be"
P14-1006,D11-1129,0,0.0101236,"ic and morphological content (Collobert and Weston, 2008; Turian et al., 2010, inter alia). We describe a multilingual objective function that uses a noise-contrastive update between semantic representations of different languages to learn these word embeddings. As part of this, we use a compositional vector model (CVM, henceforth) to compute semantic representations of sentences and documents. A CVM learns semantic representations of larger syntactic units given the semantic representations of their constituents (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Hermann and Blunsom, 2013, inter alia). A key difference between our approach and those listed above is that we only require sentencealigned parallel data in our otherwise unsupervised learning function. This removes a number of constraints that normally come with CVM models, such as the need for syntactic parse trees, word alignment or annotated data as a training signal. At the same time, by using multiple CVMs to transfer information between languages, we enable our models to capture a broader semantic context than would otherwise be possible. The idea of extracting s"
P14-1006,P08-1088,0,0.0952777,"r is used to recreate words in two languages in parallel. This is effectively the linguistic extension of Ngiam et al. (2011), who used a similar method for audio and video data. Hermann and Blunsom (2014) propose a largemargin learner for multilingual word representations, similar to the basic additive model proposed here, which, like the approaches above, relies on a bag-of-words model for sentence representations. Klementiev et al. (2012), our baseline in §5.2, use a form of multi-agent learning on wordaligned parallel data to transfer embeddings from one language to another. Earlier work, Haghighi et al. (2008), proposed a method for inducing bilingual lexica using monolingual feature representations and a small initial lexicon to bootstrap with. This approach has recently been extended by Mikolov et al. (2013a), Mikolov et al. (2013b), who developed a method for learning transformation matrices to convert semantic vectors of one language into those of another. Is was demonstrated that this approach can be applied to improve tasks related to machine translation. Their CBOW model is also worth noting for its similarities to the A DD composition function used here. Using a slightly different approach,"
P14-1006,P13-1088,1,0.896478,"8; Turian et al., 2010, inter alia). We describe a multilingual objective function that uses a noise-contrastive update between semantic representations of different languages to learn these word embeddings. As part of this, we use a compositional vector model (CVM, henceforth) to compute semantic representations of sentences and documents. A CVM learns semantic representations of larger syntactic units given the semantic representations of their constituents (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Hermann and Blunsom, 2013, inter alia). A key difference between our approach and those listed above is that we only require sentencealigned parallel data in our otherwise unsupervised learning function. This removes a number of constraints that normally come with CVM models, such as the need for syntactic parse trees, word alignment or annotated data as a training signal. At the same time, by using multiple CVMs to transfer information between languages, we enable our models to capture a broader semantic context than would otherwise be possible. The idea of extracting semantics from multilingual data stems from prior"
P14-1006,2012.eamt-1.60,0,0.03878,"ys semantic transfer through compositional representations. Unlike most methods for learning word representations, which are restricted to a single language, our approach learns to represent meaning across languages in a shared multilingual semantic space. We present experiments on two corpora. First, we show that for cross-lingual document classification on the Reuters RCV1/RCV2 corpora (Lewis et al., 2004), we outperform the prior state of the art (Klementiev et al., 2012). Second, we also present classification results on a massively multilingual corpus which we derive from the TED corpus (Cettolo et al., 2012). The results on this task, in comparison with a number of strong baselines, further demonstrate the relevance of our approach and the success of our method in learning multilingual semantic representations over a wide range of languages. Introduction Distributed representations of words provide the basis for many state-of-the-art approaches to various problems in natural language processing today. Such word embeddings are naturally richer representations than those of symbolic or discrete models, and have been shown to be able to capture both syntactic and semantic information. Successful app"
P14-1006,W13-3214,1,0.827131,"of our method in learning multilingual semantic representations over a wide range of languages. Introduction Distributed representations of words provide the basis for many state-of-the-art approaches to various problems in natural language processing today. Such word embeddings are naturally richer representations than those of symbolic or discrete models, and have been shown to be able to capture both syntactic and semantic information. Successful applications of such models include language modelling (Bengio et al., 2003), paraphrase detection (Erk and Pad´o, 2008), and dialogue analysis (Kalchbrenner and Blunsom, 2013). Within a monolingual context, the distributional hypothesis (Firth, 1957) forms the basis of most approaches for learning word representations. In 58 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 58–68, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Overview alternative to such secondary representations, as parallel texts share their semantics, and thus one language can be used to ground the other. Some work has exploited this idea for transferring linguistic knowledge into low-resource languages"
P14-1006,C12-1089,0,0.250958,"joint-space embeddings. We present a novel unsupervised technique for learning semantic representations that leverages parallel corpora and employs semantic transfer through compositional representations. Unlike most methods for learning word representations, which are restricted to a single language, our approach learns to represent meaning across languages in a shared multilingual semantic space. We present experiments on two corpora. First, we show that for cross-lingual document classification on the Reuters RCV1/RCV2 corpora (Lewis et al., 2004), we outperform the prior state of the art (Klementiev et al., 2012). Second, we also present classification results on a massively multilingual corpus which we derive from the TED corpus (Cettolo et al., 2012). The results on this task, in comparison with a number of strong baselines, further demonstrate the relevance of our approach and the success of our method in learning multilingual semantic representations over a wide range of languages. Introduction Distributed representations of words provide the basis for many state-of-the-art approaches to various problems in natural language processing today. Such word embeddings are naturally richer representation"
P14-1006,P07-1092,0,0.0393616,"D model is trained on 500k sentence pairs of the English-German parallel section of the Europarl corpus. The A DD + model uses an additional 500k parallel sentences from the English-French corpus, resulting in one million English sentences, each paired up with either a German or a French sentence, with B I and B I + trained accordingly. The motivation behind A DD + and B I + is to investigate whether we can learn better embeddings by introducing additional data from other languages. A similar idea exists in machine translation where English is frequently used to pivot between other languages (Cohn and Lapata, 2007). MT System We develop a machine translation baseline as follows. We train a machine translation tool on the parallel training data, using the development data of each language pair to optimize the translation system. We use the cdec decoder (Dyer et al., 2010) with default settings for this purpose. With this system we translate the test data, and then use a Na¨ıve Bayes classifier7 for the actual experiments. To exemplify, this means the de→ar result is produced by training a translation system from Arabic to German. The Arabic test set is translated into German. A classifier is then trained"
P14-1006,2005.mtsummit-papers.11,0,0.0407306,"e use of a non-linearity enables the model to learn interesting interactions between words in a document, which the bag-of-words approach of A DD is not capable of learning. We use the hyperbolic tangent as activation function. 3.2 Document-level Semantics 4 For a number of tasks, such as topic modelling, representations of objects beyond the sentence level are required. While most approaches to compositional distributed semantics end at the word Corpora We use two corpora for learning semantic representations and performing the experiments described in this paper. 60 The Europarl corpus v71 (Koehn, 2005) was used during initial development and testing of our approach, as well as to learn the representations used for the Cross-Lingual Document Classification task described in §5.2. We considered the English-German and English-French language pairs from this corpus. From each pair the final 100,000 sentences were reserved for development. Second, we developed a massively multilingual corpus based on the TED corpus2 for IWSLT 2013 (Cettolo et al., 2012). This corpus contains English transcriptions and multilingual, sentencealigned translations of talks from the TED conference. While the corpus i"
P14-1006,W02-1001,0,0.00908904,"nd evaluating without further supervision in another. Thus, CLDC can be used to establish whether our learned representations are semantically useful across multiple languages. We follow the experimental setup described in Klementiev et al. (2012), with the exception that we learn our embeddings using solely the Europarl data and use the Reuters corpora only during for classifier training and testing. Each document in the classification task is represented by the average of the d-dimensional representations of all its sentences. We train the multiclass classifier using an averaged perceptron (Collins, 2002) with the same settings as in Klementiev et al. (2012). Experiments We report results on two experiments. First, we replicate the cross-lingual document classification task of Klementiev et al. (2012), learning distributed representations on the Europarl corpus and evaluating on documents from the Reuters RCV1/RCV2 corpora. Subsequently, we design a 1 http://www.statmt.org/europarl/ https://wit3.fbk.eu/ 3 http://www.clg.ox.ac.uk/tedcldc/ 4 English to Arabic, German, French, Spanish, Italian, Dutch, Polish, Brazilian Portuguese, Romanian, Russian and Turkish. Chinese, Farsi and Slowenian were r"
P14-1006,P10-4002,1,0.598471,"er a German or a French sentence, with B I and B I + trained accordingly. The motivation behind A DD + and B I + is to investigate whether we can learn better embeddings by introducing additional data from other languages. A similar idea exists in machine translation where English is frequently used to pivot between other languages (Cohn and Lapata, 2007). MT System We develop a machine translation baseline as follows. We train a machine translation tool on the parallel training data, using the development data of each language pair to optimize the translation system. We use the cdec decoder (Dyer et al., 2010) with default settings for this purpose. With this system we translate the test data, and then use a Na¨ıve Bayes classifier7 for the actual experiments. To exemplify, this means the de→ar result is produced by training a translation system from Arabic to German. The Arabic test set is translated into German. A classifier is then trained The actual CLDC experiments are performed by training on English and testing on German documents and vice versa. Following prior work, we use varying sizes between 100 and 10,000 documents when training the multiclass classifier. The results of this task acros"
P14-1006,P08-1028,0,0.222229,"r words, and have been used to capture semantic, syntactic and morphological content (Collobert and Weston, 2008; Turian et al., 2010, inter alia). We describe a multilingual objective function that uses a noise-contrastive update between semantic representations of different languages to learn these word embeddings. As part of this, we use a compositional vector model (CVM, henceforth) to compute semantic representations of sentences and documents. A CVM learns semantic representations of larger syntactic units given the semantic representations of their constituents (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Hermann and Blunsom, 2013, inter alia). A key difference between our approach and those listed above is that we only require sentencealigned parallel data in our otherwise unsupervised learning function. This removes a number of constraints that normally come with CVM models, such as the need for syntactic parse trees, word alignment or annotated data as a training signal. At the same time, by using multiple CVMs to transfer information between languages, we enable our models to capture a broader semantic con"
P14-1006,D11-1014,0,0.277617,"a higher-level CVM, computing a semantic representation of a document (Figure 2). This recursive approach integrates documentlevel representations into the learning process. We can thus use corpora of parallel documents— regardless of whether they are sentence aligned or not—to propagate a semantic signal back to the individual words. If sentence alignment is available, of course, the document-signal can simply be combined with the sentence-signal, as we did with the experiments described in §5.3. This concept of learning compositional representations for documents contrasts with prior work (Socher et al., 2011; Klementiev et al., 2012, inter alia) who rely on summing or averaging sentencevectors if representations beyond the sentencelevel are required for a particular task. We evaluate the models presented in this paper both with and without the document-level signal. We refer to the individual models used as A DD and B I if used without, and as D OC /A DD and D OC /B I is used with the additional document composition function and error signal. (2) Two Composition Models The objective function in Equation 2 could be coupled with any two given vector composition functions f, g from the literature. A"
P14-1006,D12-1110,0,0.0647563,"obert and Weston, 2008; Turian et al., 2010, inter alia). We describe a multilingual objective function that uses a noise-contrastive update between semantic representations of different languages to learn these word embeddings. As part of this, we use a compositional vector model (CVM, henceforth) to compute semantic representations of sentences and documents. A CVM learns semantic representations of larger syntactic units given the semantic representations of their constituents (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Hermann and Blunsom, 2013, inter alia). A key difference between our approach and those listed above is that we only require sentencealigned parallel data in our otherwise unsupervised learning function. This removes a number of constraints that normally come with CVM models, such as the need for syntactic parse trees, word alignment or annotated data as a training signal. At the same time, by using multiple CVMs to transfer information between languages, we enable our models to capture a broader semantic context than would otherwise be possible. The idea of extracting semantics from multili"
P14-1006,P10-1040,0,0.153926,"oo, cross-lingually. Distributed representation learning describes the task of learning continuous representations for discrete objects. Here, we focus on learning semantic representations and investigate how the use of multilingual data can improve learning such representations at the word and higher level. We present a model that learns to represent each word in a lexicon by a continuous vector in Rd . Such distributed representations allow a model to share meaning between similar words, and have been used to capture semantic, syntactic and morphological content (Collobert and Weston, 2008; Turian et al., 2010, inter alia). We describe a multilingual objective function that uses a noise-contrastive update between semantic representations of different languages to learn these word embeddings. As part of this, we use a compositional vector model (CVM, henceforth) to compute semantic representations of sentences and documents. A CVM learns semantic representations of larger syntactic units given the semantic representations of their constituents (Clark and Pulman, 2007; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011; Socher et al., 2012; Hermann and Blunsom, 2"
P14-1006,W11-0329,0,0.0571356,"distributed representation induction has focused on single languages. English, with its large number of annotated resources, has enjoyed most attention. However, there exists a corpus of prior work on learning multilingual embeddings or on using parallel data to transfer linguistic information across languages. One has to differentiate between approaches such as Al-Rfou’ et al. (2013), that learn embeddings across a large variety of languages and models such as ours, that learn joint embeddings, that is a projection into a shared semantic space across multiple languages. Related to our work, Yih et al. (2011) proposed S2Nets to learn joint embeddings of tf-idf vectors for comparable documents. Their architecture optimises the cosine similarity of documents, using relative semantic similarity scores during learning. More recently, Lauly et al. (2013) proposed a bag-of-words autoencoder model, where the bagof-words representation in one language is used to train the embeddings in another. By placing their vocabulary in a binary branching tree, the probabilistic setup of this model is similar to that of 7 Conclusion To summarize, we have presented a novel method for learning multilingual word embeddi"
P14-1006,D13-1141,0,0.727541,"e basis of most approaches for learning word representations. In 58 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 58–68, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Overview alternative to such secondary representations, as parallel texts share their semantics, and thus one language can be used to ground the other. Some work has exploited this idea for transferring linguistic knowledge into low-resource languages or to learn distributed representations at the word level (Klementiev et al., 2012; Zou et al., 2013; Lauly et al., 2013, inter alia). So far almost all of this work has been focused on learning multilingual representations at the word level. As distributed representations of larger expressions have been shown to be highly useful for a number of tasks, it seems to be a natural next step to attempt to induce these, too, cross-lingually. Distributed representation learning describes the task of learning continuous representations for discrete objects. Here, we focus on learning semantic representations and investigate how the use of multilingual data can improve learning such representations a"
P14-1062,P13-1088,1,0.0620606,"l sentence models have been described. A general class of basic sentence models is that of Neural Bag-of-Words (NBoW) models. These generally consist of a projection layer that maps words, sub-word units or n-grams to high dimensional embeddings; the latter are then combined component-wise with an operation such as summation. The resulting combined vector is classified through one or more fully connected layers. A model that adopts a more general structure provided by an external parse tree is the Recursive Neural Network (RecNN) (Pollack, 1990; K¨uchler and Goller, 1996; Socher et al., 2011; Hermann and Blunsom, 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. The Recurrent Neural Network (RNN) is a special case of the recursive network where the structure that is followed is a simple linear chain (Gers and Schmidhuber, 2001; Mikolov et al., 2011). The RNN is primarily used as a language model, but may also be viewed as a sentence model with a linear structure. The layer computed at the last w"
P14-1062,D10-1115,0,0.0270483,"meaning have been proposed. Composition based methods have been applied to vector representations of word meaning obtained from co-occurrence statistics to obtain vectors for longer phrases. In some cases, composition is defined by algebraic operations over word meaning vectors to produce sentence meaning vectors (Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Turney, 2012; Erk, 2012; Clarke, 2012). In other cases, a composition function is learned and either tied to particular syntactic relations (Guevara, 2010; Zanzotto et al., 2010) or to particular word types (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2013; Grefenstette, 2013). Another approach represents the meaning of sentences by way of automatically extracted logical forms (Zettlemoyer and Collins, 2005). Introduction The aim of a sentence model is to analyse and represent the semantic content of a sentence for purposes of classification or generation. The sentence modelling problem is at the core of many tasks involving a degree of natural language comprehension. These tasks include sentiment analysis, paraphrase detection, entailment recognition, summar"
P14-1062,J12-1002,0,0.0550815,"er layers can relate phrases far apart in the input sentence. by which the features of the sentence are extracted from the features of the words or n-grams. Various types of models of meaning have been proposed. Composition based methods have been applied to vector representations of word meaning obtained from co-occurrence statistics to obtain vectors for longer phrases. In some cases, composition is defined by algebraic operations over word meaning vectors to produce sentence meaning vectors (Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Turney, 2012; Erk, 2012; Clarke, 2012). In other cases, a composition function is learned and either tied to particular syntactic relations (Guevara, 2010; Zanzotto et al., 2010) or to particular word types (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2013; Grefenstette, 2013). Another approach represents the meaning of sentences by way of automatically extracted logical forms (Zettlemoyer and Collins, 2005). Introduction The aim of a sentence model is to analyse and represent the semantic content of a sentence for purposes of classification or generation. The sente"
P14-1062,D08-1097,0,0.115961,"put sentence. Besides the RecNN that uses an external parser to produce structural features for the model, the other models use ngram based or neural features that do not require external resources or additional annotations. In the next experiment we compare the performance of the DCNN with those of methods that use heavily engineered resources. Table 2: Accuracy of six-way question classification on the TREC questions dataset. The second column details the external features used in the various approaches. The first four results are respectively from Li and Roth (2002), Blunsom et al. (2006), Huang et al. (2008) and Silva et al. (2011). In the three neural sentence models—the MaxTDNN, the NBoW and the DCNN—the word vectors are parameters of the models that are randomly initialised; their dimension d is set to 48. The Max-TDNN has a filter of width 6 in its narrow convolution at the first layer; shorter phrases are padded with zero vectors. The convolutional layer is followed by a non-linearity, a maxpooling layer and a softmax classification layer. The NBoW sums the word vectors and applies a non-linearity followed by a softmax classification layer. The adopted non-linearity is the tanh function. The"
P14-1062,D13-1176,1,0.590552,"DCNN including central concepts and related neural sentence models. Section 3 defines the relevant operators and the layers of the network. Section 4 treats of the induced feature graph and other properties of the network. Section 5 discusses the experiments and inspects the learnt feature detectors.1 A central class of models are those based on neural networks. These range from basic neural bag-of-words or bag-of-n-grams models to the more structured recursive neural networks and to time-delay neural networks based on convolutional operations (Collobert and Weston, 2008; Socher et al., 2011; Kalchbrenner and Blunsom, 2013b). Neural sentence models have a number of advantages. They can be trained to obtain generic vectors for words and phrases by predicting, for instance, the contexts in which the words and phrases occur. Through supervised training, neural sentence models can fine-tune these vectors to information that is specific to a certain task. Besides comprising powerful classifiers as part of their architecture, neural sentence models can be used to condition a neural language model to generate sentences word by word (Schwenk, 2012; Mikolov and Zweig, 2012; Kalchbrenner and Blunsom, 2013a). We define a"
P14-1062,D08-1094,0,0.0385435,"Missing"
P14-1062,W13-3214,1,0.542238,"DCNN including central concepts and related neural sentence models. Section 3 defines the relevant operators and the layers of the network. Section 4 treats of the induced feature graph and other properties of the network. Section 5 discusses the experiments and inspects the learnt feature detectors.1 A central class of models are those based on neural networks. These range from basic neural bag-of-words or bag-of-n-grams models to the more structured recursive neural networks and to time-delay neural networks based on convolutional operations (Collobert and Weston, 2008; Socher et al., 2011; Kalchbrenner and Blunsom, 2013b). Neural sentence models have a number of advantages. They can be trained to obtain generic vectors for words and phrases by predicting, for instance, the contexts in which the words and phrases occur. Through supervised training, neural sentence models can fine-tune these vectors to information that is specific to a certain task. Besides comprising powerful classifiers as part of their architecture, neural sentence models can be used to condition a neural language model to generate sentences word by word (Schwenk, 2012; Mikolov and Zweig, 2012; Kalchbrenner and Blunsom, 2013a). We define a"
P14-1062,D13-1166,0,0.012984,"Missing"
P14-1062,D11-1129,1,0.116996,"Missing"
P14-1062,C02-1150,0,0.289854,"e sentiment features of the words in the input sentence. Besides the RecNN that uses an external parser to produce structural features for the model, the other models use ngram based or neural features that do not require external resources or additional annotations. In the next experiment we compare the performance of the DCNN with those of methods that use heavily engineered resources. Table 2: Accuracy of six-way question classification on the TREC questions dataset. The second column details the external features used in the various approaches. The first four results are respectively from Li and Roth (2002), Blunsom et al. (2006), Huang et al. (2008) and Silva et al. (2011). In the three neural sentence models—the MaxTDNN, the NBoW and the DCNN—the word vectors are parameters of the models that are randomly initialised; their dimension d is set to 48. The Max-TDNN has a filter of width 6 in its narrow convolution at the first layer; shorter phrases are padded with zero vectors. The convolutional layer is followed by a non-linearity, a maxpooling layer and a softmax classification layer. The NBoW sums the word vectors and applies a non-linearity followed by a softmax classification layer. The ado"
P14-1062,C10-1142,0,0.0348229,"Missing"
P14-1062,P08-1028,0,0.0201988,"and 2 respectively. With dynamic pooling, a filter with small width at the higher layers can relate phrases far apart in the input sentence. by which the features of the sentence are extracted from the features of the words or n-grams. Various types of models of meaning have been proposed. Composition based methods have been applied to vector representations of word meaning obtained from co-occurrence statistics to obtain vectors for longer phrases. In some cases, composition is defined by algebraic operations over word meaning vectors to produce sentence meaning vectors (Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Turney, 2012; Erk, 2012; Clarke, 2012). In other cases, a composition function is learned and either tied to particular syntactic relations (Guevara, 2010; Zanzotto et al., 2010) or to particular word types (Baroni and Zamparelli, 2010; Coecke et al., 2010; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2013; Grefenstette, 2013). Another approach represents the meaning of sentences by way of automatically extracted logical forms (Zettlemoyer and Collins, 2005). Introduction The aim of a sentence model is to analyse and represent the semantic content of"
P14-1062,C12-2104,0,0.0131592,"Missing"
P14-1062,D11-1014,0,0.649724,"Missing"
P14-1062,D13-1170,0,0.307481,"tional and dynamic pooling operations induce a structured feature graph over the input sentence. Figure 1 illustrates such a graph. Small filters at higher layers can capture syntactic or semantic relations between noncontinuous phrases that are far apart in the input sentence. The feature graph induces a hierarchical structure somewhat akin to that in a syntactic parse tree. The structure is not tied to purely syntactic relations and is internal to the neural network. We experiment with the network in four settings. The first two experiments involve predicting the sentiment of movie reviews (Socher et al., 2013b). The network outperforms other approaches in both the binary and the multi-class experiments. The third experiment involves the categorisation of questions in six question types in the TREC dataset (Li and Roth, 2002). The network matches the accuracy of other state-of-theart methods that are based on large sets of engineered features and hand-coded knowledge resources. The fourth experiment involves predicting the sentiment of Twitter posts using distant supervision (Go et al., 2009). The network is trained on 1.6 million tweets labelled automatically according to the emoticon that occurs"
P14-1062,P10-1040,0,0.0403352,"id , so ill conceived , too slow , too long and too , too dull and pretentious to be feels too formulaic and too familiar to hood rats butt their ugly heads in is too predictable and too self conscious Figure 4: Top five 7-grams at four feature detectors in the first layer of the network. tures based on long n-grams and to hierarchically combine these features is highly beneficial. ment experiment of Sect. 5.2. As the dataset is rather small, we use lower-dimensional word vectors with d = 32 that are initialised with embeddings trained in an unsupervised way to predict contexts of occurrence (Turian et al., 2010). The DCNN uses a single convolutional layer with filters of size 8 and 5 feature maps. The difference between the performance of the DCNN and that of the other high-performing methods in Tab. 2 is not significant (p < 0.09). Given that the only labelled information used to train the network is the training set itself, it is notable that the network matches the performance of state-of-the-art classifiers that rely on large amounts of engineered features and rules and hand-coded resources. 5.4 5.5 Visualising Feature Detectors A filter in the DCNN is associated with a feature detector or neuron"
P14-1062,Q14-1017,0,\N,Missing
P14-2037,C12-1089,0,0.729436,"ed tasks. Such problems include document classification, machine translation, and cross-lingual information retrieval, where multilingual data is frequently the norm. Furthermore, learning multilingual representations can also be useful for cross-lingual information transfer, that is exploiting resource-fortunate languages to generate supervised data in resource-poor ones. We propose a probabilistic model that simultaneously learns word alignments and bilingual distributed word representations. As opposed to previous work in this field, which has relied on hard alignments or bilingual lexica (Klementiev et al., 2012; Mikolov et al., 2013), we marginalize out the alignments, thus capturing more bilingual semantic context. Further, this results in our distributed word alignment (DWA) model being the first probabilistic account of bilingual word representations. This is desirable as it allows better reasoning about the derived representations and furthermore, makes the model suitable for inclusion in higher-level tasks such as machine translation. The contributions of this paper are as follows. We present a new probabilistic similarity measure which is based on an alignment model and prior language modeling"
P14-2037,2005.mtsummit-papers.11,0,0.056473,"performs on par with it when training on English data.1 It seems that our model learns more informative representations towards document classification, even without additional monolingual language models or context information. Again the impact of context is inconclusive. Alignment Evaluation We compare the alignments learned here with those of the FASTA LIGN model which produces very good alignments and translation BLEU scores. We use the same language pairs and datasets as in Dyer et al. (2013), that is the FBIS Chinese-English corpus, and the French-English section of the Europarl corpus (Koehn, 2005). We used the preprocessing tools from CDEC and further replaced all unique tokens with UNK. We trained our models with 100 dimensional representations for up to 40 iterations, and the FA model for 5 iterations as is the default. Table 1 shows that our model learns alignments on part with those of the FA model. This is in line with expectation as our model was trained using the FA expectations. However, it confirms that the learned word representations are able to explain translation probabilities. Surprisingly, context seems to have little impact on the alignment error, suggesting that the mo"
P14-2037,J93-2003,0,0.0733328,"ations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (2008)). While most work employing distributed representations has focused on monolingual tasks, multilingual representations would also be useful for 2 Background The IBM alignment models, introduced by Brown et al. (1993), form the basis of most statistical machine translation systems. In this paper we base our alignment model on FASTA LIGN (FA), a vari224 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–229, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics using the energy function ation of IBM model 2 introduced by Dyer et al. (2013). This model is both fast and produces alignments on par with the state of the art. Further, to induce the distributed representations we incorporate ideas from the log-bil"
P14-2037,P10-4002,1,0.797522,"m over k context words to each side of the aligned source word. We evaluate different context sizes and report the results in Section 5. We define the energy function for the translation probabilities to be ! k X T E(f, ei ) = − rei+s Ts rf −bTr rf −bf (1) 4 Learning The original FA model optimizes the likelihood using the expectation maximization (EM) algorithm where, in the M-step, the parameter update is analytically solvable, except for the λ parameter (the diagonal tension), which is optimized using gradient descent (Dyer et al., 2013). We modified the implementations provided with CDEC (Dyer et al., 2010), retaining its default parameters. In our model, DWA, we optimize the likelihood using the EM as well. However, while training we fix the counts of the E-step to those computed by FA, trained for the default 5 iterations, to aid the convergence rate, and optimize the M-step only. Let θ be the parameters for our model. Then the gradient for each sentence is given by s=−k where rei , rf ∈ Rd are vector representations for source and target words ei+s ∈ VE , f ∈ VF in their respective vocabularies, Ts ∈ Rd×d is the transformation matrix for each surrounding context position, br ∈ Rd are the repr"
P14-2037,N13-1073,0,0.414714,"epresentations and furthermore, makes the model suitable for inclusion in higher-level tasks such as machine translation. The contributions of this paper are as follows. We present a new probabilistic similarity measure which is based on an alignment model and prior language modeling work which learns and relates word representations across languages. Subsequently, we apply these embeddings to a standard document classification task and show that they outperform the current published state of the art (Hermann and Blunsom, 2014b). As a by-product we develop a distributed version of FASTA LIGN (Dyer et al., 2013), which performs on par with the original model, thereby demonstrating the efficacy of the learned bilingual representations. We present a probabilistic model that simultaneously learns alignments and distributed representations for bilingual data. By marginalizing over word alignments the model captures a larger semantic context than prior work relying on hard alignments. The advantage of this approach is demonstrated in a cross-lingual classification task, where we outperform the prior published state of the art. 1 Introduction Distributed representations have become an increasingly importan"
P14-2037,E14-1049,0,0.240651,"near Language Model Language models assign a probability measure to sequences of words. We use the log-bilinear language model proposed by Mnih and Hinton (2007). It is an n-gram based model defined in terms of an energy function E(wn ; w1:n−1 ). The probability for predicting the next word wn given its preceding context of n − 1 words is expressed 225 reconcile monolingual embedding spaces, in an l2 norm sense, using dictionary entries instead of alignments, as well as Schwenk et al. (2007) and Schwenk (2012), who also use distributed representations for estimating translation probabilities. Faruqui and Dyer (2014) use a technique based on CCA and alignments to project monolingual word representations to a common vector space. 3 have parameterized the translation probabilities for the null word using a softmax over an additional weight vector. 3.1 Class Factorization We improve training performance using a class factorization strategy (Morin and Bengio, 2005) as follows. We augment the translation probability to be p(f |e) = p(cf |e) p(f |cf , e) where cf is a unique predetermined class of f ; the class probability is modeled using a similar log-bilinear model as above, but instead of predicting a word"
P14-2037,D07-1045,0,0.0180523,"Missing"
P14-2037,P08-1088,0,0.181556,"= p(J|I) J X I Y T rw C rwn −bTr rwn −bwn i i Multilingual Representation Learning There is some recent prior work on multilingual distributed representation learning. Similar to the model presented here, Klementiev et al. (2012) and Zou et al. (2013) learn bilingual embeddings using word alignments. These two models are non-probabilistic and conditioned on the output of a separate alignment model, unlike our model, which defines a probability distribution over translations and marginalizes over all alignments. These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b). These models avoid word alignment by transferring information across languages using a composed sentence-level representation. While all of these approaches are related to the model proposed in this paper, it is important to note that our approach is novel by providing a probabilistic account of these word embeddings. Further, we learn word alignments and simultaneously use these alignments to guide the representation learning, which could be advantageous particularly for"
P14-2037,C12-2104,0,0.0191326,"ameterization of a language model as n-gram features are not stored. IBM Model 2 J Y n−1 X Log-Bilinear Language Model Language models assign a probability measure to sequences of words. We use the log-bilinear language model proposed by Mnih and Hinton (2007). It is an n-gram based model defined in terms of an energy function E(wn ; w1:n−1 ). The probability for predicting the next word wn given its preceding context of n − 1 words is expressed 225 reconcile monolingual embedding spaces, in an l2 norm sense, using dictionary entries instead of alignments, as well as Schwenk et al. (2007) and Schwenk (2012), who also use distributed representations for estimating translation probabilities. Faruqui and Dyer (2014) use a technique based on CCA and alignments to project monolingual word representations to a common vector space. 3 have parameterized the translation probabilities for the null word using a softmax over an additional weight vector. 3.1 Class Factorization We improve training performance using a class factorization strategy (Morin and Bengio, 2005) as follows. We augment the translation probability to be p(f |e) = p(cf |e) p(f |cf , e) where cf is a unique predetermined class of f ; the"
P14-2037,P13-1088,1,0.812885,"on Distributed representations have become an increasingly important tool in machine learning. Such representations—typically continuous vectors learned in an unsupervised setting—can frequently be used in place of hand-crafted, and thus expensive, features. By providing a richer representation than what can be encoded in discrete settings, distributed representations have been successfully used in many areas. This includes AI and reinforcement learning (Mnih et al., 2013), image retrieval (Kiros et al., 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (2008)). While most work employ"
P14-2037,D11-1014,0,0.0311666,"the art. 1 Introduction Distributed representations have become an increasingly important tool in machine learning. Such representations—typically continuous vectors learned in an unsupervised setting—can frequently be used in place of hand-crafted, and thus expensive, features. By providing a richer representation than what can be encoded in discrete settings, distributed representations have been successfully used in many areas. This includes AI and reinforcement learning (Mnih et al., 2013), image retrieval (Kiros et al., 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (20"
P14-2037,P10-1040,0,0.0836987,"., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (2008)). While most work employing distributed representations has focused on monolingual tasks, multilingual representations would also be useful for 2 Background The IBM alignment models, introduced by Brown et al. (1993), form the basis of most statistical machine translation systems. In this paper we base our alignment model on FASTA LIGN (FA), a vari224 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–229, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics us"
P14-2037,P14-1136,1,0.554816,"asingly important tool in machine learning. Such representations—typically continuous vectors learned in an unsupervised setting—can frequently be used in place of hand-crafted, and thus expensive, features. By providing a richer representation than what can be encoded in discrete settings, distributed representations have been successfully used in many areas. This includes AI and reinforcement learning (Mnih et al., 2013), image retrieval (Kiros et al., 2013), language modelling (Bengio et al., 2003), sentiment analysis (Socher et al., 2011; Hermann and Blunsom, 2013), framesemantic parsing (Hermann et al., 2014), and document classification (Klementiev et al., 2012). In Natural Language Processing (NLP), the use of distributed representations is motivated by the idea that they could capture semantics and/or syntax, as well as encoding a continuous notion of similarity, thereby enabling information sharing between similar words and other units. The success of distributed approaches to a number of tasks, such as listed above, supports this notion and its implied benefits (see also Turian et al. (2010) and Collobert and Weston (2008)). While most work employing distributed representations has focused on"
P14-2037,D13-1141,0,0.196757,"ignment and translation probabilities in a generative style as follows: p(f , a|e) = p(J|I)  p(aj |j, I, J) p fj |eaj , j=1 2.3 where p(J|I) captures the two sentence lengths;  p(aj |j, I, J) the alignment and p fj |eaj the translation probability. Sentence likelihood is given by marginalizing out the alignments, which results in the following equation: p(f |e) = p(J|I) J X I Y T rw C rwn −bTr rwn −bwn i i Multilingual Representation Learning There is some recent prior work on multilingual distributed representation learning. Similar to the model presented here, Klementiev et al. (2012) and Zou et al. (2013) learn bilingual embeddings using word alignments. These two models are non-probabilistic and conditioned on the output of a separate alignment model, unlike our model, which defines a probability distribution over translations and marginalizes over all alignments. These models are also highly related to prior work on bilingual lexicon induction (Haghighi et al., 2008). Other recent approaches include Sarath Chandar et al. (2013), Lauly et al. (2013) and Hermann and Blunsom (2014a, 2014b). These models avoid word alignment by transferring information across languages using a composed sentence-"
P14-2037,P14-1006,1,\N,Missing
P14-6005,W13-3214,1,\N,Missing
P14-6005,D13-1176,1,\N,Missing
P14-6005,J07-2002,0,\N,Missing
P14-6005,W13-0112,0,\N,Missing
P14-6005,J92-4003,0,\N,Missing
P14-6005,P13-1088,1,\N,Missing
P14-6005,J10-4006,0,\N,Missing
P14-6005,P08-1028,0,\N,Missing
P14-6005,P13-1149,0,\N,Missing
P14-6005,marelli-etal-2014-sick,0,\N,Missing
P14-6005,P14-1059,1,\N,Missing
P14-6005,D11-1014,0,\N,Missing
P15-2142,N15-1083,1,0.826347,"p(w|t, h) can be estimated similarly. However, to reduce the computational cost of normalising over the entire vocabulary, we factorize the probability as P (w|h) = P (c|t, h)P (w|c, t, h), where c = c(w) is the unique class of word w. For each c, let Γ(c) be the set of words in that class.pThe vocabulary is clustered into approximately |V |classes using Brown clustering (Brown et al., 1992), reducing the number of items to sum over p in the normalisation factor from O(|V |) to O( |V |). Classbased factorization has been shown to be an effective strategy in normalizing neural language models (Baltescu and Blunsom, 2015), The class prediction score is defined as ψ(c, h) = sTc φ(h) + dc , where sc ∈ RD is the output weight vector for class c and dc is the class bias weight. The output layer then consists of a softmax function for p(c|h) and another softmax for the word prediction exp(Φ(w, h)) , 0 w0 ∈Γ(c) exp(Φ(w , h)) p(w|c, h) = P where Φ(w, h) = rTw φ(h)+bw is the word scoring function with output word representation rw and bias weight bw . The model is trained with minibatch stochastic gradient descent (SGD) with Adagrad (Duchi et al., 2011) and L2 regularisation, to minimise the negative log likelihood of"
P15-2142,P14-2131,0,0.0315722,"Missing"
P15-2142,D12-1133,0,0.0240622,"for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 863–869, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Order 1 2 Generative Transition-based Parsing Our parsing model is based on transition-based arc-standard projective dependency parsing (Nivre and Scholz, 2004). The generative formulation is similar to previous generative transition-based parsers (Titov and Henderson, 2007; Cohen et al., 2011; Buys and Blunsom, 2015), and also related to the joint tagging and parsing model of Bohnet and Nivre (2012). The model predicts a sequence of parsing transitions: A shift transition generates a word (and its POS tag), while a reduce transition adds an arc (i, l, j), where i is the head node, j the dependent and l is the dependency label. The joint probability distribution over a sentence with words w1:n , tags t1:n and a transition sequence a1:2n is defined as n  Y i=1 mi+1 p(ti |hmi )p(wi |ti , hmi ) Y 3 Elements σ1 , σ2 , σ3 , σ4 lc1 (σ1 ), rc1 (σ1 ), lc1 (σ2 ), rc1 (σ2 ) lc2 (σ1 ), rc2 (σ1 ), lc2 (σ2 ), rc2 (σ2 ) lc1 (lc1 (σ1 )), rc1 (rc1 (σ1 )) lc1 (lc1 (σ2 )), rc1 (rc1 (σ2 )) Table 1: Conditi"
P15-2142,P15-1033,0,0.109679,"Missing"
P15-2142,C96-1058,0,0.623026,"Missing"
P15-2142,J92-4003,0,0.498815,"on: exp(χ(a, h)) . 0 a0 ∈A exp(χ(a , h)) p(a|h) = P The output layer for predicting the next tag has a similar form, using the scoring function τ (t, h) = tTt φ(h) + ot for tag representation tt and bias ot . The probability p(w|t, h) can be estimated similarly. However, to reduce the computational cost of normalising over the entire vocabulary, we factorize the probability as P (w|h) = P (c|t, h)P (w|c, t, h), where c = c(w) is the unique class of word w. For each c, let Γ(c) be the set of words in that class.pThe vocabulary is clustered into approximately |V |classes using Brown clustering (Brown et al., 1992), reducing the number of items to sum over p in the normalisation factor from O(|V |) to O( |V |). Classbased factorization has been shown to be an effective strategy in normalizing neural language models (Baltescu and Blunsom, 2015), The class prediction score is defined as ψ(c, h) = sTc φ(h) + dc , where sc ∈ RD is the output weight vector for class c and dc is the class bias weight. The output layer then consists of a softmax function for p(c|h) and another softmax for the word prediction exp(Φ(w, h)) , 0 w0 ∈Γ(c) exp(Φ(w , h)) p(w|c, h) = P where Φ(w, h) = rTw φ(h)+bw is the word scoring f"
P15-2142,W07-2416,0,0.0246312,"al., 2011) and L2 regularisation, to minimise the negative log likelihood of the joint distribution over parsed training sentences. For our experiments we train the model while the training objective improves, and choose the parameters of the iteration with the best development set accuracy (early stopping). The model obtains high accuracy with only a few training iterations. 5 Experiments We evaluate our model for parsing and language modelling on the English Penn Treebank (Marcus et al., 1993) WSJ parsing setup1 . Constituency trees are converted to projective CoNLL syntactic dependencies (Johansson and Nugues, 2007) with the LTH converter2 . For some experiments 1 Training on sections 02-21, development on section 22, and testing on section 23. 2 http://nlp.cs.lth.se/software/treebank converter/ 865 Activation linear rectifier tanh sigmoid UAS 88.40 89.99 90.91 91.48 LAS 86.48 88.31 89.22 89.94 Model Wallach et al. (2008) Titov and Henderson (2007) NN-GenDP Chen and Manning (2014) UAS 85.7 90.93 91.11 92.0 LAS 89.42 89.41 90.7 Table 2: Parsing accuracies using different neural network activation functions. Table 3: Parsing accuracies for dependency parsers on the WSJ test set, CoNLL dependencies. we also"
P15-2142,W15-2108,1,0.771764,"ed contexts are used. 863 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 863–869, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Order 1 2 Generative Transition-based Parsing Our parsing model is based on transition-based arc-standard projective dependency parsing (Nivre and Scholz, 2004). The generative formulation is similar to previous generative transition-based parsers (Titov and Henderson, 2007; Cohen et al., 2011; Buys and Blunsom, 2015), and also related to the joint tagging and parsing model of Bohnet and Nivre (2012). The model predicts a sequence of parsing transitions: A shift transition generates a word (and its POS tag), while a reduce transition adds an arc (i, l, j), where i is the head node, j the dependent and l is the dependency label. The joint probability distribution over a sentence with words w1:n , tags t1:n and a transition sequence a1:2n is defined as n  Y i=1 mi+1 p(ti |hmi )p(wi |ti , hmi ) Y 3 Elements σ1 , σ2 , σ3 , σ4 lc1 (σ1 ), rc1 (σ1 ), lc1 (σ2 ), rc1 (σ2 ) lc2 (σ1 ), rc2 (σ1 ), lc2 (σ2 ), rc2 (σ2"
P15-2142,D13-1176,1,0.639818,"sifier offer an appealing trade-off between speed and accuracy (Nivre, 2008; Zhang and Nivre, 2011; Choi and Mccallum, 2013). Recently neural network transition-based dependency parsers have been shown to give state-ofthe-art performance (Chen and Manning, 2014; Dyer et al., 2015; Weiss et al., 2015). However, the downstream integration of syntactic structure in language understanding and generation tasks is often done heuristically. Neural networks have also been shown to be powerful generative models for language modelling (Bengio et al., 2003; Mikolov et al., 2010) and machine translation (Kalchbrenner and Blunsom, 2013; Devlin et al., 2014; Sutskever et al., 2014). However, currently these models lack awareness of syntax, which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used. 863 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 863–869, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Order 1 2 Generative Transition-based Parsing Our parsing model is based on transition-based a"
P15-2142,P01-1017,0,0.206655,"Missing"
P15-2142,P03-1054,0,0.0460742,"Missing"
P15-2142,P14-2050,0,0.0599106,"Missing"
P15-2142,D14-1082,0,0.0685204,"s n  Y i=1 mi+1 p(ti |hmi )p(wi |ti , hmi ) Y 3 Elements σ1 , σ2 , σ3 , σ4 lc1 (σ1 ), rc1 (σ1 ), lc1 (σ2 ), rc1 (σ2 ) lc2 (σ1 ), rc2 (σ1 ), lc2 (σ2 ), rc2 (σ2 ) lc1 (lc1 (σ1 )), rc1 (rc1 (σ1 )) lc1 (lc1 (σ2 )), rc1 (rc1 (σ2 )) Table 1: Conditioning context elements for neural network input: First, second and third order dependencies are used. 3 Neural Network Model Our probability model is based on neural network language models with distributed representations (Bengio et al., 2003; Mnih and Hinton, 2007), as well as feed-forward neural network models for transition-based dependency parsing (Chen and Manning, 2014; Weiss et al., 2015). We estimate the distributions p(ti |hi ), p(wi |ti , hi ) and p(aj |hj ) with neural networks with shared input and hidden layers but separate output layers. The templates for the conditioning context used are defined in Table 1. In the templates we obtain sentence indexes, which are then mapped to the corresponding words, tags and labels (for the dependencies of 2nd and 3rd order elements). The neural network allows us to include a large number of elements without suffering from sparsity. In the input layer we make use of additive representations (Botha and Blunsom, 201"
P15-2142,J93-2004,0,0.0574002,"on rw and bias weight bw . The model is trained with minibatch stochastic gradient descent (SGD) with Adagrad (Duchi et al., 2011) and L2 regularisation, to minimise the negative log likelihood of the joint distribution over parsed training sentences. For our experiments we train the model while the training objective improves, and choose the parameters of the iteration with the best development set accuracy (early stopping). The model obtains high accuracy with only a few training iterations. 5 Experiments We evaluate our model for parsing and language modelling on the English Penn Treebank (Marcus et al., 1993) WSJ parsing setup1 . Constituency trees are converted to projective CoNLL syntactic dependencies (Johansson and Nugues, 2007) with the LTH converter2 . For some experiments 1 Training on sections 02-21, development on section 22, and testing on section 23. 2 http://nlp.cs.lth.se/software/treebank converter/ 865 Activation linear rectifier tanh sigmoid UAS 88.40 89.99 90.91 91.48 LAS 86.48 88.31 89.22 89.94 Model Wallach et al. (2008) Titov and Henderson (2007) NN-GenDP Chen and Manning (2014) UAS 85.7 90.93 91.11 92.0 LAS 89.42 89.41 90.7 Table 2: Parsing accuracies using different neural net"
P15-2142,P13-1104,0,0.0198483,"ransition from derivation dj , its particles are divided according to p(a|h). In practice, adding only shift and the most likely reduce transition leads to almost no accuracy loss. After all the derivations have been advanced to shift word i + 1, a selection step is performed: The number of particles of each derivation is redistributed according to its probability, weighted by its current number of particles. Some derivations may be assigned 0 particles, in which case they are removed. The particle filtering method lets the beam size depend of the uncertainty of the model, somewhat similar to Choi and Mccallum (2013), while fixing the total number of particles constrains the decoding time to be linear. The particle filter also allow us to sample outputs, and to marginalise over the syntax when generating. where ka is the transition output representation and ea is the bias weight. The score is normalised with the soft-max function: exp(χ(a, h)) . 0 a0 ∈A exp(χ(a , h)) p(a|h) = P The output layer for predicting the next tag has a similar form, using the scoring function τ (t, h) = tTt φ(h) + ot for tag representation tt and bias ot . The probability p(w|t, h) can be estimated similarly. However, to reduce t"
P15-2142,D11-1114,0,0.0316205,"Missing"
P15-2142,W08-1301,0,0.0164116,"Missing"
P15-2142,C04-1010,0,0.0553325,"14). However, currently these models lack awareness of syntax, which limits their ability to include longer-distance dependencies even when potentially unbounded contexts are used. 863 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 863–869, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Order 1 2 Generative Transition-based Parsing Our parsing model is based on transition-based arc-standard projective dependency parsing (Nivre and Scholz, 2004). The generative formulation is similar to previous generative transition-based parsers (Titov and Henderson, 2007; Cohen et al., 2011; Buys and Blunsom, 2015), and also related to the joint tagging and parsing model of Bohnet and Nivre (2012). The model predicts a sequence of parsing transitions: A shift transition generates a word (and its POS tag), while a reduce transition adds an arc (i, l, j), where i is the head node, j the dependent and l is the dependency label. The joint probability distribution over a sentence with words w1:n , tags t1:n and a transition sequence a1:2n is defined as"
P15-2142,P14-1129,0,0.0657315,"Missing"
P15-2142,J08-4003,0,0.0302047,"Missing"
P15-2142,J01-2004,0,0.715777,"Missing"
P15-2142,W07-2218,0,0.154382,"ance dependencies even when potentially unbounded contexts are used. 863 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 863–869, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Order 1 2 Generative Transition-based Parsing Our parsing model is based on transition-based arc-standard projective dependency parsing (Nivre and Scholz, 2004). The generative formulation is similar to previous generative transition-based parsers (Titov and Henderson, 2007; Cohen et al., 2011; Buys and Blunsom, 2015), and also related to the joint tagging and parsing model of Bohnet and Nivre (2012). The model predicts a sequence of parsing transitions: A shift transition generates a word (and its POS tag), while a reduce transition adds an arc (i, l, j), where i is the head node, j the dependent and l is the dependency label. The joint probability distribution over a sentence with words w1:n , tags t1:n and a transition sequence a1:2n is defined as n  Y i=1 mi+1 p(ti |hmi )p(wi |ti , hmi ) Y 3 Elements σ1 , σ2 , σ3 , σ4 lc1 (σ1 ), rc1 (σ1 ), lc1 (σ2 ), rc1 (σ"
P15-2142,N03-1033,0,0.115772,"Missing"
P15-2142,P15-1032,0,0.0129235,"mi )p(wi |ti , hmi ) Y 3 Elements σ1 , σ2 , σ3 , σ4 lc1 (σ1 ), rc1 (σ1 ), lc1 (σ2 ), rc1 (σ2 ) lc2 (σ1 ), rc2 (σ1 ), lc2 (σ2 ), rc2 (σ2 ) lc1 (lc1 (σ1 )), rc1 (rc1 (σ1 )) lc1 (lc1 (σ2 )), rc1 (rc1 (σ2 )) Table 1: Conditioning context elements for neural network input: First, second and third order dependencies are used. 3 Neural Network Model Our probability model is based on neural network language models with distributed representations (Bengio et al., 2003; Mnih and Hinton, 2007), as well as feed-forward neural network models for transition-based dependency parsing (Chen and Manning, 2014; Weiss et al., 2015). We estimate the distributions p(ti |hi ), p(wi |ti , hi ) and p(aj |hj ) with neural networks with shared input and hidden layers but separate output layers. The templates for the conditioning context used are defined in Table 1. In the templates we obtain sentence indexes, which are then mapped to the corresponding words, tags and labels (for the dependencies of 2nd and 3rd order elements). The neural network allows us to include a large number of elements without suffering from sparsity. In the input layer we make use of additive representations (Botha and Blunsom, 2014) so that for each w"
P15-2142,D08-1059,0,0.0311045,"ons when both may lead to a valid derivation. L X φ(h) = g( Cj qhj ), j=1 where Cj ∈ RD×D are transformation matrices defined for each position in sequence h, L = |h| and g is a (usually non-linear) activation function applied element-wise. The matrices Cj can be approximated to be diagonal to reduce the number of model parameters and speed up the model by avoiding expensive matrix multiplications. For the output layer predicting the next transition a, the hidden layer is mapped with a scoring 864 4 function χ(a, h) = kTa φ(h) + ea , Decoding Beam-search decoders for transition-based parsing (Zhang and Clark, 2008) keep a beam of partial derivations, advancing each derivation by one transition at a time. When the size of the beam exceeds a set threshold, the lowest-scoring derivations are removed. However, in an incremental generative model we need to compare derivations with the same number of words shifted, rather than transitions performed. To let the decoding time remain linear, we also need to bound the total number of reduce transitions that can be performed over all derivations between two shift transitions. To achieve this, we use a decoding method recently proposed for generative incremental pa"
P15-2142,P11-2033,0,0.0726833,"Missing"
P16-1057,D15-1198,0,0.0188294,"Missing"
P16-1057,N13-1103,0,0.142382,"r networks to copy keywords from the input. Along with other extensions, namely structured attention and code compression, our model is applied on on both existing datasets and also on a newly created one with implementations of TCG game cards. Our experiments show that our model out-performs multiple benchmarks, which demonstrate the importance of combining different types of predictors. Related Work While we target widely used programming languages, namely, Java and Python, our work is related to studies on the generation of any executable code. These include generating regular expressions (Kushman and Barzilay, 2013), and the code for parsing input documents (Lei et al., 2013). Much research has also been invested in generating formal languages, such as database queries (Zelle and Mooney, 1996; Berant et al., 2013), agent specific language (Kate et al., 2005) or smart phone instructions (Le et al., 2013). Finally, mapping natural language into a sequence of actions for the generation of executable code (Branavan et al., 2009). Finally, a considerable effort in this task has focused on semantic parsing (Wong and Mooney, 2006; Jones et al., 2012; Lei et al., 2013; Artzi et al., 2015; Quirk et al., 2015). Re"
P16-1057,D13-1160,0,0.116214,"Missing"
P16-1057,D10-1119,0,0.00897507,"al., 2014) and an attention-based model (Bahdanau et al., 2014). The former is adapted to work with multiple input fields by concatenating them, while the latter uses our proposed attention model. These models are denoted as “Sequence” and “Attention”. Machine Translation Baselines Our problem can also be viewed in the framework of semantic parsing (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012; Artzi et al., 2015). Unfortunately, these approaches define strong assumptions regarding the grammar and structure of the output, which makes it difficult to generalize for other domains (Kwiatkowski et al., 2010). However, the work in Andreas et al. (2013) provides 604 ial. For instance, the correctness cards with conditional (e.g. if player has no cards, then draw a card) or non-deterministc (e.g. put a random card in your hand) effects cannot be simply validated by running the code. evidence that using machine translation systems without committing to such assumptions can lead to results competitive with the systems described above. We follow the same approach and create a phrase-based (Koehn et al., 2007) model and a hierarchical model (or PCFG) (Chiang, 2007) as benchmarks for the work presented h"
P16-1057,P09-1010,0,0.0853311,"Missing"
P16-1057,N16-1030,0,0.00411849,"viously been used to generate code from natural language in (Mou et al., 2015). Inspired by these works, LPNs provide a richer framework by employing attention models (Bahdanau et al., 2014), pointer networks (Vinyals et al., 2015) and character-based embeddings (Ling et al., 2015). Our formulation can also be seen as a generalization of Allamanis et al. (2016), who implement a special case where two predictors have the same granularity (a sub-token softmax and a pointer network). Finally, HMMs have been employed in neural models to marginalize over label sequences in (Collobert et al., 2011; Lample et al., 2016) by modeling transitions between labels. Figure 5: Examples of decoded cards from HS. Copied segments are marked in green and incorrect segments are marked in red. 8 9 Conclusion We introduced a neural network architecture named Latent Prediction Network, which allows efficient marginalization over multiple predictors. Under this architecture, we propose a generative model for code generation that combines a character level softmax to generate language-specific tokens and multiple pointer networks to copy keywords from the input. Along with other extensions, namely structured attention and cod"
P16-1057,J07-2003,0,0.0181301,"Missing"
P16-1057,P13-1127,0,0.601031,"599–609, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (e.g., name and cost) and has an effect that is described in a text box. Digital implementations of these games implement the game logic, which includes the card effects. This is attractive from a data extraction perspective as not only are the data annotations naturally generated, but we can also view the card as a specification communicated from a designer to a software engineer. This dataset presents additional challenges to prior work in code generation (Wong and Mooney, 2006; Jones et al., 2012; Lei et al., 2013; Artzi et al., 2015; Quirk et al., 2015), including the handling of structured input—i.e. cards are composed by multiple sequences (e.g., name and description)—and attributes (e.g., attack and cost), and the length of the generated sequences. Thus, we propose an extension to attention-based neural models (Bahdanau et al., 2014) to attend over structured inputs. Finally, we propose a code compression method to reduce the size of the code without impacting the quality of the predictions. Experiments performed on our new datasets, and a further pre-existing one, suggest that our extensions outpe"
P16-1057,P13-2121,0,0.0152035,"Missing"
P16-1057,D15-1176,1,0.220142,"Missing"
P16-1057,P12-1051,0,0.057978,"Linguistics, pages 599–609, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (e.g., name and cost) and has an effect that is described in a text box. Digital implementations of these games implement the game logic, which includes the card effects. This is attractive from a data extraction perspective as not only are the data annotations naturally generated, but we can also view the card as a specification communicated from a designer to a software engineer. This dataset presents additional challenges to prior work in code generation (Wong and Mooney, 2006; Jones et al., 2012; Lei et al., 2013; Artzi et al., 2015; Quirk et al., 2015), including the handling of structured input—i.e. cards are composed by multiple sequences (e.g., name and description)—and attributes (e.g., attack and cost), and the length of the generated sequences. Thus, we propose an extension to attention-based neural models (Bahdanau et al., 2014) to attend over structured inputs. Finally, we propose a code compression method to reduce the size of the code without impacting the quality of the predictions. Experiments performed on our new datasets, and a further pre-existing one, suggest that ou"
P16-1057,D08-1082,0,0.0676968,"able as s increases. The algorithm stops once s reaches max or the newly generated list L(s) contains no elements. Neural Benchmarks We implement two standard neural networks, namely a sequence-tosequence model (Sutskever et al., 2014) and an attention-based model (Bahdanau et al., 2014). The former is adapted to work with multiple input fields by concatenating them, while the latter uses our proposed attention model. These models are denoted as “Sequence” and “Attention”. Machine Translation Baselines Our problem can also be viewed in the framework of semantic parsing (Wong and Mooney, 2006; Lu et al., 2008; Jones et al., 2012; Artzi et al., 2015). Unfortunately, these approaches define strong assumptions regarding the grammar and structure of the output, which makes it difficult to generalize for other domains (Kwiatkowski et al., 2010). However, the work in Andreas et al. (2013) provides 604 ial. For instance, the correctness cards with conditional (e.g. if player has no cards, then draw a card) or non-deterministc (e.g. put a random card in your hand) effects cannot be simply validated by running the code. evidence that using machine translation systems without committing to such assumptions"
P16-1057,P07-2045,0,0.0311268,"marginal likelihood over latent predictors and generated segments allowing for scalable training. We introduce a new corpus for the automatic generation of code for cards in Trading Card Games (TCGs), on which we validate our model 1 . TCGs, such as Magic the Gathering (MTG) and Hearthstone (HS), are games played between two players that build decks from an ever expanding pool of cards. Examples of such cards are shown in Figure 1. Each card is identified by its attributes Introduction The generation of both natural and formal languages often requires models conditioned on diverse predictors (Koehn et al., 2007; Wong and Mooney, 2006). Most models take the restrictive approach of employing a single predictor, such as a word softmax, to predict all tokens of the output sequence. To illustrate its limitation, suppose we wish to generate the answer to the question “Who wrote The Foundation?” as “The Foundation was written by Isaac Asimov”. The generation of the words “Issac Asimov” and “The Foundation” from a word softmax trained on annotated data is unlikely to succeed as these words are sparse. A robust model might, for example, employ one pre1 Dataset available at https://deepmind.com/publications.h"
P16-1057,J03-1002,0,0.0340537,"Missing"
P16-1057,P02-1040,0,0.114005,"Missing"
P16-1057,P15-1085,0,0.232073,"2, 2016. 2016 Association for Computational Linguistics (e.g., name and cost) and has an effect that is described in a text box. Digital implementations of these games implement the game logic, which includes the card effects. This is attractive from a data extraction perspective as not only are the data annotations naturally generated, but we can also view the card as a specification communicated from a designer to a software engineer. This dataset presents additional challenges to prior work in code generation (Wong and Mooney, 2006; Jones et al., 2012; Lei et al., 2013; Artzi et al., 2015; Quirk et al., 2015), including the handling of structured input—i.e. cards are composed by multiple sequences (e.g., name and description)—and attributes (e.g., attack and cost), and the length of the generated sequences. Thus, we propose an extension to attention-based neural models (Bahdanau et al., 2014) to attend over structured inputs. Finally, we propose a code compression method to reduce the size of the code without impacting the quality of the predictions. Experiments performed on our new datasets, and a further pre-existing one, suggest that our extensions outperform strong benchmarks. The paper is str"
P16-1057,2011.eamt-1.33,0,0.0262721,"Missing"
P16-1057,U05-1027,0,0.02406,"nerated incorrectly. In the card “Preparation”, we observe that while the properties of the card are generated correctly, the effect implements a unrelated one, with the exception of the value 3, which is correctly copied. Yet, interestingly, it still generates a valid effect, which sets a minion’s attack to 3. Investigating better methods to accurately generate these effects will be object of further studies. et al., 2015), Bayesian Tree Transducers (Jones et al., 2012; Lei et al., 2013) and Probabilistic Context Free Grammars (Andreas et al., 2013). The work in natural language programming (Vadas and Curran, 2005; Manshadi et al., 2013), where users write lines of code from natural language, is also related to our work. Finally, the reverse mapping from code into natural language is explored in (Oda et al., 2015). Character-based sequence-to-sequence models have previously been used to generate code from natural language in (Mou et al., 2015). Inspired by these works, LPNs provide a richer framework by employing attention models (Bahdanau et al., 2014), pointer networks (Vinyals et al., 2015) and character-based embeddings (Ling et al., 2015). Our formulation can also be seen as a generalization of Al"
P16-1057,N06-1056,0,0.607791,"over latent predictors and generated segments allowing for scalable training. We introduce a new corpus for the automatic generation of code for cards in Trading Card Games (TCGs), on which we validate our model 1 . TCGs, such as Magic the Gathering (MTG) and Hearthstone (HS), are games played between two players that build decks from an ever expanding pool of cards. Examples of such cards are shown in Figure 1. Each card is identified by its attributes Introduction The generation of both natural and formal languages often requires models conditioned on diverse predictors (Koehn et al., 2007; Wong and Mooney, 2006). Most models take the restrictive approach of employing a single predictor, such as a word softmax, to predict all tokens of the output sequence. To illustrate its limitation, suppose we wish to generate the answer to the question “Who wrote The Foundation?” as “The Foundation was written by Isaac Asimov”. The generation of the words “Issac Asimov” and “The Foundation” from a word softmax trained on annotated data is unlikely to succeed as these words are sparse. A robust model might, for example, employ one pre1 Dataset available at https://deepmind.com/publications.html 599 Proceedings of t"
P16-1057,P13-2009,0,\N,Missing
P17-1015,P13-2009,0,0.014434,"7) 0.002 E div(m4,m5) Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem. To this end, we collect 100,000 question and rational"
P17-1015,P02-1040,0,0.108663,"Missing"
P17-1015,D13-1160,0,0.104533,"1,m2) y 0.023 check(m7) 0.002 E div(m4,m5) Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem. To this end, we collect 100,000"
P17-1015,P15-1085,0,0.0287949,"Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem. To this end, we collect 100,000 question and rationale pairs, and propose"
P17-1015,D15-1202,0,0.38211,"n four parts, two inputs and two outputs: the description of the problem, which we will denote as the question, and the possible (multiple choice) answer options, denoted as options. Our goal is to generate the description of the rationale used to reach the correct answer, denoted as rationale and the correct option label. Problem 1 illustrates an example of an algebra problem, which must be translated into an expression (i.e., (27x + 17y)/(x + y) = 23) and then the desired quantity (x/y) solved for. Problem 2 is an example that could be solved by multi-step arithmetic operations proposed in (Roy and Roth, 2015). Finally, Problem 3 describes a problem that is solved by testing each of the options, which has not been addressed in the past. 2.1 Construction We first collect a set of 34,202 seed problems that consist of multiple option math questions covering a broad range of topics and difficulty levels. Examples of exams with such problems include the GMAT (Graduate Management Admission Test) and GRE (General Test). Many websites contain example math questions in such exams, where the answer is supported by a rationale. Next, we turned to crowdsourcing to generate new questions. We create a task where"
P17-1015,D16-1117,0,0.218012,"ill an unsolved problem, as each additional step adds complexity to the problem both during inference and decoding. Yet, this is the first result showing that it is possible to solve math problems in such a manner, and we believe this modeling approach and dataset will drive work on this problem. 7 Related Work Extensive efforts have been made in the domain of math problem solving (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015), which aim at obtaining the correct answer to a given math problem. Other work has focused on learning to map math expressions into formal languages (Roy et al., 2016). We aim to generate natural language rationales, where the bindings between variables and the problem solving approach are mixed into a single generative model that attempts to solve the problem while explaining the approach taken. Our approach is strongly tied with the work on sequence to sequence transduction using the encoder-decoder paradigm (Sutskever et al., 2014; BLEU. We observe that the regular sequence to sequence model achieves a low BLEU score. In fact, due to the high perplexities the model generates very short rationales, which frequently consist of segments similar to “Answer s"
P17-1015,D14-1058,0,0.0514733,", as these cannot be copied from the input or output. 6.5 Discussion. While we show that our model can outperform the models built up to date, generating complex rationales as those shown in Figure 1 correctly is still an unsolved problem, as each additional step adds complexity to the problem both during inference and decoding. Yet, this is the first result showing that it is possible to solve math problems in such a manner, and we believe this modeling approach and dataset will drive work on this problem. 7 Related Work Extensive efforts have been made in the domain of math problem solving (Hosseini et al., 2014; Kushman et al., 2014; Roy and Roth, 2015), which aim at obtaining the correct answer to a given math problem. Other work has focused on learning to map math expressions into formal languages (Roy et al., 2016). We aim to generate natural language rationales, where the bindings between variables and the problem solving approach are mixed into a single generative model that attempts to solve the problem while explaining the approach taken. Our approach is strongly tied with the work on sequence to sequence transduction using the encoder-decoder paradigm (Sutskever et al., 2014; BLEU. We observ"
P17-1015,P12-1051,0,0.01495,"0.025 130 2.99 div(m1,m2) y 0.023 check(m7) 0.002 E div(m4,m5) Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the problem, but also generate a description of the method used to solve the problem. To this en"
P17-1015,D13-1176,1,0.441142,"r capsule for bottle T ? (A) $ 0.25 (B) $ 0.12 (C) $ 0.05 (D) $ 0.03 (E) $ 0.002 m sub(m6,m3) 250 6.25 0.025 130 2.99 div(m1,m2) y 0.023 check(m7) 0.002 E div(m4,m5) Cost per capsule in R is 6.25 / 250 = 0.025 
 Cost per capsule in T is 2.99 / 130 = 0.023 
 The diﬀerence 0.002 
 The answer is is E &lt;EOS&gt; Figure 4: Illustration of the most likely latent program inferred by our algorithm to explain a held-out question-rationale pair. like ours, have jointly modeled rationales and answer predictions; however, we are the first to use rationales to guide program induction. Bahdanau et al., 2014; Kalchbrenner and Blunsom, 2013), and inherits ideas from the extensive literature on semantic parsing (Jones et al., 2012; Berant et al., 2013; Andreas et al., 2013; Quirk et al., 2015; Liang et al., 2016; Neelakantan et al., 2016) and program generation (Reed and de Freitas, 2016; Graves et al., 2016), namely, the usage of an external memory, the application of different operators over values in the memory and the copying of stored values into the output sequence. 8 Conclusion In this work, we addressed the problem of generating rationales for math problems, where the task is to not only obtain the correct answer of the pr"
P17-1015,P14-1026,0,0.327097,"gth Figure 2: Distribution of examples per length. Table 1: Descriptive statistics of our dataset. define in the instructions that this is not allowed, as it will generate multiple copies of the same problem in the dataset if two or more Turkers copy from the same resource. These Turkers can be detected by checking the nearest neighbours within the collected datasets as problems obtained from online resources are frequently submitted by more than one Turker. Using this method, we obtained 70,318 additional questions. 2.2 17y)/(x + y) = 23 must be solved to obtain the answer. In previous work (Kushman et al., 2014), this could be done by feeding the equation into an expression solver to obtain x/y = 3/2. However, this would skip the intermediate steps 27x + 17y = 23x + 23y and 4x = 6y, which must also be generated in our problem. We propose a model that jointly learns to generate the text in the rationale, and to perform the math operations required to solve the problem. This is done by generating a program, containing both instructions that generate output and instructions that simply generate intermediate values used by following instructions. Statistics Descriptive statistics of the dataset is shown"
P17-1015,D16-1011,0,0.0607361,"Missing"
P17-1015,P16-1057,1,0.916495,"Missing"
P17-1112,D15-1198,0,0.237449,"Missing"
P17-1112,L16-1197,0,0.0290164,"r Lexical Functional Grammar (LFG) (Kaplan and Bresnan, 1982). MRS has been implement the English Resource Grammar (ERG) (Flickinger, 2000), a broad-coverage high-precision HPSG grammar. Oepen and Lønning (2006) proposed Elementary Dependency Structure (EDS), a conversion of MRS to variable-free dependency graphs which drops scope underspecification. Copestake (2009) extended this conversion to avoid information loss, primarily through richer edge labels. The resulting representation, Dependency MRS (DMRS), can be converted back to the original MRS, or used directly in MRS-based applications (Copestake et al., 2016). We are interested in the empirical performance of parsers for both of these representations: while EDS is more interpretable as an independent semantic graph representation, DMRS can be related back to underspecified logical forms. A bilexical simplification of EDS has previously been used for semantic dependency parsing (Oepen et al., 2014, 2015). Figure 1 illustrates an EDS graph. MRS makes an explicit distinction between surface and abstract predicates (by convention surface predicates are prefixed by an underscore). Surface predicates consist of a lemma followed by a coarse part-of-speec"
P17-1112,1995.tmi-1.2,0,0.106978,"rformed traditional pipeline approaches, predicting syntactic or semantic structure as intermediate steps, on NLU tasks such as sentiment analysis and semantic relatedness (Le and Mikolov, 2014; Kiros et al., 2015), question answering (Hermann et al., 2015) and textual entailment (Rockt¨aschel et al., 2015). However the linguistic structure used in applications has predominantly been shallow, restricted to bilexical dependencies or trees. In this paper we focus on robust parsing into linguistically deep representations. The main representation that we use is Minimal Recursion Semantics (MRS) (Copestake et al., 1995, 2005), which serves as the semantic representation of the English Resource Grammar (ERG) (Flickinger, 2000). Existing parsers for full MRS (as opposed to bilexical semantic graphs derived from, but simplifying MRS) are grammar-based, performing disambiguation with a maximum entropy model (Toutanova et al., 2005; Zhang et al., 2007); this approach has high precision but incomplete coverage. Our main contribution is to develop a fast and robust parser for full MRS-based semantic graphs. We exploit the power of global conditioning enabled by deep learning to predict linguistically deep graphs i"
P17-1112,W13-2322,0,0.788202,"n contribution is to develop a fast and robust parser for full MRS-based semantic graphs. We exploit the power of global conditioning enabled by deep learning to predict linguistically deep graphs incrementally. The model does not have access to the underlying ERG or syntactic structures from which the MRS analyses were originally derived. We develop parsers for two graph-based conversions of MRS, Elementary Dependency Structure (EDS) (Oepen and Lønning, 2006) and Dependency MRS (DMRS) (Copestake, 2009), of which the latter is inter-convertible with MRS. Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a graph-based semantic representation that shares the goals of MRS. Aside from differences in the choice of which linguistic phenomena are annotated, MRS is a compositional representation explicitly coupled with the syntactic structure of the sentence, while AMR does not assume compositionality or alignment with the sentence structure. Recently a number of AMR parsers have been developed (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; 1215 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1215–1226 c Vancouver, Canada, July 30 -"
P17-1112,P16-2006,0,0.0195307,"ion, as αji = softmax(uij ). However instead of making a hard selection, a weighted average the encoder vectors is computed as Pover i=I i qj = α i=1 j hi . This vector is used instead of haj for prediction and feeding to the next timestep. 1219 4.3 Stack-based model We extend the hard attention model to include features based on the transition system stack. These features are embeddings from the bidirectional RNN encoder, corresponding to the alignments of the nodes on the buffer and on top of the stack. This approach is similar to the features proposed by Kiperwasser and Goldberg (2016) and Cross and Huang (2016a) for dependency parsing, although they do not use RNN decoders. To implement these features the layer that computes the output vector is extended to oj = W (3) sj + W (4) haj + W (7) hst0 , where st0 is the sentence alignment index of the element on top of the stack. The input layer to the next RNN time-step is similarly extended to dj+1 = W (5) e(tj ) + W (6) hbuf + W (8) hst0 , where buf is the buffer alignment after tj is executed. Our implementation of the stack-based model enables batch processing in static computation graphs, similar to Bowman et al. (2016). We maintain a stack of alig"
P17-1112,S16-1176,0,0.146164,", t)1:j−1 , e)p(tj |a1:j , t1:j−1 , e). We also predict the end-of-span alignments as a seperate sequence a(e) . 3.1 Top-down linearization We now consider how to linearize the semantic graphs, before defining the neural models to parameterize the parser in section 4. The first approach is to linearize a graph as the pre-order traversal of its spanning tree, starting at a designated root node (see Figure 2). Variants of this approach have been proposed for neural constituency parsing (Vinyals et al., 2015b), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). In the linearization, labels of edges whose direction are reversed in the spanning tree are marked 1217 by adding -of. Edges not included in the spanning tree, referred to as reentrancies, are represented with special edges whose dependents are dummy nodes pointing back to the original nodes. Our potentially lossy representation represents these edges by repeating the dependent node labels and alignments, which are recovered heuristically. The alignment does not influence the linearized node ordering. 3.2 Transition-based parsing Figure 1 shows that the semantic graphs we"
P17-1112,E17-1051,0,0.359654,"to sentence tokens. Transition-based parsing (Nivre, 2008) has been used extensively to predict dependency graphs incrementally. We apply a variant of the arc-eager transition system that has been proposed for graph (as opposed to tree) parsing (Sagae and Tsujii, 2008; Titov et al., 2009; G´omez-Rodr´ıguez and Nivre, 2010) to derive a transition-based parser for deep semantic graphs. In dependency parsing the sentence tokens also act as nodes in the graph, but here we need to generate the nodes incrementally as the transition-system proceeds, conditioning the generation on the given sentence. Damonte et al. (2017) proposed an arc-eager AMR parser, but their transition system is more narrowly restricted to AMR graphs. The transition system consists of a stack of graph nodes being processed and a buffer, holding a single node at a time. The main transition actions are shift, reduce, left-arc, right-arc. Figure 3 shows an example transition sequence together with the stack and buffer after each step. The shift transition moves the element on the buffer to the top of the stack, and generates a predicate and its alignment as the next node on the buffer. Left-arc and right-arc actions add labeled arcs betwee"
P17-1112,W15-0128,0,0.146038,"Missing"
P17-1112,P16-1004,0,0.0208414,"l distribution p(t, a|e) which decomposes as J Y j=1 p(aj |(a, t)1:j−1 , e)p(tj |a1:j , t1:j−1 , e). We also predict the end-of-span alignments as a seperate sequence a(e) . 3.1 Top-down linearization We now consider how to linearize the semantic graphs, before defining the neural models to parameterize the parser in section 4. The first approach is to linearize a graph as the pre-order traversal of its spanning tree, starting at a designated root node (see Figure 2). Variants of this approach have been proposed for neural constituency parsing (Vinyals et al., 2015b), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). In the linearization, labels of edges whose direction are reversed in the spanning tree are marked 1217 by adding -of. Edges not included in the spanning tree, referred to as reentrancies, are represented with special edges whose dependents are dummy nodes pointing back to the original nodes. Our potentially lossy representation represents these edges by repeating the dependent node labels and alignments, which are recovered heuristically. The alignment does not influence the linearized node ordering. 3.2 Tra"
P17-1112,P16-1139,0,0.0594325,"Missing"
P17-1112,S17-2157,1,0.856121,"tionary, and where no dictionary entry is available with a lemmatizer. The same approach is applied to predict constants, along with additional normalizations such as mapping numbers to digit strings. We use the Stanford CoreNLP toolkit (Manning et al., 2014) to tokenize and lemmatize sentences, and tag tokens with the Stanford Named Entity Recognizer (Finkel et al., 2005). The tokenization is customized to correspond closely to the ERG tokenization; hyphens are removed pre-processing step. For AMR we use automatic alignments and the graph topology to classify concepts as surface or abstract (Buys and Blunsom, 2017). The lexicon is restricted to Propbank (Palmer et al., 2005) predicates; for other concepts we extract a lexicon from the training data. 1218 Action init(1, person) sh(1, every q) la(BV) sh(2, v 1) re la(ARG1) Stack [] [(1, 1, person)] [(1, 1, person)] [(1, 1, person), (2, 1, every q)] [(1, 1, person)] [(1, 1, person)] Buffer (1, 1, person) (2, 1, every q) (2, 1, every q) (2, 1, v 1) (3, 2, v 1) (3, 2, v 1) Arc added (2, BV, 1) (3, ARG1, 1) Figure 3: Start of the transition sequence for parsing the graph in Figure 1. The transitions are shift (sh), reduce (re), left arc (la) and right arc (ra"
P17-1112,P13-2131,0,0.286414,"et al., 2014). 6.2 Evaluation Dridan and Oepen (2011) proposed an evaluation metric called Elementary Dependency Matching (EDM) for MRS-based graphs. EDM computes the F1-score of tuples of predicates and arguments. A predicate tuple consists of the label and character span of a predicate, while an argument tuple consists of the character spans of the head and dependent nodes of the relation, together with the argument label. In order to tolerate subtle tokenization differences with respect to punctuation, we allow span pairs whose ends differ by one character to be matched. The Smatch metric (Cai and Knight, 2013), proposed for evaluating AMR graphs, also measures graph overlap, but does not rely on sentence alignments to determine the correspondences between graph nodes. Smatch is instead computed by performing inference over graph alignments to estimate the maximum F1-score obtainable from a one-to-one matching between the predicted and gold graph nodes. 2 http://svn.delph-in.net/erg/tags/ 1214/ 3 http://moin.delph-in.net/LogonTop 4 https://github.com/delph-in/pydelphin Model TD lex TD unlex AE lex AE unlex EDM 81.44 81.72 81.35 82.56 EDMP 85.20 85.59 85.79 86.76 EDMA 76.87 77.04 76.02 77.54 Table 1:"
P17-1112,E09-1001,0,0.634431,"utanova et al., 2005; Zhang et al., 2007); this approach has high precision but incomplete coverage. Our main contribution is to develop a fast and robust parser for full MRS-based semantic graphs. We exploit the power of global conditioning enabled by deep learning to predict linguistically deep graphs incrementally. The model does not have access to the underlying ERG or syntactic structures from which the MRS analyses were originally derived. We develop parsers for two graph-based conversions of MRS, Elementary Dependency Structure (EDS) (Oepen and Lønning, 2006) and Dependency MRS (DMRS) (Copestake, 2009), of which the latter is inter-convertible with MRS. Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a graph-based semantic representation that shares the goals of MRS. Aside from differences in the choice of which linguistic phenomena are annotated, MRS is a compositional representation explicitly coupled with the syntactic structure of the sentence, while AMR does not assume compositionality or alignment with the sentence structure. Recently a number of AMR parsers have been developed (Flanigan et al., 2014; Wang et al., 2015b; Artzi et al., 2015; 1215 Proceedings of the 55"
P17-1112,D16-1001,0,0.0201644,"ion, as αji = softmax(uij ). However instead of making a hard selection, a weighted average the encoder vectors is computed as Pover i=I i qj = α i=1 j hi . This vector is used instead of haj for prediction and feeding to the next timestep. 1219 4.3 Stack-based model We extend the hard attention model to include features based on the transition system stack. These features are embeddings from the bidirectional RNN encoder, corresponding to the alignments of the nodes on the buffer and on top of the stack. This approach is similar to the features proposed by Kiperwasser and Goldberg (2016) and Cross and Huang (2016a) for dependency parsing, although they do not use RNN decoders. To implement these features the layer that computes the output vector is extended to oj = W (3) sj + W (4) haj + W (7) hst0 , where st0 is the sentence alignment index of the element on top of the stack. The input layer to the next RNN time-step is similarly extended to dj+1 = W (5) e(tj ) + W (6) hbuf + W (8) hst0 , where buf is the buffer alignment after tj is executed. Our implementation of the stack-based model enables batch processing in static computation graphs, similar to Bowman et al. (2016). We maintain a stack of alig"
P17-1112,W11-2927,0,0.325439,"esponding to ERG 12142 , following the suggested split of sections 0 to 19 as training data data, 20 for development and 21 for testing. The gold-annotated training data consists of 35,315 sentences. We use the LOGON environment3 and the pyDelphin library4 to extract DMRS and EDS graphs. For AMR parsing we use LDC2015E86, the dataset released for the SemEval 2016 AMR parsing Shared Task (May, 2016). This data includes newswire, weblog and discussion forum text. The training set has 16,144 sentences. We obtain alignments using the rule-based JAMR aligner (Flanigan et al., 2014). 6.2 Evaluation Dridan and Oepen (2011) proposed an evaluation metric called Elementary Dependency Matching (EDM) for MRS-based graphs. EDM computes the F1-score of tuples of predicates and arguments. A predicate tuple consists of the label and character span of a predicate, while an argument tuple consists of the character spans of the head and dependent nodes of the relation, together with the argument label. In order to tolerate subtle tokenization differences with respect to punctuation, we allow span pairs whose ends differ by one character to be matched. The Smatch metric (Cai and Knight, 2013), proposed for evaluating AMR gr"
P17-1112,P15-1033,0,0.0670335,"Missing"
P17-1112,N16-1024,0,0.0751512,"Missing"
P17-1112,P05-1045,0,0.00574526,"ized predicates consisting only of sense labels. The full surface predicates are then recovered through the predicted alignments. We extract a dictionary mapping words to lemmas from the ERG lexicon. Candidate lemmas are predicted using this dictionary, and where no dictionary entry is available with a lemmatizer. The same approach is applied to predict constants, along with additional normalizations such as mapping numbers to digit strings. We use the Stanford CoreNLP toolkit (Manning et al., 2014) to tokenize and lemmatize sentences, and tag tokens with the Stanford Named Entity Recognizer (Finkel et al., 2005). The tokenization is customized to correspond closely to the ERG tokenization; hyphens are removed pre-processing step. For AMR we use automatic alignments and the graph topology to classify concepts as surface or abstract (Buys and Blunsom, 2017). The lexicon is restricted to Propbank (Palmer et al., 2005) predicates; for other concepts we extract a lexicon from the training data. 1218 Action init(1, person) sh(1, every q) la(BV) sh(2, v 1) re la(ARG1) Stack [] [(1, 1, person)] [(1, 1, person)] [(1, 1, person), (2, 1, every q)] [(1, 1, person)] [(1, 1, person)] Buffer (1, 1, person) (2, 1, e"
P17-1112,P14-1134,0,0.354642,"sed with the grammar-based parser. Ytrestøl (2012) proposed a transition-based approach to HPSG parsing that produces derivations from which both syntactic and semantic (MRS) parses can be extracted. The parser has an option not to be restricted by the ERG. However, neither of these approaches have results available that can be compared directly to our setup, or generally available implementations. Although AMR parsers produce graphs that are similar in structure to MRS-based graphs, most of them make assumptions that are invalid for MRS, and rely on extensive external AMR-specific resources. Flanigan et al. (2014) proposed a twostage parser that first predicts concepts or subgraphs corresponding to sentence segments, and then parses these concepts into a graph structure. However MRS has a large proportion of abstract nodes that cannot be predicted from short segments, and interact closely with the graph structure. Wang et al. (2015b,a) proposed a custom transition-system for AMR parsing that converts dependency trees to AMR graphs, relying on assumptions on the relationship between these. Pust et al. (2015) proposed a parser based on syntaxbased machine translation (MT), while AMR has also been integra"
P17-1112,P16-1002,0,0.0282321,") which decomposes as J Y j=1 p(aj |(a, t)1:j−1 , e)p(tj |a1:j , t1:j−1 , e). We also predict the end-of-span alignments as a seperate sequence a(e) . 3.1 Top-down linearization We now consider how to linearize the semantic graphs, before defining the neural models to parameterize the parser in section 4. The first approach is to linearize a graph as the pre-order traversal of its spanning tree, starting at a designated root node (see Figure 2). Variants of this approach have been proposed for neural constituency parsing (Vinyals et al., 2015b), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). In the linearization, labels of edges whose direction are reversed in the spanning tree are marked 1217 by adding -of. Edges not included in the spanning tree, referred to as reentrancies, are represented with special edges whose dependents are dummy nodes pointing back to the original nodes. Our potentially lossy representation represents these edges by repeating the dependent node labels and alignments, which are recovered heuristically. The alignment does not influence the linearized node ordering. 3.2 Transition-based parsing"
P17-1112,Q16-1023,0,0.0402646,"tion is computed as with hard attention, as αji = softmax(uij ). However instead of making a hard selection, a weighted average the encoder vectors is computed as Pover i=I i qj = α i=1 j hi . This vector is used instead of haj for prediction and feeding to the next timestep. 1219 4.3 Stack-based model We extend the hard attention model to include features based on the transition system stack. These features are embeddings from the bidirectional RNN encoder, corresponding to the alignments of the nodes on the buffer and on top of the stack. This approach is similar to the features proposed by Kiperwasser and Goldberg (2016) and Cross and Huang (2016a) for dependency parsing, although they do not use RNN decoders. To implement these features the layer that computes the output vector is extended to oj = W (3) sj + W (4) haj + W (7) hst0 , where st0 is the sentence alignment index of the element on top of the stack. The input layer to the next RNN time-step is similarly extended to dj+1 = W (5) e(tj ) + W (6) hbuf + W (8) hst0 , where buf is the buffer alignment after tj is executed. Our implementation of the stack-based model enables batch processing in static computation graphs, similar to Bowman et al. (2016). W"
P17-1112,J16-4009,0,0.171421,"Missing"
P17-1112,P14-5010,0,0.00453473,"e predicate. We decompose surface predicate prediction by predicting candidate lemmas for input tokens, and delexicalized predicates consisting only of sense labels. The full surface predicates are then recovered through the predicted alignments. We extract a dictionary mapping words to lemmas from the ERG lexicon. Candidate lemmas are predicted using this dictionary, and where no dictionary entry is available with a lemmatizer. The same approach is applied to predict constants, along with additional normalizations such as mapping numbers to digit strings. We use the Stanford CoreNLP toolkit (Manning et al., 2014) to tokenize and lemmatize sentences, and tag tokens with the Stanford Named Entity Recognizer (Finkel et al., 2005). The tokenization is customized to correspond closely to the ERG tokenization; hyphens are removed pre-processing step. For AMR we use automatic alignments and the graph topology to classify concepts as surface or abstract (Buys and Blunsom, 2017). The lexicon is restricted to Propbank (Palmer et al., 2005) predicates; for other concepts we extract a lexicon from the training data. 1218 Action init(1, person) sh(1, every q) la(BV) sh(2, v 1) re la(ARG1) Stack [] [(1, 1, person)]"
P17-1112,S16-1166,0,0.411803,"Missing"
P17-1112,D16-1183,0,0.0286896,"cepts or subgraphs corresponding to sentence segments, and then parses these concepts into a graph structure. However MRS has a large proportion of abstract nodes that cannot be predicted from short segments, and interact closely with the graph structure. Wang et al. (2015b,a) proposed a custom transition-system for AMR parsing that converts dependency trees to AMR graphs, relying on assumptions on the relationship between these. Pust et al. (2015) proposed a parser based on syntaxbased machine translation (MT), while AMR has also been integrated into CCG Semantic Parsing (Artzi et al., 2015; Misra and Artzi, 2016). Recently Damonte et al. (2017) and Peng et al. (2017) proposed AMR parsers based on neural networks. 6 Experiments 6.1 Data DeepBank (Flickinger et al., 2012) is an HPSG and MRS annotation of the Penn Treebank Wall Street Journal (WSJ) corpus. It was developed following an approach known as dynamic treebanking (Oepen et al., 2004) that couples treebank annotation with grammar development, in this case 1220 1 http://sweaglesw.org/linguistics/ace/ of the ERG. This approach has been shown to lead to high inter-annotator agreement: 0.94 against 0.71 for AMR (Bender et al., 2015). Parses are only"
P17-1112,J08-4003,0,0.0183696,"7 by adding -of. Edges not included in the spanning tree, referred to as reentrancies, are represented with special edges whose dependents are dummy nodes pointing back to the original nodes. Our potentially lossy representation represents these edges by repeating the dependent node labels and alignments, which are recovered heuristically. The alignment does not influence the linearized node ordering. 3.2 Transition-based parsing Figure 1 shows that the semantic graphs we work with can also be interpreted as dependency graphs, as nodes are aligned to sentence tokens. Transition-based parsing (Nivre, 2008) has been used extensively to predict dependency graphs incrementally. We apply a variant of the arc-eager transition system that has been proposed for graph (as opposed to tree) parsing (Sagae and Tsujii, 2008; Titov et al., 2009; G´omez-Rodr´ıguez and Nivre, 2010) to derive a transition-based parser for deep semantic graphs. In dependency parsing the sentence tokens also act as nodes in the graph, but here we need to generate the nodes incrementally as the transition-system proceeds, conditioning the generation on the given sentence. Damonte et al. (2017) proposed an arc-eager AMR parser, bu"
P17-1112,S15-2153,0,0.594099,"Missing"
P17-1112,S14-2008,0,0.116843,"on. Copestake (2009) extended this conversion to avoid information loss, primarily through richer edge labels. The resulting representation, Dependency MRS (DMRS), can be converted back to the original MRS, or used directly in MRS-based applications (Copestake et al., 2016). We are interested in the empirical performance of parsers for both of these representations: while EDS is more interpretable as an independent semantic graph representation, DMRS can be related back to underspecified logical forms. A bilexical simplification of EDS has previously been used for semantic dependency parsing (Oepen et al., 2014, 2015). Figure 1 illustrates an EDS graph. MRS makes an explicit distinction between surface and abstract predicates (by convention surface predicates are prefixed by an underscore). Surface predicates consist of a lemma followed by a coarse part-of-speech tag and an optional sense label. Predicates absent from the ERG lexicon are represented by their surface forms and POS tags. We convert the character-level predicate spans given by MRS to token-level spans for parsing purposes, but the representation does not require gold tokenization. Surface predicates usually align with the span of the t"
P17-1112,J05-1004,0,0.641582,"do not currently model these features. AMR (Banarescu et al., 2013) graphs can be represented in the same framework, despite a number of linguistic differences with MRS. Some in:root( &lt;2&gt; _v_1 :ARG1( &lt;1&gt; person :BV-of( &lt;1&gt; every_q ) ) :ARG2 &lt;4&gt; _v_1 :ARG1*( &lt;1&gt; person :ARG2( &lt;5&gt; named_CARG :BV-of ( &lt;5&gt; proper_q ) ) ) Figure 2: A top-down linearization of the EDS graph in Figure 1, using unlexicalized predicates. formation annotated explicitly in MRS is latent in AMR, including alignments and the distinction between surface (lexical) and abstract concepts. AMR predicates are based on PropBank (Palmer et al., 2005), annotated as lemmas plus sense labels, but they form only a subset of concepts. Other concepts are either English words or special keywords, corresponding to overt lexemes in some cases but not others. 3 Incremental Graph Parsing We parse sentences to their meaning representations by incrementally predicting semantic graphs together with their alignments. Let e = e1 , e2 , . . . , eI be a tokenized English sentence, t = t1 , t2 , . . . , tJ a sequential representation of its graph derivation and a = a1 , a2 , . . . , aJ an alignment sequence consisting of integers in the range 1, . . . , I."
P17-1112,S16-1183,0,0.0709824,"ags behind the performance of state-of-the-art AMR parsers such as CAMR (Wang et al., 2016) and AMR Eager (Damonte et al., 2017). These models make extensive use of external resources, including syntactic parsers and semantic role labellers. Our attention-based encoder-decoder model already outperforms previous sequence-to-sequence 6 http://sdp.delph-in.net/osdp-12.tgz Smatch 56 66.54 64 55 52 43.3 56.56 60.11 AMR parsers (Barzdins and Gosko, 2016; Peng et al., 2017), and the arc-eager model boosts accuracy further. Our model also outperforms a Synchronous Hyperedge Replacement Grammar model (Peng and Gildea, 2016) which is comparable as it does not make extensive use of external resources. 7 Conclusion In this paper we advance the state of parsing by employing deep learning techniques to parse sentence to linguistically expressive semantic representations that have not previously been parsed in an end-to-end fashion. We presented a robust, wide-coverage parser for MRS that is faster than existing parsers and amenable to batch processing. We believe that there are many future avenues to explore to further increase the accuracy of such parsers, including different training objectives, more structured arc"
P17-1112,N15-1142,0,0.0200572,"se single-layer LSTMs with dropout of 0.3 (tuned on the development set) on input and output connections. We use encoder and decoder embeddings of size 256, and POS and NE tag embeddings of size 32, For DMRS and EDS graphs the hidden units size is set to 256, for AMR it is 128. This configuration, found using grid search and heuristic search within the range of models that fit into a single GPU, gave the best performance on the development set under multiple graph linearizations. Encoder word embeddings are initialized (in the first 100 dimensions) with pre-trained order-sensitive embeddings (Ling et al., 2015). Singletons in the encoder input are replaced with an unknown word symbol with probability 0.5 for each iteration. 6.4 MRS parsing results We compare different linearizations and model architectures for parsing DMRS on the development data, showing that our approach is more accurate than baseline neural approaches. We report EDM scores, including scores for predicate (EDMP ) and argument (EDMA ) prediction. First we report results using standard attentionbased encoder-decoders, with the alignments encoded as token strings in the linearization. (Table 1). We compare the top-down (TD) and arcea"
P17-1112,E17-1035,0,0.647658,"t1:j−1 , e). We also predict the end-of-span alignments as a seperate sequence a(e) . 3.1 Top-down linearization We now consider how to linearize the semantic graphs, before defining the neural models to parameterize the parser in section 4. The first approach is to linearize a graph as the pre-order traversal of its spanning tree, starting at a designated root node (see Figure 2). Variants of this approach have been proposed for neural constituency parsing (Vinyals et al., 2015b), logical form prediction (Dong and Lapata, 2016; Jia and Liang, 2016) and AMR parsing (Barzdins and Gosko, 2016; Peng et al., 2017). In the linearization, labels of edges whose direction are reversed in the spanning tree are marked 1217 by adding -of. Edges not included in the spanning tree, referred to as reentrancies, are represented with special edges whose dependents are dummy nodes pointing back to the original nodes. Our potentially lossy representation represents these edges by repeating the dependent node labels and alignments, which are recovered heuristically. The alignment does not influence the linearized node ordering. 3.2 Transition-based parsing Figure 1 shows that the semantic graphs we work with can also"
P17-1112,D15-1136,0,0.152692,"assumptions that are invalid for MRS, and rely on extensive external AMR-specific resources. Flanigan et al. (2014) proposed a twostage parser that first predicts concepts or subgraphs corresponding to sentence segments, and then parses these concepts into a graph structure. However MRS has a large proportion of abstract nodes that cannot be predicted from short segments, and interact closely with the graph structure. Wang et al. (2015b,a) proposed a custom transition-system for AMR parsing that converts dependency trees to AMR graphs, relying on assumptions on the relationship between these. Pust et al. (2015) proposed a parser based on syntaxbased machine translation (MT), while AMR has also been integrated into CCG Semantic Parsing (Artzi et al., 2015; Misra and Artzi, 2016). Recently Damonte et al. (2017) and Peng et al. (2017) proposed AMR parsers based on neural networks. 6 Experiments 6.1 Data DeepBank (Flickinger et al., 2012) is an HPSG and MRS annotation of the Penn Treebank Wall Street Journal (WSJ) corpus. It was developed following an approach known as dynamic treebanking (Oepen et al., 2004) that couples treebank annotation with grammar development, in this case 1220 1 http://sweaglesw"
P17-1112,W11-2923,0,0.0279051,"consistent with the grammar, in this case the ERG (Flickinger, 2000). The nodes in the derivation trees are feature structures, from which MRS is extracted through unification. This approach fails to parse sentences for which no valid derivation is found. Maximum entropy models are used to score the derivations in order to find the most likely parse (Toutanova et al., 2005). This approach is implemented in the PET (Callmeier, 2000) and ACE1 parsers. There have also been some efforts to develop robust MRS parsers. One proposed approach learns a PCFG grammar to approximate the HPSG derivations (Zhang and Krieger, 2011; Zhang et al., 2014). MRS is then extracted with robust unification to compose potentially incompatible feature structures, although that still fails for a small proportion of sentences. The model is trained on a large corpus of Wikipedia text parsed with the grammar-based parser. Ytrestøl (2012) proposed a transition-based approach to HPSG parsing that produces derivations from which both syntactic and semantic (MRS) parses can be extracted. The parser has an option not to be restricted by the ERG. However, neither of these approaches have results available that can be compared directly to o"
P17-1112,W07-2207,0,0.0205758,"ture used in applications has predominantly been shallow, restricted to bilexical dependencies or trees. In this paper we focus on robust parsing into linguistically deep representations. The main representation that we use is Minimal Recursion Semantics (MRS) (Copestake et al., 1995, 2005), which serves as the semantic representation of the English Resource Grammar (ERG) (Flickinger, 2000). Existing parsers for full MRS (as opposed to bilexical semantic graphs derived from, but simplifying MRS) are grammar-based, performing disambiguation with a maximum entropy model (Toutanova et al., 2005; Zhang et al., 2007); this approach has high precision but incomplete coverage. Our main contribution is to develop a fast and robust parser for full MRS-based semantic graphs. We exploit the power of global conditioning enabled by deep learning to predict linguistically deep graphs incrementally. The model does not have access to the underlying ERG or syntactic structures from which the MRS analyses were originally derived. We develop parsers for two graph-based conversions of MRS, Elementary Dependency Structure (EDS) (Oepen and Lønning, 2006) and Dependency MRS (DMRS) (Copestake, 2009), of which the latter is"
P17-1112,S16-1181,0,0.100907,"Missing"
P17-1112,P15-2141,0,0.0885285,"compared directly to our setup, or generally available implementations. Although AMR parsers produce graphs that are similar in structure to MRS-based graphs, most of them make assumptions that are invalid for MRS, and rely on extensive external AMR-specific resources. Flanigan et al. (2014) proposed a twostage parser that first predicts concepts or subgraphs corresponding to sentence segments, and then parses these concepts into a graph structure. However MRS has a large proportion of abstract nodes that cannot be predicted from short segments, and interact closely with the graph structure. Wang et al. (2015b,a) proposed a custom transition-system for AMR parsing that converts dependency trees to AMR graphs, relying on assumptions on the relationship between these. Pust et al. (2015) proposed a parser based on syntaxbased machine translation (MT), while AMR has also been integrated into CCG Semantic Parsing (Artzi et al., 2015; Misra and Artzi, 2016). Recently Damonte et al. (2017) and Peng et al. (2017) proposed AMR parsers based on neural networks. 6 Experiments 6.1 Data DeepBank (Flickinger et al., 2012) is an HPSG and MRS annotation of the Penn Treebank Wall Street Journal (WSJ) corpus. It wa"
P17-1112,N15-1040,0,0.185146,"compared directly to our setup, or generally available implementations. Although AMR parsers produce graphs that are similar in structure to MRS-based graphs, most of them make assumptions that are invalid for MRS, and rely on extensive external AMR-specific resources. Flanigan et al. (2014) proposed a twostage parser that first predicts concepts or subgraphs corresponding to sentence segments, and then parses these concepts into a graph structure. However MRS has a large proportion of abstract nodes that cannot be predicted from short segments, and interact closely with the graph structure. Wang et al. (2015b,a) proposed a custom transition-system for AMR parsing that converts dependency trees to AMR graphs, relying on assumptions on the relationship between these. Pust et al. (2015) proposed a parser based on syntaxbased machine translation (MT), while AMR has also been integrated into CCG Semantic Parsing (Artzi et al., 2015; Misra and Artzi, 2016). Recently Damonte et al. (2017) and Peng et al. (2017) proposed AMR parsers based on neural networks. 6 Experiments 6.1 Data DeepBank (Flickinger et al., 2012) is an HPSG and MRS annotation of the Penn Treebank Wall Street Journal (WSJ) corpus. It wa"
P17-1112,P10-1151,0,\N,Missing
P17-1137,N13-1140,1,0.778979,"neural language models have been widely explored (Sutskever et al., 2011; Mikolov et al., 2012; Graves, 2013, inter alia). Attempts to make them more aware of word-level dynamics, using models similar to our hierarchical formulation, have also been proposed (Chung et al., 2017). The only models that are open vocabulary language modeling together with a caching mechanism are the nonparametric Bayesian language models based on hierarchical Pitman–Yor processes which generate a lexicon of word types using a character model, and then generate a text using these (Teh, 2006; Goldwater et al., 2009; Chahuneau et al., 2013). These, however, do not use distributed representations on RNNs to capture long-range dependencies. 8 Conclusion In this paper, we proposed a character-level language model with an adaptive cache which selectively assign word probability from past history or character-level decoding. And we empirically show that our model efficiently model the word sequences and achieved better perplexity in every standard dataset. To further validate the performance of our model on different languages, we collected multilingual wikipedia corpus for 7 typologically diverse languages. We also show that our mod"
P17-1137,C00-1027,0,0.165129,"are typically replaced by a special token, called the unknown word token, hUNKi. Although fixedvocabulary language models have some important practical applications and are appealing models for study, they fail to capture two empirical facts about the distribution of words in natural languages. First, vocabularies keep growing as the number of documents in a corpus grows: new words are constantly being created (Heaps, 1978). Second, rare and newly created words often occur in “bursts”, i.e., once a new or rare word has been used once in a document, it is often repeated (Church and Gale, 1995; Church, 2000). The open-vocabulary problem can be solved by dispensing with word-level models in favor of models that predict sentences as sequences of characters (Sutskever et al., 2011; Chung et al., 2017). Character-based models are quite successful at learning what (new) word forms look like (e.g., they learn a language’s orthographic conventions that tell us that sustinated is a plausible English word and bzoxqir is not) and, when based on models that learn long-range dependencies such as RNNs, they can also be good models of how words fit together to form sentences. However, existing character-sequen"
P17-1137,D15-1176,1,0.797601,"n, we describe our hierarchical character language model with a word cache. As is typical for RNN language models, our model uses the chain rule to decompose the problem into incremental predictions of the next word conditioned on the history: p(w) = |w| Y t=1 p(wt |w<t ). We make two modifications to the traditional RNN language model, which we describe in turn. First, we begin with a cache-less model we call the hierarchical character language model (HCLM; §2.1) which generates words as a sequence of characters and constructs a “word embedding” by encoding a character sequence with an LSTM (Ling et al., 2015). However, like conventional closedvocabulary, word-based models, it is based on an LSTM that conditions on words represented by fixed-length vectors.1 The HCLM has no mechanism to reuse words that it has previously generated, so new forms will 1 The HCLM is an adaptation of the hierarchical recurrent encoder-decoder of Sordoni et al. (2015) which was used to model dialog as a sequence of actions sentences which are themselves sequences of words. The original model was proposed to compose words into query sequences but we use it to compose characters into word sequences. only be repeated with"
P17-1137,C16-1165,0,0.018917,"Missing"
P17-1137,P06-1124,0,0.0147366,"ave et al., 2017). Open vocabulary neural language models have been widely explored (Sutskever et al., 2011; Mikolov et al., 2012; Graves, 2013, inter alia). Attempts to make them more aware of word-level dynamics, using models similar to our hierarchical formulation, have also been proposed (Chung et al., 2017). The only models that are open vocabulary language modeling together with a caching mechanism are the nonparametric Bayesian language models based on hierarchical Pitman–Yor processes which generate a lexicon of word types using a character model, and then generate a text using these (Teh, 2006; Goldwater et al., 2009; Chahuneau et al., 2013). These, however, do not use distributed representations on RNNs to capture long-range dependencies. 8 Conclusion In this paper, we proposed a character-level language model with an adaptive cache which selectively assign word probability from past history or character-level decoding. And we empirically show that our model efficiently model the word sequences and achieved better perplexity in every standard dataset. To further validate the performance of our model on different languages, we collected multilingual wikipedia corpus for 7 typologic"
P18-1132,D10-1066,0,0.025447,"The fox eats worms Stack 0 1 2 3 4 5 6 7 8 9 10 11 12 The (NP |The (NP |The |fox (NP The fox) (S |(NP The fox) (S |(NP The fox) |eats (S |(NP The fox) |(VP |eats (S |(NP The fox) |(VP |eats |worms (S |(NP The fox) |(VP |eats |(NP |worms (S |(NP The fox) |(VP |eats |(NP worms) (S |(NP The fox) |(VP eats (NP worms)) (S (NP The fox) (VP eats (NP worms))) Action GEN (The) NT SW (NP) GEN (fox) anticipatory representations, it is said, could explain the rapid, incremental processing that humans seem to exhibit (Marslen-Wilson, 1973; Tanenhaus et al., 1995); this line of thinking similarly motivates Charniak (2010), among others. While most work in this domain has been concerned with the parsing problem, our findings suggest that anticipatory mechanisms are also beneficial in capturing structural dependencies in language modeling. We note that our results are achieved using models that, in theory, are able to condition on the entire derivation history, while earlier work in sentence processing has focused on cognitive memory considerations, such as the memory-bounded model of Schuler et al. (2010). REDUCE 5 NT SW (S) Conclusion GEN (eats) NT SW (VP) GEN (worms) NT SW (NP) REDUCE REDUCE REDUCE N/A Figure"
P18-1132,D16-1257,0,0.420859,"that explicitly models syntactic structures, the recurrent neural network grammars (Dyer et al., 2016, RNNGs), considerably outperforms sequential LSTM language models for cases with multiple attractors (§3). We present experiments affirming that this gain is due to an explicit composition operator rather than the presence of predicted syntactic annotations. Rather surprisingly, syntactic LSTM language models without explicit composition have no advantage over sequential LSTMs that operate on word sequences, although these models can nevertheless be excellent predictors of phrase structures (Choe and Charniak, 2016). Having established the importance of modeling structures, we explore the hypothesis that how we build the structure affects the model’s ability to identify structural dependencies in English. As RNNGs build phrase-structure trees through top-down operations, we propose extensions to the structure-building sequences and model architecture that enable left-corner (Henderson, 2003, 2004) and bottom-up (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) generation orders (§4). Extensive prior work has characterized topdown, left-corner, and bottom-up parsing strategies in terms of cognitive plau"
P18-1132,E17-1117,1,0.823907,"Missing"
P18-1132,E17-2020,0,0.0449163,"Missing"
P18-1132,1997.iwpt-1.18,0,0.450127,"y of strings and phrase-structure trees, thereby imposing different biases on the learner. Earlier work in parsing has characterized the plausibility of top-down, left-corner, and bottom-up strategies as viable candidates of human sentence processing, especially in terms of memory constraints and human difficulties with center embedding constructions (Johnson-Laird, 1983; Pulman, 1986; Abney and Johnson, 1991; Resnik, 1992, inter alia), along with neurophysiological evidence in human sentence processing (Nelson et al., 2017). Here we cast the three strategies as models of language generation (Manning and Carpenter, 1997), and focus on the empirical question: which generation order has the most appropriate bias in modeling non-local structural dependencies in English? These alternative orders organize the learning problem so as to yield intermediate states in generation that condition on different aspects of the grammatical structure. In number agreement, this amounts to making an agreement controller, such as the word flowers in Fig. 3, more or less salient. If it is more salient, the model should be better-able to inflect the main verb in agreement with this controller, without getting distracted by the attr"
P18-1132,N16-1024,1,0.564713,"in the number agreement task. Given the strong performance of word-based LSTM language models, are there are any substantial benefits, in terms of number agreement accuracy, to explicitly modeling hierarchical structures as an inductive bias? We discover that a 1426 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1426–1436 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics certain class of LSTM language models that explicitly models syntactic structures, the recurrent neural network grammars (Dyer et al., 2016, RNNGs), considerably outperforms sequential LSTM language models for cases with multiple attractors (§3). We present experiments affirming that this gain is due to an explicit composition operator rather than the presence of predicted syntactic annotations. Rather surprisingly, syntactic LSTM language models without explicit composition have no advantage over sequential LSTMs that operate on word sequences, although these models can nevertheless be excellent predictors of phrase structures (Choe and Charniak, 2016). Having established the importance of modeling structures, we explore the hyp"
P18-1132,J93-2004,0,0.0611748,"rather than vase. In some sense, the composition operator can be understood as injecting a structural recency bias into the model design, as subjects and verbs that are sequentially apart are encouraged to be close together in the RNNGs’ representation. Recurrent Neural Network Grammars Experiments Here we summarize the experimental settings of running RNNGs on the number agreement dataset and discuss the empirical findings. Experimental settings. We obtain phrasestructure trees for the Linzen et al. (2016) dataset using a publicly available discriminative model8 trained on the Penn Treebank (Marcus et al., 1993). At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences.9 At test time, we compare the probabilities of the correct and incorrect verb forms given the prefix, which now includes both nonterminal and terminal symbols. An example of the stack contents (i.e. the prefix) when predicting the verb is provided in Fig. 3(a). We similarly run a grid search over the same hyper-parameter range as the sequential 7 For a complete example of action sequences, we refer the reader to the example provided by Dyer et al. (2016)"
P18-1132,P17-2025,0,0.0870533,"Missing"
P18-1132,N18-1108,0,0.0498081,"ale language model, the primary difference is that we do not map infrequent word types to their POS tags and that we subsample to obtain 500 test instances of each number of attractor due to computation cost; both preprocessing were also done by Linzen et al. (2016). 4 The pretrained large-scale language model is obtained from https://github.com/tensorflow/models/ tree/master/research/lm_1b. 5 This trend is also observed by comparing results with H=150 and H=250. While both models achieve near-identical performance for zero attractor, the model with H=250 persame finding as the recent work of Gulordava et al. (2018), who also find that LSTMs trained with language modeling objectives are able to learn number agreement well; here we additionally identify model capacity as one of the reasons for the discrepancy with the Linzen et al. (2016) results. While the pretrained large-scale language model of Jozefowicz et al. (2016) has certain advantages in terms of model capacity, more training data, and richer vocabulary, we suspect that the poorer performance is due to differences between their training domain and the number agreement testing domain, although the model still performs reasonably well in the numbe"
P18-1132,N03-1014,0,0.0402388,"c LSTM language models without explicit composition have no advantage over sequential LSTMs that operate on word sequences, although these models can nevertheless be excellent predictors of phrase structures (Choe and Charniak, 2016). Having established the importance of modeling structures, we explore the hypothesis that how we build the structure affects the model’s ability to identify structural dependencies in English. As RNNGs build phrase-structure trees through top-down operations, we propose extensions to the structure-building sequences and model architecture that enable left-corner (Henderson, 2003, 2004) and bottom-up (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) generation orders (§4). Extensive prior work has characterized topdown, left-corner, and bottom-up parsing strategies in terms of cognitive plausibility (Pulman, 1986; Abney and Johnson, 1991; Resnik, 1992) and neurophysiological evidence in human sentence processing (Nelson et al., 2017). Here we move away from the realm of parsing and evaluate the three strategies as models of generation instead, and address the following empirical question: which generation order is most appropriately biased to model structural depend"
P18-1132,P04-1013,0,0.258321,"Missing"
P18-1132,D17-1215,0,0.0146658,", more training data, and richer vocabulary, we suspect that the poorer performance is due to differences between their training domain and the number agreement testing domain, although the model still performs reasonably well in the number agreement test set. Prior work has confirmed the notion that, in many cases, statistical models are able to achieve good performance under some aggregate metric by overfitting to patterns that are predictive in most cases, often at the expense of more difficult, infrequent instances that require deeper language understanding abilities (Rimell et al., 2009; Jia and Liang, 2017). In the vast majority of cases, structural dependencies between subjects and verbs highly overlap with sequential dependencies (Table 1). Nevertheless, the fact that number agreement accuracy gets worse as the number of attractors increases is consistent with a sequential recency bias in LSTMs: under this conjecture, identifying the correct structural dependency becomes harder when there are more adjacent nouns of different number forms than the true subject. If the sequential recency conjecture is correct, then LSTMs would perform worse when the structural dependency is more distant in the s"
P18-1132,C92-1032,0,0.803938,"tructures, we explore the hypothesis that how we build the structure affects the model’s ability to identify structural dependencies in English. As RNNGs build phrase-structure trees through top-down operations, we propose extensions to the structure-building sequences and model architecture that enable left-corner (Henderson, 2003, 2004) and bottom-up (Chelba and Jelinek, 2000; Emami and Jelinek, 2005) generation orders (§4). Extensive prior work has characterized topdown, left-corner, and bottom-up parsing strategies in terms of cognitive plausibility (Pulman, 1986; Abney and Johnson, 1991; Resnik, 1992) and neurophysiological evidence in human sentence processing (Nelson et al., 2017). Here we move away from the realm of parsing and evaluate the three strategies as models of generation instead, and address the following empirical question: which generation order is most appropriately biased to model structural dependencies in English, as indicated by number agreement accuracy? Our key finding is that the top-down generation outperforms left-corner and bottom-up variants for difficult cases with multiple attractors. In theory, the three traversal strategies approximate the same chain rule tha"
P18-1132,D09-1085,1,0.694538,"rms of model capacity, more training data, and richer vocabulary, we suspect that the poorer performance is due to differences between their training domain and the number agreement testing domain, although the model still performs reasonably well in the number agreement test set. Prior work has confirmed the notion that, in many cases, statistical models are able to achieve good performance under some aggregate metric by overfitting to patterns that are predictive in most cases, often at the expense of more difficult, infrequent instances that require deeper language understanding abilities (Rimell et al., 2009; Jia and Liang, 2017). In the vast majority of cases, structural dependencies between subjects and verbs highly overlap with sequential dependencies (Table 1). Nevertheless, the fact that number agreement accuracy gets worse as the number of attractors increases is consistent with a sequential recency bias in LSTMs: under this conjecture, identifying the correct structural dependency becomes harder when there are more adjacent nouns of different number forms than the true subject. If the sequential recency conjecture is correct, then LSTMs would perform worse when the structural dependency is"
P18-1132,J10-1001,0,0.0137268,"humans seem to exhibit (Marslen-Wilson, 1973; Tanenhaus et al., 1995); this line of thinking similarly motivates Charniak (2010), among others. While most work in this domain has been concerned with the parsing problem, our findings suggest that anticipatory mechanisms are also beneficial in capturing structural dependencies in language modeling. We note that our results are achieved using models that, in theory, are able to condition on the entire derivation history, while earlier work in sentence processing has focused on cognitive memory considerations, such as the memory-bounded model of Schuler et al. (2010). REDUCE 5 NT SW (S) Conclusion GEN (eats) NT SW (VP) GEN (worms) NT SW (NP) REDUCE REDUCE REDUCE N/A Figure 6: Example Derivation for left-corner traversal. Each NT SW(X) action adds the open nonterminal symbol (X to the stack, followed by a deterministic swap operator that swaps the top two elements on the stack. Discussion. In Table 5, we focus on empirical results for cases where the structural dependencies matter the most, corresponding to cases with two, three, and four attractors. All three RNNG variants outperform the sequential LSTM language model baseline for these cases. Nevertheles"
P18-1132,P17-1099,0,0.01002,"-up variants in capturing long-distance structural dependencies. 1 Figure 1: An example of the number agreement task with two attractors and a subject-verb distance of five. Introduction Recurrent neural networks (RNNs) are remarkably effective models of sequential data. Recent years have witnessed the widespread adoption of recurrent architectures such as LSTMs (Hochreiter and Schmidhuber, 1997) in various NLP tasks, with state of the art results in language modeling (Melis et al., 2018) and conditional generation tasks like machine translation (Bahdanau et al., 2015) and text summarization (See et al., 2017). Here we revisit the question asked by Linzen et al. (2016): as RNNs model word sequences without explicit notions of hierarchical structure, to what extent are these models able to learn non-local syntactic dependencies in natural language? Identifying number agreement between subjects and verbs—especially in the presence of attractors—can be understood as a cognitivelymotivated probe that seeks to distinguish hierarchical theories from sequential ones, as models that rely on sequential cues like the most recent noun would favor the incorrect verb form. We provide an example of this task in"
P18-1132,E17-2060,0,0.0252701,"to resolve dependencies between word tokens. Second, by nature of modeling characters, non-local structural dependencies are sequentially further apart than in the wordbased language model. On the other hand, character LSTMs have the ability to exploit and share informative morphological cues, such as the fact that plural nouns in English tend to end with ‘s’. As demonstrated on the last row of Table 2, we find that the character LSTM language model performs much worse at number agreement with multiple attractors compared to its word-based counterparts. This finding is consistent with that of Sennrich (2017), who find that character-level decoders in neural machine translation perform worse than subword models in capturing morphosyntactic agreement. To some extent, our finding demonstrates the limitations that character LSTMs face in learning structure from language modeling objectives, despite earlier evidence that character LSTM language models are able to implicitly acquire a lexicon (Le Godais et al., 2017). 3.1 RNNGs (Dyer et al., 2016) are language models that estimate the joint probability of string terminals and phrase-structure tree nonterminals. Here we use stack-only RNNGs that achieve"
P18-1132,C00-2137,0,0.311374,"cal results for cases where the structural dependencies matter the most, corresponding to cases with two, three, and four attractors. All three RNNG variants outperform the sequential LSTM language model baseline for these cases. Nevertheless, the top-down variant outperforms both left-corner and bottom-up strategies for difficult cases with three or more attractors, suggesting that the top-down strategy is most appropriately biased to model difficult number agreement dependencies in English. We run an approximate randomization test by stratifying the output and permuting within each stratum (Yeh, 2000) and find that, for four attractors, the performance difference between the top-down RNNG and the other variants is statistically significant at p &lt; 0.05. The success of the top-down traversal in the domain of number-agreement prediction is consistent with a classical view in parsing that argues top-down parsing is the most human-like parsing strategy since it is the most anticipatory. Only Given enough capacity, LSTMs trained on language modeling objectives are able to learn syntax-sensitive dependencies, as evidenced by accurate number agreement accuracy with multiple attractors. Despite thi"
P19-1337,P17-2021,0,0.0203852,"elinek, 2000; Roark, 2001; Henderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the primary difference that here the student DSALSTM has no direct access to syntactic annotations; it does, however, have access to the teacher RNNG’s softmax distribution over the next word. Our approach is also closely related to recent work that introduces structurally-motivated inductive biases into language models. Chung et al. (2017) segmented the hidden state update of an RNN through a multi-scale hierarchical recurrence, thereby providing a shortcut to the gra"
P19-1337,J99-2004,0,0.176336,"0.77 (±0.02) 0.99 (±0.01) 0.93 (±0.02) 0.96 (±0.02) 0.94 (±0.03) 0.95 (±0.01) 0.95 (±0.03) 0.95 (±0.03) 0.93 (±0.02) 0.96 (±0.01) 0.96 (±0.02) 0.95 (±0.01) 0.96 0.93 0.94 0.82 0.85 0.88 0.85 0.82 0.78 0.79 0.86 0.93 (±0.01) 0.77 (±0.03) 0.63 (±0.02) 0.78 (±0.01) 0.83 (±0.02) 0.46 (±0.05) 0.82 (±0.02) 0.70 (±0.02) 0.96 0.91 0.87 0.91 0.93 (±0.06) 0.82 (±0.09) 0.88 (±0.05) 0.79 (±0.02) 0.28 (±0.05) 0.78 (±0.06) 0.53 (±0.04) 0.85 (±0.02) 0.98 0.81 0.90 0.88 Table 1: Replication of Marvin and Linzen (2018) results. M&L-Multi is the Marvin and Linzen (2018) LSTM trained on LM and CCG supertagging (Bangalore and Joshi, 1999; Clark and Curran, 2007) losses with an interpolation factor of 0.5. We report our LSTM LM, small LSTM† , and RNNG† performance († smaller training data; §3) in the format of mean (±standard deviation) of 10 identical models from different seeds. Results in bold denote the best among models trained on similar amounts of training data. 3.1 Recurrent Neural Network Grammars An RNNG defines the joint probability of surface string x and phrase-structure tree y, denoted as t(x, y). The model generates phrase-structure trees in a top-down, left-to-right manner through a series of action sequences i"
P19-1337,P17-1080,0,0.02996,"times as much data as the DSA-LSTM, this finding suggests that, at least in terms of syntactic competence, structural biases continue to be relevant even as the current generation of sequential LMs is able to exploit increasingly large amounts of data. 4.3 Probing for Hierarchical Information Having established the advantages of the DSALSTM on targeted syntactic evaluations, we turn to the question of analysing how its internal representation differs from that of a standard LSTM LM. To this end, we adopt the method of Blevins et al. (2018) and use a probe (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hewitt and Manning, 2019, inter alia) that 11 Goldberg (2019) applies an additional pre-processing step, removing sentences in which the focus verb does not appear as a single word in the word piece-based vocabulary; hence, the evaluation sentences are slightly different. predicts the grandparent constituent of a word token xt , based on its encoding ht under the pretrained LSTM. Under this framework, the accuracy of the probe on a held-out set can be understood as an indication of how well the hidden states encode the relevant syntactic information required to succeed"
P19-1337,P18-2003,0,0.0912624,"Association for Computational Linguistics faster to train. On targeted syntactic evaluations, it achieves better accuracy than: (i) a strong LSTM LM which, through careful hyperparameter tuning, performs much better than previously thought (§2); (ii) the teacher RNNG that exploits a hierarchical inductive bias but lacks scalability (§3); and (iii) a born-again network (Furlanello et al., 2018) that similarly learns from KD, albeit without a hierarchical bias from the teacher. We analyse the DSA-LSTM’s internal representation through the syntactic probe (Shi et al., 2016; Adi et al., 2017) of Blevins et al. (2018), and find that the learned representations encode hierarchical information to a large extent, despite the DSA-LSTM lacking direct access to syntactic annotation. While not directly comparable, on subject-verb agreement both the teacher RNNG and student DSA-LSTM outperform BERT (Devlin et al., 2019; Goldberg, 2019), which benefits from bidirectional information and is trained on 30 times as much data. Altogether, these findings suggest that structural biases continue to play an important role, even at massive data scales, in improving the linguistic competence of LMs. 2 Replication of Targeted"
P19-1337,W15-2108,1,0.890398,"Missing"
P19-1337,N16-1024,1,0.952594,"STMs (Hochreiter and Schmidhuber, 1997) have numerous practical applications, but it has also been shown that they do not always develop accurate syntactic generalisations (Marvin and Linzen, 2018). Thus, one strategy for improving LSTMs is to change their biases to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried et al., 2017), and correlate well with encephalography signals (Hale et al., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten"
P19-1337,K17-1003,0,0.0736648,"Missing"
P19-1337,P16-1078,0,0.0251677,"some form of syntactic structure (Jurafsky et al., 1995; Chelba and Jelinek, 2000; Roark, 2001; Henderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the primary difference that here the student DSALSTM has no direct access to syntactic annotations; it does, however, have access to the teacher RNNG’s softmax distribution over the next word. Our approach is also closely related to recent work that introduces structurally-motivated inductive biases into language models. Chung et al. (2017) segmented the hidden state update of an RNN through a mu"
P19-1337,P17-2012,0,0.0147635,"enderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the primary difference that here the student DSALSTM has no direct access to syntactic annotations; it does, however, have access to the teacher RNNG’s softmax distribution over the next word. Our approach is also closely related to recent work that introduces structurally-motivated inductive biases into language models. Chung et al. (2017) segmented the hidden state update of an RNN through a multi-scale hierarchical recurrence, thereby providing a shortcut to the gradient propagation of lon"
P19-1337,P17-2025,0,0.0199916,"2018). Thus, one strategy for improving LSTMs is to change their biases to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried et al., 2017), and correlate well with encephalography signals (Hale et al., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures"
P19-1337,N18-1108,0,0.304663,"ecoding (Kuncoro et al., 2018). 1 By modelling each sentence separately, our setup is consistent with that of Marvin and Linzen (2018) but differs from those with cross-sentential context (Mikolov et al., 2010). 2 While BERT (Devlin et al., 2019) achieves even better number agreement performance (Goldberg, 2019), the results are not directly comparable since BERT operates nonincrementally and was trained on 500 times as much data. The current state of the art among models trained on the Linzen et al. (2016) training set is the adaptive universal transformer model (Dehghani et al., 2019). 3473 Gulordava et al. (2018) test perplexity Simple In a sentential complement Short VP coordination Long VP coordination Across a prepositional phrase Across a subject relative clause Across an object relative clause Across an object relative clause (no that) In an object relative clause In an object relative clause (no that) Average of subject-verb agreement Simple In a sentential complement Across a relative clause Average of reflexive anaphora Simple Across a relative clause Average of negative polarity items Average of all constructions Marvin & Linzen models Ours M&L-LSTM M&L-Multi Our LSTM 78.65 61.10 53.73 (±0.16"
P19-1337,D16-1257,0,0.022929,"orical distribution defined by an affine transformation and a softmax: at ∼ softmax(Wa ht + ba ). • If at ∈ {GEN, NT}, the model samples a terminal x or a non-terminal n from each respective categorical distribution as the next input: x ∼ softmax(Wx ht + bx ), n ∼ softmax(Wn ht + bn ). • If at = REDUCE, the topmost stack elements going back to the last incomplete non-terminal are popped, and a composition function (here a bidirectional LSTM) is executed to represent the completed phrase on the stack. This recursive composition function constitutes a primary difference with the syntactic LM of Choe and Charniak (2016) that operates sequentially, and has been found to be crucial for achieving good number agreement (Kuncoro et al., 2018) and correlation with brain signals (Hale et al., 2018). The stack LSTM, composition function, lookup embeddings, and pairs of affine transformation weights and biases {W, b} are model parameters. 3.2 Experiments Here we outline the experimental settings and present our RNNG findings. Experimental settings. We implement the RNNG with DyNet and enable autobatching on GPU. Predicted phrase-structure trees for the training and validation sets of the Gulordava et al. (2018) Wikip"
P19-1337,P18-1254,1,0.940646,"es to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried et al., 2017), and correlate well with encephalography signals (Hale et al., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures that are used to build language models from massive corpora (Peters e"
P19-1337,P04-1013,0,0.195863,"Missing"
P19-1337,J07-4004,1,0.60422,".93 (±0.02) 0.96 (±0.02) 0.94 (±0.03) 0.95 (±0.01) 0.95 (±0.03) 0.95 (±0.03) 0.93 (±0.02) 0.96 (±0.01) 0.96 (±0.02) 0.95 (±0.01) 0.96 0.93 0.94 0.82 0.85 0.88 0.85 0.82 0.78 0.79 0.86 0.93 (±0.01) 0.77 (±0.03) 0.63 (±0.02) 0.78 (±0.01) 0.83 (±0.02) 0.46 (±0.05) 0.82 (±0.02) 0.70 (±0.02) 0.96 0.91 0.87 0.91 0.93 (±0.06) 0.82 (±0.09) 0.88 (±0.05) 0.79 (±0.02) 0.28 (±0.05) 0.78 (±0.06) 0.53 (±0.04) 0.85 (±0.02) 0.98 0.81 0.90 0.88 Table 1: Replication of Marvin and Linzen (2018) results. M&L-Multi is the Marvin and Linzen (2018) LSTM trained on LM and CCG supertagging (Bangalore and Joshi, 1999; Clark and Curran, 2007) losses with an interpolation factor of 0.5. We report our LSTM LM, small LSTM† , and RNNG† performance († smaller training data; §3) in the format of mean (±standard deviation) of 10 identical models from different seeds. Results in bold denote the best among models trained on similar amounts of training data. 3.1 Recurrent Neural Network Grammars An RNNG defines the joint probability of surface string x and phrase-structure tree y, denoted as t(x, y). The model generates phrase-structure trees in a top-down, left-to-right manner through a series of action sequences in a process reminiscent o"
P19-1337,N19-1419,0,0.0365748,"ding suggests that, at least in terms of syntactic competence, structural biases continue to be relevant even as the current generation of sequential LMs is able to exploit increasingly large amounts of data. 4.3 Probing for Hierarchical Information Having established the advantages of the DSALSTM on targeted syntactic evaluations, we turn to the question of analysing how its internal representation differs from that of a standard LSTM LM. To this end, we adopt the method of Blevins et al. (2018) and use a probe (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hewitt and Manning, 2019, inter alia) that 11 Goldberg (2019) applies an additional pre-processing step, removing sentences in which the focus verb does not appear as a single word in the word piece-based vocabulary; hence, the evaluation sentences are slightly different. predicts the grandparent constituent of a word token xt , based on its encoding ht under the pretrained LSTM. Under this framework, the accuracy of the probe on a held-out set can be understood as an indication of how well the hidden states encode the relevant syntactic information required to succeed in this task. We use a linear classifier for the"
P19-1337,P18-1198,0,0.0312272,"the DSA-LSTM, this finding suggests that, at least in terms of syntactic competence, structural biases continue to be relevant even as the current generation of sequential LMs is able to exploit increasingly large amounts of data. 4.3 Probing for Hierarchical Information Having established the advantages of the DSALSTM on targeted syntactic evaluations, we turn to the question of analysing how its internal representation differs from that of a standard LSTM LM. To this end, we adopt the method of Blevins et al. (2018) and use a probe (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017; Conneau et al., 2018; Hewitt and Manning, 2019, inter alia) that 11 Goldberg (2019) applies an additional pre-processing step, removing sentences in which the focus verb does not appear as a single word in the word piece-based vocabulary; hence, the evaluation sentences are slightly different. predicts the grandparent constituent of a word token xt , based on its encoding ht under the pretrained LSTM. Under this framework, the accuracy of the probe on a held-out set can be understood as an indication of how well the hidden states encode the relevant syntactic information required to succeed in this task. We use a"
P19-1337,P18-1031,0,0.024229,"ly, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures that are used to build language models from massive corpora (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019). As RNNGs are hard to scale, we instead use the predictions of an RNNG teacher model trained on a small training set, to guide the learning of syntactic structure in a sequential LSTM student model, which is trained on the training set in its entirety. We denote the resulting lanugage model (i.e., the student LSTM) as a distilled syntaxaware LSTM LM (DSA-LSTM). Intuitively, the RNNG teacher is an expert on syntactic generalisation, although it lacks the opportunity to learn the relevant semantic and common-sense knowledge from a large training corpu"
P19-1337,N19-1423,0,0.170169,"at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures that are used to build language models from massive corpora (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019). As RNNGs are hard to scale, we instead use the predictions of an RNNG teacher model trained on a small training set, to guide the learning of syntactic structure in a sequential LSTM student model, which is trained on the training set in its entirety. We denote the resulting lanugage model (i.e., the student LSTM) as a distilled syntaxaware LSTM LM (DSA-LSTM). Intuitively, the RNNG teacher is an expert on syntactic generalisation, although it lacks the opportunity to learn the relevant semantic and common-sense knowledge from a large training corpus. By learning from b"
P19-1337,P15-1033,1,0.812139,"aining data; §3) in the format of mean (±standard deviation) of 10 identical models from different seeds. Results in bold denote the best among models trained on similar amounts of training data. 3.1 Recurrent Neural Network Grammars An RNNG defines the joint probability of surface string x and phrase-structure tree y, denoted as t(x, y). The model generates phrase-structure trees in a top-down, left-to-right manner through a series of action sequences in a process reminiscent of shift-reduce parsing. At any given state, the decision over which action to take is parameterised by a stack LSTM (Dyer et al., 2015) encoding partially-completed constituents. Let ht be the stack LSTM hidden state at time t. The next action at ∈ {GEN, NT, REDUCE} is sampled according to a categorical distribution defined by an affine transformation and a softmax: at ∼ softmax(Wa ht + ba ). • If at ∈ {GEN, NT}, the model samples a terminal x or a non-terminal n from each respective categorical distribution as the next input: x ∼ softmax(Wx ht + bx ), n ∼ softmax(Wn ht + bn ). • If at = REDUCE, the topmost stack elements going back to the last incomplete non-terminal are popped, and a composition function (here a bidirection"
P19-1337,D16-1139,0,0.28832,"interthat syntactic regularities are inferred. In contrast, polate the distillation (left) and LM (right) losses: the interpolated target assigns a minimum probability of 0.5 to the correct label, but crucially con5 This procedure of training a student LSTM LM on string tains additional information about the plausibility samples from the RNNG with K ≈ 3, 000, 000 yields a high of every alternative based on the teacher RNNG’s validation perplexity of above 1,000, due to the enormity of the sample space and the use of discrete samples. predictions. Under this objective, the plural verbs 6 While Kim and Rush (2016) proposed a technique for sequence-level KD for machine translation through beam search, the same technique is not directly applicable to LM, which is an unconditional language generation problem. 7 Recall that `KD (x; θ) does not depend on the true next word x∗j . 8 We use the same pre-trained Berkeley parser to obtain training and validation trees in §3. 9 ˆ berk The resulting syntactic prefix y &lt;j (x) for approximating ∗ t(w |x &lt;j ) under the RNNG is obtained from a Berkeley parser that has access to yet unseen words x&gt;j . 3476 Figure 1: Example of the KD target (top), the standard LM targe"
P19-1337,N19-1114,1,0.836935,"esting that the means by which the DSA-LSTM achieves better syntactic competence is by tracking more hierarchical information during sequential processing. 5 Related Work Augmenting language models with syntactic information and structural inductive bias has been a long-standing area of research. To this end, syntactic language models estimate the joint probability of surface strings and some form of syntactic structure (Jurafsky et al., 1995; Chelba and Jelinek, 2000; Roark, 2001; Henderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the"
P19-1337,E17-1117,1,0.884318,"Missing"
P19-1337,P18-1132,1,0.922299,"at they do not always develop accurate syntactic generalisations (Marvin and Linzen, 2018). Thus, one strategy for improving LSTMs is to change their biases to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried et al., 2017), and correlate well with encephalography signals (Hale et al., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such,"
P19-1337,P18-1129,0,0.0160896,"eralisations (and we have shown that it largely does in §3), every training instance provides the student LSTM with a wealth of information about all the possible legitimate continuations according to the predictions of the hierarchical teacher, thereby making it easier for the student to learn the appropriate hierarchical constraints and generalisations. Differences with other KD work. Our approach departs from the predominant view of distillation primarily as a means of compressing knowledge from a bigger teacher or an ensemble to a compact student (Ba and Caruana, 2014; Kim and Rush, 2016; Liu et al., 2018, inter alia) in two important ways. First, here the teacher and student models are different in character, and not just in size: we transfer knowledge from a teacher that models the joint probability of strings and phrasestructure trees through hierarchical operations, to a student that only models surface strings through sequential operations. This setup presents an interesting dynamic since the DSA-LSTM has to mimic the predictions of the RNNG, which conditions on syntactic annotation to guide hierarchical operations, even though the DSA-LSTM itself has no direct access to any syntactic ann"
P19-1337,D18-1151,0,0.402372,"training data it learns from. On targeted syntactic evaluations, we find that, while sequential LSTMs perform much better than previously reported, our proposed technique substantially improves on this baseline, yielding a new state of the art. Our findings and analysis affirm the importance of structural biases, even in models that learn from large amounts of data. 1 Introduction Language models (LMs) based on sequential LSTMs (Hochreiter and Schmidhuber, 1997) have numerous practical applications, but it has also been shown that they do not always develop accurate syntactic generalisations (Marvin and Linzen, 2018). Thus, one strategy for improving LSTMs is to change their biases to facilitate more linguistically valid generalisations. This paper introduces a scalable method for introducing syntactic biases to LSTMs (and indeed, to any left-to-right language model trained with a cross-entropy objective) by distilling knowledge (Bucilˇa et al., 2006; Hinton et al., 2015) from recurrent neural network grammars (Dyer et al., 2016, RNNGs). RNNGs have been shown to successfully capture non-local syntactic dependencies (Kuncoro et al., 2018), achieve excellent parsing performance (Kuncoro et al., 2017; Fried"
P19-1337,P15-2084,0,0.0621342,"Missing"
P19-1337,W17-4707,0,0.0161857,"structure (Jurafsky et al., 1995; Chelba and Jelinek, 2000; Roark, 2001; Henderson, 2004; Emami and Jelinek, 2005; Buys and Blunsom, 2015; Mirowski and Vlachos, 2015; Dyer et al., 2016; Kim et al., 2019). In contrast to these approaches, the DSA-LSTM only models the probability of surface strings, albeit with an auxiliary loss that distills the next-word predictive distribution of a syntactic language model. Earlier work has also explored multi-task learning with syntactic objectives as an auxiliary loss in language modelling and machine translation (Luong et al., 2016; Eriguchi et al., 2016; Nadejde et al., 2017; Enguehard et al., 2017; Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Our approach of injecting syntactic bias through a KD objective is orthogonal to this approach, with the primary difference that here the student DSALSTM has no direct access to syntactic annotations; it does, however, have access to the teacher RNNG’s softmax distribution over the next word. Our approach is also closely related to recent work that introduces structurally-motivated inductive biases into language models. Chung et al. (2017) segmented the hidden state update of an RNN through a multi-scale hierarchical"
P19-1337,N18-1202,0,0.0669526,"., 2018). Unfortunately, these benefits come at the expense of scalability, since the hierarchical constituent composition process (§3) within RNNGs means that the structure of the computation graph for a sentence varies according to its tree structure. Even with the help of automatic dynamic batching (Neubig et al., 2017a,b), RNNGs can be ten times slower to train than a comparable LSTM as they benefit less from specialised hardware like GPUs. As such, RNNGs are an impractical alternative to computationally convenient architectures that are used to build language models from massive corpora (Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019; Radford et al., 2019). As RNNGs are hard to scale, we instead use the predictions of an RNNG teacher model trained on a small training set, to guide the learning of syntactic structure in a sequential LSTM student model, which is trained on the training set in its entirety. We denote the resulting lanugage model (i.e., the student LSTM) as a distilled syntaxaware LSTM LM (DSA-LSTM). Intuitively, the RNNG teacher is an expert on syntactic generalisation, although it lacks the opportunity to learn the relevant semantic and common-sense knowledge fro"
P19-1337,N07-1051,0,0.207262,"Missing"
P19-1337,J01-2004,0,0.073207,"Missing"
P19-1337,D16-1159,0,0.185934,", Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics faster to train. On targeted syntactic evaluations, it achieves better accuracy than: (i) a strong LSTM LM which, through careful hyperparameter tuning, performs much better than previously thought (§2); (ii) the teacher RNNG that exploits a hierarchical inductive bias but lacks scalability (§3); and (iii) a born-again network (Furlanello et al., 2018) that similarly learns from KD, albeit without a hierarchical bias from the teacher. We analyse the DSA-LSTM’s internal representation through the syntactic probe (Shi et al., 2016; Adi et al., 2017) of Blevins et al. (2018), and find that the learned representations encode hierarchical information to a large extent, despite the DSA-LSTM lacking direct access to syntactic annotation. While not directly comparable, on subject-verb agreement both the teacher RNNG and student DSA-LSTM outperform BERT (Devlin et al., 2019; Goldberg, 2019), which benefits from bidirectional information and is trained on 30 times as much data. Altogether, these findings suggest that structural biases continue to play an important role, even at massive data scales, in improving the linguistic"
P19-1337,D17-1178,0,0.0710463,"Missing"
P19-1337,D18-1503,0,0.0447877,"Missing"
P19-1645,P09-1012,0,0.222391,"ber of efforts to annotate word boundaries. We evaluate against two corpora that have been manually segmented according different segmentation standards. unknown languages or domains. Thus, our experiments are designed to assess how well the models infers word segmentations of unsegmented inputs when they are trained and tuned to maximize the likelihood of the held-out text. DP/HDP Benchmarks Among the most effective existing word segmentation models are those based on hierarchical Dirichlet process (HDP) models (Goldwater et al., 2009; Teh et al., 2006) and hierarchical Pitman–Yor processes (Mochihashi et al., 2009). As a representative of these, we use a simple bigram HDP model: θ· ∼ DP(α0 , p0 ) Beijing University Corpus (PKU) The Beijing University Corpus was one of the corpora used for the International Chinese Word Segmentation Bakeoff (Emerson, 2005). Chinese Penn Treebank (CTB) We use the Penn Chinese Treebank Version 5.1 (Xue et al., 2005). It generally has a coarser segmentation than PKU (e.g., in CTB a full name, consisting of a given name and family name, is a single token), and it is a larger corpus. 5.3 Image Caption Dataset To assess whether jointly learning about meanings of words from non"
P19-1645,N09-1036,0,0.108255,"menting text (although we will see below that its performance is much less good on other datasets). Second, despite careful tuning, the HMLSTM of Chung et al. (2017) fails to discover good segments, although in their paper they show that when spaces are present between, HMLSTMs learn to switch between their internal models in response to them. Furthermore, the priors used in the DP/HDP models were tuned to maximize the likelihood assigned to the validation set by the inferred posterior predictive distribution, in contrast to previous papers which either set them subjectively or inferred them (Johnson and Goldwater, 2009). For example, the DP and HDP model with subjective priors obtained 53.8 and 72.3 F1 scores, respectively (Goldwater et al., 2009). However, when the hyperparameters are set to maximize held-out likelihood, this drops obtained 56.1 and 56.9. Another result on this dataset is the feature unigram model of Berg-Kirkpatrick et al. (2010), which obtains an 88.0 F1 score with hand-crafted features and by selecting the regularization strength to optimize segmentation performance. Once the features are removed, the model achieved a 71.5 F1 score when it is tuned on segmentation performance and only 11"
P19-1645,P17-1137,1,0.544214,"the induced segmentations, in both unconditional (sequence-only) contexts and when conditioning on a related image. First, we look at the segmentations induced by our model. We find that these correspond closely to human intuitions about word segments, competitive with the best existing models for unsupervised word discovery. Importantly, these segments are obtained in models whose hyperparameters are tuned to optimize validation (held-out) likelihood, whereas tuning the hyperparameters of our benchmark models using held-out likelihood produces poor segmentations. Second, we confirm findings (Kawakami et al., 2017; Mielke and Eisner, 2018) that show that word segmentation information leads to better language models compared to pure character models. However, in contrast to previous work, we realize this performance improvement without having to observe the segment boundaries. Thus, our model may be applied straightforwardly to Chinese, where word boundaries are not part of the orthography. 1 Since the lexical memory stores strings that appear in the training data, each sentence could, in principle, be generated as a single lexical unit, thus the model could fit the training data perfectly while general"
P19-1645,W01-0714,0,0.0168171,"l memory or length regularization; it obtains 80.2 F1 on the PKU dataset; however, it uses gold segmentation data during training and hyperparameter selection,4 whereas our approach requires no gold standard segmentation data. 8 Related Work Learning to discover and represent temporally extended structures in a sequence is a fundamental problem in many fields. For example in language processing, unsupervised learning of multiple levels of linguistic structures such as morphemes (Snyder and Barzilay, 2008), words (Goldwater et al., 2009; Mochihashi et al., 2009; Wang et al., 2014) and phrases (Klein and Manning, 2001) have been investigated. Recently, speech recognition has benefited from techniques that enable the discovery of subword units (Chan et al., 2017; Wang et al., 2017); however, in that work, the optimally discovered character sequences look quite unlike orthographic words. In fact, the model proposed by Wang et al. (2017) is essentially our model without a lexicon or the expected length regularization, i.e., (−memory, −length), which we have shown performs quite poorly in terms of segmentation accuracy. Finally, some prior work has also sought to discover lexical units directly from speech base"
P19-1645,D09-1005,0,0.0419014,"rk (Liang and Klein, 2009; Berg-Kirkpatrick et al., 2010). This expectation is a differentiable function of the model parameters. Because of the linearity of the penalty across segments, it can be computed efficiently using the above dynamic programming algorithm under the expectation semiring (Eisner, 2002). This is particularly efficient since the expectation semiring jointly computes the expectation and marginal likelihood in a single forward pass. For more details about computing gradients of expectations under distributions over structured objects with dynamic programs and semirings, see Li and Eisner (2009). 4.1 Training Objective The model parameters are trained by minimizing the penalized log likelihood of a training corpus D of unsegmented sentences, X L= [− log p(x) + λR(x, β)]. x∈D 5 Datasets We evaluate our model on both English and Chinese segmentation. For both languages, we used standard datasets for word segmentation and language modeling. We also use MS-COCO to evaluate how the model can leverage conditioning context information. For all datasets, we used train, validation and test splits.2 Since our model assumes a closed character set, we removed validation and test samples which co"
P19-1645,P08-1084,0,0.0514809,"r language models than neural models. The neural model of Sun and Deng (2018) is similar to our model without lexical memory or length regularization; it obtains 80.2 F1 on the PKU dataset; however, it uses gold segmentation data during training and hyperparameter selection,4 whereas our approach requires no gold standard segmentation data. 8 Related Work Learning to discover and represent temporally extended structures in a sequence is a fundamental problem in many fields. For example in language processing, unsupervised learning of multiple levels of linguistic structures such as morphemes (Snyder and Barzilay, 2008), words (Goldwater et al., 2009; Mochihashi et al., 2009; Wang et al., 2014) and phrases (Klein and Manning, 2001) have been investigated. Recently, speech recognition has benefited from techniques that enable the discovery of subword units (Chan et al., 2017; Wang et al., 2017); however, in that work, the optimally discovered character sequences look quite unlike orthographic words. In fact, the model proposed by Wang et al. (2017) is essentially our model without a lexicon or the expected length regularization, i.e., (−memory, −length), which we have shown performs quite poorly in terms of s"
P19-1645,D18-1531,0,0.130379,"antially non-local dependencies). Additionally, the features and grammars used in prior work reflect certain English-specific design considerations (e.g., syllable structure in the case of adaptor grammars and phonotactic equivalence classes in the feature unigram model), which make them questionable models if the goal is to ex6436 plore what models and biases enable word discovery in general. For Chinese, the best nonparametric models perform better at segmentation (Zhao and Kit, 2008; Mochihashi et al., 2009), but again they are weaker language models than neural models. The neural model of Sun and Deng (2018) is similar to our model without lexical memory or length regularization; it obtains 80.2 F1 on the PKU dataset; however, it uses gold segmentation data during training and hyperparameter selection,4 whereas our approach requires no gold standard segmentation data. 8 Related Work Learning to discover and represent temporally extended structures in a sequence is a fundamental problem in many fields. For example in language processing, unsupervised learning of multiple levels of linguistic structures such as morphemes (Snyder and Barzilay, 2008), words (Goldwater et al., 2009; Mochihashi et al.,"
P19-1645,N09-1069,0,0.0408723,"ed by replacing the summation with a max operator in Eq. 3 and maintaining backpointers. 4 Expected length regularization When the lexical memory contains all the substrings in the training data, the model easily overfits by copying the longest continuation from the memory. To prevent overfitting, we introduce a regularizer that penalizes based on the expectation of the exponentiated (by a hyperparameter β) length of each segment: X X |s|β . R(x, β) = p(s |x) s:π(s)=x s∈s This can be understood as a regularizer based on the double exponential prior identified to be effective in previous work (Liang and Klein, 2009; Berg-Kirkpatrick et al., 2010). This expectation is a differentiable function of the model parameters. Because of the linearity of the penalty across segments, it can be computed efficiently using the above dynamic programming algorithm under the expectation semiring (Eisner, 2002). This is particularly efficient since the expectation semiring jointly computes the expectation and marginal likelihood in a single forward pass. For more details about computing gradients of expectations under distributions over structured objects with dynamic programs and semirings, see Li and Eisner (2009). 4.1"
P19-1645,J01-3002,0,0.142534,"esults using (1) the surprisal criterion of Elman (1990) and (2) a two-level hierarchical multiscale LSTM (Chung et al., 2017), which has been shown to predict boundaries in 6433 whitespace-containing character sequences at positions corresponding to word boundaries. As with all experiments in this paper, the BR-corpora for this experiment do not contain spaces. SNLM Model configurations and Evaluation LSTMs had 512 hidden units with parameters learned using the Adam update rule (Kingma and Ba, 2015). We evaluated our models with bits-percharacter (bpc) and segmentation accuracy (Brent, 1999; Venkataraman, 2001; Goldwater et al., 2009). Refer to Appendices C–F for details of model configurations and evaluation metrics. For the image caption dataset, we extend the model with a standard attention mechanism in the backbone LSTM (LSTMenc ) to incorporate image context. For every character-input, the model calculates attentions over image features and use them to predict the next characters. As for image representations, we use features from the last convolution layer of a pre-trained VGG19 model (Simonyan and Zisserman, 2014). 7 Results In this section, we first do a careful comparison of segmentation p"
P19-1645,P14-2122,0,0.013653,"ilar to our model without lexical memory or length regularization; it obtains 80.2 F1 on the PKU dataset; however, it uses gold segmentation data during training and hyperparameter selection,4 whereas our approach requires no gold standard segmentation data. 8 Related Work Learning to discover and represent temporally extended structures in a sequence is a fundamental problem in many fields. For example in language processing, unsupervised learning of multiple levels of linguistic structures such as morphemes (Snyder and Barzilay, 2008), words (Goldwater et al., 2009; Mochihashi et al., 2009; Wang et al., 2014) and phrases (Klein and Manning, 2001) have been investigated. Recently, speech recognition has benefited from techniques that enable the discovery of subword units (Chan et al., 2017; Wang et al., 2017); however, in that work, the optimally discovered character sequences look quite unlike orthographic words. In fact, the model proposed by Wang et al. (2017) is essentially our model without a lexicon or the expected length regularization, i.e., (−memory, −length), which we have shown performs quite poorly in terms of segmentation accuracy. Finally, some prior work has also sought to discover l"
P19-1645,I08-1002,0,0.0496608,"igram word model; the adaptor grammar model is effectively phrasal unigram model; both are incapable of generalizing about substantially non-local dependencies). Additionally, the features and grammars used in prior work reflect certain English-specific design considerations (e.g., syllable structure in the case of adaptor grammars and phonotactic equivalence classes in the feature unigram model), which make them questionable models if the goal is to ex6436 plore what models and biases enable word discovery in general. For Chinese, the best nonparametric models perform better at segmentation (Zhao and Kit, 2008; Mochihashi et al., 2009), but again they are weaker language models than neural models. The neural model of Sun and Deng (2018) is similar to our model without lexical memory or length regularization; it obtains 80.2 F1 on the PKU dataset; however, it uses gold segmentation data during training and hyperparameter selection,4 whereas our approach requires no gold standard segmentation data. 8 Related Work Learning to discover and represent temporally extended structures in a sequence is a fundamental problem in many fields. For example in language processing, unsupervised learning of multiple"
Q18-1023,P14-1035,0,0.0268979,"l require transferring text understanding capability from other supervised learning tasks. 7 Related Work This paper is the first large-scale question answering dataset on full-length books and movie scripts. However, although we are the first to look at the QA task, learning to understand books through other modeling objectives has become an important subproblem in NLP. These include high level plot understanding through clustering of novels (Frermann and Szarvas, 2017) or summarization of movie scripts (Gorinski and Lapata, 2015), to more fine grained processing by inducing character types (Bamman et al., 2014b; Bamman et al., 2014a), understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017), or understanding plans, goals, and narrative structure in terms of abstract narratives (Schank and Abelson, 1977; Wilensky, 1978; Black and Wilensky, 1979; Chambers and Jurafsky, 2009). In computer vision, the MovieQA dataset (Tapaswi et al., 2016) fulfills a similar role as NarrativeQA. It seeks to test the ability of models to comprehend movies via question answering, and part of the dataset includes full length scripts. 8 Conclusion We have introduced a new dataset and a s"
Q18-1023,P09-1068,0,0.0848706,"s through other modeling objectives has become an important subproblem in NLP. These include high level plot understanding through clustering of novels (Frermann and Szarvas, 2017) or summarization of movie scripts (Gorinski and Lapata, 2015), to more fine grained processing by inducing character types (Bamman et al., 2014b; Bamman et al., 2014a), understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017), or understanding plans, goals, and narrative structure in terms of abstract narratives (Schank and Abelson, 1977; Wilensky, 1978; Black and Wilensky, 1979; Chambers and Jurafsky, 2009). In computer vision, the MovieQA dataset (Tapaswi et al., 2016) fulfills a similar role as NarrativeQA. It seeks to test the ability of models to comprehend movies via question answering, and part of the dataset includes full length scripts. 8 Conclusion We have introduced a new dataset and a set of tasks for training and evaluating reading comprehension systems, borne from an analysis of the limitations 326 Title: Jacob’s Ladder Question: What is the fatal injury that Jacob sustains which ultimately leads to his death ? Answer: A bayonete stabbing to his gut. Summary snippet: A terrified Jac"
Q18-1023,P16-1223,0,0.173691,"Missing"
Q18-1023,W11-2107,0,0.0187853,"e the design of architectures capable of modeling such relationships. This setting is similar to the previous tasks in that the questions and answers were constructed based on these supporting documents. The full version of NarrativeQA requires reading and understanding entire stories (i.e., books and movie scripts). At present, this task is intractable for existing neural models out of the box. We further discuss the challenges and possible approaches in the following sections. We require the use of metrics for generated text. We evaluate using BLEU-1, BLEU-4 (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), and ROUGE-L (Lin, 2004), using two references for each question,6 except for the human baseline where we evaluate one reference against the other. We also evaluate our models using a ranking metric. This allows us to evaluate how good our model is at reading comprehension regardless of how good it is at generating answers. We rank answers for questions associated with the same summary/story and compute the mean reciprocal rank (MRR).7 4 Baselines and Oracles In this section, we show that NarrativeQA presents a challenging problem for current approaches to reading comprehension by evaluating"
Q18-1023,D17-1200,0,0.016072,"iptive summaries, requiring models to “read between the lines”. We expect that understanding narratives as complex as those presented in NarrativeQA will require transferring text understanding capability from other supervised learning tasks. 7 Related Work This paper is the first large-scale question answering dataset on full-length books and movie scripts. However, although we are the first to look at the QA task, learning to understand books through other modeling objectives has become an important subproblem in NLP. These include high level plot understanding through clustering of novels (Frermann and Szarvas, 2017) or summarization of movie scripts (Gorinski and Lapata, 2015), to more fine grained processing by inducing character types (Bamman et al., 2014b; Bamman et al., 2014a), understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017), or understanding plans, goals, and narrative structure in terms of abstract narratives (Schank and Abelson, 1977; Wilensky, 1978; Black and Wilensky, 1979; Chambers and Jurafsky, 2009). In computer vision, the MovieQA dataset (Tapaswi et al., 2016) fulfills a similar role as NarrativeQA. It seeks to test the ability of models to compr"
Q18-1023,N15-1113,0,0.0220503,"We expect that understanding narratives as complex as those presented in NarrativeQA will require transferring text understanding capability from other supervised learning tasks. 7 Related Work This paper is the first large-scale question answering dataset on full-length books and movie scripts. However, although we are the first to look at the QA task, learning to understand books through other modeling objectives has become an important subproblem in NLP. These include high level plot understanding through clustering of novels (Frermann and Szarvas, 2017) or summarization of movie scripts (Gorinski and Lapata, 2015), to more fine grained processing by inducing character types (Bamman et al., 2014b; Bamman et al., 2014a), understanding relationships between characters (Iyyer et al., 2016; Chaturvedi et al., 2017), or understanding plans, goals, and narrative structure in terms of abstract narratives (Schank and Abelson, 1977; Wilensky, 1978; Black and Wilensky, 1979; Chambers and Jurafsky, 2009). In computer vision, the MovieQA dataset (Tapaswi et al., 2016) fulfills a similar role as NarrativeQA. It seeks to test the ability of models to comprehend movies via question answering, and part of the dataset i"
Q18-1023,N16-1180,0,0.0574426,"Missing"
Q18-1023,P16-1086,0,0.219579,"issing word) and are produced from either short abstractive summaries (CNN/Daily Mail) or from the next sentence in the document the context was taken from (CBT and BookTest). The tasks associated with these datasets are all selecting an answer from a set of options, which is explicitly provided for CBT and BookTest, and is implicit for CNN/Daily Mail, as the answers are always entities from the document. This significantly favors models that operate by pointing to a particular token (or type). Indeed, the most successful models on these datasets, such as the Attention Sum Reader (AS Reader) (Kadlec et al., 2016), exploit precisely this bias in the data. However, these models are inappropriate for answers requiring synthesis of a new answer. This bias towards answers that are shallowly salient is a more serious limitation of the CNN/Daily Mail dataset, since its context documents are news stories which usually contain a small number of salient entities and focus on a single event. Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2016) offer a different challenge. A large number of questions and answers are provided for a set of documents, where the ans"
Q18-1023,W04-1013,0,0.114151,"modeling such relationships. This setting is similar to the previous tasks in that the questions and answers were constructed based on these supporting documents. The full version of NarrativeQA requires reading and understanding entire stories (i.e., books and movie scripts). At present, this task is intractable for existing neural models out of the box. We further discuss the challenges and possible approaches in the following sections. We require the use of metrics for generated text. We evaluate using BLEU-1, BLEU-4 (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), and ROUGE-L (Lin, 2004), using two references for each question,6 except for the human baseline where we evaluate one reference against the other. We also evaluate our models using a ranking metric. This allows us to evaluate how good our model is at reading comprehension regardless of how good it is at generating answers. We rank answers for questions associated with the same summary/story and compute the mean reciprocal rank (MRR).7 4 Baselines and Oracles In this section, we show that NarrativeQA presents a challenging problem for current approaches to reading comprehension by evaluating several baselines based o"
Q18-1023,P02-1040,0,0.102406,"pe that NarrativeQA will motivate the design of architectures capable of modeling such relationships. This setting is similar to the previous tasks in that the questions and answers were constructed based on these supporting documents. The full version of NarrativeQA requires reading and understanding entire stories (i.e., books and movie scripts). At present, this task is intractable for existing neural models out of the box. We further discuss the challenges and possible approaches in the following sections. We require the use of metrics for generated text. We evaluate using BLEU-1, BLEU-4 (Papineni et al., 2002), Meteor (Denkowski and Lavie, 2011), and ROUGE-L (Lin, 2004), using two references for each question,6 except for the human baseline where we evaluate one reference against the other. We also evaluate our models using a ranking metric. This allows us to evaluate how good our model is at reading comprehension regardless of how good it is at generating answers. We rank answers for questions associated with the same summary/story and compute the mean reciprocal rank (MRR).7 4 Baselines and Oracles In this section, we show that NarrativeQA presents a challenging problem for current approaches to"
Q18-1023,D14-1162,0,0.0896265,"Missing"
Q18-1023,D16-1264,0,0.282564,"that operate by pointing to a particular token (or type). Indeed, the most successful models on these datasets, such as the Attention Sum Reader (AS Reader) (Kadlec et al., 2016), exploit precisely this bias in the data. However, these models are inappropriate for answers requiring synthesis of a new answer. This bias towards answers that are shallowly salient is a more serious limitation of the CNN/Daily Mail dataset, since its context documents are news stories which usually contain a small number of salient entities and focus on a single event. Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016) and NewsQA (Trischler et al., 2016) offer a different challenge. A large number of questions and answers are provided for a set of documents, where the answers are spans of the context document, i.e. contiguous sequences of words from the document. Although the answers are not Dataset Documents Questions Answers MCTest (Richardson et al., 2013) 660 short stories, grade school level 93K+220K news articles 2640 human generated, based on the document 387K+997K Cloze-form, based on highlights Cloze-form, from the 21st sentence Cloze-form, similar to CBT 108K human generated, based on the paragrap"
Q18-1023,D13-1020,0,0.710414,"the author (e.g. “muggle” in Harry Potter novels) but the reader need only appeal to the book itself to understand the meaning of these concepts, and their place in the narrative. This ability to form new concepts based on the contexts of a text is a crucial aspect of reading comprehension, and is in part tested as part of the question answering tasks we present. 2 318 ing comprehension models. We summarize the key features of a collection of popular recent datasets in Table 1. In this section, we briefly discuss the nature and limitations of these datasets and their associated tasks. MCTest (Richardson et al., 2013) is a collection of short stories, each with multiple questions. Each such question has set of possible answers, one of which is labelled as correct. While this could be used as a QA task, the MCTest corpus is in fact intended as an answer selection corpus. The data is human generated, and the answers can be phrases or sentences. The main limitation of this dataset is that it serves more as a an evaluation challenge than as the basis for end-to-end training of models, due to its relatively small size. In contrast, CNN/Daily Mail (Hermann et al., 2015), Children’s Book Test (CBT) (Hill et al.,"
Q18-1023,P17-1018,0,0.0890611,"Missing"
S12-1011,D10-1115,0,0.0143673,"tion for disambiguating word meaning. 1 Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference. We propose a probabilistic model of the semantic representations for nouns and modifiers. The foundation of this model is a latent variable representation of noun and adjective semantics together with their comp"
S12-1011,W03-1022,0,0.212358,"|M| Figure 1: Plate diagram illustrating our model of noun and modifier semantic classes (designated N and M , respectively), a modifier-noun pair (m,n), and its context. N Parameterization and Inference Multi(ΨM Ni ) Multi(ΨnNi ) Multi(Ψm Mi ) Multi(ΨcNi ) 71 As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British"
S12-1011,J02-2003,0,0.426222,"erence Multi(ΨM Ni ) Multi(ΨnNi ) Multi(Ψm Mi ) Multi(ΨcNi ) 71 As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are"
S12-1011,P05-1004,0,0.123956,"valuations we test the degree to which adjective-noun relationships can be categorised. We analyse the effect of lexical context on these relationships, and the efficacy of the latent semantic representation for disambiguating word meaning. 1 Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional prefer"
S12-1011,J10-4007,0,0.0302055,"Missing"
S12-1011,D11-1129,0,0.0350219,"Missing"
S12-1011,N09-1036,0,0.0334129,"re treated as a bag of words and 1 We evaluate this hypothesis as well as its inverse. 70 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 70–74, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics 2.2 αc Ψc αN αM ΨN ΨM |N| We use Gibbs sampling to estimate the distributions of N and M , integrating out the multinomial parameters Ψx (Griffiths and Steyvers, 2004). The Dirichlet parameters α are drawn independently from a Γ(1, 1) distribution, and are resampled using slice sampling at frequent intervals throughout the sampling process (Johnson and Goldwater, 2009). This “vague” prior encourages sparse draws from the Dirichlet distribution. The number of noun and adjective classes N and M was set to 50 each; other sizes (100,150) did not significantly alter results. |N| c N M n m k |D| Ψn Ψm |N| n α 3 m α include the words to the left and right of the noun, its siblings and governing verbs. We designate the vocabulary Vn for nouns, Vm for modifiers and Vc for context. We use zi to refer to the ith tuple in D and refer to variables within that tuple by subscripting them with i, e.g., ni and c3,i are the noun and the third context variable of zi . The lat"
S12-1011,J03-3005,0,0.414645,"d on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are marginalised over every 10th iteration. 4 4.1 Evaluation Supersense Tagging Supersense taggin"
S12-1011,P08-1028,0,0.04565,"latent semantic representation for disambiguating word meaning. 1 Chris Dyer Language Technologies Institute Carnegie Mellon University Pittsburgh, PA, 15213, USA cdyer@cs.cmu.edu Introduction Developing models of the meanings of words and phrases is a key challenge for computational linguistics. Distributed representations are useful in capturing such meaning for individual words (Sato et al., 2008; Maas and Ng, 2010; Curran, 2005). However, finding a compelling account of semantic compositionality that utilises such representations has proven more difficult and is an active research topic (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette and Sadrzadeh, 2011). It is in this area that our paper makes its contribution. The dominant approaches to distributional semantics have relied on relatively simple frequency counting techniques. However, such approaches fail to generalise to the much sparser distributions encountered when modeling compositional processes and provide no account of selectional preference. We propose a probabilistic model of the semantic representations for nouns and modifiers. The foundation of this model is a latent variable representation of noun and adjective seman"
S12-1011,P10-1045,0,0.0241587,"Missing"
S12-1011,P93-1024,0,0.564443,"ulti(ΨnNi ) Multi(Ψm Mi ) Multi(ΨcNi ) 71 As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are marginalised over ever"
S12-1011,P99-1014,0,0.0273177,"i ) Multi(ΨcNi ) 71 As our model was developed on the basis of several hypotheses, we design the experiments and evaluation so that these hypotheses can be examined on their individual merit. We test the first hypothesis, that nouns and adjectives can be represented by semantic classes, recoverable using co-occurence, using a sense clustering evaluation by Ciaramita and Johnson (2003). The second hypothesis, that the distribution with respect to context and to each other is governed by these semantic classes is evaluated using pseudo-disambiguation (Clark and Weir, 2002; Pereira et al., 1993; Rooth et al., 1999) and bigram plausibility (Keller and Lapata, 2003) tests. To test whether noun classes indeed select for adjective classes, we also evaluate an inverse model (M odi ), where the adjective class is drawn first, in turn generating both context and the noun class. In addition, we evaluate copies of both models ignoring context (M odnc and M odinc ). We use the British National Corpus (BNC), training on 90 percent and testing on 10 percent of the corpus. Results are reported after 2,000 iterations including a burn-in period of 200 iterations. Classes are marginalised over every 10th iteration. 4 4"
S12-1021,W06-1201,0,0.023401,"s an idiosyncratic element to its meaning. 2 Definition from http://www.thefreedictionary.com We define lexicality as the degree to which idiosyncrasy contributes to a compound’s semantics. Inversely phrased, the compositionality of a compound can be defined as the degree to which its sense is related to the senses of its constituents.3 This graded representation follows Sp¨arck Jones (1985), who argued that “it is not possible to maintain a principled distinction between lexicalised and non-lexicalised compounds”. Some recent work also supports this view (Reddy et al., 2011; Bu et al., 2010; Baldwin, 2006). From a practical perspective, a real-valued representation of compositionality should help improve interpretation of compounds. This is especially true when factoring in the respective semantic contributions of its parts. 3.2 Context Generation According to the distributional hypothesis, the semantics of a lexical item can be expressed by its context. We apply this hypothesis to the problem of noun compound compositionality by using a generative model on compound context. Our model allows context to be generated by the compound itself or by either one of its constituents. By learning which e"
S12-1021,D10-1115,0,0.098579,"e thus able to proliferate infinitely. At the same time, semantic composition can take many different forms, making uniform interpretation of compounds impossible (Zanzotto et al., 2010). Most current work on MWEs focuses on interpreting compounds and sidesteps the task of determining whether a compound is compositional in the first place (Butnariu et al., 2010; Kim and Baldwin, 2008). Such methods, aimed at learning the semantics of compounds, can roughly be divided into two major strands of research. One group relies on data intensive methods to extract semantics vectors from large corpora (Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Giesbrecht, 2009). The focus of these approaches is to develop methods for composing the vectors of unigrams into a semantic vector representing a compound. Some of the work in this area touches on the issue of lexicality, as models learning distributional representations of MWEs ideally would first establish whether a given MWE is compositional or not (Mitchell and Lapata, 2010). The other group are knowledge intensive approaches collecting linguistic features (Kim and Baldwin, 2005; Korkontzelos and Manandhar, 2009). Tratz and Hovy (2010), for instance, train a class"
S12-1021,D07-1090,0,0.00961934,"ffect of parameter tuning decreases on larger data. 137 Working on problems related to non-unigram data, sparsity is a frequently encountered problem. As already explored in the previous section, this is also the case for our generative models of lexicality. It would be possible to use an even larger training corpus, but there are limitations as to what extent this is possible. The BNC, containing 100 million words, is already one of the largest corpora regularly used in Computational Linguistics. However, adding more data in an unsupervised sense is unlikely to significantly improve results (Brants et al., 2007). Alternatively, it would be possible to add specific training data that included the noun compounds from the evaluation data sets. This would, however, compromise the unsupervised nature of our approach, and it thus not an option either. In this paper, we will instead focus on extenuating the effects of data sparsity through other unsupervised means. For this purpose we investigate interpolating on a larger set of noun compounds. Kim and Baldwin (2007) observed that semantic similarity of verb-particle compounds correlates with their lexicality. We extend this observation for noun compounds,"
S12-1021,C10-1014,0,0.0441993,"unds by extrapolating their semantics from observations where the two nouns forming a compound are in an intransitive relationship. For example extracting the phrase ‘the family owns a car’ from the training data would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a classification option, considering it jointly with the actual compound interpretation. Next to the work on MWE interpretation there has been some work focused on determining lexicality in its own right (Reddy et al., 2011; Bu et al., 2010; Kim and Baldwin, 2007). One possibility is to exploit special properties of 133 lexical MWEs such as high statistical association of their constituents (Pedersen, 2011) or syntactic rigidity (Fazly et al., 2009; McCarthy et al., 2007). However, these approaches are limited in their applicability to compound nouns (Reddy et al., 2011). Another method is to compare the semantics of a compound and its constituents to decide compositionality. The approaches used to determine those semantics can again be divided into knowledge intensive and data-driven methods. Depending on the chosen representat"
S12-1021,S10-1007,0,0.058076,"Missing"
S12-1021,J09-1005,0,0.031615,"ata would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a classification option, considering it jointly with the actual compound interpretation. Next to the work on MWE interpretation there has been some work focused on determining lexicality in its own right (Reddy et al., 2011; Bu et al., 2010; Kim and Baldwin, 2007). One possibility is to exploit special properties of 133 lexical MWEs such as high statistical association of their constituents (Pedersen, 2011) or syntactic rigidity (Fazly et al., 2009; McCarthy et al., 2007). However, these approaches are limited in their applicability to compound nouns (Reddy et al., 2011). Another method is to compare the semantics of a compound and its constituents to decide compositionality. The approaches used to determine those semantics can again be divided into knowledge intensive and data-driven methods. Depending on the chosen representation of semantics these approaches can either be used for supervised classifiers or together with a distance metric comparing vector space representations of semantics. In a binary setting, a threshold would then"
S12-1021,I05-1082,0,0.0346288,"elies on data intensive methods to extract semantics vectors from large corpora (Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Giesbrecht, 2009). The focus of these approaches is to develop methods for composing the vectors of unigrams into a semantic vector representing a compound. Some of the work in this area touches on the issue of lexicality, as models learning distributional representations of MWEs ideally would first establish whether a given MWE is compositional or not (Mitchell and Lapata, 2010). The other group are knowledge intensive approaches collecting linguistic features (Kim and Baldwin, 2005; Korkontzelos and Manandhar, 2009). Tratz and Hovy (2010), for instance, train a classifier for noun compound interpretation on a large set of W ORD N ET and Thesaurus features. Combined approaches include Kim and Baldwin (2008), who interpret noun compounds by extrapolating their semantics from observations where the two nouns forming a compound are in an intransitive relationship. For example extracting the phrase ‘the family owns a car’ from the training data would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised clas"
S12-1021,P09-2017,0,0.551517,"methods to extract semantics vectors from large corpora (Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Giesbrecht, 2009). The focus of these approaches is to develop methods for composing the vectors of unigrams into a semantic vector representing a compound. Some of the work in this area touches on the issue of lexicality, as models learning distributional representations of MWEs ideally would first establish whether a given MWE is compositional or not (Mitchell and Lapata, 2010). The other group are knowledge intensive approaches collecting linguistic features (Kim and Baldwin, 2005; Korkontzelos and Manandhar, 2009). Tratz and Hovy (2010), for instance, train a classifier for noun compound interpretation on a large set of W ORD N ET and Thesaurus features. Combined approaches include Kim and Baldwin (2008), who interpret noun compounds by extrapolating their semantics from observations where the two nouns forming a compound are in an intransitive relationship. For example extracting the phrase ‘the family owns a car’ from the training data would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a cla"
S12-1021,N10-1089,0,0.0473838,"ion of a MWE, it needs to be established whether the expression should be treated as lexical (idiomatic) or as compositional. The final step, learning the semantics of the MWE, strongly depends on this decision. 1 Definition taken from Wikipedia, and clearly not recoverable if one only knows the meaning of the words ‘rat’ and ‘race’. The problem posed by MWEs is considered hard, but at the same time it is highly relevant and interesting. MWEs occur frequently in language and interpreting them correctly would directly improve results in a number of tasks in NLP such as translation and parsing (Korkontzelos and Manandhar, 2010). By extension this makes deciding the lexicality of MWEs an important challenge for various fields including machine translation, question answering and information retrieval. In this paper we discuss compositionality with respect to noun-noun compounds. Most Computational Linguistics literature treats compositionality as a binary problem, classifying compounds as either lexical or compositional. We show that this approach is too simplistic and argue for the real-valued treatment of compositionality. We propose two unsupervised models that learn compositionality rankings for compounds, placin"
S12-1021,P95-1007,0,0.0699465,"ther be used for supervised classifiers or together with a distance metric comparing vector space representations of semantics. In a binary setting, a threshold would then be applied to the result of that distance function (Korkontzelos and Manandhar, 2009). In a real-valued setting the distance metric itself can be used as a measure for compositionality (Reddy et al., 2011). Related to the vector space based models, some research focuses on improving the distance metrics used to compare induced semantics (Bu et al., 2010). 3 Methodology English noun-noun compounds are majority leftbranching (Lauer, 1995), with a head (the second element), modified by an attributive noun (first element). For example: Ground Floor — The floor of a building at or nearest ground level.2 In this paper, we will use the terms attributive noun (AN) and head noun (HN) to refer to the first and second noun in a noun compound. 3.1 Real-Valued Representation Lexicality of MWEs is frequently treated as a bi´ S´eaghdha, nary property (Tratz and Hovy, 2010; O 2007). We argue that lexicality should instead be treated as a graded property, as most compound semantics exhibit a mixture of compositional and lexical influences. F"
S12-1021,D07-1039,0,0.0653848,"that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a classification option, considering it jointly with the actual compound interpretation. Next to the work on MWE interpretation there has been some work focused on determining lexicality in its own right (Reddy et al., 2011; Bu et al., 2010; Kim and Baldwin, 2007). One possibility is to exploit special properties of 133 lexical MWEs such as high statistical association of their constituents (Pedersen, 2011) or syntactic rigidity (Fazly et al., 2009; McCarthy et al., 2007). However, these approaches are limited in their applicability to compound nouns (Reddy et al., 2011). Another method is to compare the semantics of a compound and its constituents to decide compositionality. The approaches used to determine those semantics can again be divided into knowledge intensive and data-driven methods. Depending on the chosen representation of semantics these approaches can either be used for supervised classifiers or together with a distance metric comparing vector space representations of semantics. In a binary setting, a threshold would then be applied to the result"
S12-1021,P07-3013,0,0.0362171,"Missing"
S12-1021,W11-1306,0,0.0282471,"family owns a car’ from the training data would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a classification option, considering it jointly with the actual compound interpretation. Next to the work on MWE interpretation there has been some work focused on determining lexicality in its own right (Reddy et al., 2011; Bu et al., 2010; Kim and Baldwin, 2007). One possibility is to exploit special properties of 133 lexical MWEs such as high statistical association of their constituents (Pedersen, 2011) or syntactic rigidity (Fazly et al., 2009; McCarthy et al., 2007). However, these approaches are limited in their applicability to compound nouns (Reddy et al., 2011). Another method is to compare the semantics of a compound and its constituents to decide compositionality. The approaches used to determine those semantics can again be divided into knowledge intensive and data-driven methods. Depending on the chosen representation of semantics these approaches can either be used for supervised classifiers or together with a distance metric comparing vector space representations of semantics. In"
S12-1021,I11-1024,0,0.516843,"interpret noun compounds by extrapolating their semantics from observations where the two nouns forming a compound are in an intransitive relationship. For example extracting the phrase ‘the family owns a car’ from the training data would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a classification option, considering it jointly with the actual compound interpretation. Next to the work on MWE interpretation there has been some work focused on determining lexicality in its own right (Reddy et al., 2011; Bu et al., 2010; Kim and Baldwin, 2007). One possibility is to exploit special properties of 133 lexical MWEs such as high statistical association of their constituents (Pedersen, 2011) or syntactic rigidity (Fazly et al., 2009; McCarthy et al., 2007). However, these approaches are limited in their applicability to compound nouns (Reddy et al., 2011). Another method is to compare the semantics of a compound and its constituents to decide compositionality. The approaches used to determine those semantics can again be divided into knowledge intensive and data-driven methods. Depending on the c"
S12-1021,P10-1070,0,0.141948,"rs from large corpora (Baroni and Zamparelli, 2010; Zanzotto et al., 2010; Giesbrecht, 2009). The focus of these approaches is to develop methods for composing the vectors of unigrams into a semantic vector representing a compound. Some of the work in this area touches on the issue of lexicality, as models learning distributional representations of MWEs ideally would first establish whether a given MWE is compositional or not (Mitchell and Lapata, 2010). The other group are knowledge intensive approaches collecting linguistic features (Kim and Baldwin, 2005; Korkontzelos and Manandhar, 2009). Tratz and Hovy (2010), for instance, train a classifier for noun compound interpretation on a large set of W ORD N ET and Thesaurus features. Combined approaches include Kim and Baldwin (2008), who interpret noun compounds by extrapolating their semantics from observations where the two nouns forming a compound are in an intransitive relationship. For example extracting the phrase ‘the family owns a car’ from the training data would help learn that the compound ‘family car’ describes a P OSSESSOR-OWNED /P OSSESSED relationship. Some of these supervised classifiers include lexicality as a classification option, con"
S12-1021,C10-1142,0,0.124978,"Missing"
S12-1021,W09-2416,0,\N,Missing
S17-2157,P13-2131,0,0.223412,"Missing"
S17-2157,P05-1045,0,0.0447292,"Missing"
S17-2157,P14-1134,0,0.228689,". Each token ei is represented by a hidden state hi , which is the concatenation of its forward and backward LSTM state vectors. Let sj be the RNN decoder hidden state at output position j. We set s0 to be the final RNN state of the backward encoder LSTM. The alignment aj is predicted at each time-step with a pointer network (Vinyals et al., 2015), although it will only affect the output when fj is a lexical concept or constant. The alignment logits are computed with an MLP (for i = 1, . . . , I): NE tagging with the same toolkit. The training data is aligned with the rule-based JAMR aligner (Flanigan et al., 2014). However, our approach requires single-token alignments for all nodes, which JAMR is not guaranteed to give. We align each Wiki node to the token with the highest prefix overlap. Other nodes without alignments are aligned to the left-most alignment of their children (if they have any), otherwise to that of their parents. JAMR aligns multi-word named entities as single subgraph to token span alignments. We split these alignments to be 1-1 between tokens and constants. For other nodes with multi-token alignments we use the start of the given span. For each token we predict candidate lexemes usi"
S17-2157,S16-1185,0,0.14636,"Missing"
S17-2157,N15-1142,0,0.0164159,"ch SGD, we sample Bio AMR sentences with a weight of 7 to each LDC sentence to balance the two sources in sampled minibatches. Our models are implemented in TensorFlow (Abadi et al., 2015). We train models with Adam (Kingma and Ba, 2015) with learning rate 0.01 and minibatch size 64. Gradients norms are clipped to 5.0 (Pascanu et al., 2013). We use single-layer LSTMs with hidden state size 256, with dropout 0.3 on the input and output connections. The encoder takes word embeddings of size 512, initialized (in the first 100 dimensions) with embeddings trained with a structured skip-gram model (Ling et al., 2015), and POS and NE embeddings of size 32. Singleton tokens are replaced with an unknown word symbol with probability 0.5 during training. We compare our pointer-based architecture against an attention-based encoder-decoder that does not make use of alignments or external lexical resources. We report results for two versions of this baseline: In the first, the input is purely word-based. The second embeds named entity and POS embeddings in the encoder, and utilizes pre-trained word embeddings. Development set Model Bio AMR LDC Smatch F1 59.27 61.89 Table 4: Test set results for the Bio AMR and LD"
S17-2157,W13-2322,0,0.291289,"Missing"
S17-2157,P14-5010,0,0.0135738,"ent nodes are simply repeated. During post-processing reentrancies are recovered heuristically by finding the closest nodes in the linear representation with the same concepts. An example of this representation is given in Figure 2. In the second representation (lexicalized) every graph node is aligned to an input token. The alignments could be encoded as strings in the graph linerization, but in our model we will predict them separately. Every constant is replaced with a placeholder CONST token; the constant string is 2.1 Pre-processing We tokenize the data with the Stanford CoreNLP toolkit (Manning et al., 2014). This tokenization corresponds more closely to AMR concepts and constants than other tokenizers we experimented with, especially due to its handling of hyphenation in the biomedical domain. We perform POS and 915 e and predict f and a, the latter with a pointer network (Vinyals et al., 2015). We use a standard LSTM architecture (Jozefowicz et al., 2015). For every token e we embed its word, POS tag and named entity (NE) tag as vectors; these embeddings are concatenated and passed through a linear layer such that the output g(e) has the same dimension as the LSTM cell. This representation of e"
S17-2157,S16-1176,0,0.235963,"ssumed to have any relation to the sentence syntax, no alignments are given and no distinction is made between concepts that correspond directly to lexemes in the input sentences and those that don’t. This underspecification creates significant challenges for training an end-to-end AMR parser, which are exacerbated by the relatively small sizes of available training sets. Consequently most AMR parsers are pipelines that make extensive use of additional resources. Neural encoder-decoders have previously been proposed for AMR parsing, but reported accuracies are well below the state-of-the-art (Barzdins and Gosko, 2016), even 2 Graph Linearization and Lemmatization We start by discussing how to linearize AMR graphs to enable sequential prediction. AMR node labels are referred to as concepts and edge labels as relations. A special class of node modifiers, called constants, are used to denote the string values of named entities and numbers. An example AMR graph is visualized in Figure 1. In AMR datasets, graphs are represented as spanning trees with designated root nodes. Edges whose direction in the spanning tree are reversed are marked by adding “-of” to the argument label. 914 Proceedings of the 11th Intern"
S17-2157,H92-1116,0,0.631687,"Missing"
S17-2157,J05-1004,0,0.0387034,"e meaning of an aligned token, or non-lexical. This distinction, together with alignments, is annotated explicitly in Minimal Recursion Semantics predicates in the English Resource Grammar (ERG) (Copestake et al., 2005). However for AMR we classify concepts heuristically, based on automatic alignments. We assume that each word in a sentence aligns to at most one lexical node in its AMR graph. Where multiple nodes are aligned to the same token, usually forming a subgraph, the lowest element is taken to be the lexical concept. A subset of AMR concepts are predicates based on PropBank framesets (Palmer et al., 2005), represented as sense-labeled lemmas. The remaining lexical concepts are usually English words in lemma form, while non-lexical concepts are usually special keywords. Lemmas can be predicted with high accuracy from the words they align to. Our third linearization (delexicalized) factorizes the lemmas of lexical concepts out of the linerization, so that they are represented by their alignments and sense labels, e.g. -01 for predicates and -u for other concepts. Candidate lemmas are predicted independently and lexicalized concepts are recovered as a post-processing step. This representation (se"
S17-2157,E17-1035,0,0.18538,"Missing"
U04-1015,J96-1002,0,0.00575353,"on of multiple features over observations. A limitation of HMMs is that it is hard to extend them to allow multiple features of observations, rather than atomic observations themselves. An alternative to the HMM was proposed by McCallum et al. (2000) in which the transition and observation probability matrices are replaced by maximum entropy classifiers for each state. These classifiers encode the probability distribution Ps0 (s|o), the probability of making the transition to s from s0 and observing o. 4.1 Conditional Exponential Transition Model The maximum entropy framework, as presented by Berger et al. (1996), aims to “model all that is known and assume nothing about that which is unknown.” This is achieved by choosing the model that fits all the constraints expressed by the training data and is the most uniform, i.e. the one with the highest entropy. Many classification tasks are most naturally handled by representing the instance to be classified as a vector of features. By combining the state and observation transition functions into a single maximum entropy model for each state we can condition the tag sequence assigned to a sentence on such things as part–of–speech tags, phrasal tags, predica"
U04-1015,W04-2412,0,0.0201991,"created from Propbank annotations on six sections of the Wall Street Journal component of the Penn Treebank. The standard semantic labels from Propbank were used: Verb specific arguments Arguments with a specific semantic meaning for a verb are labelled A0-A5. The semantics of the roles corresponding to these numbered arguments are defined in the Propbank frame for the predicate verb, but in general A0 maps to agent and A1 to patient or theme. Adjunctive arguments General arguments that any verb may take. These include AM-LOC, for locative and AM-TMP, for temporal. For the complete list see (Carreras and Marquez, 2004). Argument references Predicate arguments that reference other predicate arguments. Labelled R-A? with the argument referenced as the suffix, eg. R-A1 is a reference to the A1 defined elsewhere. Predicate verb The predicate verb that defines the proposition being labelled is tagged V. The data contained annotations of part-of-speech (PoS), base-phrase chunks, clause embedding and named entities. An example of the data is shown in figure 3. For each of the target verbs a column is provided with the argument labelling for that verb. PoS and base–phrase chunks are annotated in IOB2(inside, outsid"
U04-1015,P97-1003,0,0.0684899,"Missing"
U04-1015,W03-1007,0,0.168842,"Missing"
U04-1015,J02-3001,0,0.121263,"Missing"
U04-1015,W04-2419,0,0.123348,"Missing"
U06-1011,P03-1059,1,0.778864,"gories for the lexicon of a precision grammar, such as the English Resource Grammar (ERG; Flickinger (2002);Copestake and Flickinger (2000)), as seen in Baldwin (2005). A common tool for this is supertagging, where the classes are predicted in a task analogous to partof-speech (POS) tagging (Clark, 2002; Blunsom and Baldwin, 2006). The former technique is exemplified by expert systems, where the target language is occasionally not English. These learn properties such as verb subcategorisation frames (e.g. Korhonen (2002) for English or Schulte im Walde (2003) for German) or countability (e.g. Baldwin and Bond (2003) for English, or van der Beek and Baldwin (2004) for Dutch, with a crosslingual component). Common methodologies vary from mining lexical items from a lexical resource directly (e.g. Sanfilippo and Poznanski (1992) for a machine– readable dictionary), learning a particular property from a resource to apply it to a lexical type system (e.g. Carroll and Fang (2004) for verb subcategorisation frames), restricting possible target types according to evidence, and unifying to a consolidated entry (e.g. Fouvry (2003) for precision grammar lexical types), or applying the lexical category of similar in"
U06-1011,W05-1008,1,0.930574,"LPH - IN (Oepen et al., 2002) or PARGRAM (Butt et al., 2002) grammars, and modelling with richly annotated treebanks such as PropBank (Palmer et al., 2005) or CCGbank (Hockenmaier and Steedman, 2002). Unfortunately, the increasing complexity of these language resources and the desire for broad coverage has meant that their traditionally manual mode of creation, development and maintenance has become infeasibly labour-intensive. As a consequence, deep lexical acquisition (DLA) has been proposed as a means of automatically learning deep linguistic representations to expand the coverage of DLRs (Baldwin, 2005). DLA research can be divided into two main categories: targeted DLA, in which lexemes are classified according to a given lexical property (e.g. noun countability, or subcategorisation properties); and generalised DLA, in which lexemes are classified according to the full range of lexical properties captured in a given DLR (e.g. the full range of lexical relations in a lexical ontology, or the system of lexical types in an HPSG). As we attest in Section 2, most work in deep lexical acquisition has focussed on the English language. This can be explained in part by the ready availability of tar"
U06-1011,W06-1620,1,0.808124,"es defined for a given resource. The latter technique is often construed as a classification task where the classes are the lexical categories from the target resource. One example is extending an ontology such as WordNet (e.g. Pantel (2005), Daud´e et al. (2000)). Another is learning the categories for the lexicon of a precision grammar, such as the English Resource Grammar (ERG; Flickinger (2002);Copestake and Flickinger (2000)), as seen in Baldwin (2005). A common tool for this is supertagging, where the classes are predicted in a task analogous to partof-speech (POS) tagging (Clark, 2002; Blunsom and Baldwin, 2006). The former technique is exemplified by expert systems, where the target language is occasionally not English. These learn properties such as verb subcategorisation frames (e.g. Korhonen (2002) for English or Schulte im Walde (2003) for German) or countability (e.g. Baldwin and Bond (2003) for English, or van der Beek and Baldwin (2004) for Dutch, with a crosslingual component). Common methodologies vary from mining lexical items from a lexical resource directly (e.g. Sanfilippo and Poznanski (1992) for a machine– readable dictionary), learning a particular property from a resource to apply i"
U06-1011,W02-1503,0,0.0351909,"osyntax, in order to create language resources in a language-independent manner. 1 Introduction As a result of incremental annotation efforts and advances in algorithm design and statistical modelling, deep language resources (DLRs, i.e. language resources with a high level of linguistic sophistication) are increasingly being applied to mainstream NLP applications. Examples include analysis of semantic similarity through ontologies such as WordNet (Fellbaum, 1998) or VerbOcean (Chklovski and Pantel, 2004), parsing with precision grammars such as the DELPH - IN (Oepen et al., 2002) or PARGRAM (Butt et al., 2002) grammars, and modelling with richly annotated treebanks such as PropBank (Palmer et al., 2005) or CCGbank (Hockenmaier and Steedman, 2002). Unfortunately, the increasing complexity of these language resources and the desire for broad coverage has meant that their traditionally manual mode of creation, development and maintenance has become infeasibly labour-intensive. As a consequence, deep lexical acquisition (DLA) has been proposed as a means of automatically learning deep linguistic representations to expand the coverage of DLRs (Baldwin, 2005). DLA research can be divided into two main ca"
U06-1011,W04-3205,0,0.02577,". These results describe further scope in analysing other properties in languages displaying a more challenging morphosyntax, in order to create language resources in a language-independent manner. 1 Introduction As a result of incremental annotation efforts and advances in algorithm design and statistical modelling, deep language resources (DLRs, i.e. language resources with a high level of linguistic sophistication) are increasingly being applied to mainstream NLP applications. Examples include analysis of semantic similarity through ontologies such as WordNet (Fellbaum, 1998) or VerbOcean (Chklovski and Pantel, 2004), parsing with precision grammars such as the DELPH - IN (Oepen et al., 2002) or PARGRAM (Butt et al., 2002) grammars, and modelling with richly annotated treebanks such as PropBank (Palmer et al., 2005) or CCGbank (Hockenmaier and Steedman, 2002). Unfortunately, the increasing complexity of these language resources and the desire for broad coverage has meant that their traditionally manual mode of creation, development and maintenance has become infeasibly labour-intensive. As a consequence, deep lexical acquisition (DLA) has been proposed as a means of automatically learning deep linguistic"
U06-1011,W02-2203,0,0.0183067,"m of properties defined for a given resource. The latter technique is often construed as a classification task where the classes are the lexical categories from the target resource. One example is extending an ontology such as WordNet (e.g. Pantel (2005), Daud´e et al. (2000)). Another is learning the categories for the lexicon of a precision grammar, such as the English Resource Grammar (ERG; Flickinger (2002);Copestake and Flickinger (2000)), as seen in Baldwin (2005). A common tool for this is supertagging, where the classes are predicted in a task analogous to partof-speech (POS) tagging (Clark, 2002; Blunsom and Baldwin, 2006). The former technique is exemplified by expert systems, where the target language is occasionally not English. These learn properties such as verb subcategorisation frames (e.g. Korhonen (2002) for English or Schulte im Walde (2003) for German) or countability (e.g. Baldwin and Bond (2003) for English, or van der Beek and Baldwin (2004) for Dutch, with a crosslingual component). Common methodologies vary from mining lexical items from a lexical resource directly (e.g. Sanfilippo and Poznanski (1992) for a machine– readable dictionary), learning a particular propert"
U06-1011,copestake-flickinger-2000-open,0,0.0742019,"8 2 Background 2.1 Deep Lexical Acquisition As mentioned above, DLA traditionally takes two forms: targeted toward a specific lexical property, or generalised to map a term to an amalgam of properties defined for a given resource. The latter technique is often construed as a classification task where the classes are the lexical categories from the target resource. One example is extending an ontology such as WordNet (e.g. Pantel (2005), Daud´e et al. (2000)). Another is learning the categories for the lexicon of a precision grammar, such as the English Resource Grammar (ERG; Flickinger (2002);Copestake and Flickinger (2000)), as seen in Baldwin (2005). A common tool for this is supertagging, where the classes are predicted in a task analogous to partof-speech (POS) tagging (Clark, 2002; Blunsom and Baldwin, 2006). The former technique is exemplified by expert systems, where the target language is occasionally not English. These learn properties such as verb subcategorisation frames (e.g. Korhonen (2002) for English or Schulte im Walde (2003) for German) or countability (e.g. Baldwin and Bond (2003) for English, or van der Beek and Baldwin (2004) for Dutch, with a crosslingual component). Common methodologies var"
U06-1011,N03-1006,0,0.666612,"dictions of language users when confronted with unseen words.1 When contextual or lexicographic information is available for a language, this is usually a reliable method for the prediction of gender. Consequently, automatic prediction of gender in languages which have inflectional morphology is usually seen as the domain of the POS tagger (such as Haji˘c and Hladk´a (1998)), or morphological analyser (e.g. GERTWOL (Haapalainen and Majorin, 1995) for German and FLEMM (Namer, 2000) for French). One work in automatic gender prediction that is similar to this one is the bootstrapping approach of Cucerzan and Yarowsky (2003). Starting with a seed set of nouns whose gender is presumably language–invariant, they mine contextual features to hypothesise the gender of novel instances. They then extract simple morphological features of their larger predicted set, and use these to predict the gender of all nouns in their corpora. The major differences between this work and our own are in the approach Cucerzan and Yarowsky use, and the classes that they can handle. First, their semi-automatic approach relies on 1 See Tucker et al. (1977), among others, for detailed studies of L1 and L2 gender acquisition. 69 a bilingual"
U06-1011,P98-1080,0,0.0966055,"Missing"
U06-1011,hockenmaier-steedman-2002-acquiring,0,0.0133019,"otation efforts and advances in algorithm design and statistical modelling, deep language resources (DLRs, i.e. language resources with a high level of linguistic sophistication) are increasingly being applied to mainstream NLP applications. Examples include analysis of semantic similarity through ontologies such as WordNet (Fellbaum, 1998) or VerbOcean (Chklovski and Pantel, 2004), parsing with precision grammars such as the DELPH - IN (Oepen et al., 2002) or PARGRAM (Butt et al., 2002) grammars, and modelling with richly annotated treebanks such as PropBank (Palmer et al., 2005) or CCGbank (Hockenmaier and Steedman, 2002). Unfortunately, the increasing complexity of these language resources and the desire for broad coverage has meant that their traditionally manual mode of creation, development and maintenance has become infeasibly labour-intensive. As a consequence, deep lexical acquisition (DLA) has been proposed as a means of automatically learning deep linguistic representations to expand the coverage of DLRs (Baldwin, 2005). DLA research can be divided into two main categories: targeted DLA, in which lexemes are classified according to a given lexical property (e.g. noun countability, or subcategorisation"
U06-1011,J05-1004,0,0.00912527,"on As a result of incremental annotation efforts and advances in algorithm design and statistical modelling, deep language resources (DLRs, i.e. language resources with a high level of linguistic sophistication) are increasingly being applied to mainstream NLP applications. Examples include analysis of semantic similarity through ontologies such as WordNet (Fellbaum, 1998) or VerbOcean (Chklovski and Pantel, 2004), parsing with precision grammars such as the DELPH - IN (Oepen et al., 2002) or PARGRAM (Butt et al., 2002) grammars, and modelling with richly annotated treebanks such as PropBank (Palmer et al., 2005) or CCGbank (Hockenmaier and Steedman, 2002). Unfortunately, the increasing complexity of these language resources and the desire for broad coverage has meant that their traditionally manual mode of creation, development and maintenance has become infeasibly labour-intensive. As a consequence, deep lexical acquisition (DLA) has been proposed as a means of automatically learning deep linguistic representations to expand the coverage of DLRs (Baldwin, 2005). DLA research can be divided into two main categories: targeted DLA, in which lexemes are classified according to a given lexical property ("
U06-1011,P05-1016,0,0.0130401,"of which we made use, and Section 4 details the feature set. Finally, we evaluate our method in Section 5, and supply a discussion and brief conclusion in Sections 6 and 7. 68 2 Background 2.1 Deep Lexical Acquisition As mentioned above, DLA traditionally takes two forms: targeted toward a specific lexical property, or generalised to map a term to an amalgam of properties defined for a given resource. The latter technique is often construed as a classification task where the classes are the lexical categories from the target resource. One example is extending an ontology such as WordNet (e.g. Pantel (2005), Daud´e et al. (2000)). Another is learning the categories for the lexicon of a precision grammar, such as the English Resource Grammar (ERG; Flickinger (2002);Copestake and Flickinger (2000)), as seen in Baldwin (2005). A common tool for this is supertagging, where the classes are predicted in a task analogous to partof-speech (POS) tagging (Clark, 2002; Blunsom and Baldwin, 2006). The former technique is exemplified by expert systems, where the target language is occasionally not English. These learn properties such as verb subcategorisation frames (e.g. Korhonen (2002) for English or Schul"
U06-1011,W04-2104,0,0.0144916,"hology or syntax. This corpus is heavily domain–specific, and the lack of annotation provides particular problems, which we explain below. Note that we make no use of the English component of BAF in this paper. 3.2 Inflectional Lexicons Whereas our German corpus has gold-standard judgements of gender for each token, the French corpus has no such information. Consequently, we use a semi-automatic method to match genders to nouns. Using the Lefff syntactic lexi2 3 http://www.ims.uni-stuttgart.de/projekte/TIGER http://rali.iro.umontreal.ca/Ressources/BAF con4 (Sagot et al., 2006) and Morphalou5 (Romary et al., 2004), a lexicon of inflected forms, we automatically annotate tokens for which the sources predict an unambiguous gender, and handannotate ambiguous tokens using contextual information. These ambiguous tokens are generally animate nouns like coll`egue, which are masculine or feminine according to their referent, or polysemous nouns like aide, whose gender depends on the applicable sense. 3.3 POS Taggers Again, TIGER comes annotated with handcorrected part-of-speech tags, while the BAF does not. For consistency, we tag both corpora with TreeTagger6 (Schmid, 1994), a decision tree– based probabilist"
U06-1011,sagot-etal-2006-lefff,0,0.056038,"Missing"
U06-1011,A92-1011,0,0.271756,"g, where the classes are predicted in a task analogous to partof-speech (POS) tagging (Clark, 2002; Blunsom and Baldwin, 2006). The former technique is exemplified by expert systems, where the target language is occasionally not English. These learn properties such as verb subcategorisation frames (e.g. Korhonen (2002) for English or Schulte im Walde (2003) for German) or countability (e.g. Baldwin and Bond (2003) for English, or van der Beek and Baldwin (2004) for Dutch, with a crosslingual component). Common methodologies vary from mining lexical items from a lexical resource directly (e.g. Sanfilippo and Poznanski (1992) for a machine– readable dictionary), learning a particular property from a resource to apply it to a lexical type system (e.g. Carroll and Fang (2004) for verb subcategorisation frames), restricting possible target types according to evidence, and unifying to a consolidated entry (e.g. Fouvry (2003) for precision grammar lexical types), or applying the lexical category of similar instances, based on some notion of similarity (e.g. Baldwin (2005), also for lexical types). It is this last approach that we use in this work. Implicit in all of these methods is a notion of the secondary language r"
U06-1011,P06-1040,0,0.0218069,"Missing"
U06-1011,J06-2001,0,\N,Missing
U06-1011,C98-1077,0,\N,Missing
W06-1620,W05-1008,1,0.835659,"to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage. Attempts to (semi-)automate the process of expanding the coverage of precision grammars have focused on either: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (van Noord, 2004; Zhang and Kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (Riezler et al., 2002); or (b) lexical coverage, e.g. in bootstrapping from a pre-existing grammar and lexicon to learn new lexical items (Baldwin, 2005a). Our particular interest in this paper is in the latter of these two, that is the development of methods for automatically expanding the lexical coverage of an existing precision grammar, or more broadly deep lexical acquisition (DLA hereafter). In this, we follow Baldwin (2005a) in assuming a semi-mature precision grammar with a fixed inventory of lexical types, based on which we learn new lexical items. For the purposes of this paper, we focus specifically on supertagging as the mechanism for hypothesising new lexical items. We propose a conditional random fieldbased method for supertaggi"
W06-1620,J99-2004,0,0.47271,"re that they are frequently bidirectional, and output a rich semantic abstraction for each spanning parse of the input string. Examples include DELPH - IN grammars such as the English Resource Grammar (Flickinger, 2002; Uszkoreit, 2002), the various PARGRAM grammars (Butt et al., 1999), and the Edinburgh CCG parser (Bos et al., 2004). Supertagging can be defined as the process of applying a sequential tagger to the task of predicting the lexical type(s) associated with each word in an input string, relative to a given grammar. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). In both of these cases, supertagging provides the means to perform a beam search over the plausible lexical items for a given string context, and ideally reduces parsing complexity without sacrificing parser accuracy. An alternate application of supertagging is in DLA, in postulating novel lexical items with which to populate the lexicon of a given grammar to boost parser coverage. This can take place 164 Proceedings of the 2006 Conference on Empirical Metho"
W06-1620,W02-1502,0,0.0269699,"tems for unknown words, by generating underspecified lexical items for all unknown words and parsing with them. Syntactico-semantic interaction between unknown words and pre-existing lexical items during parsing provides insight into the nature of each unknown word. By combining such fragments of information, it is possible to incrementally arrive at a consolidated lexical entry for that word. That is, the precision grammar itself drives the incremental learning process within a parsing context. In this research, we focus particularly on the Grammar Matrix-based DELPH - IN family of grammars (Bender et al., 2002), which includes grammars of English, Japanese, Norwegian, Modern Greek, Portuguese and Korean. The Grammar Matrix is a framework for streamlining and standardising HPSG-based multilingual grammar development. One property of Grammar Matrixbased grammars is that they are strongly lexicalist and adhere to a highly constrained lexicongrammar interface via a unique (terminal) lexical type for each lexical item. As such, lexical item creation in any of the Grammar Matrix-based grammars, irrespective of language, consists predominantly of predicting the appropriate lexical type for each lexical ite"
W06-1620,N01-1006,0,0.0469605,"ing, such as Tagalog, we may want to include -gram infixes in addition to -gram prefixes and suffixes. Here again, however, the decision about what range of affixes is appropriate for a given language requires only superficial knowledge of its morphology. 5 is predicted by:  ?BA""C DF ?    In the instance that  was not observed in the training data, we back off to the majority lexical type in the training data. 5.2 Benchmark: fnTBL In order to benchmark our results with the CRF models, we reimplemented the supertagger model proposed by Baldwin (2005b) which simply takes FN TBL 1.1 (Ngai and Florian, 2001) off the shelf and trains it over our particular training set. FN TBL is a transformation-based learner that is distributed with pre-optimised POS tagging modules for English and other European languages that can be redeployed over the task of supertagging. Following Baldwin (2005b), the only modifications we make to the default English POS tagging methodology are: (1) to set the default lexical types for singular common and proper nouns to n - c le and n - pn le, respectively; and (2) reduce the threshold score for lexical and context transformation rules to 1. It is important to realise that"
W06-1620,C04-1180,0,0.0274301,"natural language which capture finegrained linguistic distinctions, and are generative in the sense of distinguishing between grammatical and ungrammatical inputs (or at least have some in-built notion of linguistic “markedness”). Additional characteristics of precision grammars are that they are frequently bidirectional, and output a rich semantic abstraction for each spanning parse of the input string. Examples include DELPH - IN grammars such as the English Resource Grammar (Flickinger, 2002; Uszkoreit, 2002), the various PARGRAM grammars (Butt et al., 1999), and the Edinburgh CCG parser (Bos et al., 2004). Supertagging can be defined as the process of applying a sequential tagger to the task of predicting the lexical type(s) associated with each word in an input string, relative to a given grammar. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). In both of these cases, supertagging provides the means to perform a beam search over the plausible lexical items for a given string context, and ideally reduces parsing c"
W06-1620,P02-1035,0,0.0264849,"win Computer Science and Software Engineering University of Melbourne, Victoria 3010 Australia {pcbl,tim}@csse.unimelb.edu.au Abstract Due to their linguistic complexity, precision grammars are generally hand-constructed and thus restricted in size and coverage. Attempts to (semi-)automate the process of expanding the coverage of precision grammars have focused on either: (a) constructional coverage, e.g. in the form of error mining for constructional expansion (van Noord, 2004; Zhang and Kordoni, 2006), or relaxation of lexico-grammatical constraints to support partial and/or robust parsing (Riezler et al., 2002); or (b) lexical coverage, e.g. in bootstrapping from a pre-existing grammar and lexicon to learn new lexical items (Baldwin, 2005a). Our particular interest in this paper is in the latter of these two, that is the development of methods for automatically expanding the lexical coverage of an existing precision grammar, or more broadly deep lexical acquisition (DLA hereafter). In this, we follow Baldwin (2005a) in assuming a semi-mature precision grammar with a fixed inventory of lexical types, based on which we learn new lexical items. For the purposes of this paper, we focus specifically on s"
W06-1620,N03-1028,0,0.0502315,"Missing"
W06-1620,C04-1041,0,0.289443,"lude DELPH - IN grammars such as the English Resource Grammar (Flickinger, 2002; Uszkoreit, 2002), the various PARGRAM grammars (Butt et al., 1999), and the Edinburgh CCG parser (Bos et al., 2004). Supertagging can be defined as the process of applying a sequential tagger to the task of predicting the lexical type(s) associated with each word in an input string, relative to a given grammar. It was first introduced as a means of reducing parser ambiguity by Bangalore and Joshi (1999) in the context of the LTAG formalism, and has since been applied in a similar context within the CCG formalism (Clark and Curran, 2004). In both of these cases, supertagging provides the means to perform a beam search over the plausible lexical items for a given string context, and ideally reduces parsing complexity without sacrificing parser accuracy. An alternate application of supertagging is in DLA, in postulating novel lexical items with which to populate the lexicon of a given grammar to boost parser coverage. This can take place 164 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 164–171, c Sydney, July 2006. 2006 Association for Computational Linguistics eithe"
W06-1620,W02-1210,0,0.0715803,"th the left-adjacent word to form a single lexeme, and shares the same lexical type. This tagging convention is based on that used, e.g., in the CLAWS7 part-of-speech tagset. Optionally discontinuous lexical items are less of a concern, as selection of each of the discontinuous “components” is done via lexical types. E.g. in the case of pick up, the lexical entry looks as follows: Task and Resources In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2000)) and the JACY grammar of Japanese (Siegel and Bender, 2002). Note that our choice of the ERG and JACY as testbeds for experimentation in this paper is somewhat arbitrary, and that we could equally run experiments over any Grammar Matrix-based grammar for which there is treebank data. Both the ERG and JACY are implemented open-source broad-coverage precision Headdriven Phrase Structure Grammars (HPSGs: Pollard and Sag (1994)). A lexical item in each of the grammars consists of a unique identifier, a lexical type (a leaf type of a type hierarchy), an orthography, and a semantic relation. For example, in the English grammar, the lexical item for the noun"
W06-1620,copestake-flickinger-2000-open,0,0.137222,"indicates that the current word combines (possibly recursively) with the left-adjacent word to form a single lexeme, and shares the same lexical type. This tagging convention is based on that used, e.g., in the CLAWS7 part-of-speech tagset. Optionally discontinuous lexical items are less of a concern, as selection of each of the discontinuous “components” is done via lexical types. E.g. in the case of pick up, the lexical entry looks as follows: Task and Resources In this section, we outline the resources targeted in this research, namely the English Resource Grammar (ERG: Flickinger (2002), Copestake and Flickinger (2000)) and the JACY grammar of Japanese (Siegel and Bender, 2002). Note that our choice of the ERG and JACY as testbeds for experimentation in this paper is somewhat arbitrary, and that we could equally run experiments over any Grammar Matrix-based grammar for which there is treebank data. Both the ERG and JACY are implemented open-source broad-coverage precision Headdriven Phrase Structure Grammars (HPSGs: Pollard and Sag (1994)). A lexical item in each of the grammars consists of a unique identifier, a lexical type (a leaf type of a type hierarchy), an orthography, and a semantic relation. For ex"
W06-1620,P04-1057,0,0.0922746,"Missing"
W06-1620,E03-1040,0,0.031734,"r corpus occurrences of an unknown word in such contexts. That is, the morphological, syntactic and/or semantic predictions implicit in each lexical type are made explicit in the form of templates which represent distinguishing lexical contexts of that lexical type. This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). Other work on DLA (e.g. Korhonen (2002), Joanis and Stevenson (2003), Baldwin (2005a)) has tended to take an in vitro DLA approach, in extrapolating away from a DLR to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. a secondary parser or POS tagger). In vitro DLA can also take the form of resource translation, in mapping one DLR onto another to arrive at the lexical information in the desired format. 3 countable, ""dog"" specifies the lexical stem, and "" dog n 1 rel"" introduces an ad hoc predicate name for the lexical item to use in constructing a semantic representation. In the context of the ERG and JACY"
W06-1620,U05-1006,0,0.0611088,"ion for Computational Linguistics either: (a) off-line for the purposes of rounding out the coverage of a static lexicon, in which case we are generally interested in globally maximising precision over a given corpus and hence predicting the single most plausible lexical type for each word token (off-line DLA: Baldwin (2005b)); or (b) on the fly for a given input string to temporarily expand lexical coverage and achieve a spanning parse, in which case we are interested in maximising recall by producing a (possibly weighted) list of lexical item hypotheses to run past the grammar (on-line DLA: Zhang and Kordoni (2005)). Our immediate interest in this paper is in the first of these tasks, although we would ideally like to develop an off-line method which is trivially portable to the second task of on-line DLA. supertagger-based DLA methods. The remainder of this paper is structured as follows. Section 2 outlines past work relative to this research, and Section 3 reviews the resources used in our supertagging experiments. Section 4 outlines the proposed supertagger model and reviews previous research on supertaggerbased DLA. Section 5 then outlines the set-up and results of our evaluation. 2 Past Research Ac"
W06-1620,zhang-kordoni-2006-automated,0,0.0794524,"Missing"
W06-1620,N04-1016,0,0.0313107,"n that they do not rely on preprocessing of any form), and check for corpus occurrences of an unknown word in such contexts. That is, the morphological, syntactic and/or semantic predictions implicit in each lexical type are made explicit in the form of templates which represent distinguishing lexical contexts of that lexical type. This approach has been shown to be particularly effective over web data, where the sheer size of the data precludes the possibility of linguistic preprocessing but at the same time ameliorates the effects of data sparseness inherent in any lexicalised DLA approach (Lapata and Keller, 2004). Other work on DLA (e.g. Korhonen (2002), Joanis and Stevenson (2003), Baldwin (2005a)) has tended to take an in vitro DLA approach, in extrapolating away from a DLR to corpus or web data, and analysing occurrences of words through the conduit of an external resource (e.g. a secondary parser or POS tagger). In vitro DLA can also take the form of resource translation, in mapping one DLR onto another to arrive at the lexical information in the desired format. 3 countable, ""dog"" specifies the lexical stem, and "" dog n 1 rel"" introduces an ad hoc predicate name for the lexical item to use in cons"
W06-1620,W02-2018,0,0.034304,"le, oqpsrqt “mouldy” would be flagged as containing katakana character(s), kanji character(s) and hiragana character(s) only. Note that the only language-dependent component of J  4.2 b c6de.fhgji C   &lt;   2  g""i   C  R  (4) In order to train the model, we maximize (4). While the log-pseudo-likelihood cannot be maximised for the parameters, * , in closed form, it is a convex function, and thus we resort to numerical optimisation to find the globally optimal parameters. We use L-BFGS, an iterative quasi-Newton optimisation method, which performs well for training log-linear models (Malouf, 2002; Sha and 168 Baseline FN TBL  CRF !  CRF ACC 0.802 0.915 0.911 0.917 ACC 0.053 0.236 0.427 0.489 ERG P REC 0.184 0.370 0.339 0.509 R EC 0.019 0.038 0.053 0.059 F- SCORE 0.034 0.068 0.092 0.105 ACC 0.866 — 0.920 0.932 ACC 0.592 — 0.816 0.827 JACY P REC 0.680 — 0.548 0.696 R EC 0.323 — 0.414 0.424 F- SCORE 0.438 — 0.471 0.527 Table 3. Results of supertagging for the ERG and JACY (best result in each column in bold) the lexical features is the character sets, which requires little or no specialist knowledge of the language. Note also that for languages with infixing, such as Tagalog,"
W09-0434,D08-1078,1,0.823619,"e of these systems adequately deal with longer range reordering. Our analysis provides a deeper understanding of why hierarchical models demonstrate better performance for Chinese-English translation, and also why phrase-based approaches do well at Arabic-English. We begin by reviewing related work in Section 2. Section 3 describes our method for extracting and measuring reorderings in aligned and parsed parallel corpora. We apply our techniques to human aligned parallel treebank sentences in Section 4, and to machine translation outputs in Section 5. We summarise our findings in Section 6. 2 Birch et al. (2008) proposed a method for extracting reorderings from aligned parallel sentences. We extend this method in order to constrain the reorderings to a derivation over the source sentence where possible. 3 Measuring Reordering Reordering is largely driven by syntactic differences between languages and can involve complex rearrangements between nodes in synchronous trees. Modeling reordering exactly would be sparse and heterogeneous and thus we make an important simplifying assumption in order for the detection and extraction of reordering data to be tractable and useful. We assume that reordering is a"
W09-0434,E06-1032,1,0.705755,"it very difficult to model long distance movement of words between languages. Synchronous grammar models can encode structural mappings between languages which allow complex, long distance reordering. Some grammar-based models such as the hierarchical model (Chiang, 2005) and the syntactified target language phrases model (Marcu et al., 2006) have shown better performance than phrase-based models on certain language pairs. To date our understanding of the variation in reordering performance between phrase-based and synchronous grammar models has been limited to relative B LEU scores. However, Callison-Burch et al. (2006) showed that B LEU score alone is insufficient for comparing reordering as it only measures a partial ordering on n-grams. There has been little direct research on empirically evaluating reordering. We evaluate the reordering characteristics of these two paradigms on Chinese-English and Arabic-English translation. Our main findings are as follows: (1) Chinese-English parallel sentences exhibit many medium and long-range reorderings, but less short range ones than Arabic-English, (2) phrase-based models account for short-range reorderings better than hierarchical models do, (3) Reordering is a"
W09-0434,H05-1098,0,0.0367168,"ere are few empirical studies of reordering behaviour in the statistical machine translation literature. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. This study was performed manually and did not compare different language pairs or translation paradigms. There are some comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by using the frequency of POS sequences in the output. This method is a first step towards a better understanding of comparative reordering performance, but neglects the question of what kind of reordering is occurring in corpora and in translation output. Zollmann et al. (2008) performed an empirical comparison of the B LEU score performance of hierarchical models with phrase-based models. They tried to ascertain which is the stronger model under different reordering scenarios by varying distortion limits"
W09-0434,P05-1033,0,0.0716465,"last few years, showing state-of-the-art performance for many language pairs. They search all possible reorderings within a restricted window, and their output is guided by the language model and a lexicalised reordering model (Och et al., 2004), both of which are local in scope. However, the lack of structure in phrase-based models makes it very difficult to model long distance movement of words between languages. Synchronous grammar models can encode structural mappings between languages which allow complex, long distance reordering. Some grammar-based models such as the hierarchical model (Chiang, 2005) and the syntactified target language phrases model (Marcu et al., 2006) have shown better performance than phrase-based models on certain language pairs. To date our understanding of the variation in reordering performance between phrase-based and synchronous grammar models has been limited to relative B LEU scores. However, Callison-Burch et al. (2006) showed that B LEU score alone is insufficient for comparing reordering as it only measures a partial ordering on n-grams. There has been little direct research on empirically evaluating reordering. We evaluate the reordering characteristics of"
W09-0434,J07-2003,0,0.0242398,"e the reorderings in the parallel corpora with the reorderings that exist in the translated sentences. We com200 367 379 367 379 250 1.51 0.57 200 0.82 0.25 Table 1. The RQuantity and the number of sentences for each reordering test set. 0 2 3 4 5 6 7−8 9−10 16−20 Widths of Reorderings 22 Figure 6. Number of reorderings in the CH-EN test set plotted against the total width of the reorderings. 16 18 20 MOSES HIERO Reordering Test Corpus 14 5.1 50 pare two state-of-the-art models: the phrase-based system Moses (Koehn et al., 2007) (with lexicalised reordering), and the hierarchical model Hiero (Chiang, 2007). We use default settings for both models: a distortion limit of seven for Moses, and a maximum source span limit of 10 words for Hiero. We trained both models on subsets of the NIST 2008 data sets, consisting mainly of news data, totalling 547,420 CH-EN and 1,069,658 AREN sentence pairs. We used a trigram language model on the entire English side (211M words) of the NIST 2008 Chinese-English training corpus. Minimum error rate training was performed on the 2002 NIST test for CH-EN, and the 2004 NIST test set for AR-EN. Low Medium High 150 High 100 Medium Number of Reorderings None Low Average"
W09-0434,W02-1039,0,0.161844,"we can see a sentence pair with an alignment and a parse tree over the source. We perform a depth first recursion through the tree, extracting the reorderings that occur between whole sibling nodes. Initially a reordering is detected between the leaf nodes P and NN. The block growing algorithm described in Birch et al. (2008) is then used to grow block A to include NT and NN, and block B to include P and NR. The source and target spans of these nodes do not overlap the spans Related Work There are few empirical studies of reordering behaviour in the statistical machine translation literature. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. This study was performed manually and did not compare different language pairs or translation paradigms. There are some comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by usin"
W09-0434,N03-1017,0,0.040066,"Missing"
W09-0434,P07-2045,1,0.00797446,"on models perform specifically with regard to reordering. To evaluate this, we compare the reorderings in the parallel corpora with the reorderings that exist in the translated sentences. We com200 367 379 367 379 250 1.51 0.57 200 0.82 0.25 Table 1. The RQuantity and the number of sentences for each reordering test set. 0 2 3 4 5 6 7−8 9−10 16−20 Widths of Reorderings 22 Figure 6. Number of reorderings in the CH-EN test set plotted against the total width of the reorderings. 16 18 20 MOSES HIERO Reordering Test Corpus 14 5.1 50 pare two state-of-the-art models: the phrase-based system Moses (Koehn et al., 2007) (with lexicalised reordering), and the hierarchical model Hiero (Chiang, 2007). We use default settings for both models: a distortion limit of seven for Moses, and a maximum source span limit of 10 words for Hiero. We trained both models on subsets of the NIST 2008 data sets, consisting mainly of news data, totalling 547,420 CH-EN and 1,069,658 AREN sentence pairs. We used a trigram language model on the entire English side (211M words) of the NIST 2008 Chinese-English training corpus. Minimum error rate training was performed on the 2002 NIST test for CH-EN, and the 2004 NIST test set for AR"
W09-0434,W04-3250,0,0.218516,"Missing"
W09-0434,W06-1606,0,0.0281669,"uage pairs. They search all possible reorderings within a restricted window, and their output is guided by the language model and a lexicalised reordering model (Och et al., 2004), both of which are local in scope. However, the lack of structure in phrase-based models makes it very difficult to model long distance movement of words between languages. Synchronous grammar models can encode structural mappings between languages which allow complex, long distance reordering. Some grammar-based models such as the hierarchical model (Chiang, 2005) and the syntactified target language phrases model (Marcu et al., 2006) have shown better performance than phrase-based models on certain language pairs. To date our understanding of the variation in reordering performance between phrase-based and synchronous grammar models has been limited to relative B LEU scores. However, Callison-Burch et al. (2006) showed that B LEU score alone is insufficient for comparing reordering as it only measures a partial ordering on n-grams. There has been little direct research on empirically evaluating reordering. We evaluate the reordering characteristics of these two paradigms on Chinese-English and Arabic-English translation."
W09-0434,J04-4002,0,0.0234622,"Missing"
W09-0434,P06-1123,0,0.0178902,"lude P and NR. The source and target spans of these nodes do not overlap the spans Related Work There are few empirical studies of reordering behaviour in the statistical machine translation literature. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. This study was performed manually and did not compare different language pairs or translation paradigms. There are some comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by using the frequency of POS sequences in the output. This method is a first step towards a better understanding of comparative reordering performance, but neglects the question of what kind of reordering is occurring in corpora and in translation output. Zollmann et al. (2008) performed an empirical comparison of the B LEU score performance of hierarchical models with phrase-based models. They tried to ascertain"
W09-0434,J97-3002,0,0.241369,"lex rearrangements between nodes in synchronous trees. Modeling reordering exactly would be sparse and heterogeneous and thus we make an important simplifying assumption in order for the detection and extraction of reordering data to be tractable and useful. We assume that reordering is a binary process occurring between two blocks that are adjacent in the source. We extend the methods proposed by Birch et al. (2008) to identify and measure reordering. Modeling reordering as the inversion in order of two adjacent blocks is similar to the approach taken by the Inverse Transduction Model (ITG) (Wu, 1997), except that here we are not limited to a binary tree. We also detect and include non-syntactic reorderings as they constitute a significant proportion of the reorderings. Birch et al. (2008) defined the extraction process for a sentence pair that has been word aligned. This method is simple, efficient and applicable to all aligned sentence pairs. However, if we have access to the syntax tree, we can more accurately determine the groupings of embedded reorderings, and we can also access interesting information about the reordering such as the type of constituents that get reordered. Figure 1"
W09-0434,P03-1019,0,0.0432249,", and block B to include P and NR. The source and target spans of these nodes do not overlap the spans Related Work There are few empirical studies of reordering behaviour in the statistical machine translation literature. Fox (2002) showed that many common reorderings fall outside the scope of synchronous grammars that only allow the reordering of child nodes. This study was performed manually and did not compare different language pairs or translation paradigms. There are some comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by using the frequency of POS sequences in the output. This method is a first step towards a better understanding of comparative reordering performance, but neglects the question of what kind of reordering is occurring in corpora and in translation output. Zollmann et al. (2008) performed an empirical comparison of the B LEU score performance of hierarchical models with phrase-based models"
W09-0434,C08-1144,0,0.0520931,"comparative studies of the reordering restrictions that can be imposed on the phrase-based or grammar-based models (Zens and Ney, 2003; Wellington et al., 2006), however these do not look at the reordering performance of the systems. Chiang et al. (2005) proposed a more fine-grained method of comparing the output of two translation systems by using the frequency of POS sequences in the output. This method is a first step towards a better understanding of comparative reordering performance, but neglects the question of what kind of reordering is occurring in corpora and in translation output. Zollmann et al. (2008) performed an empirical comparison of the B LEU score performance of hierarchical models with phrase-based models. They tried to ascertain which is the stronger model under different reordering scenarios by varying distortion limits the strength of language models. They show that the hierarchical models do slightly better for Chinese-English systems, but worse for Arabic-English. However, there was no analysis of the reorderings existing in their parallel corpora, or on what kinds of reorderings were produced in their output. We perform a focused evaluation of these issues. 198 A A B B Figure"
W09-0434,N04-1021,0,\N,Missing
W09-1114,P08-1024,1,0.253533,"r an advantage on an appropriately optimised model. 4 Minimum risk training In the previous section, we described how our sampler can be used to search for the best translation under a variety of decoding criteria (max derivation, translation, and minimum risk). However, there appeared to be little benefit to marginalizing over the latent derivations. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al., 2008). In this section we 107 describe a further application of the Gibbs sampler: to do unbiased minimum risk training. While there have been at least two previous attempts to do minimum risk training for MT, both approaches relied on biased k-best approximations (Smith and Eisner, 2006; Zens and Ney, 2007). Since we sample from the whole distribution, we will have a more accurate risk assessment. The risk, or expected loss, of a probabilistic translation model on a corpus D, defined with respect to a particular loss function `eˆ(e), where eˆ is the reference translation and e is a hypothesis tran"
W09-1114,J93-2003,0,0.0151602,"xpected risk training and decoding. 1 Introduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus"
W09-1114,D08-1033,0,0.0826863,"n reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP). Though effective for some proble"
W09-1114,W06-1673,0,0.0295365,"rementally improves it via operators such as R ETRANS and M ERGE -S PLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from its increasing use in Bayesian inference problems in NLP (Finkel et al., 2006; Johnson et al., 2007b). Most closely related is the work of DeNero et al. (2008), who derive a Gibbs sampler for phrase-based alignment, using it to infer phrase translation probabilities. The use of Monte Carlo techniques to calculate posteriors is similar to that of Chappelier and Rajman (2000) who use those techniques to find the best parse under models where the derivation and the parse are not isomorphic. 109 This research was supported in part by the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR0011-06-2-001; and by the EuroMatrix project funded by the"
W09-1114,P01-1030,0,0.032787,"ns. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the translation community that we envision will allow the development and analysis of increasing theoretically well motivated techniques. Acknowledgments Related work Our sampler is similar to the decoder of Germann et al. (2001), which starts with an approximate solution and then incrementally improves it via operators such as R ETRANS and M ERGE -S PLIT. It is also similar to the estimator of Marcu and Wong (2002), who employ the same operators to search the alignment space from a heuristic initialisation. Although the operators are similar, the use is different. These previous efforts employed their operators in a greedy hill-climbing search. In contrast, our operators are applied probabilistically, making them theoretically well-founded for a variety of inference problems. Our use of Gibbs sampling follows from it"
W09-1114,D07-1103,0,0.222812,"on P (a, e|f ) and therefore to determine the maximum of this distribution, in other words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either runn"
W09-1114,N07-1018,0,0.206505,"on P (a, e|f ) and therefore to determine the maximum of this distribution, in other words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either runn"
W09-1114,N03-1017,1,0.202563,"s for probabilistic inference: 1. It typically differs from the true model maximum. 2. It often requires additional approximations in search, leading to further error. 3. It introduces restrictions on models, such as use of only local features. 4. It provides no good solution to compute the normalization factor Z(f ) required by many probabilistic algorithms. In this work, we solve these problems using a Monte Carlo technique with none of the above drawbacks. Our technique is based on a novel Gibbs sampler that draws samples from the posterior distribution of a phrase-based translation model (Koehn et al., 2003) but operates in linear time with respect to the number of input words (Section 2). We show Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 102–110, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics that it is effective for both decoding (Section 3) and minimum risk training (Section 4). 2 A Gibbs sampler for phrase-based translation models We begin by assuming a phrase-based translation model in which the input sentence, f , is segmented into phrases, which are sequences of adjacent words.1 Each foreign phrase is"
W09-1114,P07-2045,1,0.0245677,"en the distribution is too flat and the sampler spends too much time exploring unimportant probability regions. If it is too large, then the distribution is too peaked and the sampler may concentrate on a very narrow probability region. We optimised the scaling factor on a 200-sentence portion of the tuning set, finding that a multiplicative factor of 10 worked best for fr-en and a multiplicative factor of 6 for de-en. 3 The first experiment shows the effect of different initialisations and numbers of sampler iterations on max-derivation decoding performance of the sampler. The Moses decoder (Koehn et al., 2007) was used to generate the starting hypothesis, either in full DP max-derivation mode, or alternatively with restrictions on the features and reordering, or with zero weights to simulate a random initialisation, and the number of iterations varied from 100 to 200,000, with a 100 iteration burn-in in each case. Figure 3 shows the variation of model score with sampler iteration, for the different starting points, and for both language pairs. 3 We experimented with annealing, where the scale factor is gradually increased to sharpen the distribution while sampling. However, we found no improvements"
W09-1114,N04-1022,0,0.549837,"formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and"
W09-1114,W02-1018,0,0.133102,"ntroduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfo"
W09-1114,N06-1045,0,0.0117505,"led to a situation where entire classes of potentially useful features are not considered because they would be impractical to integrate into a DP based translation system. With the sampler this restriction is mitigated: any function of h(e, f, a) may participate in the translation model subject only to its own computability. Freed from the rusty manacles of dynamic programming, we anticipate development of many useful features. 6 To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the t"
W09-1114,C00-2163,0,0.042141,"g and decoding. 1 Introduction Statistical machine translation (SMT) poses the problem: given a foreign sentence f , find the translation e∗ that maximises the conditional posterior probability p(e|f ). This probabilistic formulation of translation has driven development of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all o"
W09-1114,P03-1021,0,0.0606256,"nstructed a phrase-based translation model as described in Koehn et al. (2003), limiting the phrase length to 5. The target side of the parallel corpus was used to train a 3-gram language model. 2 The Arabic-English training data consists of the eTIRR corpus (LDC2004E72), the Arabic news corpus (LDC2004T17), the Ummah corpus (LDC2004T18), and the sentences with confidence c > 0.995 in the ISI automatically extracted web parallel corpus (LDC2006T02). 105 Translation performance For the experiments reported in this section, we used feature weights trained with minimum error rate training (MERT; Och, 2003) . Because MERT ignores the denominator in Equation 1, it is invariant with respect to the scale of the weight vector θ — the Moses implementation simply normalises the weight vector it finds by its `1 -norm. However, when we use these weights in a true probabilistic model, the scaling factor affects the behaviour of the model since it determines how peaked or flat the distribution is. If the scaling factor is too small, then the distribution is too flat and the sampler spends too much time exploring unimportant probability regions. If it is too large, then the distribution is too peaked and t"
W09-1114,P02-1040,0,0.0772047,"words the most likely derivation. Furthermore, we can marginalise over the alignments to estimate P (e|f ) ● ● For the German and French systems, the DEV 2006 set was used for model tuning and the TEST 2007 (in-domain) and NEWS - DEV 2009 B (out-of-domain) sets for testing. For the Arabic system, the MT 02 set (10 reference translations) was used for tuning and MT 03 (4 reference translations) was used for evaluation. To reduce the size of the phrase table, we used the association-score technique suggested by Johnson et al. (2007a). Translation quality is reported using case-insensitive BLEU (Papineni et al., 2002). KL Divergence ● ● ● 0.1 ● ● ● ● 0.01 ● ● 0.001 KL divergence ● ● ● ● ● 10 100 1000 10000 100000 ● 3.2 1000000 Iterations Figure 2: The KL divergence of the true posterior distribution and the distribution estimated by the Gibbs sampler at different numbers of iterations for the Arabic source sentence r}ys wzrA’ mAlyzyA yzwr Alflbyn (in English, The prime minister of Malaysia visits the Philippines). and so obtain the most likely translation. The Gibbs sampler can therefore be used as a decoder, either running in max-derivation and max-translation mode. Using the Gibbs sampler in this way mak"
W09-1114,C96-2215,0,0.0376306,"Missing"
W09-1114,P06-2101,0,0.637386,"ared to be little benefit to marginalizing over the latent derivations. This is almost certainly a side effect of the MERT training approach that was used to construct the models so as to maximise the performance of the model on its single best derivation, without regard to the shape of the rest of the distribution (Blunsom et al., 2008). In this section we 107 describe a further application of the Gibbs sampler: to do unbiased minimum risk training. While there have been at least two previous attempts to do minimum risk training for MT, both approaches relied on biased k-best approximations (Smith and Eisner, 2006; Zens and Ney, 2007). Since we sample from the whole distribution, we will have a more accurate risk assessment. The risk, or expected loss, of a probabilistic translation model on a corpus D, defined with respect to a particular loss function `eˆ(e), where eˆ is the reference translation and e is a hypothesis translation L= X X p(e|f )`eˆ(e) (3) hˆ e,f i∈D e This value can be trivially computed using equation (2). In this section, we are concerned with finding the parameters θ that minimise (3). Fortunately, with the log-linear parameterization of p(e|f ), L is differentiable with respect to"
W09-1114,N07-1062,0,0.101321,"of state-of-the-art systems which are able to learn from parallel corpora which were generated for other purposes — a direct result of employing a mathematical framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common"
W09-1114,D07-1055,0,0.0381723,"th the sampler this restriction is mitigated: any function of h(e, f, a) may participate in the translation model subject only to its own computability. Freed from the rusty manacles of dynamic programming, we anticipate development of many useful features. 6 To our knowledge, we are the first to apply Monte Carlo methods to maximum translation and minimum risk translation. Approaches to the former (Blunsom et al., 2008; May and Knight, 2006) rely on dynamic programming techniques which do not scale well without heuristic approximations, while approaches to the latter (Smith and Eisner, 2006; Zens et al., 2007) use biased k-best approximations. 7 Conclusion We have described a Gibbs sampler for approximating two intractable problems in SMT: maximum translation decoding (and its variant, minimum risk decoding) and minimum risk training. By using Monte Carlo techniques we avoid the biases associated with the more commonly used DP based maxderivation (or k-best derivation) approximation. In doing so we provide a further tool to the translation community that we envision will allow the development and analysis of increasing theoretically well motivated techniques. Acknowledgments Related work Our sample"
W09-1114,P08-1012,0,0.00643602,"framework that we can reason about independently of any particular model. For example, we can train SMT models using maximum likelihood estimation (Brown et al., 1993; Och and Ney, 2000; Marcu and Wong, 2002). Alternatively, we can train to minimise probabilistic conceptions of risk (expected loss) with respect to translation metrics, thereby obtaining better results for those metrics (Kumar and Byrne, 2004; Smith and 102 Eisner, 2006; Zens and Ney, 2007). We can also use Bayesian inference techniques to avoid resorting to heuristics that damage the probabilistic interpretation of the models (Zhang et al., 2008; DeNero et al., 2008; Blunsom et al., 2009). Most models define multiple derivations for each translation; the probability of a translation is thus the sum over all of its derivations. Unfortunately, finding the maximum probability translation is NPhard for all but the most trivial of models in this setting (Sima’an, 1996). It is thus necessary to resort to approximations for this sum and the search for its maximum e∗ . The most common of these approximations is the max-derivation approximation, which for many models can be computed in polynomial time via dynamic programming (DP). Though effe"
W12-1907,P11-1087,1,0.919797,"increasing sample variance with increasing sequence length (Kitagawa, 1996). Particle smoothing variants of SMC reduce the relative variance of marginals early in the sequence, as well improving the diversity of the final sample (Fearnhead et al., 2008). Particle Markov chain Monte Carlo (PMCMC) formally augments classic Markov chain Monte Carlo (MCMC) approaches, like Gibbs sampling, with samples generated by particle filters (Andrieu et al., 2010). 3 The PYP-HMM The PYP-HMM model of PoS generation demonstrates the tightly coupled correlations that complicate many standard inference methods (Blunsom and Cohn, 2011). The model applies a hierarchical Pitman-Yor process (PYP) prior to a trigram hidden Markov model (HMM) to jointly model the distribution of a sequence of latent word classes, t, and word tokens, w. This model performs well on corpora in multiple languages, but the lack of a closed form solution for the sample probabilities makes it a strong canditate for PG sampling. The joint proba48 bility defined by a trigram HMM is Pθ (t, w) = N +1 Y Pθ (tl |tn−1 , tn−2 )Pθ (wn |tn ) n=1 where N = |t |= |w |and the special tag $ is added to the boundaries on the sentence. The model defines transition and"
W12-1907,U11-1004,0,0.223078,"particle filters, are particularly well suited to estimating tightly coupled distributions (Andrieu et al., 2010). Particle filters sample sequences of latent variable assignments by concurrently generating several representative sequences consistent with a model’s conditional dependencies. The sequential nature of the sampling simplifies inference by ignoring ambiguous correlations with unsampled variables at the cost of sampling the sequence multiple times. The few applications of particle filters in computational linguistics generally focus on the online nature of SMC (Canini et al., 2009; Borschinger and Johnson, 2011). However, batch applications still benefit from the power of SMC to generate samples from tightly coupled distributions that would otherwise need to be approximated. Furthermore, the time cost of the additional samples generated by SMC can be mitigated by generating them in parallel. This report presents an initial approach to the integration of SMC and block sampling, sometimes reffered to as Particle Gibbs (PG) sampling (Andrieu et al., 2010). Unsupervised PoS induction serves as a motivating example for future extensions to other problems. Section 3 reviews the PYP-HMM model used for PoS i"
W12-1907,W06-2920,0,0.0250238,"ould have a higher variance than the target distribution. If the variance is too high, the sampler will be slower to converge. While additional particles lower the relative variance, they also increase the run time linearly. It is possible that there is a threshold of particles necessary to ensure that some are high likelihood sequences, beyond which inference gains are minimal the additional computational expense is wasted. All of the experiments in this section were run on the Arabic corpus from the CoNLL-X shared language task, which is small enough to quickly experiment with these issues (Buchholz and Marsi, 2006). The sentence based sampler, sent, samples from a distribution that can be exactly computed, facilitating comparisons between the exact sampler and the SMC approach. Figure 5.1 compares the posterior log-likelihoods of the sent sampler and the exact sentence sampler over 200 iterations. As expected, the likelihoods of the particle filters approach that of the exact sentence sampler as the number of particles increases from 25 to 100, which completely overlaps the performance of the exact sampler by the 50th iteration. This is impressive, because even with 99 additional sequences sampled (one"
W12-1907,N07-1030,0,0.0330214,"necessarily correspond to better Many-to-One accuracy. The results suggest that the approach has potential and more advanced particle filters are likely to lead to stronger performance. 1 Introduction Modern research is steadily revealing more of the subtle structure of natural language to create increasingly intricate models. Many modern problems in computational linguistics require or benefit from modeling the long range correlations between latent variables, e.g. part of speech (PoS) induction (Liang 47 et al., 2010), dependency parsing (Smith and Eisner, 2008), and coreference resolution (Denis and Baldridge, 2007). These correlations make inference difficult because they reflect the complicated effect variables have on each other in such tightly coupled models. Sequential Monte Carlo (SMC) methods, like particle filters, are particularly well suited to estimating tightly coupled distributions (Andrieu et al., 2010). Particle filters sample sequences of latent variable assignments by concurrently generating several representative sequences consistent with a model’s conditional dependencies. The sequential nature of the sampling simplifies inference by ignoring ambiguous correlations with unsampled varia"
W12-1907,D08-1036,0,0.121435,"pdate reserves one special particle, 0, for which the proposal distribution always chooses the previous iteration’s value at that site. 4.1 Sentence Sampling The sent particle filter samples blocks of tag assignments tS1:n for a sentence, S, composed of toS . Sampling an entire sentence minimizes kens, w1:n the risk of assigning a tag with a high probability given its local context but minimal probability given the entire sentence. Sentences can be sampled by ignoring table counts while sampling a proposal sentence, incorporating them after the fact with a Metropolis-Hastings acceptance test (Gao and Johnson, 2008). The Metropolis-Hastings step simplifies the sentence block particle filter further by not requiring the conditional SMC update. While there is already a tractable dynamic programming approach to sampling an entire sentence based on the Forward-Backward algorithm, particle filtering the sentences PYP-HMM model should prove beneficial. For the trigram HMM defined by the model, the forward-backward sampling approach has time complexity in O(N T 3 ) for a sentence of length N with T possible tag assignments at each site. Particle filters with P particles can approximate these samples in O(N T P"
W12-1907,N10-1082,0,0.149354,"lculate the correct probabilities (per equation (1)) by storing any counts that are different from the base counts, z−W , and defering to the base counts for any counts it does not have stored. At each token occurence n, the next tag assignment, tW,p for each particle p ∈ [1, P ] is chosen first n according to the word type proposal distribution Type Sampling The type sampling case for the PYP-HMM is more complicated than the sent sampler. The long-range couplings defined by the hierarchical PYP priors strongly influence the joint distribution of tags assigned to tokens of the same word type (Liang et al., 2010). Therefore, the affects of the seating decisions of new customers cannot be postponed during filtering as in sentence sampling. To account for this, the type particle filter samples sequences of seating arrangements and tag assignments jointly, W W xW 1:n = (t1:n , z1:n ), for the word-type, W . The final table counts are resampled once a tag assignment has been chosen from the particles. Tracking the seating arrangement history for each particle adds an additional complication to the type particle filter. The exchangeability of seating decisions means that only counts of customers are necess"
W12-1907,D08-1016,0,0.0270615,"ever, we found that high posterior likelihood do not necessarily correspond to better Many-to-One accuracy. The results suggest that the approach has potential and more advanced particle filters are likely to lead to stronger performance. 1 Introduction Modern research is steadily revealing more of the subtle structure of natural language to create increasingly intricate models. Many modern problems in computational linguistics require or benefit from modeling the long range correlations between latent variables, e.g. part of speech (PoS) induction (Liang 47 et al., 2010), dependency parsing (Smith and Eisner, 2008), and coreference resolution (Denis and Baldridge, 2007). These correlations make inference difficult because they reflect the complicated effect variables have on each other in such tightly coupled models. Sequential Monte Carlo (SMC) methods, like particle filters, are particularly well suited to estimating tightly coupled distributions (Andrieu et al., 2010). Particle filters sample sequences of latent variable assignments by concurrently generating several representative sequences consistent with a model’s conditional dependencies. The sequential nature of the sampling simplifies inference"
W12-1907,P06-1124,0,0.178651,"path of simpler PYP priors, Tij |aT , bT , Bi ∼ PYP(aT , bT , Bi ) Bi |aB , bB , U ∼ PYP(aB , bb , U ) U |aU , bU ∼ PYP(aU , bU , Uniform). Ei |aE , bE , C ∼ PYP(aE , bE , Ci ), where Tij , Bi , and U are trigram, bigram, and unigram transition distributions respectively and Ci is either a uniform distribution (PYP-HMM) or a bigram character language model emission distribution (PYP-HMM+LM, intended to model basic morphology). Draws from the posterior of the hierarchical PYP can be calculated with a variant of the Chinese Restaraunt Process (CRP) called the Chinese Restaurant Franchise (CRF) (Teh, 2006; Goldwater et al., 2006). In the CRP analogy, each latent variable in a sequence is represented by a customer entering a restaurant and sitting at one of an infinite number of tables. A customer chooses to sit at a table in a restaurant according to the probability ( − ck −a 1 ≤ k ≤ K− P (zn = k|z1:n−1 ) = n−1+b K − a+b k = K− + 1 n−1+b (1) where zn is the index of the table chosen by the nth customer to the restaurant, z1:n−1 is the seating arrangement of the previous n − 1 customers to enter, − c− k is the count of the customers at table k, and K is the total number of tables chosen by the"
W12-1909,afonso-etal-2002-floresta,0,0.0149427,"d, morphology, part-of-speech and dependency markup, and developed our own conversion into UPOS. Our testing and development sets were drawn from the first 15 Eve files which were manually annotated for dependency structure. The rest of the corpus, which had not been manually annotated for syntax, was merged to form the training set. Phrase-structure treebanks As well as dependency treebanks, we used three different phrasestructure treebanks: The Dutch Alpino treebank (Bouma et al., 2000), the English Penn Treebank V3 (Marcus et al., 1993),9 and the Portuguese Floresta Sint´a(c)tica treebank (Afonso et al., 2002). As these treebanks do not explicitly mark dependencies, we automatically extracted these using head finding heuristics. Thankfully the difficult work of creating such scripts has already been done as part of the CoNLL shared tasks. We have reused their scripts to create dependency representations of these treebanks, before converting into our file format and augmenting with UPOS annotation. In the case of Dutch, we have reused the same CoNLL 2006 data; note that this dataset includes predicted part-of-speech rather than gold standard annotation (Buchholz and Marsi, 2006). For the Portuguese,"
W12-1909,W12-1912,0,0.191162,"ously dominant DMV system. Two forms of light supervision were popular, the first being the inclusion of pre-specified constraints or rules for allowable dependency links, and the second being the tuning of model parameters or selecting between competing models on the labelled development data. Obviously the merits of such supervision would depend on the desired application for the induced parser. The direct comparison of models which include a form of universal prior syntactic information with those that don’t does permit interesting development linguistic questions to be explored in future. Bisk and Hockenmaier (2012) chose to induce a restricted form of Combinatory Categorial Grammar (CCG), the parses of which were then mapped to dependency structures. Restrictions on head-child dependencies were encoded in the allowable categories for each POS tag and the heads of sentences. Key features of their approach were a maximum likelihood objective function and an iterative procedure for generating composite categories from simple ones. Such composite categories allow the parameterisation of larger units than just head-child dependencies, improving over the more limited conditioning of DMV. ˇ Maraˇcek and Zabokr"
W12-1909,D10-1117,1,0.87001,"dency for languages to favour one attachment direction over another. The most frequently cited and extended model for dependency induction is DMV (Klein and Manning, 2004). We provide results for this model trained on each of the coarse (DMVc ), fine (DMVp ), and universal (DMVu ) POS tag sets, all initialised with the original harmonic initialiser. As a further baseline we also evaluated the dependency trees resulting from directly using the harmonic initialiser without any training (H). As a strong benchmark we include the results of the non-parametric Bayesian model previously published in Blunsom and Cohn (2010) (BC). The stated results are for the unlexicalised model described in that paper where the final analysis is formed by choosing the maximum marginal probability dependency links estimated from forty independent Gibbs sampler runs. For part-of-speech tagging we include results from an implementation of the Brown word clustering algorithm (Brown et al., 1992) (Bc,p,u ), and the mkcls tool written by Franz Och (Och, 1999) (MKc,p,u ). Both of these benchmarks were trained with the number of classes matching the number in the gold standard of each of the tagsets in turn: coarse (c), fine (p), and"
W12-1909,P11-1087,1,0.829147,"n et al., 1992) (Bc,p,u ), and the mkcls tool written by Franz Och (Och, 1999) (MKc,p,u ). Both of these benchmarks were trained with the number of classes matching the number in the gold standard of each of the tagsets in turn: coarse (c), fine (p), and universal (u). A notable property of both of these word class models is that they enforce a one-tag-per-type restriction that ensures there is a one-to-one mapping between word types and classes. For POS tagging we also provide benchmark results from two previously published models. The first of these is the Pitman-Yor HMM model described in (Blunsom and Cohn, 2011), which incorporates ta one-tag-per-type restriction (BC). This model was trained with the same number of tags as in the gold standard fine tag set for each corpus. The second benchmark is the HMM with Sparsity Constraints trained using Posterior Regularization (PR) described in (Grac¸a et al., 2011). In this model the HMM emission probabilitiy distribution are estimated using small Maximum Entropy models (features set described in the original paper). The models were trained for 200 iterations of PR using both the same number of hidden states as the coarse Gc and universal Gu gold standard. A"
W12-1909,J92-4003,0,0.0708554,"baseline we also evaluated the dependency trees resulting from directly using the harmonic initialiser without any training (H). As a strong benchmark we include the results of the non-parametric Bayesian model previously published in Blunsom and Cohn (2010) (BC). The stated results are for the unlexicalised model described in that paper where the final analysis is formed by choosing the maximum marginal probability dependency links estimated from forty independent Gibbs sampler runs. For part-of-speech tagging we include results from an implementation of the Brown word clustering algorithm (Brown et al., 1992) (Bc,p,u ), and the mkcls tool written by Franz Och (Och, 1999) (MKc,p,u ). Both of these benchmarks were trained with the number of classes matching the number in the gold standard of each of the tagsets in turn: coarse (c), fine (p), and universal (u). A notable property of both of these word class models is that they enforce a one-tag-per-type restriction that ensures there is a one-to-one mapping between word types and classes. For POS tagging we also provide benchmark results from two previously published models. The first of these is the Pitman-Yor HMM model described in (Blunsom and Coh"
W12-1909,D11-1059,0,0.0301117,"a two stage approach to inducing part-of-speech tags. The first stage used an LDA style probabilistic model to induce a distribution over possible tags for a given word type. These distributions were then hierarchically clustered and the final tags selected using the prefix of the path from the root node to the word type in the cluster tree. The length of the prefixes, and thus the number of tags, was tuned on the labelled development data. The system of Christodoulopoulos et al. (2012) was based upon an LDA type model which included both contexts and other conditionally independent features (Christodoulopoulos et al., 2011). This base system was then iterated with a DMV system and with the resultant dependencies being repeatedly fed back into the POS model as features. This submission is notable for being one of the first to attempt joint POS and dependency induction rather than taking a pipeline approach. 5.2 Dependency Induction The dependency parsing task saw a variety of approaches with only a couple based on the previously dominant DMV system. Two forms of light supervision were popular, the first being the inclusion of pre-specified constraints or rules for allowable dependency links, and the second being"
W12-1909,W12-1913,0,0.15553,"ll of these submissions made significant departures from the benchmark HMM and DMV approaches which have dominated the published literature on these tasks in recent years. The submissions were characterised by varied choices of model structure, parameterisation, regularisation, and the degree to which light supervision was provided through constraints or the use of labelled tuning data. In the following sections we summarise the approaches taken by the systems submitted for each task. 5.1 Part-of-Speech Induction The part-of-speech induction challenge received two submission, (Chrupała, 2012; Christodoulopoulos et al., 2012). Both of these submissions based their induction systems on LDA inspired models for clustering word types by the contexts in which they appear. Notably, the strongest of the provided benchmarks and the two submissions modelled part-of-speech tags at the type level, thus restricting all tokens of a given word type to share the same tag. Though 69 clearly out of step with the gold standard tagging, this one-tag-per-type restriction has previously been shown to be a crude but effective way of regularising models towards a good solution. Below we summarise the approach of each submission, identif"
W12-1909,W12-1914,0,0.0150223,"Encouragingly all of these submissions made significant departures from the benchmark HMM and DMV approaches which have dominated the published literature on these tasks in recent years. The submissions were characterised by varied choices of model structure, parameterisation, regularisation, and the degree to which light supervision was provided through constraints or the use of labelled tuning data. In the following sections we summarise the approaches taken by the systems submitted for each task. 5.1 Part-of-Speech Induction The part-of-speech induction challenge received two submission, (Chrupała, 2012; Christodoulopoulos et al., 2012). Both of these submissions based their induction systems on LDA inspired models for clustering word types by the contexts in which they appear. Notably, the strongest of the provided benchmarks and the two submissions modelled part-of-speech tags at the type level, thus restricting all tokens of a given word type to share the same tag. Though 69 clearly out of step with the gold standard tagging, this one-tag-per-type restriction has previously been shown to be a crude but effective way of regularising models towards a good solution. Below we summarise the ap"
W12-1909,erjavec-etal-2010-jos,0,0.0235483,"Missing"
W12-1909,W12-1915,0,\N,Missing
W12-1909,nivre-etal-2006-talbanken05,0,\N,Missing
W12-1909,J93-2004,0,\N,Missing
W12-1909,D10-1120,0,\N,Missing
W12-1909,W07-0604,0,\N,Missing
W12-1909,W06-2920,0,\N,Missing
W12-1909,N06-1041,0,\N,Missing
W12-1909,E99-1010,0,\N,Missing
W12-1909,P11-1067,0,\N,Missing
W12-1909,D10-1056,0,\N,Missing
W12-1909,D11-1036,0,\N,Missing
W12-1909,W07-2416,0,\N,Missing
W12-1909,P04-1061,0,\N,Missing
W12-1909,P07-1031,0,\N,Missing
W12-1909,W12-1910,0,\N,Missing
W12-1909,D07-1096,0,\N,Missing
W12-1909,D07-1031,0,\N,Missing
W12-1909,D07-1043,0,\N,Missing
W13-3209,D08-1094,0,0.0602453,"Missing"
W13-3209,W11-0112,0,0.0403182,"esent a short analysis of the limitation of these matrixvector models with regard to the task of modelling non-boolean logical operations, and present an improved model bypassing these limitations in Section 6. Finally, in Section 7, we conclude by suggesting future work which will extend and build upon the theoretical foundations presented in this paper. 2 Vector-assisted logic The first class of approaches seeks to use distributional models of word semantics to enhance logic-based models of textual inference. The work which best exemplifies this strand of research is found in the efforts of Garrette et al. (2011) and, more recently, Beltagy et al. (2013). This line of research converts logical representations obtained from syntactic parses using Bos’ Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pad´o (2008) to deal with issues polysemy and ambiguity. As this class of approaches deals with improving logic-based models rather than giving a distributional account of logical function words, we view such models as orthogonal to the effort presented in this paper. Logic with vectors The second class of ap"
W13-3209,D11-1129,1,0.86921,"nsional vector spaces, have been applied to tasks such as thesaurus extraction (Grefenstette, 1994; Curran, 2004), word-sense discrimination (Sch¨utze, 1998), automated essay marking (Landauer and Dumais, 1997), and so on. During the past few years, research has shifted from using distributional methods for modelling the semantics of words to using them for modelling the semantics of larger linguistic units such as phrases or entire sentences. This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), and semantic relation classification (ibid.). Most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations (Mitchell and Lapata, 2008; Mitchell 74 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 74–82, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics vide a brief overview of related work in the other two in order to better situate the work this paper w"
W13-3209,W13-0112,0,0.0580332,"Missing"
W13-3209,P13-1088,1,0.379872,"urran, 2004), word-sense discrimination (Sch¨utze, 1998), automated essay marking (Landauer and Dumais, 1997), and so on. During the past few years, research has shifted from using distributional methods for modelling the semantics of words to using them for modelling the semantics of larger linguistic units such as phrases or entire sentences. This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), and semantic relation classification (ibid.). Most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations (Mitchell and Lapata, 2008; Mitchell 74 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 74–82, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics vide a brief overview of related work in the other two in order to better situate the work this paper will describe in the literature. would best be represented in a continuous vector space model re"
W13-3209,D10-1115,0,0.308121,"Missing"
W13-3209,C12-2054,0,0.0444666,"Missing"
W13-3209,S13-1002,0,0.0146147,"these matrixvector models with regard to the task of modelling non-boolean logical operations, and present an improved model bypassing these limitations in Section 6. Finally, in Section 7, we conclude by suggesting future work which will extend and build upon the theoretical foundations presented in this paper. 2 Vector-assisted logic The first class of approaches seeks to use distributional models of word semantics to enhance logic-based models of textual inference. The work which best exemplifies this strand of research is found in the efforts of Garrette et al. (2011) and, more recently, Beltagy et al. (2013). This line of research converts logical representations obtained from syntactic parses using Bos’ Boxer (Bos, 2008) into Markov Logic Networks (Richardson and Domingos, 2006), and uses distributional semantics-based models such as that of Erk and Pad´o (2008) to deal with issues polysemy and ambiguity. As this class of approaches deals with improving logic-based models rather than giving a distributional account of logical function words, we view such models as orthogonal to the effort presented in this paper. Logic with vectors The second class of approaches seeks to integrate boolean-like l"
W13-3209,P08-1028,0,0.113509,"matically instantiated as sets of vectors in high dimensional vector spaces, have been applied to tasks such as thesaurus extraction (Grefenstette, 1994; Curran, 2004), word-sense discrimination (Sch¨utze, 1998), automated essay marking (Landauer and Dumais, 1997), and so on. During the past few years, research has shifted from using distributional methods for modelling the semantics of words to using them for modelling the semantics of larger linguistic units such as phrases or entire sentences. This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), and semantic relation classification (ibid.). Most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations (Mitchell and Lapata, 2008; Mitchell 74 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 74–82, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics vide a brief overview of related work in t"
W13-3209,S12-1035,0,0.0210555,"-zero Mnot . Clearly, any other complex semantic representation would suffer from the same issue here—the failure of double-negation to revert a representation to its (diminutive) original. 5 not Analysis The issue identified with the MV-RNN style models described above extends to a number of other models of vector spaced compositionality. It can be viewed as a problem of uninformed composition caused by a composition function that fails to account for syntax and thus for scope. Of course, identifying the scope of negation is a hard problem in its own right—see e.g. the *SEM 2012 shared task (Morante and Blanco, 2012). However, at least for simple cases, we can deduce scope by considering the parse tree of a sentence: 6 An improved model As we outlined in this paper, a key requirement for a compositional model motivated by formal semantics is the ability to propagate functional representations, but also to not propagate these representations when doing so is not semantically appropriate. Here, we propose a modification of the MV-RNN class of models that can capture this dis79 tinction without the need to move the composition logic into the non-linearity. We add a parameter α to the representation of each w"
W13-3209,J98-1004,0,0.229183,"Missing"
W13-3209,D12-1110,0,0.155154,"Grefenstette, 1994; Curran, 2004), word-sense discrimination (Sch¨utze, 1998), automated essay marking (Landauer and Dumais, 1997), and so on. During the past few years, research has shifted from using distributional methods for modelling the semantics of words to using them for modelling the semantics of larger linguistic units such as phrases or entire sentences. This move from word to sentence has yielded models applied to tasks such as paraphrase detection (Mitchell and Lapata, 2008; Mitchell and Lapata, 2010; Grefenstette and Sadrzadeh, 2011; Blacoe and Lapata, 2012), sentiment analysis (Socher et al., 2012; Hermann and Blunsom, 2013), and semantic relation classification (ibid.). Most efforts approach the problem of modelling phrase meaning through vector composition using linear algebraic vector operations (Mitchell and Lapata, 2008; Mitchell 74 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 74–82, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics vide a brief overview of related work in the other two in order to better situate the work this paper will describe in the literature. would best be represented in a cont"
W13-3209,C10-1142,0,0.0500445,"Missing"
W13-3209,W08-2222,0,\N,Missing
W13-3209,D12-1050,0,\N,Missing
W13-3214,D10-1115,0,0.0185939,"Missing"
W13-3214,D12-1050,0,0.0158222,"Missing"
W13-3214,W11-0114,0,0.0255128,"Missing"
W13-3214,P13-1088,1,0.672742,"Missing"
W13-3214,D12-1110,0,0.359315,"Missing"
W13-3214,J00-3003,0,0.899208,"Missing"
W13-3214,P12-2018,0,0.0275673,"Missing"
W13-3214,W12-1812,0,0.0262853,"Missing"
W13-3519,J98-4004,0,0.340545,"nal Bayesian inference algorithm for PCFGs that has the advantages of two dominant Bayesian training algorithms for PCFGs, namely variational Bayesian inference and Markov chain Monte Carlo. In three kinds of experiments, we illustrate that our algorithm achieves close performance to the Hastings sampling algorithm while using an order of magnitude less training time; and outperforms the standard variational Bayesian inference and the EM algorithms with similar training time. 1 Introduction Probabilistic context-free grammars (PCFGs) are commonly used in parsing and grammar induction systems (Johnson, 1998; Collins, 1999; Klein and Manning, 2003; Matsuzaki et al., 2005). The traditional method for estimating the parameters of PCFGs from terminal strings is the inside-outside (IO) algorithm (Baker, 1979). As a special instance of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977), based on the principle of maximum-likelihood estimation (MLE), the standard IO algorithm learns relatively uniform probability distributions for grammars, while the true distributions can be highly skewed (Johnson et al., 2007). In order to encourage sparse grammars and avoid overfitting, recent resear"
W13-3519,P03-1054,0,0.36875,"hm for PCFGs that has the advantages of two dominant Bayesian training algorithms for PCFGs, namely variational Bayesian inference and Markov chain Monte Carlo. In three kinds of experiments, we illustrate that our algorithm achieves close performance to the Hastings sampling algorithm while using an order of magnitude less training time; and outperforms the standard variational Bayesian inference and the EM algorithms with similar training time. 1 Introduction Probabilistic context-free grammars (PCFGs) are commonly used in parsing and grammar induction systems (Johnson, 1998; Collins, 1999; Klein and Manning, 2003; Matsuzaki et al., 2005). The traditional method for estimating the parameters of PCFGs from terminal strings is the inside-outside (IO) algorithm (Baker, 1979). As a special instance of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977), based on the principle of maximum-likelihood estimation (MLE), the standard IO algorithm learns relatively uniform probability distributions for grammars, while the true distributions can be highly skewed (Johnson et al., 2007). In order to encourage sparse grammars and avoid overfitting, recent research for training PCFGs has drifted away f"
W13-3519,D10-1117,1,0.890423,"rm A[a] → B[b]C[c], where a, b, c ∈ [1, H] are the hidden subtypes, and H denotes the number of subtypes for each non-terminal. Dependency model with valence As a second empirical validation of our CVB inference algorithm, we apply it to unsupervised grammar induction with the popular Dependency Model with Valence (DMV) (Klein and Manning, 2004). Although the original maximum likelihood formulation of this model has long since been surpassed by more advanced models, all of the stateof-the-art approaches to unsupervised dependency parsing still have DMV at their core (Headden III et al., 2009; Blunsom and Cohn, 2010; Spitkovsky et al., 2012). As such we believe demonstrating 2 Annealing is not used in order to facilitate the perplexity calculation in the test set. 179 0.49 0.9 EM 0.85 VB 0.48 F1 Scores F1 Scores CVB 0.8 0.75 0.47 0.7 EM 0.46 VB 0.65 CVB 0.45 1 1.0 0.1 Bayesian Priors Precision 75.84 76.98 78.85 Recall 72.92 73.32 76.98 F1 74.35 75.11 77.90 we can ignore the local dependencies induced by collapsing the parameters. The assumptions in our CVB algorithm are reasonable for a range of parsing applications and justified in three tasks by the empirical observations: it produces more accurate res"
W13-3519,P04-1061,0,0.460154,"ato, 2006) for PCFGs extends EM and places no constraints when updating parameters in the M step. By minimising the divergence between the 173 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 173–182, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics 2.2 An alternative approach is also included in brief. In Section 4, we validate our CVB algorithm in three simple experiments. They are inferring a sparse grammar that describes the morphology of the Sotho language (Johnson et al., 2007), unsupervised dependency parsing (Klein and Manning, 2004) and supervised parsing with latent annotations (Matsuzaki et al., 2005). Section 5 concludes with future work. 2 The standard inside-outside algorithm for PCFGs belongs to the general EM class, which is further a subclass of VB (Beal, 2003). VB maximises the negative free energy −F(Q(t, θ)), a lower bound of the log marginal likelihood of the observation log P (w|α). This is equivalent to minimising the Kullback-Leibler divergence. log P (w|α) ≥ −F(Q(t, θ)) Approximate inference for PCFGs 2.1 =EQ(t,θ) [log P (w, t, θ|α)] − EQ(t,θ) [log Q(t, θ)] Definitions Q(t, θ) is an approximate posterior,"
W13-3519,N09-1009,0,0.0508614,"1.0 0.1 Bayesian Priors Precision 75.84 76.98 78.85 Recall 72.92 73.32 76.98 F1 74.35 75.11 77.90 we can ignore the local dependencies induced by collapsing the parameters. The assumptions in our CVB algorithm are reasonable for a range of parsing applications and justified in three tasks by the empirical observations: it produces more accurate results than standard VB, and close results to sampling with significantly less training time. While not state-of-the-art, the models we have demonstrated our CVB algorithm on underlie a number of high performance grammar induction and parsing systems (Cohen and Smith, 2009; Blunsom and Cohn, 2010; Petrov and Klein, 2007; Liang et al., 2007). Therefore, our work naturally extends to employing our CVB algorithm in more advanced models such as hierarchical splitting and merging system used in Berkeley parser (Petrov and Klein, 2007), and generalising our CVB algorithm to the non-parametric models such as tree substitution grammars (Blunsom and Cohn, 2010) and infinite PCFGs (Liang et al., 2007). We have also sketched an alternative CVB algorithm which makes a harsher independence assumption for the latent variables but then requires no approximation of the variati"
W13-3519,N09-1069,0,0.0322511,"tation can be easily achieved by modifying code of the EM algorithm. We keep track of the expected counts at global level, subtract the local mean counts for ti before update, run the inside-outside recursion using θCVB , and finally add the updated distribution back into the global counts. Therefore, we only need to replace the parameters with the expected counts, and make update after each sentence; the core of the insideoutside implementation remains the same. Our CVB algorithm bears some similarities to the online EM algorithm with maximum a posterior (MAP) updates (Neal and Hinton, 1998; Liang and Klein, 2009), but they differ in several ways. The online EM algorithm updates each tree ti based on the statistics of all the trees, optimising the same objective function p(w|θ) as the batch EM algorithm. MAP estimation searches for the optimal posterior p(w|θ)p(θ). On the other hand, our CVB algorithm optimises the data likelihood p(w). The smoothing effects for the MAP estimation (αA→β − 1) prevent the use of sparse priors, whereas the CVB algorithm (αA→β ) overcomes such difficulty by parameter integration. 3.3 Figure 4: A fragment of a tree structure are reasonable and weak, we expect its results to"
W13-3519,D07-1072,0,0.0981034,"76.98 F1 74.35 75.11 77.90 we can ignore the local dependencies induced by collapsing the parameters. The assumptions in our CVB algorithm are reasonable for a range of parsing applications and justified in three tasks by the empirical observations: it produces more accurate results than standard VB, and close results to sampling with significantly less training time. While not state-of-the-art, the models we have demonstrated our CVB algorithm on underlie a number of high performance grammar induction and parsing systems (Cohen and Smith, 2009; Blunsom and Cohn, 2010; Petrov and Klein, 2007; Liang et al., 2007). Therefore, our work naturally extends to employing our CVB algorithm in more advanced models such as hierarchical splitting and merging system used in Berkeley parser (Petrov and Klein, 2007), and generalising our CVB algorithm to the non-parametric models such as tree substitution grammars (Blunsom and Cohn, 2010) and infinite PCFGs (Liang et al., 2007). We have also sketched an alternative CVB algorithm which makes a harsher independence assumption for the latent variables but then requires no approximation of the variational posterior by performing inference individually for each parse no"
W13-3519,J93-2004,0,0.043908,"Missing"
W13-3519,P05-1010,0,0.616027,"advantages of two dominant Bayesian training algorithms for PCFGs, namely variational Bayesian inference and Markov chain Monte Carlo. In three kinds of experiments, we illustrate that our algorithm achieves close performance to the Hastings sampling algorithm while using an order of magnitude less training time; and outperforms the standard variational Bayesian inference and the EM algorithms with similar training time. 1 Introduction Probabilistic context-free grammars (PCFGs) are commonly used in parsing and grammar induction systems (Johnson, 1998; Collins, 1999; Klein and Manning, 2003; Matsuzaki et al., 2005). The traditional method for estimating the parameters of PCFGs from terminal strings is the inside-outside (IO) algorithm (Baker, 1979). As a special instance of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977), based on the principle of maximum-likelihood estimation (MLE), the standard IO algorithm learns relatively uniform probability distributions for grammars, while the true distributions can be highly skewed (Johnson et al., 2007). In order to encourage sparse grammars and avoid overfitting, recent research for training PCFGs has drifted away from MLE in favor of Bayes"
W13-3519,N09-1012,0,0.131237,"Missing"
W13-3519,N07-1051,0,0.070019,"8.85 Recall 72.92 73.32 76.98 F1 74.35 75.11 77.90 we can ignore the local dependencies induced by collapsing the parameters. The assumptions in our CVB algorithm are reasonable for a range of parsing applications and justified in three tasks by the empirical observations: it produces more accurate results than standard VB, and close results to sampling with significantly less training time. While not state-of-the-art, the models we have demonstrated our CVB algorithm on underlie a number of high performance grammar induction and parsing systems (Cohen and Smith, 2009; Blunsom and Cohn, 2010; Petrov and Klein, 2007; Liang et al., 2007). Therefore, our work naturally extends to employing our CVB algorithm in more advanced models such as hierarchical splitting and merging system used in Berkeley parser (Petrov and Klein, 2007), and generalising our CVB algorithm to the non-parametric models such as tree substitution grammars (Blunsom and Cohn, 2010) and infinite PCFGs (Liang et al., 2007). We have also sketched an alternative CVB algorithm which makes a harsher independence assumption for the latent variables but then requires no approximation of the variational posterior by performing inference individua"
W13-3519,N07-1018,0,0.573563,"t-free grammars (PCFGs) are commonly used in parsing and grammar induction systems (Johnson, 1998; Collins, 1999; Klein and Manning, 2003; Matsuzaki et al., 2005). The traditional method for estimating the parameters of PCFGs from terminal strings is the inside-outside (IO) algorithm (Baker, 1979). As a special instance of the Expectation-Maximization (EM) algorithm (Dempster et al., 1977), based on the principle of maximum-likelihood estimation (MLE), the standard IO algorithm learns relatively uniform probability distributions for grammars, while the true distributions can be highly skewed (Johnson et al., 2007). In order to encourage sparse grammars and avoid overfitting, recent research for training PCFGs has drifted away from MLE in favor of Bayesian inference algorithms that make either deterministic or stochastic approximations (Kurihara and Sato, 2006; Johnson et al., 2006; Johnson et al., 2007). Variational Bayesian inference (VB) (Kurihara and Sato, 2006) for PCFGs extends EM and places no constraints when updating parameters in the M step. By minimising the divergence between the 173 Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 173–182, c Sofia,"
W13-3519,P06-1055,0,0.241722,"Missing"
W13-3519,D12-1063,0,0.0141407,"e a, b, c ∈ [1, H] are the hidden subtypes, and H denotes the number of subtypes for each non-terminal. Dependency model with valence As a second empirical validation of our CVB inference algorithm, we apply it to unsupervised grammar induction with the popular Dependency Model with Valence (DMV) (Klein and Manning, 2004). Although the original maximum likelihood formulation of this model has long since been surpassed by more advanced models, all of the stateof-the-art approaches to unsupervised dependency parsing still have DMV at their core (Headden III et al., 2009; Blunsom and Cohn, 2010; Spitkovsky et al., 2012). As such we believe demonstrating 2 Annealing is not used in order to facilitate the perplexity calculation in the test set. 179 0.49 0.9 EM 0.85 VB 0.48 F1 Scores F1 Scores CVB 0.8 0.75 0.47 0.7 EM 0.46 VB 0.65 CVB 0.45 1 1.0 0.1 Bayesian Priors Precision 75.84 76.98 78.85 Recall 72.92 73.32 76.98 F1 74.35 75.11 77.90 we can ignore the local dependencies induced by collapsing the parameters. The assumptions in our CVB algorithm are reasonable for a range of parsing applications and justified in three tasks by the empirical observations: it produces more accurate results than standard VB, and"
W13-3519,J03-4003,0,\N,Missing
W14-2405,Q13-1005,0,0.00990853,"need for parsing, which makes it especially suitable to grammatically malformed or syntactically atypical text, such as tweets, as well as permitting the development of semantic parsers for resourcepoor languages. 1 Introduction The ubiquity of always-online computers in the form of smartphones, tablets, and notebooks has boosted the demand for effective question answering systems. This is exemplified by the growing popularity of products like Apple’s Siri or Google’s Google Now services. In turn, this creates the need for increasingly sophisticated methods for semantic parsing. Recent work (Artzi and Zettlemoyer, 2013; Kwiatkowski et al., 2013; Matuszek et al., 2012; Liang et al., 2011, inter alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related 2 Background Semantic parsing describes a task within the larger"
W14-2405,P13-1042,0,0.0608185,"Missing"
W14-2405,P13-1088,1,0.5604,"Missing"
W14-2405,P14-1136,1,0.388414,"Missing"
W14-2405,D13-1176,1,0.0512398,"parallel corpora of sentences, it is important to note that the model is agnostic concerning the inputs of functions g and h. In this paper we will discuss how this model can be applied to non-sentential inputs. 23 3.2 Conditional Neural Language Models Neural language models (Bengio et al., 2006) provide a distributed alternative to n-gram language models, permitting the joint learning of a prediction function for the next word in a sequence given the distributed representations of a subset of the last n−1 words alongside the representations themselves. Recent work in dialogue act labelling (Kalchbrenner and Blunsom, 2013b) and in machine translation (Kalchbrenner and Blunsom, 2013a) has demonstrated that a particular kind of neural language model based on recurrent neural networks (Mikolov et al., 2010; Sutskever et al., 2011) could be extended so that the next word in a sequence is jointly generated by the word history and the distributed representation for a conditioning element, such as the dialogue class of a previous sentence, or the vector representation of a source sentence. In this section, we briefly describe a general formulation of conditional neural language models, based on the log-bilinear model"
W14-2405,D13-1161,0,0.106969,"s it especially suitable to grammatically malformed or syntactically atypical text, such as tweets, as well as permitting the development of semantic parsers for resourcepoor languages. 1 Introduction The ubiquity of always-online computers in the form of smartphones, tablets, and notebooks has boosted the demand for effective question answering systems. This is exemplified by the growing popularity of products like Apple’s Siri or Google’s Google Now services. In turn, this creates the need for increasingly sophisticated methods for semantic parsing. Recent work (Artzi and Zettlemoyer, 2013; Kwiatkowski et al., 2013; Matuszek et al., 2012; Liang et al., 2011, inter alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related 2 Background Semantic parsing describes a task within the larger field of natural language"
W14-2405,P11-1060,0,0.0168638,"ed or syntactically atypical text, such as tweets, as well as permitting the development of semantic parsers for resourcepoor languages. 1 Introduction The ubiquity of always-online computers in the form of smartphones, tablets, and notebooks has boosted the demand for effective question answering systems. This is exemplified by the growing popularity of products like Apple’s Siri or Google’s Google Now services. In turn, this creates the need for increasingly sophisticated methods for semantic parsing. Recent work (Artzi and Zettlemoyer, 2013; Kwiatkowski et al., 2013; Matuszek et al., 2012; Liang et al., 2011, inter alia) has answered this call by progressively moving away from strictly rule-based semantic parsing, towards the use of distributed representations in conjunction with traditional grammatically-motivated re-write rules. This paper seeks to extend this line of thinking to its logical conclusion, by providing the first (to our knowledge) entirely distributed neural semantic generative parsing model. It does so by adapting deep learning methods from related 2 Background Semantic parsing describes a task within the larger field of natural language understanding. Within computational lingui"
W14-2405,N13-1008,0,0.0246657,"Missing"
W14-2405,D12-1110,0,0.0273292,"Missing"
W14-2405,P10-1040,0,0.0186559,"nctions g and h may differ from those discussed in Hermann and Blunsom (2014a), the basic noise-contrastive optimisation function remains the same. It is possible to initialise the model fully randomly, in which 25 References case the model parameters θ learned at this stage include the two distributed representation lexica for questions and queries, DQ and DR respectively, as well as all parameters for g and h. Alternatively, word embeddings in DQ could be initialised with representations learned separately, for instance with a neural language model or a similar system (Mikolov et al., 2010; Turian et al., 2010; Collobert et al., 2011, inter alia). Likewise, the relation and object embeddings in DR could be initialised with representations learned from distributed relation extraction schemas such as that of Riedel et al. (2013). Having learned representations for queries in DR as well as function g, the second training phase of the model uses a new parallel corpus consisting of pairs hg(Q), Ri ∈ C 0 to train the CNLM as presented in §3.3. The two training steps can be applied iteratively, and further, it is trivial to modify the learning procedure to use composition function h as another input for t"
W14-2405,W11-0329,0,0.0415463,"Missing"
W14-2405,W13-3214,1,\N,Missing
W14-2405,P14-1006,1,\N,Missing
W15-2108,2003.mtsummit-papers.6,0,0.117655,"Missing"
W15-2108,P01-1017,0,0.104389,"eloping on 21 − 22 and testing on 23 − 24. Punctua4 Available at http://www.statmt.org/wmt14/translationtask.html. 64 Model HPYP 5-gram Chelba and Jelinek (2000) Emami and Jelinek (2005) HPYP-DP HPYP 5-gram HPYP-DP Perplexity 147.22 146.1 131.3 145.54 178.13 163.96 rich contexts (they are usually restricted to firstorder dependencies and valency). Although any generative parsing model can be applied to language modelling by marginalising out the possible parses of a sentence, in practice the success of such models has been limited. Lexicalised PCFGs applied to language modelling (Roark, 2001; Charniak, 2001) show improvements over n-gram models, but decoding is prohibitively expensive for practical integration in language generation applications. Table 6: Language modelling test results. Above, training and testing on WSJ. Below, training semisupervised and testing on WMT. 5.5 Generation Chelba and Jelinek (2000) as well as Emami and Jelinek (2005) proposed incremental syntactic language models with some similarities to our model. Those models predict binarized constituency trees with a transition-based model, and are parameterized by deleted interpolation and neural networks, respectively. Rastr"
W15-2108,D14-1082,0,0.0771975,"parse trees. The UAS of the parser is on par with that of a greedy discriminative baseline. As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation. 1 Introduction Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014). Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time. Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower. In this paper we aim to transfer the advantages of transition-based parsing to generative dependency parsing. While generative models have been used widely and successfully for constituency 58 Proceedings of the Third International Conference on Dependency"
W15-2108,P13-1104,0,0.159728,"s better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation. 1 Introduction Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014). Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time. Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower. In this paper we aim to transfer the advantages of transition-based parsing to generative dependency parsing. While generative models have been used widely and successfully for constituency 58 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 58–67, Uppsala, Sweden, August 24–26 2015. The types of transitions in this model are sh"
W15-2108,D11-1114,0,0.172179,"Missing"
W15-2108,P97-1003,0,0.611823,"Missing"
W15-2108,W08-1301,0,0.032278,"Missing"
W15-2108,D10-1117,1,0.849554,"generated siblings. To counter sparsity in the conditioning context of the distributions, backoff and smoothing are performed. Wallach et al. (2008) proposed a Bayesian HPYP parameterisation of this model. Other generative models for dependency trees have been proposed mostly in the context of unsupervised parsing. The first successful model was the dependency model with valence (DMV) (Klein and Manning, 2004). Several extensions have been proposed for this model, for example using structural annaeling (Smith and Eisner, 2006), Viterbi EM training (Spitkovsky et al., 2010) or richer contexts (Blunsom and Cohn, 2010). However, these models are not powerful enough for either accurate parsing or language modelling with A generative transition-based parsing model for non-projective parsing is proposed in (Cohen et al., 2011), along with a dynamic program for inference. The parser is similar to ours, but the dynamic program restricts the conditioning context to the top 2 or 3 words on the stack. No experimental results are included. Le and Zuidema (2014) proposed a recursive neural network generative model over dependency trees. However, their model can only score trees, not perform parsing, and its perplexit"
W15-2108,P09-2085,1,0.825597,"er has been reached. We shall also consider unlexicalised models, based only on POS tags. 3.1 where the base distribution of a PYP corresponds to another restaurant. So when a customer sits at a new table, the dish is chosen by letting a new customer enter the base distribution restaurant. All dishes can be traced back to a uniform base distribution at the top of the hierarchy. Inference over seating arrangements in the model is performed with Gibbs sampling, based on routines to add or remove a customer from a restaurant. In our implementation we use the efficient data structures proposed by Blunsom et al. (2009). In addition to sampling the seating arrangement, the discount and strength parameters are also sampled, using slice sampling. In our model Tht , Whw and Aha are HPYPs for the tag, word and transition distributions, respectively. The PYPs for the transition prediction distribution, with conditioning context sequence ha1:L , are defined hierarchically as Hierarchical Pitman-Yor processes The probability distributions for predicting words, tags and transitions are drawn from hierarchical Pitmar-Yor Process (HPYP) priors. HPYP models were originally proposed for n-gram language modelling (Teh, 2"
W15-2108,D12-1133,0,0.0295412,"y node a, lc1 (a) refers to the leftmost child of a in A, and rc1 (a) to its rightmost child. The initial configuration is ([], 0, ∅). A terminal configuration is reached when β &gt; |s|, and σ consists only of the root. A sentence is generated leftto-right by performing a sequence of transitions. As a generative model it assigns probabilities to sentences and dependency trees: A word w (including its POS tag) is generated when it is shifted on to the stack, similar to the generative models proposed by Titov and Henderson (2007) and Cohen et al. (2011), and the joint tagging and parsing model of Bohnet and Nivre (2012). 3 Probabilistic Generative Model Our model defines a joint probability distribution over a parsed sentence with POS tags t1:n , words w1:n and a transition sequence a1:2n as p(t1:n , w1:n , a1:2n ) mi+1 n   Y Y a = p(ti |htmi )p(wi |ti , hw ) p(a |h ) , j j mi i=1 59 j=mi +1 where mi is the number of transitions that have been performed when (ti , wi ) is generated and ht , hw and ha are sequences representing the conditioning contexts for the tag, word and transition distributions, respectively. In the generative process a shift transition is followed by a sequence of 0 or more reduce tra"
W15-2108,P15-1033,0,0.0989699,"Missing"
W15-2108,C96-1058,0,0.57534,"ining the local coherence of n-gram models. The dependency model was also able to generate balanced quotation marks. 6 Titov and Henderson (2007) introduced a generative latent variable model for transition-based parsing. The model is based on an incremental sigmoid belief networks, using the arc-eager parsing strategy. Exact inference is intractable, so neural networks and variational mean field methods are proposed to perform approximate inference. However, this is much slower and therefore less scalable than our model. Related work One of the earliest graph-based dependency parsing models (Eisner, 1996) is generative, estimating the probability of dependents given their head and previously generated siblings. To counter sparsity in the conditioning context of the distributions, backoff and smoothing are performed. Wallach et al. (2008) proposed a Bayesian HPYP parameterisation of this model. Other generative models for dependency trees have been proposed mostly in the context of unsupervised parsing. The first successful model was the dependency model with valence (DMV) (Klein and Manning, 2004). Several extensions have been proposed for this model, for example using structural annaeling (Sm"
W15-2108,J08-4003,0,0.0418519,"ile jointly predicting POS tags and parse trees. The UAS of the parser is on par with that of a greedy discriminative baseline. As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation. 1 Introduction Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014). Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time. Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower. In this paper we aim to transfer the advantages of transition-based parsing to generative dependency parsing. While generative models have been used widely and successfully for constituency 58 Proceedings of"
W15-2108,P06-1055,0,0.034644,"Missing"
W15-2108,Q13-1033,0,0.012419,"To illustrate the generative process, the configuration of a partially generated dependency tree is given in Figure 1. In general parses may have multiple derivations. In transition-based parsing it is common to define an oracle o(c, G) that maps the current configuration c and the gold parse G to the next transition that should be performed. In our probabilistic model we are interested in performing inference over all latent structure, including spurious derivations. Therefore we propose a non-deterministic oracle which allows us to find all derivations of G. In contrast to dynamic oracles (Goldberg and Nivre, 2013), we are only interested in derivations of the correct parse tree, so the oracle can assume that given c there exists a derivation for G. First, to enforce the bottom-up property our oracle has to ensure that an arc (i, j) in G may only be added once j has been attached to all its children – we refer to these arcs as valid. Most deterministic oracles add valid arcs greedily. Second, we note that if there exists a valid arc between σ2 and σ1 and the oracle decides to shift, the same pair will only occur on the top of the stack again after a right dependent has been attached to σ1 . Therefore ri"
W15-2108,P10-1110,0,0.0921839,"nguage model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation. 1 Introduction Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014). Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time. Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower. In this paper we aim to transfer the advantages of transition-based parsing to generative dependency parsing. While generative models have been used widely and successfully for constituency 58 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 58–67, Uppsala, Sweden, August 24–26 2015. The types of transi"
W15-2108,J01-2004,0,0.65092,"00 − 20, developing on 21 − 22 and testing on 23 − 24. Punctua4 Available at http://www.statmt.org/wmt14/translationtask.html. 64 Model HPYP 5-gram Chelba and Jelinek (2000) Emami and Jelinek (2005) HPYP-DP HPYP 5-gram HPYP-DP Perplexity 147.22 146.1 131.3 145.54 178.13 163.96 rich contexts (they are usually restricted to firstorder dependencies and valency). Although any generative parsing model can be applied to language modelling by marginalising out the possible parses of a sentence, in practice the success of such models has been limited. Lexicalised PCFGs applied to language modelling (Roark, 2001; Charniak, 2001) show improvements over n-gram models, but decoding is prohibitively expensive for practical integration in language generation applications. Table 6: Language modelling test results. Above, training and testing on WSJ. Below, training semisupervised and testing on WMT. 5.5 Generation Chelba and Jelinek (2000) as well as Emami and Jelinek (2005) proposed incremental syntactic language models with some similarities to our model. Those models predict binarized constituency trees with a transition-based model, and are parameterized by deleted interpolation and neural networks, re"
W15-2108,P03-1054,0,0.0345736,"developing on section 22, and testing on section 23. We use the head-finding rules of Yamada and Matsumoto (2003) (YM)1 for constituencyto-dependency conversion, to enable comparison with previous results. We also evaluate on the Stanford dependency representation (De Marneffe and Manning, 2008) (SD)2 . Words that occur only once in the training data are treated as unknown words. We classify unknown words according to capitalization, numbers, punctuation and common suffixes into classes similar to those used in the implementation of generative constituency parsers such as the Stanford parser (Klein and Manning, 2003). As a discriminative baseline we use MaltParser (Nivre et al., 2006), a discriminative, greedy transition-based parser, performing arcstandard parsing with LibLinear as classifier. Although the accuracy of this model is not state-ofthe-art, it does enable us to compare our model against an optimised discriminative model with a feature-set based on the same elements as we include in our conditioning contexts. Our HPYP dependency parser (HPYP-DP) is trained with 20 iterations of Gibbs sampling, resampling the hyper-parameters after every iteration, except when performing inference over latent s"
W15-2108,P06-1072,0,0.0239437,"6) is generative, estimating the probability of dependents given their head and previously generated siblings. To counter sparsity in the conditioning context of the distributions, backoff and smoothing are performed. Wallach et al. (2008) proposed a Bayesian HPYP parameterisation of this model. Other generative models for dependency trees have been proposed mostly in the context of unsupervised parsing. The first successful model was the dependency model with valence (DMV) (Klein and Manning, 2004). Several extensions have been proposed for this model, for example using structural annaeling (Smith and Eisner, 2006), Viterbi EM training (Spitkovsky et al., 2010) or richer contexts (Blunsom and Cohn, 2010). However, these models are not powerful enough for either accurate parsing or language modelling with A generative transition-based parsing model for non-projective parsing is proposed in (Cohen et al., 2011), along with a dynamic program for inference. The parser is similar to ours, but the dynamic program restricts the conditioning context to the top 2 or 3 words on the stack. No experimental results are included. Le and Zuidema (2014) proposed a recursive neural network generative model over dependen"
W15-2108,P04-1061,0,0.129078,"refore less scalable than our model. Related work One of the earliest graph-based dependency parsing models (Eisner, 1996) is generative, estimating the probability of dependents given their head and previously generated siblings. To counter sparsity in the conditioning context of the distributions, backoff and smoothing are performed. Wallach et al. (2008) proposed a Bayesian HPYP parameterisation of this model. Other generative models for dependency trees have been proposed mostly in the context of unsupervised parsing. The first successful model was the dependency model with valence (DMV) (Klein and Manning, 2004). Several extensions have been proposed for this model, for example using structural annaeling (Smith and Eisner, 2006), Viterbi EM training (Spitkovsky et al., 2010) or richer contexts (Blunsom and Cohn, 2010). However, these models are not powerful enough for either accurate parsing or language modelling with A generative transition-based parsing model for non-projective parsing is proposed in (Cohen et al., 2011), along with a dynamic program for inference. The parser is similar to ours, but the dynamic program restricts the conditioning context to the top 2 or 3 words on the stack. No expe"
W15-2108,W10-2902,0,0.0226807,"f dependents given their head and previously generated siblings. To counter sparsity in the conditioning context of the distributions, backoff and smoothing are performed. Wallach et al. (2008) proposed a Bayesian HPYP parameterisation of this model. Other generative models for dependency trees have been proposed mostly in the context of unsupervised parsing. The first successful model was the dependency model with valence (DMV) (Klein and Manning, 2004). Several extensions have been proposed for this model, for example using structural annaeling (Smith and Eisner, 2006), Viterbi EM training (Spitkovsky et al., 2010) or richer contexts (Blunsom and Cohn, 2010). However, these models are not powerful enough for either accurate parsing or language modelling with A generative transition-based parsing model for non-projective parsing is proposed in (Cohen et al., 2011), along with a dynamic program for inference. The parser is similar to ours, but the dynamic program restricts the conditioning context to the top 2 or 3 words on the stack. No experimental results are included. Le and Zuidema (2014) proposed a recursive neural network generative model over dependency trees. However, their model can only score t"
W15-2108,P06-1124,0,0.0564891,"(2009). In addition to sampling the seating arrangement, the discount and strength parameters are also sampled, using slice sampling. In our model Tht , Whw and Aha are HPYPs for the tag, word and transition distributions, respectively. The PYPs for the transition prediction distribution, with conditioning context sequence ha1:L , are defined hierarchically as Hierarchical Pitman-Yor processes The probability distributions for predicting words, tags and transitions are drawn from hierarchical Pitmar-Yor Process (HPYP) priors. HPYP models were originally proposed for n-gram language modelling (Teh, 2006), and have been applied to various NLP tasks. A version of approximate inference in the HPYP model recovers interpolated Kneser-Ney smoothing (Kneser and Ney, 1995), one of the best preforming n-gram language models. The Pitman-Yor Process (PYP) is a generalization of the Dirichlet process which defines a distribution over distributions over a probability space X, with discount parameter 0 ≤ d &lt; 1, strength parameter θ &gt; −d and base distribution B. PYP priors encode the power-law distribution found in the distribution of words. Sampling from the posterior is characterized by the Chinese Restau"
W15-2108,P10-1001,0,0.0192643,"lled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation. 1 Introduction Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014). Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time. Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower. In this paper we aim to transfer the advantages of transition-based parsing to generative dependency parsing. While generative models have been used widely and successfully for constituency 58 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 58–67, Uppsala, Sweden, August 24–26 2015. The types of transitions in this model are shift (sh), left-arc (la) and right-arc (ra): NMOD NAME VMOD NMOD Ms. Waleson is a free-lance wri"
W15-2108,D14-1081,0,0.0362324,"Missing"
W15-2108,W07-2218,0,0.490757,". The stack elements are referred to as σ1 , . . . , σ|σ |, where σ1 is the top element. For any node a, lc1 (a) refers to the leftmost child of a in A, and rc1 (a) to its rightmost child. The initial configuration is ([], 0, ∅). A terminal configuration is reached when β &gt; |s|, and σ consists only of the root. A sentence is generated leftto-right by performing a sequence of transitions. As a generative model it assigns probabilities to sentences and dependency trees: A word w (including its POS tag) is generated when it is shifted on to the stack, similar to the generative models proposed by Titov and Henderson (2007) and Cohen et al. (2011), and the joint tagging and parsing model of Bohnet and Nivre (2012). 3 Probabilistic Generative Model Our model defines a joint probability distribution over a parsed sentence with POS tags t1:n , words w1:n and a transition sequence a1:2n as p(t1:n , w1:n , a1:2n ) mi+1 n   Y Y a = p(ti |htmi )p(wi |ti , hw ) p(a |h ) , j j mi i=1 59 j=mi +1 where mi is the number of transitions that have been performed when (ti , wi ) is generated and ht , hw and ha are sequences representing the conditioning contexts for the tag, word and transition distributions, respectively. In"
W15-2108,P14-1130,0,0.0194407,"at the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation. 1 Introduction Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014). Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time. Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower. In this paper we aim to transfer the advantages of transition-based parsing to generative dependency parsing. While generative models have been used widely and successfully for constituency 58 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 58–67, Uppsala, Sweden, August 24–26 2015. The types of transitions in this model are shift (sh), left-arc (la) and right-arc (ra): NMOD NAME VMOD NMOD Ms. Waleson is a free-lance writer based NNP NNP V"
W15-2108,N03-1033,0,0.191051,"Missing"
W15-2108,J93-2004,0,0.0639567,"e that we divide particles proportionally instead of sampling with replacement, and in the selection step we base the redistribution on the derivation weight instead of the importance weight (the word generation probability). Our method can be interpreted as maximizing 61 Model MaltParser Unlex MaltParser Lex Unlexicalised Lexicalised, unlex context Lexicalised, tagger POS Lexicalised, predict POS Lexicalised, gold POS by sampling from a peaked version of the distribution over derivations. 5 Experiments 5.1 Parsing Setup We evaluate our model as a parser on the standard English Penn Treebank (Marcus et al., 1993) setup, training on WSJ sections 02-21, developing on section 22, and testing on section 23. We use the head-finding rules of Yamada and Matsumoto (2003) (YM)1 for constituencyto-dependency conversion, to enable comparison with previous results. We also evaluate on the Stanford dependency representation (De Marneffe and Manning, 2008) (SD)2 . Words that occur only once in the training data are treated as unknown words. We classify unknown words according to capitalization, numbers, punctuation and common suffixes into classes similar to those used in the implementation of generative constituen"
W15-2108,P15-1032,0,0.0390333,"Missing"
W15-2108,P05-1012,0,0.0954314,"ing over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation. 1 Introduction Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014). Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time. Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower. In this paper we aim to transfer the advantages of transition-based parsing to generative dependency parsing. While generative models have been used widely and successfully for constituency 58 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 58–67, Uppsala, Sweden, August 24–26 2015. The types of transitions in this model are shift (sh), left-arc (la) and right-arc (ra): NMOD NAME VMOD NMOD Ms. Wale"
W15-2108,W03-3023,0,0.174707,"tion weight instead of the importance weight (the word generation probability). Our method can be interpreted as maximizing 61 Model MaltParser Unlex MaltParser Lex Unlexicalised Lexicalised, unlex context Lexicalised, tagger POS Lexicalised, predict POS Lexicalised, gold POS by sampling from a peaked version of the distribution over derivations. 5 Experiments 5.1 Parsing Setup We evaluate our model as a parser on the standard English Penn Treebank (Marcus et al., 1993) setup, training on WSJ sections 02-21, developing on section 22, and testing on section 23. We use the head-finding rules of Yamada and Matsumoto (2003) (YM)1 for constituencyto-dependency conversion, to enable comparison with previous results. We also evaluate on the Stanford dependency representation (De Marneffe and Manning, 2008) (SD)2 . Words that occur only once in the training data are treated as unknown words. We classify unknown words according to capitalization, numbers, punctuation and common suffixes into classes similar to those used in the implementation of generative constituency parsers such as the Stanford parser (Klein and Manning, 2003). As a discriminative baseline we use MaltParser (Nivre et al., 2006), a discriminative,"
W15-2108,D11-1006,0,0.0324252,"Missing"
W15-2108,D08-1059,0,0.31395,"ative baseline. As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation. 1 Introduction Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014). Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time. Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower. In this paper we aim to transfer the advantages of transition-based parsing to generative dependency parsing. While generative models have been used widely and successfully for constituency 58 Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015), pages 58–67, Uppsala, Sweden, August 24–26 20"
W15-2108,C04-1010,0,0.063571,"ily if they are valid, while adding a valid left arc may be delayed if σ1 has unattached right dependents in G. syntactic structure into incremental word prediction. With supervised training the model’s perplexity is comparable to that of n-gram models, although generated examples shows greater syntactic coherence. With semi-supervised learning over a large unannotated corpus its perplexity is considerably better than that of a n-gram model. 2 Generative Transition-based Parsing Our parsing model is based on transition-based projective dependency parsing with the arcstandard parsing strategy (Nivre and Scholz, 2004). Parsing is restricted to (labelled) projective trees. An arc (i, l, j) ∈ A encodes a dependency between two words, where i is the head node, j the dependent and l is the dependency type of j. In our generative model a word can be represented by its lexical (word) type and/or its POS tag. We add a root node to the beginning of the sentence (although it could also be added at the end of the sentence), such that the head word of the sentence is the dependent of the root node. A parser configuration (σ, β, A) for sentence s consists of a stack σ of indices in s, an index β to the next word to be"
W15-2108,P11-2033,0,0.0611653,"redicting POS tags and parse trees. The UAS of the parser is on par with that of a greedy discriminative baseline. As a language model, it obtains better perplexity than a n-gram model by performing semi-supervised learning over a large unlabelled corpus. We show that the model is able to generate locally and syntactically coherent sentences, opening the door to further applications in language generation. 1 Introduction Transition-based dependency parsing algorithms that perform greedy local inference have proven to be very successful at fast and accurate discriminative parsing (Nivre, 2008; Zhang and Nivre, 2011; Chen and Manning, 2014). Beam-search decoding further improves performance (Zhang and Clark, 2008; Huang and Sagae, 2010; Choi and McCallum, 2013), but increases decoding time. Graphbased parsers (McDonald et al., 2005; Koo and Collins, 2010; Lei et al., 2014) perform global inference and although they are more accurate in some cases, inference tends to be slower. In this paper we aim to transfer the advantages of transition-based parsing to generative dependency parsing. While generative models have been used widely and successfully for constituency 58 Proceedings of the Third International"
W15-2108,P81-1022,0,0.617459,"Missing"
W15-2108,nivre-etal-2006-maltparser,0,\N,Missing
