1993.iwpt-1.24,P91-1027,0,0.0286173,"Missing"
1993.iwpt-1.24,H91-1067,0,0.0366546,"Missing"
1993.iwpt-1.24,H92-1022,0,0.0248536,"Missing"
1993.iwpt-1.24,A88-1019,0,0.0215909,"ctionary for each possible domain. 309 310 USHIODA - EVANS - G IBSON - WAIBEL parsing with domain-specific heuristics. Indeed, it would be desirable to have a subcat dictionary for each possible domain . . This paper describes a mechanism for auto matically acquiring subcat frames and their fre quencies based on a tagged corpus. The method utilizes a tagged corpus because (i) we don &apos; t have to deal with a lexical ambiguity (ii) tagged cor pora in various domains are becoming readily available and (iii) simple and robust tagging techniques using such corpora recently have been de veloped (Church 1988, Brill 1 992) . Brent reports a method for automatically acquiring subcat frames but without frequency measurements (Brent and Berwick 199 1 , Brent 199 1 ) . His approach is to count occurrences of those unambiguous verb phrases that contain no noun phrases other than pronouns or proper nouns. By thus restricting the ""features"" that trigger identification of a verb phrase, he avoids possible errors due to syntactic ambiguity. Al though the rate of false positives is very low in his system, his syntactic features are so selective that most verb tokens fail to satisfy them. (For b: k: i: n: v"
1995.tmi-1.13,P95-1016,0,0.062519,"rman, and English) spoken language translation system. The techniques involve statistical models as well as knowledge-based models including discourse plan inference. This work is carried out in the context of the Janus project at Carnegie Mellon University and the University of Karlsruhe ([1]). There has been much recent work on using context to constrain spoken language processing. Most of this work involves making predictions about possible sequences of utterances and using these predictions to limit the search space of the speech recognizer or some other component (See [2], [3], [4], [5], [6], [7], [8], [9]). The goal of such an approach is to increase the accuracy of the top best hypothesis of the speech recognizer, which is then passed on to the language processing components of the system. The underlying assumption being made is that design and complexity considerations require that each component of the system pass on a single hypothesis to the following stage, and that this can achieve sufficiently accurate translation results. However, this approach forces components to make disambiguation choices based solely on the level of knowledge available at that stage of processing."
1995.tmi-1.13,E95-1026,0,0.0140163,"English) spoken language translation system. The techniques involve statistical models as well as knowledge-based models including discourse plan inference. This work is carried out in the context of the Janus project at Carnegie Mellon University and the University of Karlsruhe ([1]). There has been much recent work on using context to constrain spoken language processing. Most of this work involves making predictions about possible sequences of utterances and using these predictions to limit the search space of the speech recognizer or some other component (See [2], [3], [4], [5], [6], [7], [8], [9]). The goal of such an approach is to increase the accuracy of the top best hypothesis of the speech recognizer, which is then passed on to the language processing components of the system. The underlying assumption being made is that design and complexity considerations require that each component of the system pass on a single hypothesis to the following stage, and that this can achieve sufficiently accurate translation results. However, this approach forces components to make disambiguation choices based solely on the level of knowledge available at that stage of processing. Thus, comp"
1995.tmi-1.13,1995.tmi-1.15,0,0.606961,"agram is shown in Figure 2.1 Processing starts with speech input in the source language. Recognition of the speech signal is done with acoustic modeling methods, constrained by a language model. The output of speech recognition is a word lattice. We prefer working with word lattices rather than the more common approach of processing N-best lists of hypotheses. An N-best list may be largely redundant and can be efficiently represented in the form of a lattice. Using a lattice parser can thus reduce time and space 1 Another approach being pursued in parallel in the Janus project is described in [10] 174 175 complexity relative to parsing a corresponding N-best list. Selection of the correct path through the lattice is accomplished during parsing when more information is available. Lattices, however, are potentially inefficient because of their size. We apply four steps to make them more tractable ([11]). The first step involves cleaning the lattice by mapping all non-human noises and pauses into a generic pause. Consecutive pauses are then adjoined to one long pause. The resulting lattice contains only linguistically meaningful information. The lattice is then broken at points where no h"
1995.tmi-1.13,1993.iwpt-1.12,1,0.826786,"et of sub-lattices which are highly correspondent to sentence breaks in the utterance. Each of the sub-lattices is then re-scored using a new language model. Finally the lattices are pruned to a size that the parser can process in reasonable time and space. The re-scoring raises the probability that the correct hypothesis will not be lost during the pruning stage. Each of the resulting sub-lattices are passed on to the parser, the first component of the translation process. Parsing a word lattice involves finding all paths of connecting words within the lattice that are grammatical. The GLR* ([12], [13]) parser skips parts of the utterance that it cannot incorporate into a well-formed structure. Thus it is well-suited to domains in which extra-grammaticality is common. The parser can identify additional sentence breaks within each sub-lattice with the help of a statistical method that determines the probability of sentence breaks at each point in the utterance. The output of parsing a sub-lattice is a set of interlingua texts, or ILTs, representing all of the grammatical paths through the sub-lattice and all of the ambiguities in each grammatical path. The ILTs from each sub-lattice ar"
1995.tmi-1.13,P94-1045,1,0.890553,"sub-lattices which are highly correspondent to sentence breaks in the utterance. Each of the sub-lattices is then re-scored using a new language model. Finally the lattices are pruned to a size that the parser can process in reasonable time and space. The re-scoring raises the probability that the correct hypothesis will not be lost during the pruning stage. Each of the resulting sub-lattices are passed on to the parser, the first component of the translation process. Parsing a word lattice involves finding all paths of connecting words within the lattice that are grammatical. The GLR* ([12], [13]) parser skips parts of the utterance that it cannot incorporate into a well-formed structure. Thus it is well-suited to domains in which extra-grammaticality is common. The parser can identify additional sentence breaks within each sub-lattice with the help of a statistical method that determines the probability of sentence breaks at each point in the utterance. The output of parsing a sub-lattice is a set of interlingua texts, or ILTs, representing all of the grammatical paths through the sub-lattice and all of the ambiguities in each grammatical path. The ILTs from each sub-lattice are comb"
1995.tmi-1.13,P92-1025,0,0.0443447,"a statistical method that determines the probability of sentence breaks at each point in the utterance. The output of parsing a sub-lattice is a set of interlingua texts, or ILTs, representing all of the grammatical paths through the sub-lattice and all of the ambiguities in each grammatical path. The ILTs from each sub-lattice are combined, yielding a list of ILT sequences that represent the possible sentences in a full multi-sentence turn. An ILT n-gram is applied to each such list to determine the probability of each sequence of sentences. The discourse processor, based on Lambert’s work ([14, 15]), disambiguates the speech act of each sentence, normalizes temporal expressions, and incorporates the sentence into a discourse plan tree. The discourse processor's focusing heuristics and plan operators eliminate some ambiguity by filtering out hypotheses that do not fit into the current discourse context. The discourse component also updates a calendar in the dynamic discourse memory to keep track of what the speakers have said about their schedules. As processing continues, the N-best hypotheses for sequences of ILTs in a multisentence turn are sent to the generator. The generation output"
1996.amta-1.30,1993.iwpt-1.12,1,0.894049,"Missing"
1996.amta-1.30,P94-1045,1,0.845094,"Missing"
1996.amta-1.30,P81-1022,0,0.0225358,"Missing"
2003.mtsummit-papers.53,W03-1502,1,0.772759,"re NEs have been manually or automatically annotated. Starting from a bilingual corpus where NEs are automatically tagged for each language, NE pairs are aligned in order to minimize a multi-feature alignment cost including the transliteration cost, the NE tagging cost, and word-based translation cost. These features are designed to capture the semantic or phonetic similarities between NE pairs as well as NE tagging confidence, and are derived from several information sources using unsupervised and partly supervised methods. A greedy search algorithm is applied to minimize the alignment cost (Huang et al., 2003). Online NE translation is specially designed for translating NEs which appear in the given test document, but are not covered by the Offline translation. The missing source NEs and target NE translations are “retrieved” cross-lingually from topic-relevant documents (w.r.t. the test document). Relevant documents are retrieved from a monolingual corpus using a 1st-pass translation of the test document as the query. NEs in the retrieved documents are extracted and aligned with source NEs according to their transliteration cost. The NE pairs with minimum transliteration cost are considered as tra"
2003.mtsummit-papers.53,J93-2003,0,0.119946,"ical machine translation system. This system combines phrase-tophrase translations extracted from a bilingual corpus using different alignment approaches. Special methods to extract and align named entities are used. We show how a manual lexicon can be incorporated into the statistical system in an optimized way. Experiments on Chinese-toEnglish and Arabic-to-English translation tasks are presented. 1 Introduction Statistical machine translation is currently the most promising approach to large vocabulary text translation. In the spirit of the Candide system developed in the early 90s at IBM (Brown et al., 1993), a number of statistical machine translation systems have been presented in the last few years (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamada and Knight, 2000). These systems share the basic underlying principles of applying a translation model to capture the lexical and word reordering relationships between two languages, complemented by a target language model to drive the search process through translation model hypotheses. Their primary differences lie in the structure and source of their translation models. Whereas the original IBM system was based on purely word-based translation"
2003.mtsummit-papers.53,W02-1018,0,0.110892,"Missing"
2003.mtsummit-papers.53,P03-1041,1,0.837652,"Missing"
2003.mtsummit-papers.53,C96-2141,1,0.3498,"four different approaches to phrase pair extraction, each of which will be described below. We also describe our technique for adding generalization power by allowing for overlapping phrases. 2.1 From Viterbi Path of HMM Word Alignment A simple approach to extract phrase translations from a bilingual corpus is to harvest the Viterbi path generated by a word alignment model. A number of probabilistic word alignment models have been proposed (Brown et al., 1993) (Och and Ney, 2000) and shown to be effective for statistical machine translation. We use the HMM-based alignment model introduced in (Vogel et al., 1996) which estimates position alignment probabilities in addition to lexical probabilities. The HMM-based alignment model is based on relative positions: it addresses the likelihood that the word at source position j +1 is aligned to target position i0 when source position j is aligned to target position i. The Viterbi path can be used not only to map source words to target words, i.e. building a statistical lexicon, but also to map source phrases to target phrases. For each source phrase ranging from positions j1 to j2 the corresponding target phrase is given by imin = minj {i = a(j)} and imax ="
2003.mtsummit-papers.53,P00-1004,1,0.851861,"Missing"
2003.mtsummit-papers.53,J97-3002,0,0.054098,"ng from positions j1 to j2 the corresponding target phrase is given by imin = minj {i = a(j)} and imax = maxj {i = a(j)}, where j = j1 ...j2 . This is a very simple criterion which does not test if the source phrase actually aligns to two or more noncontiguous sequences of words in the target sentence. Due to the potential for alignment errors, such a test would be unreliable. However, by preventing the length of the aligned target phrase from exceeding the length of the source phrase by a given factor, the problem of non-contiguous alignments can be reduced. 2.2 From Bilingual Bracketing In (Wu, 1997) a word alignment model was proposed which adds additional alignment restrictions over the IBM-style alignment models. The bilingual bracketing builds an hierarchical alignment, which can be viewed as a simple top-down bilingual parse: split source and target segment into two halves f˜l , f˜r and e˜l , e˜r . Then either align f˜l to e˜l and f˜r to e˜r , which is called a straight alignment, or align f˜l to e˜r and f˜r to e˜l , called a reversed alignment. Repeat this for each aligned segment pair down to the word level. At each level the optimization is over the split points and the direction,"
2003.mtsummit-papers.53,W03-0303,1,0.85137,"Missing"
2003.mtsummit-papers.53,P02-1040,0,\N,Missing
2003.mtsummit-papers.53,P01-1067,0,\N,Missing
2003.mtsummit-papers.53,P00-1056,0,\N,Missing
2004.iwslt-evaluation.10,takezawa-etal-2002-toward,0,0.0142947,"irst, we simply cascaded the Chinese-English and English-Spanish systems, feeding the output of the former into the latter ones. We then translated the same test set using the full EDTRL system’s definition of an augmented, formalized version of English as an interlingua. For further comparison, the direct and cascaded translation steps were also done with Systran’s publicly available online machine translation system [Systran2004]. The data for these experiments were taken from the Basic Travel Expression Corpus (BTEC), a multilingual collection of conversational phrases in the travel domain [Takezawa2002]. The Chinese-English system was trained on 162316 parallel phrases. As only a subset of 6027 phrases was available in Spanish, only the corresponding parallel phrases were used to train the English-Spanish and ChineseSpanish systems. The test set consisted of 506 new sentences created for the 2003 CSTAR evaluation campaign, and the scores were calculated using 16 English and in average of 34 Spanish reference translations. We report the NIST score using the mteval script [MTeval2002] in version 11. Systems C E E S C S C E S C EIL S EDTRL 7.34 5.17 3.17 3.41 3.69 Method fluency adequacy BLEU G"
2004.iwslt-evaluation.11,C96-2141,1,0.873342,"Missing"
2004.iwslt-evaluation.11,2003.mtsummit-papers.53,1,0.846307,"Missing"
2004.iwslt-evaluation.11,J93-2003,0,0.0333438,"Missing"
2004.iwslt-evaluation.11,1999.mtsummit-1.31,0,0.0819927,"Missing"
2004.iwslt-evaluation.11,P00-1056,0,0.138706,"Missing"
2004.iwslt-evaluation.11,P02-1040,0,0.0695583,"Missing"
2004.iwslt-evaluation.11,P01-1067,0,\N,Missing
2004.iwslt-evaluation.11,takezawa-etal-2002-toward,0,\N,Missing
2005.eamt-1.18,J93-2003,0,0.00549346,"translations produced by the TM system were compared with the translations produced by the statistical decoder. In our current experiments TM system did not show an improvement in terms of automatic evaluation metrics. However, a subjective human evaluation found that, in several instances, the TM system produced better translations than the statistical decoder. In the following section we explain the TM system in detail. We also describe the phrase extraction method we used to identify alignments between source words and target words, which is a modified version of the IBM1 alignment model (Brown et al. 1993). In Section 3, we present the experimental setting and the results of the evaluation. It is followed by a discussion in section 4, and conclusions in section 5. We have identified a number of improvements to the current system, some of which are already in progress. 2. Translation Memory System 2.1. Extracting Similar Sentences For each new test sentence F, we find a set of similar source sentences {F1, F2, …} from the training corpus. The similarity is measured in terms of the standard edit distance criterion with equal penalties for insertion, deletion and substitution operations. The corre"
2005.eamt-1.18,langlais-simard-2002-merging,0,0.0180005,"ne translation memory with other machine translation approaches. In (Marcu, 2001) an automatically derived TM is used along with a statistical model to obtain translations of higher probability than those found using only a statistical model. Sumita (2001) describes an example-based technique which extracts similar translations and modifies them using a bilingual dictionary. Watanabe and Sumita (2003) proposed an example-based decoder that start with close matching example translations, and then modify them using a greedy search algorithm. Instead of extracting complete sentences from the TM, Langlais and Simard, (2002) work on sub sentential leEAMT 2005 Conference Proceedings Augmenting a statistical translation system with a translation memory vel. Translations for word sequences are extracted from a TM and then fed into a statistical engine to generate the desired translation. In this paper, we present an experiment where we attempted to augment a statistical translation system with a translation memory. For a sentence which has a close match in the training corpus, the idea is to start with the available translation and apply specific modifications to produce the desired translation. By a close match, we"
2005.eamt-1.18,P01-1050,0,0.0198843,"words; sometimes even an exact matching sentence. Translation memory (TM) systems typically work well in these situations. In its pure form, a TM system is simply a database of past translations, stored as sentence pairs in source and target languages. Whenever an exact match is found for a new sentence to be translated, the desired translation is extracted from the translation memory. TM systems have been successfully used in Computer Aided Translations (CAT) as a tool for human translators. There have been attempts to combine translation memory with other machine translation approaches. In (Marcu, 2001) an automatically derived TM is used along with a statistical model to obtain translations of higher probability than those found using only a statistical model. Sumita (2001) describes an example-based technique which extracts similar translations and modifies them using a bilingual dictionary. Watanabe and Sumita (2003) proposed an example-based decoder that start with close matching example translations, and then modify them using a greedy search algorithm. Instead of extracting complete sentences from the TM, Langlais and Simard, (2002) work on sub sentential leEAMT 2005 Conference Proceed"
2005.eamt-1.18,lavie-etal-2002-nespole,0,0.0280647,"Missing"
2005.eamt-1.18,1999.mtsummit-1.31,0,0.0492566,"Missing"
2005.eamt-1.18,2001.mtsummit-papers.68,0,0.0451136,"Missing"
2005.eamt-1.18,takezawa-etal-2002-toward,0,0.0143,"with other machine translation approaches. In (Marcu, 2001) an automatically derived TM is used along with a statistical model to obtain translations of higher probability than those found using only a statistical model. Sumita (2001) describes an example-based technique which extracts similar translations and modifies them using a bilingual dictionary. Watanabe and Sumita (2003) proposed an example-based decoder that start with close matching example translations, and then modify them using a greedy search algorithm. Instead of extracting complete sentences from the TM, Langlais and Simard, (2002) work on sub sentential leEAMT 2005 Conference Proceedings Augmenting a statistical translation system with a translation memory vel. Translations for word sequences are extracted from a TM and then fed into a statistical engine to generate the desired translation. In this paper, we present an experiment where we attempted to augment a statistical translation system with a translation memory. For a sentence which has a close match in the training corpus, the idea is to start with the available translation and apply specific modifications to produce the desired translation. By a close match, we"
2005.eamt-1.18,2004.iwslt-evaluation.11,1,\N,Missing
2005.eamt-1.18,W01-1401,0,\N,Missing
2005.eamt-1.18,P02-1040,0,\N,Missing
2005.eamt-1.19,J93-2003,0,0.00707137,"Missing"
2005.eamt-1.19,eck-etal-2004-language,1,0.491287,"Missing"
2005.eamt-1.19,P02-1040,0,0.0949776,"Missing"
2005.eamt-1.19,takezawa-etal-2002-toward,0,0.0178463,"Missing"
2005.eamt-1.19,2003.mtsummit-papers.53,1,0.418648,"Missing"
2005.eamt-1.19,P04-3002,0,0.0319308,"Missing"
2005.eamt-1.19,C04-1059,1,0.531263,"Missing"
2005.eamt-1.19,C96-2141,1,\N,Missing
2005.iwslt-1.6,J93-2003,0,0.00594062,"For Chinese-English direction we also worked on ASR output of the supplied data, and with additional data in unrestricted and C-STAR tracks. 1. Introduction Large vocabulary text translation has been the primary focus in machine translation research during the past. Much improvements have been achieved with projects such as TIDES, which focused on large vocabulary text translation. With the availability of reliable speech recognition systems and spoken language corpora, now the focus is shifting towards speech translation; and further towards speech-to-speech translation. With the IBM system [1] in early 90’s, statistical machine translation (SMT) has been the most promising approach for machine translation. Many approaches for SMT have been proposed since then [2], [3], [4]. Whereas the original IBM system was based on purely word translation models, current SMT systems incorporate more sophisticated models. The CMU statistical machine translation system uses phrase-to-phrase translations as the primary building blocks to capture local context information, leading to better lexical choice and more reliable local reordering. In section 2, we describe the phrase alignment approaches u"
2005.iwslt-1.6,P03-1021,0,0.00823941,"ity is restricted. However, the reliability of one model might be higher than the reliability of another model. So, we should put more weight on this model in the overall decision. This can be done by doing a log-linear combination of the models. In other words, each model score is weighted and we have to find an optimal set of these weights or scaling factors. When dealing with two or three models, grid search is still feasible. When adding more and more features (models) this no longer is the case and automatic optimization needs to be done. We use the Minimum Error Training as described in [11], which uses rescoring of the n-best list to find the scaling factors with maximize BLEU or NIST score. Starting with some reasonably chosen model weights a first decoding for some development test set is done. An n-best list is generated, typically a 1000-best list. Then a multi-linear search is performed, for each model weight in turn. The weight, for which the change gives the best improvement in the MT evaluation metric, is then fixed to the new value, and the search repeated, till no further improvement is possible. The optimization is therefore based on an n-best list, which resulted fro"
2005.iwslt-1.6,P00-1056,0,0.0770785,"t translation has been the primary focus in machine translation research during the past. Much improvements have been achieved with projects such as TIDES, which focused on large vocabulary text translation. With the availability of reliable speech recognition systems and spoken language corpora, now the focus is shifting towards speech translation; and further towards speech-to-speech translation. With the IBM system [1] in early 90’s, statistical machine translation (SMT) has been the most promising approach for machine translation. Many approaches for SMT have been proposed since then [2], [3], [4]. Whereas the original IBM system was based on purely word translation models, current SMT systems incorporate more sophisticated models. The CMU statistical machine translation system uses phrase-to-phrase translations as the primary building blocks to capture local context information, leading to better lexical choice and more reliable local reordering. In section 2, we describe the phrase alignment approaches used by our system. The main obstacle in using additional data for a translation task is that the new data may belong to a different domain. We explored methods of adapting both t"
2005.iwslt-1.6,P01-1067,0,0.0435513,"nslation has been the primary focus in machine translation research during the past. Much improvements have been achieved with projects such as TIDES, which focused on large vocabulary text translation. With the availability of reliable speech recognition systems and spoken language corpora, now the focus is shifting towards speech translation; and further towards speech-to-speech translation. With the IBM system [1] in early 90’s, statistical machine translation (SMT) has been the most promising approach for machine translation. Many approaches for SMT have been proposed since then [2], [3], [4]. Whereas the original IBM system was based on purely word translation models, current SMT systems incorporate more sophisticated models. The CMU statistical machine translation system uses phrase-to-phrase translations as the primary building blocks to capture local context information, leading to better lexical choice and more reliable local reordering. In section 2, we describe the phrase alignment approaches used by our system. The main obstacle in using additional data for a translation task is that the new data may belong to a different domain. We explored methods of adapting both the tr"
2005.iwslt-1.6,takezawa-etal-2002-toward,0,0.0745974,"ip} where sp is a skip penalty (sp &lt; 0); d(vm−1 , vm ) is the number of skipped words between vm−1 and vm ; ip is an insertion penalty [14]. The skip penalty is incorporated to avoid high compression of the original sentence because high compression of a sentence often alters the meaning of the sentence. The insertion penalty is used to control the overall compression ratio. 6. Evaluation The evaluations were primarily based on the Basic Travel Expression Corpus (BTEC) which contains conversations in tourism-related activities. The corpus was originally created in Japanese and English by ATR [15] and was later extended to other languages. We participated in the supplied data track for the translation directions Arabic-English, Chinese-English, Japanese-English and Korean-English. For ChineseEnglish direction we also worked on ASR output. In both unrestricted and C-STAR tracks, we participated for Chinese-English direction. For each translation direction, except KoreanEnglish, two development sets (C-STAR’03 and Supplied Data Track Chinese Japanese Korean Manual ASR 20,000 176,199 198,453 208,763 8,687 9,277 9,132 506 3,511 2,835 4,130 4,084 913 1,024 920 976 117 245 70 95 500 3,590 2,"
2005.iwslt-1.6,2005.mtsummit-papers.33,1,0.835754,"ecoder that combines the translation model, language model, and other models to generate the complete translation. When translating speech recognition output, we integrate multiple translation hypotheses into a single structure and then derive the best hypothesis. This approach is described in section 5. Finally, in section 6 we give an overview of the data and tasks and present the results of the experiments we carried out for different data conditions. 2. Phrase Alignment In this evaluation, we applied a variation of the alignment-free approach, which is an extension to the previous work in [5] and [6] to extract bilingual phrase pairs for the supplied data tracks. In this extension, we used eleven feature functions including phrase level fertilities and phrase level IBM Model-1 probabilities aiming to locate the phrase pairs from the parallel sentences. The feature functions are then combined in a log-linear model as follows: PM exp( m=1 λm φm (X, e, f )) P (X|e, f )= P PM 0 {X 0 } exp( m=1 λm φm (X ,e,f )) where X→(fjj+l , ei+k ) corresponds to a phrase-pair cani didate extracted from a given sentence-pair (e, f ); φm is a feature function designed to be informative for phrase ext"
2005.iwslt-1.6,W05-0825,1,0.830118,"hat combines the translation model, language model, and other models to generate the complete translation. When translating speech recognition output, we integrate multiple translation hypotheses into a single structure and then derive the best hypothesis. This approach is described in section 5. Finally, in section 6 we give an overview of the data and tasks and present the results of the experiments we carried out for different data conditions. 2. Phrase Alignment In this evaluation, we applied a variation of the alignment-free approach, which is an extension to the previous work in [5] and [6] to extract bilingual phrase pairs for the supplied data tracks. In this extension, we used eleven feature functions including phrase level fertilities and phrase level IBM Model-1 probabilities aiming to locate the phrase pairs from the parallel sentences. The feature functions are then combined in a log-linear model as follows: PM exp( m=1 λm φm (X, e, f )) P (X|e, f )= P PM 0 {X 0 } exp( m=1 λm φm (X ,e,f )) where X→(fjj+l , ei+k ) corresponds to a phrase-pair cani didate extracted from a given sentence-pair (e, f ); φm is a feature function designed to be informative for phrase extraction."
2005.iwslt-1.6,I05-3011,1,0.902,"nsion, we used eleven feature functions including phrase level fertilities and phrase level IBM Model-1 probabilities aiming to locate the phrase pairs from the parallel sentences. The feature functions are then combined in a log-linear model as follows: PM exp( m=1 λm φm (X, e, f )) P (X|e, f )= P PM 0 {X 0 } exp( m=1 λm φm (X ,e,f )) where X→(fjj+l , ei+k ) corresponds to a phrase-pair cani didate extracted from a given sentence-pair (e, f ); φm is a feature function designed to be informative for phrase extraction. Feature function weights {λm }, are the same as in our previous experiments [7]. This log-linear model serves as a performance measure function in a local search. The search starts from fetching a test-set specific source phrase (e.g. Chinese ngram); it localizes the candidate ngram’s center in the English sentence; and then around the projected center, it finds out all the candidate phrase pairs ranked with the log-linear model scores. In the local search, down-hill moves are allowed so that functional words can be attached to the left or right boundaries of the candidate phrase-pairs. The eleven (M =11) feature functions that compute different aspects of phrase pair (f"
2005.iwslt-1.6,J03-1002,0,0.00619115,"1 scores for the phrase-pairs P (fjj+l |eii+k ) and P (ei+k |fjj+l ); i the remaining parts of (e, f ) excluding the phrasepair is modeled by P (fj 0 ∈[j,j+l] |ei0 ∈[i,i+k] ) and / / 0 P (ei0 ∈[i,i+k] |f ) using the translation lex/ j ∈[j,j+l] / icons of P (f |e) and P (e|f ). • Another two of the scores aim to bracket the sentence pair with the phrase-pair as detailed in [7]. • The last function computes the average word alignment links per source word in the candidate phrasepair. We assume each phrase-pair should contain at least one word alignment link. We train the IBM Model-4 with GIZA++ [8] in both directions and grow the intersection with word pairs in the union to collect the word alignment. Because of the last feature-function, our approach is no longer truly “alignment-free”. More details of the log-linear model and experimental analysis of the feature-functions are given in [7]. To use the extracted phrase-pairs in the decoder, a set of eight scores for each phrase-pair are computed: relative frequency of both directions, phraselevel fertility scores for both directions computed via dynamic programming, the standard IBM Model-1 scores for P both directions (i.e. P (fjj+l |e"
2005.iwslt-1.6,W05-0829,1,0.826281,"ecoder a scaling factor can be used to modify the contribution of this model to the overall score. Varying this scaling factors can change the performance of the system considerable. Minimum error training is used to find a good set of scaling factors. In the following sub-sections, these different steps will be described in some more detail. 4.1. Building Translation Lattice The CMU SMT decoder can use phrase tables, generated at training time, but can also do just-in-time phrase alignment. This means that the entire bilingual corpus is loaded and the source side indexed using a suffix array [10]. For all ngrams in the test sentence, occurrences in the corpus are located using the suffix array. For a number of occurrences, where the number can be given as a parameter to the decoder, phrase alignment as described in section 2 is performed and the found target phrase added to the translation lattice. If phrase translations have already been collected during training time, then this phrase table is loaded into the decoder and a prefix tree constructed over the source phrases. This allows for an efficient search to find all source phrases in the phrase table which match a sequence of word"
2005.iwslt-1.6,P02-1040,0,0.0779885,"1 2,835 4,130 4,084 913 1,024 920 976 117 245 70 95 500 3,590 2,896 4,131 975 1,068 945 116 223 61 506 3,743 3,003 4,226 4,563 963 1,091 975 969 155 249 169 84 English 183,452 6,956 - IWSLT’04) were made available. For Korean-English only C-STAR’03 test set was available. Table 1 shows corpus statistics for the training and test sets. As a preprocessing step, we separated punctuations from words in the English (target) side and converted the text into lowercase. No preprocessing was done on any of the source side data. We report translation results using the well known evaluation metrics BLEU [16] and NIST [17]. For our primary system and the best system, we report results also in WER, PER, METEOR [18] and GTM [19]. 6.1. Supplied Data Track During the evaluation our primary focus was on the Chinese-English direction. We applied both PESA and Alignment-Free phrase extraction methods to the supplied data track. In building phrase tables using the Alignment Free method, we extracted phrase-pairs with source side up to 8-gram in length. PESA online phrase extraction method can extract phrases up to full length of the sentence. Table 2 summarizes the official translation results for our pri"
2005.iwslt-1.6,W05-0909,0,0.0223458,"3 3,003 4,226 4,563 963 1,091 975 969 155 249 169 84 English 183,452 6,956 - IWSLT’04) were made available. For Korean-English only C-STAR’03 test set was available. Table 1 shows corpus statistics for the training and test sets. As a preprocessing step, we separated punctuations from words in the English (target) side and converted the text into lowercase. No preprocessing was done on any of the source side data. We report translation results using the well known evaluation metrics BLEU [16] and NIST [17]. For our primary system and the best system, we report results also in WER, PER, METEOR [18] and GTM [19]. 6.1. Supplied Data Track During the evaluation our primary focus was on the Chinese-English direction. We applied both PESA and Alignment-Free phrase extraction methods to the supplied data track. In building phrase tables using the Alignment Free method, we extracted phrase-pairs with source side up to 8-gram in length. PESA online phrase extraction method can extract phrases up to full length of the sentence. Table 2 summarizes the official translation results for our primary submissions. We also give contrastive results for the Arabic-English and Chinese-English directions in"
2005.iwslt-1.6,2003.mtsummit-papers.51,0,0.0310678,"4,563 963 1,091 975 969 155 249 169 84 English 183,452 6,956 - IWSLT’04) were made available. For Korean-English only C-STAR’03 test set was available. Table 1 shows corpus statistics for the training and test sets. As a preprocessing step, we separated punctuations from words in the English (target) side and converted the text into lowercase. No preprocessing was done on any of the source side data. We report translation results using the well known evaluation metrics BLEU [16] and NIST [17]. For our primary system and the best system, we report results also in WER, PER, METEOR [18] and GTM [19]. 6.1. Supplied Data Track During the evaluation our primary focus was on the Chinese-English direction. We applied both PESA and Alignment-Free phrase extraction methods to the supplied data track. In building phrase tables using the Alignment Free method, we extracted phrase-pairs with source side up to 8-gram in length. PESA online phrase extraction method can extract phrases up to full length of the sentence. Table 2 summarizes the official translation results for our primary submissions. We also give contrastive results for the Arabic-English and Chinese-English directions in Table 3. The"
2005.iwslt-1.6,2005.mtsummit-papers.30,1,\N,Missing
2005.iwslt-1.6,W06-1626,1,\N,Missing
2005.iwslt-1.6,2005.iwslt-1.16,0,\N,Missing
2005.iwslt-1.6,2005.eamt-1.19,1,\N,Missing
2005.iwslt-1.7,J93-2003,0,0.00423534,"t the time we select the actual training data. Statistical machine translation can be described in a formal way as follows: t * = arg max P (t |s) = arg max P( s |t ) ⋅ P (t ) t t Here t is the target sentence, and s is the source sentence. P(t) is the target language model and P(s|t) is the translation model used in the decoder. Statistical machine translation searches for the best target sentence from the space defined by the target language model and the translation model. Statistical translation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM ([1], [2], [3]). Some recent developments focused on online phrase extraction ([4], [5]). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation (SMT) is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with fewer resources ([6], [7]). To improve the data situation for these languages it is necessary to hire human translators at enormo"
2005.iwslt-1.7,C96-2141,1,0.561503,"time we select the actual training data. Statistical machine translation can be described in a formal way as follows: t * = arg max P (t |s) = arg max P( s |t ) ⋅ P (t ) t t Here t is the target sentence, and s is the source sentence. P(t) is the target language model and P(s|t) is the translation model used in the decoder. Statistical machine translation searches for the best target sentence from the space defined by the target language model and the translation model. Statistical translation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM ([1], [2], [3]). Some recent developments focused on online phrase extraction ([4], [5]). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation (SMT) is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with fewer resources ([6], [7]). To improve the data situation for these languages it is necessary to hire human translators at enormous co"
2005.iwslt-1.7,P05-1032,0,0.147595,"can be described in a formal way as follows: t * = arg max P (t |s) = arg max P( s |t ) ⋅ P (t ) t t Here t is the target sentence, and s is the source sentence. P(t) is the target language model and P(s|t) is the translation model used in the decoder. Statistical machine translation searches for the best target sentence from the space defined by the target language model and the translation model. Statistical translation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM ([1], [2], [3]). Some recent developments focused on online phrase extraction ([4], [5]). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation (SMT) is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with fewer resources ([6], [7]). To improve the data situation for these languages it is necessary to hire human translators at enormous costs who translate corpora that can later be used to train SMT systems. Ou"
2005.iwslt-1.7,2005.eamt-1.39,1,0.90465,"be described in a formal way as follows: t * = arg max P (t |s) = arg max P( s |t ) ⋅ P (t ) t t Here t is the target sentence, and s is the source sentence. P(t) is the target language model and P(s|t) is the translation model used in the decoder. Statistical machine translation searches for the best target sentence from the space defined by the target language model and the translation model. Statistical translation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM ([1], [2], [3]). Some recent developments focused on online phrase extraction ([4], [5]). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation (SMT) is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with fewer resources ([6], [7]). To improve the data situation for these languages it is necessary to hire human translators at enormous costs who translate corpora that can later be used to train SMT systems. Our ide"
2005.iwslt-1.7,mcenery-etal-2000-corpus,0,0.160881,"l translation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM ([1], [2], [3]). Some recent developments focused on online phrase extraction ([4], [5]). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation (SMT) is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with fewer resources ([6], [7]). To improve the data situation for these languages it is necessary to hire human translators at enormous costs who translate corpora that can later be used to train SMT systems. Our idea focuses on sorting the available source sentences that should be translated by a human translator according to their approximate importance. The importance is estimated using a frequency based and an information retrieval approach. 2. Motivation There are three inherently different motivations for the goal of limiting the amount of necessary training data for a competitive translation system. We describ"
2005.iwslt-1.7,2004.eamt-1.14,1,0.922835,"nslation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM ([1], [2], [3]). Some recent developments focused on online phrase extraction ([4], [5]). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation (SMT) is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with fewer resources ([6], [7]). To improve the data situation for these languages it is necessary to hire human translators at enormous costs who translate corpora that can later be used to train SMT systems. Our idea focuses on sorting the available source sentences that should be translated by a human translator according to their approximate importance. The importance is estimated using a frequency based and an information retrieval approach. 2. Motivation There are three inherently different motivations for the goal of limiting the amount of necessary training data for a competitive translation system. We described th"
2005.iwslt-1.7,2005.mtsummit-papers.30,1,0.0889638,"ecessary to hire human translators at enormous costs who translate corpora that can later be used to train SMT systems. Our idea focuses on sorting the available source sentences that should be translated by a human translator according to their approximate importance. The importance is estimated using a frequency based and an information retrieval approach. 2. Motivation There are three inherently different motivations for the goal of limiting the amount of necessary training data for a competitive translation system. We described those motivations and their applications already in the paper [8]. Application 1: Reducing Human Translation Cost The main problem of portability of SMT systems to new languages is the involved cost to generate parallel bilingual training data as it is necessary to have sentences translated by human translators. An assumption could be that a 1 million word corpus needs to be translated to a new language in order to build a decent SMT system. A human translator could charge in the range of approximately 0.10-0.25 USD per word depending on the involved languages and the difficulty of the text. The translation of a 1 million word corpus would then cost between"
2005.iwslt-1.7,J04-3001,0,0.0607757,"able to hold them in memory. (The last issue will certainly be resolved by the widespread introduction of 64 bit machines which can theoretically address 17 million terabytes of memory.) 3. Previous Work This research can generally be regarded as an example of active learning. This means the machine learning algorithm does not just passively train on the available training data but plays an active role in selecting the best training data. Active learning, as a standard method in machine learning, has been applied to a variety of problems in natural language processing, for example to parsing ([9]) and to automatic speech recognition ([10]). It is important to note the difference between this approach and approaches to Translation Model Adaptation ([11]) or simple subsampling techniques that are based on the actual test data. Here we assume that the test data is not known at selection time so the intention is to get the best possible translation system for every possible test data. Our previous work in this area focused on improving the ngram (type-) coverage by selecting the sentences based on the number of previously unseen n-grams they contain [8]. Section 4.2 will give a short over"
2005.iwslt-1.7,2005.eamt-1.19,1,0.842734,"illion terabytes of memory.) 3. Previous Work This research can generally be regarded as an example of active learning. This means the machine learning algorithm does not just passively train on the available training data but plays an active role in selecting the best training data. Active learning, as a standard method in machine learning, has been applied to a variety of problems in natural language processing, for example to parsing ([9]) and to automatic speech recognition ([10]). It is important to note the difference between this approach and approaches to Translation Model Adaptation ([11]) or simple subsampling techniques that are based on the actual test data. Here we assume that the test data is not known at selection time so the intention is to get the best possible translation system for every possible test data. Our previous work in this area focused on improving the ngram (type-) coverage by selecting the sentences based on the number of previously unseen n-grams they contain [8]. Section 4.2 will give a short overview over our previous best method. 4. Description of sentence sorting 4.1. Algorithm The sentences are sorted according to the following very simple algorithm"
2005.iwslt-1.7,takezawa-etal-2002-toward,0,0.0243937,"easily be generalized to n-grams by using every ngram as an entry in the document vectors (instead of only using words). We tried this for n-grams up to bigrams and plan on doing experiments with higher n-grams. The following section 5 will give an overview over the experiments that were done using the three presented approaches to sort sentences according to their estimated importance. 5. Experiments English-Spanish Baseline The full training data for the translation experiments consisted of 123,416 English sentences with 903,525 English words (tokens). This data is part of the BTEC corpus ([12]) with relatively simple sentences from the travel domain. The whole training data was also available in Spanish (852,362 words). The testing data which was used to measure the machine translation performance consisted of 500 lines of data from the medical domain. All translations in this task were done translating English to Spanish. 5.2. Machine Translation System The applied statistical machine translation system uses an online phrase extraction algorithm based on IBM1 lexicon probabilities ([3], [13]). The language model is a trigram language model with Kneser-Ney-discounting built with th"
2005.iwslt-1.7,2004.iwslt-evaluation.11,1,0.807644,"glish sentences with 903,525 English words (tokens). This data is part of the BTEC corpus ([12]) with relatively simple sentences from the travel domain. The whole training data was also available in Spanish (852,362 words). The testing data which was used to measure the machine translation performance consisted of 500 lines of data from the medical domain. All translations in this task were done translating English to Spanish. 5.2. Machine Translation System The applied statistical machine translation system uses an online phrase extraction algorithm based on IBM1 lexicon probabilities ([3], [13]). The language model is a trigram language model with Kneser-Ney-discounting built with the SRI-Toolkit ([14]) using only the Spanish part of the training data. We applied the standard metrics introduced for machine translation, NIST ([15]) and BLEU ([16]). NIST score 5.1. Test and Training Data Previous best 4.4 4.2 4.0 3.8 3.6 3.4 3.2 3.0 2.8 2.6 2.4 2.2 2.0 1.8 1.6 0 200000 400000 600000 800000 translated words Diagram 1: NIST scores for Baseline and Previous best The picture is similar for the BLEU scores. The previous best selection reached a BLEU score of 0.13 at 400,000 translated word"
2005.iwslt-1.7,P02-1040,0,0.0815562,"to measure the machine translation performance consisted of 500 lines of data from the medical domain. All translations in this task were done translating English to Spanish. 5.2. Machine Translation System The applied statistical machine translation system uses an online phrase extraction algorithm based on IBM1 lexicon probabilities ([3], [13]). The language model is a trigram language model with Kneser-Ney-discounting built with the SRI-Toolkit ([14]) using only the Spanish part of the training data. We applied the standard metrics introduced for machine translation, NIST ([15]) and BLEU ([16]). NIST score 5.1. Test and Training Data Previous best 4.4 4.2 4.0 3.8 3.6 3.4 3.2 3.0 2.8 2.6 2.4 2.2 2.0 1.8 1.6 0 200000 400000 600000 800000 translated words Diagram 1: NIST scores for Baseline and Previous best The picture is similar for the BLEU scores. The previous best selection reached a BLEU score of 0.13 at 400,000 translated words. The reason for the necessity to translate more words to reach a BLEU score in the confidence interval of the final system could be that the BLEU score puts higher importance on fluency. Larger systems might benefit from more robust estimations of the la"
2005.iwslt-1.7,W01-1409,0,0.0433966,".02 2.63 2.57 2.40 3.05 3.00 3.02 3.00 2.05 3.25 3.29 2.98 3.30 3.27 2.90 2.82 2.58 3.56 3.31 3.49 3.50 3.40 3.63 3.63 3.36 3.65 3.62 3.23 3.19 3.34 3.81 3.42 3.72 3.71 3.55 3.86 3.85 3.57 3.80 3.77 3.53 3.50 650k 170k 380k 230k 240k 410k 140k 190k 400k 180k 220k 360k 370k 850k 220k 760k 300k 320k 450k 300k 280k 450k 220k 270k 390k 430k Table 1: Performance Overview One might argue that improvements at very small data sizes are not relevant, as the translations will still be very deficient. This might be the case, but there are applications where even a low-quality translation can be helpful ([17]). And as we showed in [8] - some translations are surprisingly good, even for very small amounts of training data. 6. Future Work The presented weighting schemes could certainly incorporate other features of the original training data. The pure frequency based approach “tries” to cover every n gram once and then does not consider it anymore. It might be helpful to have a goal of covering every n-gram a number of times to get better estimates of translation probabilities. The TF-IDF based sorting did not yet show improvements over the earlier approaches. We hope that it will be beneficial to f"
2005.mtsummit-papers.30,takezawa-etal-2002-toward,0,0.0872405,"nces are more difficult for the training of statistical translation models. (When training the translation model IBM1, for example, every possible word alignment between sentences is considered.) 5 5.1 To fix these shortcomings, we changed the weighting terms to incorporate the actual length of a sentence by dividing the number of unseen n-grams by the length of the sentence (in words): Experiments English-Spanish Test and Training Data The training data for the first translations consisted of 123,416 English sentences with 903,525 English words (tokens). This data is part of the BTEC corpus (Takezawa et al., 2002) with relatively simple sentences from the travel domain. The whole training data was also available in Spanish (852,362 words). The testing data which was used to measure the machine translation performance consisted of 500 lines of data from the medical domain. All translations in this task were done translating English to Spanish. j weight j ( sentence) = ∑ # (unseen n − grams) ∑ # (unseen n − grams) n =1 sentence This changes the weight to – informally speaking – “new n-grams per word to translate."" As noted earlier, the algorithms for training translation models in statistical machine tra"
2005.mtsummit-papers.30,2004.iwslt-evaluation.11,1,0.910197,"d on N-gram Coverage Matthias Eck, Stephan Vogel and Alex Waibel Interactive Systems Laboratories Carnegie Mellon University Pittsburgh, PA, 15213, USA matteck@cs.cmu.edu, vogel+@cs.cmu.edu, waibel@cs.cmu.edu Statistical machine translation searches for the best target sentence from the space defined by the target language model and the translation model. Statistical translation models are usually either phrase- or word-based and include most notably IBM1 to IBM4 and HMM (Brown et al., 1993; Vogel et al., 1996; Vogel et al., 2003). Some recent developments focused on online phrase extraction (Vogel et al., 2004). All models use available bilingual training data in the source and target languages to estimate their parameters and approximate the translation probabilities. One of the main problems of Statistical Machine Translation is the necessity to have large parallel corpora available. This might not be a big issue for major languages, but it certainly is a problem for languages with less resources. To improve the data situation for these languages, it is necessary to hire human translators at enormous costs who translate corpora that can later be used to train statistical machine translation system"
2005.mtsummit-papers.30,C96-2141,1,0.68059,"Missing"
2005.mtsummit-papers.30,J93-2003,0,0.0094299,"Missing"
2005.mtsummit-papers.30,P05-1032,0,0.0461345,"Missing"
2005.mtsummit-papers.30,W01-1409,0,0.043825,"o term weight0,j. We notice the same lower scores after about translating 200,000 words and a score recovery towards the end. The optimizations based on uni- and bigrams and uni-, bi- and trigrams are clearly improved compared to weight0,j. We also do not see any significant differences between the optimization based on uni- plus bigrams and the optimization incorporating These problems are clearly fixed by incorporating the bi- and trigrams into the optimization process. The scores no longer fall 231 be the case, but there are applications where even a low quality translation can be helpful (Germann, 2001). Besides that, some translations are surprisingly good and show considerable improvements over the baseline system (Table 1). The first sentence especially demonstrates the improved coverage; here for the words “heart”, “beating” and “normally”. The word “beating” is unfortunately not correctly translated but the final result is still much better than the translation of the baseline system. trigrams, too. The performance is very similar with slight advantages for the optimization based on uni- and bigrams only. In this case a NIST score of 4.0 was already reached at 170,000 translated words w"
2005.mtsummit-papers.30,2005.eamt-1.19,1,0.848924,"y et al., 2000). 2.2 3 In general this research can be regarded as an example of active learning. This means the machine learning algorithm does not just passively train on the available training data but plays an active role in selecting the best training data. Active learning, as a standard method in machine learning, has been applied to a variety of problems in natural language processing, for example to parsing (Hwa, 2004) and to automatic speech recognition (Kamm and Meyer, 2002). It is important to note the difference between this approach and approaches to Translation Model Adaptation (Hildebrand et al., 2005) or simple subsampling techniques that are based on the actual test data. Here, we assume that the test data is not known at selection time, so the intention is to get the best possible translation system for every possible test data. Application 2: Translation on Small Devices Another possible application is the usage of statistical machine translation on portable small devices like PDAs or cell phones. Those devices tend to have a limited amount of memory available which limits the size of the models the device can actually hold and a larger training corpus will usually result in a larger mo"
2005.mtsummit-papers.30,J04-3001,0,0.0636434,"nslation effort, a considerable amount of money could be saved. This could especially be applied to low density languages with limited resources (compare Lavie et al., 2004; McEnery et al., 2000). 2.2 3 In general this research can be regarded as an example of active learning. This means the machine learning algorithm does not just passively train on the available training data but plays an active role in selecting the best training data. Active learning, as a standard method in machine learning, has been applied to a variety of problems in natural language processing, for example to parsing (Hwa, 2004) and to automatic speech recognition (Kamm and Meyer, 2002). It is important to note the difference between this approach and approaches to Translation Model Adaptation (Hildebrand et al., 2005) or simple subsampling techniques that are based on the actual test data. Here, we assume that the test data is not known at selection time, so the intention is to get the best possible translation system for every possible test data. Application 2: Translation on Small Devices Another possible application is the usage of statistical machine translation on portable small devices like PDAs or cell phones"
2005.mtsummit-papers.30,2004.eamt-1.14,1,0.843763,"ately 0.10-0.25 USD per word depending on the involved languages and the difficulty of the text. The translation of a 1 million word corpus would then cost between 100,000 and 250,000 USD. The concept here is to select the most important sentences from the original 1 million word corpus and have only those translated by the human translators. If it would still be possible to get a similar translation performance with a significantly lower translation effort, a considerable amount of money could be saved. This could especially be applied to low density languages with limited resources (compare Lavie et al., 2004; McEnery et al., 2000). 2.2 3 In general this research can be regarded as an example of active learning. This means the machine learning algorithm does not just passively train on the available training data but plays an active role in selecting the best training data. Active learning, as a standard method in machine learning, has been applied to a variety of problems in natural language processing, for example to parsing (Hwa, 2004) and to automatic speech recognition (Kamm and Meyer, 2002). It is important to note the difference between this approach and approaches to Translation Model Adap"
2005.mtsummit-papers.30,mcenery-etal-2000-corpus,0,0.233194,"Missing"
2005.mtsummit-papers.30,P02-1040,0,0.071487,"Missing"
2005.mtsummit-papers.30,2005.eamt-1.39,1,\N,Missing
2006.eamt-1.12,W05-0909,0,0.182464,"Missing"
2006.eamt-1.12,W05-0900,0,0.214133,"Missing"
2006.eamt-1.12,niessen-etal-2000-evaluation,0,0.194064,"Missing"
2006.eamt-1.12,P02-1040,0,0.0945405,"Missing"
2006.eamt-1.12,2004.tmi-1.9,1,0.83745,"Missing"
2006.eamt-1.12,2005.iwslt-1.1,1,\N,Missing
2006.eamt-1.12,2004.iwslt-evaluation.1,0,\N,Missing
2006.iwslt-evaluation.19,P02-1040,0,0.0877239,"irections, phrase-level fertility scores in both directions (via DP as in section 3.1.3), The IBM-1 Viterbi scores in both directions, un-normalized scores in both directions Q IBM-1 lexicon Q P (fjj+l |eii+k ) = j ′ ∈[j,j+l] i′ ∈[i,i+k] P (fj ′ |ei′ ) (favoring long phrase-pairs, see [2]), the phrase-level normalized frequency, and the normalized number of alignment links within the phrase-pair (as in section 3.1.3). 4. Translation Results This section gives an overview of all official and contrastive results achieved by the UKA/CMU translation systems. All results will only be given in BLEU([9]) and NIST([10]) scores according to the official evaluation specifications (mixed case with punctuation marks). For other scores for the submitted systems please refer to the official scoring publication of IWSLT 2006. Submissions were done for all language pairs in the Open and C-STAR data conditions. We always translated ASR output (as 1-best) and as a comparison the correct recognition results (CRR). The development set numbers always refer to the development set that was also provided for the evaluation campaign for IWSLT 2006. 4.1. Data Conditions For each language pair 20,000 or 40,000"
2006.iwslt-evaluation.19,2005.mtsummit-papers.33,1,0.840167,"lation system that was used is basically the same in both places. It has to be seen as a combined effort of a close-knit research group [1],[2]. 3. Translation System 3.1. Phrase Alignment 3.1.1. Overview We used two phrase alignment methods for IWSLT 2006, namely PESA and LogLin. The LogLin phrase extraction method is computationally intensive so it was not used for all submitted systems. More extensive experiments with contrastive systems showed that LogLin generally improves the results compared to the PESA phrase extraction method. 3.1.2. PESA: Phrase pair extraction as sentence splitting [3] The PESA phrase extraction method is based on the well known IBM-1 word alignment model [4]. The IBM-1 model assigns a probability to all possible word alignments of respective sentences in the training data. Assuming a sentence in the bilingual corpus contains a phrase from a source sentence eii21 = ei1 ...ei2 we are inter130 ested in the sequence of words fjj12 = fj1 ...fj2 from the respective target sentence that is the optimal translation for this source phrase. We can now estimate the quality of a translation candidate by using the IBM-1 word alignment probabilities between the source an"
2006.iwslt-evaluation.19,J93-2003,0,0.0164133,"bined effort of a close-knit research group [1],[2]. 3. Translation System 3.1. Phrase Alignment 3.1.1. Overview We used two phrase alignment methods for IWSLT 2006, namely PESA and LogLin. The LogLin phrase extraction method is computationally intensive so it was not used for all submitted systems. More extensive experiments with contrastive systems showed that LogLin generally improves the results compared to the PESA phrase extraction method. 3.1.2. PESA: Phrase pair extraction as sentence splitting [3] The PESA phrase extraction method is based on the well known IBM-1 word alignment model [4]. The IBM-1 model assigns a probability to all possible word alignments of respective sentences in the training data. Assuming a sentence in the bilingual corpus contains a phrase from a source sentence eii21 = ei1 ...ei2 we are inter130 ested in the sequence of words fjj12 = fj1 ...fj2 from the respective target sentence that is the optimal translation for this source phrase. We can now estimate the quality of a translation candidate by using the IBM-1 word alignment probabilities between the source and target phrases. If the candidate is actually a good translation of the source phrase we ex"
2006.iwslt-evaluation.19,W05-0825,1,0.839004,"j1 and j2 we can determine the optimal sentence splitting and the best translation candidate. The same ideas can be applied if we use the IBM-1 probabilities for the reverse direction thus calculating pj1 ,j2 (f |e) and we interpolate the two phrase alignment probabilities to get the optimal translation candidate. In the actual system we not only use the top translation candidate but all candidates to a certain threshold. This covers translation alternatives and leaves the final decision to other models, mainly the language model. 3.1.3. LogLin: Phrase pair extraction with Log-Linear Features [5],[6] Another phrase extraction method applied was LogLin. LogLin formulates the phrase-extraction problem as a local search, guided by a simple heuristic function. Given the source n-gram ei+k (span from position i to i position i+k, with a length of k+1), the local search starts by first localizing the projected center of the target phrase, and then it searches the best scored width, corresponding to the left- and right- boundaries (j, j+l) of the target phrase: fjj+l . Previously the heuristic function included phrase-level fertility score, a simple IBM-1 lexicon score, and a phraselevel pos"
2006.iwslt-evaluation.19,2005.eamt-1.19,1,0.837963,"Full BTEC + Travel Books + 4xSupplied Data Scores (BLEU & NIST) ASR spont. 0.1622 5.1865 ASR read 0.1645 5.2372 CRR 0.2057 6.0548 8.2301 6.6064 7.6730 8.5923 7.0812 8.1408 Table 7: Contrastive Results for Italian → English 4.5. Chinese → English 4.5.1. Training Corpora For Chinese → English 40,000 lines of data were supplied. The Full BTEC data for this language pair is complete at about 160,000 sentences. In addition we added 106,826 lines of data that was gathered from freely available bilingual newswire data using the test set of IWSLT 2005 as a query. We applied the technique described in [12]. Also the monolingual travel books were added to the language model for the C-STAR data track. Data Condition Supplied Full BTEC IR data Travel books #lines 39,953 163,326 106,826 31,388 #wordsChinese/English 351,060/306,149 1,008,568/954,591 1,838,597/1,871,748 English: 255,534 Table 9: Official submissions Chinese → English 4.5.4. Contrastive Results For the contrastive results we first investigated the impact of the LogLin phrase alignment model. The results in table 10 clearly show that for the Supplied Data and BTEC data the LogLin model shows consistent improvements, especially regardin"
2006.iwslt-evaluation.19,2005.mtsummit-papers.30,1,0.831989,"131 0.2434 0.1830 0.2009 5.7787 6.4009 5.5749 5.6201 Table 15: Contrastive Results for Japanese → English Full BTEC data PESA Dev Set (ASR) 0.1938 5.5076 Dev Set (CRR) 0.2222 6.2152 Test Set (ASR) 0.1850 5.4349 Test Set (CRR) 0.2045 5.8719 Full BTEC + any available data (C-STAR track) PESA Dev Set (ASR) 0.2001 5.5914 Dev Set (CRR) 0.2289 5.3980 Test Set (ASR) 0.1841 5.3980 Test Set (CRR) 0.2007 5.8584 Table 16: Contrastive Results for Japanese → English “informative” and not to contain too much repetition given the already available translations. This was done using the technique presented in [14]. The 55,000 lines of Italian → English data were produced using a similar method. This means that the bare number of lines underestimates the impact of these additional translations in comparison to the impact the Full BTEC corpus of 160,000 lines would have. This means that the relative increase in training data (especially useful training data) is probably larger for the Italian → English and Arabic → English systems than for the Japanese → English and Chinese → English systems. Concerning the two investigated phrase extraction methods PESA and LogLin we notice the general trend that LogLin"
2006.iwslt-evaluation.19,I05-3011,1,0.762827,"nd j2 we can determine the optimal sentence splitting and the best translation candidate. The same ideas can be applied if we use the IBM-1 probabilities for the reverse direction thus calculating pj1 ,j2 (f |e) and we interpolate the two phrase alignment probabilities to get the optimal translation candidate. In the actual system we not only use the top translation candidate but all candidates to a certain threshold. This covers translation alternatives and leaves the final decision to other models, mainly the language model. 3.1.3. LogLin: Phrase pair extraction with Log-Linear Features [5],[6] Another phrase extraction method applied was LogLin. LogLin formulates the phrase-extraction problem as a local search, guided by a simple heuristic function. Given the source n-gram ei+k (span from position i to i position i+k, with a length of k+1), the local search starts by first localizing the projected center of the target phrase, and then it searches the best scored width, corresponding to the left- and right- boundaries (j, j+l) of the target phrase: fjj+l . Previously the heuristic function included phrase-level fertility score, a simple IBM-1 lexicon score, and a phraselevel positio"
2006.iwslt-evaluation.20,P05-1033,0,0.0890792,"x Augmented Machine Translation System that was used in the IWSLT-06 evaluation campaign. We participated in the C-Star data track using only the Full BTEC corpus, for Chinese-English translation, focusing on transcript translation. We applied techniques that produce true-cased, punctuated translations from non-punctuated Chinese transcripts, generating translations which score higher against the Official metric than against the lower-cased, punctuation removed metric. Our results demonstrate the impact of syntax and hierarchy based models for speech transcript translation. 1. Introduction As [1] and [2] note, phrase-based models suffer from sparse data effects when required to translate conceptual elements that span or skip across several words, and distortion based re-ordering techniques tend to limit their range of operation for reasons of efficiency and model strength [3]. Syntax driven [4], [5] and hierarchical translation models [1] model structured re-ordering constraints and extend the domain of locality in the decoding process. Systems such as [6] and [7] demonstrate effective decoding using these models. For the IWSLT-06 evaluation, we applied syntax augmented translation as"
2006.iwslt-evaluation.20,P04-1083,0,0.0414364,"non-punctuated Chinese transcripts, generating translations which score higher against the Official metric than against the lower-cased, punctuation removed metric. Our results demonstrate the impact of syntax and hierarchy based models for speech transcript translation. 1. Introduction As [1] and [2] note, phrase-based models suffer from sparse data effects when required to translate conceptual elements that span or skip across several words, and distortion based re-ordering techniques tend to limit their range of operation for reasons of efficiency and model strength [3]. Syntax driven [4], [5] and hierarchical translation models [1] model structured re-ordering constraints and extend the domain of locality in the decoding process. Systems such as [6] and [7] demonstrate effective decoding using these models. For the IWSLT-06 evaluation, we applied syntax augmented translation as per [7] to the speech translation task, using the only Full BTEC corpus to model translational equivalence. We begin by extracting lexical phrases as per [2] as the basis for our syntax augmented translation rules. We annotate and generalize these phrases by parsing the target side of the training data with"
2006.iwslt-evaluation.20,W06-1606,0,0.0327353,"ric. Our results demonstrate the impact of syntax and hierarchy based models for speech transcript translation. 1. Introduction As [1] and [2] note, phrase-based models suffer from sparse data effects when required to translate conceptual elements that span or skip across several words, and distortion based re-ordering techniques tend to limit their range of operation for reasons of efficiency and model strength [3]. Syntax driven [4], [5] and hierarchical translation models [1] model structured re-ordering constraints and extend the domain of locality in the decoding process. Systems such as [6] and [7] demonstrate effective decoding using these models. For the IWSLT-06 evaluation, we applied syntax augmented translation as per [7] to the speech translation task, using the only Full BTEC corpus to model translational equivalence. We begin by extracting lexical phrases as per [2] as the basis for our syntax augmented translation rules. We annotate and generalize these phrases by parsing the target side of the training data with the Stanford Parser [8] (trained on the Penn Treebank). Our results indicate steady improvements as we introduce hierarchy and syntax into the translation proc"
2006.iwslt-evaluation.20,W06-3119,1,0.82497,"results demonstrate the impact of syntax and hierarchy based models for speech transcript translation. 1. Introduction As [1] and [2] note, phrase-based models suffer from sparse data effects when required to translate conceptual elements that span or skip across several words, and distortion based re-ordering techniques tend to limit their range of operation for reasons of efficiency and model strength [3]. Syntax driven [4], [5] and hierarchical translation models [1] model structured re-ordering constraints and extend the domain of locality in the decoding process. Systems such as [6] and [7] demonstrate effective decoding using these models. For the IWSLT-06 evaluation, we applied syntax augmented translation as per [7] to the speech translation task, using the only Full BTEC corpus to model translational equivalence. We begin by extracting lexical phrases as per [2] as the basis for our syntax augmented translation rules. We annotate and generalize these phrases by parsing the target side of the training data with the Stanford Parser [8] (trained on the Penn Treebank). Our results indicate steady improvements as we introduce hierarchy and syntax into the translation process desp"
2006.iwslt-evaluation.20,P03-1054,0,0.00574076,"nslation models [1] model structured re-ordering constraints and extend the domain of locality in the decoding process. Systems such as [6] and [7] demonstrate effective decoding using these models. For the IWSLT-06 evaluation, we applied syntax augmented translation as per [7] to the speech translation task, using the only Full BTEC corpus to model translational equivalence. We begin by extracting lexical phrases as per [2] as the basis for our syntax augmented translation rules. We annotate and generalize these phrases by parsing the target side of the training data with the Stanford Parser [8] (trained on the Penn Treebank). Our results indicate steady improvements as we introduce hierarchy and syntax into the translation process despite the domain mismatch with the English parser. As defined in the evaluation’s Official Specifications, translations are evaluated considering case and punctuation markers, artifacts not typically present in Chinese ASR transcripts. We incorporate the generation of punctuation directly into the translation process, learning translation rules that represent case and punctuation decisions. Our Official evaluation results demonstrate the effectiveness of"
2006.iwslt-evaluation.20,P03-1021,0,0.0307265,"ctuation decisions. Our Official evaluation results demonstrate the effectiveness of these approaches; our submission achieved higher performance when the system output was evaluated with punctuation and case than in the lower-cased, punctuation-free evaluation. This result is especially relevant considering the presence of multiple sentences (which should be punctuation-separated in English) within each speech transcript utterance. We begin by summarizing the syntax augmented rule extraction process from [7] and the decoding settings used to train the model parameters to maximize performance [9] on the BLEU metric [10]. We provide a detailed description of the data-processing used to generate our evaluation submission, and demonstrate the impact of syntax for the IWSLT-06 speech translation task. 2. Syntax Augmented Translation Traditional phrase-based translations serves as the lexical foundation for the syntactic synchronous grammar (SynCFG) presented in [7]. Syntactic, since its nonterminals are syntactic categories derived from parsing the target (English) side of the parallel training corpus, and synchronous because they define operations to derive the source and target language"
2006.iwslt-evaluation.20,P02-1040,0,0.0723637,"Official evaluation results demonstrate the effectiveness of these approaches; our submission achieved higher performance when the system output was evaluated with punctuation and case than in the lower-cased, punctuation-free evaluation. This result is especially relevant considering the presence of multiple sentences (which should be punctuation-separated in English) within each speech transcript utterance. We begin by summarizing the syntax augmented rule extraction process from [7] and the decoding settings used to train the model parameters to maximize performance [9] on the BLEU metric [10]. We provide a detailed description of the data-processing used to generate our evaluation submission, and demonstrate the impact of syntax for the IWSLT-06 speech translation task. 2. Syntax Augmented Translation Traditional phrase-based translations serves as the lexical foundation for the syntactic synchronous grammar (SynCFG) presented in [7]. Syntactic, since its nonterminals are syntactic categories derived from parsing the target (English) side of the parallel training corpus, and synchronous because they define operations to derive the source and target language simultaneously. We trai"
2006.iwslt-evaluation.20,P99-1039,0,0.0211737,"lignment file. Calling phrase-extract (a binary provided in the workshop tools) with these marked files produces a phrase table with separators between each set of phrases extracted from a sentence. For the set of phrases extracted for each sentence, we consider the corresponding parse tree for the whole target sentence. For each source phrase-pair f1 . . . fm /e1 . . . en , we annotate this phrase-pair with the constituent that spans e1 . . . en in the target side parse tree. As an extension to the nonterminal set provided by the Penn Treebank, we consider the CCG nonterminal set proposed by [11]. Under this approach, rules can be assigned partially formed categories, like DT N P , indicating a constituent that forms a noun-phrase, but is missing its determiner at the left. [12] demonstrates the importance of considering phrases not corresponding to pure syntactic constituents in translation, and in [13], we demonstrate the value of using extended categories in our translation system. If neither a Penn Treebank or CCG constituent can be found for e1 . . . en , we associate a generic nonterminal symbol _X with this rule, allowing it to still take part in hierarchically motivated synch"
2006.iwslt-evaluation.20,koen-2004-pharaoh,0,0.137793,"a sentence. For the set of phrases extracted for each sentence, we consider the corresponding parse tree for the whole target sentence. For each source phrase-pair f1 . . . fm /e1 . . . en , we annotate this phrase-pair with the constituent that spans e1 . . . en in the target side parse tree. As an extension to the nonterminal set provided by the Penn Treebank, we consider the CCG nonterminal set proposed by [11]. Under this approach, rules can be assigned partially formed categories, like DT N P , indicating a constituent that forms a noun-phrase, but is missing its determiner at the left. [12] demonstrates the importance of considering phrases not corresponding to pure syntactic constituents in translation, and in [13], we demonstrate the value of using extended categories in our translation system. If neither a Penn Treebank or CCG constituent can be found for e1 . . . en , we associate a generic nonterminal symbol _X with this rule, allowing it to still take part in hierarchically motivated synchronous derivations. The set of rules resulting from the process above is more than 10 times the size of the initial lexical rules / phrase pairs used to create them. As an initial pruning"
2006.iwslt-evaluation.20,N06-1001,0,0.0236721,"umeric or ordinal terms in the development data, which typically lead to unreliable language model estimates (due to the sparsity in encountered numeric forms). We handle source and target number forms as described below. 4.1. Case Information Word alignment and subsequent phrase and rule extraction rely on well estimated lexical probabilities; a requirement that is often strained in limited data scenarios with true-cased data. Common solutions include training translation and language models on lower-cased text, generating system output in lower-case and then applying a true-case system like [16], or training all models in true-case. We take a intermediate approach, based on the premise that the real source of sparsity in the direct true-case approach comes from the first word of the sentence being upper-cased due to its position. Upper-case words used within the sentence (rather than in the first position) tend to be consistently cased across the corpus (a notable exception being “May” vs “may”). For each first word of the target side of the corpus we estimate its most common case in the corpus (ignoring first word occurrences). If the most common case is lower, we lower case the wor"
2006.iwslt-evaluation.20,J04-4002,0,\N,Missing
2006.iwslt-evaluation.20,N03-1017,0,\N,Missing
2007.iwslt-1.4,W06-3711,0,0.0292572,"able speech translation system which allows an English speaker to converse with a target language speaker. Our systems have been evaluated on a regular basis as part of the DARPA TransTac program. These evaluations are run by NIST, and involve military users and target language users who have never used our system before. The evaluations consist of communicating through the translation device for a number of pre-designed scenarios (which were previously unknown to us). The tests take place both indoors and outdoors. Other systems in the TransTac program include those developed by BBN [5], IBM [6], SRI, Sehda/Fluential, and USC [7] [8]. 2. Challenges The two target languages in the TransTac program are Iraqi Arabic and Farsi. Iraqi Arabic is defined as the spoken form of Arabic used by the people of Iraq in everyday conversations. It is distinct from the formal Modern Standard Arabic (MSA) used in written communication. As Iraqi Arabic is normally not written, even with transcription conventions there is greater variability in the spelling conventions than in a standard written language. Farsi (Persian), mainly spoken in Iran and areas of Afghanistan, also uses the Arabic script, thoug"
2007.iwslt-1.4,2005.mtsummit-papers.33,1,0.758733,"ple raftin vs. raftid ""you went"". The word forms (inside of the word) may be modified to represent their colloquial pronunciation for instance khune vs. khAne 'house', midam vs. midaham 'i give'). 5.4. Language models The language model is a standard 6-gram language model with Good-Turing smoothing implemented as a suffix array (SA LM) [10]. Another option of language model is the 4-gram modified Kneser-Ney smoothing trained using the SRI language modeling toolkit (SRI LM) [11]. 5.5. Translation models 5.5.1. PESA phrase extraction In Iraqi-English we applied the PESA phrase extraction method [9]. For a given source phrase PESA tries to find the optimal sentence splits of the training sentences containing this source phrase based on inner and outer IBM1 word alignment probabilities. We applied PESA as an online phrase extraction which means that phrase pairs are dynamically extracted from the training data as needed during the translation of the test set. We compared the performance here with a standard Pharaoh phrase table but we saw considerable improvements using the PESA approach. For Iraqi-English a considerable amount of training data is available and parts of the test dialogs a"
2007.iwslt-1.4,2005.eamt-1.39,1,0.834249,"lization steps need to be agreed upon. However, it is not easy to reach a consensus since Iraqi Arabic lacks a standard writing system. Furthermore, there are issues with speaking style. Words can be used with their formal or informal/colloquial endings for example raftin vs. raftid ""you went"". The word forms (inside of the word) may be modified to represent their colloquial pronunciation for instance khune vs. khAne 'house', midam vs. midaham 'i give'). 5.4. Language models The language model is a standard 6-gram language model with Good-Turing smoothing implemented as a suffix array (SA LM) [10]. Another option of language model is the 4-gram modified Kneser-Ney smoothing trained using the SRI language modeling toolkit (SRI LM) [11]. 5.5. Translation models 5.5.1. PESA phrase extraction In Iraqi-English we applied the PESA phrase extraction method [9]. For a given source phrase PESA tries to find the optimal sentence splits of the training sentences containing this source phrase based on inner and outer IBM1 word alignment probabilities. We applied PESA as an online phrase extraction which means that phrase pairs are dynamically extracted from the training data as needed during the t"
2007.iwslt-1.4,2005.eamt-1.36,1,0.72462,"ar in the training corpus, because they occur in the phrase table only embedded in longer phrases. This leads to an unnecessary high number of untranslated words. On the other side, the PESA phrase alignment will generate translations for all n-grams including all individual words, which can be found in the training corpus. To guarantee that the phrase table can cover all source vocabulary and to leverage the PESA’s strength in arbitrary long matching, we trained two phrase tables and interpolated them. The interpolation parameters are optimized through a minimum-error-rate training framework [12]. 5.5.4. Speed constraint To limit delays, the translation has to be performed during the replay of the ASR output. This has to be the case for even very long sentences. For all practical considerations we assume to have about 200 ms on average to do the translation. Some of speeding strategies we applied is phrase table pruning and restrict the search space during the decoding process. Those techniques help to decrease the system running time significantly. 5.5.5. Decoder For this evaluation the system is running on a standard laptop with 2 GB of memory so we could use our regular decoder [2]"
2007.iwslt-1.4,2007.mtsummit-papers.72,1,0.769394,"ery long sentences. For all practical considerations we assume to have about 200 ms on average to do the translation. Some of speeding strategies we applied is phrase table pruning and restrict the search space during the decoding process. Those techniques help to decrease the system running time significantly. 5.5.5. Decoder For this evaluation the system is running on a standard laptop with 2 GB of memory so we could use our regular decoder [2]. The previous system described in [1] was running on a PDA. Due to lack of memory and computing power an earlier version of the decoder described in [18] had to be used that did not support word reordering and required heavily pruned models. 5.5.6. Translation results We report the performance of translation component in terms of BLEU score [20]. On the test sets the system achieved a score of 42.12 for English to Iraqi and 63.49 for Iraqi to English. The Farsi systems use similar technologies as the Iraqi systems. Table 8 shows the translation performance of the provided training data on various setups. Table 8: Farsi translation performance (in BLEU) Farsi→English Dev. Pharaoh + 4-gram SRI LM 24.64 PESA + 6-gram SA LM 23.06 English→Farsi Pha"
2007.iwslt-1.4,koen-2004-pharaoh,0,0.0298013,"online phrase extraction does not have to extract the phrases pairs dynamically. Instead, the online phrase extraction is only used for long or rarely seen phrases. This did not give any significant change in performance but resulted in a considerable speedup. The system uses the same corpora to extract online PESA phrases for both translation directions so we combined the Iraqi-English and English-Iraqi corpora for this. However, the pre-extracted phrases were extracted separately for each direction from the respective corpus. 5.5.3. Interpolate Pharaoh and PESA We observed that the Pharaoh [19] phrase table does not contain entries for all words in the source vocabulary. This comes from the heuristics applied to avoid unlikely translations. Therefore, some words will not be translated, even though they appear in the training corpus, because they occur in the phrase table only embedded in longer phrases. This leads to an unnecessary high number of untranslated words. On the other side, the PESA phrase alignment will generate translations for all n-grams including all individual words, which can be found in the training corpus. To guarantee that the phrase table can cover all source v"
2007.iwslt-1.4,P02-1040,0,0.073748,"the search space during the decoding process. Those techniques help to decrease the system running time significantly. 5.5.5. Decoder For this evaluation the system is running on a standard laptop with 2 GB of memory so we could use our regular decoder [2]. The previous system described in [1] was running on a PDA. Due to lack of memory and computing power an earlier version of the decoder described in [18] had to be used that did not support word reordering and required heavily pruned models. 5.5.6. Translation results We report the performance of translation component in terms of BLEU score [20]. On the test sets the system achieved a score of 42.12 for English to Iraqi and 63.49 for Iraqi to English. The Farsi systems use similar technologies as the Iraqi systems. Table 8 shows the translation performance of the provided training data on various setups. Table 8: Farsi translation performance (in BLEU) Farsi→English Dev. Pharaoh + 4-gram SRI LM 24.64 PESA + 6-gram SA LM 23.06 English→Farsi Pharaoh + SRI LM 10.07 PESA + SA LM 9.45 Pharaoh + SA LM 10.41 Pharaoh + PESA + SA LM 10.23 Unseen 23.3 19.9 14.87 14.67 15.42 16.44 6. Text-to-Speech Text-to-speech was provided by Cepstral, LLC's"
2007.iwslt-1.4,2005.iwslt-1.16,0,\N,Missing
2007.iwslt-1.4,2005.iwslt-1.6,1,\N,Missing
2007.iwslt-1.9,J93-2003,0,0.0119348,"Missing"
2007.iwslt-1.9,P03-1021,0,0.0132789,"hood, given: eˆI1 = arg max P (eI1 |f1J ) eI1 exp = arg max P eI1 e′ I1 ′ P exp M m=1 P λm hm eI1 , f1J M m=1  λm hm e′ I1 , f1J ′  In this framework, the posterior probability P (eI1 |f1J ) is directly maximized using  a log-linear combination of feature functions hm eI1 , f1J (during decoding the denominator is dropped since it depends only on f1J ). Feature functions applied during translation include: language models, translation models, and sentence length models. The scaling factors (λ1 , λ2 , . . . , λM ) applied during search are optimized via MERT (minimum error rate training) [3] for a specific translation metric such as BLEU [4]. Search is performed using our STTK beam-search-decoder [5] which allows restricted word re-ordering during translation. 3. Topic-Aware Japanese-to-English SLT For the Japanese-to-English submission we focused on two research areas. First, we compared various methods to recover intra-utterance sentence boundaries and secondary punctuation (commas); and second, we investigated approaches to incorporate topic-knowledge into the SMT framework. These works are described in Sections 3.3 and 3.4, respectively. By incorporating publicly available co"
2007.iwslt-1.9,P02-1040,0,0.0730825,"arg max P eI1 e′ I1 ′ P exp M m=1 P λm hm eI1 , f1J M m=1  λm hm e′ I1 , f1J ′  In this framework, the posterior probability P (eI1 |f1J ) is directly maximized using  a log-linear combination of feature functions hm eI1 , f1J (during decoding the denominator is dropped since it depends only on f1J ). Feature functions applied during translation include: language models, translation models, and sentence length models. The scaling factors (λ1 , λ2 , . . . , λM ) applied during search are optimized via MERT (minimum error rate training) [3] for a specific translation metric such as BLEU [4]. Search is performed using our STTK beam-search-decoder [5] which allows restricted word re-ordering during translation. 3. Topic-Aware Japanese-to-English SLT For the Japanese-to-English submission we focused on two research areas. First, we compared various methods to recover intra-utterance sentence boundaries and secondary punctuation (commas); and second, we investigated approaches to incorporate topic-knowledge into the SMT framework. These works are described in Sections 3.3 and 3.4, respectively. By incorporating publicly available corpora from related domains and applying the propose"
2007.iwslt-1.9,2003.mtsummit-papers.53,1,0.904427,"λm hm e′ I1 , f1J ′  In this framework, the posterior probability P (eI1 |f1J ) is directly maximized using  a log-linear combination of feature functions hm eI1 , f1J (during decoding the denominator is dropped since it depends only on f1J ). Feature functions applied during translation include: language models, translation models, and sentence length models. The scaling factors (λ1 , λ2 , . . . , λM ) applied during search are optimized via MERT (minimum error rate training) [3] for a specific translation metric such as BLEU [4]. Search is performed using our STTK beam-search-decoder [5] which allows restricted word re-ordering during translation. 3. Topic-Aware Japanese-to-English SLT For the Japanese-to-English submission we focused on two research areas. First, we compared various methods to recover intra-utterance sentence boundaries and secondary punctuation (commas); and second, we investigated approaches to incorporate topic-knowledge into the SMT framework. These works are described in Sections 3.3 and 3.4, respectively. By incorporating publicly available corpora from related domains and applying the proposed techniques the translation accuracy of our system improved"
2007.iwslt-1.9,2005.mtsummit-papers.33,1,0.822923,"Missing"
2007.iwslt-1.9,P03-1010,0,0.0472683,"Missing"
2007.iwslt-1.9,2006.iwslt-evaluation.12,0,0.057142,"Missing"
2007.iwslt-1.9,W06-3119,1,0.869494,"well structured target language output under the premise that human language is essentially hierarchical in its generation. Hierarchical approaches gain their representational power by allowing transformation rules to condition on larger fragments of target language tree structure. The application of hierarchically structured models to statistical machine translation requires the development of techniques to induce and estimate transformation rules from parallel data (grammar induction), and efficient algorithms to apply these rules to translate source language text (decoding). In recent work [11], we presented the first results that leverage target language syntactic structure to achieve higher performance than comparable phrase based translation. [12] presents results that show the impact of hierarchical structure alone, and [13] achieves significant improvements using treeto-string transformations. For the Chinese-to-English task, we used the latest version of the Syntax-Augmented Machine Translation (SAMT) system first described in [11]. The system is available opensource under the GNU General Public License at: www.cs.cmu.edu/˜zollmann/samt 4.1. Synchronous Grammars for SMT Probab"
2007.iwslt-1.9,P05-1033,0,0.0721247,"representational power by allowing transformation rules to condition on larger fragments of target language tree structure. The application of hierarchically structured models to statistical machine translation requires the development of techniques to induce and estimate transformation rules from parallel data (grammar induction), and efficient algorithms to apply these rules to translate source language text (decoding). In recent work [11], we presented the first results that leverage target language syntactic structure to achieve higher performance than comparable phrase based translation. [12] presents results that show the impact of hierarchical structure alone, and [13] achieves significant improvements using treeto-string transformations. For the Chinese-to-English task, we used the latest version of the Syntax-Augmented Machine Translation (SAMT) system first described in [11]. The system is available opensource under the GNU General Public License at: www.cs.cmu.edu/˜zollmann/samt 4.1. Synchronous Grammars for SMT Probabilistic synchronous context-free grammars (PSCFGs) are defined by a source terminal set (source vocabulary) TS , a target terminal set (target vocabulary) TT ,"
2007.iwslt-1.9,W06-1606,0,0.0261321,"ragments of target language tree structure. The application of hierarchically structured models to statistical machine translation requires the development of techniques to induce and estimate transformation rules from parallel data (grammar induction), and efficient algorithms to apply these rules to translate source language text (decoding). In recent work [11], we presented the first results that leverage target language syntactic structure to achieve higher performance than comparable phrase based translation. [12] presents results that show the impact of hierarchical structure alone, and [13] achieves significant improvements using treeto-string transformations. For the Chinese-to-English task, we used the latest version of the Syntax-Augmented Machine Translation (SAMT) system first described in [11]. The system is available opensource under the GNU General Public License at: www.cs.cmu.edu/˜zollmann/samt 4.1. Synchronous Grammars for SMT Probabilistic synchronous context-free grammars (PSCFGs) are defined by a source terminal set (source vocabulary) TS , a target terminal set (target vocabulary) TT , a shared nonterminal set N and induce rules of the form X → hγ, α, ∼, wi 3.5. T"
2007.iwslt-1.9,N04-1035,0,0.024464,"om nonterminal tokens in γ to nonterminal tokens in α, and S qqMMMMM q q MM qqq VP qMMMM q q MMM q q qq VB PRN AUX RB NP • w ∈ [0, ∞) is a nonnegative real-valued weight assigned to the rule. In our notation, we will assume ∼ to be implicitly defined by indexing the NT occurrences in γ from left to right starting with 1, and by indexing the NT occurrences in α by the indices of their corresponding counterparts in γ. Syntaxoriented PSCFG approaches often ignore source structure, instead focusing on generating syntactically well-formed target derivations. [12] use a single nonterminal category, [14] use syntactic constituents for the PSCFG nonterminal set, and [11] take advantage of CCG [15] inspired “slash” and “plus” categories. he il go not qMMMMqqq q q M q q qqq MM qqq ne va pas does Figure 3: Alignment graph (word alignment and target parse tree) for a French-English sentence pair. 4.2. Grammar Induction The SAMT model generates a PSCFG given parallel sentence pairs hf, ei, a parse tree π for each e, the maximum a posteriori word alignment a over hf, ei, and a set of phrase pairs Phrases(a) identified by any alignment-driven phrase induction technique such as e.g. [16]. Each phrase"
2007.iwslt-1.9,P99-1039,0,0.0467032,"MM q q qq VB PRN AUX RB NP • w ∈ [0, ∞) is a nonnegative real-valued weight assigned to the rule. In our notation, we will assume ∼ to be implicitly defined by indexing the NT occurrences in γ from left to right starting with 1, and by indexing the NT occurrences in α by the indices of their corresponding counterparts in γ. Syntaxoriented PSCFG approaches often ignore source structure, instead focusing on generating syntactically well-formed target derivations. [12] use a single nonterminal category, [14] use syntactic constituents for the PSCFG nonterminal set, and [11] take advantage of CCG [15] inspired “slash” and “plus” categories. he il go not qMMMMqqq q q M q q qqq MM qqq ne va pas does Figure 3: Alignment graph (word alignment and target parse tree) for a French-English sentence pair. 4.2. Grammar Induction The SAMT model generates a PSCFG given parallel sentence pairs hf, ei, a parse tree π for each e, the maximum a posteriori word alignment a over hf, ei, and a set of phrase pairs Phrases(a) identified by any alignment-driven phrase induction technique such as e.g. [16]. Each phrase in Phrases(a) is first annotated with a syntactic category to produce initial rules, where γ i"
2007.iwslt-1.9,J04-4002,0,0.0986589,"nal category, [14] use syntactic constituents for the PSCFG nonterminal set, and [11] take advantage of CCG [15] inspired “slash” and “plus” categories. he il go not qMMMMqqq q q M q q qqq MM qqq ne va pas does Figure 3: Alignment graph (word alignment and target parse tree) for a French-English sentence pair. 4.2. Grammar Induction The SAMT model generates a PSCFG given parallel sentence pairs hf, ei, a parse tree π for each e, the maximum a posteriori word alignment a over hf, ei, and a set of phrase pairs Phrases(a) identified by any alignment-driven phrase induction technique such as e.g. [16]. Each phrase in Phrases(a) is first annotated with a syntactic category to produce initial rules, where γ is set to the source side of the phrase, α is set to the target side of the phrase, and X is assigned based on the corresponding target side span in π. If the target span of the phrase does not match a constituent in π, heuristics are used to assign categories that correspond to partial rewriting of the tree. These heuristics first consider concatenation operations, forming categories like “NP+VP”, and then resort to CCG style “slash” categories like “NP/NN.” Preference for the concatenat"
2007.iwslt-1.9,N07-1063,1,0.825446,"l symbols spanned by D. Our distribution p over derivations is defined by a loglinear model. The probability of a derivation D is defined in terms of the rules r that are used in D: Q Q pLM (tgt(D))λLM × r∈D i φi (r)λi p(D) = (2) Z(λ) where φi refers to features defined on each rule, pLM is a n-gram LM probability applied to the target terminal symbols generated by the derivation D, and Z(λ) is a normalization constant chosen such that the probabilities sum up to one. The computational challenges of this search task (compounded by the integration of the language model) are addressed elsewhere [18, 19]. All feature weights λi are trained in concert with the language model weight via minimumerror training [3]. Here, we focus on the estimation of the feature values φ during the grammar induction process. The feature values are statistics estimated from rule counts. where lhs returns the left-hand-side of a rule, src returns the source side γ, and tgt returns the target side α of a rule r. The function ul removes all syntactic labels from its arguments, but retains ordering notation. For example, ul(NP+AUX1 does not go) = 1 does not go. The last two features are extensions to the feature set"
2007.iwslt-1.9,N03-1017,0,0.0142849,"]. They represent the same kind of relative frequency estimates commonly used in phrase based systems. The ul function allows us to calculate these estimates for rules with nonterminals as well. To estimate these probabilistic features, we use maximum likelihood estimates based on counts of the rules extracted from the training data. For example, pˆ(r|lhs(r)) is estimated by computing #(r)/#(lhs(r)), aggregating counts from all extracted rules. As in phrase-based translation model estimation, φ also contains two lexical weights pˆw (lex(src(r)) |lex(tgt(r))) and pˆw (lex(tgt(r)) |lex(src(r))) [20] that are based on the lexical symbols of γ, α. These weights are estimated based on an pair of statistical lexicons that represent pˆ(s|t), pˆ(t|s), where s and t are single words in the source and target vocabulary. These word-level translation models are typically estimated by maximum likelihood, considering the word-toword links from “single-best” alignments as evidence. We also store several boolean and count features in φ: the rule is purely lexical in α and γ; the rule is purely non-lexical in α and γ; the number of target words in the rule. 4.5. Training Corpora We used the provided 40"
2007.iwslt-1.9,H05-1060,0,0.0547858,"age is critical. However, for very diverse language pairs, i.e. translating between a rich morphology language such as Arabic and a poor morphology language such as English, a significant mismatch is present. For this language-pair, prefixes and suffixes of an Arabic word often correspond to separate English words. When translating from Arabic to English, a preprocessing step on Arabic is necessary to maintain consistency between two languages. In our A→E submission system we applied full morphological decomposition to the training corpora using a stateof-the-art Arabic morphological analyzer [22]. Morphological decomposition replaces each word in the training corpora with a sequence of its component morphemes prefix-, stem, -suffix. This approach also improves the coverage of the system, enabling it to translate words that do not occur in the training data, by performing translation at the sub-word, morpheme level. The prefix of an Arabic word can be a combination of conjunction (wa - and), article (Al - the), and preposition (li - to/for). Its’ suffix can be a pronoun (hm - their/them), case marker (u, i, a) gender (f - female singular), number, or voice, etc. Even though many morphe"
2007.iwslt-1.9,2006.iwslt-evaluation.19,1,0.861871,"ssion system and compare various approaches to recover punctuation (Section 3.3). In Section 3.4 we investigate methods to incorporate topic-knowledge into the translation framework via N -best list re-scoring. Our syntax-augmented SMT framework is detailed in Section 4. Sections 4.5 and 4.6 describe its application to the C→E task. Our A→E translation system which incorporates morphological decomposition is introduced in Section 5. 2. The CMU-UKA Phrase-based SMT System Our J→E and A→E systems built upon the STTK (Statistical Translation Toolkit) framework used for our IWSLT 2006 submissions [1]. STTK implements phrase-based statistical machine translation using a log-linear model [2] in which a foreign language sentence f1J = f1 , f2 , . . . , fJ is translated into another language eI1 = e1 , e2 , . . . , eI by searching for the hypothesis eˆI1 with maximum likelihood, given: eˆI1 = arg max P (eI1 |f1J ) eI1 exp = arg max P eI1 e′ I1 ′ P exp M m=1 P λm hm eI1 , f1J M m=1  λm hm e′ I1 , f1J ′  In this framework, the posterior probability P (eI1 |f1J ) is directly maximized using  a log-linear combination of feature functions hm eI1 , f1J (during decoding the denominator is dro"
2007.iwslt-1.9,C96-2141,1,0.692703,"|tgt(r)) : Probability of a rule given its target side pˆ(ul(src(r)), ul(tgt(r)) |ul(src(r)) : Probability of the unlabeled source and target side of the rule given its unlabeled source side. 5. Morphological-Decomposition for Robust Arabic-to-English SMT Statistical machine translation relies on a word alignment model, between source and target language, to extract and score phrase translations. In current word alignment methods Development Data IWSLT 04 500 sent. IWSLT 05 506 sent. 157,795 Arabic words 189,861 English words 0.7 16 references 16 references 0.5 Table 5: Arabic - English Data [2, 21] the one-to-one mapping between tokens in the source and target language is critical. However, for very diverse language pairs, i.e. translating between a rich morphology language such as Arabic and a poor morphology language such as English, a significant mismatch is present. For this language-pair, prefixes and suffixes of an Arabic word often correspond to separate English words. When translating from Arabic to English, a preprocessing step on Arabic is necessary to maintain consistency between two languages. In our A→E submission system we applied full morphological decomposition to the tr"
2007.iwslt-1.9,J03-1002,0,0.00627488,", case marker (u, i, a) gender (f - female singular), number, or voice, etc. Even though many morphemes have an equivalent English translation, some specific morphemes like gender, number, and case markers are redundant and can be discarded when translating to English. Previous work [23], used knowledge of the Arabic language to explicitly remove inflectional features from Arabic text before translation. In this work, we attempt to discard non-informative morphemes using a data driven approach. First, Arabic full morphology analysis and English text are passed to Giza++ word alignment toolkit [24]. Figure 4 shows the fertility distributions of Arabic morphemes from this alignment for the training corpora defined below. Morphemes that are not aligned to any English word are indicated by high zerofertility probability. These morphemes must be discarded to reduce NULL alignments and balance sentence length between the Arabic and English data. Morphemes for which the zero-fertility are greater than a threshold θth are discarded from the Arabic text. The threshold θth is selected to maximize translation quality (BLEU) on a development set. In the experimental evaluation, the “IWSLT A→E prov"
2007.iwslt-1.9,2005.iwslt-1.6,1,\N,Missing
2007.iwslt-1.9,J07-2003,0,\N,Missing
2007.mtsummit-papers.22,P05-1032,0,0.0129915,"the decoding process and form a translation lattice (or word graph, Ueffing et al., 2002). Each possible path through this translation lattice is evaluated according to a number of models and the best path is chosen as the final translation (model-best path). The target sides of the phrase pairs in this path are combined to form the final translation. Various algorithms for phrase pair extraction have been proposed (e.g. Koehn et al. 2003; Vogel, 2005; Zhao and Waibel, 2005). Recent developments extract the phrase pairs from the bilingual data as needed depending on the actual test sentence (Callison-Burch et al., 2005; Zhang and Vogel, 2005). These techniques usually improve the performance, but need more computing power and memory compared to pre-extracting the phrase pairs so they will most likely not be used for small devices. For this reason we concentrated on translation models consisting of pre-extracted phrase pairs. 1.2 Phrase Pair Pruning To limit the memory requirements of a translation system we now try to eliminate some of these phrase pairs (pruning of the translation model). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, wh"
2007.mtsummit-papers.22,2006.iwslt-evaluation.19,1,0.863264,"Missing"
2007.mtsummit-papers.22,N07-2006,1,0.773168,"Missing"
2007.mtsummit-papers.22,koen-2004-pharaoh,0,0.0534213,"ecially ones that do not have a clear translation in the target language sometimes have a high number of possible translation candidates. The final translation path will have to choose one out of those. It seems natural to restrict the translation variety, especially if memory space is limited. The translation variety threshold imposes this limit. The pruning is accomplished by sorting the phrase pairs for each source phrase according to their probability and eliminating low probability ones until the threshold is reached. Both threshold pruning strategies are well known. The Pharaoh decoder (Koehn, 2004) for example has an option to directly apply them to a phrase table. 2.2 Pruning via usage statistics In Eck et al. (2007) we introduced a pruning strategy that utilizes usage statistics to eliminate phrase pairs. This pruning strategy was inspired by the Optimal Brain Damage algorithm (Le Cun et al., 1990) and collected statistics for phrase pairs by translating the whole training corpus with the originally extracted phrase pairs. For each phrase pair two statistics were collected during this translation: • c(phrase pair) = Count how often a phrase pair was considered during decoding (i.e. wa"
2007.mtsummit-papers.22,N03-1017,0,0.0249865,"anslation model assigns translation probabilities to phrase pairs of source and target phrases extracted from a parallel bilingual text. These phrase pairs are applied during the decoding process and form a translation lattice (or word graph, Ueffing et al., 2002). Each possible path through this translation lattice is evaluated according to a number of models and the best path is chosen as the final translation (model-best path). The target sides of the phrase pairs in this path are combined to form the final translation. Various algorithms for phrase pair extraction have been proposed (e.g. Koehn et al. 2003; Vogel, 2005; Zhao and Waibel, 2005). Recent developments extract the phrase pairs from the bilingual data as needed depending on the actual test sentence (Callison-Burch et al., 2005; Zhang and Vogel, 2005). These techniques usually improve the performance, but need more computing power and memory compared to pre-extracting the phrase pairs so they will most likely not be used for small devices. For this reason we concentrated on translation models consisting of pre-extracted phrase pairs. 1.2 Phrase Pair Pruning To limit the memory requirements of a translation system we now try to eliminat"
2007.mtsummit-papers.22,P02-1040,0,0.0718021,"Missing"
2007.mtsummit-papers.22,2005.mtsummit-papers.33,1,0.852423,"igns translation probabilities to phrase pairs of source and target phrases extracted from a parallel bilingual text. These phrase pairs are applied during the decoding process and form a translation lattice (or word graph, Ueffing et al., 2002). Each possible path through this translation lattice is evaluated according to a number of models and the best path is chosen as the final translation (model-best path). The target sides of the phrase pairs in this path are combined to form the final translation. Various algorithms for phrase pair extraction have been proposed (e.g. Koehn et al. 2003; Vogel, 2005; Zhao and Waibel, 2005). Recent developments extract the phrase pairs from the bilingual data as needed depending on the actual test sentence (Callison-Burch et al., 2005; Zhang and Vogel, 2005). These techniques usually improve the performance, but need more computing power and memory compared to pre-extracting the phrase pairs so they will most likely not be used for small devices. For this reason we concentrated on translation models consisting of pre-extracted phrase pairs. 1.2 Phrase Pair Pruning To limit the memory requirements of a translation system we now try to eliminate some of the"
2007.mtsummit-papers.22,2005.eamt-1.39,1,0.84793,"m a translation lattice (or word graph, Ueffing et al., 2002). Each possible path through this translation lattice is evaluated according to a number of models and the best path is chosen as the final translation (model-best path). The target sides of the phrase pairs in this path are combined to form the final translation. Various algorithms for phrase pair extraction have been proposed (e.g. Koehn et al. 2003; Vogel, 2005; Zhao and Waibel, 2005). Recent developments extract the phrase pairs from the bilingual data as needed depending on the actual test sentence (Callison-Burch et al., 2005; Zhang and Vogel, 2005). These techniques usually improve the performance, but need more computing power and memory compared to pre-extracting the phrase pairs so they will most likely not be used for small devices. For this reason we concentrated on translation models consisting of pre-extracted phrase pairs. 1.2 Phrase Pair Pruning To limit the memory requirements of a translation system we now try to eliminate some of these phrase pairs (pruning of the translation model). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, while not impacting the tr"
2007.mtsummit-papers.22,I05-3011,1,0.640476,"ion probabilities to phrase pairs of source and target phrases extracted from a parallel bilingual text. These phrase pairs are applied during the decoding process and form a translation lattice (or word graph, Ueffing et al., 2002). Each possible path through this translation lattice is evaluated according to a number of models and the best path is chosen as the final translation (model-best path). The target sides of the phrase pairs in this path are combined to form the final translation. Various algorithms for phrase pair extraction have been proposed (e.g. Koehn et al. 2003; Vogel, 2005; Zhao and Waibel, 2005). Recent developments extract the phrase pairs from the bilingual data as needed depending on the actual test sentence (Callison-Burch et al., 2005; Zhang and Vogel, 2005). These techniques usually improve the performance, but need more computing power and memory compared to pre-extracting the phrase pairs so they will most likely not be used for small devices. For this reason we concentrated on translation models consisting of pre-extracted phrase pairs. 1.2 Phrase Pair Pruning To limit the memory requirements of a translation system we now try to eliminate some of these phrase pairs (pruning"
2008.iwslt-papers.5,W08-0303,1,0.811017,"till needed. For a given source sentence f1J and a given target sentence eI1 a set of links (j, i) has to be found, which describes which source word fj is translated into which target word ei . Most SMT systems use the freely available GIZA++Toolkit [12] to generate the word alignment. This toolkit implements the IBM- and HMM-models introduced in [13, 14]. They have the advantage of unsupervised training and are well suited for a noisy-channel approach, but introducing additional features into these models is difficult. In contrast, we use the discriminative word alignment model presented in [15], which uses a conditional random field (CRF) to model the alignment matrix. This model is symmetric and no restrictions to the alignment are required. Furthermore, it is easy to use additional knowledge sources and the alignment can be biased towards precision or recall. In the framework, the different aspects of the word alignment are modeled by three groups of features. The first group of features depend only on the source and target words and may therefore be called local features. The lexical features, which represent the lexical translation probability of the words as well as the source"
2008.iwslt-papers.5,E03-1076,0,0.0214995,"arliament Plenary Speeches (EPPS) and News Commentary corpora, as provided by WMT08 [16]. To this we added a smaller corpus of about 660K running words consisting of spoken language expressions in the travel domain, and a corpus of about 100K running words of German lectures held at Universit¨at Karlsruhe (TH) which were transcribed and translated into English, yielding a parallel training corpus of about 1.46M sentence pairs and 36M running words. For machine translation experiments, we applied compound splitting to the German side of the corpus, using the frequency-based method described in [17] which was trained on the corpus itself. Even though this method splits more aggressively and generates shorter and partially erroneous word fragments, it produced better translation quality than the method used for the speech recognizer described in Section 3.3. After tokenization and compound splitting, we performed word alignment, using the GIZA++ toolkit and, for the final system, the method described in the previous secProceedings of IWSLT 2008, Hawaii - U.S.A. tion, and extracted bilingual phrase pairs with the Pharaoh toolkit [18]. 4-gram language models were used in all experiments, es"
2008.iwslt-papers.5,W06-3114,0,0.0570922,"he corpus, using the frequency-based method described in [17] which was trained on the corpus itself. Even though this method splits more aggressively and generates shorter and partially erroneous word fragments, it produced better translation quality than the method used for the speech recognizer described in Section 3.3. After tokenization and compound splitting, we performed word alignment, using the GIZA++ toolkit and, for the final system, the method described in the previous secProceedings of IWSLT 2008, Hawaii - U.S.A. tion, and extracted bilingual phrase pairs with the Pharaoh toolkit [18]. 4-gram language models were used in all experiments, estimated with modified Kneser-Ney smoothing as implemented in the SRILM toolkit [7]. 4.3. Model adaptation The performance of data-driven MT systems depends heavily on a good match between training and test data in terms of domain coverage and speaking style. This applies even more so in our case, as university lectures can go deeply into a very narrow topic and, depending on the lecturer, are often given in a much more informal and conversational style than that found in prepared speeches or even written text. To adapt the MT component t"
2008.iwslt-papers.5,2007.tmi-papers.21,0,0.0126456,"l adaptation are shown in Table 3. 4.4. Front-end In order to get the best translation results, the output of the speech recognizer has to be pre-processed to match the format expected by the translation component. We first perform compound splitting as described in Section 4.2, on top of the compound splitting already done in the speech recognizer. Because the word order in German and English is very different, reordering over a rather limited distance like done in many phrase-based systems does not lead to a good translation quality. We experimented with rule-based reordering as proposed in [19], in which a word lattice is created as a pre-processing step to encode different reorderings and allow somewhat longer distance reordering. The rules to create the lattice are automatically learned from the corpus and the part-of-speech (POS) tags created by the TreeTagger [20]. In the training, POS based reordering patterns were extracted from word alignment information. The context in which a reordering pattern is seen was used as an additional feature. At decoding time, we build a lattice structure for each source utterance as input for our decoder, which contains reordering alternatives c"
2008.iwslt-papers.5,J93-2003,0,\N,Missing
2008.iwslt-papers.5,C96-2141,0,\N,Missing
2008.iwslt-papers.5,J03-1002,0,\N,Missing
2008.iwslt-papers.5,P03-1021,0,\N,Missing
2010.eamt-1.29,W09-0432,0,0.0240657,"n systems to a domain. Some authors adapted only the language model inspired by similar approaches in speech recognition (Bulyko et al., 2007). The main advantage of language model adaptation in contrast to translation model adaptation is that only monolingual in-domain data is needed. To be able to adapt the translation model in these conditions, other authors tried to generated a synthetic parallel text by translating the monolingual corpus with a baseline system and use this corpus to train a new system or to adapt the baseline system (Ueffing et al., 2007), (Schwenk and Senellart, 2009), (Bertoldi and Federico, 2009). Snover et al. (2008) used cross-lingual information retrieval to find similar target language corpora. This data was used to adapt the language model as well as to learn new possible translations. Wu et al. (2008) presented an approach to adapt the system using hand-made dictionaries and monolingual source and target language text. In cases where also in-domain parallel data is available, authors also tried to adapt the translation model. Koehn and Schroeder (2007) adapted the language model by linear and log-linear interpolation. Furthermore, they could improve the translation performance u"
2010.eamt-1.29,J93-2003,0,0.0160364,". We propose an approach to adapt an SMT system using small amounts of parallel in-domain data by introducing the corpus identifier (corpus id) as an additional target factor. Then we added features to model the generation of the tags and features to judge a sequence of tags. Using this approach we could improve the translation performance in two domains by up to 1 BLEU point when translating from German to English. 1 Introduction Statistical machine translation (SMT) is currently the most promising approach to machine translation of large vocabulary tasks. The approach was first presented in Brown et al. (1993) and has been used in many translation systems since then. One drawback of this approach is that large amounts of training data are needed. Furthermore, the performance of the SMT system improves if this data is selected from a similar topic and from a similar genre. Since this is not possible for many real-world scenarios, one approach to overcome this problem is to use all available data to train a general system and to adapt the system using indomain training data. Factored translation models as presented in Koehn and Hoang (2007) are able to tightly integrate additional knowledge into a ph"
2010.eamt-1.29,W07-0717,0,0.112906,". In cases where also in-domain parallel data is available, authors also tried to adapt the translation model. Koehn and Schroeder (2007) adapted the language model by linear and log-linear interpolation. Furthermore, they could improve the translation performance using two translation models and combining them with the alternate decoding path model as described in Birch et al. (2007). Although this approach does also use factored translation models, the way they integrate the domain and the type of features they use is different from ours. An approach based on mixture models was presented by Foster and Kuhn (2007). They tried to use linear and log-linear, language model and translation model adaptation. Furthermore, they tried to optimize the weights for the different domains on a development set as well as to set the weights according to text distance measures. Matsoukas et al. (2009) also adapt the system by changing the weights of the phrase pairs. In their approach this is done by assigning discriminative weights for the sentences of the parallel corpus. In contrast, Hildebrand et al. (2005) proposed a method to adapt the translation towards similar sentences which are automatically found using inf"
2010.eamt-1.29,2005.eamt-1.19,1,0.751116,"and the type of features they use is different from ours. An approach based on mixture models was presented by Foster and Kuhn (2007). They tried to use linear and log-linear, language model and translation model adaptation. Furthermore, they tried to optimize the weights for the different domains on a development set as well as to set the weights according to text distance measures. Matsoukas et al. (2009) also adapt the system by changing the weights of the phrase pairs. In their approach this is done by assigning discriminative weights for the sentences of the parallel corpus. In contrast, Hildebrand et al. (2005) proposed a method to adapt the translation towards similar sentences which are automatically found using information retrieval techniques. Factored translation models were introduced by Koehn and Hoang (2007) to enable the straightforward integration of additional annotations at the word-level. This is done by representing a word by a vector of factors for the different types of annotations instead of using only the word token. The additional factors can be used to better judge the generated output as well as to generate the target word from the other factors, if no direct translation of the"
2010.eamt-1.29,D07-1091,0,0.320516,"of large vocabulary tasks. The approach was first presented in Brown et al. (1993) and has been used in many translation systems since then. One drawback of this approach is that large amounts of training data are needed. Furthermore, the performance of the SMT system improves if this data is selected from a similar topic and from a similar genre. Since this is not possible for many real-world scenarios, one approach to overcome this problem is to use all available data to train a general system and to adapt the system using indomain training data. Factored translation models as presented in Koehn and Hoang (2007) are able to tightly integrate additional knowledge into a phrase-based statistical machine translation system. In most c 2010 European Association for Machine Translation. cases, the approach is used to incorporate linguistic knowledge, such as morphological, syntactic and semantic information. In contrast, we will use the approach to integrate domain knowledge into the system by introducing a corpus identifier (corpus id) tag. Using the corpus id as a target word factor enables us to adapt the SMT system by introducting two new types of features in the log-linear model in phrase-based SMT sy"
2010.eamt-1.29,W07-0733,0,0.1737,"use this corpus to train a new system or to adapt the baseline system (Ueffing et al., 2007), (Schwenk and Senellart, 2009), (Bertoldi and Federico, 2009). Snover et al. (2008) used cross-lingual information retrieval to find similar target language corpora. This data was used to adapt the language model as well as to learn new possible translations. Wu et al. (2008) presented an approach to adapt the system using hand-made dictionaries and monolingual source and target language text. In cases where also in-domain parallel data is available, authors also tried to adapt the translation model. Koehn and Schroeder (2007) adapted the language model by linear and log-linear interpolation. Furthermore, they could improve the translation performance using two translation models and combining them with the alternate decoding path model as described in Birch et al. (2007). Although this approach does also use factored translation models, the way they integrate the domain and the type of features they use is different from ours. An approach based on mixture models was presented by Foster and Kuhn (2007). They tried to use linear and log-linear, language model and translation model adaptation. Furthermore, they tried"
2010.eamt-1.29,N03-1017,0,0.00795716,"this language pair, the biggest parallel corpus available are the Proceedings of the European Parliament (EPPS). The first system we built was designed to translate documents from the news-commentary domain. As test set we used the test set from the WMT Evaluation in 2007. As parallel training data we used the EPPS corpus as well as an in-domain news-commentary corpus with about 1M words. In contrast, the corpus from the EPPS domain has about 39M words. In a preprocessing step we cleaned the data and performed a compound splitting on the German text based on the frequency method described in Koehn et al. (2003). We generated a word alignment for the parallel corpus using a discriminative approach as described in Niehues and Vogel (2008) trained on a set of 500 hand-aligned sentences. Afterwards, the phrase pairs were extracted using the training scripts from the Moses package (Koehn et al., 2007). We use two language models in our SMT system. The first one, a general language model, was trained on the English Gigaword corpus. In addition, we use a second one, trained only on the English part of the parallel in-domain corpus. Using this additional language model, our baseline system was already partl"
2010.eamt-1.29,P07-2045,0,0.0125364,"Missing"
2010.eamt-1.29,D09-1074,0,0.0580419,"ranslation models and combining them with the alternate decoding path model as described in Birch et al. (2007). Although this approach does also use factored translation models, the way they integrate the domain and the type of features they use is different from ours. An approach based on mixture models was presented by Foster and Kuhn (2007). They tried to use linear and log-linear, language model and translation model adaptation. Furthermore, they tried to optimize the weights for the different domains on a development set as well as to set the weights according to text distance measures. Matsoukas et al. (2009) also adapt the system by changing the weights of the phrase pairs. In their approach this is done by assigning discriminative weights for the sentences of the parallel corpus. In contrast, Hildebrand et al. (2005) proposed a method to adapt the translation towards similar sentences which are automatically found using information retrieval techniques. Factored translation models were introduced by Koehn and Hoang (2007) to enable the straightforward integration of additional annotations at the word-level. This is done by representing a word by a vector of factors for the different types of ann"
2010.eamt-1.29,W09-0435,1,0.835368,"rs were extracted using the training scripts from the Moses package (Koehn et al., 2007). We use two language models in our SMT system. The first one, a general language model, was trained on the English Gigaword corpus. In addition, we use a second one, trained only on the English part of the parallel in-domain corpus. Using this additional language model, our baseline system was already partly adapted to the target domain. To be able to model the quite difficult reordering between German and English we used a partof-speech based reordering model as described in Rottmann and Vogel (2007) and Niehues and Kolss (2009). In this approach reordering rules based on part-of-speech tags are learned from the parallel corpus. For every test sentence, different possible reorderings of the source sentence are encoded in a word lattice. Then the decoder translates this lattice instead of the original input sentence. We use a phrase-based decoder as described in Vogel (2003) using the language models, phrase scores as well as a word count and phrase count model. The optimization was done by MER training described in Venugopal et al. (2005). We performed a second series of experiments on the translation task of lecture"
2010.eamt-1.29,W08-0303,1,0.867836,"system we built was designed to translate documents from the news-commentary domain. As test set we used the test set from the WMT Evaluation in 2007. As parallel training data we used the EPPS corpus as well as an in-domain news-commentary corpus with about 1M words. In contrast, the corpus from the EPPS domain has about 39M words. In a preprocessing step we cleaned the data and performed a compound splitting on the German text based on the frequency method described in Koehn et al. (2003). We generated a word alignment for the parallel corpus using a discriminative approach as described in Niehues and Vogel (2008) trained on a set of 500 hand-aligned sentences. Afterwards, the phrase pairs were extracted using the training scripts from the Moses package (Koehn et al., 2007). We use two language models in our SMT system. The first one, a general language model, was trained on the English Gigaword corpus. In addition, we use a second one, trained only on the English part of the parallel in-domain corpus. Using this additional language model, our baseline system was already partly adapted to the target domain. To be able to model the quite difficult reordering between German and English we used a partof-s"
2010.eamt-1.29,2007.tmi-papers.21,0,0.0607317,"es. Afterwards, the phrase pairs were extracted using the training scripts from the Moses package (Koehn et al., 2007). We use two language models in our SMT system. The first one, a general language model, was trained on the English Gigaword corpus. In addition, we use a second one, trained only on the English part of the parallel in-domain corpus. Using this additional language model, our baseline system was already partly adapted to the target domain. To be able to model the quite difficult reordering between German and English we used a partof-speech based reordering model as described in Rottmann and Vogel (2007) and Niehues and Kolss (2009). In this approach reordering rules based on part-of-speech tags are learned from the parallel corpus. For every test sentence, different possible reorderings of the source sentence are encoded in a word lattice. Then the decoder translates this lattice instead of the original input sentence. We use a phrase-based decoder as described in Vogel (2003) using the language models, phrase scores as well as a word count and phrase count model. The optimization was done by MER training described in Venugopal et al. (2005). We performed a second series of experiments on th"
2010.eamt-1.29,2009.mtsummit-posters.17,0,0.0108605,"re proposed to adapt translation systems to a domain. Some authors adapted only the language model inspired by similar approaches in speech recognition (Bulyko et al., 2007). The main advantage of language model adaptation in contrast to translation model adaptation is that only monolingual in-domain data is needed. To be able to adapt the translation model in these conditions, other authors tried to generated a synthetic parallel text by translating the monolingual corpus with a baseline system and use this corpus to train a new system or to adapt the baseline system (Ueffing et al., 2007), (Schwenk and Senellart, 2009), (Bertoldi and Federico, 2009). Snover et al. (2008) used cross-lingual information retrieval to find similar target language corpora. This data was used to adapt the language model as well as to learn new possible translations. Wu et al. (2008) presented an approach to adapt the system using hand-made dictionaries and monolingual source and target language text. In cases where also in-domain parallel data is available, authors also tried to adapt the translation model. Koehn and Schroeder (2007) adapted the language model by linear and log-linear interpolation. Furthermore, they could improv"
2010.eamt-1.29,D08-1090,0,0.0249194,"thors adapted only the language model inspired by similar approaches in speech recognition (Bulyko et al., 2007). The main advantage of language model adaptation in contrast to translation model adaptation is that only monolingual in-domain data is needed. To be able to adapt the translation model in these conditions, other authors tried to generated a synthetic parallel text by translating the monolingual corpus with a baseline system and use this corpus to train a new system or to adapt the baseline system (Ueffing et al., 2007), (Schwenk and Senellart, 2009), (Bertoldi and Federico, 2009). Snover et al. (2008) used cross-lingual information retrieval to find similar target language corpora. This data was used to adapt the language model as well as to learn new possible translations. Wu et al. (2008) presented an approach to adapt the system using hand-made dictionaries and monolingual source and target language text. In cases where also in-domain parallel data is available, authors also tried to adapt the translation model. Koehn and Schroeder (2007) adapted the language model by linear and log-linear interpolation. Furthermore, they could improve the translation performance using two translation m"
2010.eamt-1.29,W05-0836,1,0.89216,"of-speech based reordering model as described in Rottmann and Vogel (2007) and Niehues and Kolss (2009). In this approach reordering rules based on part-of-speech tags are learned from the parallel corpus. For every test sentence, different possible reorderings of the source sentence are encoded in a word lattice. Then the decoder translates this lattice instead of the original input sentence. We use a phrase-based decoder as described in Vogel (2003) using the language models, phrase scores as well as a word count and phrase count model. The optimization was done by MER training described in Venugopal et al. (2005). We performed a second series of experiments on the translation task of lectures from German to English. The system was trained on the data from the European Parliament, the news-commentary data, German-English BTEC data and a small amount of translated lectures. The in-domain corpus contained only around 210K words. The system was built similar to the systems in the other experiments except to some small changes due to speech translation. Instead of doing a separate compound splitting, we used the same splitting as it was used by the speech recognizer. Since the output of the speech recognit"
2010.eamt-1.29,C08-1125,0,0.0790346,"only the language model inspired by similar approaches in speech recognition (Bulyko et al., 2007). The main advantage of language model adaptation in contrast to translation model adaptation is that only monolingual in-domain data is needed. To be able to adapt the translation model in these conditions, other authors tried to generated a synthetic parallel text by translating the monolingual corpus with a baseline system and use this corpus to train a new system or to adapt the baseline system (Ueffing et al., 2007), (Schwenk and Senellart, 2009), (Bertoldi and Federico, 2009). Snover et al. (2008) used cross-lingual information retrieval to find similar target language corpora. This data was used to adapt the language model as well as to learn new possible translations. Wu et al. (2008) presented an approach to adapt the system using hand-made dictionaries and monolingual source and target language text. In cases where also in-domain parallel data is available, authors also tried to adapt the translation model. Koehn and Schroeder (2007) adapted the language model by linear and log-linear interpolation. Furthermore, they could improve the translation performance using two translation m"
2010.iwslt-evaluation.11,W10-1719,1,\N,Missing
2010.iwslt-evaluation.11,D09-1022,0,\N,Missing
2010.iwslt-evaluation.11,W08-1006,0,\N,Missing
2010.iwslt-evaluation.11,E03-1076,0,\N,Missing
2010.iwslt-evaluation.11,N04-4015,0,\N,Missing
2010.iwslt-evaluation.11,E99-1010,0,\N,Missing
2010.iwslt-evaluation.11,W11-2124,1,\N,Missing
2010.iwslt-evaluation.11,P03-1054,0,\N,Missing
2010.iwslt-evaluation.11,W09-0435,1,\N,Missing
2010.iwslt-evaluation.11,W06-1607,0,\N,Missing
2010.iwslt-evaluation.11,P07-2045,0,\N,Missing
2010.iwslt-evaluation.11,N06-2013,0,\N,Missing
2010.iwslt-evaluation.11,P06-1096,0,\N,Missing
2010.iwslt-evaluation.11,J03-1002,0,\N,Missing
2010.iwslt-evaluation.11,2011.iwslt-evaluation.9,1,\N,Missing
2010.iwslt-evaluation.11,2005.iwslt-1.8,0,\N,Missing
2010.iwslt-evaluation.11,2011.iwslt-papers.7,0,\N,Missing
2010.iwslt-evaluation.11,W11-2145,1,\N,Missing
2010.iwslt-evaluation.11,W11-2123,0,\N,Missing
2011.iwslt-evaluation.12,2011.iwslt-evaluation.1,1,0.870478,"an.saam|sebastian.stueker|alex.waibel}@kit.edu Abstract This paper describes our English Speech-to-Text (STT) system for the 2011 IWSLT ASR track. The system consists of 2 subsystems with different front-ends—one MVDR based, one MFCC based—which are combined using confusion network combination to provide a base for a second pass speaker adapted MVDR system. We demonstrate that this set-up produces competitive results on the IWSLT 2010 dev and test sets. 1. Introduction In this paper we describe our English Speech-to-Text (STT) system with which we participated in the 2011 IWSLT STT evaluation [1]. Our system makes use of system combination and cross-adaptation, by utilising acoustic models which are trained with different acoustic front-ends. The system has been derived from our 2010 English Quaero ASR evaluation system, by taking acoustic models out of that system and combining them with a language model that has been specifically tailored to the IWSLT lecture task. 1.1. IWSLT The goal of the International Workshop on Spoken Language Translation (IWSLT) evaluation campaign is the translation of TED Talks (http://www.ted.com/talks), short 5-25min presentations by people from various f"
2011.iwslt-evaluation.15,2011.iwslt-evaluation.16,1,0.746987,"led in the Quaero program is 1 http://www.quaero.org spoken language translation (SLT). In this work, the 2011 project-internal evaluation campaign on SLT is described. The campaign focuses on the language pair German-French in both directions, and both human and automatic transcripts of the spoken text are considered as input data. The automatic transcripts were produced by the Rover combination of single-best output of the best submission from each of the three sites participating in the internal 2010 automatic speech recognition (ASR) evaluation, which is described in an accompanying paper [1]. The campaign was designed and conducted by DGA and compares the different approaches taken by the four participating partners RWTH, KIT, LIMSI and SYSTRAN. In addition to publicly available data, monolingual and bilingual corpora collected in the Quaero program were used for training and evaluating the systems. The approaches to machine translation taken by the partners differ substantially. KIT, LIMSI and RWTH apply statistical techniques to perform the task, whereas SYSTRAN uses their commercial rule-based translation engine. KIT makes use of a phrase-based decoder augmented with partof-sp"
2011.iwslt-evaluation.15,P02-1040,0,0.0815552,"s been built from the test sets of the previous years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized"
2011.iwslt-evaluation.15,2006.amta-papers.25,0,0.0225424,"evious years. • arte.tv 2.3. Metrics and Scoring The corpora used to evaluate this task have been built from French and German (manual) transcriptions extracted from the test set used in the previous year’s Quaero evaluation campaign of ASR [1]. These transcriptions come from recordings of broadcast shows. The transcriptions were resegmented manually by the human translators into sentences. Indeed the time-based segmentation, traditionally used for ASR purposes, induced translation issues in the previous 2 http://www.statmt.org/wmt10/ The B LEU-4 score [3] and the Translation Edit Rate (T ER) [4] were chosen as the evaluation metrics for machine translation in Quaero program. B LEU measures the closeness of a candidate translation to one or several reference translations by counting the number of n-grams in the system output that also occur in the reference translation. T ER is an error measure for machine translation that measures the number of edits required to change a system output into one of the references. T ER is defined as the minimum number of 115 edits needed to change a hypothesis so that it matches one of the references, normalized by the average length of the references."
2011.iwslt-evaluation.15,J05-4003,0,0.0172208,"asing variant and change the case as required to be able to translate it. Some of the available data contains a lot of noise. The Giga corpus, for example, includes a large amount of noise such as non-standardized HTML characters. Also, the Bookshop and Presseurop corpora contain truncated lines, which do not match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built"
2011.iwslt-evaluation.15,2007.tmi-papers.21,0,0.0128424,"ot match its aligned translation sentence. These noisy pairs potentially degrade the quality of the translation model. The special filtering was applied to the Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words"
2011.iwslt-evaluation.15,W09-0435,1,0.688116,"Giga corpus and some of the Quaero data. We used a Support Vector Machines classifier to filter the corpus, inspired by the work of [5] on comparable data. To generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using"
2011.iwslt-evaluation.15,P07-2045,0,0.00836467,"generate the translation model, we used the MGIZA++ Toolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We ad"
2011.iwslt-evaluation.15,W11-2145,1,0.808022,"oolkit to calculate the word alignment for the training corpus. Afterwards, the alignments were combined using the grow-diag-final-and heuristic. Word reordering is addressed using the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingua"
2011.iwslt-evaluation.15,W11-2124,1,0.82441,"g the POS-based reordering model as described in [6] to account for the different word orders in the languages. To cover long-range reorderings, we apply a modified reordering model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grain"
2011.iwslt-evaluation.15,W05-0836,1,0.88503,"ng model with non-continuous rules [7]. The part-of-speech tags for the reordering model are obtained using the TreeTagger [8]. The phrase table and the phrases were built with the Moses Toolkit [9] and scored by our inhouse parallel phrase scorer [10]. We used 4-gram language models with Kneser-Ney smoothing, which are generated by using the SRILM toolkit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering"
2011.iwslt-evaluation.15,C08-1098,0,0.0239782,"kit [11]. The system applied a bilingual language model as described in [12] to extend the context of source language words available for translation. Tuning is performed using minimum error rate training against the B LEU score as described in [13]. Translations are generated using our in-house phrase-based decoder [14]. German-French For German to French we applied longrange POS-based reordering rules and lattice phrase extraction. We added a bilingual language model and a POSbased bilingual language model. The part-of-speeches for this model were generated by using the RF tagger for German [15] and the LIA Tagger for French 3 . These taggers produce more fine-grained linguistic information than the TreeTagger, whose output is used for POS-based reordering. French-German For French to German we also used long-range POS based reordering rules and lattice phrase extraction. Using the POS-based language model led to a big improvement. 3.2. LIMSI LIMSI’s participation in Quaero 2011 evaluation campaign was focused on the translation of German from and into French. The adaptation of our text translation system to speech inputs is mostly performed in preprocessing, aimed at removing dysflu"
2011.iwslt-evaluation.15,N04-4026,0,0.0164881,"slation system based on bilingual n-grams. N-code overview N-code’s translation model implements a stochastic finite-state transducer (FST) trained using an n-gram model (source,target) pairs. The training requires source-side sentence reorderings to match the target word order, also performed by a stochastic FST reordering model, which uses POS information to generalize reordering patterns beyond lexical regularities. Complementary to the translation model, ten more features are used in a linear scoring function: a target-language model; four lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-"
2011.iwslt-evaluation.15,P03-1021,0,0.0157097,"ur lexicon models; two lexicalized reordering models [16] to predict the orientation of the next translation unit; a weak distancebased distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the standard ones in phrase-based systems: two scores correspond to the relative frequencies of the tuples and two lexical weights, estimated from the automatically generated word alignments. The weights associated to features are found using the minimum error rate training procedure [17] on the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagg"
2011.iwslt-evaluation.15,P10-1052,1,0.744516,"the development set. The decoding is beam-search-based on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder as word lattices [18]. German-French Part-of-speech information for German 3 http://lia.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the r"
2011.iwslt-evaluation.15,D09-1022,1,0.784658,"al machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The German and French data submitted by SYSTRAN were obtained by the SYSTRAN baseline engine, being traditionally classified as a rule-based system. However, over the decades, its devel"
2011.iwslt-evaluation.15,2010.iwslt-papers.6,0,0.0142765,"a.univ-avignon.fr/fileadmin/documents/ Users/Intranet/chercheurs/bechet/download_fred. html 4 http://www.limsi.fr/Individu/jmcrego/n-code 116 is computed using in-house CRF-based tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6"
2011.iwslt-evaluation.15,C00-2162,1,0.678983,"sed tagger [19]. All the available data has been preprocessed and word aligned using MGIZA++; these alignments were then used in a standard Ncode pipeline. As development set we used the WMT 2010 newstest set; internal tests were conducted on the test data of 2009 and 2011. LIMSI used the best available text translation system and the preprocessing with tools initially developed and used for our German to English systems [20]. These tools have also been augmented so as to perform a restricted form of longrange reorderings, notably to move separable particles closer to the verbs they depend on [21]. For the reordering models we selected the monotone-swap-discontinuous (MSD) model. Language models Large 4-gram language models were trained on all the available data as described in [22]. Additionally, SOUL, a neuronal language model was used to rescore the n-best hypotheses. These models were trained following the methodology of [23] and used for rescoring n-best lists. We used 10-gram history size (differences with 6-gram were insignificant). Using the neural language model led to (small but consistent) improvements in all tasks. With the help of system combination, we combined the hypoth"
2011.iwslt-evaluation.15,2008.iwslt-papers.8,1,0.818685,"the pipeline was unchanged as compared to text translations. For the Quaero 2011 evaluation RWTH utilized state-ofthe-art phrase-based and hierarchical translation systems as well as our in-house system combination framework. GIZA [24] was employed to train word alignments, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 1"
2011.iwslt-evaluation.15,E06-1005,1,0.810679,"ents, all language models were created with the SRILM toolkit [11] and are standard 4-gram language models with interpolated modified Kneser-Ney smoothing. The phrase-based statistical machine translation system (pbt) used in this work is an inhouse implementation of the state-of-the-art machine translation decoder described in [25]. For our hierarchical setups, we employed the open source translation toolkit Jane [26], which has been developed at RWTH and is freely available for non-commercial use. The basic concept of RWTH’s approach to machine translation system combination is described in [27, 28]. With both decoders, we did several setups with different amounts of models. Optional additional models are discriminative word lexicon (dwl) models, triplet lexicon models [29] and additionally binary count features. Unless stated otherwise, we optimized the model weights with standard minimum error rate training [17] on 100-best lists on B LEU. • pbt with additional models dwl and triplets • pbt with additional model triplets With the system combination of all different systems, we got an improvement in B LEU and in T ER compared to the best single system of both tasks. 3.4. SYSTRAN The Ger"
2011.iwslt-evaluation.15,J03-1002,1,\N,Missing
2011.iwslt-evaluation.15,W11-2135,1,\N,Missing
2011.iwslt-evaluation.9,2007.tmi-papers.21,0,0.269099,"from the training data may not help or may be even harmful when translating. Instead of using a translation model with phrase tables that are built on data containing punctuation, we tried to train the system for the speech translation task using the training corpus without punctuation marks. Therefore we mapped the alignment from the parallel corpus with punctuation to the corpus without source punctuation. Then we retrained the phrase table, the POS-based reordering model and the bilingual language model. 4. Word Reordering Model Our word reordering model relies on POS tags as introduced by [9]. The reordering is performed as preprocessing step. Rule extraction is based on two types of input: the Giza alignment of the parallel corpus and its corresponding POS tags generated by the TreeTagger for the source side. For each sequence of POS tags, where a reordering between source and target sentences is detected, a rule is generated. Its head consists of the sequence of source tags and its body is the permutation of POS tags in the head which matches the order of the corresponding aligned target words. After that, the rules are scored according to their occurrence and then pruned accord"
2011.iwslt-evaluation.9,W11-2124,1,0.64468,"nslation model, the adaptation of the language model is also achieved by a log-linear combination of different models. This also fits well into the global log-linear model used in the translation system. Therefore, we trained a separate language model using only the in-domain data for TED provided in the workshop. Then it was used as an additional language model during decoding and received optimal weights during tuning by the Minimum Error Rate training. 6. Bilingual Language Models To increase the context used during the translation process, we use a bilingual language model as described in [10]. To model the dependencies between source and target words even beyond borders of phrase pairs, we create a bilingual token out of every target word and all its aligned source words. The tokens are ordered like the target words. For training, we create a corpus of bilingual tokens from each of the parallel corpora (TED, UN, EPPS, NC and Giga) and then we train one SRI language model based on all the corpora of bilingual tokens. We use an n-gram length of four words. During decoding, this language model is then used to score the different translation hypotheses. 7. Cluster Language Models As m"
2011.iwslt-evaluation.9,E99-1010,0,0.385179,"language model, which is separately trained on the TED corpus and then combined (in a log-linear way) with the other models. Since the TED corpus is much smaller than the other corpora, the probabilities cannot be estimated as reliably. Furthermore, for the style of a document the word order may not be as important, but the sequence of used word classes may be sufficient to specify the style. To tackle both problems, we try to use a language model based on word classes in addition. This is done in the following way: In a first step, we cluster the words of the corpus using the MKCLS algorithm [11]. Then we replace the words in the TED corpus by their cluster IDs and train a n-gram language model on this corpus consisting of word classes (all cluster language models used in our systems are 5-gram). During decoding we use the cluster-based language model as an additional model in the log-linear combination. 8. Discriminative Word Lexica In [12] it was shown that the use of discriminative word lexica (DWL) can improve the translation quality quite significantly. For every target word, they trained a maximum entropy model to determine whether this target word should be in the translated se"
2011.iwslt-evaluation.9,D09-1022,0,0.0938278,"ses may be sufficient to specify the style. To tackle both problems, we try to use a language model based on word classes in addition. This is done in the following way: In a first step, we cluster the words of the corpus using the MKCLS algorithm [11]. Then we replace the words in the TED corpus by their cluster IDs and train a n-gram language model on this corpus consisting of word classes (all cluster language models used in our systems are 5-gram). During decoding we use the cluster-based language model as an additional model in the log-linear combination. 8. Discriminative Word Lexica In [12] it was shown that the use of discriminative word lexica (DWL) can improve the translation quality quite significantly. For every target word, they trained a maximum entropy model to determine whether this target word should be in the translated sentence or not. As features for their classifier they used one feature per source word. One specialty of this task is that we have a lot of parallel data we can train our models on, but only a quite small portion of these data, the TED corpus, is very important to the translation quality. Since building the classifiers on the whole corpus is quite tim"
2011.iwslt-evaluation.9,J03-1002,0,0.0105903,"we used the provided monolingual data. Systems were tuned and tested against the provided Dev and Test sets. Before training any of our models, we perform the usual preprocessing, such as removing long sentences and sentences with length difference exceeding a certain threshold. In addition, special symbols, dates and numbers are normalized; then the first word of every sentence is smart-cased. All the language models used are 4-gram language models with Kneser-Ney smoothing, trained with the SRILM toolkit [2]. The word alignment of the parallel corpora was generated using the GIZA++-Toolkit [3] for both directions. Afterwards, the alignments were combined using the grow-diagfinal-and heuristic. The phrases were extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. 73 Word reordering is addressed using the POS-based reordering model and is described in detail in Section 4. The part-of-speech tags for the reordering model are obtained using the TreeTagger [6]. Tuning is performed using Minimum Error Rate Training against the BLEU score as described in [7]. All translations are generated using our in-house phrase-based decoder [8]. 3. Preproc"
2011.iwslt-evaluation.9,P07-2045,0,0.0204732,"he usual preprocessing, such as removing long sentences and sentences with length difference exceeding a certain threshold. In addition, special symbols, dates and numbers are normalized; then the first word of every sentence is smart-cased. All the language models used are 4-gram language models with Kneser-Ney smoothing, trained with the SRILM toolkit [2]. The word alignment of the parallel corpora was generated using the GIZA++-Toolkit [3] for both directions. Afterwards, the alignments were combined using the grow-diagfinal-and heuristic. The phrases were extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. 73 Word reordering is addressed using the POS-based reordering model and is described in detail in Section 4. The part-of-speech tags for the reordering model are obtained using the TreeTagger [6]. Tuning is performed using Minimum Error Rate Training against the BLEU score as described in [7]. All translations are generated using our in-house phrase-based decoder [8]. 3. Preprocessing The Giga corpus received a special preprocessing in a similar manner to [5]. An SVM classifier was used to filter out bad pairs from the Giga parallel"
2011.iwslt-evaluation.9,W05-0836,1,0.492103,"[2]. The word alignment of the parallel corpora was generated using the GIZA++-Toolkit [3] for both directions. Afterwards, the alignments were combined using the grow-diagfinal-and heuristic. The phrases were extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. 73 Word reordering is addressed using the POS-based reordering model and is described in detail in Section 4. The part-of-speech tags for the reordering model are obtained using the TreeTagger [6]. Tuning is performed using Minimum Error Rate Training against the BLEU score as described in [7]. All translations are generated using our in-house phrase-based decoder [8]. 3. Preprocessing The Giga corpus received a special preprocessing in a similar manner to [5]. An SVM classifier was used to filter out bad pairs from the Giga parallel corpus. We used the same set of features. These are: IBM1 score in both directions, the number of unaligned source words, the difference in number of words between source and target, the maximum source word fertility, the number of unaligned target words, and the maximum target word fertility. The lexicons used were generated by Giza++ alignments train"
2011.iwslt-evaluation.9,W11-2145,1,\N,Missing
2011.iwslt-evaluation.9,2011.iwslt-evaluation.1,0,\N,Missing
2011.iwslt-papers.4,2008.iwslt-papers.5,1,0.922615,"ional educational content no matter their physical location. However, although physical barriers are reduced using these technologies, language barriers remain. Lectures may be given in a language different from the student’s native tongue and often the students that could benefit the most from this content may not have sufficient language skills to understand the lecture unaided. Interpreters are not a practical solution in many cases as the costs involved are prohibitively high. Recent works have thus investigated the use of speech-translation technologies to translate lectures in real-time [1]. The biggest downfall of these systems however is portability. Current systems only perform well if topic-specific models trained from similar lectures are available. For each new topic, significant effort and cost is required to manually transcribe and translate similar lectures, without which the system will generally perform poorly. In this work, we propose to overcome this limitation by introducing approaches to automatically adapt speech translation systems to the diverse topics that occur in educational lectures. Utilizing materials that are available before the lecture begins, such as"
2011.iwslt-papers.4,E03-1050,0,0.0224057,"e remained high compared to using topic-specific vocabularies. In this work, we propose a novel approach to improve vocabulary coverage based on a feature-based vocabulary ranking scheme applied on documents automatically collected from the WWW. Our proposed approach improves vocabulary coverage, LM perplexity, and speech recognition accuracy compared to a lecture-independent system and further improves the effectiveness of other adaptation approaches including both language model adaptation for speech recognition [2] and possibly the adaptation of machine translation using comparable corpora [6]. 2. The interACT Simultaneous Lecture Translation System The interACT Simultaneous Lecture Translation System [1] is a real-time lecture translation system developed at the InFigure 1: The interACT Lecture Translation System. Web-‐based Topic Adapta3on Source Language Acous2c Model Vocabulary Speech Recogni2on Target Language Language Model Text Transla3on Model Language Model Machine Transla2on Voice Speech Synthesis Figure 2: Components of the Lecture Translation System ternational Center for Advanced Communication Technologies (interACT) at Karlsruhe Institute of Technology (Germany) and"
2011.iwslt-papers.4,2003.mtsummit-papers.53,1,0.782481,"possible to send the translated audio only to a small group of people while the other listeners are not disturbed. Figure 2 illustrates the three main components of our lecture translation system: Automatic speech recognition (ASR), machine translation (MT), and speech synthesis (Text-to-Speech, TTS). Input speech from the lecturer is recognized by the ASR component [7] and the resulting output is segmented into sentence-like units which are then passed to MT. The resulting segments are then translated into one or more target languages via our statistical machine translation (SMT) engine STTK [8]. The translated text is either directly displayed to attendees or optionally converted 215 Seed document(s) Seed document(s) Word Extrac+on Document Collec5on Words Queries Selec+on Document Corpus Topic-‐independent Lecture Vocabulary Queries Web-‐Search Vocabulary Selec5on Language Model Adapta5on Acous,c Model Vocabulary Speech Recogni,on Documents Topic-‐independent Corpora Language Veriﬁca+on Document Corpus Language Model Text Figure 3: Unsupervised Vocabulary Selection and Language Model Adaptation into speech output using a TTS engine. For each lecture the speech recognition vocabu"
2011.iwslt-papers.6,C88-1016,0,0.659552,"Missing"
2011.iwslt-papers.6,P11-2071,0,0.0190164,"erent translations for one word. In Section 5 we describe the generation of morphological operations that allow us to handle different morphological forms of the domain specific terms. In the end, we will evaluate the approach and close with a conclusion. 2. Related Work To adapt an SMT system towards a new domain, different problems have to be solved. One important question is to find translations for domain specific terms. The other main direction of research is to adapt the parameters of the proba230 bilistic models. There have been several approaches to acquire domain specific vocabulary. [2] used canonical correlation analysis to mine unseen words in comparable data. They used different approaches to integrate the new found translations into their SMT system and could show improvements on 4 different domains in German to English and French to English translation. Another approach to extract translations for rare words from comparable corpora was presented in [3]. In [4] domain specific vocabulary was acquired by using a bilingual dictionary. Different ways to integrate the vocabulary were investigated. They also performed research on adapting the parameters with in-domain monolin"
2011.iwslt-papers.6,P11-1133,0,0.031428,"is to find translations for domain specific terms. The other main direction of research is to adapt the parameters of the proba230 bilistic models. There have been several approaches to acquire domain specific vocabulary. [2] used canonical correlation analysis to mine unseen words in comparable data. They used different approaches to integrate the new found translations into their SMT system and could show improvements on 4 different domains in German to English and French to English translation. Another approach to extract translations for rare words from comparable corpora was presented in [3]. In [4] domain specific vocabulary was acquired by using a bilingual dictionary. Different ways to integrate the vocabulary were investigated. They also performed research on adapting the parameters with in-domain monolingual data by linear and log-linear combinations of the models. The approaches to adapt the parameters were inspired by similar approaches in speech recognition ([5]). Among them approaches using only monolingual data can be distinguished from those using parallel in-domain data. If only monolingual data is used either only the language model is adapted or the translation mode"
2011.iwslt-papers.6,C08-1125,0,0.0629483,"nd translations for domain specific terms. The other main direction of research is to adapt the parameters of the proba230 bilistic models. There have been several approaches to acquire domain specific vocabulary. [2] used canonical correlation analysis to mine unseen words in comparable data. They used different approaches to integrate the new found translations into their SMT system and could show improvements on 4 different domains in German to English and French to English translation. Another approach to extract translations for rare words from comparable corpora was presented in [3]. In [4] domain specific vocabulary was acquired by using a bilingual dictionary. Different ways to integrate the vocabulary were investigated. They also performed research on adapting the parameters with in-domain monolingual data by linear and log-linear combinations of the models. The approaches to adapt the parameters were inspired by similar approaches in speech recognition ([5]). Among them approaches using only monolingual data can be distinguished from those using parallel in-domain data. If only monolingual data is used either only the language model is adapted or the translation model is ada"
2011.iwslt-papers.6,2009.mtsummit-posters.17,0,0.0175943,"rmed research on adapting the parameters with in-domain monolingual data by linear and log-linear combinations of the models. The approaches to adapt the parameters were inspired by similar approaches in speech recognition ([5]). Among them approaches using only monolingual data can be distinguished from those using parallel in-domain data. If only monolingual data is used either only the language model is adapted or the translation model is adapted using synthetic parallel data which is generated by translating the monolingual data using a baseline MT system. This is for example done in [6], [7] and [8]. In this case, of course, no new translation options can be learned. In the other case, where also parallel in-domain data is available, authors tried different linear and log-linear combinations of the in-domain and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language proces"
2011.iwslt-papers.6,W09-0432,0,0.0203857,"earch on adapting the parameters with in-domain monolingual data by linear and log-linear combinations of the models. The approaches to adapt the parameters were inspired by similar approaches in speech recognition ([5]). Among them approaches using only monolingual data can be distinguished from those using parallel in-domain data. If only monolingual data is used either only the language model is adapted or the translation model is adapted using synthetic parallel data which is generated by translating the monolingual data using a baseline MT system. This is for example done in [6], [7] and [8]. In this case, of course, no new translation options can be learned. In the other case, where also parallel in-domain data is available, authors tried different linear and log-linear combinations of the in-domain and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. Fo"
2011.iwslt-papers.6,W07-0717,0,0.0346657,"es using only monolingual data can be distinguished from those using parallel in-domain data. If only monolingual data is used either only the language model is adapted or the translation model is adapted using synthetic parallel data which is generated by translating the monolingual data using a baseline MT system. This is for example done in [6], [7] and [8]. In this case, of course, no new translation options can be learned. In the other case, where also parallel in-domain data is available, authors tried different linear and log-linear combinations of the in-domain and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been s"
2011.iwslt-papers.6,W07-0733,0,0.0237567,"ing only monolingual data can be distinguished from those using parallel in-domain data. If only monolingual data is used either only the language model is adapted or the translation model is adapted using synthetic parallel data which is generated by translating the monolingual data using a baseline MT system. This is for example done in [6], [7] and [8]. In this case, of course, no new translation options can be learned. In the other case, where also parallel in-domain data is available, authors tried different linear and log-linear combinations of the in-domain and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several"
2011.iwslt-papers.6,D10-1044,0,0.017086,"a is used either only the language model is adapted or the translation model is adapted using synthetic parallel data which is generated by translating the monolingual data using a baseline MT system. This is for example done in [6], [7] and [8]. In this case, of course, no new translation options can be learned. In the other case, where also parallel in-domain data is available, authors tried different linear and log-linear combinations of the in-domain and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improve"
2011.iwslt-papers.6,D08-1090,0,0.0251582,"slating the monolingual data using a baseline MT system. This is for example done in [6], [7] and [8]. In this case, of course, no new translation options can be learned. In the other case, where also parallel in-domain data is available, authors tried different linear and log-linear combinations of the in-domain and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improvements when translating into morphologically rich languages by integrating an additional mophological model using an maximum entropy model introd"
2011.iwslt-papers.6,P07-2045,0,0.00483631,"edia into a translation system on the task of translating German computer science lectures into English. The baseline system is described in detail below. In addition to the improvements measured by automatic The translation system was trained on the European Parliament corpus, News Commentary corpus and the BTEC corpus. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in [20] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package [21]. The language model was trained on the target side of the parallel data. Reordering was performed as a preprocessing step using POS information from the TreeTagger [22]. We used the reordering approach described in [23] together with the extensions presented in [24] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MER training. When adding the Wikipedia data no new optimizations were performed. We used transcribed university l"
2011.iwslt-papers.6,2007.tmi-papers.21,0,0.520717,"tion system was trained on the European Parliament corpus, News Commentary corpus and the BTEC corpus. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in [20] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package [21]. The language model was trained on the target side of the parallel data. Reordering was performed as a preprocessing step using POS information from the TreeTagger [22]. We used the reordering approach described in [23] together with the extensions presented in [24] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MER training. When adding the Wikipedia data no new optimizations were performed. We used transcribed university lectures from the computer science department as development and test data. Each set contains around 30K words. 6.2. Integration The results for the baseline system as well as the results for both methods (Lexicon and Cor"
2011.iwslt-papers.6,W09-0413,1,0.851372,"ent corpus, News Commentary corpus and the BTEC corpus. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in [20] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package [21]. The language model was trained on the target side of the parallel data. Reordering was performed as a preprocessing step using POS information from the TreeTagger [22]. We used the reordering approach described in [23] together with the extensions presented in [24] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MER training. When adding the Wikipedia data no new optimizations were performed. We used transcribed university lectures from the computer science department as development and test data. Each set contains around 30K words. 6.2. Integration The results for the baseline system as well as the results for both methods (Lexicon and Corpus) to integrate the data described in Section"
2011.iwslt-papers.6,2010.iwslt-evaluation.11,1,0.813051,"be combined with other adaptation techniques. Although it is hard to find parallel data that matches the domain of university lectures in computer science, it is possible to find data that at least matches the genre. In our case, we used the TED corpus consisting of the subtitles and translations of the talks published on the TED Website1 . We built additional translation systems, one which just uses the additional data from the TED corpus and one that is also adapted towards TED using a log-linear combination for the phrase table as well as for the language model as for example described in [25]. The results of these experiments are described in Table 5. First, we repeated the result for using the information from Wikipedia on the baseline system without and with the morphological operations. Afterwards, we performed the same series of experiments first with the system using in addition the TED corpus and secondly, using the system also adapted to the TED corpus. As it can be seen using the additional data from the same genre could improve the translation quality significantly on the devlopment as well as on the test set. But in all cases further improvements using also the Wikipedia"
2011.iwslt-papers.6,2009.mtsummit-posters.26,0,0.0311766,"n and out-of-domain data ([9], [10]). Others could improve the adaptation by using a discriminative approach to select the adaptation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improvements when translating into morphologically rich languages by integrating an additional mophological model using an maximum entropy model introduced by Minkov et al. ([17]). Bojar and Tamachyna tried to handle morphological problems by using reverse self training ([18]). Macherey et al. ([19]) tried to learn morphological operations for parts of the translation system. They could improve compound splitting as done in a machine translation system by lear"
2011.iwslt-papers.6,P08-1059,0,0.0205617,"tation weights ([11], [12]). Another direction of research is to find in-domain documents, for example by using cross-lingual information retrieval techniques ([13]). Wikipedia has already been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improvements when translating into morphologically rich languages by integrating an additional mophological model using an maximum entropy model introduced by Minkov et al. ([17]). Bojar and Tamachyna tried to handle morphological problems by using reverse self training ([18]). Macherey et al. ([19]) tried to learn morphological operations for parts of the translation system. They could improve compound splitting as done in a machine translation system by learning morphological operations for the compound parts [19]. 3. Lexicon creation Wikipedia is a multilingual encyclopedia contain"
2011.iwslt-papers.6,P07-1017,0,0.0315678,"y been shown to be a valuable resource for natural language processing. For example, Erdmann et al. proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improvements when translating into morphologically rich languages by integrating an additional mophological model using an maximum entropy model introduced by Minkov et al. ([17]). Bojar and Tamachyna tried to handle morphological problems by using reverse self training ([18]). Macherey et al. ([19]) tried to learn morphological operations for parts of the translation system. They could improve compound splitting as done in a machine translation system by learning morphological operations for the compound parts [19]. 3. Lexicon creation Wikipedia is a multilingual encyclopedia containing articles about the same topics in different languages. To be able to extract bilingual terms the so-called inter-language links are very important. They link pages in different langua"
2011.iwslt-papers.6,W11-2138,0,0.0168165,". proposed to extract bilingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improvements when translating into morphologically rich languages by integrating an additional mophological model using an maximum entropy model introduced by Minkov et al. ([17]). Bojar and Tamachyna tried to handle morphological problems by using reverse self training ([18]). Macherey et al. ([19]) tried to learn morphological operations for parts of the translation system. They could improve compound splitting as done in a machine translation system by learning morphological operations for the compound parts [19]. 3. Lexicon creation Wikipedia is a multilingual encyclopedia containing articles about the same topics in different languages. To be able to extract bilingual terms the so-called inter-language links are very important. They link pages in different languages about the same topic. Using these links we can align the articles in source and target languag"
2011.iwslt-papers.6,P11-1140,0,0.0445901,"lingual terms from the Wikipedia titles ([14]) or Yu et. al. use terms from a comparable corpus created from the Wikipedia articles using the inter-language links ([15]). There have also been several attempts to model the morphology more explicitly in machine translation. Toutanova et al. ([16]) showed improvements when translating into morphologically rich languages by integrating an additional mophological model using an maximum entropy model introduced by Minkov et al. ([17]). Bojar and Tamachyna tried to handle morphological problems by using reverse self training ([18]). Macherey et al. ([19]) tried to learn morphological operations for parts of the translation system. They could improve compound splitting as done in a machine translation system by learning morphological operations for the compound parts [19]. 3. Lexicon creation Wikipedia is a multilingual encyclopedia containing articles about the same topics in different languages. To be able to extract bilingual terms the so-called inter-language links are very important. They link pages in different languages about the same topic. Using these links we can align the articles in source and target language. Although the articles"
2011.iwslt-papers.6,W08-0303,1,0.774517,"se pairs, since there is no phrase pair that exactly matches for f2 . 6. Results We evaluated the described approach to integrate data from Wikipedia into a translation system on the task of translating German computer science lectures into English. The baseline system is described in detail below. In addition to the improvements measured by automatic The translation system was trained on the European Parliament corpus, News Commentary corpus and the BTEC corpus. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in [20] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package [21]. The language model was trained on the target side of the parallel data. Reordering was performed as a preprocessing step using POS information from the TreeTagger [22]. We used the reordering approach described in [23] together with the extensions presented in [24] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the op"
2011.iwslt-papers.6,D09-1074,0,\N,Missing
2011.mtsummit-papers.8,W04-2902,0,0.0895448,"Missing"
2011.mtsummit-papers.8,2008.iwslt-papers.5,1,0.820781,"Missing"
2011.mtsummit-papers.8,2003.mtsummit-papers.53,1,0.782634,"vocabulary and a source language model which models the likelihood of word sequences. The MT component consists of a translation model, which generates and scores translation alternatives in the target language, and a target language model which generates likelihoods for competing word sequences. Input speech from the lecturer is recognized by the ASR component (Soltau et al., 2001) and the resulting output is segmented into sentence-like units which are then passed to MT. The ASR output is then translated into one or more target languages via our statistical machine translation engine STTK (Vogel et al., 2003). The translated text is either directly displayed to attendees or optionally converted into speech output using a TTS engine. In this work, we introduce a web-based topic adaptation approach which adapts the four models indicated in Figure 2. Adaptation is performed using documents related to the lecture at hand, for example slides or lecture notes. In this paper, we focus on vocabulary selection for the speech recognition component, but our proposed approach can also be applied to adapt source and target language models and the translation model. 3    Figure 3: Document Collection and"
2011.mtsummit-papers.8,E03-1050,0,0.0276726,"n System. the out-of-vocabulary rate often remains high compared to using topic speciﬁc vocabularies. In this work, we propose a novel approach to improve vocabulary coverage based on a feature-based vocabulary ranking scheme and documents collected from the WWW. Our proposed approach signiﬁcantly improves vocabulary coverage compared to a lectureindependent system and further improves the effectiveness of other adaptation approaches including both language model adaptation for speech recognition (Munteanu et al., 2007) and adaptation of machine translation models based on comparable corpora (Vogel, 2003). 2 The interACT Simultaneous Lecture Translation System The interACT Simultaneous Lecture Translation System (F¨ugen, 2009; Kolss et al., 2008) is a realtime lecture translation system developed at the international center for Advanced Communication Technologies (interACT) at Karlsruhe Institute of Technology (Germany) and Carnegie Mellon University (USA). This system, illustrated in Figure 1, simultaneously translates lectures in real-time from the speaker’s language into multiple languages required by the audience. To minimize the distraction to the audience, our system delivers translation"
2012.amta-papers.19,2011.iwslt-evaluation.18,0,0.131199,"rase table. If the phrase pair does not occur in the in-domain phrase table, they use a backoff value. We will refer to this method as “Backoff ”. In (Niehues and Waibel, 2010) a factored translation model was used to adapt the translation model. They used the general scores together with the indomain or out-of-domain relative frequencies. In addition, a word factor represents the part of the corpus where the phrase is extracted from. Then a Domain Sequence Model counts the number of in-domain phrase pairs or words (“Factored”). Another approach using the “Fill-Up“ technique was described in (Bisazza et al., 2011). They used the in-domain and out-of-domain scores and an indicator feature. The out-of-domain scores were only used if no in-domain probabilities were available. An approach based on mixture models was presented by Foster and Kuhn (2007) and Banerjee et al. (2011). They used linear and log-linear, language model and translation model adaptation. Furthermore, they optimized the weights for the different domains on a development set and the weights are set according to text distance measures. Matsoukas et al. (2009) also adapt the system by changing the weights of the phrase pairs. In their app"
2012.amta-papers.19,J93-2003,0,0.0170495,"echniques to adapt the phrase pair scoring and characterized them by using four key aspects. By analyzing the steps separately, we are able to combine the techniques from the approaches in a new way and improve the translation quality even further. The different strategies were evaluated on two different tasks of translating German to English: the translation of TED lectures1 and computer science university lectures. Introduction Statistical machine translation (SMT) is currently the most promising approach to machine translation of large vocabulary tasks. The approach was first presented in (Brown et al., 1993) and has been used in many translation systems since then. One drawback of this approach is that large amounts of training data are needed. Furthermore, the performance of the SMT system improves if this data is matching in topic and genre. Since this is not possible for many real-world scenarios, one approach to overcome this problem is to use all available data to train a general system and to adapt the system to the task at hand using in-domain training data. Since parallel in-domain data is available for our scenario, we will focus on the adaptation of the 2 Related work In recent years di"
2012.amta-papers.19,W07-0717,0,0.0673645,"on model. They used the general scores together with the indomain or out-of-domain relative frequencies. In addition, a word factor represents the part of the corpus where the phrase is extracted from. Then a Domain Sequence Model counts the number of in-domain phrase pairs or words (“Factored”). Another approach using the “Fill-Up“ technique was described in (Bisazza et al., 2011). They used the in-domain and out-of-domain scores and an indicator feature. The out-of-domain scores were only used if no in-domain probabilities were available. An approach based on mixture models was presented by Foster and Kuhn (2007) and Banerjee et al. (2011). They used linear and log-linear, language model and translation model adaptation. Furthermore, they optimized the weights for the different domains on a development set and the weights are set according to text distance measures. Matsoukas et al. (2009) also adapt the system by changing the weights of the phrase pairs. In their approach this is done by assigning discriminative weights for all sentences in the parallel corpus. 3 Translation model When parallel in-domain data is available, we want to adapt the translation model to be able to encode the domain specifi"
2012.amta-papers.19,W06-1607,0,0.0639312,"n of histogram and beam pruning. We used at most 10 translations for every source phrase. We rank the phrase pairs in order to be able to select the top translations by scoring them using some initial weights that were determined heuristically. These weights are independent from the weights generated by MERT. In the baseline phrase table we use four different scores Φs ((f¯i , e¯i )) for every phrase pair (f¯i , e¯i ). The relative frequencies in both directions and the lexical probabilities in both directions. We use modified Kneser-Ney smoothing for the relative frequencies as described in (Foster et al., 2006). This leads to the following definition of the translation model when translating the source sentence f = f¯1I into the target sentence e = e¯I1 using the phrase pairs ((f¯1 , e¯1 ),(f¯2 , e¯2 ),..,(f¯I , e¯I )) log(p(e¯I1 |f¯1I )) = = I X log(p(e¯i |f¯i )) i=1 I X S X (1) λs log(Φs ((f¯i , e¯i ))) i=1 s=1 −log(Z) (2) The weights used for the four scores during the actual decoding are optimized using MER training on the development data. In our scenario there are three different phrase tables. One trained only on the in-domain data providing the candidate translations TIN (f¯i ) for a given s"
2012.amta-papers.19,W07-0733,0,0.0420753,"train a general system and to adapt the system to the task at hand using in-domain training data. Since parallel in-domain data is available for our scenario, we will focus on the adaptation of the 2 Related work In recent years different methods were proposed to adapt translation systems to a specific domain. Some adapted only the language model inspired by similar approaches in speech recognition (Bulyko et al., 2007). The main advantage is that only monolingual in-domain data is needed. In cases where also parallel in-domain data is available, the translation model can be adapted as well. Koehn and Schroeder (2007) proposed to use a log-linear combination of the in-domain and out-ofdomain phrase table. We will refer to this approach as “Log-Linear”. In (Niehues et al., 2010), the translation model is 1 http://www.ted.com adapted by adding the in-domain relative frequencies to the general phrase table. If the phrase pair does not occur in the in-domain phrase table, they use a backoff value. We will refer to this method as “Backoff ”. In (Niehues and Waibel, 2010) a factored translation model was used to adapt the translation model. They used the general scores together with the indomain or out-of-domain"
2012.amta-papers.19,P07-2045,0,0.00761247,"that are significantly better than the baseline system at a level of 0.05 are marked by a star(*). 6.1 System Description The translation system was trained on the European Parliament corpus, News Commentary corpus and the BTEC corpus. As parallel in-domain data, the TED talks were used in addition. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in (Niehues et al., 2011). Table 1: Different phrase table adaptation approaches Approach Candidate Selection General Log-Lin Backoff Factored Fill-Up UnionOut NoAdapt NoAdapt UnionOut X X Score Selection Adapted Unknown Prob. all Log-Lin 2 Backoff 2 Indicator all Indicator Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach desc"
2012.amta-papers.19,D09-1074,0,0.12043,"ds (“Factored”). Another approach using the “Fill-Up“ technique was described in (Bisazza et al., 2011). They used the in-domain and out-of-domain scores and an indicator feature. The out-of-domain scores were only used if no in-domain probabilities were available. An approach based on mixture models was presented by Foster and Kuhn (2007) and Banerjee et al. (2011). They used linear and log-linear, language model and translation model adaptation. Furthermore, they optimized the weights for the different domains on a development set and the weights are set according to text distance measures. Matsoukas et al. (2009) also adapt the system by changing the weights of the phrase pairs. In their approach this is done by assigning discriminative weights for all sentences in the parallel corpus. 3 Translation model When parallel in-domain data is available, we want to adapt the translation model to be able to encode the domain specific knowledge without losing the information learned from the much bigger parallel corpus. If we compare the different approaches of translation model adaptation, we see that two main aspects of the model can be adapted to better match the specific domain. The first aspect is the can"
2012.amta-papers.19,W09-0435,1,0.77875,"ata using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in (Niehues et al., 2011). Table 1: Different phrase table adaptation approaches Approach Candidate Selection General Log-Lin Backoff Factored Fill-Up UnionOut NoAdapt NoAdapt UnionOut X X Score Selection Adapted Unknown Prob. all Log-Lin 2 Backoff 2 Indicator all Indicator Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in (Rottmann and Vogel, 2007) and the extensions presented in (Niehues and Kolss, 2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MER training. We used TED talks as development and test data. In addition, we tested the systems on transcribed university lectures from the computer science(CS) department. Each test set contains at least 30K words. 6.2 Baseline In a first series of experiments we show the influence of the in-domain and out-of-domain data. We tested the systems on both tasks using three differe"
2012.amta-papers.19,W08-0303,1,0.796774,"l evaluate the influence of the candidate selection and aspects of the phrase scoring. We performed significance tests following (Zhang and Vogel, 2004). All results that are significantly better than the baseline system at a level of 0.05 are marked by a star(*). 6.1 System Description The translation system was trained on the European Parliament corpus, News Commentary corpus and the BTEC corpus. As parallel in-domain data, the TED talks were used in addition. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in (Niehues et al., 2011). Table 1: Different phrase table adaptation approaches Approach Candidate Selection General Log-Lin Backoff Factored Fill-Up UnionOut NoAdapt NoAdapt UnionOut X X Score Selection Adapted Unknown Prob. all Log-Lin 2 Backoff 2 Indicator all"
2012.amta-papers.19,2010.eamt-1.29,1,0.757814,"monolingual in-domain data is needed. In cases where also parallel in-domain data is available, the translation model can be adapted as well. Koehn and Schroeder (2007) proposed to use a log-linear combination of the in-domain and out-ofdomain phrase table. We will refer to this approach as “Log-Linear”. In (Niehues et al., 2010), the translation model is 1 http://www.ted.com adapted by adding the in-domain relative frequencies to the general phrase table. If the phrase pair does not occur in the in-domain phrase table, they use a backoff value. We will refer to this method as “Backoff ”. In (Niehues and Waibel, 2010) a factored translation model was used to adapt the translation model. They used the general scores together with the indomain or out-of-domain relative frequencies. In addition, a word factor represents the part of the corpus where the phrase is extracted from. Then a Domain Sequence Model counts the number of in-domain phrase pairs or words (“Factored”). Another approach using the “Fill-Up“ technique was described in (Bisazza et al., 2011). They used the in-domain and out-of-domain scores and an indicator feature. The out-of-domain scores were only used if no in-domain probabilities were ava"
2012.amta-papers.19,2010.iwslt-evaluation.11,1,0.888005,"focus on the adaptation of the 2 Related work In recent years different methods were proposed to adapt translation systems to a specific domain. Some adapted only the language model inspired by similar approaches in speech recognition (Bulyko et al., 2007). The main advantage is that only monolingual in-domain data is needed. In cases where also parallel in-domain data is available, the translation model can be adapted as well. Koehn and Schroeder (2007) proposed to use a log-linear combination of the in-domain and out-ofdomain phrase table. We will refer to this approach as “Log-Linear”. In (Niehues et al., 2010), the translation model is 1 http://www.ted.com adapted by adding the in-domain relative frequencies to the general phrase table. If the phrase pair does not occur in the in-domain phrase table, they use a backoff value. We will refer to this method as “Backoff ”. In (Niehues and Waibel, 2010) a factored translation model was used to adapt the translation model. They used the general scores together with the indomain or out-of-domain relative frequencies. In addition, a word factor represents the part of the corpus where the phrase is extracted from. Then a Domain Sequence Model counts the num"
2012.amta-papers.19,W11-2124,1,0.725016,"Commentary corpus and the BTEC corpus. As parallel in-domain data, the TED talks were used in addition. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in (Niehues et al., 2011). Table 1: Different phrase table adaptation approaches Approach Candidate Selection General Log-Lin Backoff Factored Fill-Up UnionOut NoAdapt NoAdapt UnionOut X X Score Selection Adapted Unknown Prob. all Log-Lin 2 Backoff 2 Indicator all Indicator Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in (Rottmann and Vogel, 2007) and the extensions presented in (Niehues and Kolss, 2009) to cover long-range reorderings, which are typical when translating between German and English. An in-hou"
2012.amta-papers.19,2007.tmi-papers.21,0,0.610892,"uage model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in (Niehues et al., 2011). Table 1: Different phrase table adaptation approaches Approach Candidate Selection General Log-Lin Backoff Factored Fill-Up UnionOut NoAdapt NoAdapt UnionOut X X Score Selection Adapted Unknown Prob. all Log-Lin 2 Backoff 2 Indicator all Indicator Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in (Rottmann and Vogel, 2007) and the extensions presented in (Niehues and Kolss, 2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MER training. We used TED talks as development and test data. In addition, we tested the systems on transcribed university lectures from the computer science(CS) department. Each test set contains at least 30K words. 6.2 Baseline In a first series of experiments we show the influence of the in-domain and out-of-domain dat"
2012.amta-papers.19,2004.tmi-1.9,0,0.0416432,"view over the different aspects of the four approaches to phrase table adapation as mentioned in the related work is given in Table 1. 6 Results After analyzing the different approaches we will now evaluate their effects on translation quality. We perform experiments on two different German-toEnglish speech translation tasks. First, we describe the SMT system and then we run some baseline experiments to demonstrate the characteristics of the data. Afterwards, we will evaluate the influence of the candidate selection and aspects of the phrase scoring. We performed significance tests following (Zhang and Vogel, 2004). All results that are significantly better than the baseline system at a level of 0.05 are marked by a star(*). 6.1 System Description The translation system was trained on the European Parliament corpus, News Commentary corpus and the BTEC corpus. As parallel in-domain data, the TED talks were used in addition. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the M"
2012.iwslt-evaluation.10,2012.iwslt-evaluation.1,0,0.0411309,"two stages, where the systems of the second stage are adapted in an unsupervised manner on the combination of the ﬁrst stage outputs using VTLN, MLLR, and cMLLR. Index Terms: speech recognition, IWSLT, TED talks, evaluation system, system development 1. Introduction The International Workshop on Spoken Language Translation (IWSLT) offers a comprehensive evaluation campaign on spoken language translation. One part of the campaign focuses on the translation of TED Talks1 , short 5-25min presentations by people from various ﬁelds related in some way to Technology, Entertainment, and Design (TED) [1]. In order to evaluate different aspects of this task IWSLT organizes several evaluation tracks on this data covering the aspects of automatic speech recognition (ASR), machine translation (MT), and the full-ﬂedged combination of the two of them into speech translation systems. The goal of the TED ASR track is the automatic transcription of TED lectures on a given segmentation, in order to interface with the machine translation components in the speech-translation track. The quality of the resulting transcriptions are measured in word error rate (WER). In this paper we describe our English ASR"
2012.iwslt-evaluation.10,2011.iwslt-evaluation.12,1,0.446759,"nslation (MT), and the full-ﬂedged combination of the two of them into speech translation systems. The goal of the TED ASR track is the automatic transcription of TED lectures on a given segmentation, in order to interface with the machine translation components in the speech-translation track. The quality of the resulting transcriptions are measured in word error rate (WER). In this paper we describe our English ASR systems with which we participated in the TED ASR track of the 2012 IWSLT evaluation campaign. This year, our system is a further development of our last year’s evaluation system [2] and makes use of system combination and cross-adaptation, by utilising acoustic models which are trained with different acoustic front-ends and employ two different phoneme sets. In addition to last year, we also included TED talks available via TED’s website by training on them in a slightly supervised manner. We submitted two primary systems. One was solely developed by KIT, the other one was developed in cooperation with NAIST in Japan. A description of the additional work done by NAIST on the KIT-NAIST (contrastive) submission can be found in [3]. On the 2011 evaluations set, which serves"
2012.iwslt-evaluation.10,2012.iwslt-evaluation.11,1,0.864393,"pment of our last year’s evaluation system [2] and makes use of system combination and cross-adaptation, by utilising acoustic models which are trained with different acoustic front-ends and employ two different phoneme sets. In addition to last year, we also included TED talks available via TED’s website by training on them in a slightly supervised manner. We submitted two primary systems. One was solely developed by KIT, the other one was developed in cooperation with NAIST in Japan. A description of the additional work done by NAIST on the KIT-NAIST (contrastive) submission can be found in [3]. On the 2011 evaluations set, which serves as a progress test set, we were able to reduce the word error rate of our transcription 1 http://www.ted.com/talks Text corpus IWSLT training data transcripts News (+news commentary) Parallel Giga Corpus LDC English Gigaword 4 UN + Europarl documents Google Books Ngrams (subset) total Word Count 3 million 2114 million 523 million 1800 million 376 million 1000 million ngrams 4816 million sources 2 4 1 6 1 1 15 Table 1: Language Model training data word count per corpus after cleaning and data selection and number of text sources included in corpus. Th"
2012.iwslt-evaluation.10,2011.iwslt-papers.2,1,0.705542,"of the ﬁnal feature vectors was empirically proven to work well and coincides with the dimensionality of a 14 dimensional static feature vector augmented with ﬁrst and second order dynamic features. In recent years neural network based features have been shown to improve ASR systems [6]. A typical setup involves training a neural network to recognize phones (or phone-states) from a window of ordinary (e.g. MFCC) feature vectors. With the help a hidden bottleneck layer the trained network can be used to project the input features onto a feature vector with an arbitrarily chosen dimensionality [7]. The input vector is derived from a 15 frame context window with each frame containing 20 MFCC or MVDR coefﬁcients. So far, we used LDA to reduce the dimensionality of this input vector, which limits the resulting LDA-features to linear combinations of the input features. A multi layer perceptron (MLP) with the bottleneck in the 2nd hidden layer can make use of nonlinear information. For our IWSLT systems we used bottleneck features for both our MVDR and MFCC front ends. 4. Acoustic Modeling 4.1. Data Preprocessing For the TED data only subtitles were available so the data had to be split int"
2012.iwslt-evaluation.11,2011.iwslt-evaluation.1,1,0.866959,"Missing"
2012.iwslt-evaluation.11,P07-1085,0,0.0270438,"Missing"
2012.iwslt-evaluation.11,N10-1103,0,0.0548812,"Missing"
2012.iwslt-papers.10,2011.iwslt-papers.2,1,0.667623,"ited sections contained more than 1000 words have been included in the EVAL set to make it reasonably different from the EDIT set. 4.2. Baseline ASR System The baseline hypothesis which are displayed in the web interface and are editable by the user are produced with the Janus Speech Recognition Toolkit’s Ibis Decoder [15] through a confusion network combination (CNC) [13] of ﬁve speaker independent systems developed for the 2011 Quaero Evaluation as depicted in Figure 2. It is an improved version of the 2010 evaluation system [16] and similar to the Spanish system described by Kilgour et al. [17]. The underlying systems use three different frontends, melfrequency cepstral coefﬁcients (MFCC), warped minimum variance distortionless response (MVDR), and MVDR based bottleneck features (MVDR-BNF). Additionally, two systems use graphemes instead of phonemes. The system combination has been chosen to provide state-of-the-art transcripts as basis for corrections. The language model is built from the transcripts of the quaero training data, scraped newspaper data and webdumps. The vocabulary is case-sensitive and fairly large containing roughly 300k sub-words and 480k pronunciation variants. T"
2012.iwslt-papers.10,P09-1086,0,0.0689391,"Missing"
2012.iwslt-papers.10,stuker-etal-2012-kit,1,0.817133,"raction. It is possible to redirect hypotheses of online recognition into the web interface. 4. Experimental Setup A user study was performed to evaluate the correction performance of students using the web interface. Since corrections will typically take place “ofﬂine” (not during the lecture), the initial ASR hypotheses have been generated by a system combination to achieve a high-quality basis for subsequent editing. 4.1. Corpus Characteristics For the experiment, German lectures from a variety of topics were used. The lectures form a subset of the KIT Lecture Corpus for Speech Translation [14]. The lectures “Algorithms for Planar Graphs” (ALGO), “Formal Systems” (FORM), “Cognitive Systems” (COGSYS), “Machine Translation” (MT) and “Multiprocessors” (PROC) cover different areas of computer science. The “Technical Mechanics” (MECH) lecture is from an unrelated, but still technical area, whereas the lectures about “Population Geography” (GEO), “World War 2” (WW2) and ”Copyright Law” (LAW) cover non-technical topics. 1 https://developers.google.com/web-toolkit/     Figure 2: Decoding systems for generation of baseline transcription The lectures were recorded with a c"
2012.iwslt-papers.15,2011.iwslt-papers.7,0,0.599292,"ve proper segmentation before the translation to match the translation models in order to achieve better translation quality. Moreover, there are algorithmic constraints as well as user preferences, such as readability. When a sentence is excessively long, it either consumes a great deal of resources and time, or readability suffers. If the input is already augmented with punctuation in the source language, it is advantageous to the training procedure of MT. In this case, there is no need to retrain the translation system with modiﬁcation on the training data, in order to match the ASR output [1]. Nevertheless, most of the current ASR systems do not provide punctuation marks. It is one of the challenging tasks to restore segmentation and punctuation in the output of an ASR system, especially for speech translation. Sentence segmentation in the ASR system is often generated using prosodic features (pause duration, pitch, etc.) and lexical cues (e.g. language model probability). However, the performance of sentence segmentation degrades in spontaneous speech. This is because a large amount of the spontaneous utterance is less grammatical compared to written texts [2] and there are fewer"
2012.iwslt-papers.15,2005.eamt-1.37,0,0.0194998,"thors made an extensive analysis on how to predict punctuation using a machine translation system. In this work, it was assumed that the ASR output already has the proper segmentation, which is sentence-like units. They investigated three different approaches to restore punctuation marks; prediction in the source language, implicit prediction, and prediction in the target language. Using a translation system to translate from unpunctuated to punctuated text, they showed signiﬁcant improvements in the evaluation campaign of IWSLT 2011. Among different motivations for the sentence segmentation, [4] split long sentence pairs in the bilingual training corpora to make full use of training data and improved model estimation for statistical machine translation (SMT). For the splitting they used the lexicon information to ﬁnd splitting points. They showed that splitting sentences improved the performance for Chinese-English translation task. Similarly, to improve the performance of Example-based machine translation (EMBT) systems, [5] suggested a method to split sentences using sentence similarity based on editdistance. Combining prosodic and lexical information to detect sentence boundaries"
2012.iwslt-papers.15,C04-1017,0,0.0610404,"Missing"
2012.iwslt-papers.15,P07-2045,0,0.0447142,"anslation system is trained on 1.8 million sentences of German-English parallel data including the European Parliament data and News Commentary corpus. Before the training, the data is preprocessed and compound splitting for the German side is applied. Preprocessing consists of text normalization, tokenization, smartcasing, conversion of German words written according to the old spelling conventions into the new form of spelling. 1 http://www.ted.com 2 http://www.wikipedia.org 253 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 The Moses package [9] is used to build the phrase table. The 4-gram language model is trained on the English side of the above data with nearly 425 million words using the SRILM toolkit [10]. To extend source word context, a bilingual language model [11] is used. The POS-based reordering model as described in [12] is used for word reordering in order to account for the different word orders in source and target language. To cover long-range reorderings, we apply the modiﬁed reordering model as described in [13]. The translation hypotheses are generated using an in-house phrase-based decoder [14] and the optimizati"
2012.iwslt-papers.15,W11-2124,1,0.59087,"man side is applied. Preprocessing consists of text normalization, tokenization, smartcasing, conversion of German words written according to the old spelling conventions into the new form of spelling. 1 http://www.ted.com 2 http://www.wikipedia.org 253 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 The Moses package [9] is used to build the phrase table. The 4-gram language model is trained on the English side of the above data with nearly 425 million words using the SRILM toolkit [10]. To extend source word context, a bilingual language model [11] is used. The POS-based reordering model as described in [12] is used for word reordering in order to account for the different word orders in source and target language. To cover long-range reorderings, we apply the modiﬁed reordering model as described in [13]. The translation hypotheses are generated using an in-house phrase-based decoder [14] and the optimization is performed using minimum error rate training (MERT) [15]. Translation models are built using the punctuated source side. Also for the other experiments, where there are no punctuation marks on the source side available, phrase t"
2012.iwslt-papers.15,2007.tmi-papers.21,0,0.540685,"tion, tokenization, smartcasing, conversion of German words written according to the old spelling conventions into the new form of spelling. 1 http://www.ted.com 2 http://www.wikipedia.org 253 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 The Moses package [9] is used to build the phrase table. The 4-gram language model is trained on the English side of the above data with nearly 425 million words using the SRILM toolkit [10]. To extend source word context, a bilingual language model [11] is used. The POS-based reordering model as described in [12] is used for word reordering in order to account for the different word orders in source and target language. To cover long-range reorderings, we apply the modiﬁed reordering model as described in [13]. The translation hypotheses are generated using an in-house phrase-based decoder [14] and the optimization is performed using minimum error rate training (MERT) [15]. Translation models are built using the punctuated source side. Also for the other experiments, where there are no punctuation marks on the source side available, phrase tables are prepared in the same way. 4. Oracle Experiments To"
2012.iwslt-papers.15,W09-0435,1,0.198885,"International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 The Moses package [9] is used to build the phrase table. The 4-gram language model is trained on the English side of the above data with nearly 425 million words using the SRILM toolkit [10]. To extend source word context, a bilingual language model [11] is used. The POS-based reordering model as described in [12] is used for word reordering in order to account for the different word orders in source and target language. To cover long-range reorderings, we apply the modiﬁed reordering model as described in [13]. The translation hypotheses are generated using an in-house phrase-based decoder [14] and the optimization is performed using minimum error rate training (MERT) [15]. Translation models are built using the punctuated source side. Also for the other experiments, where there are no punctuation marks on the source side available, phrase tables are prepared in the same way. 4. Oracle Experiments To investigate the impact of segmentation and punctuation marks on the translation quality, we conduct two experiments. In the ﬁrst experiment, we apply human-transcribed segments and punctuation marks to"
2012.iwslt-papers.15,W05-0836,1,0.940172,"odel is trained on the English side of the above data with nearly 425 million words using the SRILM toolkit [10]. To extend source word context, a bilingual language model [11] is used. The POS-based reordering model as described in [12] is used for word reordering in order to account for the different word orders in source and target language. To cover long-range reorderings, we apply the modiﬁed reordering model as described in [13]. The translation hypotheses are generated using an in-house phrase-based decoder [14] and the optimization is performed using minimum error rate training (MERT) [15]. Translation models are built using the punctuated source side. Also for the other experiments, where there are no punctuation marks on the source side available, phrase tables are prepared in the same way. 4. Oracle Experiments To investigate the impact of segmentation and punctuation marks on the translation quality, we conduct two experiments. In the ﬁrst experiment, we apply human-transcribed segments and punctuation marks to the output of the speech recognition system. Thus, words are still from an ASR system, but the segments and punctuation marks are reused from a human-generated trans"
2012.iwslt-papers.15,2011.iwslt-papers.6,1,0.827411,"ﬁcantly improved translation performance. 3. System Description In this section we brieﬂy introduce the statistical MT system that we use in this experiment. As we work on translating speech in this experiment, we use the parallel TED1 data and manual transcripts of lecture data containing 63k sentences as indomain data and adapt our models at the domain. The lecture data is collected internally at our university, and the domain of each lecture differs from the others. To better cope with domain-speciﬁc terminologies in university lectures, Wikipedia2 title information is used as presented in [8]. For development and testing, we use the lecture data from different speakers. These are also collected internally from university classes and events. They consist of talks of 30 to 45 minutes and the topic varies from one speech to the other. For the development set we use manual transcripts of lectures, while for testing we use the transcripts generated by an ASR system. The development set consists of 14K parallel sentences, with 30K words on the source side and 33K words on the target side including punctuation marks. Detailed information on the source side of the test set, including the"
2012.iwslt-papers.15,2005.iwslt-1.19,0,\N,Missing
2012.iwslt-papers.15,P02-1040,0,\N,Missing
2012.iwslt-papers.3,C90-3038,0,0.788621,"stricted Boltzmann Machine (RBM) can be calculated very efﬁciently. This enables us to use the language models during the decoding of the source sentence and not only in a re-scoring step. The remaining paper is structured as follows: First we will review related work. Afterwards a brief overview of Restricted Boltzmann Machines will be given before we describe the RBM-based language model. In Section 5 we describe the results on different translation tasks. Afterwards, we will give a conclusion. 2. Related Work A ﬁrst approach to predict word categories using neural networks was presented in [1]. Later, [2] used neuronal networks for statistical language modelling. They described in detail an approach based on multi-layer perceptrons and could show that this reduces the perplexity on a test set compared to n-gram-based and class-based language models. In addition, they gave a short outlook to energy minimization networks. An approach using multi-layer perceptrons has successfully been applied to speech recognition by [3], [4] and [5]. One main problem of continuous space language models is the size of the output vocabulary in large vocabulary continuous speech recognition. A ﬁrst way"
2012.iwslt-papers.3,W08-0303,1,0.53004,"n We evaluated the RBM-based language model on different tasks. We will ﬁrst give a brief description of our SMT system. Then we will describe in detail our experiments on the German to English translation task. Afterwards, we will describe some more experiments on the English to French translation task. 5.1. System description The translation system was trained on the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in [15] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the 1 http://www.ted.com 167 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Moses package [16]. A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit [17]. In addition we used a bilingual language model as described in [18]. Reordering was performed as a preprocessing step using POS information generated by the TreeTagger [19]. We used the reordering approach described in [20] an"
2012.iwslt-papers.3,P07-2045,0,0.00309671,"the English to French translation task. 5.1. System description The translation system was trained on the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in [15] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the 1 http://www.ted.com 167 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Moses package [16]. A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit [17]. In addition we used a bilingual language model as described in [18]. Reordering was performed as a preprocessing step using POS information generated by the TreeTagger [19]. We used the reordering approach described in [20] and the extensions presented in [21] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT[22]. We"
2012.iwslt-papers.3,W11-2124,1,0.645436,"nd TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in [15] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the 1 http://www.ted.com 167 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Moses package [16]. A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit [17]. In addition we used a bilingual language model as described in [18]. Reordering was performed as a preprocessing step using POS information generated by the TreeTagger [19]. We used the reordering approach described in [20] and the extensions presented in [21] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT[22]. We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K w"
2012.iwslt-papers.3,D10-1076,0,0.0536508,"a short list. Recently, [6] presented a structured output layer neural network which is able to handle large output vocabularies by using automatic word classes to group the output vocabulary. 164 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 A different approach also using Restricted Boltzmann Machines was presented in [7]. In contrast to our work, no approximation was performed and therefore, the calculation was more computation intensive. This approach and the beforementioned ones based on feed-forward networks were compared by Le et al. in [8]. Motivated by the improvements in speech recognition accuracy as well as in translation quality, authors tried to use the neural networks also for the translation model in a statistical machine translation system. In [9] as well as in [10] the authors modiﬁed the n-gram-based translation approach to use the neural networks to model the translation probabilities. Restricted Boltzmann machines have already been successfully used for different tasks like user rating of movies [11] and images [12]. 3. Restricted Boltzmann Machines In this section we will give a brief overview on Restricted Boltzm"
2012.iwslt-papers.3,D07-1045,0,0.0957896,"rkshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 A different approach also using Restricted Boltzmann Machines was presented in [7]. In contrast to our work, no approximation was performed and therefore, the calculation was more computation intensive. This approach and the beforementioned ones based on feed-forward networks were compared by Le et al. in [8]. Motivated by the improvements in speech recognition accuracy as well as in translation quality, authors tried to use the neural networks also for the translation model in a statistical machine translation system. In [9] as well as in [10] the authors modiﬁed the n-gram-based translation approach to use the neural networks to model the translation probabilities. Restricted Boltzmann machines have already been successfully used for different tasks like user rating of movies [11] and images [12]. 3. Restricted Boltzmann Machines In this section we will give a brief overview on Restricted Boltzmann Machines (RBM). We will concentrate only on the points that are important for our RBM-based language model, which will be described in detail in the next section. RBMs are a generative model that have already been use"
2012.iwslt-papers.3,N12-1005,0,0.30541,"anguage Translation Hong Kong, December 6th-7th, 2012 A different approach also using Restricted Boltzmann Machines was presented in [7]. In contrast to our work, no approximation was performed and therefore, the calculation was more computation intensive. This approach and the beforementioned ones based on feed-forward networks were compared by Le et al. in [8]. Motivated by the improvements in speech recognition accuracy as well as in translation quality, authors tried to use the neural networks also for the translation model in a statistical machine translation system. In [9] as well as in [10] the authors modiﬁed the n-gram-based translation approach to use the neural networks to model the translation probabilities. Restricted Boltzmann machines have already been successfully used for different tasks like user rating of movies [11] and images [12]. 3. Restricted Boltzmann Machines In this section we will give a brief overview on Restricted Boltzmann Machines (RBM). We will concentrate only on the points that are important for our RBM-based language model, which will be described in detail in the next section. RBMs are a generative model that have already been used successfully in m"
2012.iwslt-papers.3,2007.tmi-papers.21,0,0.559076,"in [15] was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the 1 http://www.ted.com 167 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Moses package [16]. A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit [17]. In addition we used a bilingual language model as described in [18]. Reordering was performed as a preprocessing step using POS information generated by the TreeTagger [19]. We used the reordering approach described in [20] and the extensions presented in [21] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT[22]. We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used 3.5K segments containing 31K words. 5.2. German to English The results for translating German TED lectures into English are shown"
2012.iwslt-papers.3,W09-0435,1,0.782064,"lignments between source and target words. The phrase table was built using the scripts from the 1 http://www.ted.com 167 The 9th International Workshop on Spoken Language Translation Hong Kong, December 6th-7th, 2012 Moses package [16]. A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit [17]. In addition we used a bilingual language model as described in [18]. Reordering was performed as a preprocessing step using POS information generated by the TreeTagger [19]. We used the reordering approach described in [20] and the extensions presented in [21] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT[22]. We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used 3.5K segments containing 31K words. 5.2. German to English The results for translating German TED lectures into English are shown in Table 1. The baseline system uses"
2012.iwslt-papers.3,W05-0836,1,0.90952,"kage [16]. A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit [17]. In addition we used a bilingual language model as described in [18]. Reordering was performed as a preprocessing step using POS information generated by the TreeTagger [19]. We used the reordering approach described in [20] and the extensions presented in [21] to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT[22]. We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used 3.5K segments containing 31K words. 5.2. German to English The results for translating German TED lectures into English are shown in Table 1. The baseline system uses a 4-gram language model trained on the target side of all parallel data. If we add a 4-gram RBM-based language model trained only on the TED data for 1 iteration using 32 hidden units we can improve the translation quality on t"
2012.iwslt-papers.3,E99-1010,0,0.422024,"erman TED lectures into English are shown in Table 1. The baseline system uses a 4-gram language model trained on the target side of all parallel data. If we add a 4-gram RBM-based language model trained only on the TED data for 1 iteration using 32 hidden units we can improve the translation quality on the test data by 0.8 BLEU points (RBMLM H32 1Iter). We can gain additional 0.6 BLEU points by carrying out 10 instead of only 1 iteration of contrastive divergence. If we use a factored language model trained on the surface word forms and the automatic clusters generated by the MKCLS algorithm [23] (FRBMLM H32 1Iter), we can get an improvement of 1.1 BLEU points already after the ﬁrst iteration. We grouped the words into 50 word classes by the MKCLS algorithm. If we add an n-gram-based language model trained only on the in-domain data (Baseline+NGRAM), we can improve by 1 BLEU point over the baseline system. So the factored RBM-based language model as well as the one trained for 10 iteration can outperform the second n-gram-based language model. We can get further improvements by combining the ngram-based in-domain language model and the RBM-based language model. In this case we use 3 d"
2013.iwslt-evaluation.13,stuker-etal-2012-kit,1,0.826204,"ues used to build our acoustic models is given in section 5. We describe the language model used for this evaluation in section 6 and our decoding strategy and results are presented in sections 7 and 8. 1. Introduction 2. Data Resources [1] The International Workshop on Spoken Language Translation (IWSLT) offers a comprehensive evaluation campaign on spoken language translation. One part of the campaign focuses on the translation of TED Talks (http://www.ted.com/talks), short 5-25min presentations by people from various fields related in some way to Technology, Entertainment, and Design (TED) [2]. In order to evaluate different aspects of this task IWSLT organizes several evaluation tracks on this data covering the aspects of automatic speech recognition (ASR), machine translation (MT), and the full-fledged combination of the two of them into speech translation systems. The goal of the TED ASR track is the automatic transcription of TED lectures on a given segmentation, in order to interface with the machine translation components in the speech-translation track. The quality of the resulting transcriptions are measured in word error rate (WER). In this paper we describe our English AS"
2013.iwslt-evaluation.16,2012.eamt-1.60,1,0.8976,"neous speech and heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been v"
2013.iwslt-evaluation.16,2005.mtsummit-papers.11,1,0.078384,"nd heterogeneous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful"
2013.iwslt-evaluation.16,eisele-chen-2010-multiun,0,0.0452925,"ous topics and styles. The 1 http://www.eu-bridge.eu task is open domain, with a wide range of heavily dissimilar subjects and jargons across talks. IWSLT subdivides the task and separately evaluates automatic transcription of talks from audio to text, speech translation of talks from audio, and text translation of talks as three different tracks [3, 4]. The training data is constrained to the corpora specified by the organizers. The supplied list of corpora comprises a large amount of publicly available monolingual and parallel training data, though, including WIT3 [5], Europarl [6], MultiUN [7], the English and French Gigaword corpora as provided by the Linguistic Data Consortium [8], and the News Crawl, 109 and News Commentary corpora from the WMT shared task training data [9]. For the two “official” language pairs [1] for translation at IWSLT 2013, English→French and German→English, these resources allow for building of systems with state-of-the-art performance by participants. The EU-BRIDGE project is funded by the European Union under the Seventh Framework Programme (FP7) [10] and brings together several project partners who have each previously been very successful in contribut"
2013.iwslt-evaluation.16,E06-1005,1,0.921596,"e-scale evaluation campaigns like IWSLT and WMT in recent years, thereby demonstrating their ability to continuously enhance their systems and promoting progress in machine translation. Machine translation research within EU-BRIDGE has a strong focus on translation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. The work described here is an attempt to attain translation quality beyond strong single system performance via system combination [11]. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in other projects, e.g. in the Quaero programme [12, 13]. Within EU-BRIDGE, we built combined system setups for text translation of talks from English to French as well as from German to English. We found that the combined translation engines of RWTH, UEDIN, KIT, and FBK systems are very effective. In the rest of the paper we will give some insight into the technology behind the combined engines which have been used to produce the joint EU-BRIDGE submission to the IWSLT 2013 MT track"
2013.iwslt-evaluation.16,P02-1040,0,0.0892795,"-BRIDGE submission to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SR"
2013.iwslt-evaluation.16,2006.amta-papers.25,0,0.15922,"sion to the IWSLT 2013 MT track. The remainder of the paper is structured as follows: We first describe the individual English→French and German→English systems by RWTH Aachen University (Section 2), the University of Edinburgh (Section 3), Karlsruhe Institute of Technology (Section 4), and Fondazione Bruno Kessler (Section 5), respectively. We then present the techniques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [2"
2013.iwslt-evaluation.16,W10-1738,1,0.880309,"iques for machine translation system combination which have been employed to obtain consensus translations from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment w"
2013.iwslt-evaluation.16,popovic-ney-2006-pos,1,0.929216,"ll available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram"
2013.iwslt-evaluation.16,P03-1021,0,0.129032,"ns from the outputs of the individual systems of the project partners (Section 6). Experimental results in B LEU [14] and T ER [15] are given in Section 7. A brief error analysis on selected examples from the test data has been conducted which we discuss in Section 8. We finally conclude the paper with Section 9. 2. RWTH Aachen University RWTH applied both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane [16, 17, 18, 19]. The model weights of all systems were tuned with standard Minimum Error Rate Training [20] on the provided dev2010 set. RWTH used B LEU as optimization objective. Language models were created with the SRILM toolkit [21]. All RWTH systems include the standard set of models provided by Jane. For English→French, the final setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all avai"
2013.iwslt-evaluation.16,P12-1031,0,0.10415,"For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective, for which we define B LEU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discri"
2013.iwslt-evaluation.16,P10-2041,0,0.0805135,"setups for RWTH scss and RWTH hiero differ in the amount of training data and in the choice of models. For the English→French hierarchical setup the bilingual data was limited to the in-domain WIT3 data, News Commentary, Europarl, and Common Crawl corpora. The word alignment was created with fast align [22]. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 41 of the French Gigaword Second Edition corpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a se"
2013.iwslt-evaluation.16,D08-1089,0,0.117877,"orpus. The monolingual data selection for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of"
2013.iwslt-evaluation.16,P07-2045,1,0.0125349,"EU on the sentence level with smoothed 3-gram and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a"
2013.iwslt-evaluation.16,W13-2212,1,0.868834,"m and 4gram precisions. RWTH performed discriminative training on the WIT3 portion of the training data. The German→English phrase-based system was furthermore improved by a lexicalized reordering model and 7-gram word class language model. RWTH finally applied domain adaptation by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation"
2013.iwslt-evaluation.16,W11-2123,0,0.0545914,"tion by adding a second translation model to the decoder which was trained on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequen"
2013.iwslt-evaluation.16,P11-1105,1,0.916011,"d on the WIT3 portion of the data only. This second translation model was likewise improved with discriminative phrase training. 3. University of Edinburgh UEDIN’s systems were trained using the Moses system [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev"
2013.iwslt-evaluation.16,D09-1022,1,0.892821,"n for using only parts of the corpora is based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resu"
2013.iwslt-evaluation.16,2012.iwslt-papers.17,1,0.860668,"em [32], replicating the settings described in [33] developed for the 2013 Workshop on Statistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate resu"
2013.iwslt-evaluation.16,D13-1138,1,0.815085,"based on cross-entropy difference as described in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running w"
2013.iwslt-evaluation.16,N04-1022,0,0.487773,"atistical Machine Translation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence m"
2013.iwslt-evaluation.16,W12-2702,0,0.051709,"cribed in [23]. The hierarchical system was extended with a second translation model. The additional translation model was trained on the WIT3 portion of the training data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RW"
2013.iwslt-evaluation.16,P07-1019,0,0.222647,"ranslation. The characteristics of the system include: a maximum sentence length of 80, grow-diag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5-gram language model with KenLM [34] used at runtime, a lexically-driven 5-gram operation sequence model [35] with four additional supportive features (two gap-based penalties, one distance-based feature and one deletion penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown wo"
2013.iwslt-evaluation.16,E03-1076,1,0.900834,"data only. For the English→French phrase-based setup, RWTH utilized all available parallel data and trained a word alignment with GIZA++ [24]. The same language model as in the hierarchical setup was used. RWTH applied the following supplementary features for the phrase-based system: a lexicalized reordering model [25], a discriminative word lexicon [26], a 7-gram word class language model [27], a continuous space language model [28], and a second translation model from the WIT3 portion of the training data only. For German→English, RWTH decompounded the German source in a preprocessing step [29] and applied partof-speech-based long-range verb reordering rules [30]. Both systems RWTH scss and RWTH hiero rest upon all available bilingual data and word alignment obtained with GIZA++. A language model was trained on the target side of all available bilingual data plus 12 of the Shuffled News corpus and 1 4 of the English Gigaword v3 corpus, resulting in a total of 1.7 billion running words. In both German→English systems, RWTH applied a more sophisticated discriminative phrase training method. Similar to [31], a gradient-based method is used to optimize a maximum expected B LEU objective"
2013.iwslt-evaluation.16,N12-1047,0,0.148125,"penalty), msdbidirectional-fe lexicalized reordering, sparse lexical and domain features [36], a distortion limit of 6, 100-best translation options, minimum Bayes risk decoding [37], cube pruning [38] with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-over-punctuation heuristic. UEDIN used the compact phrase table representation by [39]. For English→German, UEDIN used a sequence model over morphological tags. The UEDIN systems were tuned on the dev2010 set made available for the IWSLT 2013 workshop. Tuning was performed using the k-best batch MIRA algorithm [40] with a maximum number of iterations of 25. B LEU was used as the metric to evaluate results. While UEDIN’s main submission also includes sequence models and operation sequence models over Brown word clusters, these setups were not finished in time for the contribution to the EU-BRIDGE system combination. language models trained on WIT3 , Europarl, News Commentary, 109 , and Common Crawl by minimizing the perplexity on the development data. For the class-based language model, KIT utilized in-domain WIT3 data with 4grams and 50 clusters. In addition, a 9-gram POS-based language model derived fr"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.9,1,0.925869,"ge model derived from LIA POS tags [55] on all monolingual data was applied. KIT optimized the log-linear combination of all these models on the provided development data using Minimum Error Rate Training [20]. 4. Karlsruhe Institute of Technology The KIT translations have been generated by an in-house phrase-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, K"
2013.iwslt-evaluation.16,2007.tmi-papers.21,0,0.422618,"e-based translations system [41]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED"
2013.iwslt-evaluation.16,W09-0435,1,0.918776,"Commentary, WIT3 , Common Crawl corpora for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection"
2013.iwslt-evaluation.16,W13-0805,1,0.889592,"ra for both directions and WMT 109 for English→French and the additional monolingual training data. The big noisy 109 and Crawl corpora were filtered using an SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase tabl"
2013.iwslt-evaluation.16,W08-1006,0,0.169177,"SVM classifier [42]. In addition to the standard preprocessing, KIT used compound splitting [29] for the German text. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a"
2013.iwslt-evaluation.16,2005.iwslt-1.8,1,0.888473,"t. In both translation directions, KIT performed reordering using two models. KIT encoded different reorderings of the source sentences in a word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context feat"
2013.iwslt-evaluation.16,W08-0303,1,0.796238,"word lattice. For the English→French system, only short-range rules [43] were used to generate these lattices. For German→English, long-range rules [44] and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to bet"
2013.iwslt-evaluation.16,2012.amta-papers.19,1,0.890564,"and tree-based reordering rules [45] were used as well. The part-of-speech (POS) tags needed for these rules were generated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-doma"
2013.iwslt-evaluation.16,W11-2124,1,0.885306,"ated by the TreeTagger [46] and the parse trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-"
2013.iwslt-evaluation.16,W13-2264,1,0.887808,"se trees by the Stanford Parser [47]. In addition, KIT scored the different reorderings of both language pairs using a lexicalized reordering model [48]. The phrase tables of the systems were trained using GIZA++ alignment for the English→French task and using a discriminative alignment [49] for the German→English task. KIT adapted the phrase table to the TED domain using the back off approach and by also adapting the candidate selection [50]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [51] and a discriminative word lexicon [52]. For the German→English task, a discriminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, K"
2013.iwslt-evaluation.16,2012.iwslt-papers.3,1,0.886049,"criminative word lexicon with source and target context features was applied, while only the source context features were employed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a"
2013.iwslt-evaluation.16,E99-1010,0,0.0594124,"ed for the English→French task. During decoding, KIT used several language models to adapt the system to the task and to better model the sentence structure by means of class-based n-grams. For the German→English task, KIT used one language model trained on all data, an in-domain language model trained only on the WIT3 corpus and one language model trained on 5 M sentences selected using cross-entropy difference [23]. Furthermore, KIT used an RBM-based language model [53] trained on the WIT3 corpus. Finally, KIT also used a classbased language model, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two F"
2013.iwslt-evaluation.16,2011.iwslt-evaluation.18,1,0.928081,"el, trained on the WIT3 corpus using the MKCLS [54] algorithm to cluster the words. For the English→French translation task, KIT linearly combined the 5. Fondazione Bruno Kessler The FBK component of the system combination corresponds to the “contrastive 1” system of the official FBK submission. The FBK system was built upon a standard phrasebased system using the Moses toolkit [32], and exploited the huge amount of parallel English→French and monolingual French training data, provided by the organizers. It featured a statistical log-linear model including a filled-up phrase translation model [56] and lexicalized reordering models (RMs), two French language models (LMs), as well as distortion, word, and phrase penalties. In order to focus it on TED specific domain and genre, and to reduce the size of the system, data selection by means of IRSTLM toolkit [57] was performed on the whole parallel English→French corpus, using the WIT3 training data as in-domain data. Different amount of data are selected from each available corpora but the WIT3 data, for a total of 66 M English running words. Two TMs and two RMs were trained on WIT3 and selected data, separately, and combined using the fil"
2013.iwslt-evaluation.16,W05-0909,0,0.0593782,"es which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. [60]. This approach includes an enhanced alignment and reordering framework. Alignments between the system outputs are learned using METEOR [61]. A confusion network is then built using one of the hypotheses as “primary” hypothesis. We do not make a hard decision on which of the hypotheses to use for that, but instead combine all possible confusion networks into a single lattice. Majority voting on the generated lattice is performed using the prior probabilities for each system as well as other statistical models, e.g. a special n-gram language model which is learned on the input hypotheses. Scaling factors of the models are optimized using the Minimum Error Rate Training algorithm. The translation with the best total score within the"
2013.iwslt-evaluation.16,W12-3140,1,\N,Missing
2013.iwslt-evaluation.16,J03-1002,1,\N,Missing
2013.iwslt-evaluation.16,C12-3061,1,\N,Missing
2013.iwslt-evaluation.16,federico-etal-2012-iwslt,1,\N,Missing
2013.iwslt-evaluation.16,2011.iwslt-evaluation.1,1,\N,Missing
2013.iwslt-evaluation.16,W13-2223,1,\N,Missing
2013.iwslt-evaluation.16,N13-1073,0,\N,Missing
2013.iwslt-evaluation.24,2012.eamt-1.60,0,0.0112029,"r text and speech translation for all the official language pairs: English→German, German→English and English→French as well as two optional directions. The TED tasks consist of automatic translation of both the manual transcripts (MT task) and transcripts generated by automatic speech recognizers (SLT task) for talks held at the TED conferences1 . For German→English, the test data was collected from the TEDx project2 . The TED talks are given in English in a large number of different domains. Some of these talks are manually transcribed and translated by global volunteers into many languages [2]. The TED translation tasks this year bring up interesting challenges: (1) the problem of adapting general models - mainly trained on news data - towards the diverse topics in TED talks, (2) the need of universal techniques for translating texts from and to various languages, and (3) the appropriate solution for inserting punctuation marks and case information on automatic speech recognition (ASR) outputs for the spoken language translation (SLT) task. To deal with those challenges, we provided several advanced adaptation methods both for translation and language models to leverage both the wi"
2013.iwslt-evaluation.24,E03-1076,0,0.0541217,", plus Giga for English→French. The monolingual data we used include the monolingual part of those parallel data, the News Shuffle corpus for all three directions and additionally the Gigaword corpus for English→French and German→English. A common preprocessing is applied to the raw data before performing any model training. This includes removing long sentences and sentences with length difference exceeding a certain threshold. In addition, special symbols, dates and numbers are normalized. The first letter of every sentence is smart-cased. In German→English, we also apply compound splitting [3] to the source side of the corpus. Furthermore, an SVM classifier is used to filter out the noisy sentence pairs in the Giga English→French corpus and the Common Crawl as described in [4]. Unless stated otherwise, the language models used are 4gram language models with modified Kneser-Ney smoothing, trained with the SRILM toolkit [5] and scored in the decoding process with KenLM [6]. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit [7] for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use"
2013.iwslt-evaluation.24,2011.iwslt-evaluation.9,1,0.848162,"aword corpus for English→French and German→English. A common preprocessing is applied to the raw data before performing any model training. This includes removing long sentences and sentences with length difference exceeding a certain threshold. In addition, special symbols, dates and numbers are normalized. The first letter of every sentence is smart-cased. In German→English, we also apply compound splitting [3] to the source side of the corpus. Furthermore, an SVM classifier is used to filter out the noisy sentence pairs in the Giga English→French corpus and the Common Crawl as described in [4]. Unless stated otherwise, the language models used are 4gram language models with modified Kneser-Ney smoothing, trained with the SRILM toolkit [5] and scored in the decoding process with KenLM [6]. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit [7] for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use a discriminative word alignment (DWA) approach [8]. The phrases are extracted using the Moses toolkit [9] and then scored by our in-house parallel phrase scorer [10]. Phrase pair probabi"
2013.iwslt-evaluation.24,W11-2123,0,0.0188113,"ength difference exceeding a certain threshold. In addition, special symbols, dates and numbers are normalized. The first letter of every sentence is smart-cased. In German→English, we also apply compound splitting [3] to the source side of the corpus. Furthermore, an SVM classifier is used to filter out the noisy sentence pairs in the Giga English→French corpus and the Common Crawl as described in [4]. Unless stated otherwise, the language models used are 4gram language models with modified Kneser-Ney smoothing, trained with the SRILM toolkit [5] and scored in the decoding process with KenLM [6]. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit [7] for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use a discriminative word alignment (DWA) approach [8]. The phrases are extracted using the Moses toolkit [9] and then scored by our in-house parallel phrase scorer [10]. Phrase pair probabilities are computed using modified Kneser-Ney smoothing as in [11]. In all directions, beside the word-based language models, some of the non-word language models are used. In order to increase the"
2013.iwslt-evaluation.24,W08-0303,1,0.822978,"an SVM classifier is used to filter out the noisy sentence pairs in the Giga English→French corpus and the Common Crawl as described in [4]. Unless stated otherwise, the language models used are 4gram language models with modified Kneser-Ney smoothing, trained with the SRILM toolkit [5] and scored in the decoding process with KenLM [6]. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit [7] for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use a discriminative word alignment (DWA) approach [8]. The phrases are extracted using the Moses toolkit [9] and then scored by our in-house parallel phrase scorer [10]. Phrase pair probabilities are computed using modified Kneser-Ney smoothing as in [11]. In all directions, beside the word-based language models, some of the non-word language models are used. In order to increase the bilingual context used during the translation process, we use a bilingual language model as described in [12]. To model the dependencies between source and target words even beyond borders of phrase pairs, we create a bilingual token out of every target word and all"
2013.iwslt-evaluation.24,P07-2045,0,0.00484553,"nce pairs in the Giga English→French corpus and the Common Crawl as described in [4]. Unless stated otherwise, the language models used are 4gram language models with modified Kneser-Ney smoothing, trained with the SRILM toolkit [5] and scored in the decoding process with KenLM [6]. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit [7] for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use a discriminative word alignment (DWA) approach [8]. The phrases are extracted using the Moses toolkit [9] and then scored by our in-house parallel phrase scorer [10]. Phrase pair probabilities are computed using modified Kneser-Ney smoothing as in [11]. In all directions, beside the word-based language models, some of the non-word language models are used. In order to increase the bilingual context used during the translation process, we use a bilingual language model as described in [12]. To model the dependencies between source and target words even beyond borders of phrase pairs, we create a bilingual token out of every target word and all its aligned source words. The tokens are ordered like"
2013.iwslt-evaluation.24,W08-1006,0,0.0269265,"lly learned rules based on POS sequences and syntactic parse tree constituents and performs source sentence reordering according to target language word order. The rules are learned from a parallel corpus with POS tags [16] for the source side and a word alignment to learn continuous reordering rules that cover short-range reorderings [17]. Discontinuous rules consist of POS sequences with placeholders and allow long-range reorderings [18]. In addition, we apply a tree-based reordering model [19] to better address the differences in word order between German and English. Syntactic parse trees [20, 21] for the source side of the training corpus and a word alignment are required to learn rules on how to reorder the constituents in the source sentence to simulate target sentence word order. The POSbased and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a word lattice. In order to apply the lexicalized reordering model, the lattice includes the original position of each word. Then the lattice is used as input to the decoder. During decoding the lexicalized reordering m"
2013.iwslt-evaluation.24,P03-1054,0,0.00975892,"lly learned rules based on POS sequences and syntactic parse tree constituents and performs source sentence reordering according to target language word order. The rules are learned from a parallel corpus with POS tags [16] for the source side and a word alignment to learn continuous reordering rules that cover short-range reorderings [17]. Discontinuous rules consist of POS sequences with placeholders and allow long-range reorderings [18]. In addition, we apply a tree-based reordering model [19] to better address the differences in word order between German and English. Syntactic parse trees [20, 21] for the source side of the training corpus and a word alignment are required to learn rules on how to reorder the constituents in the source sentence to simulate target sentence word order. The POSbased and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a word lattice. In order to apply the lexicalized reordering model, the lattice includes the original position of each word. Then the lattice is used as input to the decoder. During decoding the lexicalized reordering m"
2013.iwslt-evaluation.24,W06-1607,0,0.0321156,"language models with modified Kneser-Ney smoothing, trained with the SRILM toolkit [5] and scored in the decoding process with KenLM [6]. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit [7] for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use a discriminative word alignment (DWA) approach [8]. The phrases are extracted using the Moses toolkit [9] and then scored by our in-house parallel phrase scorer [10]. Phrase pair probabilities are computed using modified Kneser-Ney smoothing as in [11]. In all directions, beside the word-based language models, some of the non-word language models are used. In order to increase the bilingual context used during the translation process, we use a bilingual language model as described in [12]. To model the dependencies between source and target words even beyond borders of phrase pairs, we create a bilingual token out of every target word and all its aligned source words. The tokens are ordered like the target words. In addition, to alleviate the sparsity problem for surface words, we use a cluster language model based on word classes. This is"
2013.iwslt-evaluation.24,2012.amta-papers.19,1,0.820782,"provides the reordering probability for each phrase pair. At the phrase boundaries, the reordering orientation with respect to the original position of the words is checked. The probability for the respective orientation is included as an additional score in the log-linear model of the translation system. 5. Adaptation In order to achieve the best performance on the target domain, we perform adaptation for translation models as well as language models. We adapt the translation model (TM) by using the scores from the in-domain and out-of-domain phrase table as described in the backoff approach [22]. This results in a phrase table with six scores, the four scores from the general phrase table as well as the two conditional probabilities from the in-domain phrase table. In addition, we adapt the candidate selection in some of our systems by taking the union of the candidates translations from both phrase tables (CSUnion). The language model (LM) is adapted by log-linearly combining the general language model and an in-domain language model trained only on the TED data. In addition, in some of the systems we combine these language models with a third language model. This language model was"
2013.iwslt-evaluation.24,W11-2124,1,0.900081,"s. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For German→English, we use a discriminative word alignment (DWA) approach [8]. The phrases are extracted using the Moses toolkit [9] and then scored by our in-house parallel phrase scorer [10]. Phrase pair probabilities are computed using modified Kneser-Ney smoothing as in [11]. In all directions, beside the word-based language models, some of the non-word language models are used. In order to increase the bilingual context used during the translation process, we use a bilingual language model as described in [12]. To model the dependencies between source and target words even beyond borders of phrase pairs, we create a bilingual token out of every target word and all its aligned source words. The tokens are ordered like the target words. In addition, to alleviate the sparsity problem for surface words, we use a cluster language model based on word classes. This is done in the following way: In a first step, we cluster the words of the corpus using the MKCLS algorithm [13]. Then we replace the words in the TED corpus by their cluster IDs and train an n-gram language model on this corpus consisting of w"
2013.iwslt-evaluation.24,P10-2041,0,0.0251885,"he general phrase table as well as the two conditional probabilities from the in-domain phrase table. In addition, we adapt the candidate selection in some of our systems by taking the union of the candidates translations from both phrase tables (CSUnion). The language model (LM) is adapted by log-linearly combining the general language model and an in-domain language model trained only on the TED data. In addition, in some of the systems we combine these language models with a third language model. This language model was trained on data automatically selected using cross-entropy differences [23]. We selected the top 5M sentences to train the language model. 6. Discriminative Word Lexica Mauser et al. [24] have shown that the use of DWL can improve the translation quality. For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. In our system we use the extended version using also source context and target context features [25]. When using source context features, not only the words of the sentence are used as features, but also the n-grams occurring in the sentence. T"
2013.iwslt-evaluation.24,E99-1010,0,0.0109,". In order to increase the bilingual context used during the translation process, we use a bilingual language model as described in [12]. To model the dependencies between source and target words even beyond borders of phrase pairs, we create a bilingual token out of every target word and all its aligned source words. The tokens are ordered like the target words. In addition, to alleviate the sparsity problem for surface words, we use a cluster language model based on word classes. This is done in the following way: In a first step, we cluster the words of the corpus using the MKCLS algorithm [13]. Then we replace the words in the TED corpus by their cluster IDs and train an n-gram language model on this corpus consisting of word classes. 3. Preprocessing for Speech Translation The system translating automatic transcripts needs special preprocessing on the data, since generally there is no or no reliable case information and punctuation in the automatically generated transcripts. We have used a monolingual translation system as shown in [14] to deal with the difference in casing and punctuation between a machine translation (MT) and an SLT system. In contrast to the condition in their"
2013.iwslt-evaluation.24,D09-1022,0,0.0244263,"n, we adapt the candidate selection in some of our systems by taking the union of the candidates translations from both phrase tables (CSUnion). The language model (LM) is adapted by log-linearly combining the general language model and an in-domain language model trained only on the TED data. In addition, in some of the systems we combine these language models with a third language model. This language model was trained on data automatically selected using cross-entropy differences [23]. We selected the top 5M sentences to train the language model. 6. Discriminative Word Lexica Mauser et al. [24] have shown that the use of DWL can improve the translation quality. For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. In our system we use the extended version using also source context and target context features [25]. When using source context features, not only the words of the sentence are used as features, but also the n-grams occurring in the sentence. The target context features encode information about the surrounding target words. One specialty of the TED trans"
2013.iwslt-evaluation.24,2012.iwslt-papers.15,1,0.824592,"uage model based on word classes. This is done in the following way: In a first step, we cluster the words of the corpus using the MKCLS algorithm [13]. Then we replace the words in the TED corpus by their cluster IDs and train an n-gram language model on this corpus consisting of word classes. 3. Preprocessing for Speech Translation The system translating automatic transcripts needs special preprocessing on the data, since generally there is no or no reliable case information and punctuation in the automatically generated transcripts. We have used a monolingual translation system as shown in [14] to deal with the difference in casing and punctuation between a machine translation (MT) and an SLT system. In contrast to the condition in their work, in this evaluation campaign sentence boundaries are present in the test sets. Therefore, we use this monolingual translation system for predicting commas instead of all punctuation marks in the test set. In addition to predicting commas, we also predict casing of words using the monolingual translation system. This preprocessing will be denoted as Monolingual Comma and Case Insertion (MCCI). In order to build the monolingual system which trans"
2013.iwslt-evaluation.24,2005.iwslt-1.8,0,0.0415505,"tokens is used. Word reordering is ignored in these systems. In order to capture more context, we use a 9-gram language model trained on part-ofspeech (POS) tokens. Moreover, a 9-gram cluster language model is trained on 1,000 clusters, based on the MKCLS algorithm as described in the baseline system. For the speech translation tasks, the output of the monolingual translation system becomes the input to our regular translation system which is trained using data with punctuation marks. 4. Word Reordering Model Word reordering is modeled in two ways. The first is a lexicalized reordering model [15] which stores reordering probabilities for each phrase pair. The second model consists of automatically learned rules based on POS sequences and syntactic parse tree constituents and performs source sentence reordering according to target language word order. The rules are learned from a parallel corpus with POS tags [16] for the source side and a word alignment to learn continuous reordering rules that cover short-range reorderings [17]. Discontinuous rules consist of POS sequences with placeholders and allow long-range reorderings [18]. In addition, we apply a tree-based reordering model [19"
2013.iwslt-evaluation.24,2007.tmi-papers.21,0,0.0281074,"ich is trained using data with punctuation marks. 4. Word Reordering Model Word reordering is modeled in two ways. The first is a lexicalized reordering model [15] which stores reordering probabilities for each phrase pair. The second model consists of automatically learned rules based on POS sequences and syntactic parse tree constituents and performs source sentence reordering according to target language word order. The rules are learned from a parallel corpus with POS tags [16] for the source side and a word alignment to learn continuous reordering rules that cover short-range reorderings [17]. Discontinuous rules consist of POS sequences with placeholders and allow long-range reorderings [18]. In addition, we apply a tree-based reordering model [19] to better address the differences in word order between German and English. Syntactic parse trees [20, 21] for the source side of the training corpus and a word alignment are required to learn rules on how to reorder the constituents in the source sentence to simulate target sentence word order. The POSbased and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants"
2013.iwslt-evaluation.24,W13-2264,1,0.768923,"ese language models with a third language model. This language model was trained on data automatically selected using cross-entropy differences [23]. We selected the top 5M sentences to train the language model. 6. Discriminative Word Lexica Mauser et al. [24] have shown that the use of DWL can improve the translation quality. For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. In our system we use the extended version using also source context and target context features [25]. When using source context features, not only the words of the sentence are used as features, but also the n-grams occurring in the sentence. The target context features encode information about the surrounding target words. One specialty of the TED translation task is that we have a lot of parallel data we can train our models on. However, only a quite small portion of these data, the TED corpus, is very important for the translation quality. Therefore, we achieve a better translation performance by training the models only on the TED data. The RBM used for the language model consists of two"
2013.iwslt-evaluation.24,N12-1005,0,0.0177851,"seline system also includes a cluster-based language model using the clusters automatically generated by the MKCLS toolkit. System Baseline + Tree-based Rules + Lexicalized Reordering + POSLM + DWL + Class-based 9-gram LMs + TargetContext + LM DataSelection Dev 23.58 23.61 23.74 23.81 24.44 24.19 24.24 Test 23.50 23.87 23.93 24.14 24.76 24.93 25.06 Table 1: Experiments for English→German (MT) 7. Continuous Space Language Model In recent years, different approaches to integrate continuous space models have shown significant improvements in the translation quality of machine translation systems [26]. Since the long training time is the main disadvantage of this model, we only train it on the small, but very domain-relevant TED corpus. In contrast to most other approaches, we did not use a feed-forward neural network, but used a Restricted Bolzmann Machine (RBM). The main advantage of this approach is that the free energy of the model, which is proportional to the language model probability, can be calculated very efficiently. Therefore, we are able to use the RBM-based language model during decoding and not only in the rescoring phase. By adding tree-based reordering rules and a lexicali"
2013.iwslt-evaluation.24,2012.iwslt-papers.3,1,0.814901,"our models on. However, only a quite small portion of these data, the TED corpus, is very important for the translation quality. Therefore, we achieve a better translation performance by training the models only on the TED data. The RBM used for the language model consists of two layers, which are fully connected. In the input layer, for every word position there are as many nodes as words in the vocabulary. Since we used a 4-gram language model, there are 4 word positions in the input layer. These nodes are connected to 32 hidden units in the hidden layer. The model is described in detail in [27]. 8. Results In this section, we present a summary of our experiments for all tasks we have carried out for the IWSLT 2013 evaluation. All the reported scores are case-sensitive BLEU scores calculated based on the provided development and test sets. 8.1. English→German We conducted several experiments for English→German translation using the available data. They are summarized in Table 1. The baseline system is a phrase-based translation system using POS-based reordering rules. Preprocessing of the source and target language of the training corpora is performed as described above. Adaptation o"
2013.iwslt-evaluation.24,W09-0435,1,0.841052,"in two ways. The first is a lexicalized reordering model [15] which stores reordering probabilities for each phrase pair. The second model consists of automatically learned rules based on POS sequences and syntactic parse tree constituents and performs source sentence reordering according to target language word order. The rules are learned from a parallel corpus with POS tags [16] for the source side and a word alignment to learn continuous reordering rules that cover short-range reorderings [17]. Discontinuous rules consist of POS sequences with placeholders and allow long-range reorderings [18]. In addition, we apply a tree-based reordering model [19] to better address the differences in word order between German and English. Syntactic parse trees [20, 21] for the source side of the training corpus and a word alignment are required to learn rules on how to reorder the constituents in the source sentence to simulate target sentence word order. The POSbased and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a word lattice. In order to apply the lexicalized reor"
2013.iwslt-evaluation.24,W13-0805,1,0.826891,"15] which stores reordering probabilities for each phrase pair. The second model consists of automatically learned rules based on POS sequences and syntactic parse tree constituents and performs source sentence reordering according to target language word order. The rules are learned from a parallel corpus with POS tags [16] for the source side and a word alignment to learn continuous reordering rules that cover short-range reorderings [17]. Discontinuous rules consist of POS sequences with placeholders and allow long-range reorderings [18]. In addition, we apply a tree-based reordering model [19] to better address the differences in word order between German and English. Syntactic parse trees [20, 21] for the source side of the training corpus and a word alignment are required to learn rules on how to reorder the constituents in the source sentence to simulate target sentence word order. The POSbased and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a word lattice. In order to apply the lexicalized reordering model, the lattice includes the original position o"
2013.iwslt-papers.11,2005.iwslt-1.8,0,0.0254136,"ents. We first present related work regarding reordering methods in machine translation and reference work on judging the quality of a given reordering. Then we mention work using oracles for the analysis of machine translation systems. Word reordering has been addressed by many approaches in statistical systems. In a state-of-the-art phrase-based machine translation system, the decoder processes the source sentence left to right, but allows changes in the order of source words while the translation hypothesis is generated. Many phrase-based systems also include a lexicalized reordering model [1] which provides additional reordering information for phrase pairs. It stores statistics on the orientation of adjacent phrase pairs on the lexical level. A very popular approach is to detach the reordering from the decoding procedure and to perform the reordering on the source sentence before translation. Such pre-reordering approaches use linguistic information about the source and or target language, such as parts-of-speech, dependency or constituency tree structure. They apply hand-crafted rules or automatically learn rules that change the order of the source sentence. Then monotone transl"
2013.iwslt-papers.11,C04-1073,0,0.10438,"to detach the reordering from the decoding procedure and to perform the reordering on the source sentence before translation. Such pre-reordering approaches use linguistic information about the source and or target language, such as parts-of-speech, dependency or constituency tree structure. They apply hand-crafted rules or automatically learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], autom"
2013.iwslt-papers.11,P05-1066,0,0.145231,"e and or target language, such as parts-of-speech, dependency or constituency tree structure. They apply hand-crafted rules or automatically learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reor"
2013.iwslt-papers.11,popovic-ney-2006-pos,0,0.0256285,"e and or target language, such as parts-of-speech, dependency or constituency tree structure. They apply hand-crafted rules or automatically learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reor"
2013.iwslt-papers.11,2007.mtsummit-papers.29,0,0.0388472,"e and or target language, such as parts-of-speech, dependency or constituency tree structure. They apply hand-crafted rules or automatically learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reor"
2013.iwslt-papers.11,D07-1077,0,0.027371,"e and or target language, such as parts-of-speech, dependency or constituency tree structure. They apply hand-crafted rules or automatically learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reor"
2013.iwslt-papers.11,2007.tmi-papers.21,0,0.396563,"y learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rul"
2013.iwslt-papers.11,W07-0401,0,0.0810026,"y learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rul"
2013.iwslt-papers.11,W08-0307,0,0.0177742,"y learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rul"
2013.iwslt-papers.11,W09-0435,1,0.912576,"y learn rules that change the order of the source sentence. Then monotone translation is performed. In the first pre-reordering approach, reordering rules for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rul"
2013.iwslt-papers.11,W06-1609,0,0.244986,"for English-French translation are automatically learned from source and target language dependency trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rules [7, 10]. This produces complementary reordering variants which result in an improved translation quality. While some of the presented approache"
2013.iwslt-papers.11,2009.eamt-1.27,0,0.0201326,"y trees [2]. Since then many adopted this method. In the beginning manually crafted reordering rules based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rules [7, 10]. This produces complementary reordering variants which result in an improved translation quality. While some of the presented approaches perform a deterministic reordering of the source sentence, others store reordering variants in a"
2013.iwslt-papers.11,C10-1043,0,0.0158662,"s based on syntactic or dependency parse trees or part-of-speech tags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rules [7, 10]. This produces complementary reordering variants which result in an improved translation quality. While some of the presented approaches perform a deterministic reordering of the source sentence, others store reordering variants in a word lattice leaving the selection of the reordering path to the decoder. Related work regarding r"
2013.iwslt-papers.11,D13-1049,0,0.13395,"ags were designed for particular languages [3, 4, 5, 6]. Later data-driven methods followed, learning reordering rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rules [7, 10]. This produces complementary reordering variants which result in an improved translation quality. While some of the presented approaches perform a deterministic reordering of the source sentence, others store reordering variants in a word lattice leaving the selection of the reordering path to the decoder. Related work regarding reordering metrics and reordering quality includes the first descri"
2013.iwslt-papers.11,W13-0805,1,0.629678,"g rules automatically based on part-of-speech tags or syntactic chunks [7, 8, 9, 10]. Alternatively, word class information may be used to perform a translation of the original source sentence into a reordered source sentence [11]. More recent work includes reordering rules learned from source and target side syntax trees [12], automatically learned reordering rules from IBM1 alignments and source side dependency trees [13] and using a classifier to predict source-sentence reordering [14]. An approach presenting automatically learned reordering rules based on syntactic parse tree constituents [15] further combines the tree-based rules with two types of part-of-speechbased rules [7, 10]. This produces complementary reordering variants which result in an improved translation quality. While some of the presented approaches perform a deterministic reordering of the source sentence, others store reordering variants in a word lattice leaving the selection of the reordering path to the decoder. Related work regarding reordering metrics and reordering quality includes the first description of reorderings as permutations [16]. Later, the use of permutation distance metrics to measure reordering"
2013.iwslt-papers.11,D10-1091,0,0.0244358,"arding reordering metrics and reordering quality includes the first description of reorderings as permutations [16]. Later, the use of permutation distance metrics to measure reordering quality [17] leveraged research into distance functions for ordered encodings. An approach to transform alignments into permutations [18] takes the particular characteristics of alignment functions into account. Oracle experiments have shown to be a valuable method for analyzing different aspects of machine translation. While an oracle BLEU score may serve for identifying translation errors in the phrase table [19], another approach uses oracles for punctuation and segmentation prediction in speech translation [20]. Efficient methods for finding the best translation hypothesis in a decoding lattice have been proposed [21]. Furthermore, research on oracles regarding the reordering problem have been conducted [22, 23]. The first uses linear programming to compare the best achievable BLEU scores when using different reordering constraints [22]. The latter presents a reordering method for translations from English to Spanish, Dutch and Chinese where deterministic reordering decisions are conditioned on sour"
2013.iwslt-papers.11,2012.iwslt-papers.15,1,0.840141,"utations [16]. Later, the use of permutation distance metrics to measure reordering quality [17] leveraged research into distance functions for ordered encodings. An approach to transform alignments into permutations [18] takes the particular characteristics of alignment functions into account. Oracle experiments have shown to be a valuable method for analyzing different aspects of machine translation. While an oracle BLEU score may serve for identifying translation errors in the phrase table [19], another approach uses oracles for punctuation and segmentation prediction in speech translation [20]. Efficient methods for finding the best translation hypothesis in a decoding lattice have been proposed [21]. Furthermore, research on oracles regarding the reordering problem have been conducted [22, 23]. The first uses linear programming to compare the best achievable BLEU scores when using different reordering constraints [22]. The latter presents a reordering method for translations from English to Spanish, Dutch and Chinese where deterministic reordering decisions are conditioned on source tree features and compared to several oracles [23]. Rule Type Short Long Tree Example Rule VVIMP VM"
2013.iwslt-papers.11,E12-1013,0,0.0190364,"search into distance functions for ordered encodings. An approach to transform alignments into permutations [18] takes the particular characteristics of alignment functions into account. Oracle experiments have shown to be a valuable method for analyzing different aspects of machine translation. While an oracle BLEU score may serve for identifying translation errors in the phrase table [19], another approach uses oracles for punctuation and segmentation prediction in speech translation [20]. Efficient methods for finding the best translation hypothesis in a decoding lattice have been proposed [21]. Furthermore, research on oracles regarding the reordering problem have been conducted [22, 23]. The first uses linear programming to compare the best achievable BLEU scores when using different reordering constraints [22]. The latter presents a reordering method for translations from English to Spanish, Dutch and Chinese where deterministic reordering decisions are conditioned on source tree features and compared to several oracles [23]. Rule Type Short Long Tree Example Rule VVIMP VMFIN PPER VAFIN * VVPP VP PTNEG NP VVPP →210 →021 →021 Figure 1: Rule Types Our work differs in three ways: Fi"
2013.iwslt-papers.11,W07-0414,0,0.0153014,"to permutations [18] takes the particular characteristics of alignment functions into account. Oracle experiments have shown to be a valuable method for analyzing different aspects of machine translation. While an oracle BLEU score may serve for identifying translation errors in the phrase table [19], another approach uses oracles for punctuation and segmentation prediction in speech translation [20]. Efficient methods for finding the best translation hypothesis in a decoding lattice have been proposed [21]. Furthermore, research on oracles regarding the reordering problem have been conducted [22, 23]. The first uses linear programming to compare the best achievable BLEU scores when using different reordering constraints [22]. The latter presents a reordering method for translations from English to Spanish, Dutch and Chinese where deterministic reordering decisions are conditioned on source tree features and compared to several oracles [23]. Rule Type Short Long Tree Example Rule VVIMP VMFIN PPER VAFIN * VVPP VP PTNEG NP VVPP →210 →021 →021 Figure 1: Rule Types Our work differs in three ways: First, we investigate a reordering approach where reordering decisions are not deterministic. Inst"
2013.iwslt-papers.11,I11-1005,0,0.014754,"to permutations [18] takes the particular characteristics of alignment functions into account. Oracle experiments have shown to be a valuable method for analyzing different aspects of machine translation. While an oracle BLEU score may serve for identifying translation errors in the phrase table [19], another approach uses oracles for punctuation and segmentation prediction in speech translation [20]. Efficient methods for finding the best translation hypothesis in a decoding lattice have been proposed [21]. Furthermore, research on oracles regarding the reordering problem have been conducted [22, 23]. The first uses linear programming to compare the best achievable BLEU scores when using different reordering constraints [22]. The latter presents a reordering method for translations from English to Spanish, Dutch and Chinese where deterministic reordering decisions are conditioned on source tree features and compared to several oracles [23]. Rule Type Short Long Tree Example Rule VVIMP VMFIN PPER VAFIN * VVPP VP PTNEG NP VVPP →210 →021 →021 Figure 1: Rule Types Our work differs in three ways: First, we investigate a reordering approach where reordering decisions are not deterministic. Inst"
2013.iwslt-papers.12,N13-1090,0,0.00879277,"d Cosine Distance Figure 1 depicts the 2-dimensional word projection from the real-valued 100-dimensional vectors representations using the RNN, we can observe word clusters being formed. This visualization is obtained using t-Distributed Stochastic Neighbor Embedding [16]. Due to memory consumption, only the most frequent 10K words are projected. 4.2. Word Representation using RNN Word representations have gained a great deal of attention for various NLP tasks. Especially word representation using RNNs is proven to be able to capture meaningful syntactic and semantic regularities efficiently [14]. RNNs are similar to multilayer perceptrons, but an RNN has a backwards directed loop, where the output of hidden layers becomes additional input. This allows the network to effectively capture longer history compared to other feed-forward-based n-gram models. Word embedding is a distributed word representation, where words are represented as multi-dimensional vectors. The word vectors syntactically and semantically relating to each other will be close to each other in that representation space. Thus, words within certain semantic and syntactic reFigure 1: Word projection of training data, wi"
2013.iwslt-papers.12,P04-1005,0,0.634965,"ler words. These speech disfluencies inhibit proper processing for other subsequent applications, for example machine translation (MT) systems. MT systems are generally trained using well-structured, cleanly written texts. The mismatch between this training data and the actual test data, in this case spontaneous speech, causes a performance drop. A system which reconstructs the non-fluent output from an automatic speech recognition (ASR) system into the proper form for subsequent applications will increase the performance of the application. A considerable number of works on this task such as [1] and [2] focus on English, from the point of view of the ASR systems. One of our goals is to extend this work to German, and also apply it to the MT task, in order to analyze the effect of speech disfluencies on MT. Another type of speech disfluency, where several speech fragments are dropped and new fragments are introduced, is restart fragments. As presented in Table 2, the speaker starts a new way of forming the sentence after aborting the first several utterances. Although the example shown in this table depicts a case where the context is still kept in the following new utterances, occasi"
2013.iwslt-papers.12,E09-1030,0,0.687529,"s. These speech disfluencies inhibit proper processing for other subsequent applications, for example machine translation (MT) systems. MT systems are generally trained using well-structured, cleanly written texts. The mismatch between this training data and the actual test data, in this case spontaneous speech, causes a performance drop. A system which reconstructs the non-fluent output from an automatic speech recognition (ASR) system into the proper form for subsequent applications will increase the performance of the application. A considerable number of works on this task such as [1] and [2] focus on English, from the point of view of the ASR systems. One of our goals is to extend this work to German, and also apply it to the MT task, in order to analyze the effect of speech disfluencies on MT. Another type of speech disfluency, where several speech fragments are dropped and new fragments are introduced, is restart fragments. As presented in Table 2, the speaker starts a new way of forming the sentence after aborting the first several utterances. Although the example shown in this table depicts a case where the context is still kept in the following new utterances, occasionally w"
2013.iwslt-papers.12,2007.mtsummit-papers.51,0,0.129273,"Missing"
2013.iwslt-papers.12,P09-1084,0,0.348044,"Missing"
2013.iwslt-papers.12,P05-1056,0,0.117662,"data and to enable cross validation, we divided the 61K words of annotated data as well as its translation in English into three parts, such that each part has around 20K words in the German source. For testing one corpus part out of three, the other two parts, which are around 40K words, are used as training data for the CRF model. 4. Disfluency Detection using CRF Introduced by [9], CRF is a framework dedicated to labeling sequence data. A CRF models a hidden label sequence given the observed sequence. CRFs have been applied extensively in diverse tasks of NLP, such as sentence segmentation [10], POS tagging [9] and shallow parsing [11] due to its advantages of representing long-range dependencies in the observations. In this work we use the linear chain CRF modeling technique to detect speech disfluencies. By using bigram features we can model first-order dependencies between words with a disfluency. We used the GRMM package [12] implementation of the CRF model. The CRF model was trained using L-BFGS, with the default parameters of the toolkit. 4.1. Features In this work we utilize lexical, language model, word representation, and phrase table information features. Word representati"
2013.iwslt-papers.12,W11-2124,1,0.946995,"words or phrases in a source sentence independent from their syntactic roles. As shown in Table 7, word representation tends to group those words together which are syntactically and semantically closely related. However, using the phrase table information, words which are only semantically related, but not necessarily syntactically related, can also be grouped together. Considering that many of the repetitions also have different POS tags in a sentence, this phrase table feature is expected to capture such disfluencies. In order to derive this feature, we examine the bilingual language model [17] tokens in the phrase table. The bilingual language model tokens consist of target words and their aligned source words. Using this information, we count how often a given source word is aligned to a certain target word and list the three most frequently used target words. We compare the aligned target words of the current and the following word. If the same target word(s) appears in both lists, the current word is given a phrase table feature. An equivalent feature is introduced for the phrase level. As an example, we can consider consecutive source words f1 , f2 , and f3 in one phrase. This"
2013.iwslt-papers.12,P07-2045,0,0.00417219,"allel TED data1 as in-domain data to adapt our models to the lecture domain. Preprocessing which consists of text normalization, tokenization, and smartcasing is applied before the training. For the German side, compound splitting and conversion of words written according to the old spelling conventions into the new form of spelling are applied additionally. As development data, manual transcripts of lecture data collected internally at our university are used. The talks are 14K parallel sentences from university classes and events. In order to build the phrase table, we use the Moses package [18]. Using the SRILM Toolkit [19], a 4-gram language model is trained on 462 million words from the English side of the data. A bilingual language model [17] is used to extend source word context. In order to address the different word orders between German and English, the POS-based reordering model as described in [20] is applied. This is further extended as described in [21] to cover long-range reorderings. We use Minimum Error Rate Training (MERT) [22] for the optimization in the in-house phrase-based decoder [23]. 5.2. Results To investigate the impact of disfluencies in speech translation q"
2013.iwslt-papers.12,2007.tmi-papers.21,0,0.239679,"orm of spelling are applied additionally. As development data, manual transcripts of lecture data collected internally at our university are used. The talks are 14K parallel sentences from university classes and events. In order to build the phrase table, we use the Moses package [18]. Using the SRILM Toolkit [19], a 4-gram language model is trained on 462 million words from the English side of the data. A bilingual language model [17] is used to extend source word context. In order to address the different word orders between German and English, the POS-based reordering model as described in [20] is applied. This is further extended as described in [21] to cover long-range reorderings. We use Minimum Error Rate Training (MERT) [22] for the optimization in the in-house phrase-based decoder [23]. 5.2. Results To investigate the impact of disfluencies in speech translation quality, we conduct four experiments. In the first experiment, the whole data, including annotated disfluencies, is passed through our statistical machine translation (SMT) system. For the second experiment, we remove the obvious filler words uh and uhm manually in order to study the impact of the filler words which ca"
2013.iwslt-papers.12,W09-0435,0,0.0373295,"ata, manual transcripts of lecture data collected internally at our university are used. The talks are 14K parallel sentences from university classes and events. In order to build the phrase table, we use the Moses package [18]. Using the SRILM Toolkit [19], a 4-gram language model is trained on 462 million words from the English side of the data. A bilingual language model [17] is used to extend source word context. In order to address the different word orders between German and English, the POS-based reordering model as described in [20] is applied. This is further extended as described in [21] to cover long-range reorderings. We use Minimum Error Rate Training (MERT) [22] for the optimization in the in-house phrase-based decoder [23]. 5.2. Results To investigate the impact of disfluencies in speech translation quality, we conduct four experiments. In the first experiment, the whole data, including annotated disfluencies, is passed through our statistical machine translation (SMT) system. For the second experiment, we remove the obvious filler words uh and uhm manually in order to study the impact of the filler words which can be captured systematically. Although there are a great n"
2013.iwslt-papers.12,W05-0836,1,0.748384,"re used. The talks are 14K parallel sentences from university classes and events. In order to build the phrase table, we use the Moses package [18]. Using the SRILM Toolkit [19], a 4-gram language model is trained on 462 million words from the English side of the data. A bilingual language model [17] is used to extend source word context. In order to address the different word orders between German and English, the POS-based reordering model as described in [20] is applied. This is further extended as described in [21] to cover long-range reorderings. We use Minimum Error Rate Training (MERT) [22] for the optimization in the in-house phrase-based decoder [23]. 5.2. Results To investigate the impact of disfluencies in speech translation quality, we conduct four experiments. In the first experiment, the whole data, including annotated disfluencies, is passed through our statistical machine translation (SMT) system. For the second experiment, we remove the obvious filler words uh and uhm manually in order to study the impact of the filler words which can be captured systematically. Although there are a great number of other filler words, many of these filler words are not removed in this"
2013.iwslt-papers.13,P09-2087,0,0.0632242,"Missing"
2013.iwslt-papers.13,N10-1104,0,0.0414509,"Missing"
2013.iwslt-papers.13,N07-1048,0,0.040949,"Missing"
2013.iwslt-papers.13,J96-1002,0,0.0166919,"ronunciation Generation: For the generated subword units pronunciations need to be added to the system’s dictionary. Since in general the mapping between the writing of a word and its pronunciation, i.e. phoneme sequence, is not given or easily derivable, deducting the pronunciation of the sub-word units from the pronunciation of the original words is often not straight-forward or even impossible. Often grapheme based pronunciation dictionaries can offer a solution here. 2.2. Maximum Entropy Language Models The maximum entropy approach was introduced to language modeling more than 10 years ago[13, 14, 15]. And it is being used today the state-of-the-art language models such as ModelM[16]. ModelM[16] is an exponential class-based n-gram language model. The word n-gram and word class features are incorporated into the language model within an exponential modeling framework. The model with enhanced word • Language Model Training: Based on the new vocabulary composed of the sub-word units, and potentially mixed with whole words, a new language model needs to be trained that is then used for recognition. • Word Reconstruction: After decoding, the recognized sub-words need to be recombined in order"
2013.iwslt-papers.13,N09-1053,0,0.0309328,"the system’s dictionary. Since in general the mapping between the writing of a word and its pronunciation, i.e. phoneme sequence, is not given or easily derivable, deducting the pronunciation of the sub-word units from the pronunciation of the original words is often not straight-forward or even impossible. Often grapheme based pronunciation dictionaries can offer a solution here. 2.2. Maximum Entropy Language Models The maximum entropy approach was introduced to language modeling more than 10 years ago[13, 14, 15]. And it is being used today the state-of-the-art language models such as ModelM[16]. ModelM[16] is an exponential class-based n-gram language model. The word n-gram and word class features are incorporated into the language model within an exponential modeling framework. The model with enhanced word • Language Model Training: Based on the new vocabulary composed of the sub-word units, and potentially mixed with whole words, a new language model needs to be trained that is then used for recognition. • Word Reconstruction: After decoding, the recognized sub-words need to be recombined in order to obtain a valid word sequence. 3.1. Word Decomposition and Merging For word decomp"
2013.iwslt-papers.13,2011.iwslt-evaluation.9,1,0.796347,"ring regular decoding is too computationally intensive, again we applied the language model during n-best list re-scoring. For calculating the LM score we used the three previous stems (s −3 , s −2 , s −1 ), three previous endings (e −3 , e −2 , e −1 ) and one successor stem (s1 ) and ending (s1 ) as features. The null ending is explicitly modeled with the ∼# place-holder. For training, the CRF++ Toolkit[26] is utilized. As the training of the labels, endings in our case, within a single model was not possible due to main memory usage (more than 512GB RAM was needed), a similar approach as in [18] and [19] was applied. The idea is to train a separate model for every label. Every model evaluates then only two classes: the ending, which the models stands for versus all other endings. In testing, all models, whose corresponding endings were present in the utterance, were applied. The resulting score is given by the sum of the scores from the single models. Again we re-scored the n-best lists generated by the subword system by interpolating the language model score from the maximum entropy language model with the combined acoustic and LM scores from the sub-word system. As for the interpol"
2013.iwslt-papers.13,D09-1022,0,0.0304483,"lar decoding is too computationally intensive, again we applied the language model during n-best list re-scoring. For calculating the LM score we used the three previous stems (s −3 , s −2 , s −1 ), three previous endings (e −3 , e −2 , e −1 ) and one successor stem (s1 ) and ending (s1 ) as features. The null ending is explicitly modeled with the ∼# place-holder. For training, the CRF++ Toolkit[26] is utilized. As the training of the labels, endings in our case, within a single model was not possible due to main memory usage (more than 512GB RAM was needed), a similar approach as in [18] and [19] was applied. The idea is to train a separate model for every label. Every model evaluates then only two classes: the ending, which the models stands for versus all other endings. In testing, all models, whose corresponding endings were present in the utterance, were applied. The resulting score is given by the sum of the scores from the single models. Again we re-scored the n-best lists generated by the subword system by interpolating the language model score from the maximum entropy language model with the combined acoustic and LM scores from the sub-word system. As for the interpolation des"
2013.iwslt-papers.13,W02-2018,0,0.042073,"exp  λ i f i (x, y)  , (7) pme (λ) = Z (x) i where f i (x, y) are binary feature functions. λ i are weight factors—parameters of the model. Z (x) is the normalization factor in order to ensure that result is indeed a probability distribution. 5.2. Baseline System 4.3. Training A number of algorithms can be used for estimating the parameters of a maximum entropy model. There are both–––special methods, such as Generalized Iterative Scaling[21], Improved Iterative Scaling[22], and general purpose optimization techniques, such as gradient ascent, conjugate gradient and quasi-Newton methods. [23] in its comparison of algorithms for maximum entropy parameter estimation states that the widely used iterative scaling algorithms perform quite poorly, and for all of the test problems, a limited memory variable metric algorithm outperformed the other choices. Four our experiments we used Limited-memory BFGS a limited memory variation of the Broyden–Fletcher–Goldfarb–Shanno (BFGS) method [24, 25], which is an implementation of the variable metric method. For this we used the CRF++ Toolkit[26]. 5. Experimental Set-Up and Results We evaluated our two approaches on Russian data that was recorded"
2013.iwslt-papers.6,2011.iwslt-papers.2,1,0.620866,"BDLex dictionary generally yielded the best performance, we found that the system combination benefited from the inclusion of the output of each additional subsystem in the combination. 3.1. Pseudographeme System In a traditional grapheme-based system, the symbols of the written word are used as the sub-units of pronunciation rather than phonemes. The feasibility of using graphemes instead of phonemes in ASR has been shown in several different works [14, 15, 16]. It was also shown that the combination of a grapheme system with phoneme systems lead to a significant reduction in word-error rate [17]. While French orthography is relatively regular vis a` vis a language like English, the mapping between sounds and graphemes is not bijective, which is to say that the correspondence between graphemes and phonemes can be weak. Often, clusters of graphemes produce the same sounds as other, shorter ones, such as “-ai” and “-´e”, which both correspond to the IPA [e]. We handle this weakness by using single or multiple graphemes as the base units of pronunciation. Our set of grapheme-phones contained 49 elements, among them the same five noise phones as in the BDLex system. Using knowledge of Fre"
2013.iwslt-papers.8,stuker-etal-2012-kit,1,0.825719,"where new data for retraining comes from the same speaker, channel and related conversation topics. Following the implications of [8] we add low confidence score data to the training, but unlike in other work we apply wordbased weighting in order to compensate for errors, as it was done by [9] for acoustic model adaptation. The assumption is that erroneous data is helpful to improve system generalization. Unlike other work, e.g. [10], we refrained from a lattice-based approach. 2. Data The experiments in this paper were conducted with the help of the KIT Lecture Corpus for Speech Translation [11]. The corpus consists of recorded scientific lectures that were held at the Karlsruhe Institute of Technology (KIT). Currently the corpus mainly contains computer science lectures, and a small amount of lectures from other departments and ceremonial talks. 2.1. Training Data The speaker-independent system that we used in our experiments was trained on about 94 hours of speech from the lecture corpus. Our experiments were constrained to two distinct speakers. As training data we had 7.4 hours for speaker A and 8.3 hours for speaker B respectively, which had not been used for training the speake"
2014.amta-researchers.17,W14-3313,1,0.878749,"Missing"
2014.amta-researchers.17,P07-2045,0,0.00783789,"25.46 Table 1: Results for different sample sizes system. Afterwards, the system was used to translate German news data into English. 6.1 System description The speech translation system was trained on the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate th"
2014.amta-researchers.17,D10-1076,1,0.734365,"the authors modified the n-gram-based translation approach to use the neural networks to model the translation probabilities. In Vaswani et al. (2013), noisy-contrastive estimation was used to train the neural network. Therefore, the probabilities do not need to be normalized and the language model can be used during decoding. A different approach also using Restricted Boltzmann Machines was presented in Mnih and Hinton (2007). However this approach exhibits the same complexity issue as feed-forward models. A head to head comparison between RBM and feed-forward language models can be found in Le et al. (2010). In Niehues and Waibel (2012a), another RBM-based language model was introduced. This approach differs from the one intrdoduced in Mnih and Hinton (2007) by a simpler layout that allows us for a fast probability computation. This yields the integration of the model during the decoding step feasible. However, the training complexity heavily depends on the vocabulary size and this model can be trained on a limited amount of training data. In Dahl et al. (2012), a sampling method was presented to efficiently train restricted Boltzmann machines on word observations. This approach enables us to tr"
2014.amta-researchers.17,N12-1005,1,0.845844,"language models is the size of the output vocabulary in large vocabulary continuous speech recognition. A first way to overcome this is to use a short list. Recently, Le et al. (2011) presented a structured output layer neural network which is able to handle large output vocabularies by using a clustering tree to represent the output vocabulary. Motivated by the improvements in speech recognition accuracy as well as in translation quality, authors tried to use the neural networks also for the translation model in a statistical machine translation system. In Schwenk et al. (2007) as well as in Le et al. (2012) the authors modified the n-gram-based translation approach to use the neural networks to model the translation probabilities. In Vaswani et al. (2013), noisy-contrastive estimation was used to train the neural network. Therefore, the probabilities do not need to be normalized and the language model can be used during decoding. A different approach also using Restricted Boltzmann Machines was presented in Mnih and Hinton (2007). However this approach exhibits the same complexity issue as feed-forward models. A head to head comparison between RBM and feed-forward language models can be found in"
2014.amta-researchers.17,C90-3038,0,0.341794,"advantage of the RBM-based language model to use the language model during decoding. The remaining paper is structured as follows. First we review related work and then provide in section 3 a brief overview of RBM-based language models. Section 4 describes the tailored sampling strategies while we describe how the shared word representation is integrated into the RBM layout in section 5. Afterwards we describe and discuss experimental results measured in terms of translation quality in section 6. 2 Related Work A first approach to predict word categories using neural networks was presented in Nakamura et al. (1990). Later, Bengio et al. (2003) introduced neural networks for statistical language modeling. The authors described in detail an approach based on multi-layer neural networks and reported a significative perplexity reduction compared to conventional and class-based language models. In addition, they gave a short outlook to energy minimization networks. An approach using multi-layer neural networks has successfully been applied to speech recognition by Schwenk and Gauvain (2002), Schwenk (2007) and Mikolov et al. (2010). One main problem of continuous space language models is the size of the outp"
2014.amta-researchers.17,W11-2124,1,0.832421,"the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using Minimum Error Rate Training (MERT) (Venugopal et al., 2005). We tested the language models on two different sets. First, we optimized the"
2014.amta-researchers.17,W09-0435,1,0.867497,"and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using Minimum Error Rate Training (MERT) (Venugopal et al., 2005). We tested the language models on two different sets. First, we optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used 3.5K segments containing 31K words. In addition, we opti"
2014.amta-researchers.17,W08-0303,1,0.877148,"AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 226 Sampling size No RBMLM No Sampling 5 10 50 100 1000 BLEU Score 25.16 25.42 25.15 25.01 25.23 25.25 25.46 Table 1: Results for different sample sizes system. Afterwards, the system was used to translate German news data into English. 6.1 System description The speech translation system was trained on the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kols"
2014.amta-researchers.17,2012.iwslt-papers.3,1,0.361369,"ently, most of these NN-based language models use feed-forward networks. These models can be trained on very large monolingual corpora. Furthermore, in most cases a shared word representation for all word positions is learned. Since the calculation of the language model probabilities is quite complex, often the language model can not be used during decoding, Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 222 but only in a rescoring step. Vaswani et al. (2013) presented an approach to also use feedforward language models during decoding. Niehues and Waibel (2012a) proposed a language model based on Restricted Boltzmann Machine (RBM). Since this model uses a quite simple layout, the probability computation is very fast and the language model can be used during decoding. In contrast, the training time of these models depends on the vocabulary size and therefore, the training time can be quite long. Furthermore, this model does not make use of a shared word representation, which can hinder its generalization power with large context. Motivated by techniques developed for other NN-based language model, we tackle in this work these two issues of RBM-based"
2014.amta-researchers.17,2007.tmi-papers.21,0,0.488897,"iminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using Minimum Error Rate Training (MERT) (Venugopal et al., 2005). We tested the language models on two different sets. First, we optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used"
2014.amta-researchers.17,D07-1045,0,0.0703798,"Missing"
2014.amta-researchers.17,D13-1140,0,0.128933,"borhood can be modeled. In state of the art language models contexts of up to 10 words are used. Currently, most of these NN-based language models use feed-forward networks. These models can be trained on very large monolingual corpora. Furthermore, in most cases a shared word representation for all word positions is learned. Since the calculation of the language model probabilities is quite complex, often the language model can not be used during decoding, Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 222 but only in a rescoring step. Vaswani et al. (2013) presented an approach to also use feedforward language models during decoding. Niehues and Waibel (2012a) proposed a language model based on Restricted Boltzmann Machine (RBM). Since this model uses a quite simple layout, the probability computation is very fast and the language model can be used during decoding. In contrast, the training time of these models depends on the vocabulary size and therefore, the training time can be quite long. Furthermore, this model does not make use of a shared word representation, which can hinder its generalization power with large context. Motivated by tech"
2014.amta-researchers.17,W05-0836,1,0.901514,"(Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). Reordering was performed as a preprocessing step using POS information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using Minimum Error Rate Training (MERT) (Venugopal et al., 2005). We tested the language models on two different sets. First, we optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consist of 1.7K segments containing 16K words. As test set we used 3.5K segments containing 31K words. In addition, we optimized and tested the systems on a set of computer science lectures collected at a university. The language models were tested on three different conditions. First, we used the baseline system, then we used a system, which has been adapted to the TED task by using an additional"
2014.iwslt-evaluation.17,2011.iwslt-evaluation.9,1,0.868455,"English→French and German→English. The English→Chinese system setup is described in Section 8.5. Before training and translation, the data is preprocessed. During this phase, exceedingly long sentences and sentence pairs with a large length difference are discarded from the training data. We normalize special dates, numbers and symbols and smart-case the first letter of every sentence. For German→English, we split up compounds [1] on the source side of the corpus. Since the Common Crawl and Giga English→French corpus are very noisy, we trained an SVM classifier to filter them as described in [2]. After preprocessing, the parallel corpora are wordaligned using the GIZA++ Toolkit [3] in both directions. The resulting alignments are then combined using the growdiag-final-and heuristic. The phrases are extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. Phrase table adaptation combining an indomain and out-of-domain phrase table is performed as described in [6]. All translations are generated using our inhouse phrase-based decoder [7]. Unless stated otherwise, we used 4-gram language mod119 Proceedings of the 11th International Workshop on Sp"
2014.iwslt-evaluation.17,J03-1002,0,0.0076696,"tion 8.5. Before training and translation, the data is preprocessed. During this phase, exceedingly long sentences and sentence pairs with a large length difference are discarded from the training data. We normalize special dates, numbers and symbols and smart-case the first letter of every sentence. For German→English, we split up compounds [1] on the source side of the corpus. Since the Common Crawl and Giga English→French corpus are very noisy, we trained an SVM classifier to filter them as described in [2]. After preprocessing, the parallel corpora are wordaligned using the GIZA++ Toolkit [3] in both directions. The resulting alignments are then combined using the growdiag-final-and heuristic. The phrases are extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. Phrase table adaptation combining an indomain and out-of-domain phrase table is performed as described in [6]. All translations are generated using our inhouse phrase-based decoder [7]. Unless stated otherwise, we used 4-gram language mod119 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 els with modified Knese"
2014.iwslt-evaluation.17,E03-1076,0,0.139387,"For the monolingual training data we used the target side of all bilingual corpora as well as the News Shuffle corpus. Additionally, we included the Gigaword corpus for English→French and German→English. The English→Chinese system setup is described in Section 8.5. Before training and translation, the data is preprocessed. During this phase, exceedingly long sentences and sentence pairs with a large length difference are discarded from the training data. We normalize special dates, numbers and symbols and smart-case the first letter of every sentence. For German→English, we split up compounds [1] on the source side of the corpus. Since the Common Crawl and Giga English→French corpus are very noisy, we trained an SVM classifier to filter them as described in [2]. After preprocessing, the parallel corpora are wordaligned using the GIZA++ Toolkit [3] in both directions. The resulting alignments are then combined using the growdiag-final-and heuristic. The phrases are extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. Phrase table adaptation combining an indomain and out-of-domain phrase table is performed as described in [6]. All translation"
2014.iwslt-evaluation.17,P07-2045,0,0.00813363,"ference are discarded from the training data. We normalize special dates, numbers and symbols and smart-case the first letter of every sentence. For German→English, we split up compounds [1] on the source side of the corpus. Since the Common Crawl and Giga English→French corpus are very noisy, we trained an SVM classifier to filter them as described in [2]. After preprocessing, the parallel corpora are wordaligned using the GIZA++ Toolkit [3] in both directions. The resulting alignments are then combined using the growdiag-final-and heuristic. The phrases are extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. Phrase table adaptation combining an indomain and out-of-domain phrase table is performed as described in [6]. All translations are generated using our inhouse phrase-based decoder [7]. Unless stated otherwise, we used 4-gram language mod119 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 els with modified Kneser-Ney smoothing, trained with the SRILM toolkit [8] and scored in the decoding process with KenLM [9]. In addition to common word-based language models, we u"
2014.iwslt-evaluation.17,2012.amta-papers.19,1,0.864002,"lit up compounds [1] on the source side of the corpus. Since the Common Crawl and Giga English→French corpus are very noisy, we trained an SVM classifier to filter them as described in [2]. After preprocessing, the parallel corpora are wordaligned using the GIZA++ Toolkit [3] in both directions. The resulting alignments are then combined using the growdiag-final-and heuristic. The phrases are extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. Phrase table adaptation combining an indomain and out-of-domain phrase table is performed as described in [6]. All translations are generated using our inhouse phrase-based decoder [7]. Unless stated otherwise, we used 4-gram language mod119 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 els with modified Kneser-Ney smoothing, trained with the SRILM toolkit [8] and scored in the decoding process with KenLM [9]. In addition to common word-based language models, we used two token-based language models. The bilingual language model is used to increase the bilingual context during translation beyond phrase boundaries as described in [1"
2014.iwslt-evaluation.17,W11-2123,0,0.0404059,"stic. The phrases are extracted using the Moses toolkit [4] and then scored by our in-house parallel phrase scorer [5]. Phrase table adaptation combining an indomain and out-of-domain phrase table is performed as described in [6]. All translations are generated using our inhouse phrase-based decoder [7]. Unless stated otherwise, we used 4-gram language mod119 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 els with modified Kneser-Ney smoothing, trained with the SRILM toolkit [8] and scored in the decoding process with KenLM [9]. In addition to common word-based language models, we used two token-based language models. The bilingual language model is used to increase the bilingual context during translation beyond phrase boundaries as described in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our syste"
2014.iwslt-evaluation.17,W11-2124,1,0.885113,"6]. All translations are generated using our inhouse phrase-based decoder [7]. Unless stated otherwise, we used 4-gram language mod119 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 els with modified Kneser-Ney smoothing, trained with the SRILM toolkit [8] and scored in the decoding process with KenLM [9]. In addition to common word-based language models, we used two token-based language models. The bilingual language model is used to increase the bilingual context during translation beyond phrase boundaries as described in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16,"
2014.iwslt-evaluation.17,E99-1010,0,0.400501,"8] and scored in the decoding process with KenLM [9]. In addition to common word-based language models, we used two token-based language models. The bilingual language model is used to increase the bilingual context during translation beyond phrase boundaries as described in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as describ"
2014.iwslt-evaluation.17,W08-1006,0,0.0548998,"o increase the bilingual context during translation beyond phrase boundaries as described in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic spee"
2014.iwslt-evaluation.17,P03-1054,0,0.0453561,"o increase the bilingual context during translation beyond phrase boundaries as described in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic spee"
2014.iwslt-evaluation.17,2007.tmi-papers.21,0,0.338063,"in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks"
2014.iwslt-evaluation.17,W09-0435,1,0.927039,"in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks"
2014.iwslt-evaluation.17,W13-0805,1,0.88806,"in [10]. A token consists of a target word and all its aligned source words. As a second token language model, we use a cluster language model based on word classes. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks"
2014.iwslt-evaluation.17,2005.iwslt-1.8,0,0.291731,"asses. This helps alleviate the sparsity problem for surface words by replacing every word in the training corpus with its cluster ID calculated by the MKCLS algorithm [11]. We use two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks or reliable case information. Therefore, when we use the ASR output as input for our MT system, it does not fit the style and format of the training dat"
2014.iwslt-evaluation.17,W13-2264,1,0.831034,"se two main reordering models in our systems. The first consists of automatically learned reordering rules based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks or reliable case information. Therefore, when we use the ASR output as input for our MT system, it does not fit the style and format of the training data. In order to perform special preprocessing on the SLT test data, we use a monolingual translation system as presented in [21]. The system inserts punctuation marks and corrects"
2014.iwslt-evaluation.17,W05-0836,1,0.81298,"based on POS sequences [12] and syntactic parse tree constituents [13, 14] and performs source sentence reordering according to target language word order [15, 16, 17]. The resulting reordering possibilities for each source sentence are then encoded in a lattice. The second model is a lexicalized reordering model [18] which stores reordering probabilities for each phrase pair. As an additional model, we use a Discriminative Word Lexicon (DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks or reliable case information. Therefore, when we use the ASR output as input for our MT system, it does not fit the style and format of the training data. In order to perform special preprocessing on the SLT test data, we use a monolingual translation system as presented in [21]. The system inserts punctuation marks and corrects case information, so that there is less divergence between the MT training data and the SLT input data. A"
2014.iwslt-evaluation.17,2012.iwslt-papers.15,1,0.883589,"DWL) using source context features as described in [19]. We tune our systems using Minimum Error Rate Training (MERT) against the BLEU score as described in [20]. 3. Preprocessing for speech translation A conventional automatic speech recognition (ASR) system generates a stream of recognized words without punctuation marks or reliable case information. Therefore, when we use the ASR output as input for our MT system, it does not fit the style and format of the training data. In order to perform special preprocessing on the SLT test data, we use a monolingual translation system as presented in [21]. The system inserts punctuation marks and corrects case information, so that there is less divergence between the MT training data and the SLT input data. As sentence boundaries are already given in the test sets, we leave them as they are but predict other punctuation marks within the segment. This preprocessing will be denoted as Monolingual Comma and Case Insertion (MCCI). For building the systems, we took the preprocessed source side of the parallel training data. We remove all punctuation marks from the data and insert a final period at the end of each line. In addition to this, all word"
2014.iwslt-evaluation.17,2012.iwslt-papers.3,1,0.926316,"target side of the monolingual translation system, we keep the punctuation marks as well as case information, so that the “translation” of our MCCI system consists of inserting punctuation marks and correcting case information. We built an MCCI system for English and German and applied it to all three official SLT track directions English→German, German→English and English→French. 4. n-best list rescoring We perform additional experiments to use a neural network language and translation model in n-best list rescoring. We train an 8-gram Restricted Boltzmann Machine (RBM)-based language model [22] on the in-domain TED corpus. The language model uses 32 hidden units and a shared word representation with 512 dimensionsUnigram sampling is applied as described in [23]. In addition, we use an RBM-based translation model inspired by the work of Devlin et al. [24]. The RBM models the joined probability of 8 target words and a set of attached source words. The set of attached source words is calculated as follows: We first use the the source word aligned to the last target word in the 8-gram. If this does not exist, we take the source word aligned to the nearest target word. The set of source"
2014.iwslt-evaluation.17,2014.amta-researchers.17,1,0.856794,"serting punctuation marks and correcting case information. We built an MCCI system for English and German and applied it to all three official SLT track directions English→German, German→English and English→French. 4. n-best list rescoring We perform additional experiments to use a neural network language and translation model in n-best list rescoring. We train an 8-gram Restricted Boltzmann Machine (RBM)-based language model [22] on the in-domain TED corpus. The language model uses 32 hidden units and a shared word representation with 512 dimensionsUnigram sampling is applied as described in [23]. In addition, we use an RBM-based translation model inspired by the work of Devlin et al. [24]. The RBM models the joined probability of 8 target words and a set of attached source words. The set of attached source words is calculated as follows: We first use the the source word aligned to the last target word in the 8-gram. If this does not exist, we take the source word aligned to the nearest target word. The set of source words consists then of this source word, its previous 5 source words and its following 5 source words. We create this set of 8 target and 11 source words for every target"
2014.iwslt-evaluation.17,P14-1129,0,0.0721445,"and German and applied it to all three official SLT track directions English→German, German→English and English→French. 4. n-best list rescoring We perform additional experiments to use a neural network language and translation model in n-best list rescoring. We train an 8-gram Restricted Boltzmann Machine (RBM)-based language model [22] on the in-domain TED corpus. The language model uses 32 hidden units and a shared word representation with 512 dimensionsUnigram sampling is applied as described in [23]. In addition, we use an RBM-based translation model inspired by the work of Devlin et al. [24]. The RBM models the joined probability of 8 target words and a set of attached source words. The set of attached source words is calculated as follows: We first use the the source word aligned to the last target word in the 8-gram. If this does not exist, we take the source word aligned to the nearest target word. The set of source words consists then of this source word, its previous 5 source words and its following 5 source words. We create this set of 8 target and 11 source words for every target 8-gram in the parallel in-domain TED corpus. In rescoring, we then calculate the free energy o"
2014.iwslt-evaluation.17,P12-2059,0,0.0524998,"task [26]. 6. Arabic transliteration In most cases, untranslated words break the harmony of the translation into a language which uses a different scripting (e.g. English into Arabic.) Therefore, it is more conve120 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 NP CD ten Figure 1: Examples of trivial correspondences nient to transliterate those untranslated words, as they are unlikely to hurt the system performance further. Our transliteration is mostly similar to the character-based translation in its transliteration part [27]. It is consequently a statistical phrase-based translation based on unigram characters. The corresponding training data of this system is mainly a subset of the word pairs obtained from the aligned corpora (TED and UN). First, the Arabic word of each aligned pair is roughly transliterated into English, using only trivial correspondences (see Fig. 1 for an example). The Levenshtein distance ratio is then computed between the resulting rough transliteration and the English word. Finally, we retain only pairs with ratios higher than a certain threshold (our threshold was empirically set to 0.5)."
2014.iwslt-evaluation.17,2014.iwslt-papers.18,1,0.843414,"esponding training data of this system is mainly a subset of the word pairs obtained from the aligned corpora (TED and UN). First, the Arabic word of each aligned pair is roughly transliterated into English, using only trivial correspondences (see Fig. 1 for an example). The Levenshtein distance ratio is then computed between the resulting rough transliteration and the English word. Finally, we retain only pairs with ratios higher than a certain threshold (our threshold was empirically set to 0.5). NP JJ big For our English-Chinese translation we applied a novel rulebased preordering approach [28], which uses the tree information of multiple syntactic levels. This approach extends the tree-based reordering [17] from one level into multiple levels, which has the capability to process complex reordering cases. Reordering patterns are based on multiple levels of the syntax tree. Figure 2 illustrates how the reordering patterns are detected. The detection starts from the root node of the syntax tree, goes downwards multiple levels and uses the nodes in these levels to detect the reordering pattern. In this example, the nodes that are used for detecting the reordering pattern are colored gr"
2014.iwslt-evaluation.17,P10-2041,0,0.038163,"adapted by combining two phrase tables, one trained on all training data and one trained only on the TED in-domain corpus. Furthermore, the translation process is modeled using a bilingual language model trained on all parallel data and a discriminative word lexicon trained on the TED corpus. The DWL uses source context features. Finally, five language models are used. Three are word-based models, the first of which is trained on all available German data. The second one is trained only on the TED corpus. Finally, we use a word-based model trained on 5M sentences chosen through data selection [29]. In addition, a 9-gram POS-based language model and a 9-gram cluster language model using 1000 MKCLS classes are used. Afterwards, we rescored the system using the weights trained using the ListNet algorithm described in Section 4. The rescoring was trained on the test2010 and test2011 data and dev2010 was used as a cross-validation set. This results in an improvement of 0.3 BLEU points. Then we added an RBM-based language model and an RBM-based translation model. We could improve by using the RBM-based translation model by 0.4 BLEU points, reaching the best BLEU score on test2012 with 24.31"
2014.iwslt-evaluation.17,C08-1098,0,0.0173077,"hoe, December 4th and 5th, 2014 System Primary Stemmed Dev 39.03 39.22 Test 31.98 31.68 Table 4: Contrastive system for German→English (MT) the information encoded in inflections such as gender or case may be discarded. However, stemming the whole German corpus hurts translation since too much information is lost. We therefore experimented with only stemming adjectives, which in German can have five different suffixes depending on the gender and case. The stemming was performed on the preprocessed files before compound splitting. The files were tagged with the TreeTagger [12] and the RFTagger [30]. We based our decision when and how to stem on the fine-grained tags output by the RFTagger. We only stemmed words tagged as an attributive adjective, since they are inflected in German. If the word as tagged as a comparative or superlative, we manually removed the inflected suffix in order to maintain the comparative nature of the adjective. For all other adjectives, we used the stem output by the TreeTagger. After stemming, compound splitting was applied as described in Section 2. We then trained a new alignment and phrasetable on the stemmed corpora. Previous experiments had shown that usi"
2014.iwslt-evaluation.7,D14-1003,1,0.921764,"slation of spoken language. The IWSLT TED talks task constitutes an interesting framework for empirical testing of some of the systems for spoken language translation which are developed as part of the project. In this work, we describe the EU-BRIDGE submissions to the 2014 IWSLT translation task. This year, we combined several single systems of RWTH, UEDIN, KIT, and FBK for the German→English SLT, German→English MT, English→German MT, and English→French MT tasks. Additionally to the standard system combination pipeline presented in [1, 2], we applied a recurrent neural network rescoring step [3] for the English→French MT task. Similar cooperative approaches based on system combination have proven to be valuable for machine translation in previous joint submissions, e.g. [4, 5]. 2. RWTH Aachen University RWTH applied the identical training pipeline and models on both language pairs: The state-of-the-art phrase-based baseline systems were augmented with a hierarchical reordering model, several additional language models (LMs) and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translat"
2014.iwslt-evaluation.7,W10-1738,1,0.885248,"and maximum expected B LEU training for phrasal, lexical and reordering models. Further, RWTH employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-"
2014.iwslt-evaluation.7,P03-1021,0,0.488353,"employed rescoring with novel recurrent neural language and translation models. The same systems were used for the SLT track, where RWTH ad57 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing."
2014.iwslt-evaluation.7,popovic-ney-2006-pos,1,0.798687,"th and 5th, 2014 ditionally performed punctuation prediction on the automatic transcriptions employing hierarchical phrase-based translation. Both the phrase-based and the hierarchical decoder are implemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selectio"
2014.iwslt-evaluation.7,P13-2121,1,0.819366,"mplemented in RWTH’s publicly available translation toolkit Jane [6, 7]. The model weights of all systems were tuned with standard Minimum Error Rate Training [8] on the provided dev2012 set. RWTH used B LEU as optimization objective. For the German→English translation direction, in a preprocessing step the German source was decompounded [9] and part-of-speech-based long-range verb reordering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWT"
2014.iwslt-evaluation.7,P10-2041,0,0.0916594,"rdering rules [10] were applied. RWTH’s translation systems are described in more detail in [11]. Backoff Language Models Each translation system used three backoff LMs that were estimated with the KenLM toolkit [12]: A large general domain 5-gram LM, an in-domain 5-gram LM and a 7-gram word class language model (wcLM). All of them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU ob"
2014.iwslt-evaluation.7,E99-1010,0,0.0737032,"them used interpolated Kneser-Ney smoothing. For the general domain LM, RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for"
2014.iwslt-evaluation.7,D13-1138,1,0.85854,"RWTH first selected 12 of the English Shuffled News, and 41 of the French Shuffled News as well as both the English and French Gigaword corpora by the cross-entropy difference criterion described in [13]. The selection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumv"
2014.iwslt-evaluation.7,P12-1031,0,0.0125863,"lection was then concatenated with all available remaining monolingual data and used to build and unpruned LM. The in-domain language models were estimated on the TED data only. For the word class LM, RWTH trained 200 classes on the target side of the bilingual training data using an in-house tool similar to mkcls [14]. With these class definitions, RWTH applied the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and"
2014.iwslt-evaluation.7,P10-1049,1,0.833909,"the technique shown in [15] to compute the wcLM on the same data as the general-domain LM. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LST"
2014.iwslt-evaluation.7,D14-1132,0,0.157332,"M. Maximum Expected B LEU Training RWTH applied discriminative training, learning three types of features under a maximum expected B LEU objective [16]. It was performed on the TED portion of the data, which is high quality in-domain data of reasonable size. This makes training feasible while at the same time providing an implicit domain adaptation effect. Similar to [16], RWTH generated 100-best lists on the training data which were used as training samples for a gradient based update method. Leave-oneout [17] was applied to circumvent over-fitting. Here, RWTH followed an approach similar to [18], where each feature type was condensed into a single feature for the log-linear model combination. In the first pass, RWTH trained phrase pair and phrase-internal word pair features, and in the second pass a hierarchical reordering model, resulting altogether in an additional eight models for log-linear combination. Recurrent Neural Network Models All systems applied rescoring on 1000-best lists using recurrent language and translation models. The recurrency was handled with the long short-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficienc"
2014.iwslt-evaluation.7,2011.iwslt-papers.7,1,0.944851,"ort-term memory (LSTM) architecture [19] and RWTH used a class-factored output layer for increased efficiency as described in [20]. All neural networks were trained on the TED portion of the data with 2000 word classes. In addition to the recurrent language model (RNN-LM), RWTH applied the deep bidirectional word-based translation model (RNN-BTM) described in [3], which is capable of taking the full source context into account for each translation decision. Spoken Language Translation For the SLT task, RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the f"
2014.iwslt-evaluation.7,2014.iwslt-papers.17,1,0.734908,"RWTH reintroduced punctuation and case information before the actual translation similar to [21]. However, RWTH employed a hierarchical phrase-based system with a maximum of one nonterminal symbol per rule in place of a phrase-based system. A punctuation prediction system based on hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided"
2014.iwslt-evaluation.7,P07-2045,1,0.0190208,"n hierarchical translation is able to capture long-range dependencies between words and punctuation marks and is more robust for unseen word sequences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26]"
2014.iwslt-evaluation.7,N04-1035,0,0.0565459,"equences. The model weights are tuned with standard MERT on 100best lists. As optimization criterion RWTH used F2 -Score rather than B LEU or W ER. More details can be found in [22]. Since punctuation predicting and recasing were applied before the actual translation, the final translation systems from the MT track could be kept completely unchanged. 3. University of Edinburgh The UEDIN translation engines [23] are based on the open source Moses toolkit [24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [2"
2014.iwslt-evaluation.7,W08-0509,0,0.192359,"[24]. UEDIN set up phrase-based systems for all SLT and MT tasks covered in this paper, and additionally a string-to-tree syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six"
2014.iwslt-evaluation.7,N13-1073,0,0.0453396,"e syntax-based system [25] for the English→German MT task. The systems were trained using monolingual and parallel data from WIT3 , Europarl, MultiUN, the English and French Gigaword corpora as provided by the Linguistic Data Consortium, the German Political Speeches Corpus, and the Common Crawl, 109 , and News Commentary corpora from the WMT shared task training data. Word alignments for the MT track systems were created by aligning the data in both directions with MGIZA++ [26] and symmetrizing the two trained alignments. Word alignments for the SLT track system were created using fast align [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sp"
2014.iwslt-evaluation.7,C14-1041,1,0.839592,"ndividual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable"
2014.iwslt-evaluation.7,N12-1047,0,0.0681194,"them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is"
2014.iwslt-evaluation.7,P02-1040,0,0.0918061,"d to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by trai"
2014.iwslt-evaluation.7,2006.iwslt-papers.1,1,0.862433,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,2012.iwslt-papers.15,1,0.927241,"els over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic differences. Previous research [35, 21, 36] suggests that it is preferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT tas"
2014.iwslt-evaluation.7,P05-1066,1,0.733044,"ferrable to punctuate the text before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the"
2014.iwslt-evaluation.7,E03-1076,1,0.858704,"xt before translation, which is what UEDIN did by training a translation system on the German side of the parallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE sy"
2014.iwslt-evaluation.7,2012.amta-papers.9,1,0.84942,"arallel data. The “source language” of the system had punctuation and capitalization stripped, and the “target language” was the standard German parallel text. The handling of punctuation is similar to the other groups in this paper, however UEDIN used a phrase-based model with no distortion or reordering, and tuned the model to the ASR input text using batch MIRA and the B LEU score. German→English MT For the UEDIN German→English MT task system, prereordering [37] and compound splitting [38] were applied to the German source language side in a preprocessing step. A factored translation model [39] was employed. Source side factors are word, lemma, part-of-speech (POS) tag, and morphological tag. Target side factors are word, lemma, and POS tag. UEDIN incorporated two additional LMs into the German→English MT system: a 7-gram LM over POS tags (trained on WIT3 only) and a 7-gram LM over lemmas (trained on WIT3 only). Model weights were optimized on a concatenation of dev2010 and dev2012. English→French MT UEDIN contributed two phrase-based systems for the English→French EU-BRIDGE system combination. Both comprise Brown clusters with 200 classes as additional factors on source and target"
2014.iwslt-evaluation.7,D08-1089,0,0.176922,"ign [27]. The SRILM toolkit [28] was employed to train 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation a"
2014.iwslt-evaluation.7,W14-3324,1,0.784121,"ical tag. UEDIN-A was trained with all corpora, whereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house ph"
2014.iwslt-evaluation.7,2012.iwslt-papers.17,1,0.881764,"ain 5-gram LMs with modified Kneser-Ney smoothing [29]. UEDIN trained individual LMs on each corpus and then interpolated them using weights tuned to minimize perplexity on a development set. Common features included in the UEDIN phrase-based systems are the language model, phrase translation scores in both directions smoothed with Good-Turing discounting, lexical translation scores in both directions, word and phrase penalties, six simple count-based binary features, distancebased distortion costs, a hierarchical lexicalized reordering model [30], sparse lexical and domain indicator features [31] and operation sequence models over different word representations [32]. Model weights were optimized with batch MIRA [33] to maximize B LEU [34]. Spoken Language Translation One of the main challenges of spoken language translation is to overcome the mismatch in the style of data that the 58 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 speech recognition systems output, and the written text that is used to train the translation model. ASR system output lacks punctuation and capitalization, which is the main stylistic diff"
2014.iwslt-evaluation.7,C04-1024,0,0.0400394,"ereas for UEDIN-B the parallel training data was restricted to the indomain WIT3 corpus. Additional features of the systems are: a 5-gram LM over Brown clusters, a 7-gram LM over morphological tags (UEDIN-A: trained on all data, UEDIN-B: trained on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models wer"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.18,1,0.873679,"ined on WIT3 only), and a 7-gram LM over POS tags (UEDIN-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standa"
2014.iwslt-evaluation.7,W14-3362,1,0.610881,"N-A, not UEDIN-B). Model weights of UEDIN-B were optimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for"
2014.iwslt-evaluation.7,W14-4018,1,0.774295,"ptimized on dev2010, model weights of UEDIN-A on a concatenation of dev2010 and dev2012. Syntax-based system. UEDIN-C is a string-to-tree translation system with similar features as the ones described in [40]. The target-side data was parsed with BitPar [41], and right binarization was applied to the parse trees. The system was adapted to the TED domain by extracting separate rule tables (from the WIT3 corpus and from the rest of the parallel data) and merging them with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In t"
2014.iwslt-evaluation.7,2011.iwslt-evaluation.9,1,0.861968,"m with a fill-up technique [42]. Augmenting the system with non-syntactic phrases [43] and adding soft source syntactic constraints [44] yielded further improvements. Model weights of UEDIN-C were optimized on a concatenation of dev2010 and dev2012. 4. Karlsruhe Institute of Technology The KIT translations were generated by an in-house phrasebased translations system [45]. The models were trained on the Europarl, News Commentary, WIT3 , Common Crawl corpora for all directions, as well as on the additional monolingual training data. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were"
2014.iwslt-evaluation.7,2007.tmi-papers.21,0,0.0614729,"ta. The noisy Crawl corpora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabiliti"
2014.iwslt-evaluation.7,W09-0413,1,0.842557,"pora were filtered using an SVM classifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the tra"
2014.iwslt-evaluation.7,W13-0805,1,0.85195,"ifier [46]. In addition to the standard preprocessing, KIT used compound splitting [38] for the German text when translating from German. In the SLT task, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual langu"
2014.iwslt-evaluation.7,W08-1006,0,0.0150981,"k, KIT first recased the input and added punctuation marks to the ASR hypotheses. This was done with a monolingual translation system as shown in [36]. In all translation directions, KIT used a pre-reordering approach. Different reorderings of the source sentences were encoded in a word lattice. For the English→French system, only short-range rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the ta"
2014.iwslt-evaluation.7,2012.amta-papers.19,1,0.839901,"e rules were used to generate these lattices [47]. Long-range rules [48] and tree-based reordering rules [49] were used for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WI"
2014.iwslt-evaluation.7,W11-2124,1,0.902739,"for German→English. The POS tags needed for these rules were generated by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cl"
2014.iwslt-evaluation.7,W13-2264,1,0.835602,"ed by the TreeTagger [50] and the parse trees by the Stanford Parser [51]. In addition, for the language pairs involving German KIT applied the different reorderings of both language pairs using a lexicalized reordering model. The phrase tables of the systems were trained using GIZA++ alignment [52]. KIT adapted the phrase table to the TED domain using the backoff approach and by means of candidate selection [53]. In addition to the phrase table probabilities, KIT modeled the translation process by a bilingual language model [54] and a discriminative word lexicon using source context features [55]. During decoding, KIT used several LMs to adapt the system to the task and to better model the sentence structure using a class-based LM. For the German→English task, KIT used one LM trained on all data, an in-domain LM trained only on the WIT3 corpus, and one LM trained on 5M sentences selected using cross-entropy difference [13]. As classes KIT used the clusters obtained using the mkcls algorithm on the WIT3 corpus. For German↔ English, KIT used a 9-gram LM with 100 or 1000 clusters and for the English→French MT task, a cluster-based 4-gram LM was trained on 500 clusters. For English→German"
2014.iwslt-evaluation.7,2012.eamt-1.60,1,0.892622,"Missing"
2014.iwslt-evaluation.7,D11-1033,0,0.167316,"Missing"
2014.iwslt-evaluation.7,W05-0909,0,0.085167,"m multiple hypotheses which are outputs of different translation engines. The consensus translations can be better in terms of translation quality than any of the individual hypotheses. To combine the engines of the project partners for the EU-BRIDGE joint setups, we applied a system combination implementation that has been developed at RWTH Aachen University [1]. In Fig. 1 an overview is illustrated. We first address the generation of a confusion network (CN) from I input translations. For that we need a pairwise alignment between all input hypotheses. This alignment is calculated via METEOR [60]. The hypotheses are then reordered to match the word order of a selected skeleton hypothesis. Instead of using only one of the input hypothesis as skeleton, we generate I different CNs, each having one of the input systems as skeleton. The final lattice is the union of all I previous generated CNs. In Fig. 2 an example confusion network of I = 4 input translations with one skeleton translation is illustrated. Between two adjacent nodes, we always have a choice between the I different system output words. The confusion network decoding step involves determining the shortest path through the ne"
2014.iwslt-evaluation.7,2006.amta-papers.25,0,0.0356913,"andard set of models is a word penalty, a 3-gram language model trained on the input hypotheses, and for each system one binary voting feature. During decoding the binary voting feature for system i (1 ≤ i ≤ I) is 1 iff the word is from system i, otherwise 0. The M different model weights λm are trained with MERT [8]. the red cab the a a red blue green train car car Figure 2: System A: the red cab ; System B: the red train ; System C: a blue car ; System D: a green car ; Reference: the blue car . 7. Results In this section, we present our experimental results. All reported B LEU [34] and T ER [61] scores are case-sensitive with one reference. All system combination results have been generated with RWTH’s open source system combination implementation Jane [1]. German→English SLT For the German→English SLT task, we combined three different individual systems generated by UEDIN, KIT, and RWTH. Experimental results are given in Table 1. The final system combination yields improvements of 1.5 points in B LEU and 1.2 points in T ER compared to the best single system (KIT). All single systems as well as the system combination parameters were tuned on dev2012. For this year’s IWSLT SLT track,"
2014.iwslt-evaluation.7,E06-1005,1,\N,Missing
2014.iwslt-evaluation.7,P11-1105,1,\N,Missing
2014.iwslt-evaluation.7,W10-1711,1,\N,Missing
2014.iwslt-evaluation.7,2010.iwslt-evaluation.22,1,\N,Missing
2014.iwslt-evaluation.7,E14-2008,1,\N,Missing
2014.iwslt-evaluation.7,2014.iwslt-evaluation.6,1,\N,Missing
2014.iwslt-evaluation.7,J03-1002,1,\N,Missing
2014.iwslt-evaluation.7,C12-3061,1,\N,Missing
2014.iwslt-evaluation.7,2013.iwslt-evaluation.16,1,\N,Missing
2014.iwslt-evaluation.7,W14-3310,1,\N,Missing
2014.iwslt-evaluation.9,2013.iwslt-evaluation.1,1,0.735211,"stems: 1. Introduction The 2014 International Workshop on Spoken Language Translation (IWSLT) offers a comprehensive evaluation campaign on spoken language translation. The evaluation is organized in different evaluation tracks covering automatic speech recognition (ASR), machine translation (MT), and the full-fledged combination of the two of them into speech translation systems (SLT). The evaluations in the tracks are conducted on TED Talks (http://www.ted.com/talks), short 5-25min presentations by people from various fields related in some way to Technology, Entertainment, and Design (TED) [1]. The goal of the TED ASR track is the automatic transcription of fully unsegmented TED lectures. The quality of the resulting transcriptions are measured in word error rate (WER). In this paper we describe our Italian, German and English ASR systems with which we participated in the TED ASR track of the 2014 IWSLT evaluation campaign. While our German and English ASR systems are based on our previous years’ evaluation systems [2] our Italian system is a completely new system that was developed from scratch. Our general system setup uses multiple complementary subsys• 200 hours of Quaero train"
2014.iwslt-evaluation.9,P10-2041,0,0.0847607,"Missing"
2014.iwslt-papers.10,N03-1017,0,0.0863198,"using this approach in a state-of-the-art translation system, we can improve the performance by up to 0.5 BLEU points for three different language pairs on the TED translation task. 1. Introduction Since the first attempt to statisical machine translation (SMT) [1], the approach has drawn much interest in the research community and huge improvements in translation quality have been achieved. Still, there are plenty of problems in SMT which should be addressed. One is that the translation decision depends on a quite small context. In standard phrase-based statistical machine translation (PBMT) [2], the two main components are the translation and language models. The translation model is modeled by counting phrase pairs, which are sequences of words extracted from bilingual corpora. By using phrase segments instead of words, PBMT can exploit some local source and target contexts within those segments. But no context information outside the phrase pairs is used. In an n-gram language model, only a context of up to n target words is considered. Several directions have been proposed to leverage information from wider contexts in the phrase-based SMT framework. For example, the Discriminati"
2014.iwslt-papers.10,D09-1022,0,0.282573,"nts are the translation and language models. The translation model is modeled by counting phrase pairs, which are sequences of words extracted from bilingual corpora. By using phrase segments instead of words, PBMT can exploit some local source and target contexts within those segments. But no context information outside the phrase pairs is used. In an n-gram language model, only a context of up to n target words is considered. Several directions have been proposed to leverage information from wider contexts in the phrase-based SMT framework. For example, the Discriminative Word Lexicon (DWL) [3][4] exploits the occurence of all the words in the whole source sentence to predict the presence of words in the target sentence. This wider context information is encoded as features and employed in a discriminative framework. Hence, they train a maximum entropy (MaxEnt) model for each target word. While this model can improve the translation quality in different conditions, MaxEnt models are linear classifiers. On the other hand, hierarchical non-linear classifiers can model dependencies between different source words better since they perform some abstraction over the input. Hence, introduc"
2014.iwslt-papers.10,W13-2264,1,0.936673,"are the translation and language models. The translation model is modeled by counting phrase pairs, which are sequences of words extracted from bilingual corpora. By using phrase segments instead of words, PBMT can exploit some local source and target contexts within those segments. But no context information outside the phrase pairs is used. In an n-gram language model, only a context of up to n target words is considered. Several directions have been proposed to leverage information from wider contexts in the phrase-based SMT framework. For example, the Discriminative Word Lexicon (DWL) [3][4] exploits the occurence of all the words in the whole source sentence to predict the presence of words in the target sentence. This wider context information is encoded as features and employed in a discriminative framework. Hence, they train a maximum entropy (MaxEnt) model for each target word. While this model can improve the translation quality in different conditions, MaxEnt models are linear classifiers. On the other hand, hierarchical non-linear classifiers can model dependencies between different source words better since they perform some abstraction over the input. Hence, introducing"
2014.iwslt-papers.10,W11-2124,1,0.93285,"e. In PBMT, the lexical joint models allow us to use local source and target contexts in the form of phrases. Lately, advanced joint models have been proposed to either enhance the joint probability model between source and target sides or engage more suitable contexts. The n-gram based approach [6] directly models the joint probability of source and target sentences from the conditional probability of a current n-gram pair givens sequences 223 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 of previous bilingual n-grams. In [7], this idea is introduced into the phrase-based MT approach. Thereby, parallel context over phrase boundaries can be used during the translation. Standard phrase-based or n-gram translation models are basically built upon statistical principles such as Maximum Entropy and smoothing techniques. Recently, joint models are learned using neural networks where non-linear translation relationships and semantic generalization of words can be performed [8]. Le et. al. [9] follow the n-gram translation direction but model the conditional probability of a target word given the history of bilingual phras"
2014.iwslt-papers.10,N13-1090,0,0.0129941,"ceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 of previous bilingual n-grams. In [7], this idea is introduced into the phrase-based MT approach. Thereby, parallel context over phrase boundaries can be used during the translation. Standard phrase-based or n-gram translation models are basically built upon statistical principles such as Maximum Entropy and smoothing techniques. Recently, joint models are learned using neural networks where non-linear translation relationships and semantic generalization of words can be performed [8]. Le et. al. [9] follow the n-gram translation direction but model the conditional probability of a target word given the history of bilingual phrase pairs using a neural network architecture. They then use their model in a k-best rescorer instead of in their n-gram decoder. Devlin et. al. [10] add longer source contexts and renew the joint formula so that it can be included in a decoder rather than a k-best rescoring module. Schwenk et. al. [11] calculate the conditional probability of a target phrase instead of a target word given a source phrase. Although the aforementioned works essentiall"
2014.iwslt-papers.10,N12-1005,0,0.0746463,"11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 of previous bilingual n-grams. In [7], this idea is introduced into the phrase-based MT approach. Thereby, parallel context over phrase boundaries can be used during the translation. Standard phrase-based or n-gram translation models are basically built upon statistical principles such as Maximum Entropy and smoothing techniques. Recently, joint models are learned using neural networks where non-linear translation relationships and semantic generalization of words can be performed [8]. Le et. al. [9] follow the n-gram translation direction but model the conditional probability of a target word given the history of bilingual phrase pairs using a neural network architecture. They then use their model in a k-best rescorer instead of in their n-gram decoder. Devlin et. al. [10] add longer source contexts and renew the joint formula so that it can be included in a decoder rather than a k-best rescoring module. Schwenk et. al. [11] calculate the conditional probability of a target phrase instead of a target word given a source phrase. Although the aforementioned works essentially augment the jo"
2014.iwslt-papers.10,P14-1129,0,0.0504957,"n. Standard phrase-based or n-gram translation models are basically built upon statistical principles such as Maximum Entropy and smoothing techniques. Recently, joint models are learned using neural networks where non-linear translation relationships and semantic generalization of words can be performed [8]. Le et. al. [9] follow the n-gram translation direction but model the conditional probability of a target word given the history of bilingual phrase pairs using a neural network architecture. They then use their model in a k-best rescorer instead of in their n-gram decoder. Devlin et. al. [10] add longer source contexts and renew the joint formula so that it can be included in a decoder rather than a k-best rescoring module. Schwenk et. al. [11] calculate the conditional probability of a target phrase instead of a target word given a source phrase. Although the aforementioned works essentially augment the joint translation model, they have an inherent limitation: only exploit local contexts. They estimate the joint model using sequences of words as the basic unit. On the other hand, there are several approaches utilizing global contexts. Motivated by Bangalore et. al [12], Hasan et"
2014.iwslt-papers.10,C12-2104,0,0.01501,"ently, joint models are learned using neural networks where non-linear translation relationships and semantic generalization of words can be performed [8]. Le et. al. [9] follow the n-gram translation direction but model the conditional probability of a target word given the history of bilingual phrase pairs using a neural network architecture. They then use their model in a k-best rescorer instead of in their n-gram decoder. Devlin et. al. [10] add longer source contexts and renew the joint formula so that it can be included in a decoder rather than a k-best rescoring module. Schwenk et. al. [11] calculate the conditional probability of a target phrase instead of a target word given a source phrase. Although the aforementioned works essentially augment the joint translation model, they have an inherent limitation: only exploit local contexts. They estimate the joint model using sequences of words as the basic unit. On the other hand, there are several approaches utilizing global contexts. Motivated by Bangalore et. al [12], Hasan et. al. [13] calculate the probability of a target word given two source words which do not necessarily belong to a phrase. Mauser et. al. [3] suggest anothe"
2014.iwslt-papers.10,P07-1020,0,0.0138072,"n et. al. [10] add longer source contexts and renew the joint formula so that it can be included in a decoder rather than a k-best rescoring module. Schwenk et. al. [11] calculate the conditional probability of a target phrase instead of a target word given a source phrase. Although the aforementioned works essentially augment the joint translation model, they have an inherent limitation: only exploit local contexts. They estimate the joint model using sequences of words as the basic unit. On the other hand, there are several approaches utilizing global contexts. Motivated by Bangalore et. al [12], Hasan et. al. [13] calculate the probability of a target word given two source words which do not necessarily belong to a phrase. Mauser et. al. [3] suggest another lexical translation approach, named Discriminative Word Lexicon (DWL), concentrating on predicting the presence of target words given the source words. Niehues et. al. [4] extend the model to employ the source and target contexts, but they used the same MaxEnt classifier for the task. Carpuat et. al. [14] is the most similar work to the DWL direction in terms of using the whole source sentence to perform the lexical choices of ta"
2014.iwslt-papers.10,D08-1039,0,0.0155651,"onger source contexts and renew the joint formula so that it can be included in a decoder rather than a k-best rescoring module. Schwenk et. al. [11] calculate the conditional probability of a target phrase instead of a target word given a source phrase. Although the aforementioned works essentially augment the joint translation model, they have an inherent limitation: only exploit local contexts. They estimate the joint model using sequences of words as the basic unit. On the other hand, there are several approaches utilizing global contexts. Motivated by Bangalore et. al [12], Hasan et. al. [13] calculate the probability of a target word given two source words which do not necessarily belong to a phrase. Mauser et. al. [3] suggest another lexical translation approach, named Discriminative Word Lexicon (DWL), concentrating on predicting the presence of target words given the source words. Niehues et. al. [4] extend the model to employ the source and target contexts, but they used the same MaxEnt classifier for the task. Carpuat et. al. [14] is the most similar work to the DWL direction in terms of using the whole source sentence to perform the lexical choices of target words. They tre"
2014.iwslt-papers.10,D07-1007,0,0.0417135,"ds as the basic unit. On the other hand, there are several approaches utilizing global contexts. Motivated by Bangalore et. al [12], Hasan et. al. [13] calculate the probability of a target word given two source words which do not necessarily belong to a phrase. Mauser et. al. [3] suggest another lexical translation approach, named Discriminative Word Lexicon (DWL), concentrating on predicting the presence of target words given the source words. Niehues et. al. [4] extend the model to employ the source and target contexts, but they used the same MaxEnt classifier for the task. Carpuat et. al. [14] is the most similar work to the DWL direction in terms of using the whole source sentence to perform the lexical choices of target words. They treat the selection process as a Word Sense Disambiguation (WSD) task, where target words or phrases are WSD senses. They extract a rich feature set from the source sentences, including source words, and input them into a WSD classifier. Still, the problem persists since they use the shallow classifiers for that task. Considering the advantages of non-linear models mentioned before, we opt for using deep neural network architectures to learn the DWL. W"
2014.iwslt-papers.10,2012.eamt-1.60,0,0.0264452,"e experiments. Validation Sent. Tok. (avg.) Sent. Tok. (avg.) En-Fr 149991 3.1m 6153 125k En-Zh 140006 3.3m 8962 211k De-En 130654 2.5m 7430 142k Table 1: Statistics of the corpora used to train NNDWL 4.1. System description The system we use as our baseline is a state-of-the-art translation system for English to French without any DWL. To the baseline system, we add several DWL components trained on different corpora as independent features in the log-linear framework utilized by our in-house phrase-based decoder. The system is trained on the EPPS, NC, Common Crawl, Giga corpora and TED talks[15]. The monolingual data we used to train language models includes the corresponding monolingual parts of those parallel corpora plus News Shuffle and Gigaword. The data is preprocessed and the phrase table is built using the scripts from the Moses package [16]. We adapt the general, big corpora to the in-domain TED data using the Backoff approach described in [17]. Adaptation is also conducted for the monolingual data. We train a 4-gram language model using the SRILM toolkit [18]. In addition, several non-word language models are included to capture the dependencies between source and target wo"
2014.iwslt-papers.10,P07-2045,0,0.00428231,"is a state-of-the-art translation system for English to French without any DWL. To the baseline system, we add several DWL components trained on different corpora as independent features in the log-linear framework utilized by our in-house phrase-based decoder. The system is trained on the EPPS, NC, Common Crawl, Giga corpora and TED talks[15]. The monolingual data we used to train language models includes the corresponding monolingual parts of those parallel corpora plus News Shuffle and Gigaword. The data is preprocessed and the phrase table is built using the scripts from the Moses package [16]. We adapt the general, big corpora to the in-domain TED data using the Backoff approach described in [17]. Adaptation is also conducted for the monolingual data. We train a 4-gram language model using the SRILM toolkit [18]. In addition, several non-word language models are included to capture the dependencies between source and target words and reduce the impact of data sparsity. We use a bilingual language model as described in [7] as well as a cluster language model based on word classes generated by the MKCLS algorithm [19]. Short-range reordering is performed as a preprocessing step as d"
2014.iwslt-papers.10,2012.amta-papers.19,1,0.767356,"add several DWL components trained on different corpora as independent features in the log-linear framework utilized by our in-house phrase-based decoder. The system is trained on the EPPS, NC, Common Crawl, Giga corpora and TED talks[15]. The monolingual data we used to train language models includes the corresponding monolingual parts of those parallel corpora plus News Shuffle and Gigaword. The data is preprocessed and the phrase table is built using the scripts from the Moses package [16]. We adapt the general, big corpora to the in-domain TED data using the Backoff approach described in [17]. Adaptation is also conducted for the monolingual data. We train a 4-gram language model using the SRILM toolkit [18]. In addition, several non-word language models are included to capture the dependencies between source and target words and reduce the impact of data sparsity. We use a bilingual language model as described in [7] as well as a cluster language model based on word classes generated by the MKCLS algorithm [19]. Short-range reordering is performed as a preprocessing step as described in [20]. 4.2. Network configurations In our main neural network architecture we proposed, the siz"
2014.iwslt-papers.10,E99-1010,0,0.0823202,"he phrase table is built using the scripts from the Moses package [16]. We adapt the general, big corpora to the in-domain TED data using the Backoff approach described in [17]. Adaptation is also conducted for the monolingual data. We train a 4-gram language model using the SRILM toolkit [18]. In addition, several non-word language models are included to capture the dependencies between source and target words and reduce the impact of data sparsity. We use a bilingual language model as described in [7] as well as a cluster language model based on word classes generated by the MKCLS algorithm [19]. Short-range reordering is performed as a preprocessing step as described in [20]. 4.2. Network configurations In our main neural network architecture we proposed, the sizes of the hidden layers |H1 |, |H2 |, |H3 |are 1000, 500, 1000, respectively. If we use the original source and target vocabularies, for the English→French direction trained on preprocessed TED 2013 data, Vs includes 47957 words and Vt includes 62660 words. Because of the non-linearity calculations through such a large network, the training is extremely time-consuming. In order to boost the efficiency, we limit the source an"
2014.iwslt-papers.10,2007.tmi-papers.21,0,0.380598,"he general, big corpora to the in-domain TED data using the Backoff approach described in [17]. Adaptation is also conducted for the monolingual data. We train a 4-gram language model using the SRILM toolkit [18]. In addition, several non-word language models are included to capture the dependencies between source and target words and reduce the impact of data sparsity. We use a bilingual language model as described in [7] as well as a cluster language model based on word classes generated by the MKCLS algorithm [19]. Short-range reordering is performed as a preprocessing step as described in [20]. 4.2. Network configurations In our main neural network architecture we proposed, the sizes of the hidden layers |H1 |, |H2 |, |H3 |are 1000, 500, 1000, respectively. If we use the original source and target vocabularies, for the English→French direction trained on preprocessed TED 2013 data, Vs includes 47957 words and Vt includes 62660 words. Because of the non-linearity calculations through such a large network, the training is extremely time-consuming. In order to boost the efficiency, we limit the source and target vocabularies to the most frequent ones. All words outside the lists are t"
2014.iwslt-papers.4,P04-1005,0,0.0913588,"y removal. In order to explore the importance of domain in this task, we train disfluency removal models on in-domain and out-of-domain data and compare the results. Every experiment is conducted in two conditions whether turn information is available or not. Once the disfluencies of the meeting data are removed and punctuation marks are inserted, the data goes through our English to French MT system. For comparison, oracle experiments results and a baseline system are shown. 2. Related Work There has been extensive effort on disfluency removal on telephone speech, or Switchboard data [1]. In [2], Johnson et al. combined the noisy channel approach with a tree adjoining grammar for modeling speech disfluencies. In the noisy channel model, it is assumed that fluent text goes through a channel which adds disfluencies. Disfluency removal on 176 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 the same data is modeled using a conditional random fields (CRF) model in [3], using language and lexical model, and parser information as features. In [4], segmentation and disfluency removal issue in meeting data is handled in the"
2014.iwslt-papers.4,P09-1084,0,0.0173964,"lts and a baseline system are shown. 2. Related Work There has been extensive effort on disfluency removal on telephone speech, or Switchboard data [1]. In [2], Johnson et al. combined the noisy channel approach with a tree adjoining grammar for modeling speech disfluencies. In the noisy channel model, it is assumed that fluent text goes through a channel which adds disfluencies. Disfluency removal on 176 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 the same data is modeled using a conditional random fields (CRF) model in [3], using language and lexical model, and parser information as features. In [4], segmentation and disfluency removal issue in meeting data is handled in the scope of ASR. Baron et al. explored sentence boundary and disfluency detection in meetings using prosodic and lexical cues. For multi-party meeting data they used data collected as part of the ICSI Meeting Recording Project [5]. Sentence boundary detection is treated as a sequence classification problem, where each word boundary is labeled as either a sentence boundary, a disfluency interruption point, or a clean word transition. Therefore,"
2014.iwslt-papers.4,H01-1051,0,0.0196302,"Disfluency removal on 176 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 the same data is modeled using a conditional random fields (CRF) model in [3], using language and lexical model, and parser information as features. In [4], segmentation and disfluency removal issue in meeting data is handled in the scope of ASR. Baron et al. explored sentence boundary and disfluency detection in meetings using prosodic and lexical cues. For multi-party meeting data they used data collected as part of the ICSI Meeting Recording Project [5]. Sentence boundary detection is treated as a sequence classification problem, where each word boundary is labeled as either a sentence boundary, a disfluency interruption point, or a clean word transition. Therefore, disfluency is viewed from a different perspective, as an interruption point, where once it occurs a new segment boundary is added. Baron et al. find that combining prosodic and word-based classifier information yields the best results for the given task. While the previous works have focused on enhancing the performance of speech recognition, Peitz et al. [6] compared the transla"
2014.iwslt-papers.4,2011.iwslt-papers.7,0,0.16,"ng Recording Project [5]. Sentence boundary detection is treated as a sequence classification problem, where each word boundary is labeled as either a sentence boundary, a disfluency interruption point, or a clean word transition. Therefore, disfluency is viewed from a different perspective, as an interruption point, where once it occurs a new segment boundary is added. Baron et al. find that combining prosodic and word-based classifier information yields the best results for the given task. While the previous works have focused on enhancing the performance of speech recognition, Peitz et al. [6] compared the translation performance using three different methods to punctuate TED talks. They compare methods depending on when and how the punctuation marks are inserted: prediction in the source language, implicit prediction, and prediction in the target language. They assumed that the proper segments are already available, but punctuation marks are missing therefore should be inserted. Among the three systems, translating from unpunctuated to punctuated text achieves the largest improvements. Later this work is extended in [7] for MT of university lectures, where a monolingual translatio"
2014.iwslt-papers.4,2012.iwslt-papers.15,1,0.881532,"enhancing the performance of speech recognition, Peitz et al. [6] compared the translation performance using three different methods to punctuate TED talks. They compare methods depending on when and how the punctuation marks are inserted: prediction in the source language, implicit prediction, and prediction in the target language. They assumed that the proper segments are already available, but punctuation marks are missing therefore should be inserted. Among the three systems, translating from unpunctuated to punctuated text achieves the largest improvements. Later this work is extended in [7] for MT of university lectures, where a monolingual translation system is used for punctuation combined with sentence boundary detection. They prepare the training data by cutting it randomly, so that detection of sentence-like units is possible. Cho et al. [8] use a monolingual translation system together with CRF-based disfluency removal. Using a CRF model, the disfluency probability of each token is obtained and encoded into word lattices so that potentially disfluent paths can be skipped during decoding. MT of multi-party meetings was studied in [9], with a particular view towards analyzin"
2014.iwslt-papers.4,E14-4009,1,0.687316,"rce language, implicit prediction, and prediction in the target language. They assumed that the proper segments are already available, but punctuation marks are missing therefore should be inserted. Among the three systems, translating from unpunctuated to punctuated text achieves the largest improvements. Later this work is extended in [7] for MT of university lectures, where a monolingual translation system is used for punctuation combined with sentence boundary detection. They prepare the training data by cutting it randomly, so that detection of sentence-like units is possible. Cho et al. [8] use a monolingual translation system together with CRF-based disfluency removal. Using a CRF model, the disfluency probability of each token is obtained and encoded into word lattices so that potentially disfluent paths can be skipped during decoding. MT of multi-party meetings was studied in [9], with a particular view towards analyzing the importance of modeling contextual factors. They showed that word sense disambiguation using topic and domain knowledge yields a large improvement on MT performance. Recently Hassan et al. [10] investigated the impact of segmentation and disfluency removal"
2014.iwslt-papers.4,C10-1138,0,0.0193131,"ovements. Later this work is extended in [7] for MT of university lectures, where a monolingual translation system is used for punctuation combined with sentence boundary detection. They prepare the training data by cutting it randomly, so that detection of sentence-like units is possible. Cho et al. [8] use a monolingual translation system together with CRF-based disfluency removal. Using a CRF model, the disfluency probability of each token is obtained and encoded into word lattices so that potentially disfluent paths can be skipped during decoding. MT of multi-party meetings was studied in [9], with a particular view towards analyzing the importance of modeling contextual factors. They showed that word sense disambiguation using topic and domain knowledge yields a large improvement on MT performance. Recently Hassan et al. [10] investigated the impact of segmentation and disfluency removal on translation of telephone speech. They use a CRF model to detect sentence units and a knowledge-based parser for complex disfluency removal. There are several notable differences between our and previous work. Contrary to many works in disfluency removal and punctuation insertion, our work is e"
2014.iwslt-papers.4,E09-1030,0,0.0855328,"ng involves 5 to 12 different speakers. All meetings are held in English. As in real meeting scenarios, the meeting participants consist of native and non-native English speakers. The eight meeting sessions are transcribed and then disfluencies are manually annotated. We use five of the meetings for training the disfluency removal model and the remaining three for testing. The test data is translated into French in order to evaluate the translation performance. 3.1.1. Speech disfluencies Disfluencies in the meeting data are annotated manually by human annotators. Previous work on disfluencies [2, 11, 12] categorized the disfluencies into three groups: filler, (rough)copy, and non-copy. filler contains filler words as well as discourse markers. Therefore, this class includes words such as uh, you know, and well in some cases. As the class name suggests, (rough)copy includes an exact or rough repetition of words or phrases. In spontaneous speech, speakers may repeat what has been already spoken, as stutter or correction. For example, a sentence There is, there was an advantage has (rough)copy in the phrase there is. non-copy includes the cases where the speaker aborts previously spoken segments"
2014.iwslt-papers.4,2013.iwslt-papers.12,1,0.752208,"ng involves 5 to 12 different speakers. All meetings are held in English. As in real meeting scenarios, the meeting participants consist of native and non-native English speakers. The eight meeting sessions are transcribed and then disfluencies are manually annotated. We use five of the meetings for training the disfluency removal model and the remaining three for testing. The test data is translated into French in order to evaluate the translation performance. 3.1.1. Speech disfluencies Disfluencies in the meeting data are annotated manually by human annotators. Previous work on disfluencies [2, 11, 12] categorized the disfluencies into three groups: filler, (rough)copy, and non-copy. filler contains filler words as well as discourse markers. Therefore, this class includes words such as uh, you know, and well in some cases. As the class name suggests, (rough)copy includes an exact or rough repetition of words or phrases. In spontaneous speech, speakers may repeat what has been already spoken, as stutter or correction. For example, a sentence There is, there was an advantage has (rough)copy in the phrase there is. non-copy includes the cases where the speaker aborts previously spoken segments"
2014.iwslt-papers.4,P05-1056,0,0.0287865,", we start with a sequence of words as input and need to mark parts of the sequence as disfluencies. This problem can intuitively be modeled as a sequence labeling task, where each word is either labeled by one of the disfluency classes (filler, (rough)copy, non-copy, and interruption), or by a label representing clean speech. Since sequence labeling is a common problem in NLP, it has been studied intensively. One succesful approach to model these problems is using CRF. As CRFs can represent long-range dependencies in the observations, they have shown good performance in sentence segmentation [14], parts of speech (POS) tagging [15] and shallow parsing [16]. In this work we use the CRF model implemented in the GRMM package [17] to mark the speech disfluencies. The CRF model was trained using L-BFGS, with the default parameters of the toolkit. 4.1. In-domain vs. out-of-domain data In the ideal case, disfluency annotated in-domain data is available for training the CRF model. However, the annotation of speech for different domains can be very timeconsuming. As disfluency annotated lecture data [13] is available, we use this data as our out-of-domain training data for the CRF model. As in"
2014.iwslt-papers.4,N03-1028,0,0.0305603,"parts of the sequence as disfluencies. This problem can intuitively be modeled as a sequence labeling task, where each word is either labeled by one of the disfluency classes (filler, (rough)copy, non-copy, and interruption), or by a label representing clean speech. Since sequence labeling is a common problem in NLP, it has been studied intensively. One succesful approach to model these problems is using CRF. As CRFs can represent long-range dependencies in the observations, they have shown good performance in sentence segmentation [14], parts of speech (POS) tagging [15] and shallow parsing [16]. In this work we use the CRF model implemented in the GRMM package [17] to mark the speech disfluencies. The CRF model was trained using L-BFGS, with the default parameters of the toolkit. 4.1. In-domain vs. out-of-domain data In the ideal case, disfluency annotated in-domain data is available for training the CRF model. However, the annotation of speech for different domains can be very timeconsuming. As disfluency annotated lecture data [13] is available, we use this data as our out-of-domain training data for the CRF model. As in-domain training data we use the inhouse English meeting data"
2014.iwslt-papers.4,P07-2045,0,0.00278013,"ctuation insertion. The results of disfluency removal are analyzed. Finally, the overview of our system is given in the end. 6.1. System description The translation system is trained on 2.3 million sentences of English-French parallel data including the European Parliament data and the News Commentary corpus. The parallel TED data1 is used as in-domain data for the MT models. As development data, we use manual transcripts of TED data. Preprocessing which consists of text normalization and tokenization is applied before the training. In order to build the phrase table, we use the Moses package [19]. Using the SRILM Toolkit [20], a 4-gram language model is trained on 683 million words from the French side of the data. A bilingual language model [21] is used to extend source word context. The POS-based reordering model as described in [22] is applied to address different word orders between English and French. We use Minimum Error Rate Training (MERT) [23] for the optimization in the phrase-based decoder [24]. All scores of translation into French are reported in casesensitive BLEU scores [25] in this paper. When the sentence boundaries differ from the reference translation, we use the Le"
2014.iwslt-papers.4,W11-2124,1,0.887925,"translation system is trained on 2.3 million sentences of English-French parallel data including the European Parliament data and the News Commentary corpus. The parallel TED data1 is used as in-domain data for the MT models. As development data, we use manual transcripts of TED data. Preprocessing which consists of text normalization and tokenization is applied before the training. In order to build the phrase table, we use the Moses package [19]. Using the SRILM Toolkit [20], a 4-gram language model is trained on 683 million words from the French side of the data. A bilingual language model [21] is used to extend source word context. The POS-based reordering model as described in [22] is applied to address different word orders between English and French. We use Minimum Error Rate Training (MERT) [23] for the optimization in the phrase-based decoder [24]. All scores of translation into French are reported in casesensitive BLEU scores [25] in this paper. When the sentence boundaries differ from the reference translation, we use the Levenshtein minimum edit distance algorithm [26] to align hypothesis for evaluation. 6.2. Oracle experiments Table 3 shows the translation performance for"
2014.iwslt-papers.4,2007.tmi-papers.21,0,0.0240266,"uding the European Parliament data and the News Commentary corpus. The parallel TED data1 is used as in-domain data for the MT models. As development data, we use manual transcripts of TED data. Preprocessing which consists of text normalization and tokenization is applied before the training. In order to build the phrase table, we use the Moses package [19]. Using the SRILM Toolkit [20], a 4-gram language model is trained on 683 million words from the French side of the data. A bilingual language model [21] is used to extend source word context. The POS-based reordering model as described in [22] is applied to address different word orders between English and French. We use Minimum Error Rate Training (MERT) [23] for the optimization in the phrase-based decoder [24]. All scores of translation into French are reported in casesensitive BLEU scores [25] in this paper. When the sentence boundaries differ from the reference translation, we use the Levenshtein minimum edit distance algorithm [26] to align hypothesis for evaluation. 6.2. Oracle experiments Table 3 shows the translation performance for oracle punctuation marks and oracle disfluency removal on the multi-party meeting data. Tab"
2014.iwslt-papers.4,W05-0836,1,0.818865,"the MT models. As development data, we use manual transcripts of TED data. Preprocessing which consists of text normalization and tokenization is applied before the training. In order to build the phrase table, we use the Moses package [19]. Using the SRILM Toolkit [20], a 4-gram language model is trained on 683 million words from the French side of the data. A bilingual language model [21] is used to extend source word context. The POS-based reordering model as described in [22] is applied to address different word orders between English and French. We use Minimum Error Rate Training (MERT) [23] for the optimization in the phrase-based decoder [24]. All scores of translation into French are reported in casesensitive BLEU scores [25] in this paper. When the sentence boundaries differ from the reference translation, we use the Levenshtein minimum edit distance algorithm [26] to align hypothesis for evaluation. 6.2. Oracle experiments Table 3 shows the translation performance for oracle punctuation marks and oracle disfluency removal on the multi-party meeting data. Table 3: Oracle experiments System Baseline Oracle segmentation Oracle punctuation Oracle disfluency Oracle all No turns T"
2014.iwslt-papers.4,2005.iwslt-1.19,0,0.024144,"anguage model is trained on 683 million words from the French side of the data. A bilingual language model [21] is used to extend source word context. The POS-based reordering model as described in [22] is applied to address different word orders between English and French. We use Minimum Error Rate Training (MERT) [23] for the optimization in the phrase-based decoder [24]. All scores of translation into French are reported in casesensitive BLEU scores [25] in this paper. When the sentence boundaries differ from the reference translation, we use the Levenshtein minimum edit distance algorithm [26] to align hypothesis for evaluation. 6.2. Oracle experiments Table 3 shows the translation performance for oracle punctuation marks and oracle disfluency removal on the multi-party meeting data. Table 3: Oracle experiments System Baseline Oracle segmentation Oracle punctuation Oracle disfluency Oracle all No turns Turns 9.53 12.93 13.96 15.64 12.21 15.72 20.93 1 http://www.ted.com 180 Proceedings of the 11th International Workshop on Spoken Language Translation Lake Tahoe, December 4th and 5th, 2014 In the first system, all disfluencies are kept and baseline segmentations are used. As the base"
2014.iwslt-papers.7,J03-3002,0,0.490541,"Missing"
2014.iwslt-papers.7,N12-1079,0,0.354351,"Missing"
2014.iwslt-papers.7,P13-1018,0,0.175095,"Missing"
2014.iwslt-papers.7,W14-3356,0,0.27356,"Missing"
2014.iwslt-papers.7,P02-1040,0,0.0940357,"Missing"
2015.eamt-1.18,2012.eamt-1.60,0,0.0119108,"d used for translation. This allows us to retain our generalization won by using word clusters to estimate phrase probabilities, and still use all models trained on the surExperiments Since we expect stemming to have a larger impact in cases where training data is scarce, we evaluated the three presented strategies on two different scenarios: a low-resource condition and a state-ofthe-art large-scale system. In both scenarios we stemmed German adjectives and translated from German to English. In our low-resource condition, we trained an SMT system using only training data from the TED corpus (Cettolo et al., 2012). TED translations are currently available for 107 languages2 and are being continuously expanded. Therefore, there is a high chance that a small parallel corpus of translated TED talks will be available in the chosen language. In the second scenario, we used a large-scale state-of-the-art German→English translation system. This system was trained on significantly more data than available in the low-resource condition and incorporates several additional models. 5.1 System Description The low-resource system was trained only on the TED corpus provided by the IWSLT 2014 machine translation campa"
2015.eamt-1.18,P08-1115,0,0.0282724,"-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simplification for their French→English WMT system, including replacing inflected adjective forms with their lemma using hand-written rules, and their Russian→English (Weller et al., 2013b) system, removing superfluous attributes from the highly inflected Russian surface forms. Their systems are unable to outperform the baseline system trained on the surface forms. Weller et al. argue that human t"
2015.eamt-1.18,W08-0509,0,0.0321613,"Missing"
2015.eamt-1.18,W10-1710,0,0.0163473,"Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simplification for their French→English WMT system, including replacing inflected adjective forms with their lemma using hand-written rules, and their Russian→English (Wel"
2015.eamt-1.18,W11-2123,0,0.0404294,"m/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-final-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heafield, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced"
2015.eamt-1.18,W13-0805,1,0.859129,"performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heafield, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is"
2015.eamt-1.18,D07-1091,0,0.0158193,"reviously unseen morphological variants of a word, thus leading to a better generalization of our models. To fully maximize the potential of our SMT system, we looked at three different integration strategies. We evaluated hard decision stemming, where all adjectives are replaced by their stem, as well as soft integration strategies, where we consider the words and their stemmed form as translation alternatives. 2 Related Work The specific challenges arising from the translation of morphologically rich languages have been widely studied in the field of SMT. The factored 129 translation model (Koehn and Hoang, 2007) enriches phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen wor"
2015.eamt-1.18,E03-1076,0,0.0705642,"ting before tagging and stemming. We only stemmed words tagged as attributive adjectives, since only they are inflected in German. Predicative adjectives are not inflected and therefore were left untouched. Since we want to retain the degree of comparison, we used the finegrained tags of the RFTagger to decide when and how to stem. Adjectives tagged as comparative or superlative were stemmed through the use of fixed rules. For all others, we used the lemma output by the TreeTagger, since it is the same as the stem and was already available in our system. Finally, our usual compound splitting (Koehn and Knight, 2003) was trained and performed on the stemmed corpus. 4 Integration After clustering the words into groups that can be translated in the same or at least in a similar way, there are different possibilities to use them in the translation system. A naive strategy is to replace each word by its cluster representative, called hard decision stemming. However, this carries the risk of discarding vital information. Therefore we investigated techniques to integrate both, the surface forms as well as the word stems, into the translation system. In the combined input, we add the stemmed adjectives as transl"
2015.eamt-1.18,2005.iwslt-1.8,0,0.0179766,"additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is built on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT test 2012 as a development set and IWSLT test System Baseline Hard Decision Combined Input Hidden Combination Dev Test 28.91 29.01 29.13 29.25 30.25 30.30 30.47 30.62 Table 1: TED low-resource small systems results. 2011 as test data. For the large-scale system, we used IW"
2015.eamt-1.18,P07-2045,0,0.00406102,"s. For the monolingual training data we used the target side of all bilingual corpora as well as the News Shuffle and the Gigaword corpus. Before training and translation, the data is preprocessed as described in Section 3.2. The noisy Common Crawl corpus was filtered with an SVM classifier as described by Mediani et al. (2011). After preprocessing, the parallel corpora are wordaligned with the GIZA++ toolkit (Gao and Vo2 132 http://www.ted.com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-final-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heafield, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale sy"
2015.eamt-1.18,P11-1140,0,0.0183423,"utions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simplification for their French→English WMT system, including replacing inflected adjective forms with their lemma using hand-written rules, and t"
2015.eamt-1.18,2011.iwslt-evaluation.9,1,0.838535,"nolingual training data we used the target side of the TED corpus. The large-scale system was trained on the European Parliament Proceedings, News Commentary, TED and Common Crawl corpora provided for the IWSLT 2014 machine translation campaign (Cettolo et al., 2014), encompassing 4.69M lines. For the monolingual training data we used the target side of all bilingual corpora as well as the News Shuffle and the Gigaword corpus. Before training and translation, the data is preprocessed as described in Section 3.2. The noisy Common Crawl corpus was filtered with an SVM classifier as described by Mediani et al. (2011). After preprocessing, the parallel corpora are wordaligned with the GIZA++ toolkit (Gao and Vo2 132 http://www.ted.com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-final-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modifie"
2015.eamt-1.18,P10-2041,0,0.025409,"large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is built on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT test 2012 as a development set and IWSLT test System Baseline Hard Decision Combined Input Hidden Combination Dev Test 28.91 29.01 29.13 29.25 30.25 30.30 30.47 30.62 Table 1: TED"
2015.eamt-1.18,W09-0435,1,0.825095,"of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heafield, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond p"
2015.eamt-1.18,2011.iwslt-papers.6,1,0.862723,"phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an"
2015.eamt-1.18,2012.amta-papers.19,1,0.788742,", the data is preprocessed as described in Section 3.2. The noisy Common Crawl corpus was filtered with an SVM classifier as described by Mediani et al. (2011). After preprocessing, the parallel corpora are wordaligned with the GIZA++ toolkit (Gao and Vo2 132 http://www.ted.com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-final-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heafield, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting"
2015.eamt-1.18,W11-2124,1,0.932757,"dden combination strategy, stemming can easily be implemented into current state-of-the-art SMT systems without the need to change any of the advanced models beyond the phrase table. This makes our approach highly versatile and easy to implement for any number of system architectures and languages. 5 Figure 1: Workflow for unstemming the PT. 4.3 Hidden Combination While we are able to modify our phrase table to use both surface forms and stems in the last strategy, other models in our log-linear system suffer from the different types of source input. For example, the bilingual language model (Niehues et al., 2011) is based on tokens of target words and their aligned source words. In training, we can use either the stemmed corpus or the original one, but during decoding a mixture of stems and surface forms occurs. For the unknown word forms the scores will not be accurate and the performance of our model will suffer. Similar problems occur when using other translation models such as neural network based translation models. We therefore developed a novel strategy to integrate the word stems into the translation system. Instead of stemming the input to fit the stemmed phrase table, we modified the stemmed"
2015.eamt-1.18,E99-1010,0,0.150565,"le and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is built on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT te"
2015.eamt-1.18,2007.tmi-papers.21,0,0.0581703,"ining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heafield, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context du"
2015.eamt-1.18,C08-1098,0,0.0280715,"m that does not exist in proper German. However, were we to apply the same stemming to the comparative case, we would lose the degree of comparison and still generate a valid German sentence (sch¨on wird es nicht [won’t be pretty]) with a different meaning than our original sentence. In order to differentiate between cases in which stemming is desirable and where we would lose information, a detailed morphological analysis of the source text prior to stemming is vital. 3.2 Implementation We used readily available part-of-speech (POS) taggers, namely the TreeTagger (Schmid, 1994) and RFTagger (Schmid and Laws, 2008), for morphological analysis and stemming. In order to achieve accurate results, we performed standard machine translation preprocessing on our corpora before tagging. We discarded exceedingly long sentences and sentence pairs with a large length difference from the training data. Special dates, numbers and symbols were normalized and we smart-cased the first letter of every sentence. Typically preprocessing for German also includes splitting up compounds into their separate parts. However, this would confuse the POS taggers, which have been trained on German text with proper compounds. Furthe"
2015.eamt-1.18,P06-1122,0,0.0153461,"ere we consider the words and their stemmed form as translation alternatives. 2 Related Work The specific challenges arising from the translation of morphologically rich languages have been widely studied in the field of SMT. The factored 129 translation model (Koehn and Hoang, 2007) enriches phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate th"
2015.eamt-1.18,W05-0836,1,0.751291,"lt on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT test 2012 as a development set and IWSLT test System Baseline Hard Decision Combined Input Hidden Combination Dev Test 28.91 29.01 29.13 29.25 30.25 30.30 30.47 30.62 Table 1: TED low-resource small systems results. 2011 as test data. For the large-scale system, we used IWSLT test 2011 as development data and IWSLT test 2012 as test data. All results are reported as case-sensitive BLEU scores calculated with one reference tran"
2015.eamt-1.18,P13-1058,0,0.0173103,"n of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simplification for their French→English WMT system, including replacing inflected adjective forms with their lemma using hand-written rules, and their Russian→English (Weller et al., 2013b) system, removing superfluous attributes from the highly inflected Russian surface forms. Their systems are unable to outperform the baseline system trained on the surface forms. Weller et al. argue that human translators may prefer the morphologically reduced system due to better generalization ability. Their analysis showed the Russian system often produces an incorrect verb tense,"
2015.eamt-1.18,W12-3157,0,0.0169821,"rds. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simplification for their French→English WMT system, including replacing inflected adjective forms with their lemma using hand-written rules, and their Russian→English (Weller et al., 2013b) system, removing superfluous attributes from the highly inflected Russian surface forms. Their systems are unable to outperform the baseline system trained on the surface forms. Weller et al. argue that human translators may prefer the m"
2015.eamt-1.18,E06-1006,0,0.0340252,"ion of morphologically rich languages have been widely studied in the field of SMT. The factored 129 translation model (Koehn and Hoang, 2007) enriches phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inflected suffixes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al."
2015.eamt-1.18,W13-2213,0,\N,Missing
2020.eamt-1.53,D19-1081,1,0.842597,"erpreter would not be available for capacity reasons. The 43 languages are 24 EU official languages and 19 others, spoken between Morocco and Kazachstan. With our other user partner, alfatraining, we connect our system with an online meeting platform, alfaview®. 2.2 Other Research Topics The most visible application goal of live subtitling is supported by our advancements in the related areas. We research into document-level machine translation to enable conference participants to translate documents between all the 43 languages in high-quality, taking inter-sentential phenomena into account (Voita et al., 2019a; Voita et al., 2019b; Vojtˇechov´a et al., 2019; Popel et al., 2019; Rysov´a et al., 2019). We research into multilingual machine translation to reduce the cost of targeting many languages at once, and to leverage multiple language variants of the source for higher quality (Zhang et al., 2019; Zhang and Sennrich, 2019). To face challenges of simultaneous translation, such as robustness to noise, out-of-vocabulary words, domain adaptation, and non-standard accents (Mach´acˇ ek et al., 2019), latency and quality trade-off, we aim to improve automatic speech recognition. We also explore cascade"
2020.eamt-1.53,P19-1116,1,0.81532,"erpreter would not be available for capacity reasons. The 43 languages are 24 EU official languages and 19 others, spoken between Morocco and Kazachstan. With our other user partner, alfatraining, we connect our system with an online meeting platform, alfaview®. 2.2 Other Research Topics The most visible application goal of live subtitling is supported by our advancements in the related areas. We research into document-level machine translation to enable conference participants to translate documents between all the 43 languages in high-quality, taking inter-sentential phenomena into account (Voita et al., 2019a; Voita et al., 2019b; Vojtˇechov´a et al., 2019; Popel et al., 2019; Rysov´a et al., 2019). We research into multilingual machine translation to reduce the cost of targeting many languages at once, and to leverage multiple language variants of the source for higher quality (Zhang et al., 2019; Zhang and Sennrich, 2019). To face challenges of simultaneous translation, such as robustness to noise, out-of-vocabulary words, domain adaptation, and non-standard accents (Mach´acˇ ek et al., 2019), latency and quality trade-off, we aim to improve automatic speech recognition. We also explore cascade"
2020.eamt-1.53,W19-5355,1,0.883897,"Missing"
2020.eamt-1.53,D19-1083,1,0.761049,"st visible application goal of live subtitling is supported by our advancements in the related areas. We research into document-level machine translation to enable conference participants to translate documents between all the 43 languages in high-quality, taking inter-sentential phenomena into account (Voita et al., 2019a; Voita et al., 2019b; Vojtˇechov´a et al., 2019; Popel et al., 2019; Rysov´a et al., 2019). We research into multilingual machine translation to reduce the cost of targeting many languages at once, and to leverage multiple language variants of the source for higher quality (Zhang et al., 2019; Zhang and Sennrich, 2019). To face challenges of simultaneous translation, such as robustness to noise, out-of-vocabulary words, domain adaptation, and non-standard accents (Mach´acˇ ek et al., 2019), latency and quality trade-off, we aim to improve automatic speech recognition. We also explore cascaded and fully end-to-end neural spoken language translation (Pham et al., 2019; Nguyen et al., 2019; Nguyen et al., 2020) and co-organize shared tasks at WMT and IWSLT. 2.3 Automatic Minuting The last objective of our project is an automatic system for structured summaries of meetings. It is a ch"
2020.iwltp-1.7,2012.eamt-1.60,0,0.030274,"(Ha et al., 2016; Ha et al., 2017; Johnson et al., 2017) where a single system is able to translate from and to multiple languages. This approach has many advantages: • It leverages the large availability of multi-way, multilingual corpora in European languages such as the corpus of European Parliament documents and speeches’ transcription (Europarl) (Koehn, 2005), the collection of legislative texts of the European Union (JRCAcquis) (Steinberger et al., 2006) or the texts extracted from the document of European Constitution (EUconst) as well as the WIT3 corpus extracted from TED talks (TED) (Cettolo et al., 2012). • It uses the multilingual information to help improve the translation of the language pairs which are considered as low-resource languages in some domains. Our research has shown that our multilingual translation system maintains parity with the translation quality of systems trained on individual language pairs on the same small amount of data. • In practice, having a small number of multilingual systems to cover all language pairs significantly reduces the development and deployment efforts compared with having one system for each pair. 5. While each of the components (ASR, punctuation, M"
2020.iwltp-1.7,2015.iwslt-papers.8,1,0.871161,"Missing"
2020.iwltp-1.7,Q17-1024,0,0.0913848,"Missing"
2020.iwltp-1.7,2005.mtsummit-papers.11,0,0.143289,"f that sentence is displayed and never changed again by the update mechanism, to under 5 seconds. Machine Translation System With the ultimate goal of featuring a translation system for all EUROSAI languages, we opt for the multilingual approach (Ha et al., 2016; Ha et al., 2017; Johnson et al., 2017) where a single system is able to translate from and to multiple languages. This approach has many advantages: • It leverages the large availability of multi-way, multilingual corpora in European languages such as the corpus of European Parliament documents and speeches’ transcription (Europarl) (Koehn, 2005), the collection of legislative texts of the European Union (JRCAcquis) (Steinberger et al., 2006) or the texts extracted from the document of European Constitution (EUconst) as well as the WIT3 corpus extracted from TED talks (TED) (Cettolo et al., 2012). • It uses the multilingual information to help improve the translation of the language pairs which are considered as low-resource languages in some domains. Our research has shown that our multilingual translation system maintains parity with the translation quality of systems trained on individual language pairs on the same small amount of"
2020.iwltp-1.7,W19-5337,1,0.607584,"al conferences and remote conferencing) is described in the respective sections below. Our multilingual systems are based on the neural sequenceto-sequence with attention framework (Bahdanau et al., 2014) and shares the internal representation across languages (Pham et al., 2017). At present, we have one manyto-many Transformer model (Vaswani et al., 2017) providing translation between all pairings of 36 languages, along with several specialized models focused on subsets of languages, in particular the project’s primary languages of English, Czech, and German, see i.a. (Popel and Bojar, 2018; Popel et al., 2019). The resulting multilingual models after training can be used immediately in deployment or can go through a language adaptation step. This language adaptation is simply continuing training the multilingual model on the data of a specific language pair for a few epochs in order to improve the individual translation performance. While we need to do this language adaptation for every single language pair in our system, it is a trivial job since we could automate the process with the same settings and it takes only a little of time and computing resources to reach decent performances. 4.2. Practi"
2020.iwltp-1.7,steinberger-etal-2006-jrc,0,0.163597,"Missing"
2020.lrec-1.817,W14-2201,0,0.0628734,"Missing"
2020.lrec-1.817,N13-1066,0,0.0545409,"Missing"
2020.lrec-1.817,P06-1086,0,0.16197,"Missing"
2020.lrec-1.817,habash-etal-2012-conventional,0,0.205321,"s try to find conventions for audio transcription and collected text normalization. An important work performed was the Linguistic Data Consortium guidelines for transcribing Levantine and Iraqi (Maamouri et al., 2004). It suggests a strategy of the transcription for dialectal Arabic by using MSA-based orthographic conventions, since Arab transcribers use their MSA Knowledge for the transcription of Arabic dialects. This includes using both symbols and rules of MSA orthography e.g. writing without short vowels and diacritical marks except for nunation. Inspired by this work, CODA is invented (Habash et al., 2012), a conventional orthography for dialectal Arabic, which is intended to be for general writing purposes and abstracts from phonological variations in sub-dialects in contrast to the previous work. This Work covers Egyptian dialect EGY in details and extended by (Zribi et al., 2014) for Tunisian dialect, by (Saadane and Habash, 2015) for Algerian Arabic and by (Turki et al., 2016) for Maghrebi Arabic. For the data collection, an automatic conversion from spontaneous orthography of EGY to CODA is developed as a freely available tool called CODAFY (Eskander et al., 2013). Another challenge the da"
2020.lrec-1.817,W15-3208,0,0.045514,"Missing"
2020.lrec-1.817,zribi-etal-2014-conventional,0,0.0387481,"Missing"
2021.eacl-demos.32,P19-1126,0,0.361622,"Missing"
2021.eacl-demos.32,D14-1140,0,0.362092,"Missing"
2021.eacl-demos.32,2020.iwslt-1.27,0,0.152889,"among the research partners in our project, e.g. Hindi. The scientific motivation for our efforts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quali"
2021.eacl-demos.32,E17-1099,0,0.269847,"rts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quality of the output translations using BLEU and WER metrics. Zheng et al. (2019) propose"
2021.eacl-demos.32,N12-1048,0,0.134304,"m the secured networks of the labs so it usually does not run into firewall issues. tions of the EU and nearby countries. Experimentally, we include also other languages based on available systems among the research partners in our project, e.g. Hindi. The scientific motivation for our efforts is to find an approach that allows to assemble laboratory system components to a practically usable product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speec"
2021.eacl-demos.32,2020.eamt-1.53,1,0.493793,"Missing"
2021.eacl-demos.32,C18-2020,1,0.876378,"Missing"
2021.eacl-demos.32,2020.iwslt-1.25,1,0.823962,"Missing"
2021.eacl-demos.32,N16-3017,1,0.800534,"Missing"
2021.eacl-demos.32,2020.acl-main.148,1,0.8252,"tem in end-to-end fashion and face engineering problems and technical issues on all layers from sound acquisition through network connections, worker configuration to subtitle presentation. • We are currently running a user study with nonGerman speakers watching German videos with our online subtitles, see Section 7.1. We aim to measure the comprehension loss caused by different subtitling options, latency or flicker. 42 languages (Johnson et al., 2017). The models are mostly Transformers (Vaswani et al., 2017) but we improve their performance in massively multilingual setting by extra depth (Zhang et al., 2020). 5.3 Interplay of ASR and MT Connecting ASR and MT systems is not straightforward because MT systems assume input in the form of complete sentences. We follow the strategy of Niehues et al. (2016), first inserting punctuation into the stream of tokens coming from ASR (Tilk and Alum¨ae, 2016), breaking it up at full stops and sending individual sentences to MT, either as unfinished sentence prefixes, or complete sentences. We are using re-translation, as ASR or punctuation updates are received. Currently, the main problem is that punctuation prediction does not have access to the sound any mor"
2021.eacl-demos.32,D19-1137,0,0.29802,"product and to document the problems on this journey. 3 Related Systems Live spoken language translation has been continuously studied for decades, see e.g. Osterholtz et al. (1992); F¨ugen et al. (2008); Bangalore et al. (2012). Recent systems differ in whether they provide revisions to their previous output (M¨uller et al., 2016; Niehues et al., 2016; Dessloch et al., 2018; Niehues et al., 2018; Arivazhagan et al., 2020), or whether they only append output tokens (Grissom II et al., 2014; Gu et al., 2017; Arivazhagan et al., 2019; Press and Smith, 2018; Xiong et al., 2019; Ma et al., 2019; Zheng et al., 2019). M¨uller et al. (2016) were probably the first to allow output revision when they find a better translation. Zenkel et al. (2018) released a simpler setup as an open-source toolkit consisting of a neural speech recognition system, a sentence segmentation system, and an attention-based translation system providing also some pre-trained models for their tasks. (Zenkel et al., 2018) evaluated only the quality of the output translations using BLEU and WER metrics. Zheng et al. (2019) proposed a new approach with a delay-based heuristic. The model decides to read more input (or wait for it) or wri"
2021.eacl-demos.32,2020.findings-emnlp.349,0,0.0269839,"text available, the user does not see sufficient number of words to let the brain “make up” or reconstruct the original meaning from pieces. The short-term memory of recently processed text does not seem to be sufficient for this type recovery, while seeing the words in larger context gives the user a better chance. The last step in an SLT system is the delivery of the translated content to the user. Our goal stops at the textual representation, i.e. we do not include speech synthesis and delivery of the sound, which would bring yet another set of design decisions and open problems, see e.g. Zheng et al. (2020). We experiment with two different views for our text output, both implemented as web applications. The “subtitle view” is optimized toward minimal use of screen space. Only two lines of text are available which leaves room either for e.g. a streamed video of the session or the slides, or for many languages displayed at once, if the screen is intended for a multi-lingual audience. The “paragraph view” provides more textual context to the user. 7.1 Subtitle View The subtitle view offers a simple interface with a HLS stream of the video or slides and one or more subtitles streams. Section 7.1 pr"
2021.eacl-demos.32,W19-5337,1,0.807801,"latency and hypotheses updates, as in KIT Lecture Translator (M¨uller et al., 2016). We use the hybrid ASR models based on Janus from KIT Lecture Translator, for German and English, as well as recent neural sequence-to-sequence ASR models trained on the same data (Nguyen et al., 2020). For Czech ASR, we use a Kaldi hybrid model trained on a Corpus of Czech Parliament Plenary Hearings (Kratochv´ıl et al., 2019). Czech sequence-to-sequence ASR is a work in progress. 5.2 MT Systems in ELITR We use bilingual NMT models for some high resource and well-studied language pairs e.g. for English-Czech (Popel et al., 2019; Wetesko et al., 2019). For other targets, we use multi-target models, e.g. an English-centric universal model for ELITR Flexible Architecture We always strive for the best performance for each considered language pair. With the perpetual com272 Index Name auto-iwslt2020-antrecorp(ASR) auto-iwslt2020-antrecorp(MT) auto-iwslt2020-antrecorp(MT) auto-asr-english-auditing(ASR) auto-asr-english-auditing(MT) auto-asr-english-auditing(MT) auto-iwslt2020-khanacademy(ASR) Worker en-EU-lecture KIT-s2s rb-EU fromEN-en to 41 rb-EU fromEN-en to 41 en-EU-lecture KIT-s2s rb-EU fromEN-en to 41 rb-EU fromEN-e"
2021.mtsummit-loresmt.4,E17-1088,0,0.0236693,"Koehn, 2009). On the other hand, machine translation benefits from professional human translators’ context-relevant and culturally-appropriate translation and post-editing efforts (Hutchins, 2001). Severely low resource translation is a fitting ground for close human machine collaboration (Zong, 2018; Carl et al., 2011; Martínez, 2003). 2.2 Severely Low Resource Text-based Translation Many use multiple rich-resource languages to translate to a low resource language using multilingual methods (Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Zoph and Knight, 2016; Zoph et al., 2016; Adams et al., 2017; Gillick et al., 2016; Zhou et al., 2018a,b). Some use data selection for active learning (Eck et al., 2005). Some use as few as ∼4,000 lines (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et al., 2012; Miura et al., 2016; Gangadharaiah et al., 2009). Random sampling and data selection has b"
2021.mtsummit-loresmt.4,2020.coling-main.313,0,0.0434353,"Missing"
2021.mtsummit-loresmt.4,2005.iwslt-1.7,1,0.605421,"levant and culturally-appropriate translation and post-editing efforts (Hutchins, 2001). Severely low resource translation is a fitting ground for close human machine collaboration (Zong, 2018; Carl et al., 2011; Martínez, 2003). 2.2 Severely Low Resource Text-based Translation Many use multiple rich-resource languages to translate to a low resource language using multilingual methods (Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Zoph and Knight, 2016; Zoph et al., 2016; Adams et al., 2017; Gillick et al., 2016; Zhou et al., 2018a,b). Some use data selection for active learning (Eck et al., 2005). Some use as few as ∼4,000 lines (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et al., 2012; Miura et al., 2016; Gangadharaiah et al., 2009). Random sampling and data selection has been successful (Kendall and Smith, 1938; Knuth, 1991; Clarkson and Shor, 1989; Sennrich et al., 2015; Hoang e"
2021.mtsummit-loresmt.4,N16-1101,0,0.0323257,"; Savoldi et al., 2021; Bowker, 2002; Bowker and Fisher, 2010; Koehn, 2009). On the other hand, machine translation benefits from professional human translators’ context-relevant and culturally-appropriate translation and post-editing efforts (Hutchins, 2001). Severely low resource translation is a fitting ground for close human machine collaboration (Zong, 2018; Carl et al., 2011; Martínez, 2003). 2.2 Severely Low Resource Text-based Translation Many use multiple rich-resource languages to translate to a low resource language using multilingual methods (Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Zoph and Knight, 2016; Zoph et al., 2016; Adams et al., 2017; Gillick et al., 2016; Zhou et al., 2018a,b). Some use data selection for active learning (Eck et al., 2005). Some use as few as ∼4,000 lines (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et al., 2012; Miura et al., 2016; Gangadh"
2021.mtsummit-loresmt.4,W09-4633,0,0.0468387,"., 2016; Zoph and Knight, 2016; Zoph et al., 2016; Adams et al., 2017; Gillick et al., 2016; Zhou et al., 2018a,b). Some use data selection for active learning (Eck et al., 2005). Some use as few as ∼4,000 lines (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et al., 2012; Miura et al., 2016; Gangadharaiah et al., 2009). Random sampling and data selection has been successful (Kendall and Smith, 1938; Knuth, 1991; Clarkson and Shor, 1989; Sennrich et al., 2015; Hoang et al., 2018; He et al., 2016; Gu et al., 2018). The mathematician Donald Knuth uses the population of Menlo Park to illustrate the value of random sampling (Knuth, 1991). 3 Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 34 Book Author Books Chapters Pages Languages The Bible The Little Prince Dao De Jing COVID-19 Wiki Page The Alche"
2021.mtsummit-loresmt.4,E12-1025,0,0.0326967,"ohnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Zoph and Knight, 2016; Zoph et al., 2016; Adams et al., 2017; Gillick et al., 2016; Zhou et al., 2018a,b). Some use data selection for active learning (Eck et al., 2005). Some use as few as ∼4,000 lines (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et al., 2012; Miura et al., 2016; Gangadharaiah et al., 2009). Random sampling and data selection has been successful (Kendall and Smith, 1938; Knuth, 1991; Clarkson and Shor, 1989; Sennrich et al., 2015; Hoang et al., 2018; He et al., 2016; Gu et al., 2018). The mathematician Donald Knuth uses the population of Menlo Park to illustrate the value of random sampling (Knuth, 1991). 3 Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 34 Book Author Books Chapters Pages Languages The Bible The Littl"
2021.mtsummit-loresmt.4,D18-1398,0,0.0180306,"nes (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et al., 2012; Miura et al., 2016; Gangadharaiah et al., 2009). Random sampling and data selection has been successful (Kendall and Smith, 1938; Knuth, 1991; Clarkson and Shor, 1989; Sennrich et al., 2015; Hoang et al., 2018; He et al., 2016; Gu et al., 2018). The mathematician Donald Knuth uses the population of Menlo Park to illustrate the value of random sampling (Knuth, 1991). 3 Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 34 Book Author Books Chapters Pages Languages The Bible The Little Prince Dao De Jing COVID-19 Wiki Page The Alchemist Harry Potter The Lord of the Rings Frozen Movie Script The Hand Washing Song Dream of the Red Chamber Les Misérables Multiple Antoine de Saint Exupéry Laozi Multiple Paulo Coelho J. K. Rowling"
2021.mtsummit-loresmt.4,P09-1021,0,0.0434607,"ng multilingual methods (Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Zoph and Knight, 2016; Zoph et al., 2016; Adams et al., 2017; Gillick et al., 2016; Zhou et al., 2018a,b). Some use data selection for active learning (Eck et al., 2005). Some use as few as ∼4,000 lines (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et al., 2012; Miura et al., 2016; Gangadharaiah et al., 2009). Random sampling and data selection has been successful (Kendall and Smith, 1938; Knuth, 1991; Clarkson and Shor, 1989; Sennrich et al., 2015; Hoang et al., 2018; He et al., 2016; Gu et al., 2018). The mathematician Donald Knuth uses the population of Menlo Park to illustrate the value of random sampling (Knuth, 1991). 3 Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 34 Book Author Books Chapters Pages"
2021.mtsummit-loresmt.4,W18-2703,0,0.0279092,", 2005). Some use as few as ∼4,000 lines (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et al., 2012; Miura et al., 2016; Gangadharaiah et al., 2009). Random sampling and data selection has been successful (Kendall and Smith, 1938; Knuth, 1991; Clarkson and Shor, 1989; Sennrich et al., 2015; Hoang et al., 2018; He et al., 2016; Gu et al., 2018). The mathematician Donald Knuth uses the population of Menlo Park to illustrate the value of random sampling (Knuth, 1991). 3 Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 34 Book Author Books Chapters Pages Languages The Bible The Little Prince Dao De Jing COVID-19 Wiki Page The Alchemist Harry Potter The Lord of the Rings Frozen Movie Script The Hand Washing Song Dream of the Red Chamber Les Misérables Multiple Antoine de Saint Exupéry Laozi"
2021.mtsummit-loresmt.4,Q17-1024,0,0.0208006,"Koehn and Haddow, 2009; Li et al., 2014; Savoldi et al., 2021; Bowker, 2002; Bowker and Fisher, 2010; Koehn, 2009). On the other hand, machine translation benefits from professional human translators’ context-relevant and culturally-appropriate translation and post-editing efforts (Hutchins, 2001). Severely low resource translation is a fitting ground for close human machine collaboration (Zong, 2018; Carl et al., 2011; Martínez, 2003). 2.2 Severely Low Resource Text-based Translation Many use multiple rich-resource languages to translate to a low resource language using multilingual methods (Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Zoph and Knight, 2016; Zoph et al., 2016; Adams et al., 2017; Gillick et al., 2016; Zhou et al., 2018a,b). Some use data selection for active learning (Eck et al., 2005). Some use as few as ∼4,000 lines (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et a"
2021.mtsummit-loresmt.4,P17-4012,0,0.028763,"Missing"
2021.mtsummit-loresmt.4,2009.mtsummit-papers.8,0,0.0813961,"Missing"
2021.mtsummit-loresmt.4,2020.emnlp-main.210,0,0.202744,"tion to improve performance through active learning (Settles, 2012; Carl et al., 2011; Denkowski, 2015). We propose a workflow to bring human translation and machine translation to work together seamlessly in translation of a closed text into a severely low resource language as shown in Figure 1 and Algorithm 1. Given a closed text that has many existing translations in different languages, we are interested in translating it into a severely low resource language well. Researchers recently have shown achievements in translation using very small seed parallel corpora in low resource languages (Lin et al., 2020; Qi et al., 2018; Zhou et al., 2018a). Construction methods of such seed corpora are therefore pivotal in translation performance. Historically, this is mostly determined by field linguists’ experiential and intuitive discretion. Many human translators employ a portion-based strategy when translating large texts. For example, translation of the book “The Little Prince” may be divided into smaller tasks of translating 27 chapters, or even smaller translation units like a few consecutive pages. Each translation unit contains consecutive Proceedings of the 18th Biennial Machine Translation Summi"
2021.mtsummit-loresmt.4,mayer-cysouw-2014-creating,0,0.0551049,"Missing"
2021.mtsummit-loresmt.4,N16-1003,0,0.0191822,"., 2016; Firat et al., 2016; Zoph and Knight, 2016; Zoph et al., 2016; Adams et al., 2017; Gillick et al., 2016; Zhou et al., 2018a,b). Some use data selection for active learning (Eck et al., 2005). Some use as few as ∼4,000 lines (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et al., 2012; Miura et al., 2016; Gangadharaiah et al., 2009). Random sampling and data selection has been successful (Kendall and Smith, 1938; Knuth, 1991; Clarkson and Shor, 1989; Sennrich et al., 2015; Hoang et al., 2018; He et al., 2016; Gu et al., 2018). The mathematician Donald Knuth uses the population of Menlo Park to illustrate the value of random sampling (Knuth, 1991). 3 Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 34 Book Author Books Chapters Pages Languages The Bible The Little Prince Dao De Jing"
2021.mtsummit-loresmt.4,D18-1103,0,0.0186236,"boration (Zong, 2018; Carl et al., 2011; Martínez, 2003). 2.2 Severely Low Resource Text-based Translation Many use multiple rich-resource languages to translate to a low resource language using multilingual methods (Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Zoph and Knight, 2016; Zoph et al., 2016; Adams et al., 2017; Gillick et al., 2016; Zhou et al., 2018a,b). Some use data selection for active learning (Eck et al., 2005). Some use as few as ∼4,000 lines (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et al., 2012; Miura et al., 2016; Gangadharaiah et al., 2009). Random sampling and data selection has been successful (Kendall and Smith, 1938; Knuth, 1991; Clarkson and Shor, 1989; Sennrich et al., 2015; Hoang et al., 2018; He et al., 2016; Gu et al., 2018). The mathematician Donald Knuth uses the population of Menlo Park to illustrate the value of random sampling (Knuth, 1991). 3 P"
2021.mtsummit-loresmt.4,P02-1040,0,0.110866,"Missing"
2021.mtsummit-loresmt.4,N18-2084,0,0.0389721,"rformance through active learning (Settles, 2012; Carl et al., 2011; Denkowski, 2015). We propose a workflow to bring human translation and machine translation to work together seamlessly in translation of a closed text into a severely low resource language as shown in Figure 1 and Algorithm 1. Given a closed text that has many existing translations in different languages, we are interested in translating it into a severely low resource language well. Researchers recently have shown achievements in translation using very small seed parallel corpora in low resource languages (Lin et al., 2020; Qi et al., 2018; Zhou et al., 2018a). Construction methods of such seed corpora are therefore pivotal in translation performance. Historically, this is mostly determined by field linguists’ experiential and intuitive discretion. Many human translators employ a portion-based strategy when translating large texts. For example, translation of the book “The Little Prince” may be divided into smaller tasks of translating 27 chapters, or even smaller translation units like a few consecutive pages. Each translation unit contains consecutive Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, A"
2021.mtsummit-loresmt.4,1983.tc-1.13,0,0.704571,"Missing"
2021.mtsummit-loresmt.4,W18-6324,1,0.926176,"active learning (Settles, 2012; Carl et al., 2011; Denkowski, 2015). We propose a workflow to bring human translation and machine translation to work together seamlessly in translation of a closed text into a severely low resource language as shown in Figure 1 and Algorithm 1. Given a closed text that has many existing translations in different languages, we are interested in translating it into a severely low resource language well. Researchers recently have shown achievements in translation using very small seed parallel corpora in low resource languages (Lin et al., 2020; Qi et al., 2018; Zhou et al., 2018a). Construction methods of such seed corpora are therefore pivotal in translation performance. Historically, this is mostly determined by field linguists’ experiential and intuitive discretion. Many human translators employ a portion-based strategy when translating large texts. For example, translation of the book “The Little Prince” may be divided into smaller tasks of translating 27 chapters, or even smaller translation units like a few consecutive pages. Each translation unit contains consecutive Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021"
2021.mtsummit-loresmt.4,2021.sigtyp-1.7,1,0.785046,"ource translation is a fitting ground for close human machine collaboration (Zong, 2018; Carl et al., 2011; Martínez, 2003). 2.2 Severely Low Resource Text-based Translation Many use multiple rich-resource languages to translate to a low resource language using multilingual methods (Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Zoph and Knight, 2016; Zoph et al., 2016; Adams et al., 2017; Gillick et al., 2016; Zhou et al., 2018a,b). Some use data selection for active learning (Eck et al., 2005). Some use as few as ∼4,000 lines (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et al., 2012; Miura et al., 2016; Gangadharaiah et al., 2009). Random sampling and data selection has been successful (Kendall and Smith, 1938; Knuth, 1991; Clarkson and Shor, 1989; Sennrich et al., 2015; Hoang et al., 2018; He et al., 2016; Gu et al., 2018). The mathematician Donald Knuth uses the population of Menlo Pa"
2021.mtsummit-loresmt.4,N16-1004,0,0.0274271,"21; Bowker, 2002; Bowker and Fisher, 2010; Koehn, 2009). On the other hand, machine translation benefits from professional human translators’ context-relevant and culturally-appropriate translation and post-editing efforts (Hutchins, 2001). Severely low resource translation is a fitting ground for close human machine collaboration (Zong, 2018; Carl et al., 2011; Martínez, 2003). 2.2 Severely Low Resource Text-based Translation Many use multiple rich-resource languages to translate to a low resource language using multilingual methods (Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Zoph and Knight, 2016; Zoph et al., 2016; Adams et al., 2017; Gillick et al., 2016; Zhou et al., 2018a,b). Some use data selection for active learning (Eck et al., 2005). Some use as few as ∼4,000 lines (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et al., 2012; Miura et al., 2016; Gangadharaiah et al., 2009). R"
2021.mtsummit-loresmt.4,D16-1163,0,0.0271233,"r and Fisher, 2010; Koehn, 2009). On the other hand, machine translation benefits from professional human translators’ context-relevant and culturally-appropriate translation and post-editing efforts (Hutchins, 2001). Severely low resource translation is a fitting ground for close human machine collaboration (Zong, 2018; Carl et al., 2011; Martínez, 2003). 2.2 Severely Low Resource Text-based Translation Many use multiple rich-resource languages to translate to a low resource language using multilingual methods (Johnson et al., 2017; Ha et al., 2016; Firat et al., 2016; Zoph and Knight, 2016; Zoph et al., 2016; Adams et al., 2017; Gillick et al., 2016; Zhou et al., 2018a,b). Some use data selection for active learning (Eck et al., 2005). Some use as few as ∼4,000 lines (Lin et al., 2020; Qi et al., 2018) and ∼1,000 lines (Zhou and Waibel, 2021) of data. Some do not use low resource data (Neubig and Hu, 2018; Karakanta et al., 2018). 2.3 Active Learning and Random Sampling Active learning has long been used in machine translation (Settles, 2012; Ambati, 2012; Eck et al., 2005; Haffari and Sarkar, 2009; González-Rubio et al., 2012; Miura et al., 2016; Gangadharaiah et al., 2009). Random sampling and"
A00-2025,C00-2140,1,0.763987,"Missing"
C00-2140,W98-1421,0,0.0800889,". We believe that summarization of speech will become increasingly more important, as the amount of online audio data grows and demand for rapid browsing, skimming, and access of speech data increases. Another application which particularly pertains to our interest in spoken dialogue summarization would be the generation of meeting minutes for archival purposes and/or to update participants joining at later stages on the progress of the conversation so far. Summarization of dialogues within limited domains has been attempted within the context of the Verbmobil project (protocol generation&quot;, (Alexandersson and Poller, 1998)) or by SRI's MIMI summarizer (Kameyama et al., 1996). Recent work on spoken language summarization in unrestricted domains has focused almost exclusively on Broadcast News, mostly due to the spoken language track of recent TREC evaluations (Garofolo et al., 1997; Garofolo et al., 1999). (Waibel et al., 1998) describe a Meeting Browser where summaries can be generated using technology established for written texts. (Valenza et al., 1999) go one step further and incorporate knowledge from the speech recognizer (con dence scores) into their summarization system, as well. We argue that the nature"
C00-2140,W97-0703,0,0.0349591,"er out likely candidates for incomplete clauses due to speech repair or interruption by the other speaker. 6 Topic Segmentation Since Callhome dialogues are always multi-topical, segmenting them into topical units is an important step in our summarization system. This allows us to provide signature&quot; information (frequent content words) about every topic to the user as a help for faster browsing and accessing the data. Furthermore, the subsequent information condensation component can work on smaller parts of the dialogue and thus operate more eciently. Following (Boguraev and Kennedy, 1997; Barzilay and Elhadad, 1997) who use TextTiling (Hearst, 1997) for their summarization systems of written text, we adapted this algorithm (its block comparison version) for speech data: we choose turns to be minimal units and compute block similarity between blocks of k turns every d turns. We use 9 English and 15 Spanish Callhome dialogues, manually annotated for topic boundaries, to determine the optimum values for a set of TextTiling parameters and at the same time to evaluate the accuracy of this algorithm. To do this, we ran an n-fold cross-validation (jack-kni ng&quot;) where all dialogues but one are used to determine"
C00-2140,A97-1003,1,0.610422,"Missing"
C00-2140,J97-1003,0,0.17061,"due to speech repair or interruption by the other speaker. 6 Topic Segmentation Since Callhome dialogues are always multi-topical, segmenting them into topical units is an important step in our summarization system. This allows us to provide signature&quot; information (frequent content words) about every topic to the user as a help for faster browsing and accessing the data. Furthermore, the subsequent information condensation component can work on smaller parts of the dialogue and thus operate more eciently. Following (Boguraev and Kennedy, 1997; Barzilay and Elhadad, 1997) who use TextTiling (Hearst, 1997) for their summarization systems of written text, we adapted this algorithm (its block comparison version) for speech data: we choose turns to be minimal units and compute block similarity between blocks of k turns every d turns. We use 9 English and 15 Spanish Callhome dialogues, manually annotated for topic boundaries, to determine the optimum values for a set of TextTiling parameters and at the same time to evaluate the accuracy of this algorithm. To do this, we ran an n-fold cross-validation (jack-kni ng&quot;) where all dialogues but one are used to determine the best parameters (	rain set&quot;)"
C00-2140,A00-1043,0,0.0168147,"results from these experiments. Similar to other experiments in the summarization literature (Mani et al., 1998), we nd a wide performance variation across di erent texts. 8 Telegraphic Reduction The purpose of this component is to maximize information in a xed amount of space. We shorten the output of the summarizer to a 	elegraphic style&quot;; that way, more information can be included in a summary of k words (or n bytes). Since we only use shallow methods for textual analysis that do not generate a dependency structure, we cannot use complex methods for text reduction as described, e.g., in (Jing, 2000). Our method simply excludes words occurring in the stop list from the summary, except for some highly informative words such as I&quot; or 
ot&quot;. 9 User Interface and System Performance Since we want to enable interactive summarization which allows a user to browse through a dialogue quickly to search for information he is interested in, we have integrated our summarization system into a JAVA-based graphical user interface (Meeting Browser&quot;) (Bett et al., 2000). This interface also integrates the output of a speech recognizer (Yu et al., 1999), and can display a wide variety of information about"
C00-2140,P98-2237,1,0.794011,"rformance proved to be comparable with the results in the cited papers (F1 &gt; 0:85, error < 0:05).2 For several of the clean-up lter's components, we make use of Brill's POS tagger (Brill, 1994). For English, we use a modi ed version of Brill's original tag set, and the tagger was adapted and retrained for spoken language corpora (Callhome and Switchboard) (Zechner, 1997). For Spanish, we created our own tag set, derived from the LDC lexicon and from the CRATER project (Leon, 1994), and trained the tagger on manually annotated Callhome dialogues. Furthermore, a POS based shallow chunk parser (Zechner and Waibel, 1998) is used to lter out likely candidates for incomplete clauses due to speech repair or interruption by the other speaker. 6 Topic Segmentation Since Callhome dialogues are always multi-topical, segmenting them into topical units is an important step in our summarization system. This allows us to provide signature&quot; information (frequent content words) about every topic to the user as a help for faster browsing and accessing the data. Furthermore, the subsequent information condensation component can work on smaller parts of the dialogue and thus operate more eciently. Following (Boguraev and K"
C00-2140,A00-2025,1,0.777917,"Missing"
C00-2140,E99-1011,0,\N,Missing
C00-2140,C98-2232,1,\N,Missing
C04-1114,P00-1004,1,0.893904,"Missing"
C04-1114,C96-2141,1,0.6513,"medical terms but the language is not very complex. Figure 3 shows some example test sentences (from the reference data). (…) The symptoms you are describing and given your recent change in diet, I believe you may be anemic. Patient: Anemic? Really? Is that serious? Doctor: Anemia can be very serious if left untreated. Being anemic means your body lacks a sufficient amount of red blood cells to carry oxygen through your body. (…) Figure 3: Example test sentences (reference) Doctor: The Baseline system uses IBM1 lexicon transducers and different types of phrase transducers (Zhang et al. 2003, Vogel et al. 1996, Vogel et al. 2003). The Language model is a trigram language model with Good-TuringSmoothing built with the SRI-Toolkit (SRI, 19952004) using only the English part of the training data. The Baseline system scores a 0.171 BLEU and 4.72 NIST. [BLEU and NIST are well known scoring methods for measuring machine translation quality. Both calculate the precision of a translation by comparing it to a reference translation and incorporating a length penalty (Doddington, 2001; Papineni et al., 2002).] 3.2 Extracting dictionaries from the UMLS The first way to exploit the UMLS database for a statistic"
C04-1114,P02-1040,0,\N,Missing
C16-1172,D16-1162,0,0.0249066,"s of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al., 1993), phrase-based machine translation (Koehn et al., 2003; Och and Ney, 2004) segments the sentence into continuous phrases that are used as basic translation units. This allows for many-to-many alignments. Based on this segmentation, the probability of the translation is calculated using a log-linear combination of different features: P I I exp( N n=1 λn hn (e , f )) PN 0I I e0I exp( n=1 λn hn (e , f )) P (eI , f I ) = P (1) In the initial model, the features are ba"
C16-1172,J93-2003,0,0.0601026,"al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al., 1993), phrase-based machine translation (Koehn et al., 2003; Och and Ney, 2004) segments the sentence into continuous phrases that are used as basic translation units. This allows for many-to-many alignments. Based on this segmentation, the probability of the translation is calculated using a log-linear combination of different features: P I I exp( N n=1 λn hn (e , f )) PN 0I I e0I exp( n=1 λn hn (e , f )) P (eI , f I ) = P (1) In the initial model, the features are based on language and translation model probabilities as well as a few count based features. In advanced PBMT systems, several additio"
C16-1172,W07-0732,0,0.0330267,"er is structured as follows: In the next section we will review the related work. In Section 3, we will briefly review the phrase-based and neural approach to machine translation. Section 4 will introduce the approach presented in this paper to pre-translate the input using a PBMT system. In the following section, we will evaluate the approach and analyze the errors. Finally, we will finish with a conclusion. 2 Related Work The idea of linear combining of machine translation systems using different paradigms has already been used successfully for SMT and rule-based machine translation (RBMT) (Dugast et al., 2007; Simard et al., 2007). They build an SMT system that is post-editing the output of an RBMT system. Using the combination of SMT and RBMT, they could outperform both single systems. Those experiments promote the area of automatic post-editing (Bojar et al., 2015). Recently, it was shown that models based on neural MT are very successful in this task (Junczys-Dowmunt and Grundkiewicz, 2016). For PBMT, there has been several attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound s"
C16-1172,W16-2314,1,0.88889,"Missing"
C16-1172,W13-0805,1,0.843516,"nalyze the influence of the quality of the PBMT system, we use two different systems, a baseline system and a system with advanced models. The systems were trained on all parallel data available for the WMT 20161 . The news commentary corpus, the European parliament proceedings and the common crawl corpus sum up to 3.7M sentences and around 90M words. In the baseline system, we use three language models, a word-based, a bilingual (Niehues et al., 2011) and a cluster based language model, using 100 automatically generated clusters using MKCLS (Och, 1999). The advanced system use pre-reodering (Herrmann et al., 2013) and lexicalized reordering. In addition, it uses a discriminative word lexicon (Niehues and Waibel, 2013) and a language model trained on the large monolingual data. Both systems were optimized on the tst2014 using Minimum error rate training (Och, 2003). A detailed description of the systems can be found in (Ha et al., 2016). The neural machine translation was trained using Nematus2 . For the NMT system as well as for the PreMT system, we used the default configuration. In order to limit the vocabulary size, we use BPE as 1 2 http://www.statmt.org/wmt16/translation-task.html https://github.c"
C16-1172,W16-2378,0,0.0475053,"nally, we will finish with a conclusion. 2 Related Work The idea of linear combining of machine translation systems using different paradigms has already been used successfully for SMT and rule-based machine translation (RBMT) (Dugast et al., 2007; Simard et al., 2007). They build an SMT system that is post-editing the output of an RBMT system. Using the combination of SMT and RBMT, they could outperform both single systems. Those experiments promote the area of automatic post-editing (Bojar et al., 2015). Recently, it was shown that models based on neural MT are very successful in this task (Junczys-Dowmunt and Grundkiewicz, 2016). For PBMT, there has been several attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound splitting in German (Koehn and Knight, 2003). Another example would be to use pre-reordering in order to achieve more monotone translation (Rottmann and Vogel, 2007). In addition, the usefulness of using the translations of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discri"
C16-1172,E03-1076,0,0.0615016,"7). They build an SMT system that is post-editing the output of an RBMT system. Using the combination of SMT and RBMT, they could outperform both single systems. Those experiments promote the area of automatic post-editing (Bojar et al., 2015). Recently, it was shown that models based on neural MT are very successful in this task (Junczys-Dowmunt and Grundkiewicz, 2016). For PBMT, there has been several attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound splitting in German (Koehn and Knight, 2003). Another example would be to use pre-reordering in order to achieve more monotone translation (Rottmann and Vogel, 2007). In addition, the usefulness of using the translations of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016),"
C16-1172,N03-1017,0,0.0285031,"lation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al., 1993), phrase-based machine translation (Koehn et al., 2003; Och and Ney, 2004) segments the sentence into continuous phrases that are used as basic translation units. This allows for many-to-many alignments. Based on this segmentation, the probability of the translation is calculated using a log-linear combination of different features: P I I exp( N n=1 λn hn (e , f )) PN 0I I e0I exp( n=1 λn hn (e , f )) P (eI , f I ) = P (1) In the initial model, the features are based on language and translation model probabilities as well as a few count based features. In advanced PBMT systems, several additional features to better model the 1829 Figure 1: Pre-tr"
C16-1172,P15-1002,0,0.0552301,"und splitting in German (Koehn and Knight, 2003). Another example would be to use pre-reordering in order to achieve more monotone translation (Rottmann and Vogel, 2007). In addition, the usefulness of using the translations of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al., 1993), phrase-based machine translation (Koehn et al., 2003; Och and Ney, 2004) segments the sentence into continuous phrases that are used as basic translation units. This allows for many-to-many alignments. Based on this segmentation, the probabil"
C16-1172,W13-2264,1,0.925069,"veral attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound splitting in German (Koehn and Knight, 2003). Another example would be to use pre-reordering in order to achieve more monotone translation (Rottmann and Vogel, 2007). In addition, the usefulness of using the translations of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al., 1993), phrase-based machine translation (Koehn et al., 2003; Och and Ney, 2004) s"
C16-1172,W11-2124,1,0.311374,"ending on the frequency of the words and finally show some example translations. 5.1 System description For the pre-translation, we used a PBMT system. In order to analyze the influence of the quality of the PBMT system, we use two different systems, a baseline system and a system with advanced models. The systems were trained on all parallel data available for the WMT 20161 . The news commentary corpus, the European parliament proceedings and the common crawl corpus sum up to 3.7M sentences and around 90M words. In the baseline system, we use three language models, a word-based, a bilingual (Niehues et al., 2011) and a cluster based language model, using 100 automatically generated clusters using MKCLS (Och, 1999). The advanced system use pre-reodering (Herrmann et al., 2013) and lexicalized reordering. In addition, it uses a discriminative word lexicon (Niehues and Waibel, 2013) and a language model trained on the large monolingual data. Both systems were optimized on the tst2014 using Minimum error rate training (Och, 2003). A detailed description of the systems can be found in (Ha et al., 2016). The neural machine translation was trained using Nematus2 . For the NMT system as well as for the PreMT"
C16-1172,J04-4002,0,0.063821,"es and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al., 1993), phrase-based machine translation (Koehn et al., 2003; Och and Ney, 2004) segments the sentence into continuous phrases that are used as basic translation units. This allows for many-to-many alignments. Based on this segmentation, the probability of the translation is calculated using a log-linear combination of different features: P I I exp( N n=1 λn hn (e , f )) PN 0I I e0I exp( n=1 λn hn (e , f )) P (eI , f I ) = P (1) In the initial model, the features are based on language and translation model probabilities as well as a few count based features. In advanced PBMT systems, several additional features to better model the 1829 Figure 1: Pre-translation methods (a"
C16-1172,E99-1010,0,0.053505,"e-translation, we used a PBMT system. In order to analyze the influence of the quality of the PBMT system, we use two different systems, a baseline system and a system with advanced models. The systems were trained on all parallel data available for the WMT 20161 . The news commentary corpus, the European parliament proceedings and the common crawl corpus sum up to 3.7M sentences and around 90M words. In the baseline system, we use three language models, a word-based, a bilingual (Niehues et al., 2011) and a cluster based language model, using 100 automatically generated clusters using MKCLS (Och, 1999). The advanced system use pre-reodering (Herrmann et al., 2013) and lexicalized reordering. In addition, it uses a discriminative word lexicon (Niehues and Waibel, 2013) and a language model trained on the large monolingual data. Both systems were optimized on the tst2014 using Minimum error rate training (Och, 2003). A detailed description of the systems can be found in (Ha et al., 2016). The neural machine translation was trained using Nematus2 . For the NMT system as well as for the PreMT system, we used the default configuration. In order to limit the vocabulary size, we use BPE as 1 2 htt"
C16-1172,P03-1021,0,0.0300472,"proceedings and the common crawl corpus sum up to 3.7M sentences and around 90M words. In the baseline system, we use three language models, a word-based, a bilingual (Niehues et al., 2011) and a cluster based language model, using 100 automatically generated clusters using MKCLS (Och, 1999). The advanced system use pre-reodering (Herrmann et al., 2013) and lexicalized reordering. In addition, it uses a discriminative word lexicon (Niehues and Waibel, 2013) and a language model trained on the large monolingual data. Both systems were optimized on the tst2014 using Minimum error rate training (Och, 2003). A detailed description of the systems can be found in (Ha et al., 2016). The neural machine translation was trained using Nematus2 . For the NMT system as well as for the PreMT system, we used the default configuration. In order to limit the vocabulary size, we use BPE as 1 2 http://www.statmt.org/wmt16/translation-task.html https://github.com/rsennrich/nematus 1831 described in (Sennrich et al., 2016) with 40K operations. We run the NMT system for 420K iterations and stored a model every 30K iterations. We selected the model that performed best on the development data. For the ensemble syst"
C16-1172,2007.tmi-papers.21,0,0.0298329,"they could outperform both single systems. Those experiments promote the area of automatic post-editing (Bojar et al., 2015). Recently, it was shown that models based on neural MT are very successful in this task (Junczys-Dowmunt and Grundkiewicz, 2016). For PBMT, there has been several attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound splitting in German (Koehn and Knight, 2003). Another example would be to use pre-reordering in order to achieve more monotone translation (Rottmann and Vogel, 2007). In addition, the usefulness of using the translations of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical"
C16-1172,P16-1162,0,0.803657,"translation for most of sentences. Neural machine translation systems provide the output with high fluency. A weakness of NMT systems, however, is that they sometimes lose the original meaning of the source words during translation. One example from the first conference on machine translation (WMT16) test set is the segment in Table 1. The English word goalie is not translated to the correct German word Torwart, but to the German word Gott, which means god. One problem could be that we need to limit the vocabulary size in order to train the model efficiently. We used Byte Pair Encoding (BPE) (Sennrich et al., 2016) to represent the text using a fixed size vocabulary. In our case the word goali is splitted into three parts go, al and ie. Then it is more difficult to transport the meaning to the translation. In contrast to this, in phrase-based machine translation (PBMT), we do not need to limit the vocabulary and are often able to translate words even if we have seen them only very rarely in the training. In the example mentioned before, for instance, the PBMT system had no problems translating the expression correctly. On the other hand, official evaluation campaigns (Bojar et al., 2016) have shown that"
C16-1172,N07-1064,0,0.0156756,"ollows: In the next section we will review the related work. In Section 3, we will briefly review the phrase-based and neural approach to machine translation. Section 4 will introduce the approach presented in this paper to pre-translate the input using a PBMT system. In the following section, we will evaluate the approach and analyze the errors. Finally, we will finish with a conclusion. 2 Related Work The idea of linear combining of machine translation systems using different paradigms has already been used successfully for SMT and rule-based machine translation (RBMT) (Dugast et al., 2007; Simard et al., 2007). They build an SMT system that is post-editing the output of an RBMT system. Using the combination of SMT and RBMT, they could outperform both single systems. Those experiments promote the area of automatic post-editing (Bojar et al., 2015). Recently, it was shown that models based on neural MT are very successful in this task (Junczys-Dowmunt and Grundkiewicz, 2016). For PBMT, there has been several attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound splitting in German (Ko"
C16-1172,P10-1049,0,0.0600704,"this task (Junczys-Dowmunt and Grundkiewicz, 2016). For PBMT, there has been several attempts to apply preprocessing in order to improve the performance of the translation system. A commonly used preprocessing step is morphological splitting, like compound splitting in German (Koehn and Knight, 2003). Another example would be to use pre-reordering in order to achieve more monotone translation (Rottmann and Vogel, 2007). In addition, the usefulness of using the translations of the training data of a PBMT system has been shown. The translations have been used to re-train the translation model (Wuebker et al., 2010) or to train additional discriminative translation models (Niehues and Waibel, 2013). In order to improve the translation of rare words in NMT, authors try to translate words that are not in the vocabulary in a post-processing step (Luong et al., 2015). In (Sennrich et al., 2016), a method to split words into sub-word units was presented to limit the vocabulary size. Also the integration of lexical probabilities into NMT was successfully investigated (Arthur et al., 2016). 3 Phrase-based and Neural Machine Translation Starting with the initial work on word-based translation system (Brown et al"
C16-1172,W15-3001,0,\N,Missing
C16-1172,W16-2301,0,\N,Missing
C16-1292,W05-0909,0,0.0342364,"Q(true) := wi yi (1) i∈ALL Note that this definition does not aim to handle document-level discourse phenomena such as coherence, cohesion, and and consistency, but estimates the average sentence-level quality for the document in order to evaluate the overall quality of the whole output hypothesis. In this paper, we weigh segments proportionally to their length, although future work may investigate more sophisticated notions of segment importance. Our definition of document quality is simplistic, but widely used in both MT and ASR communities, e.g. document- or corpus-level WER, TER, METEOR (Banerjee and Lavie, 2005), and human rankings are usually computed this way. BLEU is computed on the corpus level and thus not directly usable with our approach, but we can instead resort to computing average sentence-level BLEU variants such as BLEU+1 (Lin and Och, 2004) that essentially differ only in the smoothing details. Our lightly supervised estimation framework determines document quality in several steps. The first step is to automatically estimate the quality for each segment in the hypothesis (§4). This can be achieved by training a regressor with the desired target measure, as discussed in numerous previou"
C16-1292,P13-2097,0,0.027685,"n found challenging in both ASR (Negri et al., 2014) and MT (de Souza et al., 2015a). The WMT 2015 shared task on QE considered only the scenario of training and testing on output produced by the same system (Bojar et al., 2015). The usual way to address domain mismatch when training data for all domains is available is via adaptation/multitask learning (Beck et al., 2014; de Souza et al., 2015b). Our indicator features can be seen as a form of multitask learning. A strategy for the case where no in-domain training data is available is to obtain such training data cheaply via active learning (Beck et al., 2013). This work is probably most similar in spirit to ours in that it attempts reliable quality estimation at low labeling costs. A different line of research, crowd-sourced annotation, critically depends on quality control, as well. Approaches are usually based on comparing results between several workers (Passonneau and Carpenter, 2014), querying gold standard “testing” labels occasionally (Joglekar and Garcia-Molina, 2013), and/or automatically predicting quality (Roy et al., 2010; Gao et al., 2015). Our work can be seen as a generalization of the latter two, with the gold labels corresponding"
C16-1292,W14-3338,0,0.0219279,"information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-domain case investigated in this work, i.e. predicting quality for a previously unknown system or task, has been found challenging in both ASR (Negri et al., 2014) and MT (de Souza et al., 2015a). The WMT 2015 shared task on QE considered only the scenario of training and testing on output produced by the same system (Bojar et al., 2015). The usual way to address domain mismatch when training data for all domains is available is via adaptation/multitask learning (Beck et al., 2014; de Souza et al., 2015b). Our indicator features can be seen as a form of multitask learning. A strategy for the case where no in-domain training data is available is to obtain such training data cheaply via active learning (Beck et al., 2013). This work is probably most similar in spirit to ours in that it attempts reliable quality estimation at low labeling costs. A different line of research, crowd-sourced annotation, critically depends on quality control, as well. Approaches are usually based on comparing results between several workers (Passonneau and Carpenter, 2014), querying gold stan"
C16-1292,P07-1033,0,0.0452183,"Missing"
C16-1292,P15-1022,0,0.0463442,"Missing"
C16-1292,N15-1073,0,0.0557124,"Missing"
C16-1292,P15-1174,0,0.0243,"lly automatic baseline (§3.1). For comparison, we evaluate a mean-predictor baseline that always predicts the training mean, regardless of the input features. This baseline has been found surprisingly strong previously (Negri et al., 2014; Specia et al., 2015), which we confirm in Table 2. On segment-level, gains over the mean-predictor baseline are clearly visible only for the ASR setting. As expected, the out-of-domain tasks appear much more difficult than the in-domain setting. Note that even though the mean baseline sometimes achieves lower MAE, the XT regressor maintains the advantages 3 Graham (2015) argues that correlation is better for evaluating sentence-level QE, because MAE can be improved by transformation to match estimated global mean and variance. However, we find MAE more indicative for our purpose as it measures not only how well systems are compared against one another, but also how well overall quality is judged in absolute terms. Moreover, collecting global statistics for transformation seems problematic when flexibility for domain changes is required. 4 Tuning directly for MAE yielded similar results. 3108 MT.in-domain MT.out-of-domain ASR.in-domain ASR.out-of-domain ↓MAE m"
C16-1292,W04-3250,0,0.323366,"Missing"
C16-1292,C04-1072,0,0.326451,"verall quality of the whole output hypothesis. In this paper, we weigh segments proportionally to their length, although future work may investigate more sophisticated notions of segment importance. Our definition of document quality is simplistic, but widely used in both MT and ASR communities, e.g. document- or corpus-level WER, TER, METEOR (Banerjee and Lavie, 2005), and human rankings are usually computed this way. BLEU is computed on the corpus level and thus not directly usable with our approach, but we can instead resort to computing average sentence-level BLEU variants such as BLEU+1 (Lin and Och, 2004) that essentially differ only in the smoothing details. Our lightly supervised estimation framework determines document quality in several steps. The first step is to automatically estimate the quality for each segment in the hypothesis (§4). This can be achieved by training a regressor with the desired target measure, as discussed in numerous previous works. The 3104 second step is then to manually annotate the quality score for a certain number of segments. In our evaluation (§5), we experiment with typical amounts of tens to hundreds of annotated words. The final step is to aggregate manual"
C16-1292,C14-1171,0,0.0494527,"Missing"
C16-1292,P02-1040,0,0.103668,"chnology, enabling users and engineers to judge overall quality of the output, detect key problems, improve systems, and choose among competing systems. Although most users and engineers share these goals, the chosen evaluation approaches can differ strongly, with some people resorting to automatic, reference-based evaluation, while others rely on manual evaluation for their purposes. This is especially pronounced in the case of machine translation (MT), as pointed out by Harris et al. (2016). On one hand, much research effort has been devoted to devising reference-based methods such as BLEU (Papineni et al., 2002) that are well-correlated with human judgment. On the other hand, practitioners need to react to changing domains from customer to customer, and reflect multi-faceted quality requirements that are difficult to measure in a single, generic score, often leaving manual evaluation as the only choice. In recent years, automatic quality estimation (QE) has emerged as a method that could potentially address the lack of flexibility of reference-based evaluation to deal with changing requirements, and the high effort of manual evaluation. Automatic QE uses machine learning techniques that are trained o"
C16-1292,Q14-1025,0,0.0302706,"ia adaptation/multitask learning (Beck et al., 2014; de Souza et al., 2015b). Our indicator features can be seen as a form of multitask learning. A strategy for the case where no in-domain training data is available is to obtain such training data cheaply via active learning (Beck et al., 2013). This work is probably most similar in spirit to ours in that it attempts reliable quality estimation at low labeling costs. A different line of research, crowd-sourced annotation, critically depends on quality control, as well. Approaches are usually based on comparing results between several workers (Passonneau and Carpenter, 2014), querying gold standard “testing” labels occasionally (Joglekar and Garcia-Molina, 2013), and/or automatically predicting quality (Roy et al., 2010; Gao et al., 2015). Our work can be seen as a generalization of the latter two, with the gold labels corresponding to our fully manual baseline, the automatic estimation corresponding to our fully automatic baseline, and the workers being our systems. 7 Conclusion We proposed lightly supervised quality estimation at the document level, a framework that allows flexible quality estimation across changing domains and quality requirements, while requi"
C16-1292,2014.eamt-1.21,0,0.0283169,"ther or not the situation changes when switching to very different metrics, such as non reference-based metrics, is left as a question for future work. 3110 6 Relation to Prior Work A good overview over the state-of-the-art in automatic QE for MT is given by Bojar et al. (2015) and Bojar et al. (2016), and for ASR by Ogawa et al. (2012) and Negri et al. (2014). Document-level QE was first explored by Soricut and Echihabi (2010) by exploiting document-level features, and was later improved by using sentence-level information (Soricut and Narsale, 2012; Specia et al., 2015; Bojar et al., 2015). Scarton and Specia (2014) and Scarton et al. (2015) argue that document-level quality metrics should consider discourse information that cannot be captured when processing segments individually, and explore features for QE that capture discourse information. Quantitative assessment of discourse information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-domain case investigated in this work, i.e. predicting quality for a previously unknown system or task, has been found challenging in both ASR (Negri et al., 2014) and MT (de Souza et al., 2015a). T"
C16-1292,W15-4916,0,0.0378992,"Missing"
C16-1292,P10-1063,0,0.019873,"ar observations for MT, and we expect these findings to hold for other datasets. We also confirmed that results are similar when using BLEU+1 (Lin and Och, 2004) as the metric, instead of TER. Whether or not the situation changes when switching to very different metrics, such as non reference-based metrics, is left as a question for future work. 3110 6 Relation to Prior Work A good overview over the state-of-the-art in automatic QE for MT is given by Bojar et al. (2015) and Bojar et al. (2016), and for ASR by Ogawa et al. (2012) and Negri et al. (2014). Document-level QE was first explored by Soricut and Echihabi (2010) by exploiting document-level features, and was later improved by using sentence-level information (Soricut and Narsale, 2012; Specia et al., 2015; Bojar et al., 2015). Scarton and Specia (2014) and Scarton et al. (2015) argue that document-level quality metrics should consider discourse information that cannot be captured when processing segments individually, and explore features for QE that capture discourse information. Quantitative assessment of discourse information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-doma"
C16-1292,W12-3121,0,0.0227726,"n using BLEU+1 (Lin and Och, 2004) as the metric, instead of TER. Whether or not the situation changes when switching to very different metrics, such as non reference-based metrics, is left as a question for future work. 3110 6 Relation to Prior Work A good overview over the state-of-the-art in automatic QE for MT is given by Bojar et al. (2015) and Bojar et al. (2016), and for ASR by Ogawa et al. (2012) and Negri et al. (2014). Document-level QE was first explored by Soricut and Echihabi (2010) by exploiting document-level features, and was later improved by using sentence-level information (Soricut and Narsale, 2012; Specia et al., 2015; Bojar et al., 2015). Scarton and Specia (2014) and Scarton et al. (2015) argue that document-level quality metrics should consider discourse information that cannot be captured when processing segments individually, and explore features for QE that capture discourse information. Quantitative assessment of discourse information remains challenging (Bojar et al., 2016). Our aggregation approach supports only sentencelevel information. The out-of-domain case investigated in this work, i.e. predicting quality for a previously unknown system or task, has been found challengin"
C16-1292,P13-4014,0,0.0470002,"Missing"
C16-1292,P15-4020,0,0.102177,"tor who replicates these exactly. Our main evaluation measure is mean absolute error (MAE)3 between the predicted and true document-level TER/WER. We use 5 datasets as indicated in Table 1, with testing data varying over all datasets, but only the ones labeled “in-domain” used for regressor training. Moreover, for each evaluated document the QE regressor was retrained with training data excluding that which corresponded to the same system or document currently tested (for in-domain tests). This makes even our in-domain scenario more challenging than some of the previous works on automatic QE (Specia et al., 2015). We use scikit-learn (Pedregosa et al., 2011) for regressor training. We assign weights to training samples proportional to their segment length, because longer segments are weighted more strongly in our aggregation strategies (§3) and are thus more important to be accurately predicted. We perform random search with 20 iterations to optimize hyper-parameters (namely, the max-depth and min-samplessplit parameters of XTs) in terms of mean squared error.4 Tuning is conducted separately for every test document, using 10-fold cross validation on the respective training data. For regressor adaptati"
C16-1292,2015.eamt-1.17,0,\N,Missing
C96-1033,1993.iwpt-1.12,0,0.0198867,"is too rigid, causing problems with ungranlmaticality and other deviations from linguistic rules. These deviations are 188 manageable and low in number, when analyzing written language, but not for spoken language. The latter also contains spontaneous effects and speech recognition errors. (On the other hand, the good thing is that spoken language tend to contain less complex structures than written language.) Several methods have been suggested compensate for these speech related problems: e.g. score and penalties, probabilistic rules, and skipping words (Dowding et al., 1993; Seneff, 1992; Lavie and Tomita, 1993; Issar and Ward, 1993). A small community have experimented with either purely statistical approaches(Brown et al., 1990; Schiitze, 1993) or connectionist based approaches (Berg, 1991; Miikkulainen and Dyer, 1991; Jain, 1991; Wermter and Weber, 1994). The main problem when using statistical approaches for spoken language processing, is the large amounts of data required to train these models. All connectionist approaches to our knowledge, have suffered from one or more of the following problems: One, parses contains none or too few linguistic attributes to be used in translation or understand"
C96-1033,J90-2002,0,0.0128283,"Missing"
C96-1033,J92-1004,0,0.0347762,"this knowledge is too rigid, causing problems with ungranlmaticality and other deviations from linguistic rules. These deviations are 188 manageable and low in number, when analyzing written language, but not for spoken language. The latter also contains spontaneous effects and speech recognition errors. (On the other hand, the good thing is that spoken language tend to contain less complex structures than written language.) Several methods have been suggested compensate for these speech related problems: e.g. score and penalties, probabilistic rules, and skipping words (Dowding et al., 1993; Seneff, 1992; Lavie and Tomita, 1993; Issar and Ward, 1993). A small community have experimented with either purely statistical approaches(Brown et al., 1990; Schiitze, 1993) or connectionist based approaches (Berg, 1991; Miikkulainen and Dyer, 1991; Jain, 1991; Wermter and Weber, 1994). The main problem when using statistical approaches for spoken language processing, is the large amounts of data required to train these models. All connectionist approaches to our knowledge, have suffered from one or more of the following problems: One, parses contains none or too few linguistic attributes to be used in t"
C96-1033,P93-1008,0,0.0204466,"ar rules, and second, this knowledge is too rigid, causing problems with ungranlmaticality and other deviations from linguistic rules. These deviations are 188 manageable and low in number, when analyzing written language, but not for spoken language. The latter also contains spontaneous effects and speech recognition errors. (On the other hand, the good thing is that spoken language tend to contain less complex structures than written language.) Several methods have been suggested compensate for these speech related problems: e.g. score and penalties, probabilistic rules, and skipping words (Dowding et al., 1993; Seneff, 1992; Lavie and Tomita, 1993; Issar and Ward, 1993). A small community have experimented with either purely statistical approaches(Brown et al., 1990; Schiitze, 1993) or connectionist based approaches (Berg, 1991; Miikkulainen and Dyer, 1991; Jain, 1991; Wermter and Weber, 1994). The main problem when using statistical approaches for spoken language processing, is the large amounts of data required to train these models. All connectionist approaches to our knowledge, have suffered from one or more of the following problems: One, parses contains none or too few linguistic attributes t"
C96-1033,H94-1039,0,\N,Missing
C96-1075,P94-1045,1,0.888768,"Missing"
C96-1075,P95-1005,1,0.822394,"Missing"
C96-1075,J87-1004,0,0.258485,"signed to be language-independent in the sense that they 442 S • . • iSource n Language &gt; J I OU ~ rI "" FGenKit enerator [ . . I s °oo. Figure 1: T h e J A N U S expressions, and incorporates the sentence into a discourse plan tree. &apos;the discourse processor also updates a calendar which keeps track of what the speakers haw&apos;~ said about their schedules. The discourse processor is described in greater detail else.where (R,osd et 31. 1995). 3 The QLR Translation Module The (]LR.* parser (Lavie and Tomita 11993; I,avie 1994) is a parsing system based on Tomita&apos;s Generalized LI~ parsing algorithm (Tomita 1987). The parser skips parts of the utterance that it cannot incorporate into a well-formed sentence structure. Thus it is well-suited to doinains ill which nongrammaticality is c o a l i t i o n . T h e parser conducts a search for the maximal subset of the original input that is covered by the grammar. This is done using a beam search heuristic that limits tile combinations of skipped words considered by the parser, and ensures that it operates within feasible time and space bonnds. The GI,R* parser was implemented as an extension to the G LR parsing system, a unificationI&gt;ased practical natural"
C96-1075,C90-1012,0,\N,Missing
C98-1072,E93-1027,0,0.0464059,"Missing"
C98-1072,P94-1004,0,0.0128763,"mple, the parse tree in Fig. 4 contains the following set of &lt;state, observation&gt; pairs: {&lt; [time], [point] &gt;, &lt; [point], [day_of_week] &gt;, &lt; [point], [time_of_day] &gt;}. [ s~l ,low rule: [suggest_lime] &lt; - - what about [lime] (From 'k~hal about ~esday a f t e r r ~ 1. In the absence of a training corpus, the HUM parameters are seeded from the Kernel Grammar itself. 2. Training is maintained at run-time through dynamic updates of all model parameters after each utterance and learning episode. Figure 6: ...and a new rule is acquired. of unparsed words, the following stochastic models, inspired in Miller et al. (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. • Speech-act n-gram. Top-level concepts can be seen as speech acts of the domain. For instance, in the DM in Fig. 2 top-level concepts such as [greeting], [farewell] or [suggestion], correspond to discourse speech acts, and in normally-occurring conversation, they follow a distribution that is clearly non-uniform. 1° • Concept-subconcept H M M . Discrete hidden Markov model in which the states correspond l ° N e e d l e s s to say, s p e e c h - a c t t r a n s i t i o n d i s t r i b u t i o n"
C98-1072,J92-1004,0,0.0263446,"Missing"
C98-2216,J92-4003,0,0.0376998,"Missing"
C98-2216,C96-2141,0,0.105611,"Missing"
C98-2216,P97-1047,1,0.853901,"result of erroneous sentence alignment) or the two languages have different word orders, like English and German. Figure 1 and Figure 2 show some problematic a.lignments between English/German sentences made by IBM Model 2, together with the &apos;ideal&apos; align~ ments for the sentences, tIere the alignment paranleters penalize the alignment of English words with their German translation equivalents because the translation equiwdents are far away tYom the words. An ext)eriment reveals how often this ldnd of &quot;skewed&quot; alignment hat)pens in our 1.2nglish/(~erman scheduling conversation parallel corpus (Wang and Waibel, 1997). The experiment was based on the following observation: IBM translation Model 1 (where the alignment distribution is uniform) and Model 2 found similar Viterbi alignments when there were no movements or deletions, and they predicted very different Viterbi alignnlents when the skewness was severe in a sentence pair, since the alignment parameters in Model 2 penalize the long distance alignment. Figure 3 shows the Viterbi alignment discovered by Model 1 for the same sentences in Figure 21 . We memsured the distance of a Model 1 alignment a 1 and a Model 2 alignment a 2 as ~!g_l1 [ a ~ - a~[. To"
C98-2216,J93-2003,0,\N,Missing
cho-etal-2014-corpus,fitzgerald-jelinek-2008-linguistic,0,\N,Missing
cho-etal-2014-corpus,E09-1030,0,\N,Missing
cho-etal-2014-corpus,W11-2124,1,\N,Missing
cho-etal-2014-corpus,P02-1040,0,\N,Missing
cho-etal-2014-corpus,P07-2045,0,\N,Missing
cho-etal-2014-corpus,P04-1005,0,\N,Missing
cho-etal-2014-corpus,stuker-etal-2012-kit,1,\N,Missing
cho-etal-2014-corpus,E14-4009,1,\N,Missing
cho-etal-2014-corpus,maekawa-etal-2000-spontaneous,0,\N,Missing
D17-1145,P08-1115,0,0.298669,"ic speech recognition (ASR) system. Previous research on traditional phrase-based or tree-based statistical machine translation have used word lattices (e.g. Figure 1) as an effective tool to pass on uncertainties from a previous step (Ney, 1999; Casacuberta et al., 2004). Several works have shown quality improvements by translating lattices, compared to translating only the single best upstream output. Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al., 2008). Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling. These models force us to rethink approaches to handling lattices, because their recurrent design no longer allows for efficient lattice decoding using dynamic programming as was used in the earlier works. As a remedy, Su et al. (2017) proposed replacing the sequential encoder by a lattice encoder to obtain a lattice-to-sequence (lat2seq"
D17-1145,P16-1078,0,0.0159692,"ords we replace (1) by the following:  → − → − h i = LatticeLSTM xi , { h k |k∈C(i)} (4) Similarly, we encode the lattice in backward direction and replace (2) accordingly. Figure 2 illustrates the result. The computational complexity of the encoder is O(|V |+ |E|), i.e. linear in the number of nodes plus number of edges in the graph. The complexity of the attention mechanism is O(|V |M ), where M is the output sequence 2 It is perhaps more common to think of each edge representing a word, but we will motivate why we instead assign word labels to nodes in §3.3. 3 This is similar in spirit to Eriguchi et al. (2016) who used the TreeLSTM in an attentional tree-to-sequence model. 1382 Sequential LSTM recurrence ˜ i = hi−1 h forget gt.   ˜ i + bf f i = σ W f xi + Uf h TreeLSTM ˜i = P h k∈C(i) hk fik = σ(Wf xi + U f hk + bf ) Proposed LatticeLSTM Sh wb/f,k ˜i = P h k∈C(i) Zh,k hk (5)  fik = σ(Wf xi + Uf hk + (6)  ln wb/f,k Sf − Zf,k + bf ) update  ˜ i + bin ii = σ Win xi + Uin h   ˜ i + bo oi = σ Wo xi + Uo h   ˜ i + bu ui = tanh Wu xi + Uu h cell ci = ii ui + fi ci−1 ci = ii ui + P k∈C(i) fik ck as TreeLSTM hidden hi = oi tanh(ci ) as sequential as sequential input gt.  output gt. attention as se"
D17-1145,D13-1176,0,0.373947,"based or tree-based statistical machine translation have used word lattices (e.g. Figure 1) as an effective tool to pass on uncertainties from a previous step (Ney, 1999; Casacuberta et al., 2004). Several works have shown quality improvements by translating lattices, compared to translating only the single best upstream output. Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al., 2008). Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling. These models force us to rethink approaches to handling lattices, because their recurrent design no longer allows for efficient lattice decoding using dynamic programming as was used in the earlier works. As a remedy, Su et al. (2017) proposed replacing the sequential encoder by a lattice encoder to obtain a lattice-to-sequence (lat2seq) model. This is achieved by extending the encoder’s Gated Recurrent Units (GRUs) (Cho e"
D17-1145,D15-1166,0,0.0804604,"o hidden states are generated as  → − → − h i = LSTM Efwd (xi ), h i−1 (1) ← − ← −  h i = LSTM Ebwd (xi ), h i+1 , (2) where Efwd and Ebwd are source embedding lookup tables. We opt for long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) recurrent units because of their high performance and in order to later take advantage of the TreeLSTM extension (Tai et al., 2015). We stack multiple LSTM layers and concatenate the final layer → − ← − into the final source hidden state hi = h i |h i , where layer indices are omitted for simplicity. 2.2 Attention We use an attention mechanism (Luong et al., 2015) to summarize the encoder outputs into a 1381 fixed-size representation. At each decoding time step j, a context vector cj is computed as a weighted PNaverage of the source hidden states: cj = i=1 αij hi . The normalized attentional weights αij measure the relative importance of the source words for the current decoding step and are computed as a softmax with normalization factor Z summing over i: αij =  1 exp s sj−1 , hi Z (3) s(·) is a feed-forward neural network with a single layer that estimates the importance of source hidden state hi for producing the next target symbol yj , conditione"
D17-1145,2013.iwslt-papers.14,0,0.531314,"r the baseline. According to our knowledge, this is the first attempt of integrating lattice scores already at the training stage of a machine translation model. • We exploit the fact that our lattice encoder is a strict generalization of a sequential encoder by pre-training on sequential data, obtaining faster and better training convergence on large corpora of parallel sequential data. 1 This is reminiscent of the weighted pooling strategy by Ladhak et al. (2016) for spoken utterance classification. We conduct experiments on the Fisher and Callhome Spanish–English Speech Translation Corpus (Post et al., 2013) and report improvements of 1.4 BLEU points on Fisher and 0.8 BLEU points on Callhome, compared to a strong baseline optimized for translating 1-best ASR outputs. We find that the proposed integration of lattice scores is crucial for achieving these improvements. 2 Background Our work extends the seminal work on attentional encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) which we survey in this section. Given an input sequence x = (x1 , x2 , . . . , xN ), the goal is to generate an appropriate output sequence y = (y1 , y2 , . . . , yM ). T"
D17-1145,P15-1150,0,0.0479141,"Missing"
D17-1145,2005.iwslt-1.2,0,0.377035,"eech translation, where a down-stream translation system must consume the output of an up-stream automatic speech recognition (ASR) system. Previous research on traditional phrase-based or tree-based statistical machine translation have used word lattices (e.g. Figure 1) as an effective tool to pass on uncertainties from a previous step (Ney, 1999; Casacuberta et al., 2004). Several works have shown quality improvements by translating lattices, compared to translating only the single best upstream output. Examples include translating lattice representations of ASR output (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), multiple word segmentations, and morphological alternatives (Dyer et al., 2008). Recently, neural sequence-to-sequence (seq2seq) models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) have often been preferred over the traditional methods for their strong empirical results and appealing end-to-end modeling. These models force us to rethink approaches to handling lattices, because their recurrent design no longer allows for efficient lattice decoding using dynamic programming as was used in the earlier works. As a remedy, Su et al. (2017)"
D19-5535,J96-1002,0,0.370617,"dgement or echoing the user input. However, the triggered actions are not finished earlier with this approach, but in cases where long pauses cannot be avoided, even with incremental processing, such time buying strategies can be applied. The automatically generated backchannel described by R¨ude et al. (2017) gives feedback during the uttering of an utterance. However, only acoustic features are used and it does not reduce the latency of actions that can be triggered by the utterances. Studies have been conducted on incremental NLU. DeVault et al. (2009) used a maximum entropy classificator (Berger et al., 1996) to classify the partial utterances. They optimized the maximum entropy classificator for partial utterances by using an individual classificator for every utterance length. The problem of this classification approach is that it is not suitable for tasks with a lot of different parameter combinations; for such tasks, a slot filling (sequence labeling task) or word by word approach (sequence to sequence task) is more suitable. Such a more suitable approach is described by Niehues et al. (2018) for incrementally updating machine translations. The authors used an attention-based encoder decoder ("
D19-5535,W09-3902,0,0.0737373,"Missing"
D19-5535,E99-1023,0,0.0847023,"ic because of this separate treatment and therefore an arbitrary model that can process sequences can be used to process the partial and full utterances. The method is depicted in Figure 1 for the utterance Flights to Pittsburgh. 3.2 Data For our experiments, we used utterances from the Airline Travel Information System (ATIS) datasets. We used the utterances that are used by Hakkani-Tur et al. (2016) and are publicly available3 . These utterances were cleaned and every utterance is labeled with its intents and for every token, the corresponding slot is labeled with a tag (in the IOB2 format (Sang and Veenstra, 1999) that is depicted in Figure 2). We converted the data from the IOB2 format to a sequence to sequence format (Constantin et al., 2019). The source sequence is a user utterance Low-latency NLU component In this work, we present a model-agnostic method to build an incremental processing low-latency NLU component. The advantages of this modelagnostic method are that we can use state-ofthe-art neural network architectures and reuse the method for future state-of-the-art neural network architectures. Our used architecture is described 1 https://github.com/quanpn90/ NMTGMinor/tree/DbMajor 2 https://g"
E09-1040,niessen-etal-2000-evaluation,0,0.0495925,"ion, to evaluate the performance of a complete speech-to-speech translation system, we need to compare the source speech used as input to the translated output speech in the target language. To that aim, we reused a large part of the evaluation protocol from the TC-STAR project(Hamon et al., 2007). 4.2 Evaluation Protocol 4 Evaluation Tasks SLT evaluation. For the SLT evaluation, the automatically translated text from the ASR output is compared with two manual reference translations by means of automatic and human metrics. Two automatic metrics are used: BLEU (Papineni et al., 2001) and mWER (Niessen et al., 2000). For the human evaluation, each segment is evaluated in relation to adequacy and fluency (White and O’Connell, 1994). For the evaluation of adequacy, the target segment is compared to a reference segment. For the evaluation of fluency, the quality of the language is evaluated. The two types of evaluation are done independently, but each evaluator did both evaluations (first that of fluency, then that of adequacy) for a certain number of segments. For the evaluation of fluency, evaluators had to answer the question: “Is the text written in good Spanish?”. For the evaluation of adequacy, evalua"
E09-1040,H01-1048,1,0.755703,"ial hypotheses produced by the ASR module are collected in the resegmentation component, for merging and re-splitting at appropriate “semantic” boundaries. The resegmented hypotheses are then transferred to one or more machine translation components (MT), at least one per language pair. Different output technologies may be used for presenting the translations to the audience. For a detailed description of the components as well as the client-server framework used for connecting the components please refer to (Fügen et al., 2006b; Fügen et al., 2006a; Kolss et al., 2006; Fügen and Kolss, 2007; Fügen et al., 2001). 3 Automatic Simultaneous Translation Given the explanations above on human interpretation, one has to weigh two factors when considering the use of simultaneous translation systems: translation quality and cost. The major disadvantage of an automatic system compared to human interpretation is its translation quality, as we will see in the following sections. Current state-of-the-art systems may reach satisfactory quality for people not understanding the lecturer at all, but are still worse than human interpretation. Nevertheless, an automatic system may have considerable advantages. One such"
E09-1040,1997.mtsummit-papers.22,0,0.0534507,"ssary. In addition, human in3.2 End-to-End Evaluation The evaluation in speech-to-speech translation jeopardises many concepts and implies a lot of subjectivity. Three components are involved and an overall system may grow the difficulty of estimating the output quality. However, two criteria are mainly accepted in the community: measuring the information preservation and determining how much of the translation is understandable. Several end-to-end evaluations in speech-tospeech translation have been carried out in the last few years, in projects such as JANUS (Gates et al., 1996), Verbmobil (Nübel, 1997) or TC-STAR (Hamon et al., 2007). Those projects use the main criteria depicted above, and protocols differ in terms of data preparation, rating, procedure, etc. 346 Source Acoustic Source Language Source Boundary Translation Target Language Model Model Model Model Model Audio Stream Speech Recognition Hypothesis Resegmen− Translatable Machine tation Segment Translation Source Translation Dictionary Vocabulary Translated Output Spoken (Synthesis) Output Text (Subtitles) Figure 1: Schematic overview and information flow of the simultaneous translation system. The main components of the system a"
E09-1040,2001.mtsummit-papers.68,0,0.0274216,"ted by rounded boxes. To our opinion, to evaluate the performance of a complete speech-to-speech translation system, we need to compare the source speech used as input to the translated output speech in the target language. To that aim, we reused a large part of the evaluation protocol from the TC-STAR project(Hamon et al., 2007). 4.2 Evaluation Protocol 4 Evaluation Tasks SLT evaluation. For the SLT evaluation, the automatically translated text from the ASR output is compared with two manual reference translations by means of automatic and human metrics. Two automatic metrics are used: BLEU (Papineni et al., 2001) and mWER (Niessen et al., 2000). For the human evaluation, each segment is evaluated in relation to adequacy and fluency (White and O’Connell, 1994). For the evaluation of adequacy, the target segment is compared to a reference segment. For the evaluation of fluency, the quality of the language is evaluated. The two types of evaluation are done independently, but each evaluator did both evaluations (first that of fluency, then that of adequacy) for a certain number of segments. For the evaluation of fluency, evaluators had to answer the question: “Is the text written in good Spanish?”. For th"
E09-1040,2007.mtsummit-papers.30,1,0.793882,"in3.2 End-to-End Evaluation The evaluation in speech-to-speech translation jeopardises many concepts and implies a lot of subjectivity. Three components are involved and an overall system may grow the difficulty of estimating the output quality. However, two criteria are mainly accepted in the community: measuring the information preservation and determining how much of the translation is understandable. Several end-to-end evaluations in speech-tospeech translation have been carried out in the last few years, in projects such as JANUS (Gates et al., 1996), Verbmobil (Nübel, 1997) or TC-STAR (Hamon et al., 2007). Those projects use the main criteria depicted above, and protocols differ in terms of data preparation, rating, procedure, etc. 346 Source Acoustic Source Language Source Boundary Translation Target Language Model Model Model Model Model Audio Stream Speech Recognition Hypothesis Resegmen− Translatable Machine tation Segment Translation Source Translation Dictionary Vocabulary Translated Output Spoken (Synthesis) Output Text (Subtitles) Figure 1: Schematic overview and information flow of the simultaneous translation system. The main components of the system are represented by cornered boxes"
E09-1040,H94-1024,0,\N,Missing
E09-1040,P02-1040,0,\N,Missing
E14-4009,2012.iwslt-papers.15,1,0.854925,"Missing"
E14-4009,2005.iwslt-1.19,0,0.343181,"Missing"
E14-4009,2013.iwslt-papers.12,1,0.876543,"Missing"
E14-4009,W09-0435,1,0.83048,"y the rough estimation in Equation 1, as disfluency removal does not depend on maximizing the translation quality itself. For example, we can consider the sentence Use what you build, build what you use. Due to its repetitive pattern in words and structure, the first clause is often detected as a disfluency using automatic means. To avoid this, we can change the scheme how the clean string is chosen as follows: eˆ = arg max(p(e|fc ) · p(fc |f )) e,fc CRF Model Training 3.3 Lattice Implementation We construct a word lattice which encodes longrange reordering variants (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). For translation we extend this so that potentially disfluent words can be skipped. A reordering lattice of the example sentence Das sind die Vorteile, die sie uh die sie haben. (En.gls: These are the advantages, that you uh that you have.) is shown in Figure 1, where words representing a disfluency are marked in bold letters. In this sentence, the part die sie uh was manually annotated as a disfluency, due to repetition and usage of a filler word. Table 1 shows the Pd obtained from the CRF model for each token. As expected, the words die sie uh obtain a high Pd from the CRF model. In order t"
E14-4009,W11-2124,1,0.893769,"System Description 5.1 The training data for our MT system consists of 1.76 million sentences of German-English parallel data. Parallel TED talks1 are used as in-domain data and our translation models are adapted to the domain. Before training, we apply preprocessing such as text normalization, tokenization, and smartcasing. Additionally, German compound words are split. To build the phrase table we use the Moses package (Koehn et al., 2007). An LM is trained on 462 million words in English using the SRILM Toolkit (Stolcke, 2002). In order to extend source word context, we use a bilingual LM (Niehues et al., 2011). We use an in-house decoder (Vogel, 2003) with minimum error rate training (Venugopal et al., 2005) for optimization. For training and testing the CRF model, we use 61k annotated words of manual transcripts of uni1 Manual Transcripts As a baseline for manual transcripts, we use the whole uncleaned data for development and test. For “No uh”, we remove the obvious filler words uh and uhm manually. In the CRF-hard experiment, the token is removed if the label output of the CRF model is a disfluency class. The fourth experiment uses the tight integration scheme, where new source paths which jump"
E14-4009,N09-1046,0,0.0244374,"cal, LM, and parser information features. While previous work has been limited to the postprocessing step of the automatic speech recogition (ASR) system, further approaches (Wang et al., 2010; Cho et al., 2013) use extended CRF features or additional models to clean manual speech transcripts and use them as input for an MT system. While ASR systems use lattices to encode hypotheses, lattices have been used for MT systems with various purposes. Herrmann et al. (2013) use lattices to encode different reordering variants. Lattices have also been used as a segmentation tactic for compound words (Dyer, 2009), where the segmentation is encoded as input in the lattice. One of the differences between our work and previous work is that we integrate the disfluency removal into an MT system. Our work is not limited to the preprocessing step of MT, instead we use the translation model to detect and remove disfluencies. Contrary to other systems where detection is limited on manual transcripts only, our sysWe train a CRF model to obtain a disfluency probability for each word. The SMT decoder will then skip the potentially disfluent word based on its disfluency probability. Using the suggested scheme, the"
E14-4009,P02-1040,0,0.0906842,"Missing"
E14-4009,E09-1030,0,0.381251,"spontaneous speech have been studied from various points of view. In the noisy channel model (Honal and Schultz, 2003), it is assumed that clean text without any disfluencies has passed through a noisy channel. The clean string is retrieved based on language model (LM) scores and five additional models. Another noisy channel approach involves a phrase-level statistical MT system, where noisy tokens are translated into clean tokens (Maskey et al., 2006). A tree adjoining grammar is combined with this noisy channel model in (Johnson and Charniak, 2004), using a syntactic parser to build an LM. Fitzgerald et al. (2009) present a method to detect speech disfluencies using a conditional random field (CRF) with lexical, LM, and parser information features. While previous work has been limited to the postprocessing step of the automatic speech recogition (ASR) system, further approaches (Wang et al., 2010; Cho et al., 2013) use extended CRF features or additional models to clean manual speech transcripts and use them as input for an MT system. While ASR systems use lattices to encode hypotheses, lattices have been used for MT systems with various purposes. Herrmann et al. (2013) use lattices to encode different"
E14-4009,2007.tmi-papers.21,0,0.0916531,"slation system is caused by the rough estimation in Equation 1, as disfluency removal does not depend on maximizing the translation quality itself. For example, we can consider the sentence Use what you build, build what you use. Due to its repetitive pattern in words and structure, the first clause is often detected as a disfluency using automatic means. To avoid this, we can change the scheme how the clean string is chosen as follows: eˆ = arg max(p(e|fc ) · p(fc |f )) e,fc CRF Model Training 3.3 Lattice Implementation We construct a word lattice which encodes longrange reordering variants (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). For translation we extend this so that potentially disfluent words can be skipped. A reordering lattice of the example sentence Das sind die Vorteile, die sie uh die sie haben. (En.gls: These are the advantages, that you uh that you have.) is shown in Figure 1, where words representing a disfluency are marked in bold letters. In this sentence, the part die sie uh was manually annotated as a disfluency, due to repetition and usage of a filler word. Table 1 shows the Pd obtained from the CRF model for each token. As expected, the words die sie uh obtain a high Pd from"
E14-4009,W13-0805,1,0.848869,"ntactic parser to build an LM. Fitzgerald et al. (2009) present a method to detect speech disfluencies using a conditional random field (CRF) with lexical, LM, and parser information features. While previous work has been limited to the postprocessing step of the automatic speech recogition (ASR) system, further approaches (Wang et al., 2010; Cho et al., 2013) use extended CRF features or additional models to clean manual speech transcripts and use them as input for an MT system. While ASR systems use lattices to encode hypotheses, lattices have been used for MT systems with various purposes. Herrmann et al. (2013) use lattices to encode different reordering variants. Lattices have also been used as a segmentation tactic for compound words (Dyer, 2009), where the segmentation is encoded as input in the lattice. One of the differences between our work and previous work is that we integrate the disfluency removal into an MT system. Our work is not limited to the preprocessing step of MT, instead we use the translation model to detect and remove disfluencies. Contrary to other systems where detection is limited on manual transcripts only, our sysWe train a CRF model to obtain a disfluency probability for e"
E14-4009,W05-0836,1,0.863742,"erman-English parallel data. Parallel TED talks1 are used as in-domain data and our translation models are adapted to the domain. Before training, we apply preprocessing such as text normalization, tokenization, and smartcasing. Additionally, German compound words are split. To build the phrase table we use the Moses package (Koehn et al., 2007). An LM is trained on 462 million words in English using the SRILM Toolkit (Stolcke, 2002). In order to extend source word context, we use a bilingual LM (Niehues et al., 2011). We use an in-house decoder (Vogel, 2003) with minimum error rate training (Venugopal et al., 2005) for optimization. For training and testing the CRF model, we use 61k annotated words of manual transcripts of uni1 Manual Transcripts As a baseline for manual transcripts, we use the whole uncleaned data for development and test. For “No uh”, we remove the obvious filler words uh and uhm manually. In the CRF-hard experiment, the token is removed if the label output of the CRF model is a disfluency class. The fourth experiment uses the tight integration scheme, where new source paths which jump over the potentially noisy words are inserted based on the disfluency probabilities assigned by the"
E14-4009,P07-2045,0,0.00544005,"to compare the effect of the tight integration with other disfluency removal strategies, we conduct different experiments on manual transcripts as well as on the ASR output. System Description 5.1 The training data for our MT system consists of 1.76 million sentences of German-English parallel data. Parallel TED talks1 are used as in-domain data and our translation models are adapted to the domain. Before training, we apply preprocessing such as text normalization, tokenization, and smartcasing. Additionally, German compound words are split. To build the phrase table we use the Moses package (Koehn et al., 2007). An LM is trained on 462 million words in English using the SRILM Toolkit (Stolcke, 2002). In order to extend source word context, we use a bilingual LM (Niehues et al., 2011). We use an in-house decoder (Vogel, 2003) with minimum error rate training (Venugopal et al., 2005) for optimization. For training and testing the CRF model, we use 61k annotated words of manual transcripts of uni1 Manual Transcripts As a baseline for manual transcripts, we use the whole uncleaned data for development and test. For “No uh”, we remove the obvious filler words uh and uhm manually. In the CRF-hard experime"
E14-4009,P04-1005,0,\N,Missing
eck-etal-2008-communicating,P98-1069,0,\N,Missing
eck-etal-2008-communicating,C98-1066,0,\N,Missing
eck-etal-2008-communicating,2007.iwslt-1.1,0,\N,Missing
eck-etal-2008-communicating,N07-1046,1,\N,Missing
eck-etal-2008-communicating,N06-1003,0,\N,Missing
eck-etal-2008-communicating,D07-1092,0,\N,Missing
eck-etal-2008-communicating,P07-1092,0,\N,Missing
eck-etal-2008-communicating,takezawa-etal-2002-toward,0,\N,Missing
eck-etal-2008-communicating,2005.iwslt-1.6,1,\N,Missing
eck-etal-2008-communicating,2007.iwslt-1.27,0,\N,Missing
eck-etal-2008-communicating,2005.iwslt-1.4,0,\N,Missing
H01-1001,H94-1049,0,0.0180605,"features adapted from Biber (1988) and contain mostly syntactic constructions and some word classes. Wordnet a total of 40 verb and noun classes (so called lexicographers classes (Fellbaum, 1998)) are defined and a word is replaced by the most frequent class over all possible meanings of the word. dialogue acts such as statements, questions, backchannels, . . . are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4 Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used. The tagger uses the code by Brill (1994). 5 The model was trained to be very portable and therefore the following choices were taken: (a) the dialogue model is context-independent and (b) only the part of speech are taken as the input to the model plus the 50 most likely word/part of speech types. dominance is described as the distribution of the speaker dominance in a conversation. The distribution is represented as a histogram and speaker dominance is measured as the average dominance of the dialogue acts (Linell et al., 1988) of each speaker. The dialogue acts are detected and the dominance is a numeric value assigned for each di"
H01-1001,J97-1002,0,0.0194489,"ags and the distribution can be seen in Tab. 1. The set of activities can be clustered into “interactive” activities of equal contribution rights (discussion,planning), one person being active (advising, information giving, story-telling), interrogations and all others. Measure κ Mutual inf. Meeting all inter 0.41 0.51 0.35 0.25 SBC all inter 0.49 0.56 0.65 0.32 CallHome Spanish 0.59 0.61 Table 2: Intercoder agreement for activities: The meeting dialogues and Santa Barbara corpus have been annotated by a semi-naive coder and the first author of the paper. The κ-coefficient is determined as in Carletta et al. (1997) and mutual information measures how much one label “informs” the other (see Sec. 3). For CallHome Spanish 3 dialogues were coded for activities by two coders and the result seems to indicate that the task was easier. Both datasets have been annotated not only by a seminaive annotator but also by the first author of the paper. The results for κ-statistics (Carletta et al., 1997) and mutual information between the coders can be seen in Tab. 2. The intercoder agreement would be considered moderate but compares approximately to Carletta et al. (1997) agreement on transactions (κ = 0.59), especial"
H01-1001,P97-1005,0,0.0409531,"age modeling tutorial for Tim Setting up the new hard drive Willie is sick Need new coding scheme Personal stuff Figure 1: Information access hierarchy: Oral communications take place in very different formats and the first step in the search is to determine the database (or sub-database) of the rejoinder. The next step is to find the specific rejoinder. Since rejoinders can be very long the rejoinder has to segmented and a segment has to be selected. While keywords are commonly used in information access to written information the use of other indices such as style is still uncommon (but see Kessler et al. (1997); van Bretan et al. (1998)). Oral communication is richer than written communication since it is an interactive real time accomplishment between participants, may involve speech gestures such as the display of emotion and is situated in space and time. Bahktin (1986) characterizes a conversation by topic, situation and style. Information access to oral communication can therefore make use of indices that pertain to the oral nature of the discourse (Fig. 2). Indices other than topic (represented by keywords) increase in importance since browsing audio documents is cumbersome which makes the com"
H01-1001,J00-3003,1,0.537172,"tion are words the 50 most frequent words / part of speech pairs are used directly, all other pairs are replaced by their part of speech 4 . stylistic features adapted from Biber (1988) and contain mostly syntactic constructions and some word classes. Wordnet a total of 40 verb and noun classes (so called lexicographers classes (Fellbaum, 1998)) are defined and a word is replaced by the most frequent class over all possible meanings of the word. dialogue acts such as statements, questions, backchannels, . . . are detected using a language model based detector trained on Switchboard similar to Stolcke et al. (2000) 5 4 Klaus Zechner trained an English part of speech tagger tagger on Switchboard that has been used. The tagger uses the code by Brill (1994). 5 The model was trained to be very portable and therefore the following choices were taken: (a) the dialogue model is context-independent and (b) only the part of speech are taken as the input to the model plus the 50 most likely word/part of speech types. dominance is described as the distribution of the speaker dominance in a conversation. The distribution is represented as a histogram and speaker dominance is measured as the average dominance of the"
H01-1001,ries-etal-2000-shallow,1,\N,Missing
H01-1007,W00-0203,1,0.70336,"reside locally on the PC of the customer. This implies a server-type architecture in which speech recognition and translation are accomplished via interaction with a dedicated server. The extent to which this server is centralized or distributed is one of the major design considerations taken into account in our system. 2. NESPOLE! INTERLINGUA-BASED TRANSLATION APPROACH Our translation approach builds upon previous work that we have conducted within the context of the C-STAR consortium. We use an interlingua-based approach with a relatively shallow task-oriented interlingua representation [2] [1], that was initially designed for the C-STAR consortium and has been significantly extended for the NESPOLE! project. Interlingual machine translation is convenient when more than two languages are involved because it does not require each language to be connected by a set of transfer rules to each other language in each direction [3]. Adding a new language that has all-ways translation with existing languages requires only writing one analyzer that maps utterances into the interlingua and one generator that maps interlingua representations into sentences. The interlingua approach also allows"
H01-1048,woszczcyna-etal-1998-modular,1,\N,Missing
H01-1048,W97-0618,0,\N,Missing
H01-1071,C96-1030,0,0.15565,"stem relies on three key technologies: sign extraction, optical character recognition (OCR), and language translation. Although much research has been directed to automatic speech recognition, handwriting recognition, OCR, speech and text translation, little attention has been paid to automatic sign recognition and translation in the past. Our current research is focused on automatic sign detection and translation while taking advantage of OCR technology available. We have developed robust automatic sign detection algorithms. We have applied Example Based Machine Translation (EBMT) technology [1] in sign translation. Fully automatic extraction of signs from the environment is a challenging problem because signs are usually embedded in the environment. Sign translation has some special problems compared to a traditional language translation task. They can be location dependent. The same text on different signs can be treated differently. For example, it is not necessary to translate the meanings for names, such as street names or company names, in most cases. In the system development, we use a user-centered approach. The approach takes advantage of human intelligence in selecting an a"
H01-1071,C00-1019,0,0.0475912,"Missing"
H01-1071,H93-1040,0,0.0307104,"Missing"
H01-1071,hogan-frederking-1998-evaluation,0,\N,Missing
herrmann-etal-2014-manual,J11-4002,0,\N,Missing
herrmann-etal-2014-manual,W12-3102,0,\N,Missing
herrmann-etal-2014-manual,P02-1040,0,\N,Missing
herrmann-etal-2014-manual,W09-0435,1,\N,Missing
herrmann-etal-2014-manual,P11-4010,0,\N,Missing
herrmann-etal-2014-manual,W13-0805,1,\N,Missing
herrmann-etal-2014-manual,stuker-etal-2012-kit,1,\N,Missing
herrmann-etal-2014-manual,federico-etal-2012-iwslt,1,\N,Missing
herrmann-etal-2014-manual,W13-2201,0,\N,Missing
herrmann-etal-2014-manual,vilar-etal-2006-error,0,\N,Missing
I11-1053,P07-2045,0,0.0025733,"active hiker} and a verb list SV {comes from, works for, is}. Our model constructs simple sentences such as “John comes from England” , “John comes from IMF” and “John comes from an active hiker”. The total number of simple sentences, |S|, is 48. 4.2 IMF is an active hiker Figure 2: Left-right decoding by objects This section presents a solution to the decoding problem. The solution is based on a stack decoding algorithm that finds the best S given an English sentence e. Our decoding algorithm is inspired by the decoding algorithms in speech recognition and machine translation (Jelinek, 1998; Koehn et al., 2007). For example, with a sentence e “John comes from England, works for IMF, and is an active hiker”, the stack decoding algorithm tries to find S, which is a set of three sentences: “John comes from England”, “John works for IMF” and “John is an active hiker”. Note that S is a set of k simple sentences S = {s1 , ..., si , ..., sk }. We can assume the items si are drawn from a finite set S of grammatical sentences that can be derived from e. Therefore, the first step is to construct the set S. 4.1 # John is an active hiker an active hiker Figure 1: Constructing simple sentences 4 # an active hi"
I11-1053,P06-1096,0,0.0987345,"Missing"
I11-1053,W04-1013,0,0.0157446,"incorrect output. In line 6, α can be interpreted as an update step size; when α is a large number we want to update our weights aggressively, otherwise weights are 477 updated conservatively. α is computed as follow: L(eo , eh ; εt ) = AveFN (eo , εt ) − AveFN (eh , εt ) (4) where AveFN (eo , εt ) and AveFN (eh , εt ) is the average n-gram (n=[2:N]) cooccurrence F-score of (eo , εt ) and (eh , εt ), respectively. In this case, we optimize the weights directly against the AveFN metric over the training data. AveFN can be substituted by other evaluation metrics such as the ROUGE family metric (Lin, 2004). Similar to the perceptron method, the actual weight vector during decoding is averaged across the number of iterations and training instances; and it is computed in line 11. connection. One way to possibly reduce this kind of mistake is analyze the dependency chain between S, V, and O on the original dependency tree of e. Our dependency structure features include the minimum and maximum distances of (S:O), (S:V), and (V:O). Syntactic Structures Another source of information is the syntactic parse tree of e, which can be used to extract syntactic features. The sentence-like boundary feature c"
I11-1053,de-marneffe-etal-2006-generating,0,0.00934639,"rage all scores (Flesch, 1948; Gunning, 1968; McLaughlin, 1969; Kincaid et al., We now turn to the modeling problem. Our fundamental question is: given the model in Equation 1 with M feature functions, what linguistic features can be leveraged to capture semantic information of the original sentence? We address the question in this section by describing features that cover different levels of linguistic structures. Our model incorporates 177 features based on information from the original English sentence e which contains chunks, syntactic and dependency parse trees (Ramshaw and Marcus, 1995; Marneffe et al., 2006). 6.1 Interactive simple sentence features Simple sentence level features A simplification hypothesis s contains k simple sentences. Therefore, it is crucial that our model chooses reasonable simple sentences to form a hypothesis. For each simple sentence si we incorporated the following feature functions: Word Count These features count the number word in subject (S), verb (V) and object (O), also counting the number of proper nouns in S and the number of proper nouns in O. Distance between NPs and Verbs These features focus on the number of NPs and VPs in between S, V and O. This feature gro"
I11-1053,P05-1012,0,0.0483633,"e training set (St , et ) and updates the weights so that the score of the correct simplification εt is greater than the score of all other simplifications by a margin proportional to their loss. However, given a sentence there are an exponential amount of possible simplification candidates. Therefore, the optimizer has to deal with an exponentially large number of constraints. To tackle this, we only consider K-best hypotheses and choose m-oracle hypotheses to support the weight update decision. This idea is similar to the way MIRA has been used in dependency parsing and machine translation (McDonald et al., 2005; Liang et al., 2006; Watanabe et al., 2007). Learning Since defining a log-linear sentence simplification model and decoding algorithm has been completed, this section describes a discriminative learning algorithm for the learning problem. We learn optimized weight vector w by using the Margin Infused Relaxed Algorithm or MIRA (Crammer and Singer, 2003), which is an online learner closely related to both the support vector machine and perceptron learning framework. In general, weights are updated at each step time On each update, MIRA attempts to keep the new weight vector as close as possibl"
I11-1053,W10-1001,0,0.0720269,"s. There have been readability tests such as Flesch, Gunning-Fog, SMOG, Flesch-Kincaid, etc. (Flesch, 1948; Gunning, 1968; McLaughlin, 1969; Kincaid et al., 1975). In this work, we will use FleschKincaid grade level which can be interpret as the number of years of education generally required to understand a text. Furthermore, automatic evaluation of summaries has also been explored recently. The work of Lin (2004) on the ROUGE family metric is perhaps the best known study of automatic summarization evaluation. Other methods have been proposed such as Pyramid (Nenkova et al., 2007). Recently, Aluisio et al. (2010) proposed readability assessment for sentence simplification. Our models are optimized toward AveF10 , which is the average F-score of n-gram concurrence between hypothesis and reference in which n is from 2 to 10. Besides AveF10 , we will report automatic evaluation scores on the unseen test set in Flesch-Kincaid grade level, ROUGE-2 and ROUGE-4. When we evaluate on a test set, a score will be reported as the average score per sentence. 7.3 Model behaviors How well does our system learn from the labeled corpus? To answer this question we investigate the interactions of model and decoder hyper"
I11-1053,E06-1038,0,0.0244931,"ocument summarization, which produces a sentence that conveys common information of multiple sentences based upon dependency tree structures and lexical similarity. Sentence compression generates a summary of a single sentence with minimal information loss, which can also be treated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charniak, 2005) to discriminative models (McDonald, 2006) and Integer Linear Programming (Clarke, 2008). Another line of research treats sentence compression as machine translation, in which tree-based translation models have been developed (Galley and McKeown, 2007; Cohn and Lapata, 2008; Zhu et al., 2010). Recently, Woodsend and Lapata (2011) proposed a framework to combine treebased simplification with ILP. In contrast to sentence compression, sentence simplification generates multiple sentences from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to transform sentences in complicated structur"
I11-1053,J05-3002,0,0.0260945,"to the decoding process as soft constraints in order to explore a much larger search space. Related Work Given the problematic nature of text-to-text generation that takes a sentence or a document as the input and optimizes the output toward a certain objective, we briefly review state-of-art approaches of text-to-text generation methods. Early approaches in summarization focus on extraction methods which try to isolate and then summarize the most significant sentences or paragraphs of the text. However, this has been found to be insufficient because it usually generates incoherent summaries. Barzilay and McKeown (2005) proposed sentence fusion for multi-document summarization, which produces a sentence that conveys common information of multiple sentences based upon dependency tree structures and lexical similarity. Sentence compression generates a summary of a single sentence with minimal information loss, which can also be treated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charn"
I11-1053,W10-0406,0,0.109654,"Missing"
I11-1053,C96-2183,0,0.435585,"proposed system demonstrates an improvement of 0.2, 0.6, and 4.5 points in ROUGE-2, ROUGE-4, and AveF10 , respectively. 1 Introduction Complicated sentences impose difficulties on reading comprehension. For instance, a person in 5th grade can comprehend a comic book easily but will struggle to understand New York Times articles which require at least 12th grade average reading level (Flesch, 1981). Complicated sentences also challenge natural language processing applications including, but not limited to, text summarization, question answering, information extraction, and machine translation (Chandrasekar et al., 1996). An example of this is syntactic parsing in which long and complicated sentences will generate a large number of hypotheses and usually fail in disambiguating the attachments. Therefore, it is desirable to pre-process complicated sentences and generate simpler counter parts. There are direct applications of sentence simplification. Dalemans et al. (2004) applied sentence simplification so that the automatically generated closed caption can fit into limited display area. The Facilita system generates accessible content from Brazilian Portuguese web pages for low literacy readers using both sum"
I11-1053,W95-0107,0,0.0384332,"readability index, and average all scores (Flesch, 1948; Gunning, 1968; McLaughlin, 1969; Kincaid et al., We now turn to the modeling problem. Our fundamental question is: given the model in Equation 1 with M feature functions, what linguistic features can be leveraged to capture semantic information of the original sentence? We address the question in this section by describing features that cover different levels of linguistic structures. Our model incorporates 177 features based on information from the original English sentence e which contains chunks, syntactic and dependency parse trees (Ramshaw and Marcus, 1995; Marneffe et al., 2006). 6.1 Interactive simple sentence features Simple sentence level features A simplification hypothesis s contains k simple sentences. Therefore, it is crucial that our model chooses reasonable simple sentences to form a hypothesis. For each simple sentence si we incorporated the following feature functions: Word Count These features count the number word in subject (S), verb (V) and object (O), also counting the number of proper nouns in S and the number of proper nouns in O. Distance between NPs and Verbs These features focus on the number of NPs and VPs in between S, V"
I11-1053,C08-1018,0,0.106052,"with minimal information loss, which can also be treated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charniak, 2005) to discriminative models (McDonald, 2006) and Integer Linear Programming (Clarke, 2008). Another line of research treats sentence compression as machine translation, in which tree-based translation models have been developed (Galley and McKeown, 2007; Cohn and Lapata, 2008; Zhu et al., 2010). Recently, Woodsend and Lapata (2011) proposed a framework to combine treebased simplification with ILP. In contrast to sentence compression, sentence simplification generates multiple sentences from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to transform sentences in complicated structures to a set of easy-to-read sentences, which will be easier for human to comprehend, and hopefully easier for computers to deal with. Numerous attempts have been made to tackle the sentence simplification problem. One line of resear"
I11-1053,P05-1036,0,0.185035,"d McKeown (2005) proposed sentence fusion for multi-document summarization, which produces a sentence that conveys common information of multiple sentences based upon dependency tree structures and lexical similarity. Sentence compression generates a summary of a single sentence with minimal information loss, which can also be treated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charniak, 2005) to discriminative models (McDonald, 2006) and Integer Linear Programming (Clarke, 2008). Another line of research treats sentence compression as machine translation, in which tree-based translation models have been developed (Galley and McKeown, 2007; Cohn and Lapata, 2008; Zhu et al., 2010). Recently, Woodsend and Lapata (2011) proposed a framework to combine treebased simplification with ILP. In contrast to sentence compression, sentence simplification generates multiple sentences from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to t"
I11-1053,D07-1080,0,0.0242227,"eights so that the score of the correct simplification εt is greater than the score of all other simplifications by a margin proportional to their loss. However, given a sentence there are an exponential amount of possible simplification candidates. Therefore, the optimizer has to deal with an exponentially large number of constraints. To tackle this, we only consider K-best hypotheses and choose m-oracle hypotheses to support the weight update decision. This idea is similar to the way MIRA has been used in dependency parsing and machine translation (McDonald et al., 2005; Liang et al., 2006; Watanabe et al., 2007). Learning Since defining a log-linear sentence simplification model and decoding algorithm has been completed, this section describes a discriminative learning algorithm for the learning problem. We learn optimized weight vector w by using the Margin Infused Relaxed Algorithm or MIRA (Crammer and Singer, 2003), which is an online learner closely related to both the support vector machine and perceptron learning framework. In general, weights are updated at each step time On each update, MIRA attempts to keep the new weight vector as close as possible to the old weight vector. Subject to margi"
I11-1053,N07-1023,0,0.0210113,"mmary of a single sentence with minimal information loss, which can also be treated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charniak, 2005) to discriminative models (McDonald, 2006) and Integer Linear Programming (Clarke, 2008). Another line of research treats sentence compression as machine translation, in which tree-based translation models have been developed (Galley and McKeown, 2007; Cohn and Lapata, 2008; Zhu et al., 2010). Recently, Woodsend and Lapata (2011) proposed a framework to combine treebased simplification with ILP. In contrast to sentence compression, sentence simplification generates multiple sentences from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to transform sentences in complicated structures to a set of easy-to-read sentences, which will be easier for human to comprehend, and hopefully easier for computers to deal with. Numerous attempts have been made to tackle the sentence simplification prob"
I11-1053,D11-1038,0,0.170634,"eated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charniak, 2005) to discriminative models (McDonald, 2006) and Integer Linear Programming (Clarke, 2008). Another line of research treats sentence compression as machine translation, in which tree-based translation models have been developed (Galley and McKeown, 2007; Cohn and Lapata, 2008; Zhu et al., 2010). Recently, Woodsend and Lapata (2011) proposed a framework to combine treebased simplification with ILP. In contrast to sentence compression, sentence simplification generates multiple sentences from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to transform sentences in complicated structures to a set of easy-to-read sentences, which will be easier for human to comprehend, and hopefully easier for computers to deal with. Numerous attempts have been made to tackle the sentence simplification problem. One line of research has explored simplification with linguistic rules. Jon"
I11-1053,C10-1152,0,0.4484,"on loss, which can also be treated as sentence-level summarization. This approach applies word deletion, in which non informative words will be removed from the original sentence. A variety of models were developed based on this perspective, ranging from generative models (Knight and Marcu, 2002; Turner and Charniak, 2005) to discriminative models (McDonald, 2006) and Integer Linear Programming (Clarke, 2008). Another line of research treats sentence compression as machine translation, in which tree-based translation models have been developed (Galley and McKeown, 2007; Cohn and Lapata, 2008; Zhu et al., 2010). Recently, Woodsend and Lapata (2011) proposed a framework to combine treebased simplification with ILP. In contrast to sentence compression, sentence simplification generates multiple sentences from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to transform sentences in complicated structures to a set of easy-to-read sentences, which will be easier for human to comprehend, and hopefully easier for computers to deal with. Numerous attempts have been made to tackle the sentence simplification problem. One line of research has explored sim"
I11-1053,N09-2045,0,0.0309621,"from one input sentence and tries to preserve the meaning of the original sentence. The major objective is to transform sentences in complicated structures to a set of easy-to-read sentences, which will be easier for human to comprehend, and hopefully easier for computers to deal with. Numerous attempts have been made to tackle the sentence simplification problem. One line of research has explored simplification with linguistic rules. Jonnalagadda (2006) developed a rule-based system that take into account the discourse information. This method is applied on simplification of biomedical text (Jonnalagadda et al., 2009) and protein-protein information extraction (Jonnalagadda and Gonzalez, 2010). Chandrasekar and Srinivas (1997) automatically induced simplification rules based on dependency trees. Additionally, Klebanov et al. (2004) develop a set of rules that generate a set of EAS from syntactically complicated sentences. Heilman and Smith (2010) proposed an algorithm for extracting simplified declarative sentences from syntactically complex sentences. The rule-based systems performs well on English. 3 Statistical Sentence Simplification with Log-linear Models Assume that we are given an English sentence e"
I11-1053,daelemans-etal-2004-automatic,0,\N,Missing
L16-1293,E09-1040,1,0.817996,"Missing"
L16-1314,bazillon-etal-2008-manual,0,0.0834192,"Missing"
L16-1314,D14-1172,0,0.0283067,"ther differences are caused by transcriber characteristics or by experimental settings. More generally, our experiments involve “random” factors that are difficult to control for, and that potentially have a significant influence on our observations. In fact, this is a common problem in user studies. Recently, linear mixed-effects models 2 www.msperber.com/research/lrec-iterative-gui (short: mixed models) have become popular as a convenient way of dealing with such situations. For instance, mixed models have been used for error analysis in ASR (Goldwater et al., 2010) and machine translation (Federico et al., 2014), and for analysis of post-editing for translation (Green et al., 2013). Mixed models are specified by the following components: • Response variable: The central quantity for which we wish to determine how it is influenced by other measured covariates. In our experiments, this will be the post-correction error rate or the transcription time. • Fixed effects: Numerical or categorical attributes that influence the response variable in a meaningful way. In this paper, we assume a linear relationship. In the case of categorical variables, the assumption is that the observations include all values"
L16-1314,2012.iwslt-papers.10,1,0.873966,"Missing"
L16-1314,N10-1024,0,0.169441,"Missing"
L16-1314,Q14-1014,1,0.861183,"Our goal in this paper is to design a computer-assisted transcription user interface such that the outcome quality is optimized while avoiding unnecessary effort. The key interface feature we investigate is support for iterative transcription. This term is borrowed from iterative human computation processes (Little et al., 2010), in which humans solve tasks by improving upon a previously obtained solution. We consider computer-assisted transcription performed in an efficient segment-by-segment fashion, where only lowconfidence segments are selected for manual transcription (Roy and Roy, 2009; Sperber et al., 2014b). Our iterative interfaces then provide the initial transcription as created by the ASR as a starting point for each segment, upon which the transcriber improves (cf. Figure 1). The benefit of the iterative interfaces is that the transcriber can simply use the initially correct parts from the ASR as-is, and focus attention on the problematic parts. Ideally, words that were recognized correctly by the ASR will not be changed, reducing the chance of correction errors. In addition, the iterative approach can assist transcription of parts that are difficult for the transcriber to understand by p"
L18-1318,W17-4705,0,0.0255393,"y introduced modification process are targeting the same overall task, the performed inference on the data varies. The LAMBADA inference task targets the continuation of the current text passage at a known position in the text by predicting the last word within the paragraph. In contrast to that, our newly introduced substitution process replaces contextrelevant words within the text passage at arbitrary positions, increasing the complexity of the task. Sennrich (2016) introduces a dataset with automatically inserted errors focusing on advanced computational models for NMT tasks. The paper by Burlot and Yvon (2017) proposes an evaluation process for NMT models, assessing the morphological properties of a system. The process substitutes nouns, as well as other part-of-speech tokens with filtered and randomly chosen replacement words. 3. Task The task introduced in this paper is designed to evaluate the performance of computational models for out-of-context error detections. The fully automated modification process described in section 4. provides the ground truth for the task. The artificially inserted out-of-context tokens are uniformly distributed over the dataset, elevating the complexity of the task"
L18-1318,2012.eamt-1.60,0,0.0206891,"Missing"
L18-1318,W14-4012,0,0.012825,"Missing"
L18-1533,W14-2201,0,0.0327313,"Missing"
L18-1533,L18-1531,1,0.779497,"Missing"
N03-4015,W02-0717,1,\N,Missing
N03-4015,lavie-etal-2002-nespole,1,\N,Missing
N04-3010,H01-1018,1,0.895093,"Missing"
N07-2006,koen-2004-pharaoh,0,0.0772576,"adation as much as possible. We will not specifically address the computing power limitations of the portable devices in this paper. 1 A “phrase” here can also refer to a single word. Small language models are also desirable and the approaches could be applied as well but this was not investigated yet. 2 21 Proceedings of NAACL HLT 2007, Companion Volume, pages 21–24, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics 2 Previous work Previous work mainly introduced two natural ideas to prune phrase pairs. Both are for example directly available in the Pharaoh decoder (Koehn, 2004). Probability threshold A very simple way to prune phrase pairs from a translation model is to use a probability threshold and remove all pairs for which the translation probability is below the threshold. The reasoning for this is that it is very unlikely that a translation with a very low probability will be chosen (over another translation candidate with a higher probability). Translation variety threshold Another way to prune phrase pairs is to impose a limit on the number of translation candidates for a certain phrase. That means the pruned translation model can only have equal or fewer p"
N07-2006,P00-1056,0,0.130907,"Generally statistical machine translation systems have recently outperformed other translation approaches so it seems natural to also apply them in these scenarios. A main component of every statistical machine translation system is the translation model. The translation model assigns translation probabilities to phrase1 pairs of source and target phrases extracted from a parallel bilingual text. These phrase pairs are applied during the decoding process and their target sides are combined to form the final translation. A variety of algorithms to extract phrase pairs has been proposed. (e.g. Och and Ney, 2000 and Vogel, 2005). Our proposed approach now tries to remove phrase pairs, which have little influence on the final translation performance, from a translation system (pruning of the translation model2). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, while not impacting the translation performance too heavily. The approach does not depend on the actual algorithm used to extract the phrase pairs and can be applied to every imaginable method that assigns probabilities to phrase pairs. We assume that the phrase pairs were pre-e"
N07-2006,P02-1040,0,0.0713786,"Missing"
N07-2006,takezawa-etal-2002-toward,0,0.0273506,"Missing"
N07-2006,2005.mtsummit-papers.33,1,0.877773,"machine translation systems have recently outperformed other translation approaches so it seems natural to also apply them in these scenarios. A main component of every statistical machine translation system is the translation model. The translation model assigns translation probabilities to phrase1 pairs of source and target phrases extracted from a parallel bilingual text. These phrase pairs are applied during the decoding process and their target sides are combined to form the final translation. A variety of algorithms to extract phrase pairs has been proposed. (e.g. Och and Ney, 2000 and Vogel, 2005). Our proposed approach now tries to remove phrase pairs, which have little influence on the final translation performance, from a translation system (pruning of the translation model2). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, while not impacting the translation performance too heavily. The approach does not depend on the actual algorithm used to extract the phrase pairs and can be applied to every imaginable method that assigns probabilities to phrase pairs. We assume that the phrase pairs were pre-extracted before d"
N07-2006,2005.eamt-1.39,1,0.873456,"uence on the final translation performance, from a translation system (pruning of the translation model2). The goal is to reduce the number of phrase pairs and in turn the memory requirement of the whole translation system, while not impacting the translation performance too heavily. The approach does not depend on the actual algorithm used to extract the phrase pairs and can be applied to every imaginable method that assigns probabilities to phrase pairs. We assume that the phrase pairs were pre-extracted before decoding. (in contrast to the proposed approaches to “online phrase extraction” (Zhang and Vogel, 2005; Callison-Burch et al., 2005)). The task now is to remove enough pre-extracted phrase pairs in order to accommodate the possibly strict memory limitations of a portable device while restricting performance degradation as much as possible. We will not specifically address the computing power limitations of the portable devices in this paper. 1 A “phrase” here can also refer to a single word. Small language models are also desirable and the approaches could be applied as well but this was not investigated yet. 2 21 Proceedings of NAACL HLT 2007, Companion Volume, pages 21–24, c Rochester, NY, A"
N07-2006,2004.iwslt-evaluation.1,0,\N,Missing
N07-2006,P05-1032,0,\N,Missing
N07-2006,2005.iwslt-1.6,1,\N,Missing
N09-2038,2007.iwslt-1.4,1,0.789571,"adapt the system based on its usage automatically without having to ship data back to the laboratory for retraining. This paper investigates the scenario of a two-day event. We wish to improve the system for the second day based on the data collected on the first day. Our system is designed for eyes-free use and hence provides no graphical user interface. This allows the user to concentrate on his surrounding environment during an operation. The system only provides audio control and feedback. Additionally the system operates on a push-totalk method. Previously the system (Hsiao et al., 2006; Bach et al., 2007) needed 2 buttons to operate, one for the English speaker and the other one for the Iraqi speaker. To make the system easier and faster to use, we propose to use a single button which can be controlled by the English speaker. We mounted a microphone and a Wii remote controller together as shown in 1. Since the Wii controller has an accelerometer which can be used to detect the orientation of the controller, this feature can be applied to identify who is speaking. When the English speaker points towards himself, the system will switch to English-Iraqi translation. However, when the Wii is point"
N09-2038,P08-2040,0,0.0125219,"0K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translation references; 2) re-tune the translation system on day 1 via minimum error rate training (MERT) (Venugopal and Vogel, 2005). Use 500 Best MT Hypos Day 1 Day 2 Baseline 29.39 27.41 1gramLM 2gramLM 3gramLM 29.18 29.53 29.36 27.23 27.50 27.23 The first question we would like to address is whether our adaptation obtains improvements via an unsupervised manner. We take day 1 baseline ASR hypothesis and use the baseline SMT to get the MT hypothesis and a 500bes"
N09-2038,eck-etal-2004-language,1,0.856377,"d and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. 4 SMT Adaptation The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translation references; 2) re-tune the translation system on day 1 via minimum error rate training (MERT) (Venugopal and Vogel, 2005). Use 500 Best MT Hypos Day 1 Day 2 Baseline 29.39 27.41 1gramLM 2gramLM 3gra"
N09-2038,P02-1040,0,0.0766368,"FCC and we concatenate adjacent 15 frames and perform LDA to reduce the dimension to 42 for the final feature vectors. The language model of the ASR system is a trigram LM trained on the audio transcripts with around three million words with Kneser-Ney smoothing (Stolcke, 2002). To perform LM adaptation for the ASR system, we use the ASR hypotheses from day 1 to build a LM. This LM is then interpolated with the original trigram LM to produce an adapted LM for day 2. We also evaluate the effect 150 Table 2 shows the impact of ASR adaptation on the performance of the translation system in BLEU (Papineni et al., 2002). In these experiments we only performed adaptation on ASR and still using the baseline SMT component. There is no obvious difference between unsupervised and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. 4 SMT Adaptation The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one a"
N09-2038,2005.eamt-1.36,1,0.647983,"exity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translation references; 2) re-tune the translation system on day 1 via minimum error rate training (MERT) (Venugopal and Vogel, 2005). Use 500 Best MT Hypos Day 1 Day 2 Baseline 29.39 27.41 1gramLM 2gramLM 3gramLM 29.18 29.53 29.36 27.23 27.50 27.23 The first question we would like to address is whether our adaptation obtains improvements via an unsupervised manner. We take day 1 baseline ASR hypothesis and use the baseline SMT to get the MT hypothesis and a 500best list. We train a domain LM using the 500-best list and use the MT hypotheses as the reference in MERT. We treat day 1 as a development set and day 2 as an unseen test set. In Table 3 we compare the performance of four systems: the baseline which does not have an"
N09-2038,2005.mtsummit-papers.33,1,0.693382,"impact of ASR adaptation on the performance of the translation system in BLEU (Papineni et al., 2002). In these experiments we only performed adaptation on ASR and still using the baseline SMT component. There is no obvious difference between unsupervised and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. 4 SMT Adaptation The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list o"
N09-2038,2005.eamt-1.39,1,0.826207,"he translation system in BLEU (Papineni et al., 2002). In these experiments we only performed adaptation on ASR and still using the baseline SMT component. There is no obvious difference between unsupervised and supervised ASR adaptation on performance of SMT on day 2. However, we can see that the difference in WER on day 2 of unsupervised and supervised ASR adaptation is relatively small. 4 SMT Adaptation The Iraqi-English SMT system is trained with around 650K sentence pairs collected under the TransTac program. We used PESA phrase extraction (Vogel, 2005) and a suffix array language model (Zhang and Vogel, 2005). To adapt SMT components one approach is to optimize LM interpolation weights by minimizing perplexity of the 1-best translation output (Bulyko et al., 2007). Related work including (Eck et al., 2004) attempts to use information retrieval to select training sentences similar to those in the test set. To adapt the SMT components we use a domain-specific LM on top of the background language models. This approach is similar to the work in (Chen et al., 2008). sThe adaptation framework is 1) create a domain-specific LM via an n-best list of day 1 machine translation hypothesis, or day 1 translati"
N16-3017,W09-0435,1,0.690583,"cribed using the Janus Recognition Toolkit (JRTk) (Woszczyna et al., 1994), which features the IBIS single-pass decoder (Soltau et al., 2001). The acoustic model was trained using several hundred hours of recordings from lectures and talks. Figure 1: User interface of the Lecture Translator showing an ongoing session For translation, we used a phrase-based decoder (Vogel, 2003). It uses advanced models for domain adaptation, bilingual and cluster language models in addition to Discriminative Word Lexica for producing the translation. We use POS-based word reordering (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). The translation model was trained on 1.8 million sentences of parallel data. It includes data from various sources and in-domain data. 4.2 System Operation The LT is in regular use for multiple years now and currently translates approx. 10 different lectures per term. We have installed this system in multiple lecture halls, among them KIT’s largest hall, called “Audimax”. In each hall, the system is tightly integrated in the PA to ensure smooth operation. The audio is captured via the PA from the microphone that the lecturer uses to address the audience. The operation of the system itself is"
N16-3017,P14-2090,0,0.0233272,"pite a difference in the overall quality of the translations, MT systems suffer from not being able to anticipate context like human interpreters. MT systems are unable to do so because of the lack of background and context knowledge. This results in a higher delay of the translation. But there has been some research towards the reduction of the latency and the translation of incomplete utterances (F¨ugen and Kolss, 2007), (Sridhar et al., 2013), (Oda et al., 83 2015). The goal is to find the optimal threshold between quality and latency (Shavarani et al., 2015), (Yarmohammadi et al., 2013), (Oda et al., 2014). With ongoing research and development, the systems have matured over the years. In order to assess whether our system helps students to better understand lectures, we have conducted a user study (M¨uller et al., 2016) (to appear). The outcome was that students actually benefit from our system. 3 Speech Translation Framework The Speech Translation Framework used for the lecture translation system is a component based architecture. It is designed to be flexible and distributed. There are 3 types of components: A central server, called the “mediator”, “workers” for performing different tasks an"
N16-3017,P15-1020,0,0.0438838,"Missing"
N16-3017,2007.tmi-papers.21,0,0.0215344,". The audio is being transcribed using the Janus Recognition Toolkit (JRTk) (Woszczyna et al., 1994), which features the IBIS single-pass decoder (Soltau et al., 2001). The acoustic model was trained using several hundred hours of recordings from lectures and talks. Figure 1: User interface of the Lecture Translator showing an ongoing session For translation, we used a phrase-based decoder (Vogel, 2003). It uses advanced models for domain adaptation, bilingual and cluster language models in addition to Discriminative Word Lexica for producing the translation. We use POS-based word reordering (Rottmann and Vogel, 2007; Niehues and Kolss, 2009). The translation model was trained on 1.8 million sentences of parallel data. It includes data from various sources and in-domain data. 4.2 System Operation The LT is in regular use for multiple years now and currently translates approx. 10 different lectures per term. We have installed this system in multiple lecture halls, among them KIT’s largest hall, called “Audimax”. In each hall, the system is tightly integrated in the PA to ensure smooth operation. The audio is captured via the PA from the microphone that the lecturer uses to address the audience. The operati"
N16-3017,2015.iwslt-papers.14,0,0.0823231,"Missing"
N16-3017,N13-1023,0,0.030049,"e very domain specific and formalized dialogues. Later, systems supported greater variety in language, but were still built for specific domains (St¨uker et al., 2007). Despite a difference in the overall quality of the translations, MT systems suffer from not being able to anticipate context like human interpreters. MT systems are unable to do so because of the lack of background and context knowledge. This results in a higher delay of the translation. But there has been some research towards the reduction of the latency and the translation of incomplete utterances (F¨ugen and Kolss, 2007), (Sridhar et al., 2013), (Oda et al., 83 2015). The goal is to find the optimal threshold between quality and latency (Shavarani et al., 2015), (Yarmohammadi et al., 2013), (Oda et al., 2014). With ongoing research and development, the systems have matured over the years. In order to assess whether our system helps students to better understand lectures, we have conducted a user study (M¨uller et al., 2016) (to appear). The outcome was that students actually benefit from our system. 3 Speech Translation Framework The Speech Translation Framework used for the lecture translation system is a component based architectu"
N16-3017,L16-1293,1,\N,Missing
P03-1041,J93-2003,0,0.0192708,"e facto standard. Direct translation approaches (Fos ! "" directly, and ter, 2000) consider estimating  work by (Och and Ney, 2002) show that similar ! '  or improved results are achieved by replacing #!  in the optimization with  , at the cost of deviating from the Bayesian framework. Regardless of the approach, the question of accurately estimating a model of translation from a large parallel or comparable corpus is one of the defining components within statistical machine translation. Re-ordering effects across languages have been modeled in several ways, including word-based (Brown et al., 1993), template-based (Och et al., 1999) and syntax-based (Yamada, Knight, 2001). Analyzing these models from a generative mindset, they all assume that the atomic unit of lexical content is the word, and re-ordering effects are applied above that level. (Marcu, Wong, 2002) illustrate the effects of assuming that lexical correspondence can only be modeled at the word level, and motivate a joint probability model that explicitly generates phrase level lexical content across both languages. (Wu, 1995) presents a bracketing method that models re-ordering at the sentence level. Both (Marcu, Wong, 2002;"
P03-1041,P02-1038,0,0.0491004,"tion problem where we need to separate the distribution of the incorrectly translated hypothesis from the distribution of the likely translations. Instead of using the maximum likelihood criteria, we use the maximal separation criteria ie. selecting a splitting point within the scores to maximize the difference of the mean score between distributions as shown below. 8: .,¤ 8X Z   *  ""¥ * P¦ B§ * (9)  ¨E M ! w©  . <W 8YX[ZB M . V ,  ª ¬ <W 8YX[ZB  « V  c , M  (10) (10) calculates direct translation probabilities, ie #!  . As mentioned earlier, (Och and Ney, 2002), show that using direction translation estimates in the decoding process as compared with calculating &!   as prescribed by the Bayesian framework does not reduce translation quality. Our results corroborate these findings and we use (10) as the phrase level translation model estimate within our decoder.  6 Integration Phrase translation pairs that are generated by the method described in this paper are finally scored with estimates of translation probability, which can be conditioned on the target language if necessary. These estimates fit cleanly into the decoding process, except for"
P03-1041,W99-0604,0,\N,Missing
P03-1041,C00-2163,0,\N,Missing
P03-1041,C96-2141,1,\N,Missing
P03-1041,P02-1040,0,\N,Missing
P03-1041,P01-1067,0,\N,Missing
P03-1041,P00-1006,0,\N,Missing
P19-1115,P08-1115,0,0.485401,"s. Introduction In many natural language processing tasks, graphbased representations have proven useful tools to enable models to deal with highly structured knowledge. Lattices are a common instance of graph-based representations that allows capturing a large number of alternative sequences in a compact form (Figure 1). Example applications include speech recognition lattices that represent alternative decoding choices (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), word segmentation lattices that capture ambiguous decisions on word boundaries or morphological alternatives (Dyer et al., 2008), word class lattices (Navigli and Velardi, 2010), and lattices for alternative video descriptions 0.4 (Senina 1 et al., 2014). 0.45 0.88 Prior work1 has madea it possible handle1 these a e to 1 e S E S through the use of recurrent 0.8neural network (RNN) 0.55 0.6 0.12 0.2et al., 2016; Su1 c c lattice representations (Ladhak b b d d et al., 2017; Sperber et al., 2017), inspired by earlier works that extended RNNs to tree structures (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Unfortunately, these models are computationally expensive, because the extension of the already slow RNNs"
P19-1115,D17-1209,0,0.0512882,"Missing"
P19-1115,P18-1026,0,0.0190043,"Related Work The translation of lattices rather than sequences has been investigated with traditional machine translation models (Ney, 1999; Casacuberta et al., 2004; Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008; Dyer et al., 2008), but these approaches rely on independence assumptions in the decoding process that no longer hold for neural encoder-decoder models. Neural latticeto-sequence models were proposed by Su et al. (2017); Sperber et al. (2017), with promising results but slow computation speeds. Other related work includes gated graph neural networks (Li et al., 2016; Beck et al., 2018). As an alternative to these RNN-based models, GCNs have been investigated (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2017), and used for devising tree-tosequence models (Bastings et al., 2017; Marcheggiani et al., 2018). We are not aware of any application of GCNs to lattice modeling. Unlike our approach, GCNs consider only local context, must be combined with slower LSTM layers for good performance, and lack support for lattice scores. Our model builds on previous works on selfattentional models (Cheng et al., 2016; Parikh et al., 2016; Lin et al"
P19-1115,D16-1244,0,0.106931,"Missing"
P19-1115,D15-1166,0,0.0667507,"Missing"
P19-1115,2013.iwslt-papers.14,0,0.225797,"Missing"
P19-1115,N18-2078,0,0.0192845,"e approaches rely on independence assumptions in the decoding process that no longer hold for neural encoder-decoder models. Neural latticeto-sequence models were proposed by Su et al. (2017); Sperber et al. (2017), with promising results but slow computation speeds. Other related work includes gated graph neural networks (Li et al., 2016; Beck et al., 2018). As an alternative to these RNN-based models, GCNs have been investigated (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2017), and used for devising tree-tosequence models (Bastings et al., 2017; Marcheggiani et al., 2018). We are not aware of any application of GCNs to lattice modeling. Unlike our approach, GCNs consider only local context, must be combined with slower LSTM layers for good performance, and lack support for lattice scores. Our model builds on previous works on selfattentional models (Cheng et al., 2016; Parikh et al., 2016; Lin et al., 2017; Vaswani et al., 2017). The idea of masking has been used for various purposes, including occlusion of future information during training (Vaswani et al., 2017), introducing directionality (Shen et al., 2018) with good results for machine translation confirm"
P19-1115,P10-1134,0,0.0352991,"processing tasks, graphbased representations have proven useful tools to enable models to deal with highly structured knowledge. Lattices are a common instance of graph-based representations that allows capturing a large number of alternative sequences in a compact form (Figure 1). Example applications include speech recognition lattices that represent alternative decoding choices (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), word segmentation lattices that capture ambiguous decisions on word boundaries or morphological alternatives (Dyer et al., 2008), word class lattices (Navigli and Velardi, 2010), and lattices for alternative video descriptions 0.4 (Senina 1 et al., 2014). 0.45 0.88 Prior work1 has madea it possible handle1 these a e to 1 e S E S through the use of recurrent 0.8neural network (RNN) 0.55 0.6 0.12 0.2et al., 2016; Su1 c c lattice representations (Ladhak b b d d et al., 2017; Sperber et al., 2017), inspired by earlier works that extended RNNs to tree structures (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Unfortunately, these models are computationally expensive, because the extension of the already slow RNNs to tree-structured inputs prevents convenient us"
P19-1115,D13-1170,0,0.00348044,"al., 2005; Matusov et al., 2008), word segmentation lattices that capture ambiguous decisions on word boundaries or morphological alternatives (Dyer et al., 2008), word class lattices (Navigli and Velardi, 2010), and lattices for alternative video descriptions 0.4 (Senina 1 et al., 2014). 0.45 0.88 Prior work1 has madea it possible handle1 these a e to 1 e S E S through the use of recurrent 0.8neural network (RNN) 0.55 0.6 0.12 0.2et al., 2016; Su1 c c lattice representations (Ladhak b b d d et al., 2017; Sperber et al., 2017), inspired by earlier works that extended RNNs to tree structures (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Unfortunately, these models are computationally expensive, because the extension of the already slow RNNs to tree-structured inputs prevents convenient use of batched computation. An alternative model, graph convolutional networks (GCN) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2017), is much faster but considers only local context and therefore requires combination with slower RNN layers for typical natural language processing tasks (Bastings et al., 2017; Cetoli et al., 2017; Vashishth et al., 2018). For li"
P19-1115,D17-1145,1,0.3653,"tion lattices that represent alternative decoding choices (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), word segmentation lattices that capture ambiguous decisions on word boundaries or morphological alternatives (Dyer et al., 2008), word class lattices (Navigli and Velardi, 2010), and lattices for alternative video descriptions 0.4 (Senina 1 et al., 2014). 0.45 0.88 Prior work1 has madea it possible handle1 these a e to 1 e S E S through the use of recurrent 0.8neural network (RNN) 0.55 0.6 0.12 0.2et al., 2016; Su1 c c lattice representations (Ladhak b b d d et al., 2017; Sperber et al., 2017), inspired by earlier works that extended RNNs to tree structures (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Unfortunately, these models are computationally expensive, because the extension of the already slow RNNs to tree-structured inputs prevents convenient use of batched computation. An alternative model, graph convolutional networks (GCN) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2017), is much faster but considers only local context and therefore requires combination with slower RNN layers for typical natural language process"
P19-1115,P15-1150,0,0.0850618,"Missing"
P19-1115,P18-1149,0,0.0225762,"structures (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Unfortunately, these models are computationally expensive, because the extension of the already slow RNNs to tree-structured inputs prevents convenient use of batched computation. An alternative model, graph convolutional networks (GCN) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2017), is much faster but considers only local context and therefore requires combination with slower RNN layers for typical natural language processing tasks (Bastings et al., 2017; Cetoli et al., 2017; Vashishth et al., 2018). For linear sequence modeling, self-attention (Cheng et al., 2016; Parikh et al., 2016; Lin et al., 2017; Vaswani et al., 2017) now provides an alternative to RNNs. Self-attention encodes sequences by relating sequence items to one another through computation of pairwise similarity, with addition of positional encoding to model positions of words in a linear sequence. Self-attention has gained popularity thanks to strong empirical results and computational efficiency afforded by paralleliz1185 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1185–"
P19-1115,2005.iwslt-1.2,0,0.694097,"and inference. 1 c 0.6 b 0.8 d 0.2 1 1 f E 1 1 1 g e Figure 1: Example of a node-labeled lattice. Nodes are labeled with word tokens and posterior scores. Introduction In many natural language processing tasks, graphbased representations have proven useful tools to enable models to deal with highly structured knowledge. Lattices are a common instance of graph-based representations that allows capturing a large number of alternative sequences in a compact form (Figure 1). Example applications include speech recognition lattices that represent alternative decoding choices (Saleem et al., 2004; Zhang et al., 2005; Matusov et al., 2008), word segmentation lattices that capture ambiguous decisions on word boundaries or morphological alternatives (Dyer et al., 2008), word class lattices (Navigli and Velardi, 2010), and lattices for alternative video descriptions 0.4 (Senina 1 et al., 2014). 0.45 0.88 Prior work1 has madea it possible handle1 these a e to 1 e S E S through the use of recurrent 0.8neural network (RNN) 0.55 0.6 0.12 0.2et al., 2016; Su1 c c lattice representations (Ladhak b b d d et al., 2017; Sperber et al., 2017), inspired by earlier works that extended RNNs to tree structures (Socher et"
P97-1047,1992.tmi-1.8,0,0.0515803,"length. It is used to calculate the mean of the heuristics over all possible source sentence length, m is the target sentence length. The parameters of the Poisson distributions can be estimated from training data. 4 for both languages) were used to train the IBM model 2 and the simplified model with the EM algorithm. A larger English monolingual corpus with around 0.5 million words was used to train a bigram for language modelling. The lexicon contains 2,800 English and 4,800 German words in morphologically inflected form. We did not do any preprocessing/analysis of the data as reported in (Brown et al., 1992). 5.1 5.2 T r a n s l a t i o n A c c u r a c y Unlike the case in speech recognition, it is quite arguable what ""accurate translations"" means. In speech recognition an output can be compared with the sample transcript of the test data. In machine translation, a sentence may have several legitimate translations. It is difficult to compare an output from a decoder with a designated translation. Instead, we used human subjects to judge the machinemade translations. The translations are classified into three categories 1. Correct translations: translations that are grammatical and convey the same"
P97-1047,C96-2141,0,0.0579145,"e c o d i n g in Statistical Machine Translation 2.1.1 Prefix score gH (3) can be used to assess a hypothesis. Although it was obtained from the alignment model, it would be easier for us to describe the scoring method if we interpret the last expression in the equation in the following way: each word el in the hypothesis contributes the amount e t(gj [ei)a(ilJ,l,m) to the probability of the target sentence word gj. For each hypothesis H = l : el,e2,-"",ek, we use SH(j) to denote the probability mass for the target word gl contributed by the words in the hypothesis: k (Brown et al., 1993) and (Vogel, Ney, and Tillman, 1996) have discussed the first two of the three problems in statistical machine translation. Although the authors of (Brown et al., 1993) stated that they would discuss the search problem in a follow-up arti• cle, so far there have no publications devoted to the decoding issue for statistical machine translation. On the other side, decoding algorithm is a crucial part in statistical machine translation. Its performance directly affects the quality and efficiency of translation. Without a good and efficient decoding algorithm, a statistical machine translation system may miss the best translation o"
P97-1047,J93-2003,0,\N,Missing
P98-1075,E93-1027,0,0.0204895,"action with the end-user. The ranked hypotheses are presented to the end-user in the form of questions about, or rephrases of, the original utterance. 3. Dynamic rule creation. If the end-user is satisfied with one of the options, a new grammar rule is dynamically created and becomes part of the end-user&apos;s grammar until further notice. Each new rule is annotated with the learning episode that gave rise to it, including end-user ID, time stamp, and a counter that will keep track of how many times the new rule fires in successful parses, s 3.2.2 P a r s e r p r e d i c t i o n s As suggested by Kiyono and Tsujii (1993), one can make use of parse failures to acquire new knowledge, both about the nature of the unparsed words and about the inadequacy of the existing grammar rules. GsG uses incomplete parses to predict what can come next (i.e. after the partially-parsed sequence 7I.e., parse trees containing concept-subconcept relations that are inconsistent with the stipulations of the DM. SThe degree of generalization or level o.f abstraction t h a t a new rule should exhibit is an open question but currently a Principle of Maximal Abstraction is followed: (a) Parse the lexical items of the new rule&apos;s right-h"
P98-1075,P94-1004,0,0.0129534,"g o s L t t l ] I +- - , l s i t I ÷-about I +-[tlm] I +-[polntl I ÷- [ day_of_woek l I I I +-ttmlday I 4.-[ t i i . . e l _ d a y ] I llutoml~ Refill a,hat i ~ u t ~ue~l~ aftemoon ok ii If L... Z..J 8,a ---q .........; lst~a~LlJ,&apos;~ } &lt; - - &quot;,,mat about [ume] {I 1. In the absence of a training corpus, the HUM parameters are seeded from the Kernel Grammar itself. 2. Training is maintained at run-time through dynamic updates of all model parameters after each utterance and learning episode. Figure 6: ...and a new rule is acquired. of unparsed words, the following stochastic models, inspired in Miller et al. (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. • Speech-act n-gram. Top-level concepts can be seen as speech acts of the domain. For instance, in the DM in Fig. 2 top-level concepts such as [greeting], Cfarewell] or [suggestion], correspond to discourse speech acts, and in normally-occurring conversation, they follow a distribution that is clearly non-uniform. 1° • Concept-subconcept HMM. Discrete hidden Markov model in which the states correspond l°Needless to say, speech-act transition distributions are empirically estimated, but, intuitiv"
P98-1075,J92-1004,0,0.0122863,"l s i t I ÷-about I +-[tlm] I +-[polntl I ÷- [ day_of_woek l I I I +-ttmlday I 4.-[ t i i . . e l _ d a y ] I llutoml~ Refill a,hat i ~ u t ~ue~l~ aftemoon ok ii If L... Z..J 8,a ---q .........; lst~a~LlJ,&apos;~ } &lt; - - &quot;,,mat about [ume] {I 1. In the absence of a training corpus, the HUM parameters are seeded from the Kernel Grammar itself. 2. Training is maintained at run-time through dynamic updates of all model parameters after each utterance and learning episode. Figure 6: ...and a new rule is acquired. of unparsed words, the following stochastic models, inspired in Miller et al. (1994) and Seneff (1992), and collectively referred to as hidden understanding model (HUM), are employed. • Speech-act n-gram. Top-level concepts can be seen as speech acts of the domain. For instance, in the DM in Fig. 2 top-level concepts such as [greeting], Cfarewell] or [suggestion], correspond to discourse speech acts, and in normally-occurring conversation, they follow a distribution that is clearly non-uniform. 1° • Concept-subconcept HMM. Discrete hidden Markov model in which the states correspond l°Needless to say, speech-act transition distributions are empirically estimated, but, intuitively, the s e q u e"
P98-2221,J92-4003,0,0.0326707,"pan is the distance between the leftmost and the rightmost target positions aligned with the words inside the candidate, averaged over all Model 1 Viterbi alignments of sample sentences. A candidate is filtered out if its average translation span is greater than the length of the candidate multiplied by a threshold. This criterion states that the words in the translation of a phrase have to be close enough to form a phrase in another language. . Clustering: Clustering words/phrases with similar meanings/grammatical functions into equivalent classes. The mutual information clustering algorithm(Brown et al., 1992) were used for this. . A m b i g u i t y R e d u c t i o n : A word occurring in a phrase should be less ambiguous than in other random context. Therefore a phrase should reduce the ambiguity (uncertainty) of the words inside it. For each source language word class c, its translation entropy is defined as )-']~gt(g [ c)log(g [ c). The average per source class entropy reduction induced by the introduction of a phrase P is therefore . Phrasing: The equivalent class sequence Cl, c2,...c k forms a phrase if P(cl, c2,'&quot; &quot;ck) log P(cI, c2,'&quot; &quot;ck) > 8, P(c,)P(c2)&quot; &quot;P(ck) where ~ is a threshold. By ch"
P98-2221,C96-2141,0,0.126413,"Missing"
P98-2221,P97-1047,1,0.900898,"may be a result of erroneous sentence alignment) or the two languages have different word orders, like English and German. Figure 1 and Figure 2 show some problematic alignments between English/German sentences made by IBM Model 2, together with the 'ideal' alignments for the sentences. Here the alignment parameters penalize the alignment of English words with their German translation equivalents because the translation equivalents are far away from the words. An experiment reveals how often this kind of &quot;skewed&quot; alignment happens in our English/German scheduling conversation parallel corpus (Wang and Waibel, 1997). The experiment was based on the following observation: IBM translation Model 1 (where the alignment distribution is uniform) and Model 2 found similar Viterbi alignments when there were no movements or deletions, and they predicted very different Viterbi alignments when the skewness was severe ill a sentence pair, since the alignment parameters in Model 2 penalize the long distance alignment. Figure 3 shows the Viterbi alignment discovered by Model 1 for the same sentences in Figure 21 . We measured the distance of a Model 1 alignment a 1 and a Model 2 alignment a z To estimate the skewa S ~"
P98-2221,J93-2003,0,\N,Missing
Q14-1014,P13-1004,0,0.0206363,"l., 2011) and many other NLP tasks (Olsson, 2009), to name but a few studies. It has also been recognized by the active learning community that correcting the most useful parts first is often not optimal in terms of efficiency, since these parts tend to be the most difficult to manually annotate (Settles et al., 2008). The authors advocate the use of a user model to predict the supervision effort, and select the instances with best “bang-for-thebuck.” This prediction of supervision effort was successful, and was further refined in other NLP-related studies (Tomanek et al., 2010; Specia, 2011; Cohn and Specia, 2013). Our approach to user modeling using GP regression is inspired by the latter. Most studies on user models consider only supervision effort, while neglecting the accuracy of human annotations. The view on humans as a perfect oracle has been criticized (Donmez and Carbonell, 2008), since human errors are common and can negatively affect supervision utility. Research on human-computer-interaction has identified the modeling of human errors as very difficult (Olson and Olson, 1990), depending on factors such as user experience, cognitive load, user interface design, and 178 fatigue. Nevertheless,"
Q14-1014,P10-2032,0,0.0484789,"Missing"
Q14-1014,N09-1047,0,0.0787448,"t to the number of segmented tokens. We feel that this is acceptable, considering that the time needed for human supervision will likely dominate the computation time, and reasonable approximations can be made as noted in Section 3.2. 6 Relation to Prior Work Efficient supervision strategies have been studied across a variety of NLP-related research areas, and received increasing attention in recent years. Examples include post editing for speech recognition (Sanchez-Cortina et al., 2012), interactive machine translation (Gonz´alez-Rubio et al., 2010), active learning for machine translation (Haffari et al., 2009; Gonz´alez-Rubio et al., 2011) and many other NLP tasks (Olsson, 2009), to name but a few studies. It has also been recognized by the active learning community that correcting the most useful parts first is often not optimal in terms of efficiency, since these parts tend to be the most difficult to manually annotate (Settles et al., 2008). The authors advocate the use of a user model to predict the supervision effort, and select the instances with best “bang-for-thebuck.” This prediction of supervision effort was successful, and was further refined in other NLP-related studies (Tomanek et al."
Q14-1014,I08-7018,0,0.0306138,"be grouped into segments are positions between adjacent characters. 5.2.1 Experimental Setup Neubig et al. (2011) have proposed a pointwise method for Japanese word segmentation that can be trained using partially annotated sentences, which makes it attractive in combination with active learning, as well as our segmentation method. The authors released their method as a software package “KyTea” that we employed in this user study. We used KyTea’s active learning domain adaptation toolkit8 as a baseline. For data, we used the Balanced Corpus of Contemporary Written Japanese (BCCWJ), created by Maekawa (2008), with the internet Q&A subcorpus as in-domain data, and the whitepaper subcorpus as background data, a domain adaptation scenario. Sentences were drawn from the in-domain corpus, and the manually annotated data was then used to train KyTea, along with the pre-annotated background data. The goal (objective function) was to improve KyTea’s classification accuracy on an indomain test set, given a constrained time budget of 30 minutes. There were again 2 supervision modes: ANNOTATE and SKIP . Note that this is essentially a batch active learning setup with only one iteration. We conducted experim"
Q14-1014,2006.iwslt-papers.1,0,0.0655285,"Missing"
Q14-1014,P11-2093,1,0.908647,"Gonz´alez-Rubio et al., 2010). The most common framework for efficient annotation in the NLP context consists of training an NLP system on a small amount of baseline data, and then running the system on unannotated data to estimate confidence scores of the system’s predictions (Settles, 2008). Sentences with the lowest confidence are then used as the data to be annotated (Figure 1 (a)). However, it has been noted that when the NLP system in question already has relatively high accuracy, annotating entire sentences can be wasteful, as most words will already be correct (Tomanek and Hahn, 2009; Neubig et al., 2011). In these cases, it is possible to achieve much higher benefit per annotated word by annotating sub-sentential units (Figure 1 (b)). However, as Settles et al. (2008) point out, simply maximizing the benefit per annotated instance is not enough, as the real supervision effort varies 169 Transactions of the Association for Computational Linguistics, 2 (2014) 169–180. Action Editor: Eric Fosler-Lussier. c Submitted 11/2013; Revised 2/2014; Published 4/2014. 2014 Association for Computational Linguistics. Avg. time / instance [sec] 6 Transcription task Word segmentation task 4 2 0 1 3 5 7 9 11 1"
Q14-1014,P10-1037,0,0.020503,"n terms of the same monetary unit. Vijayanarasimhan et al. (2010) and Donmez and Carbonell (2008) use a more practical approach that specifies a constrained optimization problem by allowing only a limited time budget for supervision. Our approach is a generalization thereof and allows either specifying an upper bound on the predicted cost, or a lower bound on the predicted utility. The main novelty of our presented approach is the explicit modeling and selection of segments of various sizes, such that annotation efficiency is optimized according to the specified constraints. While some works (Sassano and Kurohashi, 2010; Neubig et al., 2011) have proposed using subsentential segments, we are not aware of any previous work that explicitly optimizes that segmentation. 7 Conclusion We presented a method that can effectively choose a segmentation of a language corpus that optimizes supervision efficiency, considering not only the actual usefulness of each segment, but also the annotation cost. We reported noticeable improvements over strong baselines in two user studies. Future user experiments with more participants would be desirable to verify our observations, and allow further analysis of different factors s"
Q14-1014,D08-1112,0,0.840039,"rors in parentheses. The underlined parts are to be corrected by a human for (a) sentences, (b) words, or (c) the proposed segmentation. Introduction Many natural language processing (NLP) tasks require human supervision to be useful in practice, be it to collect suitable training material or to meet some desired output quality. Given the high cost of human intervention, how to minimize the supervision effort is an important research problem. Previous works in areas such as active learning, post editing, and interactive pattern recognition have investigated this question with notable success (Settles, 2008; Specia, 2011; Gonz´alez-Rubio et al., 2010). The most common framework for efficient annotation in the NLP context consists of training an NLP system on a small amount of baseline data, and then running the system on unannotated data to estimate confidence scores of the system’s predictions (Settles, 2008). Sentences with the lowest confidence are then used as the data to be annotated (Figure 1 (a)). However, it has been noted that when the NLP system in question already has relatively high accuracy, annotating entire sentences can be wasteful, as most words will already be correct (Tomanek"
Q14-1014,2011.eamt-1.12,0,0.438874,"eses. The underlined parts are to be corrected by a human for (a) sentences, (b) words, or (c) the proposed segmentation. Introduction Many natural language processing (NLP) tasks require human supervision to be useful in practice, be it to collect suitable training material or to meet some desired output quality. Given the high cost of human intervention, how to minimize the supervision effort is an important research problem. Previous works in areas such as active learning, post editing, and interactive pattern recognition have investigated this question with notable success (Settles, 2008; Specia, 2011; Gonz´alez-Rubio et al., 2010). The most common framework for efficient annotation in the NLP context consists of training an NLP system on a small amount of baseline data, and then running the system on unannotated data to estimate confidence scores of the system’s predictions (Settles, 2008). Sentences with the lowest confidence are then used as the data to be annotated (Figure 1 (a)). However, it has been noted that when the NLP system in question already has relatively high accuracy, annotating entire sentences can be wasteful, as most words will already be correct (Tomanek and Hahn, 2009"
Q14-1014,P09-1117,0,0.143235,"es, 2008; Specia, 2011; Gonz´alez-Rubio et al., 2010). The most common framework for efficient annotation in the NLP context consists of training an NLP system on a small amount of baseline data, and then running the system on unannotated data to estimate confidence scores of the system’s predictions (Settles, 2008). Sentences with the lowest confidence are then used as the data to be annotated (Figure 1 (a)). However, it has been noted that when the NLP system in question already has relatively high accuracy, annotating entire sentences can be wasteful, as most words will already be correct (Tomanek and Hahn, 2009; Neubig et al., 2011). In these cases, it is possible to achieve much higher benefit per annotated word by annotating sub-sentential units (Figure 1 (b)). However, as Settles et al. (2008) point out, simply maximizing the benefit per annotated instance is not enough, as the real supervision effort varies 169 Transactions of the Association for Computational Linguistics, 2 (2014) 169–180. Action Editor: Eric Fosler-Lussier. c Submitted 11/2013; Revised 2/2014; Published 4/2014. 2014 Association for Computational Linguistics. Avg. time / instance [sec] 6 Transcription task Word segmentation tas"
Q14-1014,P10-1118,0,0.0154553,"i et al., 2009; Gonz´alez-Rubio et al., 2011) and many other NLP tasks (Olsson, 2009), to name but a few studies. It has also been recognized by the active learning community that correcting the most useful parts first is often not optimal in terms of efficiency, since these parts tend to be the most difficult to manually annotate (Settles et al., 2008). The authors advocate the use of a user model to predict the supervision effort, and select the instances with best “bang-for-thebuck.” This prediction of supervision effort was successful, and was further refined in other NLP-related studies (Tomanek et al., 2010; Specia, 2011; Cohn and Specia, 2013). Our approach to user modeling using GP regression is inspired by the latter. Most studies on user models consider only supervision effort, while neglecting the accuracy of human annotations. The view on humans as a perfect oracle has been criticized (Donmez and Carbonell, 2008), since human errors are common and can negatively affect supervision utility. Research on human-computer-interaction has identified the modeling of human errors as very difficult (Olson and Olson, 1990), depending on factors such as user experience, cognitive load, user interface"
Q19-1020,N16-1109,0,0.132041,"ntermediate representation for the speech translation task, corresponding to the second stage output. Toshniwal et al. (2017) explore a different way of lower-level supervision during training of an attentional speech recognizer by jointly training an auxiliary phoneme recognizer based on a lower layer in the acoustic encoder. Similarly to the discussed multi-task direct model, this approach discards many of the learned parameters when used on the main task and consequently may also suffer from data efficiency issues. Direct end-to-end speech translation models were first used by Duong et al. (2016), although the authors did not actually evaluate translation performance. Weiss et al. (2017) extended this model into a multi-task model and report excellent translation results. Our baselines do not match their results, despite considerable efforts. We note that other research groups have encountered similar replicability issues (Bansal et al., 2018), explanations include the lack of a large GPU cluster to perform ASGD training, as well as to explore an ideal number of training schedules and other hyper-parameter settings. B´erard et al. (2018) explored the translation of audio books with di"
Q19-1020,N18-1008,0,0.368969,"the speech recognizer passes an erroneous source text to the machine translation component, potentially leading to compounding follow-up errors. Another advantage is the ability to train all model parameters jointly. Despite these obvious advantages, two problems persist: (1) Reports on whether direct models outperform cascaded models (Fig. 1a,d) are inconclusive, with some work in favor of direct models (Weiss et al., 2017), some work in favor of cascaded models (Kano et al., 2017; B´erard et al., 2018), and one work in favor of direct models for two out of the three examined language pairs (Anastasopoulos and Chiang, 2018). (2) Cascaded and direct models have been compared under identical data situations, but this is an unrealistic assumption: In practice, cascaded models can be trained on much more abundant independent ASR and MT corpora, whereas end-to-end models require hard-to-acquire end-to-end corpora of speech utterances paired with textual translations. Our first contribution is a closer investigation of these two issues. Regarding the question of whether direct models or cascaded models are generally stronger, we hypothesize that direct models require more data to work well, due to the more complex map"
Q19-1020,N19-1006,0,0.2679,"which are both attentional sequence-to-sequence models according to equations 1–4, trained on the appropriate data. The ASR component uses the acoustic encoder of §2.1, and the MT model uses a bidirectional LSTM with 2 layers as encoder. 3 Multi-Task Training for the Direct Model ST: Combines source speech encoder, generalpurpose-attention, target text decoder. This is our main task and requires end-to-end data for training. 3 We also experimented with a final fine-tuning phase on only the main task (Niehues and Cho, 2017), but discarded this strategy for lack of consistent gains. 4 Note that Bansal et al. (2019) do experiment with additional speech recognition data, although, differently from our work, for purposes of cross-lingual transfer learning. Incorporating Auxiliary Data The models described in §2.2 and §2.3 are trained only on speech utterances paired with translations 316 Note that somewhat related to our multi-task strategy, Kano et al. (2017) have decomposed their two-stage model in a similar way to perform pretraining for the individual stages, although not with the goal of incorporating additional auxiliary data. Figure 3: Direct multi-task model. 4 Auto-encoder (AE): Combines source te"
Q19-1020,L18-1001,0,0.109917,"h a model does not rely on intermediate ASR output and is therefore not subject to error propagation. However, the transformation from source speech inputs to target text outputs is much more complex than that of an ASR or MT system taken individually, which may cause the model to require more data to perform well. To make matters precise, given L audio encoder states e1:L computed by the audio encoder as and MT corpora exist and for which it is more realistic to obtain good speech translation accuracy. 2 As a case in point, the largest available speech translation corpora (Post et al., 2013; Kocabiyikoglu et al., 2018) are an order of magnitude smaller than the largest speech recognition corpora (Cieri et al., 2004; Panayotov et al., 2015) (∼ 200 hours vs 2000 hours) and several orders of magnitude smaller than the largest machine translation corpora, e.g., those provided by the Conference on Machine Translation (WMT). 315 (and transcripts in the case of §2.3), which is a severe limitation. To incorporate auxiliary ASR and MT data into the training, we make use of a multi-task training strategy. Such a strategy trains auxiliary ASR and MT models that share certain parameters with the main speech translation"
Q19-1020,L16-1147,0,0.0282839,"This indicates that access to ASR labels in some form contributes to favorable data efficiency of speech translation models. Adding External Data Our approach for evaluating data efficiency so far has been to assume that end-to-end data are available for only a subset of the available auxiliary data. In practice, we can often train ASR and MT tasks on abundant external data. We therefore run experiments in which we use the full Fisher training data for all tasks as before, and add OpenSubtitle11 data for the auxiliary MT task. We clean and normalize the Spanish–English OpenSubtitle 2018 data (Lison and Tiedemann, 2016) to be consistent with the employed Fisher training data by lowercasing and removing punctuation. We apply a basic length filter and obtain 61 million sentences. During training, we include the same number of sentences from in-domain and out-of-domain MT tasks in each minibatch in order to prevent degradation due to domain mismatch. 11 Fisher Table 3: Adding auxiliary OpenSubtitles MT data to the training. The two-stage models benefit much more strongly than the direct model, with our proposed model yielding the strongest overall results. Figure 6: Data efficiency across model types. All model"
Q19-1020,cieri-etal-2004-fisher,0,0.135104,"er, the transformation from source speech inputs to target text outputs is much more complex than that of an ASR or MT system taken individually, which may cause the model to require more data to perform well. To make matters precise, given L audio encoder states e1:L computed by the audio encoder as and MT corpora exist and for which it is more realistic to obtain good speech translation accuracy. 2 As a case in point, the largest available speech translation corpora (Post et al., 2013; Kocabiyikoglu et al., 2018) are an order of magnitude smaller than the largest speech recognition corpora (Cieri et al., 2004; Panayotov et al., 2015) (∼ 200 hours vs 2000 hours) and several orders of magnitude smaller than the largest machine translation corpora, e.g., those provided by the Conference on Machine Translation (WMT). 315 (and transcripts in the case of §2.3), which is a severe limitation. To incorporate auxiliary ASR and MT data into the training, we make use of a multi-task training strategy. Such a strategy trains auxiliary ASR and MT models that share certain parameters with the main speech translation model. We implement multi-task training by drawing several minibatches, one minibatch for each ta"
Q19-1020,N18-1031,0,0.0337135,"6.59 35.30 24.68 14.91 6.08 Table 1: BLEU scores (4 references) on the Fisher/ Test for various amounts of training data. The direct (multi-task) model performs best in the full data condition, but the cascaded model is best in all reduced conditions. attention MLP, 64 for target character embeddings, 256 for the encoder LSTMs in each direction, and 512 elsewhere. The model uses variational recurrent dropout with probability 0.3 and target character dropout with probability 0.1 (Gal and Ghahramani, 2016). We apply label smoothing (Szegedy et al., 2016) and fix the target embedding norm to 1 (Nguyen and Chiang, 2018). We use beam search with beam size 15 and polynomial length normalization with exponent 1.5.8 All BLEU scores are computed on Fisher/Test against 4 references. 5.1 Cascaded vs. Direct Models We first wish to shed light on the question of whether cascaded or direct models can be expected to perform better. This question has been investigated previously (Weiss et al., 2017; Kano et al., 2017; B´erard et al., 2018; Anastasopoulos and Chiang, 2018), but with contradictory findings. We hypothesize that the increased complexity of the direct mapping from speech to translation increases the data req"
Q19-1020,W17-4708,1,0.82047,"ilitate meaningful comparisons. The cascade consists of an ASR component and an MT component, which are both attentional sequence-to-sequence models according to equations 1–4, trained on the appropriate data. The ASR component uses the acoustic encoder of §2.1, and the MT model uses a bidirectional LSTM with 2 layers as encoder. 3 Multi-Task Training for the Direct Model ST: Combines source speech encoder, generalpurpose-attention, target text decoder. This is our main task and requires end-to-end data for training. 3 We also experimented with a final fine-tuning phase on only the main task (Niehues and Cho, 2017), but discarded this strategy for lack of consistent gains. 4 Note that Bansal et al. (2019) do experiment with additional speech recognition data, although, differently from our work, for purposes of cross-lingual transfer learning. Incorporating Auxiliary Data The models described in §2.2 and §2.3 are trained only on speech utterances paired with translations 316 Note that somewhat related to our multi-task strategy, Kano et al. (2017) have decomposed their two-stage model in a similar way to perform pretraining for the individual stages, although not with the goal of incorporating additiona"
Q19-1020,2013.iwslt-papers.14,0,0.50882,"ons as outputs. Such a model does not rely on intermediate ASR output and is therefore not subject to error propagation. However, the transformation from source speech inputs to target text outputs is much more complex than that of an ASR or MT system taken individually, which may cause the model to require more data to perform well. To make matters precise, given L audio encoder states e1:L computed by the audio encoder as and MT corpora exist and for which it is more realistic to obtain good speech translation accuracy. 2 As a case in point, the largest available speech translation corpora (Post et al., 2013; Kocabiyikoglu et al., 2018) are an order of magnitude smaller than the largest speech recognition corpora (Cieri et al., 2004; Panayotov et al., 2015) (∼ 200 hours vs 2000 hours) and several orders of magnitude smaller than the largest machine translation corpora, e.g., those provided by the Conference on Machine Translation (WMT). 315 (and transcripts in the case of §2.3), which is a severe limitation. To incorporate auxiliary ASR and MT data into the training, we make use of a multi-task training strategy. Such a strategy trains auxiliary ASR and MT models that share certain parameters wit"
ries-etal-2000-shallow,J97-1003,0,\N,Missing
stuker-etal-2012-kit,P02-1040,0,\N,Missing
stuker-etal-2012-kit,2011.iwslt-evaluation.1,1,\N,Missing
stuker-etal-2012-kit,2011.iwslt-evaluation.16,1,\N,Missing
stuker-etal-2012-kit,2010.iwslt-evaluation.1,1,\N,Missing
W03-0315,P91-1022,0,0.0683844,"ter-subject agreement score correlations. Pearson's correlation ranges from 0.53 up to 0.72 in our experiments. 1 Introduction In many instances, multilingual natural language systems like machine translation systems are developed and trained on parallel corpora. When faced with a different, unseen text genre, however, translation performance usually drops noticeably. One way to remedy this situation is to adapt and retrain the system parameters based on bilingual data from the same source or at least a closely related source. A bilingual sentence alignment program (Gale and Church, 1991, and Brown et al., 1991) is the crucial part in this adaptation procedure, in that it collects bilingual document pairs from the Internet, and identifies sentence pairs, which should have a high likelihood of being correct translations of each other. The set of identified bilingual parallel sentence vogel+@cs.cmu.edu ahw@cs.cmu.edu pairs is then added to the training set for parameter reestimation. As is well known, text mined from the Internet is very noisy. Even after careful html parsing and filtering for text size and language, the text from comparable html-page pairs still contains mismatches of content or non-p"
W03-0315,P93-1002,0,0.0803175,"Missing"
W03-0315,P91-1023,0,0.0549414,"s is in the range of the inter-subject agreement score correlations. Pearson's correlation ranges from 0.53 up to 0.72 in our experiments. 1 Introduction In many instances, multilingual natural language systems like machine translation systems are developed and trained on parallel corpora. When faced with a different, unseen text genre, however, translation performance usually drops noticeably. One way to remedy this situation is to adapt and retrain the system parameters based on bilingual data from the same source or at least a closely related source. A bilingual sentence alignment program (Gale and Church, 1991, and Brown et al., 1991) is the crucial part in this adaptation procedure, in that it collects bilingual document pairs from the Internet, and identifies sentence pairs, which should have a high likelihood of being correct translations of each other. The set of identified bilingual parallel sentence vogel+@cs.cmu.edu ahw@cs.cmu.edu pairs is then added to the training set for parameter reestimation. As is well known, text mined from the Internet is very noisy. Even after careful html parsing and filtering for text size and language, the text from comparable html-page pairs still contains misma"
W03-0315,W96-0201,0,0.0463513,"Missing"
W03-0315,J93-2003,0,0.0423788,"Missing"
W03-0315,P99-1068,0,0.0370012,"ty. We describe alignment experiments in section 5, focusing on the correlation between the alignment scores predicted by the sentence alignment models and by humans. Conclusions are given in section 6. 2 System of Mining Parallel Text One crucial component of statistical machine translation (SMT) system is the parallel text mining from Internet. Several processing modules are applied to collect, extract, convert, and clean the text from Internet. The components in our system include: • A web crawler, which collects potential parallel html documents based on link information following (Philip Resnik 1999); • A bilingual html parser (based on flex for efficiency), which is designed for both Chinese and English html documents. The paragraphs’ boundaries within the html structure are kept. • A character encoding detector, which judges if the Chinese html document is GB2312 encoding or BIG5 encoding. • An encoding converter, which converts the BIG5 documents to GB2312 encoding. • A language identifier to ensure that source and target documents are both of the proper language. (Noord’s Implementation). • A Chinese word segmenter, which parses the Chinese strings into Chinese words. • A document ali"
W03-0315,J93-1004,0,\N,Missing
W03-0315,P93-1001,0,\N,Missing
W03-0315,1999.mtsummit-1.79,0,\N,Missing
W03-1502,J93-2003,0,\N,Missing
W03-1502,C96-2141,1,\N,Missing
W03-1502,W98-1005,0,\N,Missing
W03-1502,P00-1004,1,\N,Missing
W03-1502,J00-2004,0,\N,Missing
W03-1502,P02-1051,0,\N,Missing
W03-1502,A97-1029,0,\N,Missing
W04-3227,J97-3002,0,0.341869,"h allows for local word reordering. 2.1 Translation Model The phrase-based statistical translation systems use not only word-to-word translation, extracted from bilingual data, but also phrase-to phrase translations. . Different types of extraction approaches have been described in the literature: syntax-based, word-alignment-based, and genuine phrase alignment models. The syntax-based approach has the advantage to model the grammar structures using models of more or less structural richness, such as the syntax-based alignment model in (Yamada and Knight, 2001) or the Bilingual Bracketing in (Wu, 1997). Popular word-alignment-based approaches usually rely on initial word alignments from the IBM and HMM alignment models (Och and Ney, 2000), from which the phrase pairs are then extracted. (Marcu and Wong 2002) and (Zhang et al. 2003) do not rely on word alignment but model directly the phrase alignment. Because all statistical machine translation systems search for a globally optimal translation using the language and translation model, a translation probability has to be assigned to each phrase translation pair. This score should be meaningful in that better translations have a higher probab"
W04-3227,P01-1067,0,0.085347,"based translation models and the decoding algorithm, which allows for local word reordering. 2.1 Translation Model The phrase-based statistical translation systems use not only word-to-word translation, extracted from bilingual data, but also phrase-to phrase translations. . Different types of extraction approaches have been described in the literature: syntax-based, word-alignment-based, and genuine phrase alignment models. The syntax-based approach has the advantage to model the grammar structures using models of more or less structural richness, such as the syntax-based alignment model in (Yamada and Knight, 2001) or the Bilingual Bracketing in (Wu, 1997). Popular word-alignment-based approaches usually rely on initial word alignments from the IBM and HMM alignment models (Och and Ney, 2000), from which the phrase pairs are then extracted. (Marcu and Wong 2002) and (Zhang et al. 2003) do not rely on word alignment but model directly the phrase alignment. Because all statistical machine translation systems search for a globally optimal translation using the language and translation model, a translation probability has to be assigned to each phrase translation pair. This score should be meaningful in tha"
W04-3227,2002.tmi-tutorials.2,0,0.0208216,"language. The advantages are obvious: It has built-in local context modeling, and provides reliable local word reordering. It has multi-word translations, and models a word’s conditional fertility given a local context. It captures idiomatic phrase translations and can be easily enriched with bilingual dictionaries. In addition, it can compensate for the segmentation errors made during preprocessing, i.e. word segmentation errors of Chinese. The advantage of using phrase-based translation in a statistical framework has been shown in many studies such as (Koehn et al. 2003; Vogel et al. 2003; Zens et al. 2002; Marcu and Wong, 2002). However, the phrase translation pairs are typically extracted from a parallel corpus based on the Viterbi alignment of some word alignment models. The leads to the question what probability should be assigned to those phrase translations. Different approaches have been suggested as using relative frequencies (Zens et al. 2002), calculate probabilities based on a statistical word-to-word dictionary (Vogel et al. 2003) or use a linear interpolation of these scores (Koehn et al. 2003). In this paper we investigate a different approach with takes the information content of"
W04-3227,C96-2141,1,0.864147,"Missing"
W04-3227,P91-1023,0,0.0595274,"statistical lexicon Pr(s|t) is non-symmetric. One can easily re-write all the distances by using Pr(t|s). But in our experiments this reverse direction of using Pr(t|s) gives trivially difference. So in all the experimental results reported in this paper, the distances defined in (1) and (11) are used. 5 Length Regularization Phrase pair extraction does not work perfectly and sometimes a short source phrase is aligned to a long target phrase or vice versa. Length regularization can be applied to penalize too long or too short candidate translations. Similar to the sentence alignment work in (Gale and Church, 1991), the phrase length ratio is assumed to be a Gaussian distribution as given in Equation (12): v v v v (l (t ) / l ( s ) − µ ) 2 (12) l (t , s ) ∝ exp(−0.5 ⋅ ) σ2 where l(t) is the target sentence length. Mean µ and variance σ can be estimated using a parallel corpus using a Maximum Likelihood criteria. The regularized score is the product of (11) and (12). 6 Experiments Experiments were carried out on the so-called large data track Chinese-English TIDES translation task, using the June 2002 test data. The training data used to train the statistical lexicon and to extract the phrase translation"
W04-3227,vogel-monson-2004-augmenting,1,0.813965,"ilt on 20 million words of general newswire text, using the SRILM toolkit (Stolcke, 2002). Decoding was carried out as described in section 2.2. The test data consists of 878 Chinese sentences or 24,337 words after word segmentation. There are four human translations per Chinese sentence as references. Both NIST score and Bleu score (in percentage) are reported for adequacy and fluency aspects of the translation quality. 6.1 ond, a large monolingual English corpus was used to filter out the new word forms. If they did not appear in the corpus, the new entries were not added to the transducer (Vogel, 2004). BiBr extracts sub-tree mappings from Bilingual Bracketing alignments (Wu, 1997); HMM extracts partial path mappings from the Viterbi path in the Hidden Markov Model alignments (Vogel et. al., 1996). ISA is an integrated segmentation and alignment for phrases (Zhang et.al, 2003), which is an extension of (Marcu and Wong, 2002). LDC 425K HMM 349K ISA 263K 1.80 1.11 1.09 Table-1 statistics of transducers 1.20 N (K ) avg (ltgt / l src ) Table-1 shows some statistics of the four transducers extracted for the translation task. N is the total number of phrase pairs in the transducer. LDC is the lar"
W04-3227,zhang-etal-2004-interpreting,1,0.835189,"Missing"
W04-3227,N03-1017,0,0.102462,"e language into an mgram in the target language. The advantages are obvious: It has built-in local context modeling, and provides reliable local word reordering. It has multi-word translations, and models a word’s conditional fertility given a local context. It captures idiomatic phrase translations and can be easily enriched with bilingual dictionaries. In addition, it can compensate for the segmentation errors made during preprocessing, i.e. word segmentation errors of Chinese. The advantage of using phrase-based translation in a statistical framework has been shown in many studies such as (Koehn et al. 2003; Vogel et al. 2003; Zens et al. 2002; Marcu and Wong, 2002). However, the phrase translation pairs are typically extracted from a parallel corpus based on the Viterbi alignment of some word alignment models. The leads to the question what probability should be assigned to those phrase translations. Different approaches have been suggested as using relative frequencies (Zens et al. 2002), calculate probabilities based on a statistical word-to-word dictionary (Vogel et al. 2003) or use a linear interpolation of these scores (Koehn et al. 2003). In this paper we investigate a different approach"
W04-3227,W02-1018,0,0.188015,"antages are obvious: It has built-in local context modeling, and provides reliable local word reordering. It has multi-word translations, and models a word’s conditional fertility given a local context. It captures idiomatic phrase translations and can be easily enriched with bilingual dictionaries. In addition, it can compensate for the segmentation errors made during preprocessing, i.e. word segmentation errors of Chinese. The advantage of using phrase-based translation in a statistical framework has been shown in many studies such as (Koehn et al. 2003; Vogel et al. 2003; Zens et al. 2002; Marcu and Wong, 2002). However, the phrase translation pairs are typically extracted from a parallel corpus based on the Viterbi alignment of some word alignment models. The leads to the question what probability should be assigned to those phrase translations. Different approaches have been suggested as using relative frequencies (Zens et al. 2002), calculate probabilities based on a statistical word-to-word dictionary (Vogel et al. 2003) or use a linear interpolation of these scores (Koehn et al. 2003). In this paper we investigate a different approach with takes the information content of words better into acco"
W04-3227,P00-1056,0,0.0991688,"rd translation, extracted from bilingual data, but also phrase-to phrase translations. . Different types of extraction approaches have been described in the literature: syntax-based, word-alignment-based, and genuine phrase alignment models. The syntax-based approach has the advantage to model the grammar structures using models of more or less structural richness, such as the syntax-based alignment model in (Yamada and Knight, 2001) or the Bilingual Bracketing in (Wu, 1997). Popular word-alignment-based approaches usually rely on initial word alignments from the IBM and HMM alignment models (Och and Ney, 2000), from which the phrase pairs are then extracted. (Marcu and Wong 2002) and (Zhang et al. 2003) do not rely on word alignment but model directly the phrase alignment. Because all statistical machine translation systems search for a globally optimal translation using the language and translation model, a translation probability has to be assigned to each phrase translation pair. This score should be meaningful in that better translations have a higher probability assigned to them, and balanced with respect to word translations. Bad phrase translations should not win over better word for word tr"
W04-3227,J93-1004,0,\N,Missing
W04-3227,J93-2003,0,\N,Missing
W05-0804,J93-2003,0,0.0271161,"Missing"
W05-0804,C00-2163,0,0.0204768,"HMM is defined as follows: P (f1J |eI1 ) = J XY P (fj |eaj )P (aj |aj−1 ), (2) j=1 aJ 1 26 where P (aj |aj−1 ) is the transition probability. This model captures the assumption that words close in the source sentence are aligned to words close in the target sentence. An additional pseudo word of “NULL” is used as the beginning of English sentence for HMM to start with. The (Och and Ney, 2003) model includes other refinements such as special treatment of a jump to a Null word, and a uniform smoothing prior. The HMM with these refinements is used as our baseline. Motivated by the work in both (Och and Ney, 2000) and (Toutanova et al., 2002), we propose the two following simplest versions of extended HMMs to utilize bilingual word clusters. 2.2 Extensions to HMM with word clusters Let F denote the cluster mapping fj → F(fj ), which assigns French word fj to its cluster ID Fj = F(fj ). Similarly E maps English word ei to its cluster ID of Ei = E(ei ). In this paper, we assume each word belongs to one cluster only. With bilingual word clusters, we can extend the HMM model in Eqn. 1 in the following two ways: P (f1J |eI1 ) = P aJ 1 QJ j=1 P (fj |eaj )· P (aj |aj−1 , E(eaj−1 ), F(fj−1 )), (3) where E(eaj−"
W05-0804,J03-1002,0,0.00752818,"ch French word fj is an observation, and it is generated by a HMM state defined as [eaj , aj ], where the alignment aj for position j is considered to have a dependency on the previous alignment aj−1 . Thus the first-order HMM is defined as follows: P (f1J |eI1 ) = J XY P (fj |eaj )P (aj |aj−1 ), (2) j=1 aJ 1 26 where P (aj |aj−1 ) is the transition probability. This model captures the assumption that words close in the source sentence are aligned to words close in the target sentence. An additional pseudo word of “NULL” is used as the beginning of English sentence for HMM to start with. The (Och and Ney, 2003) model includes other refinements such as special treatment of a jump to a Null word, and a uniform smoothing prior. The HMM with these refinements is used as our baseline. Motivated by the work in both (Och and Ney, 2000) and (Toutanova et al., 2002), we propose the two following simplest versions of extended HMMs to utilize bilingual word clusters. 2.2 Extensions to HMM with word clusters Let F denote the cluster mapping fj → F(fj ), which assigns French word fj to its cluster ID Fj = F(fj ). Similarly E maps English word ei to its cluster ID of Ei = E(ei ). In this paper, we assume each wor"
W05-0804,E99-1010,0,0.493638,"(Ei |Ei−1 )P (ei |Ei ). (7) i=1 We need to fix the number of clusters beforehand, otherwise the optimum is reached when each word 27 = arg max {F } J Y P (Fj |Eaj )P (fj |Fj ). (8) j=1 Overall, this bilingual word clustering algorithm is essentially a two-step approach. In the first step, E is inferred by optimizing the monolingual likelihood of English data, and secondly F is inferred by optimizing the bilingual part without changing E. In this way, the algorithm is easy to implement without much change from the monolingual correspondent. This approach was shown to give the best results in (Och, 1999). We use it as our baseline to compare with. 3.2 Bilingual Word Spectral Clustering Instead of using word alignment to bridge the parallel sentence pair, and optimize the likelihood in two separate steps, we develop an alignment-free algorithm using a variant of spectral clustering algorithm. The goal is to build high cluster-level translation quality suitable for translation modelling, and at the same time maintain high intra-cluster similarity , and low inter-cluster similarity for monolingual clusters. 3.2.1 Notations We define the vocabulary VF as the French vocabulary with a size of |VF |"
W05-0804,W02-1012,0,0.0294654,"Missing"
W05-0804,C96-2141,0,0.484864,"o an English sentence with I words denoted by eI1 = e1 e2 ...eI . The SMT system first proposes multiple English hypotheses in its model space. Among all the hypotheses, the system selects the one with the highest conditional probability according to Bayes’s decision rule: eˆI1 = arg max P (eI1 |f1J ) = arg max P (f1J |eI1 )P (eI1 ), {eI1 } {eI1 } (1) where is called translation model, and P (eI1 ) is called language model. The translation model is the key component, which is the focus in this paper. P (f1J |eI1 ) 2.1 HMM-based Translation Model HMM is one of the effective translation models (Vogel et al., 1996), which is easily scalable to very large training corpus. To model word-to-word translation, we introduce the mapping j → aj , which assigns a French word fj in position j to a English word ei in position i = aj denoted as eaj . Each French word fj is an observation, and it is generated by a HMM state defined as [eaj , aj ], where the alignment aj for position j is considered to have a dependency on the previous alignment aj−1 . Thus the first-order HMM is defined as follows: P (f1J |eI1 ) = J XY P (fj |eaj )P (aj |aj−1 ), (2) j=1 aJ 1 26 where P (aj |aj−1 ) is the transition probability. This"
W05-0804,W05-0825,1,0.848237,"Missing"
W05-0836,J93-2003,0,0.00476574,"lly overlapping features to select translations from a set of potential candidates. This decision rule is optimal under the zeroone loss function, minimizing the Sentence Error Rate (Mangu et al., 2000). Using the log-linear form to model pλ (e|f ) gives us the flexibility to introduce overlapping features that can represent global context while decoding (searching the space of candidate translations) and rescoring (ranking a set of candidate translations before performing the arg max operation), albeit at the cost of the traditional source-channel generative model of translation proposed in (Brown et al., 1993). A significant impact of this paradigm shift, however, has been the movement to leverage the flexibility of the exponential model to maximize performance with respect to automatic evaluation met208 Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 208–215, c Ann Arbor, June 2005. Association for Computational Linguistics, 2005 rics. Each evaluation metric considers different aspects of translation quality, both at the sentence and corpus level, often achieving high correlation to human evaluation (Doddington, 2002). It is clear that the decision rule stated in (1) do"
W05-0836,N03-1017,0,0.0974459,"parameters for this training criterion. • We can expect the error surface of the MBR training criterion to contain larger sections of similar altitude, since the decision rule emphasizes consensus. • The piecewise linearity observation made in (Papineni et al., 2002) is no longer applicable since we cannot move the log operation into the expected value. e∈Gen(f ) e0 ∈Gen(f ) 2.3 X e∈Gen(f ) e0 ∈Gen(f ) Loss(e, e0 )pλ (e0 |f ) . (4) (Kumar and Byrne, 2004) explicitly consider selecting both e and a, an alignment between the English and French sentences. Under a phrase based translation model (Koehn et al., 2003; Marcu and Wong, 2002), this distinction is important and will be discussed in more detail. The representation of the evaluation metric or the Loss function is in the decision rule, rather than in the training criterion for the exponential model. This criterion is hard to optimize for the same reason as the criterion in (3b): the objective function is not continuous in λ. To make things worse, it is more expensive to evaluate the function at a given λ, since the decision rule involves a sum over all translations. (5a) λ 3 Score Sampling Motivated by the challenges that the MBR training criter"
W05-0836,N04-1022,0,0.171941,"that it believes should be selected. This is due to the decision rule, rather than the training procedure, as we will see when we consider alternative decision rules. training method to optimize the parameters λ in the exponential model as an explicit form for the conditional distribution in equation (1). The training task under the MBR criterion is 2.2 (5b) We begin with several observations about this optimization criterion. The Minimum Bayes Risk Decision Rule The Minimum Bayes Risk Decision Rule as proposed by (Mangu et al., 2000) for the Word Error Rate Metric in speech recognition, and (Kumar and Byrne, 2004) when applied to translation, changes the decision rule in (2) to select the translation that has the lowest expected loss E[Loss(e, r)], which can be estimated by considering a weighted Loss between e and the elements of the n-best list, the approximation to E, as described in (Mangu et al., 2000). The resulting decision rule is: translλ (f ) = arg min X λ∗ = arg min Loss(translλ (~f ),~r) where translλ (f ) = arg min MBR and the Exponential Model Previous work has reported the success of the MBR decision rule with fixed parameters relating independent underlying models, typically including o"
W05-0836,W02-1018,0,0.0169788,"training criterion. • We can expect the error surface of the MBR training criterion to contain larger sections of similar altitude, since the decision rule emphasizes consensus. • The piecewise linearity observation made in (Papineni et al., 2002) is no longer applicable since we cannot move the log operation into the expected value. e∈Gen(f ) e0 ∈Gen(f ) 2.3 X e∈Gen(f ) e0 ∈Gen(f ) Loss(e, e0 )pλ (e0 |f ) . (4) (Kumar and Byrne, 2004) explicitly consider selecting both e and a, an alignment between the English and French sentences. Under a phrase based translation model (Koehn et al., 2003; Marcu and Wong, 2002), this distinction is important and will be discussed in more detail. The representation of the evaluation metric or the Loss function is in the decision rule, rather than in the training criterion for the exponential model. This criterion is hard to optimize for the same reason as the criterion in (3b): the objective function is not continuous in λ. To make things worse, it is more expensive to evaluate the function at a given λ, since the decision rule involves a sum over all translations. (5a) λ 3 Score Sampling Motivated by the challenges that the MBR training criterion presents, we presen"
W05-0836,P03-1021,0,0.281102,"etric considers different aspects of translation quality, both at the sentence and corpus level, often achieving high correlation to human evaluation (Doddington, 2002). It is clear that the decision rule stated in (1) does not reflect the choice of evaluation metric, and substantial work has been done to correct this mismatch in criteria. Approaches include integrating the metric into the decision rule, and learning λ to optimize the performance of the decision rule. In this paper we will compare and evaluate several aspects of these techniques, focusing on Minimum Error Rate (MER) training (Och, 2003) and Minimum Bayes Risk (MBR) decision rules, within a novel training environment that isolates the impact of each component of these methods. 2 Addressing Evaluation Metrics We now describe competing strategies to address the problem of modeling the evaluation metric within the decoding and rescoring process, and introduce our contribution towards training non-tractable error surfaces. The methods discussed below make use of Gen(f ), the approximation to the complete candidate translation space E, referred to as an n-best list. Details regarding n-best list generation from decoder output can"
W05-0836,P02-1040,0,0.0858947,"Model Previous work has reported the success of the MBR decision rule with fixed parameters relating independent underlying models, typically including only the language model and the translation model as features in the exponential model. We extend the MBR approach by developing a 210 Loss(e, e0 )pλ (e0 |f ) . • The MAP optimal λ∗ are not the optimal parameters for this training criterion. • We can expect the error surface of the MBR training criterion to contain larger sections of similar altitude, since the decision rule emphasizes consensus. • The piecewise linearity observation made in (Papineni et al., 2002) is no longer applicable since we cannot move the log operation into the expected value. e∈Gen(f ) e0 ∈Gen(f ) 2.3 X e∈Gen(f ) e0 ∈Gen(f ) Loss(e, e0 )pλ (e0 |f ) . (4) (Kumar and Byrne, 2004) explicitly consider selecting both e and a, an alignment between the English and French sentences. Under a phrase based translation model (Koehn et al., 2003; Marcu and Wong, 2002), this distinction is important and will be discussed in more detail. The representation of the evaluation metric or the Loss function is in the decision rule, rather than in the training criterion for the exponential model. Th"
W05-0836,W02-1021,0,0.0164824,"Bayes Risk (MBR) decision rules, within a novel training environment that isolates the impact of each component of these methods. 2 Addressing Evaluation Metrics We now describe competing strategies to address the problem of modeling the evaluation metric within the decoding and rescoring process, and introduce our contribution towards training non-tractable error surfaces. The methods discussed below make use of Gen(f ), the approximation to the complete candidate translation space E, referred to as an n-best list. Details regarding n-best list generation from decoder output can be found in (Ueffing et al., 2002). 2.1 Minimum Error Rate Training The predominant approach to reconciling the mismatch between the MAP decision rule and the evaluation metric has been to train the parameters λ of the exponential model to correlate the MAP choice with the maximum score as indicated by the evaluation metric on a development set with known references (Och, 2003). We differentiate between the decision rule translλ (f ) = arg max pλ (e|f ) (3a) e∈Gen(f ) and the training criterion ˆ = arg min Loss(translλ (~f ),~r) λ (3b) λ where the Loss function returns an evaluation result quantifying the difference between th"
W05-0836,2003.mtsummit-papers.53,1,0.897929,"rase based Pharaoh decoder. This comparison is timely, and important, as decoders evolve to represent more complex search space decisions and are evaluated against innovative evaluation metrics of translation quality. 1 Alex Waibel School of Computer Science Carnegie Mellon University waibel@cs.cmu.edu where e is a single candidate translation for f from the set of all English translations E, λ is the parameter vector for the model, and each hk is a feature function of e and f . In practice, we restrict E to the set Gen(f ) which is a set of highly likely translations discovered by a decoder (Vogel et al., 2003). Selecting a translation from this model under the Maximum A Posteriori (MAP) criteria yields translλ (f ) = arg max pλ (e|f ) . (2) e Introduction State of the art statistical machine translation takes advantage of exponential models to incorporate a large set of potentially overlapping features to select translations from a set of potential candidates. This decision rule is optimal under the zeroone loss function, minimizing the Sentence Error Rate (Mangu et al., 2000). Using the log-linear form to model pλ (e|f ) gives us the flexibility to introduce overlapping features that can represent"
W09-0413,W06-1607,0,0.0350992,"arned in a way similar to the other type of reordering rules described above, but contain a gap representing one or several arbitrary words. It is, for example, possible to have the following rule VAFIN * VVPP → VAFIN VVPP *, which puts both parts of the German verb next to each other. 4 4.2 The relative frequencies of the phrase pairs are a very important feature of the translation model, but they often overestimate rare phrase pairs. Therefore, the raw relative frequency estimates found in the phrase translation tables are smoothed by applying modified Kneser-Ney discounting as described in Foster et al. (2006). 4.3 Lattice Phrase Extraction For the test sentences the POS-based reordering allows us to change the word order in the source sentence, so that the sentence can be translated more easily. But this approach does not reorder the training sentences. This may cause problems for phrase extraction, especially for long-range reorderings. For example, if the English verb is aligned to both parts of the German verb, this phrase can not be extracted, since it is not continuous on the German side. In the case of German as source language, the phrase could be extracted if we also reorder the training c"
W09-0413,N03-1017,0,0.00774259,"vector of ones with length equal to the number of features in the other phrase table. The phrase pairs of the other phrase table were added with the features &lt; 1, θ &gt;. 5 Table 1: Translation results for English-German (BLEU Score) System Short-range + Smoothing + Adaptation + Discrim. WA + Long-range reordering 5.2 Test 14.99 15.38 15.44 15.61 15.70 German-English The German-English system was trained on the same data as the English-German except that we perform compound splitting as an additional preprocessing step. The compound splitting was done with the frequency-based method described in Koehn et al. (2003). For this language direction, the initial system already uses phrase table smoothing, adaptation and discriminative word alignment, in addition to the techniques of the English-German baseline system. The results are shown in Table 2. For this language pair, we could improve the translation quality, first, by adding the long-range reordering model. Further improvements could be achieved by using lattice phrase extraction as described before. Results We submitted system translations for the EnglishGerman, German-English, English-French and French-English task. Their performance is measured app"
W09-0413,W08-0303,1,0.791909,"ccount for the different word orders in the languages, we used the POS-based reordering model presented in Rottmann and Vogel (2007). This model learns rules from a parallel text to reorder the source side. The aim is to generate a reordered source side that can be translated in a more monotone way. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 80–84, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 80 of the submitted systems we used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead. This model is trained on a small amount of hand-aligned data and uses the lexical probability as well as the fertilities generated by the GIZA++ Toolkit and POS information. We used all local features, the GIZA and indicator fertility features as well as first order features for 6 directions. The model was trained in three steps, first using the maximum likelihood optimization and afterwards it was optimized towards the alignment error rate. For more details see Niehues and Vogel (2008). In this framework, first, reordering rules are extracted from an aligned parallel corpus and POS"
W09-0413,W09-0435,1,0.815129,"t require the verb to be shifted nearly across the whole sentence. During this shift of the verb, the rest of the sentence remains mainly unchanged. It does not matter which words are in between, since they are moved as a whole. Furthermore, rules including an explicit sequence of POS-tags spanning the whole sentence would be too specific. A lot more rules would be needed to cover long-range reorderings with each rule being applicable only very sparsely. Therefore, we model long-range reordering by generalizing over the unaffected sequences and introduce rules with gaps. (For more details see Niehues and Kolss (2009)). These are learned in a way similar to the other type of reordering rules described above, but contain a gap representing one or several arbitrary words. It is, for example, possible to have the following rule VAFIN * VVPP → VAFIN VVPP *, which puts both parts of the German verb next to each other. 4 4.2 The relative frequencies of the phrase pairs are a very important feature of the translation model, but they often overestimate rare phrase pairs. Therefore, the raw relative frequency estimates found in the phrase translation tables are smoothed by applying modified Kneser-Ney discounting a"
W09-0413,2007.tmi-papers.21,0,0.359617,"mmentary corpora using the Moses Toolkit and apply 4-gram language models created from the respective monolingual News corpora. All feature weights are automatically determined and optimized with respect to BLEU via MERT (Venugopal et al., 2005). For development and testing we used data provided by the WMT’09, news-dev2009a and newsdev2009b, consisting of 1026 sentences each. 3 Word Reordering Model One part of our system that differs from the baseline system is the reordering model. To account for the different word orders in the languages, we used the POS-based reordering model presented in Rottmann and Vogel (2007). This model learns rules from a parallel text to reorder the source side. The aim is to generate a reordered source side that can be translated in a more monotone way. Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 80–84, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 80 of the submitted systems we used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead. This model is trained on a small amount of hand-aligned data and uses the lexical probability as w"
W09-0413,W05-0836,1,0.928162,"aseline architecture, followed by descriptions of the additional system components. Translation results for the different languages and system variants are presented in Section 5. 2.1 Training, Development and Test Data We submitted translations for the EnglishGerman, German-English, English-French and French-English tasks. All systems were trained on the Europarl and News Commentary corpora using the Moses Toolkit and apply 4-gram language models created from the respective monolingual News corpora. All feature weights are automatically determined and optimized with respect to BLEU via MERT (Venugopal et al., 2005). For development and testing we used data provided by the WMT’09, news-dev2009a and newsdev2009b, consisting of 1026 sentences each. 3 Word Reordering Model One part of our system that differs from the baseline system is the reordering model. To account for the different word orders in the languages, we used the POS-based reordering model presented in Rottmann and Vogel (2007). This model learns rules from a parallel text to reorder the source side. The aim is to generate a reordered source side that can be translated in a more monotone way. Proceedings of the Fourth Workshop on Statistical M"
W09-0413,P07-2045,0,\N,Missing
W10-1719,W05-0836,1,0.919745,"Alignment (Niehues and Vogel, 2008) and the Moses Toolkit (Koehn et al., 2007) for extracting phrase pairs and generating the phrase table from the discriminative word alignments. The difficult reordering between German and English was modeled using POS-based reordering rules. These rules were learned using a word-aligned parallel corpus. The POS tags for the reordering models are generated using the TreeTagger (Schmid, 1994) for all languages. Translation is performed by the STTK Decoder (Vogel, 2003) and all systems are optimized towards BLEU using Minimum Error Rate Training as proposed in Venugopal et al. (2005). This paper describes our phrase-based Statistical Machine Translation (SMT) system for the WMT10 Translation Task. We submitted translations for the German to English and English to German translation tasks. Compared to state-of-the-art phrase-based systems we preformed additional preprocessing and used a discriminative word alignment approach. The word reordering was modeled using POS information and we extended the translation model with additional features. 1 Baseline System Introduction In this paper we describe the systems that we built for our participation in the Shared Translation Ta"
W10-1719,W06-1607,0,0.0512277,"ged from one spelling system to the other, for example replacing ’ß’ by ’ss’. If the new word is a correct word according to the hunspell lexicon using the new spelling rules, we map the words. The translation model was trained on the parallel corpus and the word alignment was generated by a discriminative word alignment model, which is described below. The phrase table was trained using the Moses training scripts, but for the German to English system we used a different phrase extraction method described in detail in Section 4.2. In addition, we applied phrase table smoothing as described in Foster et al. (2006). Furthermore, we extended the translation model by additional features for unaligned words and introduced bilingual language models. 4.1 As a last preprocessing step we remove sentences that are too long and empty lines to obtain the final training corpus. Word Reordering Model Reordering was applied on the source side prior to decoding through the generation of lattices encoding possible reorderings of each source sentence that better match the word sequence in the target language. These possible reorderings were learned based on the POS of the source language words in the training corpus an"
W10-1719,2009.mtsummit-papers.5,0,0.0856601,"rase pairs should be longer than the word-based ones. But this is not possible in many decoders or it leads to additional computation overhead. If we instead use a bilingual POS-based language model, the context length of the language model is independent from the other models. Consequently, a longer context can be considered for the POS-based language model than for the wordbased bilingual language model or the phrase pairs. Instead of using POS-based information, this approach can also be applied with other additional linguistic word-level information like word stems. Unaligned Word Feature Guzman et al. (2009) analyzed the role of the word alignment in the phrase extraction process. To better model the relation between word alignment and the phrase extraction process, they introduced two new features into the log-linear model. One feature counts the number of unaligned words on the source side and the other one does the same for the target side. Using these additional features they showed improvements on the Chinese to English translation task. In order to investigate the impact on closer related languages like English and German, we incorporated those two features into our systems. 4.4 Bilingual P"
W10-1719,E03-1076,0,0.125671,"d Vogel (2008) instead. This model is trained on a small amount of hand-aligned data and uses the lexical probability as well as the fertilities generated by the PGIZA++2 Toolkit and POS information. We used all local features, the GIZA and indicator fertility features as well as first order features for 6 directions. The model was trained in three steps, first using maximum likelihood optimization and afterwards it was optimized towards the alignment error rate. For more details see Niehues and Vogel (2008). When translating from German to English, we apply compound splitting as described in Koehn and Knight (2003) to the German corpus. 3 Translation Model 4.2 Lattice Phrase Extraction In translations from German to English, we often have the case that the English verb is aligned to both parts of the German verb. Since this phrase pair is not continuous on the German side, it cannot be extracted. The phrase could be extracted, if we also reorder the training corpus. For the test sentences the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract the ph"
W10-1719,P07-2045,0,0.0103142,"Missing"
W10-1719,W09-0435,1,0.842648,"ding possible reorderings of each source sentence that better match the word sequence in the target language. These possible reorderings were learned based on the POS of the source language words in the training corpus and the information about alignments between source and target language words in the corpus. For short-range reorderings, continuous reordering rules were applied to the test sentences (Rottmann and Vogel, 2007). To model the long-range reorderings between German and English, different types of noncontinuous reordering rules were applied depending on the translation direction. (Niehues and Kolss, 2009). When translating from English to German, most of the changes in word order consist in a shift to the right while typical word shifts in German to English translations take place in the reverse direction. 1 Word Alignment In most phrase-based SMT systems the heuristic grow-diag-final-and is used to combine the alignments generated by GIZA++ from both directions. Then these alignments are used to extract the phrase pairs. We used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead. This model is trained on a small amount of ha"
W10-1719,W08-0303,1,0.75518,"were applied depending on the translation direction. (Niehues and Kolss, 2009). When translating from English to German, most of the changes in word order consist in a shift to the right while typical word shifts in German to English translations take place in the reverse direction. 1 Word Alignment In most phrase-based SMT systems the heuristic grow-diag-final-and is used to combine the alignments generated by GIZA++ from both directions. Then these alignments are used to extract the phrase pairs. We used a discriminative word alignment model (DWA) to generate the alignments as described in Niehues and Vogel (2008) instead. This model is trained on a small amount of hand-aligned data and uses the lexical probability as well as the fertilities generated by the PGIZA++2 Toolkit and POS information. We used all local features, the GIZA and indicator fertility features as well as first order features for 6 directions. The model was trained in three steps, first using maximum likelihood optimization and afterwards it was optimized towards the alignment error rate. For more details see Niehues and Vogel (2008). When translating from German to English, we apply compound splitting as described in Koehn and Knig"
W10-1719,W09-0413,1,0.385327,"angen home Hause. As shown in the example, one problem with this approach is that unaligned source words are ignored in the model. One solution could be to have a second bilingual text ordered according to the source side. But since the target sentence and not the source sentence is generated from left to right during decoding, the integration of a source side language model is more complex. Therefore, as a first approach we only used a language model based on the target word order. Therefore, we build lattices that encode the different reorderings for every training sentence, as described in Niehues et al. (2009). Then we can not only extract phrase pairs from the monotone source path, but also from the reordered paths. So it would be possible to extract the example mentioned before, if both parts of the verb were put together by a reordering rule. To limit the number of extracted phrase pairs, we extract a source phrase only once per sentence even if it may be found on different paths. Furthermore, we do not use the weights in the lattice. If we used the same rules as for reordering the test sets, the lattice would be so big that the number of extracted phrase pairs would be still too high. As mentio"
W10-1719,2007.tmi-papers.21,0,0.171972,"es that are too long and empty lines to obtain the final training corpus. Word Reordering Model Reordering was applied on the source side prior to decoding through the generation of lattices encoding possible reorderings of each source sentence that better match the word sequence in the target language. These possible reorderings were learned based on the POS of the source language words in the training corpus and the information about alignments between source and target language words in the corpus. For short-range reorderings, continuous reordering rules were applied to the test sentences (Rottmann and Vogel, 2007). To model the long-range reorderings between German and English, different types of noncontinuous reordering rules were applied depending on the translation direction. (Niehues and Kolss, 2009). When translating from English to German, most of the changes in word order consist in a shift to the right while typical word shifts in German to English translations take place in the reverse direction. 1 Word Alignment In most phrase-based SMT systems the heuristic grow-diag-final-and is used to combine the alignments generated by GIZA++ from both directions. Then these alignments are used to extrac"
W11-2124,W10-1704,0,0.0698574,"Missing"
W11-2124,N03-2002,0,0.0604885,"been derived from the work of Casacuberta and Vidal (2004), which used finite state transducers for statistical machine translation. In this approach, units of source and target words are used as basic translation units. Then the translation model is implemented as an n-gram model over the tuples. As it is also done in phrase-based translations, the different translations are scored by a log-linear combination of the translation model and additional models. Crego and Yvon (2010) extended the approach to be able to handle different word factors. They used factored language models introduced by Bilmes and Kirchhoff (2003) to integrate different word factors into the translation process. In contrast, we use a log-linear combination of language models on different factors in our approach. A first approach of integrating the idea presented in the n-gram approach into phrase-based machine translation was described in Matusov et al. (2006). In contrast to our work, they used the bilingual units as defined in the original approach and they did not use additional word factors. Hasan et al. (2008) used lexicalized triplets to introduce bilingual context into the translation process. These triplets include source words"
W11-2124,D07-1007,0,0.048499,"the translation process. These triplets include source words from outside the phrase and form and additional probability p(f |e, e0 ) that modifies the conventional word probability of f given e depending on trigger words e0 in the sentence enabling a context-based translation of ambiguous phrases. Other approaches address this problem by integrating word sense disambiguation engines into a phrase-based SMT system. In Chan and Ng (2007) a classifier exploits information such as local col199 locations, parts-of-speech or surrounding words to determine the lexical choice of target words, while Carpuat and Wu (2007) use rich context features based on position, syntax and local collocations to dynamically adapt the lexicons for each sentence and facilitate the choice of longer phrases. In this work we present a method to extend the locally limited context of phrase pairs and n-grams by using bilingual language models. We keep the phrase-based approach as the main SMT framework and introduce an n-gram language model trained in a similar way as the one used in the finite state transducer approach as an additional feature in the loglinear model. 3 Motivation To motivate the introduction of the bilingual lang"
W11-2124,P07-1005,0,0.0236585,"lingual units as defined in the original approach and they did not use additional word factors. Hasan et al. (2008) used lexicalized triplets to introduce bilingual context into the translation process. These triplets include source words from outside the phrase and form and additional probability p(f |e, e0 ) that modifies the conventional word probability of f given e depending on trigger words e0 in the sentence enabling a context-based translation of ambiguous phrases. Other approaches address this problem by integrating word sense disambiguation engines into a phrase-based SMT system. In Chan and Ng (2007) a classifier exploits information such as local col199 locations, parts-of-speech or surrounding words to determine the lexical choice of target words, while Carpuat and Wu (2007) use rich context features based on position, syntax and local collocations to dynamically adapt the lexicons for each sentence and facilitate the choice of longer phrases. In this work we present a method to extend the locally limited context of phrase pairs and n-grams by using bilingual language models. We keep the phrase-based approach as the main SMT framework and introduce an n-gram language model trained in a"
W11-2124,C10-1040,1,0.800514,"rate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data as well as on additional monolingual News data. The translation model as well as the language model was adapted towards the target domain in a log-linear way. The Arabic-to-English system was trained using GALE Arabic data, which contains 6.1M sentences. The word alignment is generated using EMDC, which is a combination of a discriminative approach and the IBM Models as described in Gao et al. (2010). The phrase table is generated using Chaski as described in Gao and Vogel (2010). The language model data we trained on the GIGAWord V3 data plus BBN English data. After splitting the corpus according to sources, individual models were trained. Then the individual models were interpolated to minimize the perplexity on the MT03/MT04 data. For both tasks the reordering was performed as a preprocessing step using POS information from the TreeTagger (Schmid, 1994) for German and using the Amira Tagger (Diab, 2009) for Arabic. For Arabic the approach described in Rottmann and Vogel (2007) was used"
W11-2124,D08-1039,0,0.0110035,"xtended the approach to be able to handle different word factors. They used factored language models introduced by Bilmes and Kirchhoff (2003) to integrate different word factors into the translation process. In contrast, we use a log-linear combination of language models on different factors in our approach. A first approach of integrating the idea presented in the n-gram approach into phrase-based machine translation was described in Matusov et al. (2006). In contrast to our work, they used the bilingual units as defined in the original approach and they did not use additional word factors. Hasan et al. (2008) used lexicalized triplets to introduce bilingual context into the translation process. These triplets include source words from outside the phrase and form and additional probability p(f |e, e0 ) that modifies the conventional word probability of f given e depending on trigger words e0 in the sentence enabling a context-based translation of ambiguous phrases. Other approaches address this problem by integrating word sense disambiguation engines into a phrase-based SMT system. In Chan and Ng (2007) a classifier exploits information such as local col199 locations, parts-of-speech or surrounding"
W11-2124,P07-2045,0,0.0121597,"model on the English-to-German, German-to-English and French-to-English systems with which we participated in the WMT 2011. 5.1 System Description The German-to-English translation system was trained on the European Parliament corpus, News Commentary corpus and small amounts of additional Web data. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data as well as on additional monolingual News data. The translation model as well as the language model was adapted towards the target domain in a log-linear way. The Arabic-to-English system was trained using GALE Arabic data, which contains 6.1M sentences. The word alignment is generated using EMDC, which is a combination of a discriminative approach and the IBM Models as described in Gao et al. (2010). The phrase table is generated using Chaski as described in Gao and Vogel (2010). The language model data we trained on the"
W11-2124,J06-4004,0,0.431121,"Missing"
W11-2124,W08-0303,1,0.31921,"k. On the other hand, we evaluated the approach on the Arabic-to-English direction on News and Web data. Additionally, we present the impact of the bilingual language model on the English-to-German, German-to-English and French-to-English systems with which we participated in the WMT 2011. 5.1 System Description The German-to-English translation system was trained on the European Parliament corpus, News Commentary corpus and small amounts of additional Web data. The data was preprocessed and compound splitting was applied. Afterwards the discriminative word alignment approach as described in (Niehues and Vogel, 2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). The language model was trained on the target side of the parallel data as well as on additional monolingual News data. The translation model as well as the language model was adapted towards the target domain in a log-linear way. The Arabic-to-English system was trained using GALE Arabic data, which contains 6.1M sentences. The word alignment is generated using EMDC, which is a combination of a discriminative approach and the IBM Mod"
W11-2124,W09-0413,1,0.910746,"ined on the GIGAWord V3 data plus BBN English data. After splitting the corpus according to sources, individual models were trained. Then the individual models were interpolated to minimize the perplexity on the MT03/MT04 data. For both tasks the reordering was performed as a preprocessing step using POS information from the TreeTagger (Schmid, 1994) for German and using the Amira Tagger (Diab, 2009) for Arabic. For Arabic the approach described in Rottmann and Vogel (2007) was used covering short-range reorderings. For the German-to-English translation task the extended approach described in Niehues et al. (2009) was used to cover also the long-range reorderings typical when translating between German and English. For both directions an in-house phrase-based decoder (Vogel, 2003) was used to generate the translation hypotheses and the optimization was performed using MER training. The performance on the testsets were measured in case-insensitive BLEU and TER scores. 5.2 German to English We evaluated the approach on two different test sets from the News Commentary domain. The first consists of 2000 sentences with one reference. It will be referred to as Test 1. The second test set consists of 1000 sen"
W11-2124,2007.tmi-papers.21,1,0.445573,"as described in Gao et al. (2010). The phrase table is generated using Chaski as described in Gao and Vogel (2010). The language model data we trained on the GIGAWord V3 data plus BBN English data. After splitting the corpus according to sources, individual models were trained. Then the individual models were interpolated to minimize the perplexity on the MT03/MT04 data. For both tasks the reordering was performed as a preprocessing step using POS information from the TreeTagger (Schmid, 1994) for German and using the Amira Tagger (Diab, 2009) for Arabic. For Arabic the approach described in Rottmann and Vogel (2007) was used covering short-range reorderings. For the German-to-English translation task the extended approach described in Niehues et al. (2009) was used to cover also the long-range reorderings typical when translating between German and English. For both directions an in-house phrase-based decoder (Vogel, 2003) was used to generate the translation hypotheses and the optimization was performed using MER training. The performance on the testsets were measured in case-insensitive BLEU and TER scores. 5.2 German to English We evaluated the approach on two different test sets from the News Comment"
W11-2124,W10-1719,1,\N,Missing
W11-2124,J04-2004,0,\N,Missing
W11-2124,N03-1017,0,\N,Missing
W11-2124,W11-2145,1,\N,Missing
W11-2142,J04-2004,0,0.0163665,"e a bilingual language model, an additional language model in the phrase-based system in which each token consist of a target word and all 360 source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor. 2.3 LIMSI-CNRS Single System 2.3.1 System overview The LIMSI system is built with n-code2 , an open source statistical machine translation system based on bilingual n-grams. 2.3.2 n-code Overview In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model w"
W11-2142,J07-2003,0,0.0225833,"and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase"
W11-2142,W08-0310,1,0.899949,"Missing"
W11-2142,P07-1019,0,0.0206925,"or Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded ph"
W11-2142,P02-1040,0,0.102913,"Missing"
W11-2142,E03-1076,0,0.0231201,"l table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each sentence is performed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogeneous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 For the German→English task, RWTH conducted experiments comparing the standard phrase extraction with the phrase training technique described in Section 2.1.3. Further experiments included the use of additional language model training data, reranking of n-best lists generated by the phrase-based system, and different optimization criteria. A considerable increase in translation quality can be achieved by application of German compound splitting (Koehn and Knight, 20"
W11-2142,W07-0732,1,0.818478,"a statistical post editing (SPE) component. The SYSTRAN system is traditionally classified as a rule-based system. However, over the decades, its development has always been driven by pragmatic considerations, progressively integrating many of the most efficient MT approaches and techniques. Nowadays, the baseline engine can be considered as a linguistic-oriented system making use of dependency analysis, general transfer rules as well as of large manually encoded dictionaries (100k − 800k entries per language pair). The basic setup of the SPE component is identical to the one described in (L. Dugast and Koehn, 2007). A statistical translation model is trained on the rule-based translation of the source and the target side of the parallel corpus. This is done separately for each parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Moreover, the following measures − limiting unwanted statistical effects − were applied: • Named entities are replaced by special tokens on both sides. This usually improves word alignment, since the vocabulary size is significantly reduced. In addition, entity translation is handled more reliably by t"
W11-2142,E06-1005,1,0.83205,"d 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. A deeper description will be also given in the WMT11 system combination paper of RWTH Aachen University. For this task only the A2L framework has been used. 4 Experiments We tried different system combinations with different sets of single systems and different optimization criteria. As RWTH has two different translation systems, we pu"
W11-2142,W09-0435,1,0.836207,"ystem applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract also phrase pairs for originally discontinuous phrases and could apply them during translation of reordered test sentences. Therefore,"
W11-2142,J03-1002,1,0.00747756,"joint translation by combining the knowledge of the four project partners. Each group develop and maintain their own different machine translation system. These single systems differ not only in their general approach, but also in the preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algor"
W11-2142,P03-1021,0,0.147772,"etups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tab"
W11-2142,P07-2045,0,0.0131162,"parallel corpus (whose target is identical to the source). This was added to the parallel text in order to improve word alignment. • Singleton phrase pairs are deleted from the phrase table to avoid overfitting. • Phrase pairs not containing the same number of entities on the source and the target side are also discarded. • Phrase pairs appearing less than 2 times were pruned. The SPE language model was trained 15M phrases from the news/europarl corpora, provided as training data for WMT 2011. Weights for these separate models were tuned by the MERT algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accor"
W11-2142,2007.tmi-papers.21,0,0.0203552,"lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net/ ger (Schmid, 1994). In addition, the system applies a bilingual language model to extend the context of source language words available for translation. The individual models are described briefly in the following. 2.2.3 POS-based Reordering Model We use a reordering model that is based on partsof-speech (POS) and learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. In addition to continuous reordering rules that model short-range reordering (Rottmann and Vogel, 2007), we apply noncontinuous rules to address long-range reorderings as typical for German-English translation (Niehues and Kolss, 2009). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. 2.2.4 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract als"
W11-2142,N04-4026,0,0.0225696,"ew In a nutshell, the translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a weak distance-based distortion model; and finally a wordbonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003), using the news"
W11-2142,W05-0836,1,0.901096,"rd heuristic phrase extraction techniques, performing force alignment phrase training (FA) gives an improvement in BLEU on newstest2008 and newstest2009, but a degradation in TER. The addition of LDC Gigaword corpora (+GW) to the language model training data shows improvements in both BLEU and TER. Reranking was done on 1000-best lists generated by the the best available 359 Preprocessing System Overview The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation. Optimization with regard to the BLEU score is done using Minimum Error Rate Training as described by Venugopal et al. (2005). The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a GIZA++ Word Alignment. We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained on the Gigaword corpus. Reordering is performed based on continuous and non-continuous POS rules to cover short and long-range reorderings. The long-range reordering rules were also applied to the training corpus and phrase extraction was performed on the resulting reordering lattices. Part-of-speech tags are obtained using the TreeTag1 http://hunspell.sourceforge.net"
W11-2142,W10-1738,1,0.832324,"are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hierarchical System For the hierarchical setups described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-the-art extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institut"
W11-2142,P10-1049,1,0.823271,"ons. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard MERT (Och, 2003) on 100-best lists. 2.1.3 Phrase Model Training 2.2 Karlsruhe Institute of Technology Single System 2.2.1 For some PBT systems a forced alignment procedure was applied to train the phrase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and"
W11-2142,W06-3110,1,0.850765,"ase translation model as described in Wuebker et al. (2010). A modified version of the translation decoder is used to produce a phrase alignment on the bilingual training data. The phrase translation probabilities are estimated from their relative frequencies in the phrasealigned training data. In addition to providing a statistically well-founded phrase model, this has the benefit of producing smaller phrase tables and thus allowing more rapid and less memory consuming experiments with a better translation quality. 2.1.4 system (PBT (FA)+GW). Following models were applied: n-gram posteriors (Zens and Ney, 2006), sentence length model, a 6-gram LM and IBM-1 lexicon models in both normal and inverse direction. These models are combined in a log-linear fashion and the scaling factors are tuned in the same manner as the baseline system (using TER−4BLEU on newstest2009). The final table includes two identical Jane systems which are optimized on different criteria. The one optimized on TER−BLEU yields a much lower TER. Final Systems We preprocess the training data prior to training the system, first by normalizing symbols such as quotes, dashes and apostrophes. Then smart-casing of the first words of each"
W11-2142,2008.iwslt-papers.8,1,0.820865,"preprocessing of training and test data. To take the advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. RWTH Aachen Single Systems For the WMT 2011 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned bilingual corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. Parameters are optimized with the DownhillSimplex algorithm (Nelder and Mead, 1965) on the word graph. 358 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 358–364, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics 2.1.2 Hie"
W11-2142,D08-1076,0,\N,Missing
W11-2145,E03-1076,0,0.282695,"us to train bigger language models. For training a discriminative word alignment model, a small amount of hand-aligned data was used. 2.2 Preprocessing The training data is preprocessed prior to training the system. This includes normalizing special symbols, smart-casing the first words of each sentence and removing long sentences and sentences with length mismatch. For the German parts of the training corpus we use the hunspell1 lexicon to map words written according to old German spelling to new German spelling, to obtain a corpus with homogenous spelling. Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system to reduce the out-of-vocabulary problem for German compound words. 2.3 Special filtering of the Giga parallel Corpus The Giga corpus incorporates non-neglegible amounts of noise even after our usual preprocessing. This noise may be due to different causes. For instance: non-standard HTML characters, meaningless parts composed of only hypertext codes, sentences which are only partial translation of the source, or eventually not a correct translation at all. Such noisy pairs potentially degrade the translation model qu"
W11-2145,J05-4003,0,0.136591,"Missing"
W11-2145,W09-0435,1,0.795833,"t approach that relies on part-of-speech (POS) sequences. By abstracting from surface words to parts-of-speech, we expect to model the reordering more accurately. 2.4.1 POS-based Reordering Model To model reordering we first learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. Continuous reordering rules are extracted as described in Rottmann and Vogel (2007) to model short-range reorderings. When translating between German and English, we apply a modified reordering model with non-continuous rules to cover also long-range reorderings (Niehues and Kolss, 2009). The reordering rules are applied to the source text and the original order of words and the reordered sentence variants generated by the rules are encoded in a word lattice which is used as input to the decoder. 381 2.4.2 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated more easily. If we apply this also to the training sentences, we would be able to extract the phrase pairs for originally discontinuous phrases and could apply them during translation of reordered test se"
W11-2145,W08-0303,1,0.785274,"ents including additional models that enhance translation quality by introducing alternative or additional information into the translation or language modelling process. 2.5.1 Discriminative Word Alignment In most of our systems we use the PGIZA++ Toolkit4 to generate alignments between words in the training corpora. The word alignments are generated in both directions and the grow-diag-final-and heuristic is used to combine them. The phrase extraction is then done based on this word alignment. In the English-German system we applied the Discriminative Word Alignment approach as described in Niehues and Vogel (2008) instead. This alignment model is trained on a small corpus of hand-aligned data and uses the lexical probability as well as the fertilities generated by the PGIZA++ Toolkit and POS information. 2.5.2 Bilingual Language Model In phrase-based systems the source sentence is segmented by the decoder according to the best combination of phrases that maximize the translation and language model scores. This segmentation into phrases leads to the loss of context information at the phrase boundaries. Although more target side context is available to the language model, source 4 http://www.cs.cmu.edu/˜"
W11-2145,W11-2124,1,0.753565,"available to the language model, source 4 http://www.cs.cmu.edu/˜qing/ side context would also be valuable for the decoder when searching for the best translation hypothesis. To make also source language context available we use a bilingual language model, an additional language model in the phrase-based system in which each token consist of a target word and all source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor and the bilingual language model is applied to the additional factor like a normal language model. For more details see (Niehues et al., 2011). 2.5.3 Parallel phrase scoring The process of phrase scoring is held in two runs. The objective of the first run is to compute the necessary counts and to estimate the scores, all based on the source phrases; while the second run is similarly held based on the target phrases. Thus, the extracted phrases have to be sorted twice: once by source phrase and once by target phrase. These two sorting operations are almost always done on an external storage device and hence consume most of the time spent in this step. The phrase scoring step was reimplemented in order to exploit the available computa"
W11-2145,P02-1040,0,0.0863525,"perplexity drops by half since the POS language model helps constructing sentences that have a better structure. System BLEU no POS LM POS LM 16.64 16.88 avg. ngram length Word POS 2.77 3.18 2.81 3.40 PPL POS 66.78 33.36 Table 3: Analysis of context length 3 Results Using the models described above we performed several experiments leading finally to the systems used for generating the translations submitted to the workshop. The following sections describe the experiments for the individual language pairs and show the translation results. The results are reported as case-sensitive BLEU scores (Papineni et al., 2002) on one reference translation. 3.1 German-English The German-to-English baseline system applies short-range reordering rules and uses a language model trained on the EPPS and News Commentary. By exchanging the baseline language model by one trained on the News Shuffle corpus we improve the translation quality considerably, by more than 3 BLEU points. When we expand the coverage of the reordering rules to enable long-range reordering we can improve even further by 0.4 and adding a second language model trained on the English Gigaword corpus we gain another 0.3 BLEU points. To ensure that the ph"
W11-2145,2007.tmi-papers.21,0,0.252178,"7 million pairs. Thus throwing around 6 million pairs. 2.4 Word Reordering In contrast to modeling the reordering by a distancebased reordering model and/or a lexicalized distortion model, we use a different approach that relies on part-of-speech (POS) sequences. By abstracting from surface words to parts-of-speech, we expect to model the reordering more accurately. 2.4.1 POS-based Reordering Model To model reordering we first learn probabilistic rules from the POS tags of the words in the training corpus and the alignment information. Continuous reordering rules are extracted as described in Rottmann and Vogel (2007) to model short-range reorderings. When translating between German and English, we apply a modified reordering model with non-continuous rules to cover also long-range reorderings (Niehues and Kolss, 2009). The reordering rules are applied to the source text and the original order of words and the reordered sentence variants generated by the rules are encoded in a word lattice which is used as input to the decoder. 381 2.4.2 Lattice Phrase Extraction For the test sentences, the POS-based reordering allows us to change the word order in the source sentence so that the sentence can be translated"
W11-2145,C08-1098,0,0.0985813,"ge Models In addition to surface word language models, we did experiments with language models based on part-of-speech for English-German. We expect that having additional information in form of probabilities of part-of-speech sequences should help especially in case of the rich morphology of German and 382 #pairs(G) 0.203 1.444 1.693 Moses ∗103 (s) 25.99 184.19 230.97 KIT ∗103 (s) 17.58 103.41 132.79 Table 2: Comparison of Moses and KIT phrase extraction systems therefore the more difficult target language generation. The part-of-speeches were generated using the TreeTagger and the RFTagger (Schmid and Laws, 2008), which produces more fine-grained tags that include also person, gender and case information. While the TreeTagger assigns 54 different POS tags to the 357K German words in the corpus, the RFTagger produces 756 different fine-grained tags on the same corpus. We tried n-gram lengths of 4 and 7. While no improvement in translation quality could be achieved using the POS language models based on the normal POS tags, the 4-gram POS language model based on fine-grained tags could improve the translation system by 0.2 BLEU points as shown in Table 3. Surprisingly, increasing the n-gram length to 7"
W11-2145,W05-0836,1,0.916992,"sed on a GIZA++ word alignment. The language model was trained on the monolingual parts of the same corpora by the SRILM Toolkit (Stolcke, 2002). It is a 4-gram SRI language model using Kneser-Ney smoothing. The problem of word reordering is addressed using the POS-based reordering model as described in Section 2.4. The part-of-speech tags for the reordering model are obtained using the TreeTagger (Schmid, 1994). An in-house phrase-based decoder (Vogel, 2003) is used to perform translation and optimization with regard to the BLEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). During decoding only the top 20 translation options for every source phrase were considered. 2.1 Data We trained all systems using the parallel EPPS and News Commentary corpora. In addition, the UN corpus and the Giga corpus were used for training 379 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 379–385, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics the French-English systems. Optimization was done for most languages using the news-test2008 data set and news-test2010 was used as test set. The only exception is GermanE"
W12-3140,J04-2004,0,0.0800912,"nslation model relies on a specific decomposition of the joint probability of a sentence pair P(s, t) using the n-gram assumption: a sentence pair is decomposed into a sequence of bilingual units called tuples, defining a joint segmentation of the source and target. In the approach of (Mari˜no et al., 2006), this segmentation is a by-product of source reordering which ultimately derives from initial word and phrase alignments. 2.3.1 An Overview of n-code The baseline translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a wordbonus model and a tuple-bonus model"
W12-3140,J07-2003,0,0.0283293,"322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting"
W12-3140,W08-0310,1,0.935329,"Missing"
W12-3140,2010.iwslt-papers.6,0,0.0806631,"as last year5 and thus the same target language model as detailed in (Allauzen et al., 2011). For English, we took advantage of our in-house text processing tools for tokenization and detokenization steps (D´echelotte et al., 2008) and our system was built in ”true-case”. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which is detrimental both at training and decoding time. Thus, the German side was normalized using a specific pre-processing scheme (Allauzen et al., 2010; Durgar El-Kahlout and Yvon, 2010), which notably aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. 2.4 SYSTRAN Software, Inc. Single System The data submitted by SYSTRAN were obtained by a system composed of the standard SYSTRAN MT engine in combination with a statistical post editing (SPE) component. 4 http://geek.kyloo.net/software 5 The fifth edition of the English Gigaword (LDC2011T07) was not used. 325 The SYSTRAN system is traditionally classified as a rule-based system. However, over the decades, its development has alwa"
W12-3140,P07-1019,0,0.0343944,"on criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The"
W12-3140,E03-1076,0,0.55884,"ranslation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The language model is trained on the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). 2.2 2.2.1 Karlsruhe Institute of Technology Single Syst"
W12-3140,W07-0732,1,0.793396,"rule-based system. However, over the decades, its development has always been driven by pragmatic considerations, progressively integrating many of the most efficient MT approaches and techniques. Nowadays, the baseline engine can be considered as a linguistic-oriented system making use of dependency analysis, general transfer rules as well as of large manually encoded dictionaries (100k 800k entries per language pair). The SYSTRAN phrase-based SPE component views the output of the rule-based system as the source language, and the (human) reference translation as the target language, see (L. Dugast and Koehn, 2007). It performs corrections and adaptions learned from the 5-gram language model trained on the parallel target-to-target corpus. Moreover, the following measures - limiting unwanted statistical effects - were applied: • Named entities, time and numeric expressions are replaced by special tokens on both sides. This usually improves word alignment, since the vocabulary size is significantly reduced. In addition, entity translation is handled more reliably by the rule-based engine. • The intersection of both vocabularies (i.e. vocabularies of the rule-based output and the reference translation) is"
W12-3140,N12-1005,1,0.858653,"bilingual pairs, which means that the underlying vocabulary can be quite large. Unfortunately, the parallel data available to train these models are typically smaller than the corresponding monolingual corpora used to train target language models. It is very likely then, that such models should face severe estimation problems. In such setting, using neural network language 3 Part-of-speech labels for English and German are computed using the TreeTagger (Schmid, 1995). 2 http://ncode.limsi.fr/ 324 model techniques seem all the more appropriate. For this study, we follow the recommendations of Le et al. (2012), who propose to factor the joint probability of a sentence pair by decomposing tuples in two (source and target) parts, and further each part in words. This yields a word factored translation model that can be estimated in a continuous space using the SOUL architecture (Le et al., 2011). The design and integration of a SOUL model for large SMT tasks is far from easy, given the computational cost of computing n-gram probabilities. The solution used here was to resort to a two pass approach: the first pass uses a conventional back-off n-gram model to produce a k-best list; in the second pass, t"
W12-3140,J06-4004,1,0.843277,"Missing"
W12-3140,E06-1005,1,0.849031,"glish LDC Gigaword corpus using KneserNey (Kneser and Ney, 1995) smoothing was added. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice according to a couple of statistical models is selected as consensus translation. 4 Experiments This year, we tried different sets of single systems for system combination. As RWTH has two different translation systems, we put the output of both systems into system combination. Although both systems have the same preprocessing and language model, their hypotheses differ because of their different decoding approach."
W12-3140,D09-1022,1,0.869178,"done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using t"
W12-3140,2011.iwslt-evaluation.9,1,0.871665,"ocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We us"
W12-3140,P10-2041,0,0.0181873,"ound words with the frequency-based method described in (Koehn and Knight, 2003a). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.1.4 Language Model For both decoders a 4-gram language model is applied. The language model is trained on the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. For the 109 French-English, UN and LDC Gigaword corpora RWTH applied the data selection technique described in (Moore and Lewis, 2010). 2.2 2.2.1 Karlsruhe Institute of Technology Single System quotes, dashes and apostrophes. Then smart-casing of the first words of each sentence is performed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogenous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003b). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-bas"
W12-3140,W09-0435,1,0.860476,"Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Furthermore, we use a 5gram cluster-based language model trained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as i"
W12-3140,W08-0303,1,0.84967,"very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters o"
W12-3140,2011.iwslt-papers.6,1,0.847434,"he Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known translation of the related word to synthesize a translation for the OOV word. By this approach we were for example able to translate Kaminen into chimneys using the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Further"
W12-3140,W11-2124,1,0.868397,"length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system as described in (Mediani et al., 2011). At last, we tried to find translations for out-of-vocabulary (OOV) words by using quasimorphological operations as described in Niehues and Waibel (2011). For each OOV word, we try to find a related word that we can translate. We modify the ending letters of the OOV word and learn quasimorphological operations to be performed on the known"
W12-3140,J03-1002,1,0.00977827,"RO partner trained their systems on the parallel Europarl and News Commentary corpora. All single systems were tuned on the newstest2009 or newstest2010 development set. The newstest2011 dev set was used to train the system combination parameters. Finally, the newstest2008-newstest2010 dev sets were used to compare the results of the different system combination settings. In this Section all four different system engines are presented. 2.1 RWTH Aachen Single Systems For the WMT 2012 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram langu"
W12-3140,P03-1021,0,0.230828,"n (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out"
W12-3140,P07-2045,0,0.00885526,"to improve word alignment. • Singleton phrase pairs are deleted from the phrase table to avoid overfitting. • Phrase pairs not containing the same number of entities on the source and the target side are also discarded. The SPE language model was trained on 2M bilingual phrases from the news/Europarl corpora, provided as training data for WMT 2012. An additional language model built from 15M phrases of the English LDC Gigaword corpus using KneserNey (Kneser and Ney, 1995) smoothing was added. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (P. Koehn et al., 2007), using the provided news development set. 3 RWTH Aachen System Combination System combination is used to produce consensus translations from multiple hypotheses produced with different translation engines that are better in terms of translation quality than any of the individual hypotheses. The basic concept of RWTH’s approach to machine translation system combination has been described by Matusov et al. (2006; 2008). This approach includes an enhanced alignment and reordering framework. A lattice is built from the input hypotheses. The translation with the best score within the lattice accor"
W12-3140,W08-1006,0,0.100843,"ained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. For the test sentences, the reordering based on parts-of-speech and trees allows us to change the word order in the source sentence so that the sentence can be translated more easily. In addition, we build reordering lattices for all trainin"
W12-3140,2007.tmi-papers.21,0,0.266844,"the known translation Kamin # chimney. 2.2.4 Preprocessing System Overview Language Models We use two 4-gram SRI language models, one trained on the News Shuffle corpus and one trained 1 http://hunspell.sourceforge.net/ on the Gigaword corpus. Furthermore, we use a 5gram cluster-based language model trained on the News Shuffle corpus. The word clusters were created using the MKCLS algorithm. We used 100 word clusters. 2.2.5 Reordering Model Reordering is performed based on part-of-speech tags obtained using the TreeTagger (Schmid, 1994). Based on these tags we learn probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules to cover short and long-range reorderings. The rules are learned from the training corpus and the alignment. In addition, we learned tree-based reordering rules. Therefore, the training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The tree-based rules consist of the head node of a subtree and all its children as well as the new order and a probability. These rules were applied recursively. The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are"
W12-3140,N04-4026,0,0.0510078,"of n-code The baseline translation model is implemented as a stochastic finite-state transducer trained using a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finitestate reordering model, which uses part-of-speech information3 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a wordbonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones used in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003), using the n"
W12-3140,W05-0836,1,0.92366,"rmed. For the German part of the training corpus we use the hunspell1 lexicon to learn a mapping from old German spelling to new German spelling to obtain a corpus with homogenous spelling. In addition, we perform compound splitting as described in (Koehn and Knight, 2003b). Finally, we remove very long sentences, empty lines, and sentences that probably are not parallel due to length mismatch. 2.2.2 The KIT system uses an in-house phrase-based decoder (Vogel, 2003) to perform translation and optimization with regard to the B LEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). 2.2.3 We preprocess the training data prior to training the system, first by normalizing symbols such as 323 Translation Models The translation model is trained on the Europarl and News Commentary Corpus and the phrase table is based on a discriminative word alignment (Niehues and Vogel, 2008). In addition, the system applies a bilingual language model (Niehues et al., 2011) to extend the context of source language words available for translation. Furthermore, we use a discriminative word lexicon as introduced in (Mauser et al., 2009). The lexicon was trained and integrated into our system a"
W12-3140,W10-1738,1,0.875756,"by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2.1.2 Hierarchical System For the hierarchical setups (HPBT) described in this paper, the open source Jane toolkit (Vilar et al., 2010) is employed. Jane has been developed at RWTH and implements the hierarchical approach as introduced by Chiang (2007) with some state-of-theart extensions. In hierarchical phrase-based translation, a weighted synchronous context-free grammar is induced from parallel text. In addition to contiguous lexical phrases, hierarchical phrases with up to two gaps are extracted. The search is typically carried out using the cube pruning algorithm (Huang and Chiang, 2007). The model weights are optimized with standard Mert (Och, 2003) on 100-best lists. The optimization criterium is 4B LEU −T ER. 2.1.3 P"
W12-3140,2008.iwslt-papers.8,1,0.841876,"parameters. Finally, the newstest2008-newstest2010 dev sets were used to compare the results of the different system combination settings. In this Section all four different system engines are presented. 2.1 RWTH Aachen Single Systems For the WMT 2012 evaluation the RWTH utilized RWTH’s state-of-the-art phrase-based and hierarchical translation systems. GIZA++ (Och and Ney, 2003) was employed to train word alignments, language models have been created with the SRILM toolkit (Stolcke, 2002). 2.1.1 Phrase-Based System The phrase-based translation (PBT) system is similar to the one described in Zens and Ney (2008). After phrase pair extraction from the word-aligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature 322 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 322–329, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics set also includes an n-gram language model, phraselevel IBM-1 and word-, phrase- and distortionpenalties, which are combined in log-linear fashion. The model weights are optimized with standard Mert (Och, 2003) on 200-best lists. The optimization criterium is B LEU. 2."
W12-3140,W11-2135,1,\N,Missing
W12-3140,W10-1704,1,\N,Missing
W12-3140,D08-1076,0,\N,Missing
W12-3144,W11-2145,1,0.62336,"the target language. In this evaluation, the POS language model is applied for the English-German system. We expect that having additional information in form of probabilities of POS sequences should help especially in case of the rich morphology of German. The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information. We use a 9-gram language model on the News Shuffle corpus and the German side of all parallel corpora. More details and discussions about the POS language model can be found in Herrmann et al. (2011). 2.5.2 Cluster Language Models The cluster language model follows a similar idea as the POS language model. Since there is a data sparsity problem when we substitute words with the word classes, it is possible to make use of larger context information. In the POS language model, POS tags are the word classes. Here, we generated word classes in a different way. First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes. Second, we replace the words in the corpus by their cluster IDs. Finally, we train an n-gram language model on this corpus consis"
W12-3144,E03-1076,0,0.0436961,"ed on 500 hand-aligned sentences selected from the EPPS corpus. 2.2 Preprocessing The training data is preprocessed prior to training the system. This includes normalizing special symbols, smart-casing the first word of each sentence and removing long sentences and sentences with length mismatch. For the German parts of the training corpus, in order to obtain a homogenous spelling, we use the hunspell1 lexicon to map words written according to old German spelling rules to new German spelling rules. In order to reduce the OOV problem of German compound words, Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system. The Giga corpus received a special preprocessing by removing noisy pairs using an SVM classifier as described in Mediani et al. (2011). The SVM classifier training and test sets consist of randomly selected sentence pairs from the corpora of EPPS, NC, tuning, and test sets. Giving at the end around 16 million sentence pairs. 2.3 Word Reordering In contrast to modeling the reordering by a distancebased reordering model and/or a lexicalized distor1 http://hunspell.sourceforge.net/ 350 tion model, we use a different ap"
W12-3144,D09-1022,0,0.0220084,"ore target side context is available to the language model, source side context would also be valuable for the decoder when searching for the best translation hypothesis. To make also source language context available we use a bilingual language model, in which each token consists of a target word and all source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor and the bilingual language model is applied to the additional factor like a normal language model. For more details see Niehues et al. (2011). 351 2.4.3 Discriminative Word Lexica Mauser et al. (2009) have shown that the use of discriminative word lexica (DWL) can improve the translation quality. For every target word, they trained a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per one source word. When applying DWL in our experiments, we would like to have the same conditions for the training and test case. For this we would need to change the score of the feature only if a new word is added to the hypothesis. If a word is added the second time, we do not want to change the feature value. In order to keep track o"
W12-3144,2011.iwslt-evaluation.9,1,0.91403,"d of each sentence and removing long sentences and sentences with length mismatch. For the German parts of the training corpus, in order to obtain a homogenous spelling, we use the hunspell1 lexicon to map words written according to old German spelling rules to new German spelling rules. In order to reduce the OOV problem of German compound words, Compound splitting as described in Koehn and Knight (2003) is applied to the German part of the corpus for the German-to-English system. The Giga corpus received a special preprocessing by removing noisy pairs using an SVM classifier as described in Mediani et al. (2011). The SVM classifier training and test sets consist of randomly selected sentence pairs from the corpora of EPPS, NC, tuning, and test sets. Giving at the end around 16 million sentence pairs. 2.3 Word Reordering In contrast to modeling the reordering by a distancebased reordering model and/or a lexicalized distor1 http://hunspell.sourceforge.net/ 350 tion model, we use a different approach that relies on POS sequences. By abstracting from surface words to POS, we expect to model the reordering more accurately. For German-to-English, we additionally apply reordering rules learned from syntacti"
W12-3144,W09-0435,1,0.802047,"to POS, we expect to model the reordering more accurately. For German-to-English, we additionally apply reordering rules learned from syntactic parse trees. 2.3.1 POS-based Reordering Model In order to build the POS-based reordering model, we first learn probabilistic rules from the POS tags of the training corpus and the alignment. Continuous reordering rules are extracted as described in Rottmann and Vogel (2007) to model short-range reorderings. When translating between German and English, we apply a modified reordering model with non-continuous rules to cover also long-range reorderings (Niehues and Kolss, 2009). 2.3.2 Tree-based Reordering Model Word order is quite different between German and English. And during translation especially verbs or verb particles need to be shifted over a long distance in a sentence. Using discontinuous POS rules already improves the translation tremendously. In addition, we apply a tree-based reordering model for the German-English translation. Syntactic parse trees provide information about the words in a sentence that form constituents and should therefore be treated as inseparable units by the reordering model. For the tree-based reordering model, syntactic parse tr"
W12-3144,W08-0303,1,0.835417,"rase-based MT. In addition to the POS-based reordering model used in past years, for German-English we extended it to also use rules learned using syntax trees. The translation model was extended by the bilingual language model and a discriminative word lexicon using a maximum entropy classifier. For the French-English and English-French translation systems, we also used phrase table adaptation to avoid System Description For the French↔English systems the phrase table is based on a GIZA++ word alignment, while the systems for German↔English use a discriminative word alignment as described in Niehues and Vogel (2008). The language models are 4-gram SRI language models using Kneser-Ney smoothing trained by the SRILM Toolkit (Stolcke, 2002). The problem of word reordering is addressed with POS-based and tree-based reordering models as described in Section 2.3. The POS tags used in the reordering model are obtained using the TreeTagger (Schmid, 1994). The syntactic parse trees are generated using the Stanford Parser (Rafferty and Manning, 2008). An in-house phrase-based decoder (Vogel, 2003) is used to perform translation. Optimization with 349 Proceedings of the 7th Workshop on Statistical Machine Translati"
W12-3144,2011.iwslt-papers.6,1,0.852447,"., 2011) and also reduces training time. 2.4.4 Quasi-Morphological Operations for OOV words Since German is a highly inflected language, there will be always some word forms of a given Gerwe use the ending of the source and target word to determine which pair of operations should be used. Figure 1: Quasi-morphological operations man lemma that did not occur in the training data. In order to be able to also translate unseen word forms, we try to learn quasi-morphological operations that change the lexical entry of a known word form to the unknown word form. These have shown to be beneficial in Niehues and Waibel (2011) using Wikipedia2 titles. The idea is illustrated in Figure 1. If we look at the data, our system is able to translate a German word Kamin (engl. chimney), but not the dative plural form Kaminen. To address this problem, we try to automatically learn rules how words can be modified. If we look at the example, we would like the system to learn the following rule. If an “en” is appended to a German word, as it is done when creating the dative plural form of Kaminen, we need to add an “s” to the end of the English word in order to perform the same morphological word transformation. We use only ru"
W12-3144,2010.iwslt-evaluation.11,1,0.757697,"cing alternative or additional information into the translation modeling process. 2.4.1 Phrase table adaptation Since the Giga corpus is huge, but noisy, it is advantageous to also use the translation probabilities of the phrase pair extracted only from the more reliable EPPS and News commentary corpus. Therefore, we build two phrase tables for the French↔English system. One trained on all data and the other only trained on the EPPS and News commentary corpus. The two models are then combined using a log-linear combination to achieve the adaptation towards the cleaner corpora as described in (Niehues et al., 2010). The newly created translation model uses the four scores from the general model as well as the two smoothed relative frequencies of both directions from the smaller, but cleaner model. If a phrase pair does not occur in the indomain part, a default score is used instead of a relative frequency. In our case, we used the lowest probability. 2.4.2 Bilingual Language Model In phrase-based systems the source sentence is segmented by the decoder according to the best combination of phrases that maximize the translation and language model scores. This segmentation into phrases leads to the loss of"
W12-3144,W11-2124,1,0.872887,"of context information at the phrase boundaries. Although more target side context is available to the language model, source side context would also be valuable for the decoder when searching for the best translation hypothesis. To make also source language context available we use a bilingual language model, in which each token consists of a target word and all source words it is aligned to. The bilingual tokens enter the translation process as an additional target factor and the bilingual language model is applied to the additional factor like a normal language model. For more details see Niehues et al. (2011). 351 2.4.3 Discriminative Word Lexica Mauser et al. (2009) have shown that the use of discriminative word lexica (DWL) can improve the translation quality. For every target word, they trained a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per one source word. When applying DWL in our experiments, we would like to have the same conditions for the training and test case. For this we would need to change the score of the feature only if a new word is added to the hypothesis. If a word is added the second time, we do not"
W12-3144,E99-1010,0,0.070697,"n the News Shuffle corpus and the German side of all parallel corpora. More details and discussions about the POS language model can be found in Herrmann et al. (2011). 2.5.2 Cluster Language Models The cluster language model follows a similar idea as the POS language model. Since there is a data sparsity problem when we substitute words with the word classes, it is possible to make use of larger context information. In the POS language model, POS tags are the word classes. Here, we generated word classes in a different way. First, we cluster the words in the corpus using the MKCLS algorithm (Och, 1999) given a number of classes. Second, we replace the words in the corpus by their cluster IDs. Finally, we train an n-gram language model on this corpus consisting of cluster IDs. Generally, all cluster language models used in our systems are 5-gram. 3 Results Using the models described above we performed several experiments leading finally to the systems used for generating the translations submitted to the workshop. The following sections describe the experiments for the individual language pairs and show the translation results. The results are reported as case-sensitive BLEU scores (Papineni"
W12-3144,P02-1040,0,0.0865044,"h, 1999) given a number of classes. Second, we replace the words in the corpus by their cluster IDs. Finally, we train an n-gram language model on this corpus consisting of cluster IDs. Generally, all cluster language models used in our systems are 5-gram. 3 Results Using the models described above we performed several experiments leading finally to the systems used for generating the translations submitted to the workshop. The following sections describe the experiments for the individual language pairs and show the translation results. The results are reported as case-sensitive BLEU scores (Papineni et al., 2002) on one reference translation. 3.1 BLEU score of 22.31 on the test data. For the last two systems, we did not perform new optimization runs. System Baseline + Lattice Phrase Extraction + Gigaward Language Model + Bilingual LM + Cluster LM + DWL + Tree-based Reordering + OOV 353 Test 21.32 21.36 21.73 21.91 22.09 22.19 22.26 22.31 Table 1: Translation results for German-English 3.2 English-German The English-German baseline system uses also POS-based reordering, discriminative word alignment and a language model based on EPPS, NC and News Shuffle. A small gain could be achieved by the POS-based"
W12-3144,W08-1006,0,0.0209559,"the French↔English systems the phrase table is based on a GIZA++ word alignment, while the systems for German↔English use a discriminative word alignment as described in Niehues and Vogel (2008). The language models are 4-gram SRI language models using Kneser-Ney smoothing trained by the SRILM Toolkit (Stolcke, 2002). The problem of word reordering is addressed with POS-based and tree-based reordering models as described in Section 2.3. The POS tags used in the reordering model are obtained using the TreeTagger (Schmid, 1994). The syntactic parse trees are generated using the Stanford Parser (Rafferty and Manning, 2008). An in-house phrase-based decoder (Vogel, 2003) is used to perform translation. Optimization with 349 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 349–355, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics regard to the BLEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). During decoding only the top 10 translation options for every source phrase are considered. 2.1 Data Our translation models were trained on the EPPS and News Commentary (NC) corpora. Furthermore, the additional available d"
W12-3144,2007.tmi-papers.21,0,0.0608665,"by a distancebased reordering model and/or a lexicalized distor1 http://hunspell.sourceforge.net/ 350 tion model, we use a different approach that relies on POS sequences. By abstracting from surface words to POS, we expect to model the reordering more accurately. For German-to-English, we additionally apply reordering rules learned from syntactic parse trees. 2.3.1 POS-based Reordering Model In order to build the POS-based reordering model, we first learn probabilistic rules from the POS tags of the training corpus and the alignment. Continuous reordering rules are extracted as described in Rottmann and Vogel (2007) to model short-range reorderings. When translating between German and English, we apply a modified reordering model with non-continuous rules to cover also long-range reorderings (Niehues and Kolss, 2009). 2.3.2 Tree-based Reordering Model Word order is quite different between German and English. And during translation especially verbs or verb particles need to be shifted over a long distance in a sentence. Using discontinuous POS rules already improves the translation tremendously. In addition, we apply a tree-based reordering model for the German-English translation. Syntactic parse trees p"
W12-3144,C08-1098,0,0.0336822,"apply the POS and cluster language models in different systems. All language models are integrated into the translation system by a log-linear combination and received optimal weights during tuning by the MERT. 2.5.1 POS Language Models The POS language model is trained on the POS sequences of the target language. In this evaluation, the POS language model is applied for the English-German system. We expect that having additional information in form of probabilities of POS sequences should help especially in case of the rich morphology of German. The POS tags are generated with the RFTagger (Schmid and Laws, 2008) for German, which produces fine-grained tags that include person, gender and case information. We use a 9-gram language model on the News Shuffle corpus and the German side of all parallel corpora. More details and discussions about the POS language model can be found in Herrmann et al. (2011). 2.5.2 Cluster Language Models The cluster language model follows a similar idea as the POS language model. Since there is a data sparsity problem when we substitute words with the word classes, it is possible to make use of larger context information. In the POS language model, POS tags are the word cl"
W12-3144,W05-0836,1,0.887521,"nd tree-based reordering models as described in Section 2.3. The POS tags used in the reordering model are obtained using the TreeTagger (Schmid, 1994). The syntactic parse trees are generated using the Stanford Parser (Rafferty and Manning, 2008). An in-house phrase-based decoder (Vogel, 2003) is used to perform translation. Optimization with 349 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 349–355, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics regard to the BLEU score is done using Minimum Error Rate Training as described in Venugopal et al. (2005). During decoding only the top 10 translation options for every source phrase are considered. 2.1 Data Our translation models were trained on the EPPS and News Commentary (NC) corpora. Furthermore, the additional available data for French and English (i.e. UN and Giga corpora) were exploited in the corresponding systems. The systems were tuned with the news-test2011 data, while news-test2011 was used for testing in all our systems. We trained language models for each language on the monolingual part of the training corpora as well as the News Shuffle and the Gigaword (version 4) corpora. The d"
W12-3144,W10-1719,1,\N,Missing
W13-0805,P05-1033,0,0.0351559,"rdering method, which affects the scoring of translation hypotheses but does not generate new reorderings, is used e.g. in the open source ma39 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39–47, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics chine translation system Moses (Koehn et al., 2007). Syntax-based (Yamada and Knight, 2001) or syntax-augmented (Zollmann and Venugopal, 2006) MT systems address the reordering problem by embedding syntactic analysis in the decoding process. Hierarchical MT systems (Chiang, 2005) construct a syntactic hierarchy during decoding, which is independent of linguistic categories. To our best knowledge Xia and McCord (2004) were the first to model the word reordering problem as a preprocessing step. They automatically learn reordering rules for English-French translation from source and target language dependency trees. Afterwards, many followed these footsteps. Earlier approaches craft reordering rules manually based on syntactic or dependency parse trees or POS tags designed for particular languages (Collins et al., 2005; Popovi´c and Ney, 2006; Habash, 2007; Wang et al.,"
W13-0805,P05-1066,0,0.121228,"Missing"
W13-0805,W08-0307,0,0.138729,"OS tags designed for particular languages (Collins et al., 2005; Popovi´c and Ney, 2006; Habash, 2007; Wang et al., 2007). Later there were more and more approaches using data-driven methods. Costa-juss`a and Fonollosa (2006) frame the word reordering problem as a translation task and use word class information to translate the original source sentence into a reordered source sentence that can be translated more easily. A very popular approach is to automatically learn reordering rules based on POS tags or syntactic chunks (Popovi´c and Ney, 2006; Rottmann and Vogel, 2007; Zhang et al., 2007; Crego and Habash, 2008). Khalilov et al. (2009) present reordering rules learned from source and target side syntax trees. More recently, Genzel (2010) proposed to automatically learn reordering rules from IBM1 alignments and source side dependency trees. In DeNero and Uszkoreit (2011) no parser is needed, but the sentence structure used for learning the reordering model is induced automatically from a parallel corpus. Among these approaches most are able to cover short-range reorderings and some store reordering variants in a word lattice leaving the selection of the path to the decoder. Long-range reorderings are"
W13-0805,D11-1018,0,0.0126381,"as a translation task and use word class information to translate the original source sentence into a reordered source sentence that can be translated more easily. A very popular approach is to automatically learn reordering rules based on POS tags or syntactic chunks (Popovi´c and Ney, 2006; Rottmann and Vogel, 2007; Zhang et al., 2007; Crego and Habash, 2008). Khalilov et al. (2009) present reordering rules learned from source and target side syntax trees. More recently, Genzel (2010) proposed to automatically learn reordering rules from IBM1 alignments and source side dependency trees. In DeNero and Uszkoreit (2011) no parser is needed, but the sentence structure used for learning the reordering model is induced automatically from a parallel corpus. Among these approaches most are able to cover short-range reorderings and some store reordering variants in a word lattice leaving the selection of the path to the decoder. Long-range reorderings are addressed by manual rules (Collins et al., 2005) or using automatically learned rules (Niehues and Kolss, 2009). Motivated by the POS-based reordering models in Niehues and Kolss (2009) and Rottmann and Vogel (2007), we present a reordering model based on the syn"
W13-0805,C10-1043,0,0.0120148,"e more and more approaches using data-driven methods. Costa-juss`a and Fonollosa (2006) frame the word reordering problem as a translation task and use word class information to translate the original source sentence into a reordered source sentence that can be translated more easily. A very popular approach is to automatically learn reordering rules based on POS tags or syntactic chunks (Popovi´c and Ney, 2006; Rottmann and Vogel, 2007; Zhang et al., 2007; Crego and Habash, 2008). Khalilov et al. (2009) present reordering rules learned from source and target side syntax trees. More recently, Genzel (2010) proposed to automatically learn reordering rules from IBM1 alignments and source side dependency trees. In DeNero and Uszkoreit (2011) no parser is needed, but the sentence structure used for learning the reordering model is induced automatically from a parallel corpus. Among these approaches most are able to cover short-range reorderings and some store reordering variants in a word lattice leaving the selection of the path to the decoder. Long-range reorderings are addressed by manual rules (Collins et al., 2005) or using automatically learned rules (Niehues and Kolss, 2009). Motivated by th"
W13-0805,2007.mtsummit-papers.29,0,0.0711089,"l MT systems (Chiang, 2005) construct a syntactic hierarchy during decoding, which is independent of linguistic categories. To our best knowledge Xia and McCord (2004) were the first to model the word reordering problem as a preprocessing step. They automatically learn reordering rules for English-French translation from source and target language dependency trees. Afterwards, many followed these footsteps. Earlier approaches craft reordering rules manually based on syntactic or dependency parse trees or POS tags designed for particular languages (Collins et al., 2005; Popovi´c and Ney, 2006; Habash, 2007; Wang et al., 2007). Later there were more and more approaches using data-driven methods. Costa-juss`a and Fonollosa (2006) frame the word reordering problem as a translation task and use word class information to translate the original source sentence into a reordered source sentence that can be translated more easily. A very popular approach is to automatically learn reordering rules based on POS tags or syntactic chunks (Popovi´c and Ney, 2006; Rottmann and Vogel, 2007; Zhang et al., 2007; Crego and Habash, 2008). Khalilov et al. (2009) present reordering rules learned from source and targ"
W13-0805,2010.amta-papers.11,0,0.0130572,"vements could be achieved by combining POS and tree-based reordering rules and applying a lexicalized reordering model in addition. Table 2 shows the results. Up to 0.7 BLEU points could be gained by adding tree rules and another 0.1 by lexicalized reordering. System Rule Type POS POS + Tree POS + Tree rec. POS + Tree rec.+ par. 45 noLexRM Dev Test 41.29 38.07 41.94 38.47 42.35 38.66 42.48 38.79 LexRM Dev Test 42.04 38.55 42.44 38.57 42.80 38.71 42.87 38.88 Table 2: German-French 6.5 Binarized Syntactic Trees Even though related work using syntactic parse trees in SMT for reordering purposes (Jiang et al., 2010) have reported an advantage of binarized parse trees over standard parse trees, our model did not benefit from binarized parse trees. It seems that the flat hierarchical structure of standard parse trees enables our reordering model to learn the order of the constituents most efficiently. 7 Example 3 shows another aspect of how the treebased rules work. With the help of the tree-based reordering rules, it is possible to relocate the separated prefix of German verbs and find the correct translation. The verb vorschlagen consist of the main verb (MV) schlagen (here conjugated as schl¨agt) and th"
W13-0805,2009.eamt-1.27,0,0.0657715,"icular languages (Collins et al., 2005; Popovi´c and Ney, 2006; Habash, 2007; Wang et al., 2007). Later there were more and more approaches using data-driven methods. Costa-juss`a and Fonollosa (2006) frame the word reordering problem as a translation task and use word class information to translate the original source sentence into a reordered source sentence that can be translated more easily. A very popular approach is to automatically learn reordering rules based on POS tags or syntactic chunks (Popovi´c and Ney, 2006; Rottmann and Vogel, 2007; Zhang et al., 2007; Crego and Habash, 2008). Khalilov et al. (2009) present reordering rules learned from source and target side syntax trees. More recently, Genzel (2010) proposed to automatically learn reordering rules from IBM1 alignments and source side dependency trees. In DeNero and Uszkoreit (2011) no parser is needed, but the sentence structure used for learning the reordering model is induced automatically from a parallel corpus. Among these approaches most are able to cover short-range reorderings and some store reordering variants in a word lattice leaving the selection of the path to the decoder. Long-range reorderings are addressed by manual rule"
W13-0805,2005.iwslt-1.8,0,0.203704,"on hypotheses are generated. An additional reordering model might be included in the log-linear model of translation. However, these methods can cover reorderings only over a very limited distance. Recently, reordering as preprocessing has drawn much attention. The idea is to detach the reordering problem from the decoding process and 2 Related Work The problem of word reordering has been addressed by several approaches over the last years. In a phrase-based SMT system reordering can be achieved during decoding by allowing swaps of words within a defined window. Lexicalized reordering models (Koehn et al., 2005; Tillmann, 2004) include information about the orientation of adjacent phrases that is learned during phrase extraction. This reordering method, which affects the scoring of translation hypotheses but does not generate new reorderings, is used e.g. in the open source ma39 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39–47, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics chine translation system Moses (Koehn et al., 2007). Syntax-based (Yamada and Knight, 2001) or syntax-augmented (Zollmann and Venugopal,"
W13-0805,P07-2045,0,0.00413724,"ing by allowing swaps of words within a defined window. Lexicalized reordering models (Koehn et al., 2005; Tillmann, 2004) include information about the orientation of adjacent phrases that is learned during phrase extraction. This reordering method, which affects the scoring of translation hypotheses but does not generate new reorderings, is used e.g. in the open source ma39 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39–47, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics chine translation system Moses (Koehn et al., 2007). Syntax-based (Yamada and Knight, 2001) or syntax-augmented (Zollmann and Venugopal, 2006) MT systems address the reordering problem by embedding syntactic analysis in the decoding process. Hierarchical MT systems (Chiang, 2005) construct a syntactic hierarchy during decoding, which is independent of linguistic categories. To our best knowledge Xia and McCord (2004) were the first to model the word reordering problem as a preprocessing step. They automatically learn reordering rules for English-French translation from source and target language dependency trees. Afterwards, many followed thes"
W13-0805,W09-0435,1,0.750859,"yntax trees. More recently, Genzel (2010) proposed to automatically learn reordering rules from IBM1 alignments and source side dependency trees. In DeNero and Uszkoreit (2011) no parser is needed, but the sentence structure used for learning the reordering model is induced automatically from a parallel corpus. Among these approaches most are able to cover short-range reorderings and some store reordering variants in a word lattice leaving the selection of the path to the decoder. Long-range reorderings are addressed by manual rules (Collins et al., 2005) or using automatically learned rules (Niehues and Kolss, 2009). Motivated by the POS-based reordering models in Niehues and Kolss (2009) and Rottmann and Vogel (2007), we present a reordering model based on the syntactic structure of the source sentence. We intend to cover both short-range and long-range re40 ordering more reliably by abstracting to constituents extracted from syntactic parse trees instead of working only with morphosyntactic information on the word level. Furthermore, we combine POS-based and tree-based models and additionally include a lexicalized reordering model. Altogether we apply word reordering on three different levels: lexicali"
W13-0805,P02-1040,0,0.106244,"rientations at the incoming and outgoing phrase boundaries: monotone, swap and discontinuous. In order to apply the lexicalized reordering model on lattices the original position of each word is stored in the lattice. While the translation hypothesis is generated, the reordering orientation with respect to the original position of the words is checked at each phrase boundary. The probability for the respective orientation is included as an additional score. 6 Results The tree-based models are applied for GermanEnglish and German-French translation. Results are measured in case-sensitive BLEU (Papineni et al., 2002). 6.1 General System Description First we describe the general system architecture which underlies all the systems used later on. We use a phrase-based decoder (Vogel, 2003) that takes word lattices as input. Optimization is performed using MERT with respect to BLEU. All POS-based or tree-based systems apply monotone translation only. Baseline systems without reordering rules use a distance-based reordering model. In addition, a lexicalized reordering model as described in (Koehn et al., 2005) is applied where indicated. POS tags and parse trees are generated using the Tree Tagger (Schmid, 199"
W13-0805,popovic-ney-2006-pos,0,0.335373,"Missing"
W13-0805,W08-1006,0,0.264869,"tion First we describe the general system architecture which underlies all the systems used later on. We use a phrase-based decoder (Vogel, 2003) that takes word lattices as input. Optimization is performed using MERT with respect to BLEU. All POS-based or tree-based systems apply monotone translation only. Baseline systems without reordering rules use a distance-based reordering model. In addition, a lexicalized reordering model as described in (Koehn et al., 2005) is applied where indicated. POS tags and parse trees are generated using the Tree Tagger (Schmid, 1994) and the Stanford Parser (Rafferty and Manning, 2008). 6.1.1 Data The German-English system is trained on the provided data of the WMT 2012. news-test2010 and news-test2011 are used for development and testing. The type of data used for training, development and testing the German-French system is similar to WMT data, except that 2 references are available. The training corpus for the reordering models consist of the word-aligned Europarl and News Commentary corpora where POS tags and parse trees are 44 generated for the source side. 6.2 German-English We built systems using POS-based and tree-based reordering and show the impact of the individu"
W13-0805,2007.tmi-papers.21,0,0.419866,"ed on syntactic or dependency parse trees or POS tags designed for particular languages (Collins et al., 2005; Popovi´c and Ney, 2006; Habash, 2007; Wang et al., 2007). Later there were more and more approaches using data-driven methods. Costa-juss`a and Fonollosa (2006) frame the word reordering problem as a translation task and use word class information to translate the original source sentence into a reordered source sentence that can be translated more easily. A very popular approach is to automatically learn reordering rules based on POS tags or syntactic chunks (Popovi´c and Ney, 2006; Rottmann and Vogel, 2007; Zhang et al., 2007; Crego and Habash, 2008). Khalilov et al. (2009) present reordering rules learned from source and target side syntax trees. More recently, Genzel (2010) proposed to automatically learn reordering rules from IBM1 alignments and source side dependency trees. In DeNero and Uszkoreit (2011) no parser is needed, but the sentence structure used for learning the reordering model is induced automatically from a parallel corpus. Among these approaches most are able to cover short-range reorderings and some store reordering variants in a word lattice leaving the selection of the pat"
W13-0805,N04-4026,0,0.00934785,"nerated. An additional reordering model might be included in the log-linear model of translation. However, these methods can cover reorderings only over a very limited distance. Recently, reordering as preprocessing has drawn much attention. The idea is to detach the reordering problem from the decoding process and 2 Related Work The problem of word reordering has been addressed by several approaches over the last years. In a phrase-based SMT system reordering can be achieved during decoding by allowing swaps of words within a defined window. Lexicalized reordering models (Koehn et al., 2005; Tillmann, 2004) include information about the orientation of adjacent phrases that is learned during phrase extraction. This reordering method, which affects the scoring of translation hypotheses but does not generate new reorderings, is used e.g. in the open source ma39 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39–47, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics chine translation system Moses (Koehn et al., 2007). Syntax-based (Yamada and Knight, 2001) or syntax-augmented (Zollmann and Venugopal, 2006) MT systems"
W13-0805,D07-1077,0,0.0588143,"Missing"
W13-0805,C04-1073,0,0.048704,"source ma39 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39–47, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics chine translation system Moses (Koehn et al., 2007). Syntax-based (Yamada and Knight, 2001) or syntax-augmented (Zollmann and Venugopal, 2006) MT systems address the reordering problem by embedding syntactic analysis in the decoding process. Hierarchical MT systems (Chiang, 2005) construct a syntactic hierarchy during decoding, which is independent of linguistic categories. To our best knowledge Xia and McCord (2004) were the first to model the word reordering problem as a preprocessing step. They automatically learn reordering rules for English-French translation from source and target language dependency trees. Afterwards, many followed these footsteps. Earlier approaches craft reordering rules manually based on syntactic or dependency parse trees or POS tags designed for particular languages (Collins et al., 2005; Popovi´c and Ney, 2006; Habash, 2007; Wang et al., 2007). Later there were more and more approaches using data-driven methods. Costa-juss`a and Fonollosa (2006) frame the word reordering prob"
W13-0805,P01-1067,0,0.124032,"in a defined window. Lexicalized reordering models (Koehn et al., 2005; Tillmann, 2004) include information about the orientation of adjacent phrases that is learned during phrase extraction. This reordering method, which affects the scoring of translation hypotheses but does not generate new reorderings, is used e.g. in the open source ma39 Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39–47, c Atlanta, Georgia, 13 June 2013. 2013 Association for Computational Linguistics chine translation system Moses (Koehn et al., 2007). Syntax-based (Yamada and Knight, 2001) or syntax-augmented (Zollmann and Venugopal, 2006) MT systems address the reordering problem by embedding syntactic analysis in the decoding process. Hierarchical MT systems (Chiang, 2005) construct a syntactic hierarchy during decoding, which is independent of linguistic categories. To our best knowledge Xia and McCord (2004) were the first to model the word reordering problem as a preprocessing step. They automatically learn reordering rules for English-French translation from source and target language dependency trees. Afterwards, many followed these footsteps. Earlier approaches craft re"
W13-0805,W07-0401,0,\N,Missing
W13-0805,W06-1609,0,\N,Missing
W13-0805,W06-3119,0,\N,Missing
W13-2210,2010.iwslt-evaluation.11,1,0.829632,"Using the models described above we performed several experiments leading finally to the systems used for generating the translations submitted to the workshop. The results are reported as casesensitive BLEU scores on one reference translation. For the French↔English systems, we built two phrase tables; one trained with all data and the other trained only with the EPPS and NC corpora. This is due to the fact that Giga corpus is big but noisy and EPPS and NC corpus are more reliable. The two models are combined log-linearly to achieve the adaptation towards the cleaner corpora as described in Niehues et al. (2010). 6 Cluster Language Models POS Language Models For the English→German system, we use the POS language model, which is trained on the POS sequence of the target language. The POS tags are generated using the RFTagger (Schmid and Laws, 2008) for German. The RFTagger generates finegrained tags which include person, gender, and case information. The language model is trained with up to 9-gram information, using the German side of the parallel EPPS and NC corpus, as well as the News Shuffle corpus. 7.2 English→German The English to German baseline system uses POSbased reordering and language model"
W13-2210,W11-2124,1,0.831087,"mann and Vogel (2007), continuous reordering rules are extracted. This modeling of short-range reorderings was extended so that it can cover also long-range reorderings with noncontinuous rules (Niehues and Kolss, 2009), for German↔English systems. 4.2 Lexicalized Reordering 5.1 Bilingual Language Model During the decoding the source sentence is segmented so that the best combination of phrases which maximizes the scores is available. However, this causes some loss of context information at the phrase boundaries. In order to make bilingual context available, we use a bilingual language model (Niehues et al., 2011). In the bilingual language model, each token consists of a target word and all source words it is aligned to. Tree-based Reordering Model 5.2 In addition to the POS-based reordering, we apply a tree-based reordering model for the German↔English translation to better address the differences in word order between German and English. We use the Stanford Parser (Rafferty and Manning, 2008) to generate syntactic parse trees for the source side of the training corpus. Then we use the word alignment between source and target language to learn rules on how to reorder the constituents in a German sour"
W13-2210,E99-1010,0,0.0718706,"tering to the features for higher order n-grams. Furthermore, we created the training examples differently in order to focus on addressing errors of the other models of the phrase-based translation 105 6.2 system. We first translated the whole corpus with a baseline system. Then we only used the words that occur in the N-Best List and not in the reference as negative examples instead of using all words that do not occur in the reference. 5.3 In order to use larger context information, we use a cluster language model for all our systems. The cluster language model is based on the idea shown in Och (1999). Using the MKCLS algorithm, we cluster the words in the corpus, given a number of classes. Then words in the corpus are replaced with their cluster IDs. Using these cluster IDs, we train n-gram language models as well as a phrase table with this additional factor of cluster ID. Our submitted systems have diversed range of the number of clusters as well as n-gram. Quasi-Morphological Operations Because of the inflected characteristic of the German language, we try to learn quasimorphological operations that change the lexical entry of a known word form to the out-ofvocabulary (OOV) word form a"
W13-2210,W13-0805,1,0.838059,"all source words it is aligned to. Tree-based Reordering Model 5.2 In addition to the POS-based reordering, we apply a tree-based reordering model for the German↔English translation to better address the differences in word order between German and English. We use the Stanford Parser (Rafferty and Manning, 2008) to generate syntactic parse trees for the source side of the training corpus. Then we use the word alignment between source and target language to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The POS-based and tree-based reordering rules are applied to each input sentence. The resulting reordered sentence variants as well as the original sentence order are encoded in a word lattice. The lattice is then used as input to the decoder. Discriminative Word Lexicon Mauser et al. (2009) introduced the Discriminative Word Lexicon (DWL) into phrase-based machine translation. In this approach, a maximum entropy model is used to determine the probability of using a target word in the translation. In this evaluation, we used two extensions to this work as shown in (Niehues and Waibel, 2013)."
W13-2210,P02-1040,0,0.0915781,"he Karlsruhe Institute of Technology Translation Systems for the WMT 2013 Eunah Cho, Thanh-Le Ha, Mohammed Mediani, Jan Niehues, Teresa Herrmann, Isabel Slawik and Alex Waibel Karlsruhe Institute of Technology Karlsruhe, Germany firstname.lastname@kit.edu Abstract Section 4. In addition to it, tree-based reordering model and lexicalized reordering were added for German↔English systems. An in-house phrase-based decoder (Vogel, 2003) is used to perform translation. The translation was optimized using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) towards better BLEU (Papineni et al., 2002) scores. This paper describes the phrase-based SMT systems developed for our participation in the WMT13 Shared Translation Task. Translations for English↔German and English↔French were generated using a phrase-based translation system which is extended by additional models such as bilingual, fine-grained part-ofspeech (POS) and automatic cluster language models and discriminative word lexica (DWL). In addition, we combined reordering models on different sentence abstraction levels. 1 2.1 The Europarl corpus (EPPS) and News Commentary (NC) corpus were used for training our translation models. W"
W13-2210,E03-1076,0,0.0487175,"der with lattice input. The paper is organized as follows: the next section gives a detailed description of our systems including all the models. The translation results for all directions are presented afterwards and we close with a conclusion. 2 Data 2.2 Preprocessing The training data is preprocessed prior to training the system. This includes normalizing special symbols, smart-casing the first word of each sentence and removing long sentences and sentence pairs with length mismatch. Compound splitting is applied to the German part of the corpus of the German→English system as described in Koehn and Knight (2003). System Description The phrase table is based on a GIZA++ word alignment for the French↔English systems. For the German↔English systems we use a Discriminative Word Alignment (DWA) as described in Niehues and Vogel (2008). For every source phrase only the top 10 translation options are considered during decoding. The SRILM Toolkit (Stolcke, 2002) is used for training SRI language models using Kneser-Ney smoothing. For the word reordering between languages, we used POS-based reordering models as described in 3 Filtering of Noisy Pairs The filtering was applied on the corpora which are found to"
W13-2210,W08-1006,0,0.0143303,"ion of phrases which maximizes the scores is available. However, this causes some loss of context information at the phrase boundaries. In order to make bilingual context available, we use a bilingual language model (Niehues et al., 2011). In the bilingual language model, each token consists of a target word and all source words it is aligned to. Tree-based Reordering Model 5.2 In addition to the POS-based reordering, we apply a tree-based reordering model for the German↔English translation to better address the differences in word order between German and English. We use the Stanford Parser (Rafferty and Manning, 2008) to generate syntactic parse trees for the source side of the training corpus. Then we use the word alignment between source and target language to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The POS-based and tree-based reordering rules are applied to each input sentence. The resulting reordered sentence variants as well as the original sentence order are encoded in a word lattice. The lattice is then used as input to the decoder. Discriminative Word Lexicon Mauser et al. (20"
W13-2210,D09-1022,0,0.0267009,"Missing"
W13-2210,2007.tmi-papers.21,0,0.165327,"an↔English system, reordering rules learned from syntactic parse trees were used in addition. 4.1 Translation Models In addition to the models used in the baseline system described above, we conducted experiments including additional models that enhance translation quality by introducing alternative or additional information into the translation modeling process. POS-based Reordering Model In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment. As described in Rottmann and Vogel (2007), continuous reordering rules are extracted. This modeling of short-range reorderings was extended so that it can cover also long-range reorderings with noncontinuous rules (Niehues and Kolss, 2009), for German↔English systems. 4.2 Lexicalized Reordering 5.1 Bilingual Language Model During the decoding the source sentence is segmented so that the best combination of phrases which maximizes the scores is available. However, this causes some loss of context information at the phrase boundaries. In order to make bilingual context available, we use a bilingual language model (Niehues et al., 2011)"
W13-2210,C08-1098,0,0.107428,"ord Reordering 5 Word reordering was modeled based on POS sequences. For the German↔English system, reordering rules learned from syntactic parse trees were used in addition. 4.1 Translation Models In addition to the models used in the baseline system described above, we conducted experiments including additional models that enhance translation quality by introducing alternative or additional information into the translation modeling process. POS-based Reordering Model In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment. As described in Rottmann and Vogel (2007), continuous reordering rules are extracted. This modeling of short-range reorderings was extended so that it can cover also long-range reorderings with noncontinuous rules (Niehues and Kolss, 2009), for German↔English systems. 4.2 Lexicalized Reordering 5.1 Bilingual Language Model During the decoding the source sentence is segmented so that the best combination of phrases which maximizes the scores is available. However, this causes some loss of context information at the phrase boundaries. In order to make b"
W13-2210,2011.iwslt-evaluation.9,1,0.878095,"is used for training SRI language models using Kneser-Ney smoothing. For the word reordering between languages, we used POS-based reordering models as described in 3 Filtering of Noisy Pairs The filtering was applied on the corpora which are found to be noisy. Namely, the Giga EnglishFrench parallel corpus and the all the new webcrawled data . The operation was performed using 104 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 104–108, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics 4.3 an SVM classifier as in our past systems (Mediani et al., 2011). For each pair, the required lexica were extracted from Giza alignment of the corresponding EPPS and NC corpora. Furthermore, for the web-crawled data, higher precision classifiers were trained by providing a larger number of negative examples to the classifier. After filtering, we could still find English sentences in the other part of the corpus. Therefore, we performed a language identification (LID)based filtering afterwards (performed only on the French-English corpora, in this participation). 4 The lexicalized reordering model stores the reordering probabilities for each phrase pair. Po"
W13-2210,W09-0435,1,0.865694,"d experiments including additional models that enhance translation quality by introducing alternative or additional information into the translation modeling process. POS-based Reordering Model In order to train the POS-based reordering model, probabilistic rules were learned based on the POS tags from the TreeTagger (Schmid and Laws, 2008) of the training corpus and the alignment. As described in Rottmann and Vogel (2007), continuous reordering rules are extracted. This modeling of short-range reorderings was extended so that it can cover also long-range reorderings with noncontinuous rules (Niehues and Kolss, 2009), for German↔English systems. 4.2 Lexicalized Reordering 5.1 Bilingual Language Model During the decoding the source sentence is segmented so that the best combination of phrases which maximizes the scores is available. However, this causes some loss of context information at the phrase boundaries. In order to make bilingual context available, we use a bilingual language model (Niehues et al., 2011). In the bilingual language model, each token consists of a target word and all source words it is aligned to. Tree-based Reordering Model 5.2 In addition to the POS-based reordering, we apply a tre"
W13-2210,W05-0836,1,0.872186,"Missing"
W13-2210,W08-0303,1,0.80366,"close with a conclusion. 2 Data 2.2 Preprocessing The training data is preprocessed prior to training the system. This includes normalizing special symbols, smart-casing the first word of each sentence and removing long sentences and sentence pairs with length mismatch. Compound splitting is applied to the German part of the corpus of the German→English system as described in Koehn and Knight (2003). System Description The phrase table is based on a GIZA++ word alignment for the French↔English systems. For the German↔English systems we use a Discriminative Word Alignment (DWA) as described in Niehues and Vogel (2008). For every source phrase only the top 10 translation options are considered during decoding. The SRILM Toolkit (Stolcke, 2002) is used for training SRI language models using Kneser-Ney smoothing. For the word reordering between languages, we used POS-based reordering models as described in 3 Filtering of Noisy Pairs The filtering was applied on the corpora which are found to be noisy. Namely, the Giga EnglishFrench parallel corpus and the all the new webcrawled data . The operation was performed using 104 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 104–108, c"
W13-2210,2012.amta-papers.19,1,0.749834,"LS algorithm, we cluster the words in the corpus, given a number of classes. Then words in the corpus are replaced with their cluster IDs. Using these cluster IDs, we train n-gram language models as well as a phrase table with this additional factor of cluster ID. Our submitted systems have diversed range of the number of clusters as well as n-gram. Quasi-Morphological Operations Because of the inflected characteristic of the German language, we try to learn quasimorphological operations that change the lexical entry of a known word form to the out-ofvocabulary (OOV) word form as described in Niehues and Waibel (2012). 5.4 7 Phrase Table Adaptation 7.1 German→English The experiments for the German to English translation system are summarized in Table 1. The baseline system uses POS-based reordering, DWA with lattice phrase extraction and language models trained on the News Shuffle corpus and Giga corpus separately. Then we added a 5-gram cluster LM trained with 1,000 word classes. By adding a language model using the filtered crawled data we gained 0.3 BLEU on the test set. For this we combined all language models linearly. The filtered crawled data was also used to generate a phrase table, which brought a"
W13-2210,W13-2264,1,0.697889,"er (Herrmann et al., 2013). The POS-based and tree-based reordering rules are applied to each input sentence. The resulting reordered sentence variants as well as the original sentence order are encoded in a word lattice. The lattice is then used as input to the decoder. Discriminative Word Lexicon Mauser et al. (2009) introduced the Discriminative Word Lexicon (DWL) into phrase-based machine translation. In this approach, a maximum entropy model is used to determine the probability of using a target word in the translation. In this evaluation, we used two extensions to this work as shown in (Niehues and Waibel, 2013). First, we added additional features to model the order of the source words better. Instead of representing the source sentence as a bag-of-words, we used a bag-of-n-grams. We used n-grams up to the order of three and applied count filtering to the features for higher order n-grams. Furthermore, we created the training examples differently in order to focus on addressing errors of the other models of the phrase-based translation 105 6.2 system. We first translated the whole corpus with a baseline system. Then we only used the words that occur in the N-Best List and not in the reference as neg"
W13-2223,W13-0805,1,0.848446,"OS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse trees of the whole training corpus and the word alignment between source and target language are used to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. Moreover, our reordering model was extended so that it could include the features of lexicalized reordering model. The reordering probabilities for each phrase pair are stored as well as the original position of each word in the lattice. During the decoding, the reordering origin of the words is checked along with its"
W13-2223,E03-1076,0,0.0855399,"n the parallel data as well as the provided News crawl, the 109 French-English, UN and LDC Gigaword Fourth Edition corpora. 2.2 System Overview Karlsruhe Institute of Technology Single System 2.2.1 Preprocessing The training data was preprocessed prior to the training. Symbols such as quotes, dashes and apostrophes are normalized. Then the first words of each sentence are smart-cased. For the German part of the training corpus, the hunspell2 lexicon was used, in order to learn a mapping from old German spelling to new German writing rules. Compound-splitting was also performed as described in Koehn and Knight (2003). We also removed very long sentences, empty lines, and sentences which show big mismatch on the length. 2.2.2 Filtering The web-crawled corpus was filtered using an SVM classifier as described in (Mediani et al., 2011). The lexica used in this filtering task were obtained from Giza alignments trained on the 2.2.5 Translation Models The translation model uses the parallel data of EPPS, NC, and the filtered web-crawled data. As word alignment, we used the Discriminative Word Alignment (DWA) as shown in (Niehues and Vo2 http://hunspell.sourceforge.net/ 186 gel, 2008). The phrase pairs were extra"
W13-2223,P07-2045,0,0.00527113,"ne Translation. The technique of Statistical Post-Editing (Dugast et al., 2007) is used to automatically edit the output of the rule-based system. A Statistical Post-Editing (SPE) module is generated from a bilingual corpus. It is basically a translation module by itself, however it is trained on rule-based • Phrase pairs appearing less than 2 times were pruned. The SPE language model was trained on 2M phrases from the news/europarl and CommonCrawl corpora, provided as training data for WMT 2013. Weights for these separate models were tuned by the Mert algorithm provided in the Moses toolkit (Koehn et al., 2007), using the provided news development set. 5 http://geek.kyloo.net/software 6 The fifth edition of (LDC2011T07) was not used. the English Gigaword 188 0 5:that/1 7:this/3 1 3:is/3 8:was/1 2 0:*EPS*/3 4:it/1 0:*EPS*/3 2:in/1 3 4 0:*EPS*/3 6:the/1 5 0:*EPS*/1 1:future/3 6 Figure 1: Confusion network of four different hypotheses. 3 RWTH Aachen System Combination Table 1: Comparison of single systems tuned on newstest2009 and newstest2010. The results are reported on newstest2012. System combination is used to produce consensus translations from multiple hypotheses generated with different transla"
W13-2223,J04-2004,0,0.0419685,"d a bilingual language model (Niehues et al., 2011). A Discriminative Word Lexicon (DWL) introduced in (Mauser et al., 2009) was extended so that it could take the source context also into the account. For this, we used a bag-of-ngrams instead of representing the source sentence as a bag-of-words. Filtering based on counts was then applied to the features for higher order n-grams. In addition to this, the training examples were created differently so that we only used the words that occur in the n-best list but not in the reference as negative example. a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information4 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus mode"
W13-2223,W07-0734,0,0.0383886,"results are reported on newstest2012. System combination is used to produce consensus translations from multiple hypotheses generated with different translation engines. First, a word to word alignment for the given single system hypotheses is produced. In a second step a confusion network is constructed. Then, the hypothesis with the highest probability is extracted from this confusion network. For the alignment procedure, each of the given single systems generates one confusion network with its own as primary system. To this primary system all other hypotheses are aligned using the METEOR (Lavie and Agarwal, 2007) alignment and thus the primary system defines the word order. Once the alignment is given, the corresponding confusion network is constructed. An example is given in Figure 1. The final network for one source sentence is the union of all confusion networks generated from the different primary systems. That allows the system combination to select the word order from different system outputs. Before performing system combination, each translation output was normalized by tokenization and lowercasing. The output of the combination was then truecased based on the original truecased output. The mo"
W13-2223,W07-0732,0,0.0965924,"m = 10, and used k = 300. 2.3.4 translations and reference data. It applies corrections and adaptations learned from a phrase-based 5-gram language model. Using this two-step process will implicitly keep long distance relations and other constraints determined by the rule-based system while significantly improving phrasal fluency. It has the advantage that quality improvements can be achieved with very little but targeted bilingual data, thus significantly reducing training time and increasing translation performance. The basic setup of the SPE component is identical to the one described in (Dugast et al., 2007). A statistical translation model is trained on the rule-based translation of the source and the target side of the parallel corpus. Language models are trained on each target half of the parallel corpora and also on additional in-domain corpora. Moreover, the following measures - limiting unwanted statistical effects - were applied: Corpora and data pre-processing All the parallel data allowed in the constrained task are pooled together to create a single parallel corpus. This corpus is word-aligned using MGIZA++5 with default settings. For the English monolingual training data, we used the s"
W13-2223,N12-1005,0,0.0335346,"with standard n-gram translation models is that the elementary units are bilingual pairs, which means that the underlying vocabulary can be quite large, even for small translation tasks. Unfortunately, the parallel data available to train these models are typically order of magnitudes smaller than the corresponding monolingual corpora used to train target language models. It is very likely then, that such models should face severe estimation problems. In such setting, using neural network language model techniques seem all the more appropriate. For this study, we follow the recommendations of Le et al. (2012), who propose to factor the joint probability of a sentence pair by decomposing tuples in two (source and target) parts, and further each part in words. This yields a word factored translation model that LIMSI-CNRS Single System 2.3.1 System overview LIMSI’s system is built with n-code (Crego et al., 2011), an open source statistical machine translation system based on bilingual n-gram3 . In this approach, the translation model relies on a specific decomposition of the joint probability of a sentence pair using the n-gram assumption: a sentence pair is decomposed into a sequence of bilingual u"
W13-2223,2010.iwslt-papers.6,0,0.0342874,"Missing"
W13-2223,2012.iwslt-papers.7,1,0.838536,"an compound words with the frequencybased method described in (Koehn and Knight, 2003). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.2.3 The in-house phrase-based decoder (Vogel, 2003) is used to perform decoding. Optimization with regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse tree"
W13-2223,W08-0310,0,0.0216057,"nal in-domain corpora. Moreover, the following measures - limiting unwanted statistical effects - were applied: Corpora and data pre-processing All the parallel data allowed in the constrained task are pooled together to create a single parallel corpus. This corpus is word-aligned using MGIZA++5 with default settings. For the English monolingual training data, we used the same setup as last year6 and thus the same target language model as detailed in (Allauzen et al., 2011). For English, we also took advantage of our inhouse text processing tools for the tokenization and detokenization steps (Dchelotte et al., 2008) and our system is built in “true-case”. As German is morphologically more complex than English, the default policy which consists in treating each word form independently is plagued with data sparsity, which is detrimental both at training and decoding time. Thus, the German side was normalized using a specific pre-processing scheme (described in (Allauzen et al., 2010; Durgar ElKahlout and Yvon, 2010)), which notably aims at reducing the lexical redundancy by (i) normalizing the orthography, (ii) neutralizing most inflections and (iii) splitting complex compounds. 2.4 • Named entities are re"
W13-2223,2011.iwslt-papers.5,1,0.849659,"processed by splitting German compound words with the frequencybased method described in (Koehn and Knight, 2003). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.2.3 The in-house phrase-based decoder (Vogel, 2003) is used to perform decoding. Optimization with regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering"
W13-2223,J06-4004,0,0.0399278,"Missing"
W13-2223,D09-1022,1,0.905747,"Missing"
W13-2223,D08-1089,0,0.0206813,"Wuebker et al., 2012) which is part of RWTH’s open-source SMT toolkit Jane 2.1 1 . GIZA++ (Och and Ney, 2003) was employed to train a word alignment, language models have been created with the SRILM toolkit (Stolcke, 2002). After phrase pair extraction from the wordaligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phrase-level IBM-1 and word-, phrase- and distortion-penalties, which are combined in log-linear fashion. Furthermore, we used an additional reordering model as described in (Galley and Manning, 2008). By this model six 1 http://www-i6.informatik.rwth-aachen. de/jane/ 185 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 185–192, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics additional feature are added to the log-linear combination. The model weights are optimized with standard Mert (Och, 2003a) on 200-best lists. The optimization criterion is B LEU. cleaner corpora, EPPS and NC. Assuming that this corpus is very noisy, we biased our classifier more towards precision than recall. This was realized by giving higher number of f"
W13-2223,2011.iwslt-evaluation.9,1,0.888245,"ing data was preprocessed prior to the training. Symbols such as quotes, dashes and apostrophes are normalized. Then the first words of each sentence are smart-cased. For the German part of the training corpus, the hunspell2 lexicon was used, in order to learn a mapping from old German spelling to new German writing rules. Compound-splitting was also performed as described in Koehn and Knight (2003). We also removed very long sentences, empty lines, and sentences which show big mismatch on the length. 2.2.2 Filtering The web-crawled corpus was filtered using an SVM classifier as described in (Mediani et al., 2011). The lexica used in this filtering task were obtained from Giza alignments trained on the 2.2.5 Translation Models The translation model uses the parallel data of EPPS, NC, and the filtered web-crawled data. As word alignment, we used the Discriminative Word Alignment (DWA) as shown in (Niehues and Vo2 http://hunspell.sourceforge.net/ 186 gel, 2008). The phrase pairs were extracted using different source word order suggested by the POSbased reordering models presented previously as described in (Niehues et al., 2009). In order to extend the context of source language words, we applied a bilin"
W13-2223,W09-0435,1,0.861514,"ith regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse trees of the whole training corpus and the word alignment between source and target language are used to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The reordering"
W13-2223,W08-0303,1,0.903038,"Missing"
W13-2223,N04-4026,0,0.016377,"y so that we only used the words that occur in the n-best list but not in the reference as negative example. a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model requires to reorder source sentences so as to match the target word order. This is performed by a stochastic finite-state reordering model, which uses part-of-speech information4 to generalize reordering patterns beyond lexical regularities. In addition to the translation model, eleven feature functions are combined: a target-language model; four lexicon models; two lexicalized reordering models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003b). The overal"
W13-2223,W09-0413,1,0.842391,"The web-crawled corpus was filtered using an SVM classifier as described in (Mediani et al., 2011). The lexica used in this filtering task were obtained from Giza alignments trained on the 2.2.5 Translation Models The translation model uses the parallel data of EPPS, NC, and the filtered web-crawled data. As word alignment, we used the Discriminative Word Alignment (DWA) as shown in (Niehues and Vo2 http://hunspell.sourceforge.net/ 186 gel, 2008). The phrase pairs were extracted using different source word order suggested by the POSbased reordering models presented previously as described in (Niehues et al., 2009). In order to extend the context of source language words, we applied a bilingual language model (Niehues et al., 2011). A Discriminative Word Lexicon (DWL) introduced in (Mauser et al., 2009) was extended so that it could take the source context also into the account. For this, we used a bag-of-ngrams instead of representing the source sentence as a bag-of-words. Filtering based on counts was then applied to the features for higher order n-grams. In addition to this, the training examples were created differently so that we only used the words that occur in the n-best list but not in the refe"
W13-2223,W05-0836,1,0.864882,"filtering task). 2.1.1 Preprocessing In order to reduce the source vocabulary size translation, the German text was preprocessed by splitting German compound words with the frequencybased method described in (Koehn and Knight, 2003). To further reduce translation complexity for the phrase-based approach, we performed the long-range part-of-speech based reordering rules proposed by (Popovi´c et al., 2006). 2.2.3 The in-house phrase-based decoder (Vogel, 2003) is used to perform decoding. Optimization with regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short"
W13-2223,W11-2124,1,0.875547,"this filtering task were obtained from Giza alignments trained on the 2.2.5 Translation Models The translation model uses the parallel data of EPPS, NC, and the filtered web-crawled data. As word alignment, we used the Discriminative Word Alignment (DWA) as shown in (Niehues and Vo2 http://hunspell.sourceforge.net/ 186 gel, 2008). The phrase pairs were extracted using different source word order suggested by the POSbased reordering models presented previously as described in (Niehues et al., 2009). In order to extend the context of source language words, we applied a bilingual language model (Niehues et al., 2011). A Discriminative Word Lexicon (DWL) introduced in (Mauser et al., 2009) was extended so that it could take the source context also into the account. For this, we used a bag-of-ngrams instead of representing the source sentence as a bag-of-words. Filtering based on counts was then applied to the features for higher order n-grams. In addition to this, the training examples were created differently so that we only used the words that occur in the n-best list but not in the reference as negative example. a n-gram model of (source,target) pairs (Casacuberta and Vidal, 2004). Training this model r"
W13-2223,C12-3061,1,0.817205,"e of the four project partners. Each group develop and maintain their own different machine translation system. These single systems differ not only in their general approach, but also in the preprocessing of training and test data. To take advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. This paper is structured as follows. First, the different engines of all four groups are introduced. RWTH Aachen Single System For the WMT 2013 evaluation, RWTH utilized a phrase-based decoder based on (Wuebker et al., 2012) which is part of RWTH’s open-source SMT toolkit Jane 2.1 1 . GIZA++ (Och and Ney, 2003) was employed to train a word alignment, language models have been created with the SRILM toolkit (Stolcke, 2002). After phrase pair extraction from the wordaligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phrase-level IBM-1 and word-, phrase- and distortion-penalties, which are combined in log-linear fashion. Furthermore, we used an additional reordering model as described in (Galley and Manning, 2"
W13-2223,J03-1002,1,0.00903936,"translation system. These single systems differ not only in their general approach, but also in the preprocessing of training and test data. To take advantage of these differences of each translation system, we combined all hypotheses of the different systems, using the RWTH system combination approach. This paper is structured as follows. First, the different engines of all four groups are introduced. RWTH Aachen Single System For the WMT 2013 evaluation, RWTH utilized a phrase-based decoder based on (Wuebker et al., 2012) which is part of RWTH’s open-source SMT toolkit Jane 2.1 1 . GIZA++ (Och and Ney, 2003) was employed to train a word alignment, language models have been created with the SRILM toolkit (Stolcke, 2002). After phrase pair extraction from the wordaligned parallel corpus, the translation probabilities are estimated by relative frequencies. The standard feature set also includes an n-gram language model, phrase-level IBM-1 and word-, phrase- and distortion-penalties, which are combined in log-linear fashion. Furthermore, we used an additional reordering model as described in (Galley and Manning, 2008). By this model six 1 http://www-i6.informatik.rwth-aachen. de/jane/ 185 Proceedings"
W13-2223,P03-1021,0,0.694457,"models (Tillmann, 2004) aiming at predicting the orientation of the next translation unit; a ’weak’ distance-based distortion model; and finally a word-bonus model and a tuple-bonus model which compensate for the system preference for short translations. The four lexicon models are similar to the ones use in a standard phrase based system: two scores correspond to the relative frequencies of the tuples and two lexical weights estimated from the automatically generated word alignments. The weights associated to feature functions are optimally combined using a discriminative training framework (Och, 2003b). The overall search is based on a beam-search strategy on top of a dynamic programming algorithm. Reordering hypotheses are computed in a preprocessing step, making use of reordering rules built from the word reorderings introduced in the tuple extraction process. The resulting reordering hypotheses are passed to the decoder in the form of word lattices (Crego and Mario, 2006). 2.2.6 Language Models We build separate language models and combined them prior to decoding. As word-token based language models, one language model is built on EPPS, NC, and giga corpus, while another one is built u"
W13-2223,W08-1006,0,0.0614361,"and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse trees of the whole training corpus and the word alignment between source and target language are used to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The training corpus was parsed by the Stanford parser (Rafferty and Manning, 2008). The reordering rules are applied to the source sentences and the reordered sentence variants as well as the original sequence are encoded in a word lattice which is used as input to the decoder. Moreover, our reordering model was extended so that it could include the features of lexicalized reordering model. The reordering probabilities for each phrase pair are stored as well as the original position of each word in the lattice. During the decoding, the reordering origin of the words is checked along with its probability added as an additional score. 2.1.3 Language Model During decoding a 4-"
W13-2223,2007.tmi-papers.21,0,0.168794,") is used to perform decoding. Optimization with regard to the BLEU score is done using Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005). 2.1.2 Translation Model We applied filtering and weighting for domainadaptation similarly to (Mansour et al., 2011) and (Mansour and Ney, 2012). For filtering the bilingual data, a combination of LM and IBM Model 1 scores was used. In addition, we performed weighted phrase extraction by using a combined LM and IBM Model 1 weight. 2.2.4 Reordering Model We applied part-of-speech (POS) based reordering using probabilistic continuous (Rottmann and Vogel, 2007) and discontinuous (Niehues and Kolss, 2009) rules. This was learned using POS tags generated by the TreeTagger (Schmid, 1994) for short and long range reorderings respectively. In addition to this POS-based reordering, we also used tree-based reordering rules. Syntactic parse trees of the whole training corpus and the word alignment between source and target language are used to learn rules on how to reorder the constituents in a German source sentence to make it match the English target sentence word order better (Herrmann et al., 2013). The training corpus was parsed by the Stanford parser"
W13-2223,W12-3140,1,\N,Missing
W13-2223,W11-2135,0,\N,Missing
W13-2223,W10-1704,0,\N,Missing
W13-2264,W09-0435,1,0.732129,". We only considered the examples for the classifier of target word e, where e occurs in the N -Best list entry E 0 . If the word does not occur in any N -Best list entry of a training sentence, but in the reference, we created an additional example (F, E, ””). The features of this examples can then be created straight forward as: I((F, E, E 0 )) = max(I(F ); I(E 0 )) Reordering was performed as a preprocessing step using part-of-speech information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT (Venugopal et al., 2005). We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consists of 1.7k segments containing 16k words. As test set we used 3.5k segments containing 31k words. We will refer to this system as System 1. In order to show the influence of the approaches better, we evaluat"
W13-2264,W08-0303,1,0.765241,"translation quality drops. Especially for System 1, we have a significant drop in the BLEU score of the test set by 0.6 BLEU points. One problem might be that most of the bigrams occur quite rarely and therefore, we have a problem of data sparseness and generalization. If we combine the features of unigram and biSystem Description The translation system was trained on the EPPS corpus, NC corpus, the BTEC corpus and TED talks.2 The data was preprocessed and compound splitting (Koehn and Knight, 2003) was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). 2 German - English TED Experiments http://www.ted.com 516 Table 1: Experiments using different source features System FeatureSize Baseline Unigram Bigram Uni+bigram + Count filter 2 + Count filter 5 + Trigram 0 40k 319k 359k 122k 63k"
W13-2264,P07-1020,0,0.0606666,"ach is used. In this approach, instead of building the translation by translating word by word, sequences of source and target words, so-called phrase pairs, are used as the basic translation unit. A table of correspondences between source and target phrases forms the translation model. Target language fluency is modeled by a language model storing monolingual n-gram occurrences. A log-linear combination of these main models as well as additional features is used to score the different translation hypotheses. Then the decoder searches for the translation with the highest score. 2 Related Work Bangalore et al. (2007) presented an approach to machine translation using discriminative lexical selection. Motivated by their results, Mauser et al. (2009) integrated the DWL into the PBMT ap512 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 512–520, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics proach. Thereby, they are able to use global source information. This was extended by Huck et al. (2010) by a feature selection strategy in order to reduce the number of weights. In Mediani et al. (2011) a first approach to use information about MT errors i"
W13-2264,2012.amta-papers.19,1,0.706309,"potheses and the optimization was performed using MERT (Venugopal et al., 2005). We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consists of 1.7k segments containing 16k words. As test set we used 3.5k segments containing 31k words. We will refer to this system as System 1. In order to show the influence of the approaches better, we evaluated them also in a second system. In addition to the models used in the first system we performed a log-linear language model and phrase table adaptation as described in Niehues and Waibel (2012). To this system we refer as System 2 in the following experiments. (11) If we have seen the word only in the reference, we create an training example without target features. Therefore, we have again a training example which can not happen when using the DWL model. Therefore, we removed these examples in the last method (Restricted TF). 6 Experiments After presenting the different approaches to perform feature and example selection, we will now evaluate them. First, we will give a short overview of the MT system. Then we will give a detailed evaluation on the task of translating German lectur"
W13-2264,D07-1007,0,0.0450491,"kshop on Statistical Machine Translation, pages 512–520, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics proach. Thereby, they are able to use global source information. This was extended by Huck et al. (2010) by a feature selection strategy in order to reduce the number of weights. In Mediani et al. (2011) a first approach to use information about MT errors in the training of DWLs was presented. They select the training examples by using phrase table information also. The DWLs are related to work that was done in the area of word sense disambiguation (WSD). Carpuat and Wu (2007) presented an approach to disambiguate between different phrases instead of performing the disambiguation at word level. A different lexical model that uses target side information was presented in Jeong et al. (2010). The focus of this work was to model complex morphology on the target language. 3 When we have the probability for every word ej given the source sentence F , we need to combine these probabilities into a probability of the whole target sentence E = e1 . . . eJ given F . Making an assumption of independence on the target side as well, the models can be combined to the probability"
W13-2264,W11-2124,1,0.819344,"d on the EPPS corpus, NC corpus, the BTEC corpus and TED talks.2 The data was preprocessed and compound splitting (Koehn and Knight, 2003) was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). 2 German - English TED Experiments http://www.ted.com 516 Table 1: Experiments using different source features System FeatureSize Baseline Unigram Bigram Uni+bigram + Count filter 2 + Count filter 5 + Trigram 0 40k 319k 359k 122k 63k 77k System 1 Dev Test 26.32 24.24 27.46 25.56 27.34 24.92 27.69 25.55 27.75 25.71 27.81 25.67 27.76 25.76 grams with count filtering in all experiments. In the first experiment, we used the original approach to create the training examples. In this case, all sentences where the word does not occur in the reference generate negative examples. In our setup, we nee"
W13-2264,2007.tmi-papers.21,0,0.387257,"ry training sentence N -Best list translation (F, E, E 0 ). We only considered the examples for the classifier of target word e, where e occurs in the N -Best list entry E 0 . If the word does not occur in any N -Best list entry of a training sentence, but in the reference, we created an additional example (F, E, ””). The features of this examples can then be created straight forward as: I((F, E, E 0 )) = max(I(F ); I(E 0 )) Reordering was performed as a preprocessing step using part-of-speech information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT (Venugopal et al., 2005). We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consists of 1.7k segments containing 16k words. As test set we used 3.5k segments containing 31k words. We will refer to this system as System 1. In order t"
W13-2264,2010.amta-papers.32,0,0.0406929,"eatures is used to score the different translation hypotheses. Then the decoder searches for the translation with the highest score. 2 Related Work Bangalore et al. (2007) presented an approach to machine translation using discriminative lexical selection. Motivated by their results, Mauser et al. (2009) integrated the DWL into the PBMT ap512 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 512–520, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics proach. Thereby, they are able to use global source information. This was extended by Huck et al. (2010) by a feature selection strategy in order to reduce the number of weights. In Mediani et al. (2011) a first approach to use information about MT errors in the training of DWLs was presented. They select the training examples by using phrase table information also. The DWLs are related to work that was done in the area of word sense disambiguation (WSD). Carpuat and Wu (2007) presented an approach to disambiguate between different phrases instead of performing the disambiguation at word level. A different lexical model that uses target side information was presented in Jeong et al. (2010). The"
W13-2264,2010.amta-papers.33,0,0.0194973,"ded by Huck et al. (2010) by a feature selection strategy in order to reduce the number of weights. In Mediani et al. (2011) a first approach to use information about MT errors in the training of DWLs was presented. They select the training examples by using phrase table information also. The DWLs are related to work that was done in the area of word sense disambiguation (WSD). Carpuat and Wu (2007) presented an approach to disambiguate between different phrases instead of performing the disambiguation at word level. A different lexical model that uses target side information was presented in Jeong et al. (2010). The focus of this work was to model complex morphology on the target language. 3 When we have the probability for every word ej given the source sentence F , we need to combine these probabilities into a probability of the whole target sentence E = e1 . . . eJ given F . Making an assumption of independence on the target side as well, the models can be combined to the probability of E given F : p(E|F ) = ej ∈e The DWL is a maximum entropy model used to determine the probability of using a target word in the translation. Therefore, we train individual models for every target word. Each model i"
W13-2264,W05-0836,1,0.825363,"(F, E, ””). The features of this examples can then be created straight forward as: I((F, E, E 0 )) = max(I(F ); I(E 0 )) Reordering was performed as a preprocessing step using part-of-speech information generated by the TreeTagger (Schmid, 1994). We used the reordering approach described in Rottmann and Vogel (2007) and the extensions presented in Niehues and Kolss (2009) to cover long-range reorderings, which are typical when translating between German and English. An in-house phrase-based decoder was used to generate the translation hypotheses and the optimization was performed using MERT (Venugopal et al., 2005). We optimized the weights of the log-linear model on a separate set of TED talks and also used TED talks for testing. The development set consists of 1.7k segments containing 16k words. As test set we used 3.5k segments containing 31k words. We will refer to this system as System 1. In order to show the influence of the approaches better, we evaluated them also in a second system. In addition to the models used in the first system we performed a log-linear language model and phrase table adaptation as described in Niehues and Waibel (2012). To this system we refer as System 2 in the following"
W13-2264,E03-1076,0,0.606126,"on bigrams instead of unigrams, the number of features increases by a factor of eight. Furthermore, in both cases the translation quality drops. Especially for System 1, we have a significant drop in the BLEU score of the test set by 0.6 BLEU points. One problem might be that most of the bigrams occur quite rarely and therefore, we have a problem of data sparseness and generalization. If we combine the features of unigram and biSystem Description The translation system was trained on the EPPS corpus, NC corpus, the BTEC corpus and TED talks.2 The data was preprocessed and compound splitting (Koehn and Knight, 2003) was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). 2 German - English TED Experiments http://www.ted.com 516 Table 1: Experiments using different source features Syst"
W13-2264,N03-1017,0,0.0194217,"ource information, they ignore the structure of the source and target sentence. We propose to include this structure by modeling the source sentence as a bag-of-n-grams and features depending on the surrounding target words. Furthermore, as the standard DWL does not get any feedback from the MT system, we change the DWL training process to explicitly focus on addressing MT errors. By using these methods we are able to improve the translation performance by up to 0.8 BLEU points compared to a system that uses a standard DWL. 1 Introduction In many state-of-the-art SMT systems, the phrasebased (Koehn et al., 2003) approach is used. In this approach, instead of building the translation by translating word by word, sequences of source and target words, so-called phrase pairs, are used as the basic translation unit. A table of correspondences between source and target phrases forms the translation model. Target language fluency is modeled by a language model storing monolingual n-gram occurrences. A log-linear combination of these main models as well as additional features is used to score the different translation hypotheses. Then the decoder searches for the translation with the highest score. 2 Related"
W13-2264,P07-2045,0,0.0100206,"f the bigrams occur quite rarely and therefore, we have a problem of data sparseness and generalization. If we combine the features of unigram and biSystem Description The translation system was trained on the EPPS corpus, NC corpus, the BTEC corpus and TED talks.2 The data was preprocessed and compound splitting (Koehn and Knight, 2003) was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was built using the scripts from the Moses package (Koehn et al., 2007). A 4-gram language model was trained on the target side of the parallel data using the SRILM toolkit (Stolcke, 2002). In addition we used a bilingual language model as described in Niehues et al. (2011). 2 German - English TED Experiments http://www.ted.com 516 Table 1: Experiments using different source features System FeatureSize Baseline Unigram Bigram Uni+bigram + Count filter 2 + Count filter 5 + Trigram 0 40k 319k 359k 122k 63k 77k System 1 Dev Test 26.32 24.24 27.46 25.56 27.34 24.92 27.69 25.55 27.75 25.71 27.81 25.67 27.76 25.76 grams with count filtering in all experiments. In the f"
W13-2264,D09-1022,0,0.105865,"called phrase pairs, are used as the basic translation unit. A table of correspondences between source and target phrases forms the translation model. Target language fluency is modeled by a language model storing monolingual n-gram occurrences. A log-linear combination of these main models as well as additional features is used to score the different translation hypotheses. Then the decoder searches for the translation with the highest score. 2 Related Work Bangalore et al. (2007) presented an approach to machine translation using discriminative lexical selection. Motivated by their results, Mauser et al. (2009) integrated the DWL into the PBMT ap512 Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 512–520, c Sofia, Bulgaria, August 8-9, 2013 2013 Association for Computational Linguistics proach. Thereby, they are able to use global source information. This was extended by Huck et al. (2010) by a feature selection strategy in order to reduce the number of weights. In Mediani et al. (2011) a first approach to use information about MT errors in the training of DWLs was presented. They select the training examples by using phrase table information also. The DWLs are related t"
W13-2264,2011.iwslt-evaluation.9,1,\N,Missing
W13-3204,W09-0435,1,0.874539,"Missing"
W13-3204,W08-0303,1,0.787059,"differently before. We also introduced an all caps feature which is turned on if the whole word was written in capital letters. We hope that this can help detect abbreviations which are usually written in all capital letters. For example EU will be represented as 5.2 Translation System Description The translation system for the German-to-English task was trained on the European Parliament corpus, News Commentary corpus, the BTEC corpus and TED talks1 . The data was preprocessed and compound splitting was applied for German. Afterwards the discriminative word alignment approach as described in Niehues and Vogel (2008) was applied to generate the alignments between source and target words. The phrase table was w1 = e u eu &lt;w&gt;e u&lt;/w&gt;&lt;ALLCAPS&gt; 5 Word Representation Evaluation We evaluated the RBM-based language model on different statistical machine translation (SMT) tasks. We will first analyze the letter-based word 1 34 http://www.ted.com Model WordIndex Letter 1-gram Letter 2-gram Letter 3-gram Letter 3-gram Letter 4-gram Letter 4-gram Caps No No No Yes No Yes VocSize 27,748 107 1,879 12,139 8,675 43,903 25,942 TotalVectors 27,748 21,216 27,671 27,720 27,710 27,737 27,728 1 Word 27,748 17,319 27,620 27,701"
W13-3204,2012.iwslt-papers.3,1,0.627669,"oser together and generalize better over unseen words. We hope that words containing similar letter n-grams will yield a good indicator for words that have the same function inside the sentence. Introducing a method for subword units also has the advantage that the input layer can be smaller, while still representing nearly the same vocabulary with unique feature vectors. By using a smaller input layer, less weights need to be trained and the training is faster. In this work we present the letter n-gram approach to represent words in an CSLM, and compare it to the word-based CSLM presented in Niehues and Waibel (2012). The rest of this paper is structured as follows: First we will give an overview of related work. After that we give a brief overview of restricted Boltzmann machines which are the basis of the letter-based CSLM presented in Section 4. Then we will present the results of the experiments and conclude our work. We present a letter-based encoding for words in continuous space language models. We represent the words completely by letter n-grams instead of using the word index. This way, similar words will automatically have a similar representation. With this we hope to better generalize to unkno"
W13-3204,2010.iwslt-evaluation.11,1,0.856402,"ter 4-gram Dev 27.45 27.70 27.45 27.52 27.60 System Baseline +Letter 3-gram +Letter 3-gram+caps Baseline+ngram +Letter 3-gram +Letter 3-gram+caps BL+ngram+adaptpt +Letter 3-gram +Letter 3-gram+caps Test 24.06 24.34 24.15 24.25 24.30 Table 3: Results of German-to-English TED translations using an additional in-domain language model. Dev 28.40 28.55 28.31 28.31 28.46 Test 23.02 23.84 23.85 24.06 24.25 24.47 24.57 24.71 24.66 Table 5: Difference between caps and non-caps letter n-gram models. A third experiment is presented in Table 4. Here we also applied phrase table adaptation as described in Niehues et al. (2010). In this experiment the word index model improves the system by 0.4 BLEU points. In this case all letter-based models perform very similar. They are again performing slightly worse than the word index-based system, but better than the baseline system. To summarize the results, we could always improve the performance of the system by adding the letter n-gram-based language model. Furthermore, in most cases, the bigram model performs worse than the higher order models. It seems to be important for this task to have more context information. The 3- and 4-gram-based models perform almost equal, b"
W13-3204,W05-0821,0,0.0350579,"wn in Niehues and Waibel (2012), that using a restricted Boltzmann machine with a different layout during decoding can yield an increase in BLEU score. There has also been a lot of research in the field of using subword units for language modeling. In Shaik et al. (2011) linguistically motivated sub-lexical units were proposed to improve open vocabulary speech recognition for German. Research on morphology-based and subword language models on a Turkish speech recognition task has been done by Sak et al. (2010). The idea of Factored Language models in machine translation has been introduced by Kirchhoff and Yang (2005). Similar approaches to develop joint language models for morphologically rich languages in machine translation have been presented by Sarikaya and Deng (2007). In Emami et al. (2008) a factored neural network language model for Arabic was built. They used different features such as segmentation, part-of-speech and diacritics to enrich the information for each word. Restricted Boltzmann Machine-based Language Model In this section we will briefly review the continuous space language models using restricted Boltzmann machines (RBM). We will focus on the parts that are important for the implemen"
W13-3204,W11-2124,1,0.895117,"Missing"
W13-3204,P07-2045,0,0.0034696,"Missing"
W13-3204,2007.tmi-papers.21,0,0.100036,"Missing"
W13-3204,N07-2037,0,0.0254383,"s also been a lot of research in the field of using subword units for language modeling. In Shaik et al. (2011) linguistically motivated sub-lexical units were proposed to improve open vocabulary speech recognition for German. Research on morphology-based and subword language models on a Turkish speech recognition task has been done by Sak et al. (2010). The idea of Factored Language models in machine translation has been introduced by Kirchhoff and Yang (2005). Similar approaches to develop joint language models for morphologically rich languages in machine translation have been presented by Sarikaya and Deng (2007). In Emami et al. (2008) a factored neural network language model for Arabic was built. They used different features such as segmentation, part-of-speech and diacritics to enrich the information for each word. Restricted Boltzmann Machine-based Language Model In this section we will briefly review the continuous space language models using restricted Boltzmann machines (RBM). We will focus on the parts that are important for the implementation of the input layers described in the next section. A restricted Boltzmann machine is a generative stochastic neural network which consists of a visible"
W13-3204,H05-1026,0,0.0323748,"l networks were used to predict word categories. Xu and Rudnicky (2000) proposed a language model that has an input consisting of one word and no hidden units. This network was limited to infer unigram and bigram statistics. There has been research on feed forward neural network language models where they 30 Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 30–39, c Sofia, Bulgaria, August 9 2013. 2013 Association for Computational Linguistics 3 achieved a decrease in perplexity compared to standard n-gram language models (Bengio et al., 2003). In Schwenk and Gauvain (2005) and later in Schwenk (2007) research was performed on training large scale neural network language models on millions of words resulting in a decrease of the word error rate for continuous speech recognition. In Schwenk et al. (2006) they use the CSLM framework to rescore n-best lists of a machine translation system during tuning and testing steps. Usually these networks use short lists to reduce the size of the output layer and to make calculation feasible. There have been approaches to optimize the output layer of such a network, so that vocabularies of arbitrary size can be used and there"
W13-3204,W05-0836,1,0.786372,"Missing"
W13-3204,C90-3038,0,\N,Missing
W13-3204,C94-1027,0,\N,Missing
W13-3204,W12-2702,0,\N,Missing
W13-3204,N12-1005,0,\N,Missing
W14-3307,N12-1047,0,0.166607,"or WMT 2014 ∗ Quoc Khanh Do, † Teresa Herrmann, ∗† Jan Niehues, Alexandre Allauzen, ∗ Franc¸ois Yvon and † Alex Waibel ∗ LIMSI-CNRS, Orsay, France † Karlsruhe Institute of Technology, Karlsruhe, Germany ∗ surname@limsi.fr † firstname.surname@kit.edu ∗ Abstract ples as described in the n-gram approach (Mari˜no et al., 2006). We describe the integration of the SOUL models into the translation system in Section 3.2. Section 4 summarizes the experimental results and compares two different tuning algorithms: Minimum Error Rate Training (Och, 2003) and k-best Batch Margin Infused Relaxed Algorithm (Cherry and Foster, 2012). This paper describes the joined submission of LIMSI and KIT to the Shared Translation Task for the German-toEnglish direction. The system consists of a phrase-based translation system using a pre-reordering approach. The baseline system already includes several models like conventional language models on different word factors and a discriminative word lexicon. This system is used to generate a k-best list. In a second step, the list is reranked using SOUL language and translation models (Le et al., 2011). 2 The KIT translation system is an in-house implementation of the phrase-based approac"
W14-3307,W06-1607,0,0.0257787,"gopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score. 1 Baseline system Introduction This paper descri"
W14-3307,W13-0805,1,0.843382,"ection 3. While the translation system uses phrase pairs, the SOUL translation model uses tu84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model"
W14-3307,P03-1054,0,0.00441034,"uses tu84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999)."
W14-3307,E03-1076,0,0.0233691,"is an in-house implementation of the phrase-based approach and includes a pre-ordering step. This system is fully described in Vogel (2003). To train translation models, the provided Europarl, NC and Common Crawl parallel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on"
W14-3307,2005.iwslt-1.8,0,0.0327583,". 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source"
W14-3307,P07-2045,0,0.00834948,"erent language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield"
W14-3307,N12-1005,1,0.95525,"nts in terms of BLEU score. 1 Baseline system Introduction This paper describes the KIT-LIMSI system for the Shared Task of the ACL 2014 Ninth Workshop on Statistical Machine Translation. The system participates in the German-to-English translation task. It consists of two main components. First, a k-best list is generated using a phrasebased machine translation system. This system will be described in Section 2. Afterwards, the kbest list is reranked using SOUL (Structured OUtput Layer) models. Thereby, a neural network language model (Le et al., 2011), as well as several translation models (Le et al., 2012a) are used. A detailed description of these models can be found in Section 3. While the translation system uses phrase pairs, the SOUL translation model uses tu84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases fo"
W14-3307,P96-1041,0,0.212941,"target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 and t contains J target words (t1 , ..., tJ ). In the n-gram approach (Mari˜no et al., 2006; Cr"
W14-3307,J06-4004,0,0.187497,"Missing"
W14-3307,W05-0836,1,0.776382,"second step, the list is reranked using SOUL language and translation models (Le et al., 2011). 2 The KIT translation system is an in-house implementation of the phrase-based approach and includes a pre-ordering step. This system is fully described in Vogel (2003). To train translation models, the provided Europarl, NC and Common Crawl parallel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006"
W14-3307,2011.iwslt-evaluation.9,1,0.848234,"llel corpora are used. The target side of those parallel corpora, the News Shuffle corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation"
W14-3307,P10-2041,0,0.0330092,". In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 and t contains J target words (t1 , ..., tJ ). In the n-gram approach (Mari˜no et al., 2006; Crego et al., 2011), this segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments. In this framework, the basic translation"
W14-3307,W09-0435,1,0.847502,"splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score. 1 Baseline system Introduction This paper describes the KIT-LIMSI system for the Shared Task of the ACL 2014 Ninth Workshop on Statistical Machine Translation. Th"
W14-3307,W11-2124,1,0.856532,"ition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 and t contains J target words (t1 , ..., tJ ). In the n-gram approach (Mari˜no et al., 2006; Crego et al., 2011), this segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments. In this framework, the basic translation units are tuples, which are analogous to phrase pairs, and represent a matching u = (s, t) between a source phrase s and a target phrase t. Using the n-gram assumption, the joint probability of a"
W14-3307,J03-1002,0,0.0106255,"e corpus and the GigaWord corpus are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in Venugopal et al. (2005), using newstest2012 and newstest2013 as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration"
W14-3307,E99-1010,0,0.058547,"ning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MKCLS algorithm (Och, 1999). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (DWL) using source context to guide the word choices in the target sentence. 3 and t contains J target words (t1 , ..., tJ ). In the n-gram approach (Mari˜no et al., 2006; Crego et al., 2011), this segmentation is a by-product of source reordering, and ultimately derives from initial word and phrase alignments. In this framework, the basic translation units are tuples, which are analogous to phrase pairs, and represent a matching u = (s, t) between a source phrase s and a target ph"
W14-3307,P03-1021,0,0.00840255,"and the target t. The pair (s, t) decomposes into a sequence of L bilingual units (tuples) u1 , ..., uL . Each tuple ui contains a source and a target phrase: si and ti . ciated target hypothesis. The goal is to recover the information that is illustrated in Figure 1 and to apply the n-gram decomposition of a sentence pair. These (target and bilingual) neural network models produce scores for each hypothesis in the k-best list; these new features, along with the features from the baseline system, are then provided to a new phase which runs the traditional Minimum Error Rate Training (MERT ) (Och, 2003), or a recently proposed k-best Batch Margin Infused Relaxed Algorithm (KBMIRA ) (Cherry and Foster, 2012) for tuning purpose. The SOUL models used for this year’s evaluation are similar to those described in Allauzen et al. (2013) and Le et al. (2012b). However, since compared to these evaluations less parallel data is available for the German-to-English task, we use smaller vocabularies of about 100K words. model estimates the joint probability of a sentence pair using two sliding windows of length n, one for each language; however, the moves of these windows remain synchronized by the tuple"
W14-3307,W08-1006,0,0.0195332,"the SOUL translation model uses tu84 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 84–89, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics based on POS sequences are learnt automatically to perform source sentence reordering according to the target language word order. The long-range reordering rules are further applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) is applied to the source sentence. In addition to these pre-reordering models, a lexicalized reordering model (Koehn et al., 2005) is applied during decoding. Language models are trained with the SRILM toolkit (Stolcke, 2002) using modified Kneser-Ney smoothing (Chen and Goodman, 1996). The system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). The system further applies a language model based on 1000 automatically learned word classes using the MK"
W14-3307,2007.tmi-papers.21,0,0.039184,"as development and test data, respectively. Compound splitting (Koehn and Knight, 2003) is performed on the source side (German) of the corpus before training. Since the web-crawled Common Crawl corpus is noisy, this corpus is first filtered using an SVM classifier as described in Mediani et al. (2011). The word alignment is generated using the GIZA++ Toolkit (Och and Ney, 2003). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified Kneser-Ney smoothing (Foster et al., 2006). We apply short-range reorderings (Rottmann and Vogel, 2007) and long-range reorderings (Niehues and Kolss, 2009) based on part-ofspeech tags. The POS tags are generated using the TreeTagger (Schmid, 1994). Rewriting rules Originally, SOUL translation models were applied to n-gram-based translation systems that use tuples as translation units instead of phrase pairs. In this article, we describe their integration into the KIT phrase-based system. Experimental results show that their use can yield significant improvements in terms of BLEU score. 1 Baseline system Introduction This paper describes the KIT-LIMSI system for the Shared Task of the ACL 2014"
W14-3307,W12-3141,1,\N,Missing
W14-3310,E14-2008,1,0.50059,"nburgh, Scotland † Karlsruhe Institute of Technology, Karlsruhe, Germany ∗ {freitag,peitz,wuebker,ney}@cs.rwth-aachen.de ‡ {mhuck,ndurrani,pkoehn}@inf.ed.ac.uk ‡ v1rsennr@staffmail.ed.ac.uk ‡ maria.nadejde@gmail.com,p.j.williams-2@sms.ed.ac.uk † {teresa.herrmann,eunah.cho,alex.waibel}@kit.edu ‡ Matthias Abstract joint WMT submission of three EU-BRIDGE project partners. RWTH Aachen University (RWTH), the University of Edinburgh (UEDIN) and Karlsruhe Institute of Technology (KIT) all provided several individual systems which were combined by means of the RWTH Aachen system combination approach (Freitag et al., 2014). As distinguished from our EU-BRIDGE joint submission to the IWSLT 2013 evaluation campaign (Freitag et al., 2013), we particularly focused on translation of news text (instead of talks) for WMT. Besides, we put an emphasis on engineering syntaxbased systems in order to combine them with our more established phrase-based engines. We built combined system setups for translation from German to English as well as from English to German. This paper gives some insight into the technology behind the system combination framework and the combined engines which have been used to produce the joint EU-B"
W14-3310,D08-1089,0,0.0336771,"sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been emp"
W14-3310,N04-1035,0,0.0285873,"gmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, an"
W14-3310,P05-1066,1,0.0515024,"employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tune"
W14-3310,P06-1121,0,0.0128251,". The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model"
W14-3310,P13-2071,1,0.0664637,"ction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolate"
W14-3310,2012.iwslt-papers.17,1,0.805256,".1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel"
W14-3310,W13-2212,1,0.898684,"ction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolate"
W14-3310,P12-1031,0,0.00714997,"um Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes"
W14-3310,P13-2121,1,0.0604984,"Missing"
W14-3310,W14-3309,1,0.840972,"btained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translatio"
W14-3310,W11-2123,0,0.00995075,"a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus"
W14-3310,W06-1607,0,0.0222136,"., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering"
W14-3310,W13-0805,1,0.0274022,"GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with s"
W14-3310,2011.iwslt-papers.5,1,0.681344,"rying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase t"
W14-3310,2009.iwslt-papers.4,1,0.346713,"bility, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model and relies on KenLM for language model scoring during decoding. Model weights are optimized to maximize B LEU. 2000 sentences from the newstest2008-2012 sets have been selected as a development set. The selected sentences obtained high sentence-level B LEU scores when being translated with a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser"
W14-3310,D09-1022,1,0.0473567,"ingle generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newstest2012. System combination weights are either optimized on newstest2011 or newstest2012. We kept newstest2013 as an unseen test set wh"
W14-3310,P07-1019,0,0.0254758,"he settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language mod"
W14-3310,2011.iwslt-evaluation.9,1,0.882056,"n, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is d"
W14-3310,W13-2258,1,0.0602517,"r IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHK"
W14-3310,P10-2041,0,0.0226125,"for tuning the system combination or any of the individual systems. In total, the English→German system uses the following language models: two 4-gram wordbased language models trained on the parallel data and the filtered Common Crawl data separately, two 5-gram POS-based language models trained on the same data as the word-based language models, and a 4-gram cluster-based language model trained on 1,000 MKCLS word classes. The German→English system uses a 4-gram word-based language model trained on all monolingual data and an additional language model trained on automatically selected data (Moore and Lewis, 2010). Again, a 4-gram cluster-based language model trained on 1000 MKCLS word classes is applied. 5 6.1 The automatic scores of all individual systems as well as of our final system combination submission are given in Table 1. KIT, UEDIN and RWTH are each providing one individual phrasebased system output. RWTH (hiero) and UEDIN (GHKM) are providing additional systems based on the hierarchical translation model and a stringto-tree syntax model. The pairwise difference of the single system performances is up to 1.3 points in B LEU and 2.5 points in T ER. For German→English, our system combination p"
W14-3310,W14-3362,1,0.700694,"ned with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT trans"
W14-3310,W09-0435,0,0.00463497,"erated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained"
W14-3310,P03-1054,0,0.00425242,"ion obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the sy"
W14-3310,W08-0303,0,0.0192279,"sing an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word ord"
W14-3310,D07-1091,1,0.0290057,"2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side. For English→"
W14-3310,E03-1076,1,0.31096,"the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och"
W14-3310,W11-2124,1,0.0228284,"trip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newstest2012. System combination weights are either optimized on news"
W14-3310,2005.iwslt-1.8,1,0.035532,"2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single"
W14-3310,J03-1002,1,0.0147027,"of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid,"
W14-3310,E99-1010,0,0.0419329,"l., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 div"
W14-3310,P07-2045,1,0.0154876,"th a baseline phrasebased system, and each contain less than 30 words for more rapid tuning. Decoding for the syntaxbased systems is carried out with cube pruning using Moses’ hierarchical decoder (Hoang et al., 2009). UEDIN’s German→English syntax-based setup is a string-to-tree system with compound splitting University of Edinburgh UEDIN contributed phrase-based and syntaxbased systems to both the German→English and the English→German joint submission. 3.1 Syntax-based Systems Phrase-based Systems UEDIN’s phrase-based systems (Durrani et al., 2014) have been trained using the Moses toolkit (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004),"
W14-3310,P03-1021,0,0.0102254,"03) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has"
W14-3310,W14-3317,1,0.820032,"ty, the University of Edinburgh, and Karlsruhe Institute of Technology developed several individual systems which serve as system combination input. We devoted special attention to building syntax-based systems and combining them with the phrasebased ones. The joint setups yield empirical gains of up to 1.6 points in B LEU and 1.0 points in T ER on the WMT newstest2013 test set compared to the best single systems. 1 Introduction 2 EU-BRIDGE1 is a European research project which is aimed at developing innovative speech translation technology. This paper describes a RWTH Aachen University RWTH (Peitz et al., 2014) employs both the phrase-based (RWTH scss) and the hierarchical (RWTH hiero) decoder implemented in RWTH’s publicly available translation toolkit Jane (Vilar 1 http://www.eu-bridge.eu 105 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 105–113, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics els over POS and morph sequences following Durrani et al. (2013c). The English→German system additionally comprises a target-side LM over automatically built word classes (Birch et al., 2013). UEDIN has applied syntactic prereordering"
W14-3310,N04-1022,0,0.063305,"it (Koehn et al., 2007), replicating the settings described in (Durrani et al., 2013b). The features include: a maximum sentence length of 80, growdiag-final-and symmetrization of GIZA++ alignments, an interpolated Kneser-Ney smoothed 5gram language model with KenLM (Heafield, 2011) used at runtime, a lexically-driven 5-gram operation sequence model (OSM) (Durrani et al., 2013a), msd-bidirectional-fe lexicalized reordering, sparse lexical and domain features (Hasler et al., 2012), a distortion limit of 6, a maximum phrase length of 5, 100-best translation options, Minimum Bayes Risk decoding (Kumar and Byrne, 2004), cube pruning (Huang and Chiang, 2007), with a stack size of 1000 during tuning and 5000 during testing and the no-reordering-overpunctuation heuristic. UEDIN uses POS and morphological target sequence models built on the indomain subset of the parallel corpus using KneserNey smoothed 7-gram models as additional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual train"
W14-3310,W10-1738,1,0.159493,"Missing"
W14-3310,W08-1005,0,0.0331415,"nce reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system trained with target-side syntactic annotation obtained with the German Berkeley Parser (Petrov and Klein, 2007; Petrov and Klein, 2008). UEDIN GHKM T2S (Berkeley): A tree-tostring system trained with source-side syntactic annotation obtained with the English Berkeley Parser (Petrov et al., 2006). UEDIN GHKM S2S (Berkeley): A string-tostring system. The extraction is GHKMbased with syntactic target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2"
W14-3310,P06-1055,0,0.0182004,"dditional factors in phrase translation models (Koehn and Hoang, 2007). UEDIN has furthermore built OSM mod106 model. The monolingual part of those parallel corpora, the News Shuffle corpus for both directions and additionally the Gigaword corpus for German→English are used as monolingual training data for the different language models. Optimization is done with Minimum Error Rate Training as described in (Venugopal et al., 2005), using newstest2012 and newstest2013 as development and test data respectively. on the German source-language side and syntactic annotation from the Berkeley Parser (Petrov et al., 2006) on the English target-language side. For English→German, UEDIN has trained various string-to-tree GHKM syntax systems which differ with respect to the syntactic annotation. A tree-to-string system and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs ar"
W14-3310,W12-3150,1,0.544959,"uster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules (Galley et al., 2006) are extracted in addition to minimal rules, but only up to the following limits: at most twenty tree nodes per rule, a maximum depth of five, and a maximum size of five. Singleton hierarchical rules are dropped. The features for the syntax-based systems comprise Good-Turing-smoothed phrase translation probabilities, lexical translation probabilities in both directions, word and phrase penalty, a rule rareness penalty, a monolingual PCFG probability, and a 5-gram language model. UEDIN has used the SRILM toolkit (Stolcke, 2002) to train the language model a"
W14-3310,W08-1006,0,0.0232292,"e system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koeh"
W14-3310,W14-3324,1,0.0949085,"Missing"
W14-3310,2007.tmi-papers.21,0,0.125992,", 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottmann and Vogel, 2007) and longrange reorderings (Niehues and Kolss, 2009) based on POS tags (Schmid, 1994) to perform source sentence reordering according to the target language word order. The long-range reordering rules are applied to the training corpus to create reordering lattices to extract the phrases for the translation model. In addition, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Rafferty and Manning, 2008b; Klein and Manning, 2003) as well as a lexicalized reordering model (Koehn et al., 2005) are applied. UEDIN GHKM S2T (Berkeley): A string-totree system tra"
W14-3310,C12-3061,1,0.125144,"013). UEDIN has applied syntactic prereordering (Collins et al., 2005) and compound splitting (Koehn and Knight, 2003) of the source side for the German→English system. The systems have been tuned on a very large tuning set consisting of the test sets from 2008-2012, with a total of 13,071 sentences. UEDIN used newstest2013 as held-out test set. On top of UEDIN phrase-based 1 system, UEDIN phrase-based 2 augments word classes as additional factor and learns an interpolated target sequence model over cluster IDs. Furthermore, it learns OSM models over POS, morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has"
W14-3310,C08-1098,0,0.00933067,"target-side annotation from the German Berkeley Parser, but we strip off the syntactic labels. The final grammar contains rules with a single generic nonterminal instead of syntactic ones, plus rules that have been added from plain phrase-based extraction (Huck et al., 2014). 4 Language models are trained with the SRILM toolkit (Stolcke, 2002) and use modified KneserNey smoothing. Both systems utilize a language model based on automatically learned word classes using the MKCLS algorithm (Och, 1999). The English→German system comprises language models based on fine-grained part-ofspeech tags (Schmid and Laws, 2008). In addition, a bilingual language model (Niehues et al., 2011) is used as well as a discriminative word lexicon (Mauser et al., 2009) using source context to guide the word choices in the target sentence. Karlsruhe Institute of Technology The KIT translations (Herrmann et al., 2014) are generated by an in-house phrase-based translations system (Vogel, 2003). The provided News Commentary, Europarl, and Common Crawl parallel corpora are used for training the translation 107 dividual system engines have been optimized on different test sets which partially or fully include newstest2011 or newst"
W14-3310,D13-1138,1,0.0721313,", morph and word classes. et al., 2010; Wuebker et al., 2012). The model weights of all systems have been tuned with standard Minimum Error Rate Training (Och, 2003) on a concatenation of the newstest2011 and newstest2012 sets. RWTH used B LEU as optimization objective. Both for language model estimation and querying at decoding, the KenLM toolkit (Heafield et al., 2013) is used. All RWTH systems include the standard set of models provided by Jane. Both systems have been augmented with a hierarchical orientation model (Galley and Manning, 2008; Huck et al., 2013) and a cluster language model (Wuebker et al., 2013). The phrasebased system (RWTH scss) has been further improved by maximum expected B LEU training similar to (He and Deng, 2012). The latter has been performed on a selection from the News Commentary, Europarl and Common Crawl corpora based on language and translation model cross-entropies (Mansour et al., 2011). 3 3.2 UEDIN’s syntax-based systems (Williams et al., 2014) follow the GHKM syntax approach as proposed by Galley, Hopkins, Knight, and Marcu (Galley et al., 2004). The open source Moses implementation has been employed to extract GHKM rules (Williams and Koehn, 2012). Composed rules ("
W14-3310,C04-1024,0,0.0194264,"f the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster et al., 2006). UEDIN GHKM S2T (Stanford): A string-totree system trained with target-side syntactic annotation obtained with the German Stanford Parser (Rafferty and Manning, 2008a). In both systems KIT applies short-range reorderings (Rottman"
W14-3310,R13-1079,1,0.28133,"stem and a string-to-string system (with rules that are not syntactically decorated) have been trained as well. The English→German UEDIN GHKM system names in Table 3 denote: Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation before training. In order to improve the quality of the web-crawled Common Crawl corpus, noisy sentence pairs are filtered out using an SVM classifier as described by Mediani et al. (2011). UEDIN GHKM S2T (ParZu): A string-to-tree system trained with target-side syntactic annotation obtained with ParZu (Sennrich et al., 2013). It uses a modified syntactic label set, target-side compound splitting, and additional syntactic constraints. UEDIN GHKM S2T (BitPar): A string-to-tree system trained with target-side syntactic annotation obtained with BitPar (Schmid, 2004). The word alignment for German→English is generated using the GIZA++ toolkit (Och and Ney, 2003). For English→German, KIT uses discriminative word alignment (Niehues and Vogel, 2008). Phrase extraction and scoring is done using the Moses toolkit (Koehn et al., 2007). Phrase pair probabilities are computed using modified KneserNey smoothing as in (Foster e"
W14-3310,N07-1051,0,\N,Missing
W14-3310,W05-0836,1,\N,Missing
W14-3310,W05-0909,0,\N,Missing
W14-3310,N13-1001,1,\N,Missing
W14-3310,2010.iwslt-evaluation.11,1,\N,Missing
W14-3310,2013.iwslt-evaluation.16,1,\N,Missing
W14-3310,W13-2213,1,\N,Missing
W14-3310,W14-3313,1,\N,Missing
W14-3310,W11-2145,1,\N,Missing
W14-3310,2013.iwslt-evaluation.3,1,\N,Missing
W14-3313,W06-1607,0,0.102882,"us Giga for English→French and French→English. The monolingual part 130 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 130–135, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics inal position of the words is included as an additional score in the log-linear model of the translation system. done by the Moses toolkit, whereas the bigger sets (French↔English) are scored by our in-house parallel phrase scorer (Mediani et al., 2012a). The phrase pair probabilities are computed using modified Kneser-Ney smoothing as described in Foster et al. (2006). Since German is a highly inflected language, we try to alleviate the out-of-vocabulary problem through quasi-morphological operations that change the lexical entry of a known word form to an unknown word form as described in Niehues and Waibel (2011). 3.1 3.2 Adaptation In the French→English and English→French systems, we perform adaptation for translation models as well as for language models. The EPPS and NC corpora are used as in-domain data for the direction English→French, while NC corpus is the in-domain data for French→English. Two phrase tables are built: one is the outof-domain phra"
W14-3313,W11-2123,0,0.030482,"trained with 1,000 classes using EPPS, NC, and Common Crawl data. Dev 16.64 16.76 17.27 17.45 17.53 17.55 17.82 Test 18.60 18.66 19.66 19.75 19.85 19.92 20.21 Table 3: Experiments for English→German 4.4 German-English Table 4 shows the development steps of the German-English translation system. For the baseline system, the training data of the translation model consists of EPPS, NC and the filtered parallel crawled data. The phrase table is built using GIZA++ word alignment and lattice phrase extraction. All language models are trained with SRILM and scored in the decoding process with KenLM (Heafield, 2011). We use word lattices generated by short and long range reordering rules as input to the decoder. In addition, a bilingual language model and a target language model trained on word clusters with 1,000 classes are included in the system. Enhancing the word reordering with tree-based reordering rules and a lexicalized reordering 133 improve the system performance for GermanEnglish translation. In average we achieved an improvement of over 1.5 BLEU over the respective baselines for all our systems. model improved the system performance by 0.6 BLEU points. Adding a language model trained on sele"
W14-3313,W13-0805,1,0.87688,"model during decoding. Optimal weights are set during tuning by MERT. Word Reordering Models We apply automatically learned reordering rules based on part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice. The lattice, which also includes the original position of each word, is used"
W14-3313,P03-1054,0,0.00683643,"ces and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice. The lattice, which also includes the original position of each word, is used as input to the decoder. In order to acquire phrase pairs matching the reordered sentence variants, we perform lattice phrase extraction (LPE) on the training corpus where phrase"
W14-3313,E03-1076,0,0.126476,"s well as a lexicalized reordering model. In order to make use of noisy web-crawled data, we apply filtering and data selection methods for language modeling. A discriminative word lexicon using source context information proved beneficial for all translation directions. 1 3 Before training we perform a common preprocessing of the raw data, which includes removing long sentences and sentences with a length mismatch exceeding a certain threshold. Afterwards, we normalize special symbols, dates, and numbers. Then we perform smart-casing of the first letter of every sentence. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation. In order to improve the quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (200"
W14-3313,2005.iwslt-1.8,0,0.0779785,"POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice. The lattice, which also includes the original position of each word, is used as input to the decoder. In order to acquire phrase pairs matching the reordered sentence variants, we perform lattice phrase extraction (LPE) on the training corpus where phrase are extracted from the reordered word lattices instead of the original sentences. In addition, we use a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair. During decoding the lexicalized reordering model determines the reordering orientation of each phrase pair at the phrase boundaries. The probability for the respective orientation with respect to the orig3.3 Special Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. With the help of a bilingual language model (Niehues et al., 2011) we are able to increase the bilingual context between source and target words beyond phrase boundaries. This language mod"
W14-3313,J03-1002,0,0.0206269,"quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For English→German, we use discriminative word alignment trained on hand-aligned data as described in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., 2007). The phrase scoring for the small data sets (German↔English) is also Introduction We describe the KIT systems for the Shared Translation Task of the ACL 2014 Ninth Workshop on Statistical Machine Translation. We participated in the English↔German and English↔French translation directions, usi"
W14-3313,D09-1022,0,0.020404,"or cluster ID. During decoding, these language models are used as additional models in the log-linear combination. The data selection language model is trained on data automatically selected using cross-entropy differences between development sets from previous WMT workshops and the noisy crawled data (Moore and Lewis, 2010). We selected the top 10M sentences to train this language model. 3.4 Discriminative Word Lexicon A discriminative word lexicon (DWL) models the probability of a target word appearing in the translation given the words of the source sentence. DWLs were first introduced by Mauser et al. (2009). For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. We use two simplifications of this model that have shown beneficial to translation quality and training time in the past (Mediani et al., 2011). Firstly, we calculate the score for every phrase pair before translating. Secondly, we restrict the negative training examples to words that occur within matching phrase pairs. In this evaluation, we extended the DWL with n-gram source context features proposed by Niehues and W"
W14-3313,E99-1010,0,0.237269,"model (Niehues et al., 2011) we are able to increase the bilingual context between source and target words beyond phrase boundaries. This language model is trained on bilingual tokens created from a target word and all its aligned source words. The tokens are ordered according to the target language word order. Furthermore, we use language models based on fine-grained part-of-speech tags (Schmid and Laws, 2008) as well as word classes to alleviate the sparsity problem for surface words. The word classes are automatically learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). These n-gram language models are trained on the target language corpus, 131 It is noteworthy that, for this direction, we chose to tune on a subset of 1,000 pairs from newstest2012, due to the long time the whole set takes to be decoded. In a preliminary set of experiments (not reported here), we found no significant differences between tuning on the small or the big development sets. The translation model of the baseline system is trained on the whole parallel data after filtering (EPPS, NC, Common Crawl, Giga). The same data was also used for language modeling. We also use POS-based reorde"
W14-3313,2011.iwslt-evaluation.9,1,0.935211,"re training we perform a common preprocessing of the raw data, which includes removing long sentences and sentences with a length mismatch exceeding a certain threshold. Afterwards, we normalize special symbols, dates, and numbers. Then we perform smart-casing of the first letter of every sentence. Compound splitting (Koehn and Knight, 2003) is performed on the source side of the corpus for German→English translation. In order to improve the quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For English→German, we use discriminative word alignme"
W14-3313,P02-1040,0,0.0896789,"information about source word order in the model. We used one feature per n-gram up to the order of three and applied count filtering for bigrams and trigrams. 4 System Baseline + Big LMs + PT Adaptation + Bilingual + Cluster LM + Lexicalized Reordering + Source DWL Results This section presents the participating systems used for the submissions in the four translation directions of the evaluation. We describe the individual components that form part of each of the systems and report the translation qualities achieved during system development. The scores are reported in case-sensitive BLEU (Papineni et al., 2002). 4.1 Dev 15.63 16.56 16.77 16.87 16.92 17.28 Test 27.61 29.02 29.32 29.64 30.17 30.19 Table 1: Experiments for English→French 4.2 French-English Several experiments were conducted for the French→English translation system. They are summarized in Table 2. The baseline system is essentially a phrasebased translation system with some preprocessEnglish-French The development of our English→French system is shown in Table 1. 132 ing steps on the source side and utilizing the short-range POS-based reordering on all parallel data and fine-grained monolingual corpora such as EPPS and NC. Adapting the"
W14-3313,W08-1006,0,0.0827719,"part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice. The lattice, which also includes the original position of each word, is used as input to the decoder. In order to acquire phrase pairs matching the reordered sentence variants, we perform lattice phrase extraction (LPE) on the tra"
W14-3313,2007.tmi-papers.21,0,0.507964,"indomain language model. We train a separate language model using only the in-domain data. Then it is used as an additional language model during decoding. Optimal weights are set during tuning by MERT. Word Reordering Models We apply automatically learned reordering rules based on part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the or"
W14-3313,P10-2041,0,0.0621761,"nally, using a discriminative word lexicon with source context has a very small positive effect on the test score, however more than 0.3 on dev. This final configuration was the basis of our submitted official translation. where the words have been replaced either by their corresponding POS tag or cluster ID. During decoding, these language models are used as additional models in the log-linear combination. The data selection language model is trained on data automatically selected using cross-entropy differences between development sets from previous WMT workshops and the noisy crawled data (Moore and Lewis, 2010). We selected the top 10M sentences to train this language model. 3.4 Discriminative Word Lexicon A discriminative word lexicon (DWL) models the probability of a target word appearing in the translation given the words of the source sentence. DWLs were first introduced by Mauser et al. (2009). For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. We use two simplifications of this model that have shown beneficial to translation quality and training time in the past (Mediani"
W14-3313,C08-1098,0,0.0570962,"ith respect to the orig3.3 Special Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. With the help of a bilingual language model (Niehues et al., 2011) we are able to increase the bilingual context between source and target words beyond phrase boundaries. This language model is trained on bilingual tokens created from a target word and all its aligned source words. The tokens are ordered according to the target language word order. Furthermore, we use language models based on fine-grained part-of-speech tags (Schmid and Laws, 2008) as well as word classes to alleviate the sparsity problem for surface words. The word classes are automatically learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). These n-gram language models are trained on the target language corpus, 131 It is noteworthy that, for this direction, we chose to tune on a subset of 1,000 pairs from newstest2012, due to the long time the whole set takes to be decoded. In a preliminary set of experiments (not reported here), we found no significant differences between tuning on the small or the big development sets. The translatio"
W14-3313,W09-0435,1,0.901419,"model using only the in-domain data. Then it is used as an additional language model during decoding. Optimal weights are set during tuning by MERT. Word Reordering Models We apply automatically learned reordering rules based on part-of-speech (POS) sequences and syntactic parse tree constituents to perform source sentence reordering according to the target language word order. The rules are learned from a parallel corpus with POS tags (Schmid, 1994) for the source side and a word alignment to learn reordering rules that cover short range (Rottmann and Vogel, 2007) and long range reorderings (Niehues and Kolss, 2009). In addition, we apply a tree-based reordering model (Herrmann et al., 2013) to better address the differences in word order between German and English. Here, a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) for the source side of the training corpus are required to learn rules on how to reorder the constituents in the source sentence. The POS-based and tree-based reordering rules are applied to each input sentence before translation. The resulting reordered sentence variants as well as the original sentence are encoded in a reordering lattice."
W14-3313,W08-0303,1,0.888132,"e models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For English→German, we use discriminative word alignment trained on hand-aligned data as described in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., 2007). The phrase scoring for the small data sets (German↔English) is also Introduction We describe the KIT systems for the Shared Translation Task of the ACL 2014 Ninth Workshop on Statistical Machine Translation. We participated in the English↔German and English↔French translation directions, using a phrase-based decoder with lattice input. The paper is organized as follows: the next section describes the data used for each translation direction. Section 3 gives a detailed description of our systems including all the mod"
W14-3313,2011.iwslt-papers.6,1,0.862993,"s inal position of the words is included as an additional score in the log-linear model of the translation system. done by the Moses toolkit, whereas the bigger sets (French↔English) are scored by our in-house parallel phrase scorer (Mediani et al., 2012a). The phrase pair probabilities are computed using modified Kneser-Ney smoothing as described in Foster et al. (2006). Since German is a highly inflected language, we try to alleviate the out-of-vocabulary problem through quasi-morphological operations that change the lexical entry of a known word form to an unknown word form as described in Niehues and Waibel (2011). 3.1 3.2 Adaptation In the French→English and English→French systems, we perform adaptation for translation models as well as for language models. The EPPS and NC corpora are used as in-domain data for the direction English→French, while NC corpus is the in-domain data for French→English. Two phrase tables are built: one is the outof-domain phrase table, which is trained on all corpora; the other is the in-domain phrase table, which is trained on in-domain data. We adapt the translation model by using the scores from the two phrase tables with the backoff approach described in Niehues and Wai"
W14-3313,W05-0836,1,0.929843,"ehn and Knight, 2003) is performed on the source side of the corpus for German→English translation. In order to improve the quality of the web-crawled Common Crawl corpus, we filter out noisy sentence pairs using an SVM classifier for all four translation tasks as described in Mediani et al. (2011). Unless stated otherwise, we use 4-gram language models (LM) with modified Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002). All translations are generated by an in-house phrase-based translation system (Vogel, 2003), and we use Minimum Error Rate Training (MERT) as described in Venugopal et al. (2005) for optimization. The word alignment of the parallel corpora is generated using the GIZA++ Toolkit (Och and Ney, 2003) for both directions. Afterwards, the alignments are combined using the grow-diag-final-and heuristic. For English→German, we use discriminative word alignment trained on hand-aligned data as described in Niehues and Vogel (2008). The phrase table (PT) is built using the Moses toolkit (Koehn et al., 2007). The phrase scoring for the small data sets (German↔English) is also Introduction We describe the KIT systems for the Shared Translation Task of the ACL 2014 Ninth Workshop o"
W14-3313,2012.amta-papers.19,1,0.769296,"d Waibel (2011). 3.1 3.2 Adaptation In the French→English and English→French systems, we perform adaptation for translation models as well as for language models. The EPPS and NC corpora are used as in-domain data for the direction English→French, while NC corpus is the in-domain data for French→English. Two phrase tables are built: one is the outof-domain phrase table, which is trained on all corpora; the other is the in-domain phrase table, which is trained on in-domain data. We adapt the translation model by using the scores from the two phrase tables with the backoff approach described in Niehues and Waibel (2012). This results in a phrase table with six scores, the four scores from the general phrase table as well as the two conditional probabilities from the in-domain phrase table. In addition, we take the union of the candidate phrase pairs collected from both phrase tables A detailed description of the union method can be found in Mediani et al. (2012b). The language model is adapted by log-linearly combining the general language model and an indomain language model. We train a separate language model using only the in-domain data. Then it is used as an additional language model during decoding. Op"
W14-3313,W13-2264,1,0.762301,"et al. (2009). For every target word, they train a maximum entropy model to determine whether this target word should be in the translated sentence or not using one feature per source word. We use two simplifications of this model that have shown beneficial to translation quality and training time in the past (Mediani et al., 2011). Firstly, we calculate the score for every phrase pair before translating. Secondly, we restrict the negative training examples to words that occur within matching phrase pairs. In this evaluation, we extended the DWL with n-gram source context features proposed by Niehues and Waibel (2013). Instead of representing the source sentence as a bag-of-words, we model it as a bag-of-n-grams. This allows us to include information about source word order in the model. We used one feature per n-gram up to the order of three and applied count filtering for bigrams and trigrams. 4 System Baseline + Big LMs + PT Adaptation + Bilingual + Cluster LM + Lexicalized Reordering + Source DWL Results This section presents the participating systems used for the submissions in the four translation directions of the evaluation. We describe the individual components that form part of each of the system"
W14-3313,W11-2124,1,0.804622,"ted from the reordered word lattices instead of the original sentences. In addition, we use a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair. During decoding the lexicalized reordering model determines the reordering orientation of each phrase pair at the phrase boundaries. The probability for the respective orientation with respect to the orig3.3 Special Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. With the help of a bilingual language model (Niehues et al., 2011) we are able to increase the bilingual context between source and target words beyond phrase boundaries. This language model is trained on bilingual tokens created from a target word and all its aligned source words. The tokens are ordered according to the target language word order. Furthermore, we use language models based on fine-grained part-of-speech tags (Schmid and Laws, 2008) as well as word classes to alleviate the sparsity problem for surface words. The word classes are automatically learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). These n-gram lan"
W14-3313,P07-2045,0,\N,Missing
W14-3313,2010.iwslt-evaluation.11,1,\N,Missing
W15-3008,2005.iwslt-1.8,0,0.014132,"SVM classifier. Language models are built based on different tokens, such as word, partof-speech, and automacally generated word clusters. Final systems also include bilingual language models, part-of-speech and syntactic treebased reordering models as well as a lexicalized reordering model. For language modeling, a data selection strategy is also applied. A discriminative 92 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 92–97, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. original sentences. The lexicalized reordering (Koehn et al., 2005) encodes reordering probabilities for each phrase pair. By using the lexicalized reordering model, the reordering orientation of each phrase pair at the phrase boundaries can be determined during decoding. The probability for the respective orientation with respect to the original position of the words is included as an additional score in the loglinear model of the translation system. use an SVM classifier for both translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the"
W15-3008,P07-2045,0,0.00889681,"2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words beyond phrase boundaries. Target words and all their aligned source words form bilingual tokens on which a LM is trained. The tokens are then ordered according to the target language word order. For the English→German system, we use language models based on fine-grained POS tags (Schmid and Laws, 2008). In addition, we use"
W15-3008,P06-1096,0,0.312539,"Missing"
W15-3008,D09-1022,0,0.0294609,"e source sentence. Before translation, the POS-based and treebased reordering rules are applied to the each sentence. The variants of differently reordered sentences, including the original order of the sentence, are encoded in a word lattice. The word lattice is then used as an input to the decoder. Lattice phrase extraction (LPE) (Niehues et al., 2010) is applied on the training corpus, in order to get phrase pairs that match the reordered sentences. In this scheme, we use the reordered sentences to extract the phrases from, instead of the 3.3 Discriminative Word Lexicon First introduced by Mauser et al. (2009), a discriminative word lexicon (DWL) models the probability of a target word appearing in the translation 93 given the words of the source sentence. For every target word, a maximum entropy model is trained to determine whether this target word should be in the translated sentence or not using one feature per source word. 3.4 In order to facilitate more complex models like neural network translation models, we rescored the n-best lists. In our experiments we generated 300 best lists for the development and test data respectively. We used the same data to train the rescoring that we have used"
W15-3008,P14-1129,0,0.0788838,"Missing"
W15-3008,2011.iwslt-evaluation.9,1,0.889513,"ortugal, 17-18 September 2015. 2015 Association for Computational Linguistics. original sentences. The lexicalized reordering (Koehn et al., 2005) encodes reordering probabilities for each phrase pair. By using the lexicalized reordering model, the reordering orientation of each phrase pair at the phrase boundaries can be determined during decoding. The probability for the respective orientation with respect to the original position of the words is included as an additional score in the loglinear model of the translation system. use an SVM classifier for both translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German di"
W15-3008,W11-2123,0,0.0358667,"ies for each phrase pair. By using the lexicalized reordering model, the reordering orientation of each phrase pair at the phrase boundaries can be determined during decoding. The probability for the respective orientation with respect to the original position of the words is included as an additional score in the loglinear model of the translation system. use an SVM classifier for both translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different"
W15-3008,W13-0805,1,0.854708,"ls Reordering rules encode how the words in the source sentence are to be ordered according to the target word order. They are learned automatically based on part-of-speech (POS) as well as syntactic parse tree constituents. In order to learn the rules, we use POS tags (Schmid, 1994) of the source side and the word alignment information. The rules cover short range reorderings (Rottmann and Vogel, 2007) as well as long range reorderings (Niehues and Kolss, 2009). The differences in word order between German and English can be better addressed by using a tree-based reordering model as shown in Herrmann et al. (2013). The tree-based reordering rules are learned from a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) from the source side of the training corpus. The rules encode the information on how to reorder constituents in the syntactic tree of the source sentence. Before translation, the POS-based and treebased reordering rules are applied to the each sentence. The variants of differently reordered sentences, including the original order of the sentence, are encoded in a word lattice. The word lattice is then used as an input to the decoder. Lattice phrase"
W15-3008,P10-2041,0,0.0121769,"build these language models, we replace each word token of the target language corpus by its corresponding POS tag or cluster ID. The ngram language models are then built on this new corpus consisting of either POS tags or cluster IDs. During decoding, these language models are used as additional models in the log-linear combination. For the German→English system, the data selection language model is trained on data automatically selected using cross-entropy differences between development sets from previous WMT workshops and the English side of all data, including the filtered crawled data (Moore and Lewis, 2010). We selected the top 10M sentences to train this language model. For building all non-word language models used in this work smoothing is applied. Word Reordering Models Reordering rules encode how the words in the source sentence are to be ordered according to the target word order. They are learned automatically based on part-of-speech (POS) as well as syntactic parse tree constituents. In order to learn the rules, we use POS tags (Schmid, 1994) of the source side and the word alignment information. The rules cover short range reorderings (Rottmann and Vogel, 2007) as well as long range reo"
W15-3008,P02-1040,0,0.0947921,"94 described in Section 3.4 for the log-linear combination of features. By doing so, we improve the translation performance by another 0.8 BLEU points on the test set. This system was submitted to WMT 2015 and used for the translation of the official test set. sum of all free energies in the sentence is used as an additional feature for rescoring. 4 Results In this section, we present a summary of our experiments in the evaluation campaign. Individual components that lead to improvements in the translation performance are described step by step. The scores are reported in case-sensitive BLEU (Papineni et al., 2002). 4.1 System Baseline + Non-word LMs + Tree + Lex. Reorderings + Source–context DWL + ListNet rescoring English-German Table 1 shows the results of our system for English→German translation task. The baseline system consists of a phrase table derived from DWA, the word-based language models built from different parts of the corpus and POS-based long-range reordering rules. Reordering rules, however, are extracted from the POStagged EPPS and NC only, and encoded as word lattices. The parallel data used to build the word alignments and the PT are EPPS, NC and the filtered Crawl data. Similarly,"
W15-3008,W09-0435,1,0.79966,"ted the top 10M sentences to train this language model. For building all non-word language models used in this work smoothing is applied. Word Reordering Models Reordering rules encode how the words in the source sentence are to be ordered according to the target word order. They are learned automatically based on part-of-speech (POS) as well as syntactic parse tree constituents. In order to learn the rules, we use POS tags (Schmid, 1994) of the source side and the word alignment information. The rules cover short range reorderings (Rottmann and Vogel, 2007) as well as long range reorderings (Niehues and Kolss, 2009). The differences in word order between German and English can be better addressed by using a tree-based reordering model as shown in Herrmann et al. (2013). The tree-based reordering rules are learned from a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) from the source side of the training corpus. The rules encode the information on how to reorder constituents in the syntactic tree of the source sentence. Before translation, the POS-based and treebased reordering rules are applied to the each sentence. The variants of differently reordered sent"
W15-3008,W08-0303,1,0.743399,"oth translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words beyond phrase boundaries. Target words and all their aligned source words form bilingual tokens on which a LM is trained. The tokens are then ordered according to the target language word order. For the En"
W15-3008,W08-1006,0,0.0296727,"They are learned automatically based on part-of-speech (POS) as well as syntactic parse tree constituents. In order to learn the rules, we use POS tags (Schmid, 1994) of the source side and the word alignment information. The rules cover short range reorderings (Rottmann and Vogel, 2007) as well as long range reorderings (Niehues and Kolss, 2009). The differences in word order between German and English can be better addressed by using a tree-based reordering model as shown in Herrmann et al. (2013). The tree-based reordering rules are learned from a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) from the source side of the training corpus. The rules encode the information on how to reorder constituents in the syntactic tree of the source sentence. Before translation, the POS-based and treebased reordering rules are applied to the each sentence. The variants of differently reordered sentences, including the original order of the sentence, are encoded in a word lattice. The word lattice is then used as an input to the decoder. Lattice phrase extraction (LPE) (Niehues et al., 2010) is applied on the training corpus, in order to get phrase pairs that match the r"
W15-3008,2012.iwslt-papers.3,1,0.896933,"l as source DWL. This model predicts the target word for a given source word as described in detail in (Herrmann, 2015). In a first step, we identify the 20 most frequent translations of each word. Then we build a multiclass classifier to predict the correct translation. For the classifier, we used a binary maximumentropy classifier1 trained using the one-againstall approach. 3.5 RBM Translation Model In rescoring, we used an restricted Boltzmann machine (RBM)-based translation model inspired by the work of Devlin et al. (2014). The model is based on the RBM-based language model introduced in Niehues and Waibel (2012). The RBM models the joint probability of eight target words and a set of attached source words. The set of attached source words is calculated as follows: We first use the source word aligned to the last target word in the 8-gram. If this does not exist, we take the source word aligned to the nearest target word. The set of source words consists then of this source word, its previous five source words and its following five source words. We create this set of 8 target and 11 source words for every target 8-gram in the parallel corpus and train the model using unigram sampling as described in"
W15-3008,2007.tmi-papers.21,0,0.0311498,"g the filtered crawled data (Moore and Lewis, 2010). We selected the top 10M sentences to train this language model. For building all non-word language models used in this work smoothing is applied. Word Reordering Models Reordering rules encode how the words in the source sentence are to be ordered according to the target word order. They are learned automatically based on part-of-speech (POS) as well as syntactic parse tree constituents. In order to learn the rules, we use POS tags (Schmid, 1994) of the source side and the word alignment information. The rules cover short range reorderings (Rottmann and Vogel, 2007) as well as long range reorderings (Niehues and Kolss, 2009). The differences in word order between German and English can be better addressed by using a tree-based reordering model as shown in Herrmann et al. (2013). The tree-based reordering rules are learned from a word alignment and syntactic parse trees (Rafferty and Manning, 2008; Klein and Manning, 2003) from the source side of the training corpus. The rules encode the information on how to reorder constituents in the syntactic tree of the source sentence. Before translation, the POS-based and treebased reordering rules are applied to t"
W15-3008,W13-2264,1,0.785134,"e they are computed as the logarithm of relatively small probabilities. Therefore, we rescale all scores observed on the development data to the range of [−1, 1] prior to rescoring. Two simplifications of this model are used to improve the translation quality while maintaining the time efficiency as shown in Mediani et al. (2011). First, the score for every phrase pair is calculated before translation. Then we restrict the negative training examples to words that occur within matching phrase pairs. In this evaluation, the DWL is further extended with n-gram source context features proposed by Niehues and Waibel (2013). In this paper, this model will be referred to as source-context DWL. The source sentence is represented as a bag-of-ngrams, instead of a bag-of-words. By doing so it is possible to include information about source word order in the model. We used one feature per ngram up to the order of three and applied count filtering for bigrams and trigrams. In addition to this DWL, we integrated a DWL in the reverse direction in rescoring. We will refer to this model as source DWL. This model predicts the target word for a given source word as described in detail in (Herrmann, 2015). In a first step, we"
W15-3008,C08-1098,0,0.0213769,"using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words beyond phrase boundaries. Target words and all their aligned source words form bilingual tokens on which a LM is trained. The tokens are then ordered according to the target language word order. For the English→German system, we use language models based on fine-grained POS tags (Schmid and Laws, 2008). In addition, we use language models based on word classes learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). Using such language models, we can generalize better and therefore alleviate the sparsity problem for surface words. In order to build these language models, we replace each word token of the target language corpus by its corresponding POS tag or cluster ID. The ngram language models are then built on this new corpus consisting of either POS tags or cluster IDs. During decoding, these language models are used as additional models in the log-linear com"
W15-3008,W11-2124,1,0.855568,"ations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words beyond phrase boundaries. Target words and all their aligned source words form bilingual tokens on which a LM is trained. The tokens are then ordered according to the target language word order. For the English→German system, we use language models based on fine-grained POS tags (Schmid and Laws, 2008). In addition, we use language models based on word classes learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). Using such language models, we can generalize better and therefore allevi"
W15-3008,W05-0836,1,0.776161,"bility for the respective orientation with respect to the original position of the words is included as an additional score in the loglinear model of the translation system. use an SVM classifier for both translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words bey"
W15-3008,2014.amta-researchers.17,1,0.814055,". The RBM models the joint probability of eight target words and a set of attached source words. The set of attached source words is calculated as follows: We first use the source word aligned to the last target word in the 8-gram. If this does not exist, we take the source word aligned to the nearest target word. The set of source words consists then of this source word, its previous five source words and its following five source words. We create this set of 8 target and 11 source words for every target 8-gram in the parallel corpus and train the model using unigram sampling as described in Niehues et al. (2014). In rescoring, we then calculate the free energy of the RBM given the 8-gram and its source set as input. The As features for the classifier, we used the previous and following three words. Each word is represented by a continuous vector of 100 dimensions as described in (Mikolov et al., 2013). Using the predictions, we calculated four additional features. The first two features are the absolute and relative number of words, where the translation predicted by the classifier and the translation in the hypothesis is the same. The third feature is the sum of the word to word translation probabil"
W15-3008,W15-3030,1,0.840612,"rget word, a maximum entropy model is trained to determine whether this target word should be in the translated sentence or not using one feature per source word. 3.4 In order to facilitate more complex models like neural network translation models, we rescored the n-best lists. In our experiments we generated 300 best lists for the development and test data respectively. We used the same data to train the rescoring that we have used for optimizing the translation system. We trained the weights for the log-linear combination used during rescoring using the ListNet algorithm (Cao et al., 2007; Niehues et al., 2015). This technique defines a probability distribution on the permutations of the list based on the scores of the log-linear model and one based on a reference metric. In our experiments we used the BLEU+1 score introduced by Liang et al. (2006). Then we use the cross entropy between both distributions as the loss function for our training. Using this loss function, we can compute the gradient and use stochastic gradient descent. We used batch updates with ten samples and tuned the learning rate on the development data. The range of the scores of the different models may greatly differ and many o"
W15-3008,J03-1002,0,0.012327,"l position of the words is included as an additional score in the loglinear model of the translation system. use an SVM classifier for both translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words beyond phrase boundaries. Target words and all their aligned so"
W15-3008,E99-1010,0,0.0262497,"e models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between source and target words beyond phrase boundaries. Target words and all their aligned source words form bilingual tokens on which a LM is trained. The tokens are then ordered according to the target language word order. For the English→German system, we use language models based on fine-grained POS tags (Schmid and Laws, 2008). In addition, we use language models based on word classes learned by clustering the words of the corpus using the MKCLS algorithm (Och, 1999). Using such language models, we can generalize better and therefore alleviate the sparsity problem for surface words. In order to build these language models, we replace each word token of the target language corpus by its corresponding POS tag or cluster ID. The ngram language models are then built on this new corpus consisting of either POS tags or cluster IDs. During decoding, these language models are used as additional models in the log-linear combination. For the German→English system, the data selection language model is trained on data automatically selected using cross-entropy differ"
W15-3008,P03-1021,0,0.0103561,". The probability for the respective orientation with respect to the original position of the words is included as an additional score in the loglinear model of the translation system. use an SVM classifier for both translation tasks as described in Mediani et al. (2011). Language models (LM) are built using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing and scored in the decoding process with KenLM (Heafield, 2011). The in-house phrase-based translation system (Vogel, 2003) is used for generating translations. For optimization, we use minimum error rate training (MERT) (Och, 2003; Venugopal et al., 2005). For German→English, the GIZA++ Toolkit (Och and Ney, 2003) is used to generate the word alignment of the parallel corpora. Discriminative word alignment (DWA), as described in Niehues and Vogel (2008), is used for the English→German direction. We build the phrase tables (PT) using the Moses toolkit (Koehn et al., 2007). 3.1 3.2 Language Models In addition to word-based language models, we use different types of non-word language models for each of the systems. The bilingual language model (Niehues et al., 2011) is designed to increase the bilingual context between so"
W15-3008,W10-1719,1,\N,Missing
W15-3008,E03-1076,0,\N,Missing
W15-3008,P03-1054,0,\N,Missing
W15-3012,W13-0805,1,0.885747,"SVM classifier to remove noisy sentences which are not the actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word p"
W15-3012,P03-1054,0,0.0197594,"he actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et"
W15-3012,N03-1017,0,0.0324773,". 3 2.4 3.1 n-gram Translation Models Continuous Space Translation Models Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014) Language Models The n-gram-based approach in machine translation is a variant of the phrase-based approach (Koehn et al., 2003). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the source sentence has been reordered beforehand as illustrated in Figure 1. Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pair is decomposed into a sequence of L bilingual units called tuples defining a joint segmentation. In this framework, tuples constitute the basic translation units: like phrase pairs, a matching between a source"
W15-3012,2005.iwslt-1.8,0,0.122737,"g, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy classifier for every target words, taking the presence of source words as its features, in order to predict whether the word should appear in the target sentence or not. In KIT system, we use an extended version described in Niehues and Waibel (2013), which utilizes the presence of source ngrams rather than source words. The parallel data of EPPS and NC are used to"
W15-3012,P07-2045,0,0.0106185,"rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy classifier for every target words, taking the presence of source words as its features, in ord"
W15-3012,J04-2004,0,0.0881722,"s Continuous Space Translation Models Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et al., 2012; Devlin et al., 2014) Language Models The n-gram-based approach in machine translation is a variant of the phrase-based approach (Koehn et al., 2003). Introduced in (Casacuberta and Vidal, 2004), and extended in (Mari˜no et al., 2006; Crego and Mari˜no, 2006), this approach is based on a specific factorization of the joint probability of parallel sentence pairs, where the source sentence has been reordered beforehand as illustrated in Figure 1. Let (s, t) denote a sentence pair made of a source s and target t sides. This sentence pair is decomposed into a sequence of L bilingual units called tuples defining a joint segmentation. In this framework, tuples constitute the basic translation units: like phrase pairs, a matching between a source and target chunks. The joint probability of"
W15-3012,N12-1005,1,0.956768,"oint translation system from KIT and LIMSI participating in the Shared Translation Task of the EMNLP 2015 - Tenth Workshop on Statistical Machine Translation (WMT2015). Our system is the combination of two different approaches. First, a strong phrase-based system from KIT is used to generate a k-best list of translated candidates. Second, an n-gram translation model from LIMSI, named SOUL (Structured OUtput Layer), helps to rescore the k-best list by utilizing features extracted from translated tuples. In this year participation, we also use a version of the neural network translation models (Le et al., 2012) trained using NCE algorithm (Gutmann and Hyv¨arinen, 2010) as counterpart to SOUL models. A ListNet2.1 Data and Preprocessing The parallel data mainly used are the corpora extracted from Europarl Parliament (EPPS), News Commentary (NC) and the common part of webcrawled data (Common Crawl). The monolingual data are the monolingual part of those corpora. A preprocessing step is applied to the raw data before the actual training. It includes removing excessively long and length-mismatched sentences pairs. Special symbols and nummeric data are normalized, and smartcasing is applied. Sentence pair"
W15-3012,E99-1010,0,0.0623657,"onolingual data, the KIT system includes several non-word language models. A 4-gram bilingual language model (Niehues et al., 2011) trained on the parallel corpora is used to exploit wider bilingual contexts beyond phrase boundaries. 5-gram Part-of-Speech (POS) language models trained on the POS-tagged parts of all monolingual data incorporate some morphological information into the decision process. They also help to reduce the impact of the data sparsity problem, as cluster language models do. Our 4-gram cluster language model is trained on monolingual EPPS and NC as we use MKCLS algorithm (Och, 1999) to group the words into 1,000 classes and build the language model of the corresponding class IDs instead of the words. All of the language models are trained using the SRILM toolkit (Stolcke, 2002); The word-based 121 org : .... à recevoir le prix nobel de la paix s : .... s8: à s9: recevoir s10: le s11: nobel de la paix s12: prix .... t : .... t8: to t9: receive t10: the t11: nobel peace t12: prize .... u8 u9 u10 u11 u12 Figure 1: Extract of a French-English sentence pair segmented into bilingual units. The original (org) French sentence appears at the top of the figure, just abov"
W15-3012,P06-1096,0,0.312547,"Missing"
W15-3012,2007.tmi-papers.21,0,0.269383,"which contain textual elements in different 120 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 120–125, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. language model scores are estimated by KenLM toolkit (Heafield, 2011) while the non-word language models are estimated by SRILM. languages to some extent, are also taken away. The data is further filtered by using an SVM classifier to remove noisy sentences which are not the actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and Germa"
W15-3012,J06-4004,0,0.194243,"Missing"
W15-3012,D07-1045,0,0.386972,"Missing"
W15-3012,D09-1022,0,0.247402,"Missing"
W15-3012,W05-0836,1,0.875755,"(Vogel, 2003) which finds the best combinations of features in a log-linear framework. The features consist of translation scores, distortion-based and lexicalized reordering scores as well as conventional and non-word language models. In addition, several reordering rules, including short-range, long-range and tree-based reorderings, are applied before decoding step as they are encoded as word lattices. The decoder then generates a list of the best candidates from the lattices. To optimize the factors of individual features on a development dataset, we use minimum error rate training (MERT) (Venugopal et al., 2005). We are going to describe those components in detail as follows. Using these techniques, we were able to improve the BLEU score of the baseline phrase-based system by 1.4 BLEU points. 1 KIT Phrase-based Translation System Introduction In this paper, we present the English→German joint translation system from KIT and LIMSI participating in the Shared Translation Task of the EMNLP 2015 - Tenth Workshop on Statistical Machine Translation (WMT2015). Our system is the combination of two different approaches. First, a strong phrase-based system from KIT is used to generate a k-best list of translat"
W15-3012,W09-0435,1,0.878365,"eedings of the Tenth Workshop on Statistical Machine Translation, pages 120–125, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. language model scores are estimated by KenLM toolkit (Heafield, 2011) while the non-word language models are estimated by SRILM. languages to some extent, are also taken away. The data is further filtered by using an SVM classifier to remove noisy sentences which are not the actual translation from their counterparts. 2.2 2.5 Phrase-table Scores The short-range reordering (Rottmann and Vogel, 2007) and long-range reordering (Niehues and Kolss, 2009) rules are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ to"
W15-3012,W08-0303,1,0.817804,"C. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy class"
W15-3012,W13-2264,1,0.876138,"ose phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice of target words. (Mauser et al., 2009) build a maximum entropy classifier for every target words, taking the presence of source words as its features, in order to predict whether the word should appear in the target sentence or not. In KIT system, we use an extended version described in Niehues and Waibel (2013), which utilizes the presence of source ngrams rather than source words. The parallel data of EPPS and NC are used to train those classifiers. 3 2.4 3.1 n-gram Translation Models Continuous Space Translation Models Neural networks, working on top of conventional n-gram back-off language models (BOLMs), have been introduced in (Bengio et al., 2003; Schwenk, 2007) as a potential means to improve discrete language models. More recently, these techniques have been applied to statistical machine translation in order to estimate continuous-space translation models (CTMs) (Schwenk et al., 2007; Le et"
W15-3012,W11-2124,1,0.841127,"sides. This sentence pair is decomposed into a sequence of L bilingual units called tuples defining a joint segmentation. In this framework, tuples constitute the basic translation units: like phrase pairs, a matching between a source and target chunks. The joint probability of a synchronized and segmented sentence pair can be estimated using the n-gram assumption. During training, the segmentation is obtained as a Besides word-based n-gram language models trained on all preprocessed monolingual data, the KIT system includes several non-word language models. A 4-gram bilingual language model (Niehues et al., 2011) trained on the parallel corpora is used to exploit wider bilingual contexts beyond phrase boundaries. 5-gram Part-of-Speech (POS) language models trained on the POS-tagged parts of all monolingual data incorporate some morphological information into the decision process. They also help to reduce the impact of the data sparsity problem, as cluster language models do. Our 4-gram cluster language model is trained on monolingual EPPS and NC as we use MKCLS algorithm (Och, 1999) to group the words into 1,000 classes and build the language model of the corresponding class IDs instead of the words."
W15-3012,W15-3030,1,0.645912,"(Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012). This technique is readily applicable for CTMs. Therefore, NCE models deliver a positive score, by applying the exponential function to the output layer activities, 122 4 Rescoring 5 After generating translation probabilities using the neural network translation models, we need to combine them with the baseline scores of the phrase-based system in order to select better translations from the k-best lists. As it is done in the baseline decoder, we used a log-linear combination of all features. We trained the model using the ListNet algorithm (Niehues et al., 2015; Cao et al., 2007). This technique defines a probability distribution on the permutations of the list based on the scores of the log-linear model and one based on a reference metric. Therefore, a sentence-based translation quality metric is necessary. In our experiments we used the BLEU+1 score introduced by Liang et al. (2006). Then the model was trained by minimizing the cross entropy between both distributions on the development data. Using this loss function, we can compute the gradient with respect to the weight ωk as follows: System Baseline + ListNet rescoring + NCE + SOUL + NCE + SOUL"
W15-3012,J03-1002,0,0.0188001,"are extracted from POS-tagged versions of parallel EPPS and NC. The POS tags of those corpora are produced using the TreeTagger (Schmid, 1994). The learnt rules are used to reorder source sentences based on the POS sequences of their target sentences and to build reordering lattices for the translation model. Additionally, a tree-based reordering model (Herrmann et al., 2013) trained on syntactic parse trees (Klein and Manning, 2003) is applied to the source side to better address the differences in word order between English and German. We obtain the word alignments using the GIZA++ toolkit (Och and Ney, 2003) and Discriminative Word Alignment method (Niehues and Vogel, 2008) from the parallel EPPS, NC and Common Crawl. Then the Moses toolkit (Koehn et al., 2007) is used to build the phrase tables. Translation scores, which are used as features in our loglinear framework, are derived from those phrase tables. Additional scores, e.g. distortion information, word penalties and lexicalized reordering probabilities (Koehn et al., 2005), are also extracted from the phrase tables. 2.3 Prereorderings Discriminative Word Lexicon The presence of words in the source sentence can be used to guide the choice o"
W15-3012,W11-2123,0,\N,Missing
W15-3012,P14-1129,0,\N,Missing
W15-3030,N12-1047,0,0.0900077,"es not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the machine learning community (Chen et al., 2009). These methods can be grouped into pointwise, pairwise and listwise algorithms. The PRO algorithm is motivated by a pairwise technique, while the work presented in this paper is based on the listwise algorithm ListNet presented in (Cao et al., 2007). Other methods based on more complex models have also been presented, for example (Liu et al., 2013), which uses an additive neural network instead of linear models. 3 exp(sj ) Ps (j) = Pn , k=1 exp(sk ) where sj is a score assigned to the j-th entry of ("
W15-3030,D08-1024,0,0.0298173,"fω (xn(i) )} and the reference scores y (i) , a listwise loss function must be defined to learn the function fω . Since the number of permutations is n! hence prohibitive, Cao et al. (2007) suggests to replace the probability distribution over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the machine learning community ("
W15-3030,P12-1031,0,0.0157894,"over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the machine learning community (Chen et al., 2009). These methods can be grouped into pointwise, pairwise and listwise algorithms. The PRO algorithm is motivated by a pairwise technique, while the work presented in this paper is based on the listwise algorithm ListNet"
W15-3030,D11-1125,0,0.115139,"number of permutations is n! hence prohibitive, Cao et al. (2007) suggests to replace the probability distribution over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the machine learning community (Chen et al., 2009). These methods can be grouped into pointwise, pairwise and listwise algorithms. The PRO algorithm is motiva"
W15-3030,P07-2045,0,0.00929575,"n defines four different scores to evaluate a hypothesis. In such architecture, the size of the output vocabulary is a bottleneck when normalized distributions are needed. For efficient computation, these models rely on a tree-structured output layer called SOUL (Le et al., 2011). An effective alternative, which however only delivers unnormalized scores, is to train the network using the Noise 5.2 Other optimization techniques For comparison, experimental results include performance obtained with the most widely used algorithms: MERT, KB-MIRA (Cherry and Foster, 2012) as implemented in Moses (Koehn et al., 2007), along with the PRO algorithm. For the latter, we used the MegaM1 version (Daum´e III, 2004). All the results correspond to three random restarts and the weights are chosen according to the best performance on the development data. 5.3 WMT – English to German The results for the English to German news translation task are summarized in Table 1. The translations generated by the phrase-based decoder reach a BLEU score of 20.19. We compared the presented approach with MERT, KB-MIRA and PRO. KB-MIRA and MERT improve the performance by at most 0.3 BLEU points. In contrast, the PRO technique and t"
W15-3030,2014.iwslt-evaluation.1,1,0.736667,"ques. A detailed description of all models used in this system can be found in (Slawik et al., 2014). Overall, the baseline system uses 23 different features. The system is tuned on test2011 and test2012 was used to evaluate the different approaches. In the additional experiments, n-best lists generated for dev2010 and test2010 are used as additional training data for the rescoring. translation task of WMT 2015 for the German– English language pair in both directions. The second is the task of translating English TED lectures into German using the data from the IWSLT 2015 evaluation campaign (Cettolo et al., 2014). The systems using the ListNet-based rescoring were submitted to this evaluation campaigns and when evaluated using the BLEU score they were all ranking within the top 3. Before discussing the results, we summarize the translation systems used for experiments along with the additionnal features that rely on continuous space translation models. 5.1 Systems The baseline system is an in-house implementation of the phrase-based approach. The system used to generate n-best lists for the news tasks is trained on all the available training corpora of the WMT 2015 Shared Translation task. The system"
W15-3030,P06-1096,0,0.397769,"Missing"
W15-3030,P13-1078,0,0.0290246,"Missing"
W15-3030,2012.iwslt-papers.3,1,0.83718,"e of possible scores. 250 Contrastive Estimation (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012) denoted by NCE in the rest of the paper. In this work, we used these both solutions as well as their combination. For the German to English translation task, we added a source side discriminative word lexicon (Herrmann, 2015). This model used a multi-class maximum entropy classifier for every source word to predict the translation given the context of the word. In addition, we used a neural network translation model using the technique of RBM (Restricted Boltzman Machine)-based language models (Niehues and Waibel, 2012). The baseline system for the TED translation task uses the IWSLT 2015 training data. The system was adapted to the domain by using language model and translation model adaptation techniques. A detailed description of all models used in this system can be found in (Slawik et al., 2014). Overall, the baseline system uses 23 different features. The system is tuned on test2011 and test2012 was used to evaluate the different approaches. In the additional experiments, n-best lists generated for dev2010 and test2010 are used as additional training data for the rescoring. translation task of WMT 2015"
W15-3030,P03-1021,0,0.0402058,"tion for Computational Linguistics. The aim is then to find a function fω that assigns a (i) score to every feature vector xj . This function is fully defined by its set of parameters ω. Using the (i) (i) vector of scores z (i) = {fω (x1 ) . . . , fω (xn(i) )} and the reference scores y (i) , a listwise loss function must be defined to learn the function fω . Since the number of permutations is n! hence prohibitive, Cao et al. (2007) suggests to replace the probability distribution over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and th"
W15-3030,P02-1040,0,0.0926887,"entional scores calculated during decoding, as well as additional models such as neural network translation models. 4.1 Although both methods could be applied together, we did only use one of them, since both methods have similar effects. If not stated differently, we use the feature normalization method in our experiments. 4.2 To estimate the weights, we need to define a probability distribution Py associated to the reference ranking y following Euqation 1. In this work, we propose a distribution based on machine translation evaluation metrics. The most widely used evaluation metric is BLEU (Papineni et al., 2002), which only produces a score at the corpus level. As proposed by Hopkins and May (2011), we will use a smoothed sentence-wise BLEU score to generate the reference ranking. In this work, we use the BLEU+1 score introduced by Liang et al. (2006). When (i) using sj = BLEU(xj ) in Equation 1, whe get the follwing defintion of the probability distribution Py : Score normalization (i) The scores (xj )k are, for example, language model log-probabilities. Since the language model probabilities are calculated as the product of several n-gram probabilities, these values are typically very small. Theref"
W15-3030,W11-2119,0,0.0487127,"Missing"
W15-3030,2014.iwslt-evaluation.17,1,0.77366,"tive word lexicon (Herrmann, 2015). This model used a multi-class maximum entropy classifier for every source word to predict the translation given the context of the word. In addition, we used a neural network translation model using the technique of RBM (Restricted Boltzman Machine)-based language models (Niehues and Waibel, 2012). The baseline system for the TED translation task uses the IWSLT 2015 training data. The system was adapted to the domain by using language model and translation model adaptation techniques. A detailed description of all models used in this system can be found in (Slawik et al., 2014). Overall, the baseline system uses 23 different features. The system is tuned on test2011 and test2012 was used to evaluate the different approaches. In the additional experiments, n-best lists generated for dev2010 and test2010 are used as additional training data for the rescoring. translation task of WMT 2015 for the German– English language pair in both directions. The second is the task of translating English TED lectures into German using the data from the IWSLT 2015 evaluation campaign (Cettolo et al., 2014). The systems using the ListNet-based rescoring were submitted to this evaluati"
W15-3030,D07-1080,0,0.0294196,"z (i) = {fω (x1 ) . . . , fω (xn(i) )} and the reference scores y (i) , a listwise loss function must be defined to learn the function fω . Since the number of permutations is n! hence prohibitive, Cao et al. (2007) suggests to replace the probability distribution over all the permutations by the probability that an object is ranked first. This can be defined as: ing (MERT) (Och, 2003). Although new methods have been presented, this is still the standard method in many machine translation systems. One problem of this technique is that it does not scale well with many features. More recently, Watanabe et al. (2007) and Chiang et al. (2008) presented a learning algorithm using the MIRA technique. A different technique, PRO, was presented in (Hopkins and May, 2011). Additionally, several techniques to maximize the expected BLEU score (Rosti et al., 2011; He and Deng, 2012) have been proposed. The ListNet algorithm, in contrast, minimizes the difference between the model and the reference ranking. All techniques have the advantage that they can scale well to many features and an intensive comparison of these methods is reported in (Cherry and Foster, 2012). The problem of ranking is well studied in the mac"
W15-3030,2010.iwslt-evaluation.11,1,\N,Missing
W15-3030,N12-1005,1,\N,Missing
W15-3030,W15-3008,1,\N,Missing
W15-4657,D08-1027,0,0.313666,"Missing"
W15-4917,2012.eamt-1.60,0,0.012644,"d used for translation. This allows us to retain our generalization won by using word clusters to estimate phrase probabilities, and still use all models trained on the surExperiments Since we expect stemming to have a larger impact in cases where training data is scarce, we evaluated the three presented strategies on two different scenarios: a low-resource condition and a state-ofthe-art large-scale system. In both scenarios we stemmed German adjectives and translated from German to English. In our low-resource condition, we trained an SMT system using only training data from the TED corpus (Cettolo et al., 2012). TED translations are currently available for 107 languages2 and are being continuously expanded. Therefore, there is a high chance that a small parallel corpus of translated TED talks will be available in the chosen language. In the second scenario, we used a large-scale state-of-the-art German→English translation system. This system was trained on signiﬁcantly more data than available in the low-resource condition and incorporates several additional models. 5.1 System Description The low-resource system was trained only on the TED corpus provided by the IWSLT 2014 machine translation campai"
W15-4917,P08-1115,0,0.0298227,"ut-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simpliﬁcation for their French→English WMT system, including replacing inﬂected adjective forms with their lemma using hand-written rules, and their Russian→English (Weller et al., 2013b) system, removing superﬂuous attributes from the highly inﬂected Russian surface forms. Their systems are unable to outperform the baseline system trained on the surface forms. Weller et al. argue that human trans"
W15-4917,W08-0509,0,0.028417,"Missing"
W15-4917,W10-1710,0,0.0162401,"nd Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simpliﬁcation for their French→English WMT system, including replacing inﬂected adjective forms with their lemma using hand-written rules, and their Russian→English (Welle"
W15-4917,W11-2123,0,0.0843226,"com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-ﬁnal-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heaﬁeld, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced"
W15-4917,W13-0805,1,0.866971,"is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heaﬁeld, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is"
W15-4917,D07-1091,0,0.0514795,"previously unseen morphological variants of a word, thus leading to a better generalization of our models. To fully maximize the potential of our SMT system, we looked at three different integration strategies. We evaluated hard decision stemming, where all adjectives are replaced by their stem, as well as soft integration strategies, where we consider the words and their stemmed form as translation alternatives. 2 Related Work The speciﬁc challenges arising from the translation of morphologically rich languages have been widely studied in the ﬁeld of SMT. The factored 129 translation model (Koehn and Hoang, 2007) enriches phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen wor"
W15-4917,E03-1076,0,0.239742,"plitting before tagging and stemming. We only stemmed words tagged as attributive adjectives, since only they are inﬂected in German. Predicative adjectives are not inﬂected and therefore were left untouched. Since we want to retain the degree of comparison, we used the ﬁnegrained tags of the RFTagger to decide when and how to stem. Adjectives tagged as comparative or superlative were stemmed through the use of ﬁxed rules. For all others, we used the lemma output by the TreeTagger, since it is the same as the stem and was already available in our system. Finally, our usual compound splitting (Koehn and Knight, 2003) was trained and performed on the stemmed corpus. 4 Integration After clustering the words into groups that can be translated in the same or at least in a similar way, there are different possibilities to use them in the translation system. A naive strategy is to replace each word by its cluster representative, called hard decision stemming. However, this carries the risk of discarding vital information. Therefore we investigated techniques to integrate both, the surface forms as well as the word stems, into the translation system. In the combined input, we add the stemmed adjectives as transl"
W15-4917,2005.iwslt-1.8,0,0.0605771,"additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is built on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT test 2012 as a development set and IWSLT test System Baseline Hard Decision Combined Input Hidden Combination Dev Test 28.91 29.01 29.13 29.25 30.25 30.30 30.47 30.62 Table 1: TED low-resource small systems results. 2011 as test data. For the large-scale system, we used IW"
W15-4917,P07-2045,0,0.00534336,"lines. For the monolingual training data we used the target side of all bilingual corpora as well as the News Shufﬂe and the Gigaword corpus. Before training and translation, the data is preprocessed as described in Section 3.2. The noisy Common Crawl corpus was ﬁltered with an SVM classiﬁer as described by Mediani et al. (2011). After preprocessing, the parallel corpora are wordaligned with the GIZA++ toolkit (Gao and Vo2 132 http://www.ted.com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-ﬁnal-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heaﬁeld, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale syst"
W15-4917,P11-1140,0,0.0200421,"ibutions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simpliﬁcation for their French→English WMT system, including replacing inﬂected adjective forms with their lemma using hand-written rules, and the"
W15-4917,2011.iwslt-evaluation.9,1,0.901491,"monolingual training data we used the target side of the TED corpus. The large-scale system was trained on the European Parliament Proceedings, News Commentary, TED and Common Crawl corpora provided for the IWSLT 2014 machine translation campaign (Cettolo et al., 2014), encompassing 4.69M lines. For the monolingual training data we used the target side of all bilingual corpora as well as the News Shufﬂe and the Gigaword corpus. Before training and translation, the data is preprocessed as described in Section 3.2. The noisy Common Crawl corpus was ﬁltered with an SVM classiﬁer as described by Mediani et al. (2011). After preprocessing, the parallel corpora are wordaligned with the GIZA++ toolkit (Gao and Vo2 132 http://www.ted.com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-ﬁnal-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed"
W15-4917,P10-2041,0,0.0286579,"large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is built on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT test 2012 as a development set and IWSLT test System Baseline Hard Decision Combined Input Hidden Combination Dev Test 28.91 29.01 29.13 29.25 30.25 30.30 30.47 30.62 Table 1: TED"
W15-4917,W09-0435,1,0.865559,"t-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heaﬁeld, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond p"
W15-4917,2011.iwslt-papers.6,1,0.861961,"phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an al"
W15-4917,2012.amta-papers.19,1,0.791385,"ion, the data is preprocessed as described in Section 3.2. The noisy Common Crawl corpus was ﬁltered with an SVM classiﬁer as described by Mediani et al. (2011). After preprocessing, the parallel corpora are wordaligned with the GIZA++ toolkit (Gao and Vo2 132 http://www.ted.com/participate/translate gel, 2008) in both directions. The resulting alignments are combined using the grow-diag-ﬁnal-and heuristic. The Moses toolkit (Koehn et al., 2007) is used for phrase extraction. For the large-scale system, phrase table adaptation combining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heaﬁeld, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting r"
W15-4917,W11-2124,1,0.93561,"idden combination strategy, stemming can easily be implemented into current state-of-the-art SMT systems without the need to change any of the advanced models beyond the phrase table. This makes our approach highly versatile and easy to implement for any number of system architectures and languages. 5 Figure 1: Workﬂow for unstemming the PT. 4.3 Hidden Combination While we are able to modify our phrase table to use both surface forms and stems in the last strategy, other models in our log-linear system suffer from the different types of source input. For example, the bilingual language model (Niehues et al., 2011) is based on tokens of target words and their aligned source words. In training, we can use either the stemmed corpus or the original one, but during decoding a mixture of stems and surface forms occurs. For the unknown word forms the scores will not be accurate and the performance of our model will suffer. Similar problems occur when using other translation models such as neural network based translation models. We therefore developed a novel strategy to integrate the word stems into the translation system. Instead of stemming the input to ﬁt the stemmed phrase table, we modiﬁed the stemmed p"
W15-4917,E99-1010,0,0.128458,"le and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context during translation beyond phrase boundaries. It is built on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT te"
W15-4917,2007.tmi-papers.21,0,0.467512,"mbining an indomain and out-of-domain phrase table is performed (Niehues and Waibel, 2012). All translations are generated by our in-house phrase-based decoder (Vogel, 2003). We used 4-gram language models (LMs) with modiﬁed Kneser-Ney smoothing, trained with the SRILM toolkit (Stolcke, 2002) and scored in the decoding process with KenLM (Heaﬁeld, 2011). All our systems include a reordering model which automatically learns reordering rules based on part-of-speech sequences and, in case of the large-scale system, syntactic parse tree constituents to better match the target language word order (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013). The resulting reordering possibilities for each source sentence are encoded in a lattice. For the low-resource scenario, we built two systems. One small baseline with only one phrase table and language model, as well as aforementioned POS-based preordering model, and an advanced system using an extended feature set of models that are also used in the large-scale system. The extended low-resource and the large-scale system include the following additional models. A bilingual LM (Niehues et al., 2011) is used to increase the bilingual context du"
W15-4917,C08-1098,0,0.163073,"m that does not exist in proper German. However, were we to apply the same stemming to the comparative case, we would lose the degree of comparison and still generate a valid German sentence (sch¨on wird es nicht [won’t be pretty]) with a different meaning than our original sentence. In order to differentiate between cases in which stemming is desirable and where we would lose information, a detailed morphological analysis of the source text prior to stemming is vital. 3.2 Implementation We used readily available part-of-speech (POS) taggers, namely the TreeTagger (Schmid, 1994) and RFTagger (Schmid and Laws, 2008), for morphological analysis and stemming. In order to achieve accurate results, we performed standard machine translation preprocessing on our corpora before tagging. We discarded exceedingly long sentences and sentence pairs with a large length difference from the training data. Special dates, numbers and symbols were normalized and we smart-cased the ﬁrst letter of every sentence. Typically preprocessing for German also includes splitting up compounds into their separate parts. However, this would confuse the POS taggers, which have been trained on German text with proper compounds. Further"
W15-4917,P06-1122,0,0.0149241,"where we consider the words and their stemmed form as translation alternatives. 2 Related Work The speciﬁc challenges arising from the translation of morphologically rich languages have been widely studied in the ﬁeld of SMT. The factored 129 translation model (Koehn and Hoang, 2007) enriches phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the"
W15-4917,W05-0836,1,0.904528,"lt on tokens consisting of a target word and all its aligned source words. We also used a 9-gram cluster LM built on 100 automatically clustered word classes using the MKCLS algorithm (Och, 1999). The large-scale system also uses an in-domain LM trained on the TED corpus and a word-based model trained on 10M sentences chosen through data selection (Moore and Lewis, 2010). In addition to the lattice preordering, a lexicalized reordering model (Koehn et al., 2005) which stores reordering probabilities for each phrase pair is included in both extended systems. We tune all our systems using MERT (Venugopal et al., 2005) against the BLEU score. Since the systems have a varying amount of features, we reoptimized the weights for every experiment. For the low-resource system, we used IWSLT test 2012 as a development set and IWSLT test System Baseline Hard Decision Combined Input Hidden Combination Dev Test 28.91 29.01 29.13 29.25 30.25 30.30 30.47 30.62 Table 1: TED low-resource small systems results. 2011 as test data. For the large-scale system, we used IWSLT test 2011 as development data and IWSLT test 2012 as test data. All results are reported as case-sensitive BLEU scores calculated with one reference tran"
W15-4917,P13-1058,0,0.0157141,"n of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simpliﬁcation for their French→English WMT system, including replacing inﬂected adjective forms with their lemma using hand-written rules, and their Russian→English (Weller et al., 2013b) system, removing superﬂuous attributes from the highly inﬂected Russian surface forms. Their systems are unable to outperform the baseline system trained on the surface forms. Weller et al. argue that human translators may prefer the morphologically reduced system due to better generalization ability. Their analysis showed the Russian system often produces an incorrect verb tense, whic"
W15-4917,W12-3157,0,0.0169282,"words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2010) use morphological reduction in a German→English SMT system by adding the lemmas of every word output as a byproduct of compound splitting as an alternative edge to input lattices. A similar approach is used by Dyer et al. (2008) and Wuebker and Ney (2012). They used word lattices to represent different source language alternatives for Arabic→English and German→English respectively. Weller et al. (2013a) employ morphological simpliﬁcation for their French→English WMT system, including replacing inﬂected adjective forms with their lemma using hand-written rules, and their Russian→English (Weller et al., 2013b) system, removing superﬂuous attributes from the highly inﬂected Russian surface forms. Their systems are unable to outperform the baseline system trained on the surface forms. Weller et al. argue that human translators may prefer the morph"
W15-4917,E06-1006,0,0.036887,"tion of morphologically rich languages have been widely studied in the ﬁeld of SMT. The factored 129 translation model (Koehn and Hoang, 2007) enriches phrase-based MT with linguistic information. By translating the stem of a word and its morphological components separately and then applying generation rules to form the correct surface form of the target word, it is possible to generate translations for surface forms that have not been seen in training. Talbot and Osborne (2006) address lexical redundancy by automatically clustering source words with similar translation distributions, whereas Yang and Kirchhoff (2006) propose a backoff model that uses increasing levels of morphological abstractions to translate previously unseen word forms. Niehues and Waibel (2011) present quasimorphological operations as a means to translate out-of-vocabulary (OOV) words. The automatically learned operations are able to split off potentially inﬂected sufﬁxes, look up the translation for the base form using a lexicon of Wikipedia1 titles in multiple languages, and then generate the appropriate surface form on the target side. Similar operations were learned for compound parts by Macherey et al. (2011). Hardmeier et al. (2"
W15-4917,W13-2213,0,\N,Missing
W16-2208,2014.iwslt-papers.10,1,0.871256,"Missing"
W16-2208,W16-2314,1,0.843692,"Missing"
W16-2208,W09-0435,1,0.856202,"anguage models based on each of these factors. First, we performed a detailed analysis on the English-Romanian task. In addition, we used the model in a German-English and EnglishGerman translation system. In all tasks, we used the model in re-scoring of a PBMT system. Figure 2: Bilingual Model 5.1 The baseline system is an in-house implementation of the phrase-based approach. The system used to generate n-best lists for the news tasks is trained on all the available training corpora of the WMT 2015 Shared Translation task. The system uses a pre-reordering technique (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013) and facilitates several translation and language models. As shown in Table 1, we use two to three word-based language models and one to two cluster-based models using 50, 100 or 1,000 clusters. The custers were trained as described in (Och, 1999). In addition, we used a POSbased language model in the English-Romainian system and a bilingual language model (Niehues et al., 2011) in English to German and German to English systems. The POS tags for EnglishRomanian were generated by the tagger described in (Ion et al., 2012) and the ones for German by RFTagger (Schmid and"
W16-2208,2015.iwslt-papers.3,1,0.744609,"ures , we now can predict the target word given the previous target word and the aligned source word. In the example in Figure 2, we would insert (completed,VVD,87,ein,ART) to predict (a,DT,37). In this case the number of input factors and output factors are no longer the same. In the input, we have D + Ds input factors, while we have only D factors on the output of the network. 5 System Description EN-RO 2 1 2 0 22-23 EN-DE 3 0 1 1 20 DE-EN 3 0 2 1 22 In addition, we used discriminative word lexica (Niehues and Waibel, 2013) during decoding and source discriminative word lexica in rescoring (Herrman et al., 2015). A full system description can be found in (Ha et al., 2016). The German to English baseline system uses 20 features and the English to German systems uses 22 features. The English-Romanian system was optimized on the first part of news-dev2016 and the rescoring was optimized on this set and a subset of 2,000 Experiments We evaluated the factored RNNLM on three different language pairs of the WMT 2016 News Translation Task. In each language pair, we created an n-best list using our phrase-based MT system and used the factored RNNLM as an additional feature in rescoring. It is worth noting tha"
W16-2208,2012.iwslt-papers.3,1,0.916551,"gically rich languages which often have a large vocabulary size. Language models based on these factors are able to consider longer context and therefore improve the modelling of the overall structure. Furthermore, the POS information can be used to improve the modelling of word agreement, which is often a difficult task when handling morphologically rich languages. Until now, word factors have been used relatively limited in neural network models. Automatic word classes have been used to structure the output layer (Le et al., 2011) and as input in feed forward neural network language models (Niehues and Waibel, 2012). In this work, we propose a multi-factor recurrent neural network (RNN)-based language model that is able to facilitate all available information about the word in the input as well as in the output. We evaluated the technique using the surface form, POS-tag and automatic word clusters using different cluster sizes. Using this model, it is also possible to integrate source side information into the model. By using the model as a bilingual model, the probability of the translation can be modelled and not only the one of target sentence. As for the target side, we use a factored representation"
W16-2208,W13-0805,1,0.862292,"ach of these factors. First, we performed a detailed analysis on the English-Romanian task. In addition, we used the model in a German-English and EnglishGerman translation system. In all tasks, we used the model in re-scoring of a PBMT system. Figure 2: Bilingual Model 5.1 The baseline system is an in-house implementation of the phrase-based approach. The system used to generate n-best lists for the news tasks is trained on all the available training corpora of the WMT 2015 Shared Translation task. The system uses a pre-reordering technique (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013) and facilitates several translation and language models. As shown in Table 1, we use two to three word-based language models and one to two cluster-based models using 50, 100 or 1,000 clusters. The custers were trained as described in (Och, 1999). In addition, we used a POSbased language model in the English-Romainian system and a bilingual language model (Niehues et al., 2011) in English to German and German to English systems. The POS tags for EnglishRomanian were generated by the tagger described in (Ion et al., 2012) and the ones for German by RFTagger (Schmid and Laws, 2008). the model a"
W16-2208,W13-2264,1,0.835485,"(fi+1,1 , . . . , fi+1,D , fa(i+1),1 , . . . , fa(i+1),D ) s wordLM POSLM clusterLM BiLM #features , we now can predict the target word given the previous target word and the aligned source word. In the example in Figure 2, we would insert (completed,VVD,87,ein,ART) to predict (a,DT,37). In this case the number of input factors and output factors are no longer the same. In the input, we have D + Ds input factors, while we have only D factors on the output of the network. 5 System Description EN-RO 2 1 2 0 22-23 EN-DE 3 0 1 1 20 DE-EN 3 0 2 1 22 In addition, we used discriminative word lexica (Niehues and Waibel, 2013) during decoding and source discriminative word lexica in rescoring (Herrman et al., 2015). A full system description can be found in (Ha et al., 2016). The German to English baseline system uses 20 features and the English to German systems uses 22 features. The English-Romanian system was optimized on the first part of news-dev2016 and the rescoring was optimized on this set and a subset of 2,000 Experiments We evaluated the factored RNNLM on three different language pairs of the WMT 2016 News Translation Task. In each language pair, we created an n-best list using our phrase-based MT system"
W16-2208,W11-2124,1,0.842249,"n-best lists for the news tasks is trained on all the available training corpora of the WMT 2015 Shared Translation task. The system uses a pre-reordering technique (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013) and facilitates several translation and language models. As shown in Table 1, we use two to three word-based language models and one to two cluster-based models using 50, 100 or 1,000 clusters. The custers were trained as described in (Och, 1999). In addition, we used a POSbased language model in the English-Romainian system and a bilingual language model (Niehues et al., 2011) in English to German and German to English systems. The POS tags for EnglishRomanian were generated by the tagger described in (Ion et al., 2012) and the ones for German by RFTagger (Schmid and Laws, 2008). the model as a bilingual model (BM). Instead of using only monolingual information by considering the previous target factors as input, we used source factors additionally. Thereby, we can now model the probability of a word given the previous target words and information about the source sentence. So in this case we model the translation probability and no longer the language model probab"
W16-2208,ion-etal-2012-rombac,0,0.030615,"ering technique (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013) and facilitates several translation and language models. As shown in Table 1, we use two to three word-based language models and one to two cluster-based models using 50, 100 or 1,000 clusters. The custers were trained as described in (Och, 1999). In addition, we used a POSbased language model in the English-Romainian system and a bilingual language model (Niehues et al., 2011) in English to German and German to English systems. The POS tags for EnglishRomanian were generated by the tagger described in (Ion et al., 2012) and the ones for German by RFTagger (Schmid and Laws, 2008). the model as a bilingual model (BM). Instead of using only monolingual information by considering the previous target factors as input, we used source factors additionally. Thereby, we can now model the probability of a word given the previous target words and information about the source sentence. So in this case we model the translation probability and no longer the language model probability. When predicting the target word wi+1 with its factors fi+1,1 , . . . , fi+1,D , the input to the RNN is the previous target word wi = fi,1"
W16-2208,E99-1010,0,0.216665,"ed using stochastic gradient descent. The weights were updated using mini-batches with a batch size of 128. We used a maximum epoch size of 1 million examples and selected the model with the lowest perplexity on the development data. 4 Factored Language Model When using factored representation of words, words are no longer represented as indices in the neural network. Instead, they are represented a tuples of indices w = (f1 , . . . , fD ), where D is the number of different factors used to describe the word. These factors can be the word itself, as well as the POS, automatic learned classes (Och, 1999) or other information about the word. Furthermore, we can use different types of factors for the input and the output of the neural network. 1 75 http://torch.ch/ 4.2 Figure 1: Factored RNN Layout 4.1 Output Representation In addition to use different factors in the input of the neural network, we can also use different factors on the output. In phrase-based machine translation, n-gram language models based on POStags have been shown to be very successful for morphologically rich languages. Porting this idea to neural network language models, we can not only train a model to predict the origin"
W16-2208,D07-1091,0,0.0588701,"ge not only the target morphological information but also word factors from both source and target sides in our models. Furthermore, we could use as many types of word factors as we can provide. Thus, we are able to make the most of the information encoded in those factors for more accurate prediction. section, we will describe the experiments on the WMT 2016 data. Finally, we will end the paper with a conclusion of the work. 2 Related Work Additional information about words, encoded as word factors, e.g. the lemma of word, POS tags, etc., is employed in state-of-the-art phrasebased systems. (Koehn and Hoang, 2007) decomposes the translation of factored representations to smaller mapping steps, which are modelled by translation probabilities from input factor to output factor or by generating probabilities of additional output factors from existing output factors. Then those pre-computed probabilities are jointly combined in the decoding process as a standard translation feature scores. In addition, language models using these word factors have shown to be very helpful to improve the translation quality. In particular, the aligned-words, POS or word classes are used in the framework of modern language m"
W16-2208,P03-1021,0,0.0587758,"nces from the SETimes corpus. This part of the corpus was of course excluded for training the model. The system was tested on the second half of news-dev2016. The English-German and German-English systems were optimized on news-test2014 and also the re-scoring was optimized on this data. We tested the system on news-test2015. For English to Romanian and English to German we used an n-best List of 300 entries and for German to English we used an n-best list with 3,000 entries. For decoding, for all language directions, the weights of the system were optimized using minimum error rate training (Och, 2003). The weights in the rescoring were optimized using the ListNet algorithm (Cao et al., 2007) as described in (Niehues et al., 2015). The RNN-based language models for English to Romanian and German to English were trained on the target side of the parallel training data. For English to German, we trained the model and the Europarl corpus and the News commentary corpus. 5.2 Table 2: English - Romanian Single Score Input Word All factors All factors All factors All factors All factors Prediction Word Word POS 100 Cl. 1,000 Cl. All factors Single 27.88 28.46 28.48 28.23 28.49 28.54 If we predict"
W16-2208,P02-1040,0,0.0974026,"valuating the model as the only knowledge source, we also performed experiments using the model in combination with the other models. We evaluated the baseline and the best model in three different configuration in Table 3 using only the joint probability. The three baseline configuration differ in the models used during decoding. Thereby, we are able to generate different n-best lists and test the models on different conditions. English - Romanian In the first experiment on the English to Romanian task, we only used the scores of the RNN language models. The baseline system has a BLEU score (Papineni et al., 2002) of 29.67. Using only the language model instead of the 22 features, of course, leads to a lower performance, but we can see clear difference between the different language models. All systems use a word vocabulary of 5K words and we used four different factors. We used the word surface form, the POS tags and word clusters using 100 and 1,000 classes. The baseline model using words as input and words as output reaches a BLEU score of 27.88. If we instead represent the input words by factors, we select entries from the n-best list that generates a BLEU score of 28.46. As done with the n-gram la"
W16-2208,N12-1005,0,0.0205512,"013). Recently, neural network language models have been considered to perform better than standard n-gram language models (Schwenk, 2007; Le et al., 2011). Especially the neural language models constructed in recurrent architectures have shown a great performance by allowing them to take a longer context into account (Mikolov et al., 2010; Sundermeyer et al., 2013). In a different direction, there has been a great deal of research on bringing not only target words but also source words into the prediction process, instead of predicting the next target word based on the previous target words (Le et al., 2012; Devlin et al., 2014; Ha et al., 2014). However, to the best of our knowledge, word factors have been exploited in a relatively limited scope of neural network research. (Le et al., 2011; Le et al., 2012) use word classes to reduce the output layer’s complexity of such networks, both in language and translation models. In the work of (Niehues and Waibel, 2012), their Restricted Boltzmann Machines language models also encode word classes as an additional input feature in predicting the next target word. (Tran et al., 2014) use two separate feed forward networks to predict the target word and i"
W16-2208,2007.tmi-papers.21,0,0.0476998,"e system by n-gram-based language models based on each of these factors. First, we performed a detailed analysis on the English-Romanian task. In addition, we used the model in a German-English and EnglishGerman translation system. In all tasks, we used the model in re-scoring of a PBMT system. Figure 2: Bilingual Model 5.1 The baseline system is an in-house implementation of the phrase-based approach. The system used to generate n-best lists for the news tasks is trained on all the available training corpora of the WMT 2015 Shared Translation task. The system uses a pre-reordering technique (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Herrmann et al., 2013) and facilitates several translation and language models. As shown in Table 1, we use two to three word-based language models and one to two cluster-based models using 50, 100 or 1,000 clusters. The custers were trained as described in (Och, 1999). In addition, we used a POSbased language model in the English-Romainian system and a bilingual language model (Niehues et al., 2011) in English to German and German to English systems. The POS tags for EnglishRomanian were generated by the tagger described in (Ion et al., 2012) and the ones for German"
W16-2208,C08-1098,0,0.231817,"Kolss, 2009; Herrmann et al., 2013) and facilitates several translation and language models. As shown in Table 1, we use two to three word-based language models and one to two cluster-based models using 50, 100 or 1,000 clusters. The custers were trained as described in (Och, 1999). In addition, we used a POSbased language model in the English-Romainian system and a bilingual language model (Niehues et al., 2011) in English to German and German to English systems. The POS tags for EnglishRomanian were generated by the tagger described in (Ion et al., 2012) and the ones for German by RFTagger (Schmid and Laws, 2008). the model as a bilingual model (BM). Instead of using only monolingual information by considering the previous target factors as input, we used source factors additionally. Thereby, we can now model the probability of a word given the previous target words and information about the source sentence. So in this case we model the translation probability and no longer the language model probability. When predicting the target word wi+1 with its factors fi+1,1 , . . . , fi+1,D , the input to the RNN is the previous target word wi = fi,1 , . . . , fi,D . Using the alignment, we can find the source"
W16-2208,D14-1175,0,0.0264713,"ead of predicting the next target word based on the previous target words (Le et al., 2012; Devlin et al., 2014; Ha et al., 2014). However, to the best of our knowledge, word factors have been exploited in a relatively limited scope of neural network research. (Le et al., 2011; Le et al., 2012) use word classes to reduce the output layer’s complexity of such networks, both in language and translation models. In the work of (Niehues and Waibel, 2012), their Restricted Boltzmann Machines language models also encode word classes as an additional input feature in predicting the next target word. (Tran et al., 2014) use two separate feed forward networks to predict the target word and its corresponding suffixes with the source words and target stem as input features. Our work exhibits several essential differences 3 Recurrent Neural Network-based Language Models In contrast to feed forward neural network-based language models, recurrent neural network-based language models are able to store arbitrary long word sequences. Thereby, they are able to directly model P (w|h) and no approximations by limiting the history size are necessary. Recently, several authors showed that RNN-based language models could p"
W16-2208,D13-1138,0,0.0166919,"f factored representations to smaller mapping steps, which are modelled by translation probabilities from input factor to output factor or by generating probabilities of additional output factors from existing output factors. Then those pre-computed probabilities are jointly combined in the decoding process as a standard translation feature scores. In addition, language models using these word factors have shown to be very helpful to improve the translation quality. In particular, the aligned-words, POS or word classes are used in the framework of modern language models (Mediani et al., 2011; Wuebker et al., 2013). Recently, neural network language models have been considered to perform better than standard n-gram language models (Schwenk, 2007; Le et al., 2011). Especially the neural language models constructed in recurrent architectures have shown a great performance by allowing them to take a longer context into account (Mikolov et al., 2010; Sundermeyer et al., 2013). In a different direction, there has been a great deal of research on bringing not only target words but also source words into the prediction process, instead of predicting the next target word based on the previous target words (Le e"
W16-2208,N03-2002,0,\N,Missing
W16-2208,W15-3030,1,\N,Missing
W16-2208,P14-1129,0,\N,Missing
W16-2320,W16-2304,1,0.833347,"factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. T"
W16-2320,W05-0909,0,0.583183,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W16-2320,P13-2071,1,0.873371,"odel trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. The first part was used as development set while t"
W16-2320,D15-1129,1,0.847985,"training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based loc"
W16-2320,N13-1073,0,0.034805,"preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ ques"
W16-2320,J04-2004,0,0.0194677,"en systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probabilit"
W16-2320,2011.mtsummit-papers.30,0,0.020392,"-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 201"
W16-2320,N12-1047,0,0.591808,"rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard p"
W16-2320,E14-2008,1,0.72015,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,P05-1033,0,0.151933,"ative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combin"
W16-2320,J07-2003,0,0.558191,"this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the bou"
W16-2320,W14-3310,1,0.909876,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D14-1179,0,0.0138582,"Missing"
W16-2320,2014.iwslt-evaluation.7,1,0.925781,"tems are combined using RWTH’s system combination approach. The final submission shows an improvement of 1.0 B LEU compared to the best single system on newstest2016. 1 Introduction Quality Translation 21 (QT21) is a European machine translation research project with the aim 1 http://www.statmt.org/wmt16/ translation-task.html 344 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 344–355, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics mented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Freitag et al., 2013; Freitag et al., 2014b; Freitag et al., 2014c). In the remainder of the paper, we present the technical details of the QT21/HimL combined machine translation system and the experimental results obtained with it. The paper is structured as follows: We describe the common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual en"
W16-2320,D08-1089,0,0.0211464,", built using KenLM. The first is a 5-gram language model trained on all available data. Words in the Common Crawl dataset that appear fewer than 500 times were replaced by UNK, and all singleton ngrams of order 3 or higher were pruned. We also use a 7-gram class-based language model, trained on the same data. 512 word Edinburgh Phrase-based System Edinburgh’s phrase-based system is built using the Moses toolkit, with fast align (Dyer et al., 2013) for word alignment, and KenLM (Heafield et al., 2013) for language model training. In our Moses setup, we use hierarchical lexicalized reordering (Galley and Manning, 2008), operation sequence model (Durrani et al., 2013), domain indicator features, and binned phrase count features. We use all available parallel data for the translation model, and all available Romanian text for the language model. We use two different 5-gram language models; one built from all the monolingual target text concatenated, without pruning, and one 3 USFD Phrase-based System 4 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 https://github.com/rsennrich/nematus 348 the large building the large home a big huge house house a newsdev2016/1 and newsdev2016/2. T"
W16-2320,N15-1105,0,0.046766,"Missing"
W16-2320,W08-0509,0,0.06624,"probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs. We validate the model every 10 000 minibatches via B LEU on a validation set, and perform early stopping on B LEU. Decoding is performed with beam search with a beam size of 12. A more detailed description of the system, and more experimental results, can be found in (Sennrich et al., 2016a). 3.10 3.11 USFD’s phrase-based system is built using the Moses toolkit, with MGIZA (Gao and Vogel, 2008) for word alignment and KenLM (Heafield et al., 2013) for language model training. We use all available parallel data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the featur"
W16-2320,W16-2315,1,0.820309,"vs et al., 2012) that features language-specific data filtering and cleaning modules. Tilde’s system was trained on all available parallel data. Two language models are trained using KenLM (Heafield, 2011): 1) a 5-gram model using the Europarl and SETimes2 corpora, and 2) a 3-gram model using the Common Crawl corpus. We also apply a custom tokenization tool that takes into account specifics of the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural Sy"
W16-2320,D07-1103,0,0.032902,"bbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule application with syntactic context information as feature templates. The features are the same as used by Braune et al. (2015) in their string-to-tree system, including both lexical and soft source syntax features. The translation model features comprise the standard hierarchical features (Chiang, 2005) with an additional feature for the rule selection model (Braune et al., 2016). Before training, we reduce the number of translation rules using significance testing (Johnson et al., 2007). To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT ("
W16-2320,W14-3360,0,0.0191919,"NMT system on newsdev2016/2, but lags behind on newstest2016. Removing the by itself weakest system shows a slight degradation on newsdev2016/2 and newstest2016, hinting that it still provides valuable information. Table 2 shows a comparison between all systems by scoring the translation output against each other in T ER and B LEU. We see that the neural networks outputs differ the most from all the other systems. Figure 1: System A: the large building; System B: the large home; System C: a big house; System D: a huge house; Reference: the big house. classes were generated using the method of Green et al. (2014). 4 System Combination System combination produces consensus translations from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alig"
W16-2320,P07-2045,1,0.010375,", 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn et al., 2007). On the Romanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based"
W16-2320,P13-2121,0,0.0645203,"Missing"
W16-2320,W11-2123,0,0.124165,"nslation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phras"
W16-2320,N04-1022,0,0.037676,"f the Romanian language and handles non-translatable entities (e.g., file paths, 347 up with entries from a background phrase table extracted from the automatically produced News Crawl 2015 parallel data. Huck et al. (2016) give a more in-depth description of the Edinburgh/LMU hierarchical machine translation system, along with detailed experimental results. 3.9 built from only News Crawl 2015, with singleton 3-grams and above pruned out. The weights of all these features and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpu"
W16-2320,2015.iwslt-papers.3,1,0.744353,"g. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additional word factor. 2 3.2 Preprocessing The data provided for the task was preprocessed once, by LIMSI, and shared with all the participants, in order to ensure consistency between systems. On the English side, preprocessing consists of tokenizing and truecasing using the Moses toolkit (Koehn e"
W16-2320,J06-4004,0,0.106202,"Missing"
W16-2320,2009.iwslt-papers.4,0,0.0984189,"target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SETimes2) directly into the log-linear combination of the system and let MIRA (Cherry and Foster, 2012) optimize their weights along with all other features in tuning, rather than relying on a single linearly interpolated language model. We add another background language model estimated over a concatenation of all Ro"
W16-2320,P10-2041,0,0.0238386,"ontaining the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all singletons with KenLM (Heafield, 2011). We use the in-domain monolingual corpus, the Romanian side of the parallel corpora and a subset of the (out-of-domain) Common Crawl corpus as training data. We select indomain sentences from the latter using the MooreLewis (Moore and Lewis, 2010) filtering method, Translation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using"
W16-2320,P07-1019,0,0.236745,"grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language"
W16-2320,2012.amta-papers.19,1,0.778005,"common preprocessing used for most of the individual engines in Section 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set usin"
W16-2320,W11-2211,1,0.902733,"ty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system provided by the RWTH is an attention-based recurrent neural network similar to (Bahdanau et al., 2015). The implementation is based on Blocks (van Merri¨enboer et al., 2015) and Theano (Bergstra et al., 20"
W16-2320,W13-2264,1,0.852517,"stic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and the target. On the source side we use the word surface form and two automatic word classes using 100 and 1,000 classes. On the Romanian side, we add the POS information as an additio"
W16-2320,W13-2258,1,0.869431,"extraction, we impose less strict extraction constraints than the Moses defaults. We extract more hierarchical rules by allowing for a maximum of ten symbols on the source side, a maximum span of twenty words, and no lower limit to the amount of words covered by right-hand side non-terminals at extraction time. We discard rules with non-terminals on their right-hand side if they are singletons in the training data. In order to promote better reordering decisions, we implemented a feature in Moses that resembles the phrase orientation model for hierarchical machine translation as described by Huck et al. (2013) and extend our system with it. The model scores orientation classes (monotone, swap, discontinuous) for each rule application in decoding. We finally follow the approach outlined by Huck et al. (2011) for lightly-supervised training of hierarchical systems. We automatically translate parts (1.2M sentences) of the monolingual Romanian News Crawl 2015 corpus to English with a Romanian→English phrase-based statistical machine translation system (Williams et al., 2016). The foreground phrase table extracted from the human-generated parallel data is filled RWTH Neural System The second system prov"
W16-2320,E99-1010,0,0.040797,"ion 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add the source discriminative word lexica (Herrmann et al., 2015) as well as neural network language and translation models. These models use a factored word representation of the source and th"
W16-2320,P03-1021,0,0.501814,". To extract the features of the rule selection model, we parse the English part of our 2 http://hunch.net/˜vw/ (VW). Implemented by John Langford and many others. 346 URLs, e-mail addresses, etc.). During translation a rule-based localisation feature is applied. ward recurrent state encoding the source and target history, a backward recurrent state encoding the source future, and a third LSTM layer to combine them. All layers have 350 nodes. The neural networks are implemented using an extension of the RWTHLM toolkit (Sundermeyer et al., 2014b). The parameter weights are optimized with MERT (Och, 2003) towards the B LEU metric. 3.6 3.8 The UEDIN-LMU HPBT system is a hierarchical phrase-based machine translation system (Chiang, 2005) built jointly by the University of Edinburgh and LMU Munich. The system is based on the open source Moses implementation of the hierarchical phrase-based paradigm (Hoang et al., 2009). In addition to a set of standard features in a log-linear combination, a number of non-standard enhancements are employed to achieve improved translation quality. Specifically, we integrate individual language models trained over the separate corpora (News Crawl 2015, Europarl, SE"
W16-2320,D14-1003,1,0.932306,"with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integrates a discriminative rule selection model into a hierarchical SMT system, as described in (Tamchyna et al., 2014). The rule selection model is implemented using the highspeed classifier Vowpal Wabbit2 which is fully integrated in Moses’ hierarchical decoder. During decoding, the rule selection model is called at each rule appli"
W16-2320,P06-1055,0,0.0121776,"anslation Systems Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 LIMSI KIT The KIT system consists of a phrase-based machine translation system using additional models in rescoring. The phrase-based system is trained on all available parallel training data. The phrase 345 more specifically its implementation in XenC (Rousseau, 2013). As a result, one third of the initial corpus is removed. Finally, we make a linear interpolation of these models, using the SRILM toolkit (Stolcke, 2002). 3.3 training data using the Berkeley parser (Petrov et al., 2006). For model prediction during tuning and decoding, we use parsed versions of the development and test sets. We train the rule selection model using VW and tune the weights of the translation model using batch MIRA (Cherry and Foster, 2012). The 5-gram language model is trained using KenLM (Heafield et al., 2013) on the Romanian part of the Common Crawl corpus concatenated with the Romanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but als"
W16-2320,2007.tmi-papers.21,0,0.0230136,"n 2. Section 3 covers the characteristics of the different individual engines, followed by a brief overview of our system combination approach (Section 4). We then summarize our empirical results in Section 5, showing that we achieve better translation quality than with any individual engine. Finally, in Section 6, we provide a statistical analysis of certain linguistic phenomena, specifically the prediction precision on morphological attributes. We conclude the paper with Section 7. table is adapted to the SETimes2 corpus (Niehues and Waibel, 2012). The system uses a prereordering technique (Rottmann and Vogel, 2007) in combination with lexical reordering. It uses two word-based n-gram language models and three additional non-word language models. Two of them are automatic word class-based (Och, 1999) language models, using 100 and 1,000 word classes. In addition, we use a POS-based language model. During decoding, we use a discriminative word lexicon (Niehues and Waibel, 2013) as well. We rescore the system output using a 300-best list. The weights are optimized on the concatenation of the development data and the SETimes2 dev set using the ListNet algorithm (Niehues et al., 2015). In rescoring, we add t"
W16-2320,P16-1161,1,0.729045,"omanian part of the training data. LMU-CUNI The LMU-CUNI contribution is a constrained Moses phrase-based system. It uses a simple factored setting: our phrase table produces not only the target surface form but also its lemma and morphological tag. On the input, we include lemmas, POS tags and information from dependency parses (lemma of the parent node and syntactic relation), all encoded as additional factors. The main difference from a standard phrasebased setup is the addition of a feature-rich discriminative translation model which is conditioned on both source- and target-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-"
W16-2320,tufis-etal-2008-racais,0,0.107217,"Missing"
W16-2320,P16-1009,1,0.78198,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,P12-3008,0,0.0597108,"Missing"
W16-2320,P16-1162,1,0.259658,"and models are tuned with k-best MIRA (Cherry and Foster, 2012) on first the half of newsdev2016. In decoding, we use MBR (Kumar and Byrne, 2004), cube-pruning (Huang and Chiang, 2007) with a pop-limit of 5000, and the Moses ”monotone at punctuation” switch (to prevent reordering across punctuation) (Koehn and Haddow, 2009). Edinburgh Neural System Edinburgh’s neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2015), which we train with nematus.3 We use byte-pair-encoding (BPE) to achieve openvocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2016c). We produce additional parallel training data by automatically translating the monolingual Romanian News Crawl 2015 corpus into English (Sennrich et al., 2016b), which we combine with the original parallel data in a 1-to-1 ratio. We use minibatches of size 80, a maximum sentence length of 50, word embeddings of size 500, and hidden layers of size 1024. We apply dropout to all layers (Gal, 2015), with dropout probability 0.2, and also drop out full words with probability 0.1. We clip the gradient norm to 1.0 (Pascanu et al., 2013). We train the models with Adadelta (Zeiler, 2012), reshufflin"
W16-2320,W10-1738,1,0.885055,"rget-side context (Tamchyna et al., 2016). The motivation for using this model is to better condition lexical choices by using the source context and to improve morphological and topical coherence by modeling the (limited left-hand side) target context. We also take advantage of the target factors by using a 7-gram language model trained on sequences of Romanian morphological tags. Finally, our system also uses a standard lexicalized reordering model. 3.4 3.5 RWTH Aachen University: Hierarchical Phrase-based System The RWTH hierarchical setup uses the open source translation toolkit Jane 2.3 (Vilar et al., 2010). Hierarchical phrase-based translation (HPBT) (Chiang, 2007) induces a weighted synchronous context-free grammar from parallel text. In addition to the contiguous lexical phrases, as used in phrase-based translation (PBT), hierarchical phrases with up to two gaps are also extracted. Our baseline model contains models with phrase translation probabilities and lexical smoothing probabilities in both translation directions, word and phrase penalty, and enhanced low frequency features (Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical ph"
W16-2320,P15-4020,1,0.815128,"data for the translation model. A single 5-gram language model is built using all the target side of the parallel data and a subpart of the monolingual Romanian corpora selected with Xenc-v2 (Rousseau, 2013). For the latter we use all the parallel data as in-domain data and the first half of newsdev2016 as development set. The feature weights are tuned with MERT (Och, 2003) on the first half of newsdev2016. The system produces distinct 1000-best lists, for which we extend the feature set with the 17 baseline black-box features from sentencelevel Quality Estimation (QE) produced with Quest++4 (Specia et al., 2015). The 1000-best lists are then reranked and the top-best hypothesis extracted using the nbest rescorer available within the Moses toolkit. 3.12 UvA We use a phrase-based machine translation system (Moses) with a distortion limit of 6 and lexicalized reordering. Before translation, the English source side is preordered using the neural preordering model of (de Gispert et al., 2015). The preordering model is trained for 30 iterations on the full MGIZA-aligned training data. We use two language models, built using KenLM. The first is a 5-gram language model trained on all available data. Words in"
W16-2320,W16-2327,1,0.84726,"Missing"
W16-2320,D13-1138,1,0.859072,"(Chen et al., 2011). It also contains binary features to distinguish between hierarchical and non-hierarchical phrases, the glue rule, and rules with non-terminals at the boundaries. We use the cube pruning algorithm (Huang and Chiang, 2007) for decoding. The system uses three backoff language models (LM) that are estimated with the KenLM toolkit (Heafield et al., 2013) and are integrated into the decoder as separate models in the log-linear combination: a full 4-gram LM (trained on all data), a limited 5-gram LM (trained only on in-domain data), and a 7-gram word class language model (wcLM) (Wuebker et al., 2013) trained on all data and with a output vocabulary of 143K words. The system produces 1000-best lists which are reranked using a LSTM-based (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Gers et al., 2003) language model (Sundermeyer et al., 2012) and a LSTM-based bidirectional joined model (BJM) (Sundermeyer et al., 2014a). The models have a class-factored output layer (Goodman, 2001; Morin and Bengio, 2005) to speed up training and evaluation. The language model uses 3 stacked LSTM layers, with 350 nodes each. The BJM has a projection layer, and computes a forLMU The LMU system integra"
W16-2320,2002.tmi-tutorials.2,0,0.0608664,"omanian side, the data is tokenized using LIMSI’s tokro (Allauzen et al., 2016), a rulebased tokenizer that mainly normalizes diacritics and splits punctuation and clitics. This data is truecased in the same way as the English side. In addition, the Romanian sentences are also tagged, lemmatized, and chunked using the TTL tagger (Tufis¸ et al., 2008). 3 The LIMSI system uses NCODE (Crego et al., 2011), which implements the bilingual n-gram approach to SMT (Casacuberta and Vidal, 2004; Crego and Mari˜no, 2006; Mari˜no et al., 2006) that is closely related to the standard phrase-based approach (Zens et al., 2002). In this framework, translation is divided into two steps. To translate a source sentence into a target sentence, the source sentence is first reordered according to a set of rewriting rules so as to reproduce the target word order. This generates a word lattice containing the most promising source permutations, which is then translated. Since the translation step is monotonic, this approach is able to rely on the n-gram assumption to decompose the joint probability of a sentence pair into a sequence of bilingual units called tuples. We train three Romanian 4-gram language models, pruning all"
W17-3202,P11-2031,0,0.0388621,"on data we used the newstest13 set from IWSLT evaluation campaign. Therefore, this data is from TED talks. Test is applied on two domains. First domain is TED talks, same as the optimization set. We use newstest14 for this testing. Another domain is telephone conversation and we used MSLT (Christian Federmann, 2016) for testing. Since no exact genre-matching development data is published for the evaluation campaign (Cettolo et al., 2015), we used the TED-optimized system for the MSLT testing. For each experiment, we also offer oracle BLEU scores on the n-best lists, calculated using multeval (Clark et al., 2011). 4 We tried different system configurations to generate and rescore the n-best lists. By using 40K operations of BPE we had SmallVoc configuration, and with 80K BigVoc configuration. In SmallVoc.rev, target sentence are generated in the reversed order. In SmallVoc.mix, target side corpus is joined with the source side corpus to form a mixed input as described in Cho et al. (2016). We build an NMT system which takes pre-translation 4.1 System Description Our German↔English NMT systems are built using an encoder-decoder framework with attention mechanism, nematus.1 Byte pair encoding (BPE) is u"
W17-3202,N07-2035,0,0.0605851,"Missing"
W17-3202,P02-1040,0,0.099716,"he impact of the beam size used in statistical machine translation (SMT) systems. Wisniewski and Yvon (2013) conduct an in-depth analysis over several types of errors. Based on their proposal to effectively calculate oracle BLEU score for an SMT system, they can separate the errors due to the restriction of the Introduction Recent advances in NMT systems (Bahdanau et al., 2014; Cho et al., 2014) have shown impressive results in improving machine translation tasks. Not only it performed greatly in recent machine translation campaigns (Cettolo et al., 2015; Bojar et al., 2016) measured in BLEU (Papineni et al., 2002), it is considered to be able to generate sentences with better fluency. Despite the successful results in translation performance, however, the optimality of the search algorithm in NMT has been left under-explored. In this work, we analyze the influence of search and modeling of an NMT system by evaluating them 11 Proceedings of the First Workshop on Neural Machine Translation, pages 11–17, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics search space (search error) from the errors due to models not good enough to cover the best translation (model error). A"
W17-3202,D13-1111,0,0.0154739,"slation (MT) paradigms using n-best lists of hypotheses (Matusov et al., 2006; Heafield et al., 2009; Macherey and Och, 2007). The hypotheses are aligned, combined and scored by a model to produce the best candidate according to a metric. There was a thorough analysis on how the size of n, the diversity of the outputs from different systems and performance of individual systems can affect the final translation of the system combination. Hildebrand and Vogel (2008) examine the feature impact and the n-best list size of such a combination of phrase-based, hierarchical and example-based systems. Gimpel et al. (2013) show how diversity of the outputs and the size of the n-best lists determine the performance of the combined system. Costa-Juss`a et al. (2007) analyze the impact of the beam size used in statistical machine translation (SMT) systems. Wisniewski and Yvon (2013) conduct an in-depth analysis over several types of errors. Based on their proposal to effectively calculate oracle BLEU score for an SMT system, they can separate the errors due to the restriction of the Introduction Recent advances in NMT systems (Bahdanau et al., 2014; Cho et al., 2014) have shown impressive results in improving mach"
W17-3202,W09-0408,0,0.0232936,"ntly. By comparing differently performing NMT systems, we show that the better translation is already in the search space of the translation systems with less performance. This results indicate that the current search algorithms are sufficient for the NMT systems. Furthermore, we could show that even a relatively small n-best list of 50 hypotheses already contain notably better translations. 1 2 Related Work There has been a number of works devoted to combine different systems from the same or different machine translation (MT) paradigms using n-best lists of hypotheses (Matusov et al., 2006; Heafield et al., 2009; Macherey and Och, 2007). The hypotheses are aligned, combined and scored by a model to produce the best candidate according to a metric. There was a thorough analysis on how the size of n, the diversity of the outputs from different systems and performance of individual systems can affect the final translation of the system combination. Hildebrand and Vogel (2008) examine the feature impact and the n-best list size of such a combination of phrase-based, hierarchical and example-based systems. Gimpel et al. (2013) show how diversity of the outputs and the size of the n-best lists determine th"
W17-3202,2008.amta-srw.3,0,0.0234655,"ain notably better translations. 1 2 Related Work There has been a number of works devoted to combine different systems from the same or different machine translation (MT) paradigms using n-best lists of hypotheses (Matusov et al., 2006; Heafield et al., 2009; Macherey and Och, 2007). The hypotheses are aligned, combined and scored by a model to produce the best candidate according to a metric. There was a thorough analysis on how the size of n, the diversity of the outputs from different systems and performance of individual systems can affect the final translation of the system combination. Hildebrand and Vogel (2008) examine the feature impact and the n-best list size of such a combination of phrase-based, hierarchical and example-based systems. Gimpel et al. (2013) show how diversity of the outputs and the size of the n-best lists determine the performance of the combined system. Costa-Juss`a et al. (2007) analyze the impact of the beam size used in statistical machine translation (SMT) systems. Wisniewski and Yvon (2013) conduct an in-depth analysis over several types of errors. Based on their proposal to effectively calculate oracle BLEU score for an SMT system, they can separate the errors due to the"
W17-3202,D07-1105,0,0.0260245,"erently performing NMT systems, we show that the better translation is already in the search space of the translation systems with less performance. This results indicate that the current search algorithms are sufficient for the NMT systems. Furthermore, we could show that even a relatively small n-best list of 50 hypotheses already contain notably better translations. 1 2 Related Work There has been a number of works devoted to combine different systems from the same or different machine translation (MT) paradigms using n-best lists of hypotheses (Matusov et al., 2006; Heafield et al., 2009; Macherey and Och, 2007). The hypotheses are aligned, combined and scored by a model to produce the best candidate according to a metric. There was a thorough analysis on how the size of n, the diversity of the outputs from different systems and performance of individual systems can affect the final translation of the system combination. Hildebrand and Vogel (2008) examine the feature impact and the n-best list size of such a combination of phrase-based, hierarchical and example-based systems. Gimpel et al. (2013) show how diversity of the outputs and the size of the n-best lists determine the performance of the comb"
W17-3202,E06-1005,0,0.0101683,"n NMT system independently. By comparing differently performing NMT systems, we show that the better translation is already in the search space of the translation systems with less performance. This results indicate that the current search algorithms are sufficient for the NMT systems. Furthermore, we could show that even a relatively small n-best list of 50 hypotheses already contain notably better translations. 1 2 Related Work There has been a number of works devoted to combine different systems from the same or different machine translation (MT) paradigms using n-best lists of hypotheses (Matusov et al., 2006; Heafield et al., 2009; Macherey and Och, 2007). The hypotheses are aligned, combined and scored by a model to produce the best candidate according to a metric. There was a thorough analysis on how the size of n, the diversity of the outputs from different systems and performance of individual systems can affect the final translation of the system combination. Hildebrand and Vogel (2008) examine the feature impact and the n-best list size of such a combination of phrase-based, hierarchical and example-based systems. Gimpel et al. (2013) show how diversity of the outputs and the size of the n-"
W17-3202,W15-5003,0,0.0254054,"the search algorithm in NMT has been left under-explored. In this work, we analyze the influence of search and modeling of an NMT system by evaluating them 11 Proceedings of the First Workshop on Neural Machine Translation, pages 11–17, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics search space (search error) from the errors due to models not good enough to cover the best translation (model error). Although this work is the closest to our work in terms of analysis methods, our work differs from theirs by addressing the issue focused on the NMT systems. In Neubig et al. (2015), the size of the n-best list produced by a phrase-based SMT and rescored by an NMT is taken into account for an error investigation. The work also shows which types of errors from the phrase-based system can be corrected or improved after NMT rescoring. To the best of our knowledge, our work is the first to examine the impact of search and model performance in pure NMT systems. probabilities to be selected based on the context vector from the attention layer, the previous recurrent state and the embedding of the previously chosen word. The whole network is then trained in an end-toend fashion"
W17-4734,W05-0909,0,0.158301,"ions from multiple hypotheses which are obtained from different translation approaches, i.e., the systems described in the previous section. A system combination implementation developed at RWTH Aachen University (Freitag et al., 2014a) is used to combine the outputs of the different engines. The consensus translations outperform the individual hypotheses in terms of translation quality. The first step in system combination is the generation of confusion networks (CN) from I input translation hypotheses. We need pairwise alignments between the input hypotheses, which are obtained from METEOR (Banerjee and Lavie, 2005). The hypotheses are then reordered to match a selected skeleton hypothesis in terms of word ordering. We generate I different CNs, each having one of the input systems as the skeleton hypothesis, and the final lattice is the union of all I generated CNs. In Figure 1 an example of a confusion network with I = 4 input translations is depicted. Decoding of a confusion network finds the best path in the network. Each arc is assigned a score of a linear model combination of M different models, which includes word penalty, 3-gram language model trained on the input hypotheses, a binary primary syst"
W17-4734,P09-1064,0,0.0328843,"translations. pothesis, and a binary voting feature for each system. The binary voting feature for a system is 1 if and only if the decoded word is from that system, and 0 otherwise. The different model weights for system combination are trained with MERT (Och, 2003) and optimized towards 8·B LEU −T ER. 4.2 Consensus-based System Selection 5 Experimental Evaluation As a secondary solution for system combination, we used USFD’s consensus-based n-nbest list selection approach (Blain et al., 2017) for system combination by combining each system’s output in the form of a n-best list. Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varying n-best list in NMT. Given a n-best list, each translation hypothesis is scored against the other MT candidates of the search space towards an automatic metric. In our experiment we considered three automatic metrics amongst the most widely used and which have been shown to be well correlated with human judgments (Bojar et al., 2016): B LEU, B EER"
W17-4734,D17-1209,1,0.891192,"Missing"
W17-4734,E14-2008,1,0.856971,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W16-2302,1,0.832947,". Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varying n-best list in NMT. Given a n-best list, each translation hypothesis is scored against the other MT candidates of the search space towards an automatic metric. In our experiment we considered three automatic metrics amongst the most widely used and which have been shown to be well correlated with human judgments (Bojar et al., 2016): B LEU, B EER (Stanojevic and Simaan, 2014) or C HR F (Popovic, 2015). The entire list of MT candidates is then entirely re-ranked according to the averaged score of each candidate. Different from most re-ranking approaches which make use of additional information usually treated as new model components and combined with the existing ones, we here focus only on the MT candidates. The difference between the consensus-based n-best list selection and an oracle translation is the absence Since only one development set was provided we split the given development set into two parts: newsdev2017/1 a"
W17-4734,W14-3310,1,0.870459,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W17-4703,1,0.884283,"Missing"
W17-4734,2014.iwslt-evaluation.7,1,0.873477,"many 2 Charles University, Prague, Czech Republic 3 Karlsruhe Institute of Technology, Karlsruhe, Germany 4 LIMSI, CNRS, Universit´e Paris Saclay, 91 403 Orsay, France 5 Tilde, Riga, Latvia 6 University of Amsterdam, Amsterdam, Netherlands 7 University of Edinburgh, Edinburgh, UK 8 University of Sheffield, Sheffield, UK Abstract English→Latvian translation engines which have been set up by different project partners. The outputs of all these individual engines are combined using the system combination approach as implemented in Jane, RWTH’s open source statistical machine translation toolkit (Freitag et al., 2014a). The Jane system combination is a mature implementation which previously has been successfully employed in other collaborative projects and for different language pairs (Peter et al., 2016; Freitag et al., 2013, 2014b,c). As an alternative way of combining our systems, all outputs have been merged as the form of a n-best list and a consensus-based system-selection applied to obtain as best translation hypothesis the candidate that is most similar to the most likely translations amongst those systems. This paper describes the joint submission of the QT21 projects for the English→Latvian tran"
W17-4734,W17-4737,1,0.831634,"Missing"
W17-4734,W11-2123,0,0.0435093,"o this end, k-best hypothesis from the dictionary were generated, as well as the n-best hypothesis 3.4 Tilde The Tilde system is a Moses phrase-based SMT system that was trained on the Tilde MT platform (Vasil¸jevs et al., 2012). The system was trained using all available parallel data - 1.74 million unique sentence pairs after filtering, and 3 million unique sentence pairs that were acquired by re-translating a random selection of indomain monolingual sentences with a neural machine translation system (Pinnis et al., 2017). The system has a 5-gram language model that was trained using KenLM (Heafield, 2011) on all available monolingual data (27.83 million unique sentences). 3.5 UEDIN The University of Edinburgh’s system is an attentional encoder-decoder (Bahdanau et al., 2015), trained using the Nematus toolkit (Sennrich et al., 2017c). As training data, we used all parallel and synthetic data, which was tokenized, truecased, and filtered as described in Section 2. After filtering, the data was segmented into subword units using byte-pair-encoding (BPE), for which we used 90,000 operations, jointly learned over both sides of the parallel corpora. We used word embeddings of size 512 and hidden la"
W17-4734,W15-3049,0,0.0536506,"Missing"
W17-4734,E17-2025,0,0.0291776,"l and synthetic data, which was tokenized, truecased, and filtered as described in Section 2. After filtering, the data was segmented into subword units using byte-pair-encoding (BPE), for which we used 90,000 operations, jointly learned over both sides of the parallel corpora. We used word embeddings of size 512 and hidden layers of size 1024, with the size of the source and target network vocabularies fixed to the size of the respective BPE vocabularies. In order to reduce the size of the models, the target-side embedding weights were tied with the transpose of 350 the output weight matrix (Press and Wolf, 2017). We used a deep transition architecture inspired by the one proposed by Zilly et al. (2016) for language modelling. In experiments conducted during feature development, we found that this gave consistent improvements across multiple language pairs. We also applied layer normalisation (Ba et al., 2016) to all recurrent and feed-forward layers, except for layers that are followed by a softmax. In preliminary experiments, we found that using layer normalisation led to faster convergence and resulted in slightly better performance. We trained the models with adam (Kingma and Ba, 2015), using a le"
W17-4734,E17-3017,0,0.0486901,"Missing"
W17-4734,P17-4012,0,0.0301124,"stem for the WMT 2017 shared task for machine translation of news 1 are seven individual 1 http://www.statmt.org/wmt17/ translation-task.html 348 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 348–357 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics tool. The number of sentences being removed is approximately 50000. in Neural Monkey. Instead, the translations were generated using greedy search. 3 3.2 Translation Systems The neural machine translation models from KIT are built with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for p"
W17-4734,D17-1159,0,0.0716203,"Missing"
W17-4734,D16-1096,0,0.0289091,"rom backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about the effect of each technique is described in Pham et al. (2017) Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 KIT CUNI The CUNI component of"
W17-4734,P03-1021,0,0.0379532,"re case-sensitive. of reference translation: each translation hypothesis is scored against all the other hypotheses used as references while in an oracle translation each translation hypothesis is scored against a single reference. This results in obtaining as best translation hypothesis the candidate that is most similar to the most likely translations. pothesis, and a binary voting feature for each system. The binary voting feature for a system is 1 if and only if the decoded word is from that system, and 0 otherwise. The different model weights for system combination are trained with MERT (Och, 2003) and optimized towards 8·B LEU −T ER. 4.2 Consensus-based System Selection 5 Experimental Evaluation As a secondary solution for system combination, we used USFD’s consensus-based n-nbest list selection approach (Blain et al., 2017) for system combination by combining each system’s output in the form of a n-best list. Inspired by DeNero et al. (2009)’s work on consensus-based Minimum Bayes Risk (MBR) decoding which compares different types of similarity metrics (B LEU, W ER, etc.) under a SMT setup, USFD designed a reranking approach to empirically evaluate the effect of consensus on the varyi"
W17-4734,P16-1162,0,0.11892,"he effect of each technique is described in Pham et al. (2017) Each group contributed one or more systems. In this section the systems are presented in alphabetic order. 3.1 KIT CUNI The CUNI component of the system was built using Neural Monkey2 (Helcl and Libovick´y, 2017), a flexible sequence-to-sequence toolkit implementing primarily the Bahdanau et al. (2015) model but useful also in multi-modal translation and multi-task training. We used essentially the baseline setup of the system as released for the WMT17 NMT Training Task3 (Bojar et al., 2017) for an 8GB GPU card. This involves BPE (Sennrich et al., 2016) with 30k merges, maximum sentence length for both source and target limited to 50 (BPE) tokens, no dropout and embeddings (both source and target) of 600, vocabulary shared between encoder and decoder, attention and conditional GRU (Firat and Cho, 2016). We experimented with the RNN size of the encoder and decoder and increased them to 800 instead of 600, at the expense of reducing batch size to 10. The batch size of 30 with this enlarged model would still fit into our GPU card but this run was prematurely interrupted due to a hardware failure and we noticed that it converges slower in terms"
W17-4734,W14-3354,0,0.0640118,"Missing"
W17-4734,P16-5005,0,0.0206781,"with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about th"
W17-4734,P16-1008,0,0.0235141,"with the OpenNMT framework (Klein et al., 2017), which is a multi-layer LSTM encoder decoder network. We trained the models with 2.1 million parallel sentence pairs concatenated with 2.8 million pairs from backtranslation provided by University of Edinburgh. The networks have 1024 hidden units for each of 2 LSTM layers for both encoder and decoder. Furthermore, we experiment a number of features with the baseline: First, we found out that using a context gate to mask activities between the decoder hidden state and the source context vector before producing the distribution at each time step (Tu et al., 2016a) is simple yet beneficial for performance. Second, we strengthen the attentional network with a coverage vector accumulating the previous attentional information, similar to the work of Mi et al. (2016) and Tu et al. (2016b). Using the two techniques helps improve the BLEU score on the newsdev2017 set by 1.1 (tokenized) BLEU. By using ensembling 3 networks with different configs and rescoring using a model trained with reversed target sentences, we managed to reach 26.96 BLEU score for the development set, which yields 2.8 point of improvement compared to the baseline model. Details about th"
W17-4734,P12-3008,0,0.0243602,"Missing"
W18-2606,J81-4005,0,0.607392,"Missing"
W18-2606,P17-1055,0,0.015887,"k (Hermann et al., 2015), we show the adaptability of the rsDNC and achieve passable results without task-specific adaption. 2 Figure 1: System overview of the DNC. The dotted lines illustrate recurrent connections. troduce the Stanford Attentive Reader which enhances the attentive reader and adds a bilinear term to compute the attention between document and query. The Attention-Sum (AS) Reader from Kadlec et al. (2016) also uses separate encoding for the document and the query. Its successor, the Attention-over-Attention (AoA) Reader, applies a two-way attention mechanism to find the answer (Cui et al., 2017). The ReasoNet uses iterative reasoning over a hidden representation of the document (Shen et al., 2017). The Gated-Attention (GA) Reader from Dhingra et al. (2017) uses multiple hops over the document to build an attention over the candidates to select the answer token. These models are all conceptually adapted to the QA tasks they solve. In contrast, our solution is more versatile due to a more flexible and universal design. The different parts of the DNC can be exchanged or adjusted independently which allows simpler handling of new tasks. The versatility is shown in the original paper with"
W18-2606,P17-1168,0,0.0138981,"DNC. The dotted lines illustrate recurrent connections. troduce the Stanford Attentive Reader which enhances the attentive reader and adds a bilinear term to compute the attention between document and query. The Attention-Sum (AS) Reader from Kadlec et al. (2016) also uses separate encoding for the document and the query. Its successor, the Attention-over-Attention (AoA) Reader, applies a two-way attention mechanism to find the answer (Cui et al., 2017). The ReasoNet uses iterative reasoning over a hidden representation of the document (Shen et al., 2017). The Gated-Attention (GA) Reader from Dhingra et al. (2017) uses multiple hops over the document to build an attention over the candidates to select the answer token. These models are all conceptually adapted to the QA tasks they solve. In contrast, our solution is more versatile due to a more flexible and universal design. The different parts of the DNC can be exchanged or adjusted independently which allows simpler handling of new tasks. The versatility is shown in the original paper with three different tasks (Graves et al., 2016). Related Work This section considers the related work regarding the two used datasets. Related to bAbI task Rae et al."
W18-2606,P16-1086,0,0.0313559,"t random initializations. Additionally, with training-data augmentation on one task, our model solves all tasks and provides the bestrecorded results to the best of our knowledge. On the CNN RC task (Hermann et al., 2015), we show the adaptability of the rsDNC and achieve passable results without task-specific adaption. 2 Figure 1: System overview of the DNC. The dotted lines illustrate recurrent connections. troduce the Stanford Attentive Reader which enhances the attentive reader and adds a bilinear term to compute the attention between document and query. The Attention-Sum (AS) Reader from Kadlec et al. (2016) also uses separate encoding for the document and the query. Its successor, the Attention-over-Attention (AoA) Reader, applies a two-way attention mechanism to find the answer (Cui et al., 2017). The ReasoNet uses iterative reasoning over a hidden representation of the document (Shen et al., 2017). The Gated-Attention (GA) Reader from Dhingra et al. (2017) uses multiple hops over the document to build an attention over the candidates to select the answer token. These models are all conceptually adapted to the QA tasks they solve. In contrast, our solution is more versatile due to a more flexib"
W89-0224,J17-1002,0,0.0291446,"e constructs that their systems will encounter. Spoken language, with its weak grammatical structure, complicates matters. We believe that connectionist networks which learn to transform input word sequences into meaningful target representations offer advantages in this area. Much work has been done applying connectionist computational models to various aspects of language understanding. Some researchers have used connectionist networks to implement formal grammar systems for use in syntactic parsing [1,5, 10,6]. These networks do not learn their grammars. Other work has focused on semantics [8, 11,3,2] but either ignored parsing, or the networks did not learn to parse. The networks presented in this paper learn their own ""grammar rules"" for transforming an input sequence of words into a target representation, and learn to use semantic information to do role assignment The remainder of this paper is organized as follows. First, there is a description of our network formalism. Next, we describe in detail a modest experiment in which a network was taught to parse a small class of sentences. We show how the network behaves with some novel sentences and with sentences that have been corrupted as"
W93-0109,P91-1027,0,0.575421,"Missing"
W93-0109,H91-1067,0,0.408913,"Missing"
W93-0109,H92-1022,0,0.0449211,"Missing"
W93-0109,A88-1019,0,0.157578,"Missing"
W93-0109,P90-1031,0,0.0689193,"Missing"
W94-0113,1993.iwpt-1.12,0,0.0286201,"Missing"
W94-0113,1993.iwpt-1.15,0,0.0295865,"Missing"
W94-0113,H93-1041,0,0.0272365,"Missing"
W94-0113,J83-3001,0,\N,Missing
W94-0113,1993.tmi-1.16,0,\N,Missing
W97-0410,C96-1075,1,0.852035,"ented. Introduction Spoken language understanding systems have been reasonably successful in limited semantic domains I. The limited domains naturally constrain vocabulary and perplexity, making speech recognition tractable. In addition, the relatively small range of meanings that could be conveyed make parsing and understanding tractable. Now, with the increasing success of large vocabulary continuous speech recognition (LVCSR), the challenge is to similarly scale up spoken language understanding. In this paper we describe our plans for extending the JANUS speech-to-speech translation system [1] [2] from the Appointment Scheduling domain to a broader domain, Travel Planning, which has a rich sub-domain structure, covering many topics. In the last three years, the JANUS project has been developing a speech-to-speech translation system for the Appointment Scheduling domain (two people setting up a time to meet with each other). Although the data we have been working with is spontaneous speech, the scheduling scenario naturally limits the vocabulary to about 3000 words in English and about 4000 words in Spanish and German, which have more inflection. Similarly, the types of dialogues ar"
W97-0410,C96-1061,0,0.0193217,"guage input string is first analyzed by a parser, which produces a languageindependent interlingua content representation. The interlingua is then passed to a generation component, which produces an output string in the target language. In an attempt to achieve both robustness and translation accuracy when faced with speech disfluencies and recognition errors, we use two different parsing strategies: a GLFt parser designed to be more accurate, and a Phoenix parser designed to be more robust. Detailed descriptions of the system components appear in our previous publications [1] [2] [3] [4] [5] [6]. 68 s c h e d u l e - m e e t i n g in addition to syntactic categories such as NP and VP. There were several reasons for chosing semantic grammars. First, the domain lends itself well to semantic g r a m m a r s because there are many fixed expressions and c o m m o n expressions that are almost formulaic. Breaking these down syntactically would be an unnecessary complication. Additionally, spontaneous spoken language is often syntactically ill formed, yet semantically coherent. Semantic g r a m m a r s allow our robust parsers to extract the key concepts being conveyed, even when the input"
woszczcyna-etal-1998-modular,H90-1027,0,\N,Missing
woszczcyna-etal-1998-modular,H94-1093,0,\N,Missing
woszczcyna-etal-1998-modular,P98-2229,0,\N,Missing
woszczcyna-etal-1998-modular,C98-2224,0,\N,Missing
woszczcyna-etal-1998-modular,W97-0410,1,\N,Missing
woszczcyna-etal-1998-modular,P80-1024,0,\N,Missing
zhang-etal-2004-interpreting,P02-1040,0,\N,Missing
