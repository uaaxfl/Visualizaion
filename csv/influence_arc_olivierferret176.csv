2001.jeptalnrecital-long.13,P99-1044,1,0.897487,"ment Question/Phrase Séquence ordonnée de 250 et 50 caractères Figure 1. Architecture du système QALC L'analyse des questions est réalisée par un analyseur partiel dédié qui attribue aux questions des catégories correspondant aux types d'entités nommées pouvant répondre à la question. La sélection d’un sous-ensemble de documents pertinents repose sur la reconnaissance des termes de la question ou de leurs variantes dans les documents sélectionnés par un moteur de Entités nommées et variantes dans un système de question-réponse recherche classique. Cette reconnaissance est effectuée par FASTR (Jacquemin, 1999) sur la base des termes extraits de la question. Cette sélection revêt toute son importance lorsque le système applique les processus ultérieurs, à savoir la reconnaissance des entités nommées telles que les personnes, organisations, lieux et valeurs numériques, et la comparaison entre phrase et question, processus fortement consommateurs de temps de traitement. Le dernier module, qui propose un ensemble limité de réponses à chaque question, met en œuvre un calcul de similarité entre une question, représentée par un vecteur contenant ses mots pleins lemmatisés, ses termes et le type attendu de"
2001.jeptalnrecital-long.14,C98-1062,1,0.929344,"produit scalaire de ces vecteurs permet ensuite de regrouper ou de séparer les zones qu’ils décrivent s’ils sont proches ou non. La deuxième méthode d’analyse, quant à elle, utilise des sources de connaissances externes non dédiées. En généralisant la notion de répétition de termes, la cohésion thématique d’un texte se traduit par l’utilisation de termes faisant référence à une même entité par l’emploi de synonymes, d’hyponymes, de mots liés sémantiquement ou appartenant au même domaine. Afin d’introduire ces caractéristiques dans une analyse vectorielle d’un texte, les travaux décrits dans (Ferret et al., 1998) utilisent un réseau de collocations construit automatiquement sur un corpus d’articles de journaux afin d’enrichir les vecteurs décrivant un paragraphe. Le principe utilisé consiste à augmenter la valeur de descripteurs lorsqu’ils sont liés de manière significative dans le réseau à un descripteur du paragraphe. Cette modification des vecteurs tend ainsi à rapprocher des paragraphes comportant des mots liés. Notre projet vise à exploiter la complémentarité des approches statistiques et de l’approche linguistique. Les deux méthodes statistiques offrent le moyen de segmenter thématiquement des t"
2001.jeptalnrecital-long.14,J97-1003,0,0.38874,"sur des critères statistiques ou numériques, part du constat que le développement d’un thème entraîne la reprise de termes spécifiques, notamment lorsqu’il s’agit de textes techniques ou scientifiques. La reconnaissance de parties de texte liées à un même sujet est alors fondée sur la distribution des mots et leur récurrence. Si un mot apparaît souvent dans l’ensemble du texte, il est peu significatif, alors que sa répétition dans une zone limitée est très significative pour caractériser le thème de cette partie de texte. Le principe général appliqué par les différents systèmes (Masson, 1995, Hearst, 1997) consiste à associer un vecteur de descripteurs à une zone de Olivier Ferret, Brigitte Grau, Jean-Luc Minel et Sylvie Porhiel texte, où les descripteurs sont typiquement les mots lemmatisés du texte et leurs valeurs, le nombre d’occurrences de ces termes dans la zone. Le produit scalaire de ces vecteurs permet ensuite de regrouper ou de séparer les zones qu’ils décrivent s’ils sont proches ou non. La deuxième méthode d’analyse, quant à elle, utilise des sources de connaissances externes non dédiées. En généralisant la notion de répétition de termes, la cohésion thématique d’un texte se traduit"
2001.jeptalnrecital-long.14,W97-0707,0,0.0280881,"ois, si la notion de cohérence a été abondamment commentée (Halliday et al., 1976, Charolles, 1995, 1997), elle a été peu prise en compte dans les systèmes de résumé automatique ou d’analyse thématique. En fait, lorsqu’il s’agit de repérer automatiquement les thématiques d’un texte et de prendre en charge la cohérence thématique, les systèmes de Olivier Ferret, Brigitte Grau, Jean-Luc Minel et Sylvie Porhiel résumé automatique se heurtent à toutes sortes de difficultés. Premièrement, les modèles qui intègrent et exploitent des connaissances ou des ressources linguistiques (Berri et al., 1996, Mitra et al., 1997) ne s’appuient pas sur une vision globale du texte et des thèmes abordés : ils se fondent sur la notion de saillance d’une unité textuelle, d’une phrase ou d’un paragraphe, et cette saillance est calculée indépendamment de la structure thématique du texte. Deuxièmement, ces systèmes ne répondent que partiellement aux besoins des utilisateurs : ce qui est pertinent pour les uns ne l’est pas pour les autres (Sparck Jones, 1993, Minel et al. 1997), notamment parce qu’un utilisateur peut être intéressé par une thématique qui n’est pas prise en charge directement par l’auteur. Afin d’améliorer la p"
2002.jeptalnrecital-long.13,W01-0514,0,0.0734835,"Missing"
2002.jeptalnrecital-long.13,A00-2004,0,0.319003,"TDT (Topic Detection and Tracking). Cette dernière aborde les trois axes évoqués ci-dessus mais en se plaçant dans des domaines restreints et en les considérant principalement sous l’angle de tâches indépendantes. Les systèmes implémentant ces travaux peuvent être catégorisés en fonction du type des connaissances dont ils font usage. La plus grande partie de ceux dédiés à la segmentation thématique, c’est-à-dire au découpage des textes en segments thématiquement homogènes, s’appuient uniquement sur les caractéristiques intrinsèques des textes tels que la distribution des mots (Hearst, 1997 ; Choi, 2000 ; Utiyama, Isahara, 2001) ou des marqueurs linguistiques (Passonneau, Litman, 1997). Ils peuvent être utilisés sans restriction quant au domaine abordé mais leurs performances sont faibles lorsque la structure thématique des textes ne transparaît pas au travers des marques de surface qu’ils exploitent. Un second type de systèmes utilise des connaissances caractérisant la notion de cohésion lexicale. Ces connaissances, non liées à un domaine, prennent la forme d’un réseau de mots construit à partir d’un dictionnaire (Kozima, 1993 ; Morris, Hirst, 1991) ou d’un large ensemble de collocations is"
2002.jeptalnrecital-long.13,P98-2243,1,0.855579,"a, 2001) ou des marqueurs linguistiques (Passonneau, Litman, 1997). Ils peuvent être utilisés sans restriction quant au domaine abordé mais leurs performances sont faibles lorsque la structure thématique des textes ne transparaît pas au travers des marques de surface qu’ils exploitent. Un second type de systèmes utilise des connaissances caractérisant la notion de cohésion lexicale. Ces connaissances, non liées à un domaine, prennent la forme d’un réseau de mots construit à partir d’un dictionnaire (Kozima, 1993 ; Morris, Hirst, 1991) ou d’un large ensemble de collocations issues d’un corpus (Ferret, 1998 ; Kaufmann, 1999 ; Choi, 2001). Grâce aux relations entre mots qu’elles contiennent (synonymie, hyperonymie …), ces connaissances permettent d’écarter des changements de thème erronés définis sur la base de ruptures existant au niveau de la récurrence des mots. Un dernier type de systèmes s’appuie sur des connaissances directement liées aux thèmes apparaissant dans les textes qu’ils traitent. Dans le cas de TDT par exemple, ces connaissances sont construites de manière automatique à partir d’un ensemble de textes de référence caractérisant chaque thème considéré. (Bigi et al., 1998) se situe"
2002.jeptalnrecital-long.13,W98-1123,0,0.062572,"Missing"
2002.jeptalnrecital-long.13,P99-1077,0,0.23584,"marqueurs linguistiques (Passonneau, Litman, 1997). Ils peuvent être utilisés sans restriction quant au domaine abordé mais leurs performances sont faibles lorsque la structure thématique des textes ne transparaît pas au travers des marques de surface qu’ils exploitent. Un second type de systèmes utilise des connaissances caractérisant la notion de cohésion lexicale. Ces connaissances, non liées à un domaine, prennent la forme d’un réseau de mots construit à partir d’un dictionnaire (Kozima, 1993 ; Morris, Hirst, 1991) ou d’un large ensemble de collocations issues d’un corpus (Ferret, 1998 ; Kaufmann, 1999 ; Choi, 2001). Grâce aux relations entre mots qu’elles contiennent (synonymie, hyperonymie …), ces connaissances permettent d’écarter des changements de thème erronés définis sur la base de ruptures existant au niveau de la récurrence des mots. Un dernier type de systèmes s’appuie sur des connaissances directement liées aux thèmes apparaissant dans les textes qu’ils traitent. Dans le cas de TDT par exemple, ces connaissances sont construites de manière automatique à partir d’un ensemble de textes de référence caractérisant chaque thème considéré. (Bigi et al., 1998) se situe dans la même pers"
2002.jeptalnrecital-long.13,J97-1005,0,0.0557913,"Missing"
2002.jeptalnrecital-long.13,P01-1064,0,0.0905316,"Missing"
2002.jeptalnrecital-long.13,J90-1003,0,\N,Missing
2002.jeptalnrecital-long.13,J91-1002,0,\N,Missing
2002.jeptalnrecital-long.13,J97-1003,0,\N,Missing
2002.jeptalnrecital-long.28,P99-1044,0,0.0119111,"re longueur qui le forment. L&apos;analyse de la question se fonde sur l&apos;utilisation d&apos;un analyseur robuste dans le but d&apos;extraire plusieurs informations de la question, informations utiles pour la sélection de phrases ou pour l’extraction de la réponse. Cette partie sera développée dans la section suivante. Le module de traitement des documents utilise les sorties fournies par le NIST1, résultat de l&apos;application d’un moteur de recherche de type vectoriel sur le corpus de documents pour l’ensemble des questions de l’évaluation TREC. Les 200 meilleurs documents sont ré-indexés par le système FASTR (Jacquemin, 1999), analyseur transformationnel de surface qui reconnaît les occurrences et les variantes des termes produits par le module d&apos;extraction de 1 Le NIST est l’organisateur des conférences TREC. 309 Ferret O., Grau B., Hurault-Plantet M., Illouz G., Monceaux L., Robba I., Vilnat A. termes. Chaque occurrence ou variante constitue un index qui est ensuite utilisé dans le processus de classement des documents. En effet, ces index permettent à QALC de réordonner les documents et de sélectionner les plus pertinents (Ferret et al. 2001). Le module de reconnaissance des entités nommées est ensuite appliqué"
2003.jeptalnrecital-long.9,A97-1012,0,0.0518277,"Missing"
2003.jeptalnrecital-long.9,P02-1054,0,0.031363,"pté pour la recherche des réponses dans la collection de référence doublée d&apos; une autre dans une autre source d’informations afin de confronter les résultats des deux recherches. Le principe est de favoriser des réponses trouvées dans les deux sources, par rapport aux réponses, même fortement pondérées, mais trouvées dans une seule collection. Un tel raisonnement s’applique d’autant mieux que les sources de connaissances sont de nature différente, ainsi notre deuxième recherche s’effectue sur le Web, qui, de surcroît, par sa diversité et sa redondance conduit à trouver de nombreuses réponses (Magnini et al., 2002a et 2002b ; Clarke et al., 2001 ; Brill et al., 2001). Après la présentation générale de notre système, QALC, section 2, nous décrivons section 3 la reformulation des questions pour interroger le Web. La section 4 présente ensuite l’extraction des réponses pour une seule source de connaissances, et la section 5 les stratégies pour réaliser le choix final. Les résultats de QALC sont décrits en section 6 avant de rapprocher notre travail de ce qui existe dans le domaine. 2 Le système QALC Le système QALC (figure 1) participe aux évaluations TREC depuis 4 ans et a été conçu pour rechercher des r"
2003.jeptalnrecital-poster.9,C02-1033,1,0.880489,"Missing"
2003.jeptalnrecital-poster.9,C00-1072,0,0.0652362,"Missing"
2004.jeptalnrecital-long.5,E03-1020,0,0.368064,"Missing"
2004.jeptalnrecital-long.5,N03-3004,0,0.0358519,"Missing"
2004.jeptalnrecital-long.5,2003.mtsummit-papers.42,0,0.0434573,"Missing"
2004.jeptalnrecital-long.5,J98-1004,0,0.0733737,"Missing"
2004.jeptalnrecital-long.5,2003.jeptalnrecital-long.25,0,0.399494,"Missing"
2007.jeptalnrecital-long.3,P99-1016,0,0.107599,"Missing"
2007.jeptalnrecital-long.3,C92-2082,0,0.161256,"Missing"
2007.jeptalnrecital-long.3,P04-1055,0,0.0733438,"Missing"
2009.jeptalnrecital-court.46,S07-1002,0,0.0686153,"Missing"
2009.jeptalnrecital-court.46,A00-2004,0,0.0313192,"Missing"
2009.jeptalnrecital-court.46,W01-0514,0,0.104924,"Missing"
2009.jeptalnrecital-court.46,D08-1035,0,0.023682,"Missing"
2009.jeptalnrecital-court.46,C04-1194,1,0.892802,"Missing"
2009.jeptalnrecital-court.46,P03-1071,0,0.0525163,"Missing"
2009.jeptalnrecital-court.46,P94-1002,0,0.237297,"Missing"
2009.jeptalnrecital-court.46,P98-1100,0,0.059636,"Missing"
2009.jeptalnrecital-court.46,P93-1041,0,0.193597,"Missing"
2009.jeptalnrecital-court.46,J91-1002,0,0.498195,"Missing"
2009.jeptalnrecital-court.46,J97-1005,0,0.0506868,"Missing"
2009.jeptalnrecital-court.46,J02-1002,0,0.0417349,"Missing"
2009.jeptalnrecital-court.46,N03-3009,0,0.0816428,"Missing"
2009.jeptalnrecital-court.46,P01-1064,0,0.0548103,"Missing"
2009.jeptalnrecital-court.46,2003.jeptalnrecital-long.25,0,0.116683,"Missing"
2010.jeptalnrecital-court.30,ayache-etal-2006-equer,0,0.0673731,"Missing"
2010.jeptalnrecital-court.30,J07-1005,0,0.0360775,"Missing"
2010.jeptalnrecital-court.30,2007.jeptalnrecital-long.3,1,0.783806,"Missing"
2010.jeptalnrecital-court.30,W04-0509,0,0.106751,"Missing"
2010.jeptalnrecital-long.24,besancon-etal-2010-lima,1,0.881609,"Missing"
2010.jeptalnrecital-long.24,P95-1042,0,0.126011,"Missing"
2010.jeptalnrecital-long.24,C96-1079,0,0.327271,"Missing"
2010.jeptalnrecital-long.24,I08-1050,0,0.0541864,"Missing"
2010.jeptalnrecital-long.24,P07-3006,0,0.0442624,"Missing"
2010.jeptalnrecital-long.4,W09-0201,0,0.0293009,"Missing"
2010.jeptalnrecital-long.4,W02-0908,0,0.0708779,"Missing"
2010.jeptalnrecital-long.4,W05-0604,0,0.0421507,"Missing"
2010.jeptalnrecital-long.4,P98-2127,0,0.263147,"Missing"
2010.jeptalnrecital-long.4,D09-1098,0,0.0309202,"Missing"
2010.jeptalnrecital-long.4,2007.jeptalnrecital-long.27,0,0.0690499,"Missing"
2010.jeptalnrecital-long.4,C08-1114,0,0.060906,"Missing"
2010.jeptalnrecital-long.4,J09-3004,0,0.0278198,"Missing"
2010.jeptalnrecital-long.4,C98-2122,0,\N,Missing
2011.jeptalnrecital-court.26,W02-0908,0,0.104391,"Missing"
2011.jeptalnrecital-court.26,P10-1026,0,0.0277206,"Missing"
2011.jeptalnrecital-court.26,J07-2002,0,0.0375489,"Missing"
2011.jeptalnrecital-court.26,J09-3004,0,0.039746,"Missing"
2011.jeptalnrecital-court.8,P08-1004,0,0.0388798,"Missing"
2011.jeptalnrecital-court.8,P04-1053,0,0.105174,"Missing"
2011.jeptalnrecital-court.8,P06-2094,0,0.0457351,"Missing"
2011.jeptalnrecital-court.8,N06-1039,0,0.059062,"Missing"
2011.jeptalnrecital-long.4,P08-1004,0,0.0536068,"Missing"
2011.jeptalnrecital-long.4,C04-1038,0,0.0440102,"Missing"
2011.jeptalnrecital-long.4,embarek-ferret-2008-learning,1,0.904657,"Missing"
2011.jeptalnrecital-long.4,C92-2082,0,0.322729,"Missing"
2011.jeptalnrecital-long.4,P09-1113,0,0.136929,"Missing"
2011.jeptalnrecital-long.4,N06-1039,0,0.0860978,"Missing"
2011.jeptalnrecital-long.4,P05-1053,0,0.119585,"Missing"
2011.jeptalnrecital-long.4,D07-1076,0,0.0608066,"Missing"
2015.jeptalnrecital-long.13,N07-1026,0,0.0578749,"Missing"
2015.jeptalnrecital-long.13,C14-1067,1,0.83337,"Missing"
2015.jeptalnrecital-long.13,W02-1029,0,0.080016,"Missing"
2015.jeptalnrecital-long.13,W02-0908,0,0.0855881,"Missing"
2015.jeptalnrecital-long.13,P10-2017,0,0.0736331,"Missing"
2015.jeptalnrecital-long.13,ferret-2010-testing,1,0.826134,"Missing"
2015.jeptalnrecital-long.13,P13-1055,1,0.774294,"Missing"
2015.jeptalnrecital-long.13,W12-3401,0,0.027181,"Missing"
2015.jeptalnrecital-long.13,heylen-etal-2008-modelling,0,0.0608512,"Missing"
2015.jeptalnrecital-long.13,P12-1092,0,0.074771,"Missing"
2015.jeptalnrecital-long.13,P10-1026,0,0.0551324,"Missing"
2015.jeptalnrecital-long.13,W14-1503,0,0.023721,"Missing"
2015.jeptalnrecital-long.13,W02-1006,0,0.101625,"Missing"
2015.jeptalnrecital-long.13,C94-1079,0,0.254261,"Missing"
2015.jeptalnrecital-long.13,P98-2127,0,0.306882,"Missing"
2015.jeptalnrecital-long.13,N13-1090,0,0.026903,"Missing"
2015.jeptalnrecital-long.13,D12-1094,0,0.0282448,"Missing"
2015.jeptalnrecital-long.13,J07-2002,0,0.0711692,"Missing"
2015.jeptalnrecital-long.13,D14-1162,0,0.0741585,"Missing"
2015.jeptalnrecital-long.13,N10-1013,0,0.0418318,"Missing"
2015.jeptalnrecital-long.13,D13-1089,0,0.0341472,"Missing"
2015.jeptalnrecital-long.13,J09-3004,0,0.0420809,"Missing"
2015.jeptalnrecital-long.7,P98-1013,0,0.117804,"Missing"
2015.jeptalnrecital-long.7,D13-1178,0,0.024732,"Missing"
2015.jeptalnrecital-long.7,D13-1185,0,0.0242454,"Missing"
2015.jeptalnrecital-long.7,P08-1090,0,0.0662599,"Missing"
2015.jeptalnrecital-long.7,P09-1068,0,0.0811295,"Missing"
2015.jeptalnrecital-long.7,P11-1098,0,0.0366076,"Missing"
2015.jeptalnrecital-long.7,N13-1104,0,0.0238891,"Missing"
2015.jeptalnrecital-long.7,P06-2027,0,0.0225288,"tion Extraction (Shinyama & Sekine, 2006), vise à induire l’équivalent d’un formulaire à partir d’un ensemble de documents représentatifs des informations à extraire, documents typiquement obtenus par le biais de requêtes soumises à un moteur de recherche. Ce courant de recherche s’est ensuite davantage orienté vers l’extraction de relations, avec notamment Kathrin Eichler & Neumann (2008), Rosenfeld & Feldman (2007) et plus récemment Min et al. (2012), que vers l’extraction d’événements. Dans cet article, nous nous plaçons dans la voie tracée initialement par Hasegawa et al. (2004) et Sekine (2006) en considérant la possibilité d’induire des représentations d’événements à partir de textes. Plus globalement, nous cherchons à construire une base de connaissances événementielles à partir de larges corpus journalistiques afin d’offrir des moyens d’accès structurés à ces corpus. Nous nous concentrons dans le cadre du travail présenté dans cet article sur le processus d’induction de schémas d’événements. 2 Objectif Notre objectif global est de modéliser les événements décrits dans un corpus journalistique et d’identifier sans supervision les schémas (ou formulaires) récurrents ainsi que les r"
2015.jeptalnrecital-long.7,D11-1133,0,0.0197495,"Missing"
2015.jeptalnrecital-long.7,E14-1006,0,0.0210968,"Missing"
2015.jeptalnrecital-long.7,C96-1079,0,0.737202,"Missing"
2015.jeptalnrecital-long.7,P04-1053,0,0.0967503,"Missing"
2015.jeptalnrecital-long.7,I11-1081,1,0.901965,"Missing"
2015.jeptalnrecital-long.7,eichler-etal-2008-unsupervised,0,0.029375,"Missing"
2015.jeptalnrecital-long.7,P14-5010,0,0.0075885,"Missing"
2015.jeptalnrecital-long.7,D12-1094,0,0.0364988,"Missing"
2015.jeptalnrecital-long.7,D07-1075,0,0.0741971,"Missing"
2015.jeptalnrecital-long.7,D09-1016,0,0.024886,"Missing"
2015.jeptalnrecital-long.7,I08-1021,0,0.0444661,"Missing"
2015.jeptalnrecital-long.7,P10-1100,0,0.0276145,"Missing"
2015.jeptalnrecital-long.7,P06-2094,0,0.0409422,"Missing"
2015.jeptalnrecital-long.7,N06-1039,0,0.0865828,"Missing"
2015.jeptalnrecital-long.7,H91-1059,0,0.65433,"Missing"
2016.jeptalnrecital-long.22,D14-1165,0,0.0722656,"Missing"
2016.jeptalnrecital-long.22,D11-1071,0,0.0345994,"Missing"
2016.jeptalnrecital-long.22,C10-1032,0,0.0983409,"Missing"
2016.jeptalnrecital-long.22,Q14-1037,0,0.0630172,"Missing"
2016.jeptalnrecital-long.22,Q15-1023,0,0.0381122,"Missing"
2016.jeptalnrecital-long.22,D15-1104,0,0.055972,"Missing"
2016.jeptalnrecital-long.22,Q14-1019,0,0.0930077,"Missing"
2016.jeptalnrecital-long.22,E12-2015,0,0.0571457,"Missing"
2016.jeptalnrecital-long.22,N15-3010,0,0.0257504,"Missing"
2016.jeptalnrecital-long.22,W12-0508,0,0.0492924,"Missing"
2016.jeptalnrecital-poster.19,S15-2136,0,0.0366591,"Missing"
2016.jeptalnrecital-poster.19,S16-1165,0,0.0233445,"Missing"
2016.jeptalnrecital-poster.19,W06-1623,0,0.0677844,"Missing"
2016.jeptalnrecital-poster.19,P05-1022,0,0.112201,"Missing"
2016.jeptalnrecital-poster.19,deleger-etal-2014-annotation,1,0.887262,"Missing"
2016.jeptalnrecital-poster.19,W02-0109,0,0.0797351,"Missing"
2016.jeptalnrecital-poster.19,P14-5010,0,0.00491923,"Missing"
2016.jeptalnrecital-poster.19,N10-1004,0,0.0484883,"Missing"
2016.jeptalnrecital-poster.19,W11-0419,0,0.0708503,"Missing"
2016.jeptalnrecital-poster.19,Q14-1012,0,0.049267,"Missing"
2016.jeptalnrecital-poster.22,D15-1220,0,0.0217099,"Missing"
2016.jeptalnrecital-poster.22,W09-1802,0,0.0852249,"Missing"
2016.jeptalnrecital-poster.22,hong-etal-2014-repository,0,0.0263279,"Missing"
2016.jeptalnrecital-poster.22,E14-1075,0,0.0338659,"Missing"
2016.jeptalnrecital-poster.22,N15-1079,0,0.0351067,"Missing"
2016.jeptalnrecital-poster.22,P13-1099,0,0.0363299,"Missing"
2016.jeptalnrecital-poster.22,D11-1105,0,0.0574301,"Missing"
2016.jeptalnrecital-poster.22,W04-1013,0,0.0359934,"Missing"
2016.jeptalnrecital-poster.22,P11-1052,0,0.0606454,"Missing"
2016.jeptalnrecital-poster.22,N13-1090,0,0.0291977,"Missing"
2016.jeptalnrecital-poster.22,radev-etal-2004-mead,0,0.180935,"Missing"
2016.jeptalnrecital-poster.22,D11-1043,0,0.0585968,"Missing"
2016.jeptalnrecital-poster.22,S14-2039,0,0.0665737,"Missing"
2016.jeptalnrecital-poster.22,D12-1022,0,0.0638773,"Missing"
2017.jeptalnrecital-long.7,P14-1023,0,0.125018,"Missing"
2017.jeptalnrecital-long.7,W16-2502,0,0.0501348,"Missing"
2017.jeptalnrecital-long.7,J06-1003,0,0.327476,"Missing"
2017.jeptalnrecital-long.7,C14-1067,1,0.900492,"Missing"
2017.jeptalnrecital-long.7,W02-0908,0,0.24614,"Missing"
2017.jeptalnrecital-long.7,N15-1184,0,0.0591131,"Missing"
2017.jeptalnrecital-long.7,J15-4004,0,0.05557,"Missing"
2017.jeptalnrecital-long.7,W14-1503,0,0.0502327,"Missing"
2017.jeptalnrecital-long.7,Q15-1016,0,0.0754977,"Missing"
2017.jeptalnrecital-long.7,P98-2127,0,0.471542,"Missing"
2017.jeptalnrecital-long.7,N16-1018,0,0.0256533,"Missing"
2017.jeptalnrecital-long.7,D14-1162,0,0.0864985,"Missing"
2017.jeptalnrecital-long.7,D12-1111,0,0.0535733,"Missing"
2017.jeptalnrecital-long.7,P14-2089,0,0.0764377,"Missing"
2017.jeptalnrecital-long.7,D14-1161,0,0.0515586,"Missing"
2018.jeptalnrecital-court.17,D07-1074,0,0.154506,"Missing"
2018.jeptalnrecital-court.17,C10-1032,0,0.0888224,"Missing"
2018.jeptalnrecital-court.17,K16-1026,0,0.0470316,"Missing"
2018.jeptalnrecital-court.17,Q15-1023,0,0.039107,"Missing"
2018.jeptalnrecital-court.17,Q14-1019,0,0.0953257,"Missing"
2018.jeptalnrecital-court.17,N13-1008,0,0.027162,"Missing"
2018.jeptalnrecital-court.17,D13-1136,0,0.0409066,"Missing"
2018.jeptalnrecital-court.17,K16-1025,0,0.0370268,"Missing"
2018.jeptalnrecital-court.19,P14-1023,0,0.112018,"Missing"
2018.jeptalnrecital-court.19,N15-1184,0,0.0614864,"Missing"
2018.jeptalnrecital-court.19,J15-4004,0,0.0368185,"Missing"
2018.jeptalnrecital-court.19,D15-1242,0,0.0291974,"Missing"
2018.jeptalnrecital-court.19,P14-2050,0,0.0728006,"Missing"
2018.jeptalnrecital-court.19,D17-1257,0,0.0345438,"Missing"
2018.jeptalnrecital-court.19,P14-5010,0,0.00718955,"Missing"
2018.jeptalnrecital-court.19,N16-1018,0,0.0274096,"Missing"
2018.jeptalnrecital-court.19,Q17-1022,0,0.0296527,"Missing"
2018.jeptalnrecital-court.19,W12-3018,0,0.022369,"Missing"
2018.jeptalnrecital-court.19,D14-1162,0,0.076509,"Missing"
2018.jeptalnrecital-court.19,P17-1006,0,0.0226048,"Missing"
2018.jeptalnrecital-court.19,Q15-1025,0,0.0248723,"Missing"
2018.jeptalnrecital-court.19,P16-1128,0,0.0358011,"Missing"
2018.jeptalnrecital-long.10,I17-1036,0,0.0370005,"Missing"
2018.jeptalnrecital-long.10,P16-2011,0,0.0467101,"Missing"
2018.jeptalnrecital-long.10,N10-1112,0,0.0208907,"Missing"
2018.jeptalnrecital-long.10,P14-5010,0,0.00575635,"Missing"
2018.jeptalnrecital-long.10,N16-1034,0,0.0557003,"Missing"
2018.jeptalnrecital-long.10,P15-1017,0,0.0720995,"Missing"
2018.jeptalnrecital-long.10,D17-1035,0,0.0201427,"Missing"
2018.jeptalnrecital-long.10,W15-0812,0,0.0536573,"Missing"
2019.jeptalnrecital-long.3,P14-1023,0,0.0798067,"Missing"
2019.jeptalnrecital-long.3,J10-4006,0,0.0611646,"Missing"
2019.jeptalnrecital-long.3,K18-2017,0,0.0319848,"Missing"
2019.jeptalnrecital-long.3,bosco-etal-2012-parallel,0,0.0351528,"Missing"
2019.jeptalnrecital-long.3,K18-2002,0,0.0453113,"Missing"
2019.jeptalnrecital-long.3,D15-1162,0,0.0299374,"Missing"
2019.jeptalnrecital-long.3,W14-1503,0,0.0644561,"Missing"
2019.jeptalnrecital-long.3,P08-1068,0,0.0751309,"Missing"
2019.jeptalnrecital-long.3,E17-2063,0,0.0324251,"Missing"
2019.jeptalnrecital-long.3,P14-2050,0,0.0846289,"Missing"
2019.jeptalnrecital-long.3,Q15-1016,0,0.0774924,"Missing"
2019.jeptalnrecital-long.3,P98-2127,0,0.797354,"Missing"
2019.jeptalnrecital-long.3,P14-5010,0,0.00469723,"Missing"
2019.jeptalnrecital-long.3,W06-2932,0,0.077081,"Missing"
2019.jeptalnrecital-long.3,P13-2017,0,0.0809363,"Missing"
2019.jeptalnrecital-long.3,L16-1262,0,0.0554458,"Missing"
2019.jeptalnrecital-long.3,J07-2002,0,0.120705,"Missing"
2019.jeptalnrecital-long.3,paroubek-etal-2008-easy,0,0.0682698,"Missing"
2019.jeptalnrecital-long.3,N18-4005,1,0.860787,"Missing"
2019.jeptalnrecital-long.3,K18-2016,0,0.0262916,"Missing"
2019.jeptalnrecital-long.3,W04-2104,0,0.134115,"Missing"
2019.jeptalnrecital-long.3,sagot-2010-lefff,0,0.0990524,"Missing"
2019.jeptalnrecital-long.3,W13-4917,0,0.0574786,"Missing"
2019.jeptalnrecital-long.3,K17-3009,0,0.0349102,"Missing"
2019.jeptalnrecital-long.3,K18-2001,0,0.0323023,"Missing"
2020.coling-main.609,N19-1423,0,0.04791,"ractice. For these reasons, we propose CharacterBERT, a new variant of BERT that drops the wordpiece system altogether and uses a Character-CNN module instead to represent entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations. 1 Introduction Pre-trained language representations from Transformers (Vaswani et al., 2017) have become arguably the most popular choice for building NLP systems1 . Among all such models, BERT (Devlin et al., 2019) has probably been the most successful, spawning a large number of new improved variants (Liu et al., 2019; Lan et al., 2019; Sun et al., 2019; Zhang et al., 2019; Clark et al., 2020). As a result, many of the recent language representation models inherited BERT’s subword tokenization system which relies on a predefined set of wordpieces (Wu et al., 2016), supposedly striking a good balance between the flexibility of characters and the efficiency of full words. While current research mostly focuses on improving language representations for the default “generaldomain”, there seems to be a growi"
2020.coling-main.609,P19-1266,0,0.031135,"Missing"
2020.coling-main.609,P19-2041,1,0.894488,"Missing"
2020.coling-main.609,P16-1100,0,0.0205295,"eptually simpler word-level models. This new variant does not rely on wordpieces but instead consults the characters of This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 See the leaderboard of the GLUE benchmark. 2 See the baselines from the BLUE benchmark. 6903 Proceedings of the 28th International Conference on Computational Linguistics, pages 6903–6915 Barcelona, Spain (Online), December 8-13, 2020 each token to build representations similarly to previous word-level open-vocabulary systems (Luong and Manning, 2016; Kim et al., 2016; Jozefowicz et al., 2016). In practice, we replace BERT’s wordpiece embedding layer with ELMo’s (Peters et al., 2018) Character-CNN module while keeping the rest of the architecture untouched. As a result, CharacterBERT is able to produce word-level contextualized representations and does not require a wordpiece vocabulary. Furthermore, this new model seems better suited than vanilla BERT for training specialized models, as evidenced by an evaluation on multiple tasks from the medical domain. Finally, as expected from a character-based system, CharacterBERT is also seemingly"
2020.coling-main.609,W19-5006,0,0.0200639,"between the flexibility of characters and the efficiency of full words. While current research mostly focuses on improving language representations for the default “generaldomain”, there seems to be a growing interest in building suitable word embeddings for more specialized domains (El Boukkouri et al., 2019; Si et al., 2019; Elwany et al., 2019). However, with the growing complexity of recent representation models, the default trend seems to favor re-training general-domain models on specialized corpora rather than building models from scratch with a specialized vocabulary (e.g., BlueBERT (Peng et al., 2019) and BioBERT (Lee et al., 2020)). While these methods undeniably produce good models 2 , a few questions remain: How suitable are the predefined general-domain vocabularies when used in the context of specialized domains (e.g., the medical domain)? Is it better to train specialized models with specialized subword units? Do we induce any biases by training specialized models with general-domain wordpieces? In this paper, we propose CharacterBERT, a possible solution for avoiding any biases that may come from the use of a predefined wordpiece vocabulary, and an effort to revert back to conceptua"
2020.coling-main.609,N18-1202,0,0.048799,"ed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 1 See the leaderboard of the GLUE benchmark. 2 See the baselines from the BLUE benchmark. 6903 Proceedings of the 28th International Conference on Computational Linguistics, pages 6903–6915 Barcelona, Spain (Online), December 8-13, 2020 each token to build representations similarly to previous word-level open-vocabulary systems (Luong and Manning, 2016; Kim et al., 2016; Jozefowicz et al., 2016). In practice, we replace BERT’s wordpiece embedding layer with ELMo’s (Peters et al., 2018) Character-CNN module while keeping the rest of the architecture untouched. As a result, CharacterBERT is able to produce word-level contextualized representations and does not require a wordpiece vocabulary. Furthermore, this new model seems better suited than vanilla BERT for training specialized models, as evidenced by an evaluation on multiple tasks from the medical domain. Finally, as expected from a character-based system, CharacterBERT is also seemingly more robust to noise and misspellings. To the best of our knowledge, this is the first work that replaces BERT’s wordpiece system with"
2020.coling-main.609,P19-1561,0,0.0612103,"Missing"
2020.coling-main.609,D18-1187,0,0.0534015,"Missing"
2020.coling-main.609,P19-1139,0,0.0221757,"present entire words by consulting their characters. We show that this new model improves the performance of BERT on a variety of medical domain tasks while at the same time producing robust, word-level, and open-vocabulary representations. 1 Introduction Pre-trained language representations from Transformers (Vaswani et al., 2017) have become arguably the most popular choice for building NLP systems1 . Among all such models, BERT (Devlin et al., 2019) has probably been the most successful, spawning a large number of new improved variants (Liu et al., 2019; Lan et al., 2019; Sun et al., 2019; Zhang et al., 2019; Clark et al., 2020). As a result, many of the recent language representation models inherited BERT’s subword tokenization system which relies on a predefined set of wordpieces (Wu et al., 2016), supposedly striking a good balance between the flexibility of characters and the efficiency of full words. While current research mostly focuses on improving language representations for the default “generaldomain”, there seems to be a growing interest in building suitable word embeddings for more specialized domains (El Boukkouri et al., 2019; Si et al., 2019; Elwany et al., 2019). However, with the"
2020.computerm-1.4,P14-1023,0,0.0137656,"e StanfordNLP models are also grouped. However, the training corpus can also have an impact when we consider the triplets shared by all parsers, with StanfordNLPLines much closer to UDpipe-Lines than to the two other StanfordNLP models. This is why the clusterings built for the two sets of triplets are a little bit different, even if they also share some patterns: for instance, SpaCy is close to CoreNLP, which is close to StanfordNLP-Ewt while UDpipe-Gum and UDpipe-Ewt form a group for the two sets. Distributional Models 5.1. Building of Distributional Models Following the distinction made in Baroni et al. (2014), we built our distributional models according to a countbased approach, such as in (Lin, 1998), rather than according to a predictive approach such as in (Mikolov et al., 2013). The first justification of this choice is that, except for (Levy and Goldberg, 2014), the number of studies relying on dependency relations is very limited among predictive approaches. More importantly, some recent studies (Pierrejean and Tanguy, 2018) have shown that predictive approaches are unstable to some extent concerning the search of the nearest distributional neighbors of a word. Since we want specifically to"
2020.computerm-1.4,bosco-etal-2012-parallel,0,0.0183857,"aining corpora, given the high cost of the annotation and validation processes. CoreNLP (Manning et al., 2014), the main tool of the Stanford team, implements a maximum entropy tagger, which uses the Penn Treebank tagset (Marcus et al., 1993), and a transition-based parser. StanfordNLP (Qi et al., 2018) is a tool that, on top of giving access to the CoreNLP chain in Python, implements an entirely different parsing chain. Its graphbased parser relies on a LSTM neural network. StanfordNLP offers 3 English models, trained on the UD EWT (Silveira et al., 2014), LinES (Ahrenberg, 2015) and ParTUT (Bosco et al., 2012) corpora. We used all three of these models. Spacy is an industry-targeting tool whose main characteristic is its speed compared to most other parsers. The tagger is based on a perceptron, with attributes based on Brown clusters, following (Koo et al., 2008). It implements a non-monotonous transition-based parser which can revise previous decisions (Honnibal and Johnson, 2015). The default model we used was 1 2 Terminological Reference Resource https://github.com/clir/ clearnlp-guidelines/blob/master/md/ specifications/dependency_labels.md 3 https://www.ncbi.nlm.nih.gov/books/ NBK9684/#_ch02_s"
2020.computerm-1.4,C10-2013,0,0.0409887,"yntactic contexts will be a building block for the task, which syntactic parser should be used to extract these contexts? The goal of this article is, thus, to study the impact of the choice of parser on the construction of a distributional model with a frequency-based method. Our work is not the first work on comparing different parsers. Several evaluation campaigns were previously organized for various languages: the Easy (Paroubek et al., 2008), Passage (De La Clergerie et al., 2008), SPMRL (Seddah et al., 2013) and CoNLL (Zeman et al., 2018) campaigns as well as more focused studies like (Candito et al., 2010) or (De La Clergerie, 2014). However, the benchmarks used in these studies, adopting the kind of diverse, generic corpora on which the tools have been trained, might not be the most relevant option for specialized corpus parsing. Moreover, even though some of these campaigns are recent, the main tools available have not been compared on the same evaluation sets. We previously performed a first study (Tanguy 2. Overview To go beyond the limitation in (Tanguy et al., 2020), we have chosen, in the work we present in this article, to run a new evaluation on a small, specialized biomedical corpus,"
2020.computerm-1.4,W02-0908,0,0.11437,"pe of corpora are usually small-sized (a few million words or less), which poses a challenge for distributional methods, and contain specific, highly technical vocabulary, meaning that adapting methods based on large generic corpora might be difficult. We make the hypothesis, supported by the work of (Tanguy et al., 2015), that the small amount of data may be circumvented by a method based on syntactic contexts. Such methods have already been investigated by a large body of work. The largest part of it is dedicated to count-based approaches (Grefenstette, 1994; Habert et al., 1996; Lin, 1998; Curran and Moens, 2002; Padó and Lapata, 2007; Baroni and Lenci, 2010) but it also includes work adding dimensionality reduction methods (Lapesa and Evert, 2017) or more recently, work about word embeddings (Levy and Goldberg, 2014). One of our focuses is to select the best-suited tools for semantic analysis of specialized corpora. In particular, given that syntactic contexts will be a building block for the task, which syntactic parser should be used to extract these contexts? The goal of this article is, thus, to study the impact of the choice of parser on the construction of a distributional model with a frequen"
2020.computerm-1.4,villemonte-de-la-clergerie-etal-2008-passage,0,0.0483024,"14). One of our focuses is to select the best-suited tools for semantic analysis of specialized corpora. In particular, given that syntactic contexts will be a building block for the task, which syntactic parser should be used to extract these contexts? The goal of this article is, thus, to study the impact of the choice of parser on the construction of a distributional model with a frequency-based method. Our work is not the first work on comparing different parsers. Several evaluation campaigns were previously organized for various languages: the Easy (Paroubek et al., 2008), Passage (De La Clergerie et al., 2008), SPMRL (Seddah et al., 2013) and CoNLL (Zeman et al., 2018) campaigns as well as more focused studies like (Candito et al., 2010) or (De La Clergerie, 2014). However, the benchmarks used in these studies, adopting the kind of diverse, generic corpora on which the tools have been trained, might not be the most relevant option for specialized corpus parsing. Moreover, even though some of these campaigns are recent, the main tools available have not been compared on the same evaluation sets. We previously performed a first study (Tanguy 2. Overview To go beyond the limitation in (Tanguy et al.,"
2020.computerm-1.4,ferret-2010-testing,1,0.836635,"ery limited among predictive approaches. More importantly, some recent studies (Pierrejean and Tanguy, 2018) have shown that predictive approaches are unstable to some extent concerning the search of the nearest distributional neighbors of a word. Since we want specifically to concentrate on the effects resulting from the use of different syntactic parsers, we adopted a count-based approach. We implemented this approach by building on the findings of recent studies in the field (Kiela and Clark, 2014; Baroni et al., 2014; Levy et al., 2015) and more particularly took up two main options from (Ferret, 2010): the use of Positive Pointwise Mutual Information (PPMI) for weighting the context elements and the application of very loose filtering that removes the elements of these contexts with only one occurrence. The second choice is justified by both the fairly small size of our target corpus and the experiments of (Ferret, 2010) with linear co-occurrents. The main particularity of our work is the fact that the entries of our distri31 0.65 0.70 0.75 0.80 0.85 0.65 0.70 0.75 Talismane 0.80 0.85 Talismane Spacy UDpipe−Partut UDpipe−Ewt StanfordNLP−Lines UDpipe−Gum UDpipe−Lines StanfordNLP−Ewt Spacy S"
2020.computerm-1.4,C96-1083,0,0.407523,"for specialized corpora. This type of corpora are usually small-sized (a few million words or less), which poses a challenge for distributional methods, and contain specific, highly technical vocabulary, meaning that adapting methods based on large generic corpora might be difficult. We make the hypothesis, supported by the work of (Tanguy et al., 2015), that the small amount of data may be circumvented by a method based on syntactic contexts. Such methods have already been investigated by a large body of work. The largest part of it is dedicated to count-based approaches (Grefenstette, 1994; Habert et al., 1996; Lin, 1998; Curran and Moens, 2002; Padó and Lapata, 2007; Baroni and Lenci, 2010) but it also includes work adding dimensionality reduction methods (Lapesa and Evert, 2017) or more recently, work about word embeddings (Levy and Goldberg, 2014). One of our focuses is to select the best-suited tools for semantic analysis of specialized corpora. In particular, given that syntactic contexts will be a building block for the task, which syntactic parser should be used to extract these contexts? The goal of this article is, thus, to study the impact of the choice of parser on the construction of a"
2020.computerm-1.4,D15-1162,0,0.020083,"thon, implements an entirely different parsing chain. Its graphbased parser relies on a LSTM neural network. StanfordNLP offers 3 English models, trained on the UD EWT (Silveira et al., 2014), LinES (Ahrenberg, 2015) and ParTUT (Bosco et al., 2012) corpora. We used all three of these models. Spacy is an industry-targeting tool whose main characteristic is its speed compared to most other parsers. The tagger is based on a perceptron, with attributes based on Brown clusters, following (Koo et al., 2008). It implements a non-monotonous transition-based parser which can revise previous decisions (Honnibal and Johnson, 2015). The default model we used was 1 2 Terminological Reference Resource https://github.com/clir/ clearnlp-guidelines/blob/master/md/ specifications/dependency_labels.md 3 https://www.ncbi.nlm.nih.gov/books/ NBK9684/#_ch02_sec2_4_ http://www.ncbi.nlm.nih.gov/pmc 27 than the REL labels, with nearly 900 different labels in total, and we cannot assume that they are coherently used throughout the Metathesaurus. We observed that the distribution of relations among concepts (CUI) is not very well balanced: score based on 4 metrics: centrality, variation, coverage, and cohesiveness. The mapping with the"
2020.computerm-1.4,N06-2015,0,0.0630525,"Missing"
2020.computerm-1.4,W14-1503,0,0.0170472,"his choice is that, except for (Levy and Goldberg, 2014), the number of studies relying on dependency relations is very limited among predictive approaches. More importantly, some recent studies (Pierrejean and Tanguy, 2018) have shown that predictive approaches are unstable to some extent concerning the search of the nearest distributional neighbors of a word. Since we want specifically to concentrate on the effects resulting from the use of different syntactic parsers, we adopted a count-based approach. We implemented this approach by building on the findings of recent studies in the field (Kiela and Clark, 2014; Baroni et al., 2014; Levy et al., 2015) and more particularly took up two main options from (Ferret, 2010): the use of Positive Pointwise Mutual Information (PPMI) for weighting the context elements and the application of very loose filtering that removes the elements of these contexts with only one occurrence. The second choice is justified by both the fairly small size of our target corpus and the experiments of (Ferret, 2010) with linear co-occurrents. The main particularity of our work is the fact that the entries of our distri31 0.65 0.70 0.75 0.80 0.85 0.65 0.70 0.75 Talismane 0.80 0.8"
2020.computerm-1.4,P08-1068,0,0.0384987,"-based parser. StanfordNLP (Qi et al., 2018) is a tool that, on top of giving access to the CoreNLP chain in Python, implements an entirely different parsing chain. Its graphbased parser relies on a LSTM neural network. StanfordNLP offers 3 English models, trained on the UD EWT (Silveira et al., 2014), LinES (Ahrenberg, 2015) and ParTUT (Bosco et al., 2012) corpora. We used all three of these models. Spacy is an industry-targeting tool whose main characteristic is its speed compared to most other parsers. The tagger is based on a perceptron, with attributes based on Brown clusters, following (Koo et al., 2008). It implements a non-monotonous transition-based parser which can revise previous decisions (Honnibal and Johnson, 2015). The default model we used was 1 2 Terminological Reference Resource https://github.com/clir/ clearnlp-guidelines/blob/master/md/ specifications/dependency_labels.md 3 https://www.ncbi.nlm.nih.gov/books/ NBK9684/#_ch02_sec2_4_ http://www.ncbi.nlm.nih.gov/pmc 27 than the REL labels, with nearly 900 different labels in total, and we cannot assume that they are coherently used throughout the Metathesaurus. We observed that the distribution of relations among concepts (CUI) is"
2020.computerm-1.4,E17-2063,0,0.0164387,"fic, highly technical vocabulary, meaning that adapting methods based on large generic corpora might be difficult. We make the hypothesis, supported by the work of (Tanguy et al., 2015), that the small amount of data may be circumvented by a method based on syntactic contexts. Such methods have already been investigated by a large body of work. The largest part of it is dedicated to count-based approaches (Grefenstette, 1994; Habert et al., 1996; Lin, 1998; Curran and Moens, 2002; Padó and Lapata, 2007; Baroni and Lenci, 2010) but it also includes work adding dimensionality reduction methods (Lapesa and Evert, 2017) or more recently, work about word embeddings (Levy and Goldberg, 2014). One of our focuses is to select the best-suited tools for semantic analysis of specialized corpora. In particular, given that syntactic contexts will be a building block for the task, which syntactic parser should be used to extract these contexts? The goal of this article is, thus, to study the impact of the choice of parser on the construction of a distributional model with a frequency-based method. Our work is not the first work on comparing different parsers. Several evaluation campaigns were previously organized for"
2020.computerm-1.4,P14-2050,0,0.249065,"on large generic corpora might be difficult. We make the hypothesis, supported by the work of (Tanguy et al., 2015), that the small amount of data may be circumvented by a method based on syntactic contexts. Such methods have already been investigated by a large body of work. The largest part of it is dedicated to count-based approaches (Grefenstette, 1994; Habert et al., 1996; Lin, 1998; Curran and Moens, 2002; Padó and Lapata, 2007; Baroni and Lenci, 2010) but it also includes work adding dimensionality reduction methods (Lapesa and Evert, 2017) or more recently, work about word embeddings (Levy and Goldberg, 2014). One of our focuses is to select the best-suited tools for semantic analysis of specialized corpora. In particular, given that syntactic contexts will be a building block for the task, which syntactic parser should be used to extract these contexts? The goal of this article is, thus, to study the impact of the choice of parser on the construction of a distributional model with a frequency-based method. Our work is not the first work on comparing different parsers. Several evaluation campaigns were previously organized for various languages: the Easy (Paroubek et al., 2008), Passage (De La Cle"
2020.computerm-1.4,P98-2127,0,0.6986,"ra. This type of corpora are usually small-sized (a few million words or less), which poses a challenge for distributional methods, and contain specific, highly technical vocabulary, meaning that adapting methods based on large generic corpora might be difficult. We make the hypothesis, supported by the work of (Tanguy et al., 2015), that the small amount of data may be circumvented by a method based on syntactic contexts. Such methods have already been investigated by a large body of work. The largest part of it is dedicated to count-based approaches (Grefenstette, 1994; Habert et al., 1996; Lin, 1998; Curran and Moens, 2002; Padó and Lapata, 2007; Baroni and Lenci, 2010) but it also includes work adding dimensionality reduction methods (Lapesa and Evert, 2017) or more recently, work about word embeddings (Levy and Goldberg, 2014). One of our focuses is to select the best-suited tools for semantic analysis of specialized corpora. In particular, given that syntactic contexts will be a building block for the task, which syntactic parser should be used to extract these contexts? The goal of this article is, thus, to study the impact of the choice of parser on the construction of a distributio"
2020.computerm-1.4,P14-5010,0,0.00265534,". those that take in charge the whole processing chain, from raw text to dependencies. These tools were applied with their default options. All these tools use statistical models trained on annotated corpora. Their differences concern implementation choices like parsing techniques (graph- or transition-based, for instance), learning models (SVM, maximal entropy or more recently, recurrent neural networks), and upstream or side processing (segmentation, lemmatization). There is much less choice among the training corpora, given the high cost of the annotation and validation processes. CoreNLP (Manning et al., 2014), the main tool of the Stanford team, implements a maximum entropy tagger, which uses the Penn Treebank tagset (Marcus et al., 1993), and a transition-based parser. StanfordNLP (Qi et al., 2018) is a tool that, on top of giving access to the CoreNLP chain in Python, implements an entirely different parsing chain. Its graphbased parser relies on a LSTM neural network. StanfordNLP offers 3 English models, trained on the UD EWT (Silveira et al., 2014), LinES (Ahrenberg, 2015) and ParTUT (Bosco et al., 2012) corpora. We used all three of these models. Spacy is an industry-targeting tool whose main"
2020.computerm-1.4,J93-2004,0,0.0730662,"tions. All these tools use statistical models trained on annotated corpora. Their differences concern implementation choices like parsing techniques (graph- or transition-based, for instance), learning models (SVM, maximal entropy or more recently, recurrent neural networks), and upstream or side processing (segmentation, lemmatization). There is much less choice among the training corpora, given the high cost of the annotation and validation processes. CoreNLP (Manning et al., 2014), the main tool of the Stanford team, implements a maximum entropy tagger, which uses the Penn Treebank tagset (Marcus et al., 1993), and a transition-based parser. StanfordNLP (Qi et al., 2018) is a tool that, on top of giving access to the CoreNLP chain in Python, implements an entirely different parsing chain. Its graphbased parser relies on a LSTM neural network. StanfordNLP offers 3 English models, trained on the UD EWT (Silveira et al., 2014), LinES (Ahrenberg, 2015) and ParTUT (Bosco et al., 2012) corpora. We used all three of these models. Spacy is an industry-targeting tool whose main characteristic is its speed compared to most other parsers. The tagger is based on a perceptron, with attributes based on Brown clu"
2020.computerm-1.4,J07-2002,0,0.456909,"y small-sized (a few million words or less), which poses a challenge for distributional methods, and contain specific, highly technical vocabulary, meaning that adapting methods based on large generic corpora might be difficult. We make the hypothesis, supported by the work of (Tanguy et al., 2015), that the small amount of data may be circumvented by a method based on syntactic contexts. Such methods have already been investigated by a large body of work. The largest part of it is dedicated to count-based approaches (Grefenstette, 1994; Habert et al., 1996; Lin, 1998; Curran and Moens, 2002; Padó and Lapata, 2007; Baroni and Lenci, 2010) but it also includes work adding dimensionality reduction methods (Lapesa and Evert, 2017) or more recently, work about word embeddings (Levy and Goldberg, 2014). One of our focuses is to select the best-suited tools for semantic analysis of specialized corpora. In particular, given that syntactic contexts will be a building block for the task, which syntactic parser should be used to extract these contexts? The goal of this article is, thus, to study the impact of the choice of parser on the construction of a distributional model with a frequency-based method. Our wo"
2020.computerm-1.4,paroubek-etal-2008-easy,0,0.0465426,"word embeddings (Levy and Goldberg, 2014). One of our focuses is to select the best-suited tools for semantic analysis of specialized corpora. In particular, given that syntactic contexts will be a building block for the task, which syntactic parser should be used to extract these contexts? The goal of this article is, thus, to study the impact of the choice of parser on the construction of a distributional model with a frequency-based method. Our work is not the first work on comparing different parsers. Several evaluation campaigns were previously organized for various languages: the Easy (Paroubek et al., 2008), Passage (De La Clergerie et al., 2008), SPMRL (Seddah et al., 2013) and CoNLL (Zeman et al., 2018) campaigns as well as more focused studies like (Candito et al., 2010) or (De La Clergerie, 2014). However, the benchmarks used in these studies, adopting the kind of diverse, generic corpora on which the tools have been trained, might not be the most relevant option for specialized corpus parsing. Moreover, even though some of these campaigns are recent, the main tools available have not been compared on the same evaluation sets. We previously performed a first study (Tanguy 2. Overview To go b"
2020.computerm-1.4,N18-4005,1,0.829744,"to StanfordNLP-Ewt while UDpipe-Gum and UDpipe-Ewt form a group for the two sets. Distributional Models 5.1. Building of Distributional Models Following the distinction made in Baroni et al. (2014), we built our distributional models according to a countbased approach, such as in (Lin, 1998), rather than according to a predictive approach such as in (Mikolov et al., 2013). The first justification of this choice is that, except for (Levy and Goldberg, 2014), the number of studies relying on dependency relations is very limited among predictive approaches. More importantly, some recent studies (Pierrejean and Tanguy, 2018) have shown that predictive approaches are unstable to some extent concerning the search of the nearest distributional neighbors of a word. Since we want specifically to concentrate on the effects resulting from the use of different syntactic parsers, we adopted a count-based approach. We implemented this approach by building on the findings of recent studies in the field (Kiela and Clark, 2014; Baroni et al., 2014; Levy et al., 2015) and more particularly took up two main options from (Ferret, 2010): the use of Positive Pointwise Mutual Information (PPMI) for weighting the context elements an"
2020.computerm-1.4,W13-4917,0,0.0322928,"Missing"
2020.computerm-1.4,silveira-etal-2014-gold,0,0.0461894,"Missing"
2020.computerm-1.4,K17-3009,0,0.0158531,"e with their themes (or subject headings), with an indication of whether the theme is a main theme of the article or not. To obtain a corpus that was the right size for our purposes, we chose to include any article that was tagged with a heading containing the words ”Stem Cells“, which includes headings such as ”Stem Cells“, ”Adult Stem Cells“, ”Totipotent Stem Cells“, ”Mouse Embryonic Stem Cells“, and others. This was done regardless of whether the heading was indicated as a main theme of the article or not. The resulting corpus is comprised of 23,094 articles, and 104 million words. UDPipe (Straka and Straková, 2017) uses a neural network with a Gated Recurrent Unit mechanism to do both tokenization and segmentation at once. For PoS tagging, it generates possible tags for words from their suffix and performs disambiguation with a perceptron. The transition-based parsing relies on a simple one-layer neural network. UDPipe includes four English models. We used all of them, trained on the UD GUM (Zeldes, 2017), EWT, LinES and ParTUT corpora. 3.2. The UMLS is a set of knowledge sources related to biomedical sciences. The main part of the system is the UMLS Metathesaurus, which aggregates nearly 200 biomedical"
2020.computerm-1.4,2020.lrec-1.713,1,0.467381,"e et al., 2008), SPMRL (Seddah et al., 2013) and CoNLL (Zeman et al., 2018) campaigns as well as more focused studies like (Candito et al., 2010) or (De La Clergerie, 2014). However, the benchmarks used in these studies, adopting the kind of diverse, generic corpora on which the tools have been trained, might not be the most relevant option for specialized corpus parsing. Moreover, even though some of these campaigns are recent, the main tools available have not been compared on the same evaluation sets. We previously performed a first study (Tanguy 2. Overview To go beyond the limitation in (Tanguy et al., 2020), we have chosen, in the work we present in this article, to run a new evaluation on a small, specialized biomedical corpus, whose building is described in Section 3.1 and for which we may compare the relations implied by the extracted syntactic contexts against an external resource, the Unified Medical Language System (UMLS) (Bodenreider, 2004), which contains relations between medical and biomedical concepts (see Section 3.3). More precisely, we defined the following process: we applied each of the 10 studied parsers we present in Section 3.2 to the corpus, outputting morphological, syntacti"
2020.computerm-1.4,K18-2001,0,0.0121781,"emantic analysis of specialized corpora. In particular, given that syntactic contexts will be a building block for the task, which syntactic parser should be used to extract these contexts? The goal of this article is, thus, to study the impact of the choice of parser on the construction of a distributional model with a frequency-based method. Our work is not the first work on comparing different parsers. Several evaluation campaigns were previously organized for various languages: the Easy (Paroubek et al., 2008), Passage (De La Clergerie et al., 2008), SPMRL (Seddah et al., 2013) and CoNLL (Zeman et al., 2018) campaigns as well as more focused studies like (Candito et al., 2010) or (De La Clergerie, 2014). However, the benchmarks used in these studies, adopting the kind of diverse, generic corpora on which the tools have been trained, might not be the most relevant option for specialized corpus parsing. Moreover, even though some of these campaigns are recent, the main tools available have not been compared on the same evaluation sets. We previously performed a first study (Tanguy 2. Overview To go beyond the limitation in (Tanguy et al., 2020), we have chosen, in the work we present in this articl"
2020.jeptalnrecital-taln.35,D19-1588,0,0.0215614,"Missing"
2020.jeptalnrecital-taln.35,N16-1030,0,0.0893794,"Missing"
2020.jeptalnrecital-taln.35,D17-1018,0,0.0457912,"Missing"
2020.jeptalnrecital-taln.35,P14-2006,0,0.0308576,"Missing"
2020.jeptalnrecital-taln.35,W11-1901,0,0.0594136,"Missing"
2020.jeptalnrecital-taln.35,D17-1035,0,0.0201615,"Missing"
2020.jeptalnrecital-taln.35,Q14-1012,0,0.0453975,"Missing"
2020.jeptalnrecital-taln.35,M95-1002,0,0.272925,"Missing"
2020.jeptalnrecital-taln.35,P15-1137,0,0.0484465,"Missing"
2020.jeptalnrecital-taln.35,N16-1114,0,0.0388283,"Missing"
2020.jeptalnrecital-taln.35,P19-1083,0,0.0606537,"Missing"
2020.jeptalnrecital-taln.4,D18-1158,0,0.0224858,"Missing"
2020.jeptalnrecital-taln.4,I17-1036,0,0.0291079,"Missing"
2020.jeptalnrecital-taln.4,L18-1245,0,0.0608826,"Missing"
2020.jeptalnrecital-taln.4,P11-1113,0,0.0435231,"Missing"
2020.jeptalnrecital-taln.4,P10-1081,0,0.0956002,"Missing"
2020.jeptalnrecital-taln.4,P14-5010,0,0.0042933,"Missing"
2020.jeptalnrecital-taln.4,D17-1159,0,0.0595914,"Missing"
2020.jeptalnrecital-taln.4,D18-1122,0,0.0586629,"Missing"
2020.jeptalnrecital-taln.4,D14-1162,0,0.0872222,"Missing"
2020.jeptalnrecital-taln.4,N12-1008,0,0.0695673,"Missing"
2020.jeptalnrecital-taln.4,D10-1001,0,0.013662,"Missing"
2020.jeptalnrecital-taln.4,N16-1033,0,0.0562206,"Missing"
2020.jeptalnrecital-taln.4,P18-2066,0,0.0319641,"Missing"
2020.lrec-1.528,D07-1074,0,0.0739626,"nges to NLP researchers since, despite a large amount of available data, social media posts are often short and noisy, making information extraction tasks more difficult. Among these tasks, one key processing step is to map a named mention (also known as surface form) within a text to an actual entity defined in a knowledge base. This task is referred to as Entity Linking (EL) or named entity disambiguation (NED). Applications such as event extraction, knowledge base population, and relation extraction can directly benefit from Entity linking. Early approaches of EL (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010) exploit lexical entity mention similarities, entity mention context similarities and prior information about entity candidates in the KB to rank them, while global approaches (Ratinov et al., 2011; Guo and Barbosa, 2014; Pershina et al., 2015; Globerson et al., 2016) leverage all kinds of relational features between mentions within a document and between entities in a knowledge base to globally resolve them. The Entity linking task has been traditionally achieved on documents such as newspaper articles, which is rich in textual content but general"
2020.lrec-1.528,D18-1227,0,0.0125974,"es, RSS500 (R¨oder et al., 2014) contains articles from international newspapers, OKE2015 (Nuzzolese et al., 2015) uses sentences from Wikipedia articles, the different Entity Discovery and Linking (EDL) tasks from the TAC campaigns use newswires along with broadcast conversation or discussion forums (Ji et al., 2010). Concerning social media, (Rizzo et al., 2015) proposed the NEEL2015 Microposts dataset composed of event-annotated tweets. One can note that all the aforementioned datasets link entities to general-domain knowledge bases, typically DBpedia or Freebase. More similar to our work, Dai et al. (2018) proposed to link mentions to entities defined in a specific social media KB. They constructed the Yelp-EL dataset from the Yelp platform where mentions in Yelp reviews are linked to Yelp business entities, but this corpus does not include visual information. Entity linking on tweets. While the EL task was initially defined for documents such as newspaper articles, it was also applied to new textual forms such as tweets. In this context, collective approaches such as (Huang et al., 2014) use mention co-referencing to collectively resolve them while Liu et al. (2013) measure mention-mention and"
2020.lrec-1.528,C10-1032,0,0.0460433,"large amount of available data, social media posts are often short and noisy, making information extraction tasks more difficult. Among these tasks, one key processing step is to map a named mention (also known as surface form) within a text to an actual entity defined in a knowledge base. This task is referred to as Entity Linking (EL) or named entity disambiguation (NED). Applications such as event extraction, knowledge base population, and relation extraction can directly benefit from Entity linking. Early approaches of EL (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010) exploit lexical entity mention similarities, entity mention context similarities and prior information about entity candidates in the KB to rank them, while global approaches (Ratinov et al., 2011; Guo and Barbosa, 2014; Pershina et al., 2015; Globerson et al., 2016) leverage all kinds of relational features between mentions within a document and between entities in a knowledge base to globally resolve them. The Entity linking task has been traditionally achieved on documents such as newspaper articles, which is rich in textual content but generally lacks visual information that could be used"
2020.lrec-1.528,Q14-1021,0,0.016783,"ents such as newspaper articles, it was also applied to new textual forms such as tweets. In this context, collective approaches such as (Huang et al., 2014) use mention co-referencing to collectively resolve them while Liu et al. (2013) measure mention-mention and mention-entity similarities from groups of related tweets. Shen et al. (2013) propose a graph-based approach to model the interaction between the topic of interest of Twitter users and all KB entities. Other approaches combine user’s social features (user’s interest and popularity) and temporal reasoning such in (Hua et al., 2015). Fang and Chang (2014) and Chong et al. (2017) extend the context of a target mention to tweets that are close in space and time to the tweet of the target mention. Multimodal corpora. Many corpora provide images with associated textual content, in particular for the tasks of automatic image annotation (Young et al., 2014; Ginsca et al., 2015), cross-media retrieval (Karpathy and Fei-Fei, 2015; Tran et al., 2016a), image-sentence matching (Hodosh et al., 2013; Ordonez et al., 2011), text illustration (Feng and Lapata, 2010; Chami et al., 2017) and cross-media classification (Tran et al., 2016b; Tamaazousti et al.,"
2020.lrec-1.528,N10-1125,0,0.0375168,"features (user’s interest and popularity) and temporal reasoning such in (Hua et al., 2015). Fang and Chang (2014) and Chong et al. (2017) extend the context of a target mention to tweets that are close in space and time to the tweet of the target mention. Multimodal corpora. Many corpora provide images with associated textual content, in particular for the tasks of automatic image annotation (Young et al., 2014; Ginsca et al., 2015), cross-media retrieval (Karpathy and Fei-Fei, 2015; Tran et al., 2016a), image-sentence matching (Hodosh et al., 2013; Ordonez et al., 2011), text illustration (Feng and Lapata, 2010; Chami et al., 2017) and cross-media classification (Tran et al., 2016b; Tamaazousti et al., 2017). Most corpora used in this context consist in images with captions from Flickr (Ordonez et al., 2011; Hodosh et al., 2013; Young et al., 2014) or using Amazon’s Mechanical Turk (Rashtchian et al., 2010; Lin et al., 2014). Other authors nevertheless preferred to build datasets of images with captions in a real context, that is to say extracted from real news articles (Feng and Lapata, 2010; Tirilly et al., 2010; Hollink et al., 2016). In terms of size, these datasets contain from few thousands (F"
2020.lrec-1.528,P16-1059,0,0.0178354,"fined in a knowledge base. This task is referred to as Entity Linking (EL) or named entity disambiguation (NED). Applications such as event extraction, knowledge base population, and relation extraction can directly benefit from Entity linking. Early approaches of EL (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010) exploit lexical entity mention similarities, entity mention context similarities and prior information about entity candidates in the KB to rank them, while global approaches (Ratinov et al., 2011; Guo and Barbosa, 2014; Pershina et al., 2015; Globerson et al., 2016) leverage all kinds of relational features between mentions within a document and between entities in a knowledge base to globally resolve them. The Entity linking task has been traditionally achieved on documents such as newspaper articles, which is rich in textual content but generally lacks visual information that could be used in the EL task. On the other hand, social media posts like tweets provide poor and noisy textual contexts which make the Entity linking task harder, but is often associated with complementary visual information. In this paper, we propose to automatically build large-"
2020.lrec-1.528,D11-1072,0,0.182492,"Missing"
2020.lrec-1.528,L16-1219,0,0.0423041,"Missing"
2020.lrec-1.528,P14-1036,0,0.0218804,"ets link entities to general-domain knowledge bases, typically DBpedia or Freebase. More similar to our work, Dai et al. (2018) proposed to link mentions to entities defined in a specific social media KB. They constructed the Yelp-EL dataset from the Yelp platform where mentions in Yelp reviews are linked to Yelp business entities, but this corpus does not include visual information. Entity linking on tweets. While the EL task was initially defined for documents such as newspaper articles, it was also applied to new textual forms such as tweets. In this context, collective approaches such as (Huang et al., 2014) use mention co-referencing to collectively resolve them while Liu et al. (2013) measure mention-mention and mention-entity similarities from groups of related tweets. Shen et al. (2013) propose a graph-based approach to model the interaction between the topic of interest of Twitter users and all KB entities. Other approaches combine user’s social features (user’s interest and popularity) and temporal reasoning such in (Hua et al., 2015). Fang and Chang (2014) and Chong et al. (2017) extend the context of a target mention to tweets that are close in space and time to the tweet of the target me"
2020.lrec-1.528,Q15-1023,0,0.0218725,"l be released at https://github.com/OA256864/MEL_Tweets 2. Related Work The MEL task is intrinsically related to several research domains: first, EL from text, which is characterized by a set of evaluation frameworks. But EL was also applied to specific forms of text such as social media posts. Finally, MEL is closely related to work about multimedia as it combines text and image contents. Since we propose a new corpus, we particularly focus on the corpora developed in these three areas. Entity linking corpora. Existing corpora for evaluating entity linking systems are thoroughly reviewed in (Ling et al., 2015; Usbeck et al., 2015; Van Erp et al., 2016; Rosales-M´endez, 2019). They were mostly constructed from news articles: AIDA-YAGO2 (Hoffart et al., 2011) uses the Reuters newswire articles also used in the CoNLL 2003 shared task on named entity recognition, MEANTIME (Minard et al., 2016) uses Wikinews articles, RSS500 (R¨oder et al., 2014) contains articles from international newspapers, OKE2015 (Nuzzolese et al., 2015) uses sentences from Wikipedia articles, the different Entity Discovery and Linking (EDL) tasks from the TAC campaigns use newswires along with broadcast conversation or discussio"
2020.lrec-1.528,P13-1128,0,0.0108287,". More similar to our work, Dai et al. (2018) proposed to link mentions to entities defined in a specific social media KB. They constructed the Yelp-EL dataset from the Yelp platform where mentions in Yelp reviews are linked to Yelp business entities, but this corpus does not include visual information. Entity linking on tweets. While the EL task was initially defined for documents such as newspaper articles, it was also applied to new textual forms such as tweets. In this context, collective approaches such as (Huang et al., 2014) use mention co-referencing to collectively resolve them while Liu et al. (2013) measure mention-mention and mention-entity similarities from groups of related tweets. Shen et al. (2013) propose a graph-based approach to model the interaction between the topic of interest of Twitter users and all KB entities. Other approaches combine user’s social features (user’s interest and popularity) and temporal reasoning such in (Hua et al., 2015). Fang and Chang (2014) and Chong et al. (2017) extend the context of a target mention to tweets that are close in space and time to the tweet of the target mention. Multimodal corpora. Many corpora provide images with associated textual c"
2020.lrec-1.528,L16-1699,0,0.0517092,"Missing"
2020.lrec-1.528,P18-1186,0,0.0232207,"cted from real news articles (Feng and Lapata, 2010; Tirilly et al., 2010; Hollink et al., 2016). In terms of size, these datasets contain from few thousands (Feng and Lapata, 2010; Rashtchian et al., 2010; Hodosh et al., 2013) to several hundred thousand (Hollink et al., 2016) and even one million of captioned images (Ordonez et al., 2011). Other corpora have also been created for more specific usages such as visual question answering (Antol et al., 2015), visual dialogs (Das et al., 2017) or understanding the interactions and relationships between objects in an image (Krishna et al., 2017). Moon et al. (2018) addressed the task of multimedia entity linking and evaluate their approach on a corpus of 12K user-generated image and textual caption pairs from Snapshat, where mentions are linked to the general-domain Freebase KB. However, the evaluation corpus was not released and the authors provide little information on its char4286 acteristics and the method used to build it. To the best of our knowledge, our approach is thus the first to allow to build a dataset for MEL evaluation, with full access to image and corresponding text, and usable for reproducible researches. 3. MEL Task Definition As in s"
2020.lrec-1.528,N18-1049,0,0.0247563,"s representation of their textual contexts using an unsupervised language model. Moreover, the visual contexts of mi and ei are determined using a pre-trained convolution neural network employed as an image feature extractor. We additionally provide other traditional features based on popularity and BM25 similarity. We use the ExtraTrees classifier to combine these features, perform classification over the mention-entity pairs and select the best entity among candidate entities. Textual Context Features For mention and entity textual context representations, we used the unsupervised Sent2Vec (Pagliardini et al., 2018) 4288 Figure 3: KB entities examples. Each entity in the knowledge base represents a twitter user characterized by its timeline (set of pair of text-image). We assume that combining visual and textual contexts of each entity helps discriminating entities for better EL performance. sentence embedding model, and more precisely, a pretrained version on a large Twitter corpus6 . We adopted this model because we observed, in preliminary experiments, that representations built from the same type of data as ours (social media posts in our case) give better results. Therefore, the textual context of a"
2020.lrec-1.528,N15-1026,0,0.0185726,"to an actual entity defined in a knowledge base. This task is referred to as Entity Linking (EL) or named entity disambiguation (NED). Applications such as event extraction, knowledge base population, and relation extraction can directly benefit from Entity linking. Early approaches of EL (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010) exploit lexical entity mention similarities, entity mention context similarities and prior information about entity candidates in the KB to rank them, while global approaches (Ratinov et al., 2011; Guo and Barbosa, 2014; Pershina et al., 2015; Globerson et al., 2016) leverage all kinds of relational features between mentions within a document and between entities in a knowledge base to globally resolve them. The Entity linking task has been traditionally achieved on documents such as newspaper articles, which is rich in textual content but generally lacks visual information that could be used in the EL task. On the other hand, social media posts like tweets provide poor and noisy textual contexts which make the Entity linking task harder, but is often associated with complementary visual information. In this paper, we propose to a"
2020.lrec-1.528,P11-1138,0,0.0407317,"on (also known as surface form) within a text to an actual entity defined in a knowledge base. This task is referred to as Entity Linking (EL) or named entity disambiguation (NED). Applications such as event extraction, knowledge base population, and relation extraction can directly benefit from Entity linking. Early approaches of EL (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Milne and Witten, 2008; Dredze et al., 2010) exploit lexical entity mention similarities, entity mention context similarities and prior information about entity candidates in the KB to rank them, while global approaches (Ratinov et al., 2011; Guo and Barbosa, 2014; Pershina et al., 2015; Globerson et al., 2016) leverage all kinds of relational features between mentions within a document and between entities in a knowledge base to globally resolve them. The Entity linking task has been traditionally achieved on documents such as newspaper articles, which is rich in textual content but generally lacks visual information that could be used in the EL task. On the other hand, social media posts like tweets provide poor and noisy textual contexts which make the Entity linking task harder, but is often associated with complementary visu"
2020.lrec-1.528,roder-etal-2014-n3,0,0.0540912,"Missing"
2020.lrec-1.528,tirilly-etal-2010-news,0,0.0393798,"age-sentence matching (Hodosh et al., 2013; Ordonez et al., 2011), text illustration (Feng and Lapata, 2010; Chami et al., 2017) and cross-media classification (Tran et al., 2016b; Tamaazousti et al., 2017). Most corpora used in this context consist in images with captions from Flickr (Ordonez et al., 2011; Hodosh et al., 2013; Young et al., 2014) or using Amazon’s Mechanical Turk (Rashtchian et al., 2010; Lin et al., 2014). Other authors nevertheless preferred to build datasets of images with captions in a real context, that is to say extracted from real news articles (Feng and Lapata, 2010; Tirilly et al., 2010; Hollink et al., 2016). In terms of size, these datasets contain from few thousands (Feng and Lapata, 2010; Rashtchian et al., 2010; Hodosh et al., 2013) to several hundred thousand (Hollink et al., 2016) and even one million of captioned images (Ordonez et al., 2011). Other corpora have also been created for more specific usages such as visual question answering (Antol et al., 2015), visual dialogs (Das et al., 2017) or understanding the interactions and relationships between objects in an image (Krishna et al., 2017). Moon et al. (2018) addressed the task of multimedia entity linking and ev"
2020.lrec-1.528,L16-1693,0,0.0349981,"Missing"
2020.lrec-1.528,Q14-1006,0,0.0475257,"s of related tweets. Shen et al. (2013) propose a graph-based approach to model the interaction between the topic of interest of Twitter users and all KB entities. Other approaches combine user’s social features (user’s interest and popularity) and temporal reasoning such in (Hua et al., 2015). Fang and Chang (2014) and Chong et al. (2017) extend the context of a target mention to tweets that are close in space and time to the tweet of the target mention. Multimodal corpora. Many corpora provide images with associated textual content, in particular for the tasks of automatic image annotation (Young et al., 2014; Ginsca et al., 2015), cross-media retrieval (Karpathy and Fei-Fei, 2015; Tran et al., 2016a), image-sentence matching (Hodosh et al., 2013; Ordonez et al., 2011), text illustration (Feng and Lapata, 2010; Chami et al., 2017) and cross-media classification (Tran et al., 2016b; Tamaazousti et al., 2017). Most corpora used in this context consist in images with captions from Flickr (Ordonez et al., 2011; Hodosh et al., 2013; Young et al., 2014) or using Amazon’s Mechanical Turk (Rashtchian et al., 2010; Lin et al., 2014). Other authors nevertheless preferred to build datasets of images with cap"
2020.lrec-1.713,J10-4006,0,0.0217882,"e (schématisons, schématiser). This robust method can process all cases in a homogeneous and deterministic way. Thereafter, words are represented by their lemma and their POS tag. We identified 5,580 open-class words with at least 5 occurrences in each of the 11 outputs. Our comparison will use this set of words as a base. 3.2. Extraction of Syntactic Triplets The next step is to extract the dependency relations between words, which will serve as a representation of the words’ contexts for distributional analysis, following a long tradition (Lin, 1998; Bourigault, 2002; Padó and Lapata, 2007; Baroni and Lenci, 2010; Lapesa and Evert, 2017). In all these studies, the most common representation of the contexts in which a word occurs is a set of syntactic triplets (dependent_word, relation, governor_word). For instance, from the sentence &quot;Nous avons utilisé un analyseur syntaxique&quot; (&quot;We used a syntactic parser&quot;), considered as correctly parsed, we can extract the triplets (analyseur, obj (direct object), utiliser) and (syntaxique, mod (modifier), analyseur). From each triplet, we can produce two contextual representations: one for the dependent, and the other for the governor, combining the other word with"
2020.lrec-1.713,P14-1023,0,0.0822924,"les (present and past: ADJ or V); 0.05 • POS-tagging of N-N compounds (which are rare in French but can be common in technical texts): candidat terme [candidate term], langue cible [target language], vecteur contexte [context vector]): tagged as N-N, ADJ-N, N-ADJ or other. 0.10 0.15 0.20 0.25 0.30 MST (Bonsai) Talismane−FTB Spacy UDpipe−Partut Without further effort to harmonize the outputs for these cases, we used these triplets to build distributional models. 4. 4.1. UDpipe−Sequoia CoreNLP Comparison of Distributional Models Talismane−UD Building the Models Following the distinction made in Baroni et al. (2014), we built our distributional models according to a countbased approach, such as in (Lin, 1998), rather than according to a predictive model such as in (Mikolov et al., 2013). The first justification of this choice is that, except for (Levy and Goldberg, 2014), the number of studies relying on dependency relations is very limited among predictive approaches. More importantly, some recent studies (Pierrejean and Tanguy, 2018) have shown that predictive approaches are unstable to some extent concerning the nearest distributional neighbors of a word. Since we specifically want to concentrate on t"
2020.lrec-1.713,K18-2017,0,0.0517374,"Missing"
2020.lrec-1.713,bosco-etal-2012-parallel,0,0.332109,"rk. Its main particularity is that syntactic parsing occurs independently from the POS tagging, both of them using only lexicalized attributes and no morphological information. It must be noted that this tool is significantly slower than the others. We have not found any precise information on the corpora used to train the model: we assume that the sum of all available French UD corpora was used. UD French FTB is the result of the conversion to UD of the original French Treebank and is made of around 550,000 words. UD French ParTUT is the French subset of the Parallel-TUT multilingual corpus (Bosco et al., 2012), which is composed of varied text samples (legal texts, Wikipedia articles, Facebook pages, etc.) for a total of approximately 30,000 words. Spacy is an all-purpose NLP tool designed for industrial applications whose main characteristic is its high speed compared to most other parsers. The tagger is based on a perceptron, with attributes based on Brown clusters, following (Koo et al., 2008). It implements a non-monotonous transition-based parser that can revise previous decisions (Honnibal and Johnson, 2015). The available model was trained on the WikiNER corpus (Nothman et al., 2012) for Nam"
2020.lrec-1.713,2002.jeptalnrecital-long.5,0,0.139686,"lemma) pairs sharing the same ending like (schématisons, schématiser). This robust method can process all cases in a homogeneous and deterministic way. Thereafter, words are represented by their lemma and their POS tag. We identified 5,580 open-class words with at least 5 occurrences in each of the 11 outputs. Our comparison will use this set of words as a base. 3.2. Extraction of Syntactic Triplets The next step is to extract the dependency relations between words, which will serve as a representation of the words’ contexts for distributional analysis, following a long tradition (Lin, 1998; Bourigault, 2002; Padó and Lapata, 2007; Baroni and Lenci, 2010; Lapesa and Evert, 2017). In all these studies, the most common representation of the contexts in which a word occurs is a set of syntactic triplets (dependent_word, relation, governor_word). For instance, from the sentence &quot;Nous avons utilisé un analyseur syntaxique&quot; (&quot;We used a syntactic parser&quot;), considered as correctly parsed, we can extract the triplets (analyseur, obj (direct object), utiliser) and (syntaxique, mod (modifier), analyseur). From each triplet, we can produce two contextual representations: one for the dependent, and the other"
2020.lrec-1.713,F12-2024,0,0.0500961,"iews. The original PDF files of the articles were converted to raw text to extract parser-compatible data. For this reason, we filtered out some text elements, like footnotes, tables, math formulas, references, captions, or section headers. All line and paragraph breaks were removed and the whole text of each article was put on a single line. Thus, the robustness of the parsers when applied to noisy data was not a parameter we considered when comparing them. 2.2. UD French Sequoia was developed as a complement to the FTB with the main goal to improve its coverage in terms of genre and domain (Candito and Seddah, 2012). The 70,000 words in this corpus come from parliamentary debates, regional newspaper articles, Wikipedia and medical texts. Initially annotated following the FTB scheme, it has been converted to UD. Studied Parsers We selected 7 tools able to perform dependency parsing in French, focusing on the parsers that are easily available and ready-to-use, i.e. those that take in charge the whole process, from raw text to dependencies. These tools were all used with their default options and parameters. All these tools rely on machine learning models trained on manually annotated corpora. Their differe"
2020.lrec-1.713,C10-2013,0,0.857201,"lly demonstrated in (Tanguy et al., 2015). Our focus here is how the choice of the parser impacts the construction of a distributional model using a frequency-based method. This is related to the wider issue of comparing the efficiency of different available tools and models for dependency parsing. Many efforts have been made in the French Natural Language Processing (NLP) community to compare different parsers: the Easy (Paroubek et al., 2008), Passage (De La Clergerie et al., 2008), SPMRL (Seddah et al., 2013) and CoNLL (Zeman et al., 2018) shared tasks as well as more focused studies like (Candito et al., 2010) or (De La Clergerie, 2014). However, the benchmarks used in these tasks and studies, adopting the kind of diverse, generic corpora on which the tools have been trained, might not be the most relevant option for parsing specialized corpora. Moreover, even though some of these campaigns are recent, all the main tools currently available have not been evaluated on the same benchmarks. In the absence of a benchmark fitting our task and domain, we compare the current main French parsers on an external task, as it was done in the EPE campaign for English (Fares et al., 2018). Given that our only be"
2020.lrec-1.713,villemonte-de-la-clergerie-etal-2008-passage,0,0.697531,"on the assumption that the small amount of data can be compensated to some extent by a rich linguistic annotation. This has been partially demonstrated in (Tanguy et al., 2015). Our focus here is how the choice of the parser impacts the construction of a distributional model using a frequency-based method. This is related to the wider issue of comparing the efficiency of different available tools and models for dependency parsing. Many efforts have been made in the French Natural Language Processing (NLP) community to compare different parsers: the Easy (Paroubek et al., 2008), Passage (De La Clergerie et al., 2008), SPMRL (Seddah et al., 2013) and CoNLL (Zeman et al., 2018) shared tasks as well as more focused studies like (Candito et al., 2010) or (De La Clergerie, 2014). However, the benchmarks used in these tasks and studies, adopting the kind of diverse, generic corpora on which the tools have been trained, might not be the most relevant option for parsing specialized corpora. Moreover, even though some of these campaigns are recent, all the main tools currently available have not been evaluated on the same benchmarks. In the absence of a benchmark fitting our task and domain, we compare the current"
2020.lrec-1.713,Y09-1013,0,0.0182324,"used three of them, trained on the UD GSD, Sequoia and ParTUT corpora. Talismane (Urieli and Tanguy, 2013) uses a mix of statistic models and hand-crafted language-specific features and rules incorporating linguistic knowledge. The main model has been trained on the French TreeBank converted to dependencies (FTB), but we also tested the newer Universal Dependency model (UD) trained on the concatenation of all UD corpora previously described. MSTParser (McDonald et al., 2006) is a graph-based dependency parser. We used it through the BONSAI3 package, which combines it with the MElt POS tagger (Denis and Sagot, 2009) and uses the best MST model according to the (Candito et al., 2010) benchmark. It is based on the non-UD version of the FTB. We are fully aware that these parsers can only be compared on a practical level since the technologies they use, their goals, their training data, and even the times at which they were created are extremely different. Nevertheless, they form a large part of the current panel of available solutions for robust syntactic parsing in French, and in this respect, are suitable for consideration in this study. 3. Processing Parser Outputs To compare the 11 selected parsers (or"
2020.lrec-1.713,K18-2002,0,0.0192751,"focused studies like (Candito et al., 2010) or (De La Clergerie, 2014). However, the benchmarks used in these tasks and studies, adopting the kind of diverse, generic corpora on which the tools have been trained, might not be the most relevant option for parsing specialized corpora. Moreover, even though some of these campaigns are recent, all the main tools currently available have not been evaluated on the same benchmarks. In the absence of a benchmark fitting our task and domain, we compare the current main French parsers on an external task, as it was done in the EPE campaign for English (Fares et al., 2018). Given that our only benchmark for this task has limited coverage and validity, our evaluation is mainly a qualitative comparison. In this paper, we mainly examine how changing the parser affects the distributional thesauri that we produce from the parsers’ outputs, with a focus on the scale and scope of these differences. We have used the following procedure: we first applied 11 dependency parsers on the same French specialized corpus. For each of the outputs, we extracted the main syntactic contexts, which we used to build 11 distributional models with a classic frequency-based method. We t"
2020.lrec-1.713,ferret-2010-testing,1,0.816669,"elying on dependency relations is very limited among predictive approaches. More importantly, some recent studies (Pierrejean and Tanguy, 2018) have shown that predictive approaches are unstable to some extent concerning the nearest distributional neighbors of a word. Since we specifically want to concentrate on the effects resulting from the use of different syntactic parsers, we adopted a count-based approach. We based our method on the findings of recent studies in the field (Kiela and Clark, 2014; Baroni et al., 2014; Levy et al., 2015) and more particularly took up two main options from (Ferret, 2010): the use of Positive Pointwise Mutual Information (PPMI) for weighting the (co-occurrent, relation) pairs and the application of a very loose filter that removes the pairs with only one occurrence in these contexts. The second choice is justified by both the small size of our target corpus and the experiments of (Ferret, 2010) with linear co-occurrents. Stanford−Sequoia UDpipe−GSD NLPCube Stanford−GSD Figure 3: Hierarchical clustering of models according to their agreement on the Cosine similarity values of the common pairs of words. Then, we computed the agreement of our models on the neares"
2020.lrec-1.713,D15-1162,0,0.101852,"550,000 words. UD French ParTUT is the French subset of the Parallel-TUT multilingual corpus (Bosco et al., 2012), which is composed of varied text samples (legal texts, Wikipedia articles, Facebook pages, etc.) for a total of approximately 30,000 words. Spacy is an all-purpose NLP tool designed for industrial applications whose main characteristic is its high speed compared to most other parsers. The tagger is based on a perceptron, with attributes based on Brown clusters, following (Koo et al., 2008). It implements a non-monotonous transition-based parser that can revise previous decisions (Honnibal and Johnson, 2015). The available model was trained on the WikiNER corpus (Nothman et al., 2012) for Named Entities Recognition and on UD Sequoia for POS tagging and parsing. See http://universaldependencies.org/ for the detailed history and different versions. UDPipe (Straka and Straková, 2017) uses a neural network with a Gated Recurrent Unit mechanism to perform 2 5821 both tokenization and sentence segmentation at once. For POS tagging, it generates the possible tags for a word according to its suffix and uses a perceptron to assign the correct one. The transition-based parsing relies on a simple one-layer"
2020.lrec-1.713,W14-1503,0,0.256003,"2013). The first justification of this choice is that, except for (Levy and Goldberg, 2014), the number of studies relying on dependency relations is very limited among predictive approaches. More importantly, some recent studies (Pierrejean and Tanguy, 2018) have shown that predictive approaches are unstable to some extent concerning the nearest distributional neighbors of a word. Since we specifically want to concentrate on the effects resulting from the use of different syntactic parsers, we adopted a count-based approach. We based our method on the findings of recent studies in the field (Kiela and Clark, 2014; Baroni et al., 2014; Levy et al., 2015) and more particularly took up two main options from (Ferret, 2010): the use of Positive Pointwise Mutual Information (PPMI) for weighting the (co-occurrent, relation) pairs and the application of a very loose filter that removes the pairs with only one occurrence in these contexts. The second choice is justified by both the small size of our target corpus and the experiments of (Ferret, 2010) with linear co-occurrents. Stanford−Sequoia UDpipe−GSD NLPCube Stanford−GSD Figure 3: Hierarchical clustering of models according to their agreement on the Cosine"
2020.lrec-1.713,P08-1068,0,0.22721,"ed. UD French FTB is the result of the conversion to UD of the original French Treebank and is made of around 550,000 words. UD French ParTUT is the French subset of the Parallel-TUT multilingual corpus (Bosco et al., 2012), which is composed of varied text samples (legal texts, Wikipedia articles, Facebook pages, etc.) for a total of approximately 30,000 words. Spacy is an all-purpose NLP tool designed for industrial applications whose main characteristic is its high speed compared to most other parsers. The tagger is based on a perceptron, with attributes based on Brown clusters, following (Koo et al., 2008). It implements a non-monotonous transition-based parser that can revise previous decisions (Honnibal and Johnson, 2015). The available model was trained on the WikiNER corpus (Nothman et al., 2012) for Named Entities Recognition and on UD Sequoia for POS tagging and parsing. See http://universaldependencies.org/ for the detailed history and different versions. UDPipe (Straka and Straková, 2017) uses a neural network with a Gated Recurrent Unit mechanism to perform 2 5821 both tokenization and sentence segmentation at once. For POS tagging, it generates the possible tags for a word according t"
2020.lrec-1.713,E17-2063,0,0.307974,"iser). This robust method can process all cases in a homogeneous and deterministic way. Thereafter, words are represented by their lemma and their POS tag. We identified 5,580 open-class words with at least 5 occurrences in each of the 11 outputs. Our comparison will use this set of words as a base. 3.2. Extraction of Syntactic Triplets The next step is to extract the dependency relations between words, which will serve as a representation of the words’ contexts for distributional analysis, following a long tradition (Lin, 1998; Bourigault, 2002; Padó and Lapata, 2007; Baroni and Lenci, 2010; Lapesa and Evert, 2017). In all these studies, the most common representation of the contexts in which a word occurs is a set of syntactic triplets (dependent_word, relation, governor_word). For instance, from the sentence &quot;Nous avons utilisé un analyseur syntaxique&quot; (&quot;We used a syntactic parser&quot;), considered as correctly parsed, we can extract the triplets (analyseur, obj (direct object), utiliser) and (syntaxique, mod (modifier), analyseur). From each triplet, we can produce two contextual representations: one for the dependent, and the other for the governor, combining the other word with the dependency. This giv"
2020.lrec-1.713,P14-2050,0,0.100714,", N-ADJ or other. 0.10 0.15 0.20 0.25 0.30 MST (Bonsai) Talismane−FTB Spacy UDpipe−Partut Without further effort to harmonize the outputs for these cases, we used these triplets to build distributional models. 4. 4.1. UDpipe−Sequoia CoreNLP Comparison of Distributional Models Talismane−UD Building the Models Following the distinction made in Baroni et al. (2014), we built our distributional models according to a countbased approach, such as in (Lin, 1998), rather than according to a predictive model such as in (Mikolov et al., 2013). The first justification of this choice is that, except for (Levy and Goldberg, 2014), the number of studies relying on dependency relations is very limited among predictive approaches. More importantly, some recent studies (Pierrejean and Tanguy, 2018) have shown that predictive approaches are unstable to some extent concerning the nearest distributional neighbors of a word. Since we specifically want to concentrate on the effects resulting from the use of different syntactic parsers, we adopted a count-based approach. We based our method on the findings of recent studies in the field (Kiela and Clark, 2014; Baroni et al., 2014; Levy et al., 2015) and more particularly took u"
2020.lrec-1.713,P98-2127,0,0.868181,"(wordform, lemma) pairs sharing the same ending like (schématisons, schématiser). This robust method can process all cases in a homogeneous and deterministic way. Thereafter, words are represented by their lemma and their POS tag. We identified 5,580 open-class words with at least 5 occurrences in each of the 11 outputs. Our comparison will use this set of words as a base. 3.2. Extraction of Syntactic Triplets The next step is to extract the dependency relations between words, which will serve as a representation of the words’ contexts for distributional analysis, following a long tradition (Lin, 1998; Bourigault, 2002; Padó and Lapata, 2007; Baroni and Lenci, 2010; Lapesa and Evert, 2017). In all these studies, the most common representation of the contexts in which a word occurs is a set of syntactic triplets (dependent_word, relation, governor_word). For instance, from the sentence &quot;Nous avons utilisé un analyseur syntaxique&quot; (&quot;We used a syntactic parser&quot;), considered as correctly parsed, we can extract the triplets (analyseur, obj (direct object), utiliser) and (syntaxique, mod (modifier), analyseur). From each triplet, we can produce two contextual representations: one for the depende"
2020.lrec-1.713,P14-5010,0,0.00716101,"niversal tagsets (for PartOf-Speech (POS) tagging and syntactic dependencies) and was the framework for the collection and diffusion of several French annotated corpora under the same format, in particular: Despite the normalization efforts of the UD project, the previous corpora do not use the same tagsets and annotation conventions because of different choices made for dealing with some syntactic phenomenons, as well as conversions applied to some of them. As mentioned previously, we selected 7 different parsers, some of which proposing several models, trained on different corpora. CoreNLP (Manning et al., 2014), the main parser of the Stanford team, implements a maximal entropy tagger and a transition-based parser. It was trained on the UD GSD corpus. StanfordNLP (Qi et al., 2018) is a tool that, on top of giving access to the CoreNLP chain in Python, implements an entirely different parsing algorithm. Its graph-based parser relies on a LSTM neural network. StanfordNLP proposes several French models; we used two of them trained on the UD GSD and Sequoia corpora. NLPCube (Boro¸s et al., 2018) is, like StanfordNLP, based on a LSTM recurrent neural network. Its main particularity is that syntactic pars"
2020.lrec-1.713,W06-2932,0,0.0237427,"to assign the correct one. The transition-based parsing relies on a simple one-layer neural network. UDPipe includes several French models: we used three of them, trained on the UD GSD, Sequoia and ParTUT corpora. Talismane (Urieli and Tanguy, 2013) uses a mix of statistic models and hand-crafted language-specific features and rules incorporating linguistic knowledge. The main model has been trained on the French TreeBank converted to dependencies (FTB), but we also tested the newer Universal Dependency model (UD) trained on the concatenation of all UD corpora previously described. MSTParser (McDonald et al., 2006) is a graph-based dependency parser. We used it through the BONSAI3 package, which combines it with the MElt POS tagger (Denis and Sagot, 2009) and uses the best MST model according to the (Candito et al., 2010) benchmark. It is based on the non-UD version of the FTB. We are fully aware that these parsers can only be compared on a practical level since the technologies they use, their goals, their training data, and even the times at which they were created are extremely different. Nevertheless, they form a large part of the current panel of available solutions for robust syntactic parsing in"
2020.lrec-1.713,P13-2017,0,0.0361921,"rchives of the TALN and RECITAL NLP conference proceedings from 1999 to 2014, for a total size of 4.5 million words. This corpus holds several advantages for the study of distributional models: it is made of high-quality written texts, it is homogeneous in genre and topic, and we, as researchers, have a level of expertise in the domain sufficient to easily interpret the results of distributional analysis. 1 Available at http://redac.univ-tlse2.fr/ corpus/taln_en.html 5820 UD French GSD is the original corpus for the French UD project initially annotated using the Stanford dependencies scheme (McDonald et al., 2013). It contains 400,000 words of text from newspaper articles, Wikipedia articles, blogs, or product reviews. The original PDF files of the articles were converted to raw text to extract parser-compatible data. For this reason, we filtered out some text elements, like footnotes, tables, math formulas, references, captions, or section headers. All line and paragraph breaks were removed and the whole text of each article was put on a single line. Thus, the robustness of the parsers when applied to noisy data was not a parameter we considered when comparing them. 2.2. UD French Sequoia was develope"
2020.lrec-1.713,L16-1262,0,0.0222769,"Missing"
2020.lrec-1.713,J07-2002,0,0.743708,"ing the same ending like (schématisons, schématiser). This robust method can process all cases in a homogeneous and deterministic way. Thereafter, words are represented by their lemma and their POS tag. We identified 5,580 open-class words with at least 5 occurrences in each of the 11 outputs. Our comparison will use this set of words as a base. 3.2. Extraction of Syntactic Triplets The next step is to extract the dependency relations between words, which will serve as a representation of the words’ contexts for distributional analysis, following a long tradition (Lin, 1998; Bourigault, 2002; Padó and Lapata, 2007; Baroni and Lenci, 2010; Lapesa and Evert, 2017). In all these studies, the most common representation of the contexts in which a word occurs is a set of syntactic triplets (dependent_word, relation, governor_word). For instance, from the sentence &quot;Nous avons utilisé un analyseur syntaxique&quot; (&quot;We used a syntactic parser&quot;), considered as correctly parsed, we can extract the triplets (analyseur, obj (direct object), utiliser) and (syntaxique, mod (modifier), analyseur). From each triplet, we can produce two contextual representations: one for the dependent, and the other for the governor, combi"
2020.lrec-1.713,paroubek-etal-2008-easy,0,0.628819,"elying on syntactic contexts and based on the assumption that the small amount of data can be compensated to some extent by a rich linguistic annotation. This has been partially demonstrated in (Tanguy et al., 2015). Our focus here is how the choice of the parser impacts the construction of a distributional model using a frequency-based method. This is related to the wider issue of comparing the efficiency of different available tools and models for dependency parsing. Many efforts have been made in the French Natural Language Processing (NLP) community to compare different parsers: the Easy (Paroubek et al., 2008), Passage (De La Clergerie et al., 2008), SPMRL (Seddah et al., 2013) and CoNLL (Zeman et al., 2018) shared tasks as well as more focused studies like (Candito et al., 2010) or (De La Clergerie, 2014). However, the benchmarks used in these tasks and studies, adopting the kind of diverse, generic corpora on which the tools have been trained, might not be the most relevant option for parsing specialized corpora. Moreover, even though some of these campaigns are recent, all the main tools currently available have not been evaluated on the same benchmarks. In the absence of a benchmark fitting our"
2020.lrec-1.713,N18-4005,1,0.716361,"hese triplets to build distributional models. 4. 4.1. UDpipe−Sequoia CoreNLP Comparison of Distributional Models Talismane−UD Building the Models Following the distinction made in Baroni et al. (2014), we built our distributional models according to a countbased approach, such as in (Lin, 1998), rather than according to a predictive model such as in (Mikolov et al., 2013). The first justification of this choice is that, except for (Levy and Goldberg, 2014), the number of studies relying on dependency relations is very limited among predictive approaches. More importantly, some recent studies (Pierrejean and Tanguy, 2018) have shown that predictive approaches are unstable to some extent concerning the nearest distributional neighbors of a word. Since we specifically want to concentrate on the effects resulting from the use of different syntactic parsers, we adopted a count-based approach. We based our method on the findings of recent studies in the field (Kiela and Clark, 2014; Baroni et al., 2014; Levy et al., 2015) and more particularly took up two main options from (Ferret, 2010): the use of Positive Pointwise Mutual Information (PPMI) for weighting the (co-occurrent, relation) pairs and the application of"
2020.lrec-1.713,W04-2104,0,0.014187,"Missing"
2020.lrec-1.713,sagot-2010-lefff,0,0.0438734,"xample, any word that could be a verb form is lemmatized as a verb, even if it was recognized as a noun or adjective in the POS tagging step). The other parsers may make different decisions in some situations, such as not outputting lemmas for unknown words, lemmatizing feminine nouns in the masculine form, concatenating ambiguous lemmas, etc. Thus, we decided to re-lemmatize the outputs of all the parsers with the same tool, using an inflectional reference lexicon and relying on the POS tag assigned by the parsers. The lexicon we used is a fusion of Morphalou (Romary et al., 2004) and Lefff (Sagot, 2010). When the inflected form of the word was missing from the lexicon, we used a lemmatization strategy based on the substitution of the right-hand part of the form based on the longest suffix found in the lexicon, following the method described in (Tanguy and Hathout, 2007, p. 302). Thus, an unknown word such as relemmatisons, correctly tagged as a verb, will be lemmatized as relemmatiser by analogy with (wordform, lemma) pairs sharing the same ending like (schématisons, schématiser). This robust method can process all cases in a homogeneous and deterministic way. Thereafter, words are represent"
2020.lrec-1.713,W13-4917,0,0.33911,"Missing"
2020.lrec-1.713,K17-3009,0,0.0278092,"NLP tool designed for industrial applications whose main characteristic is its high speed compared to most other parsers. The tagger is based on a perceptron, with attributes based on Brown clusters, following (Koo et al., 2008). It implements a non-monotonous transition-based parser that can revise previous decisions (Honnibal and Johnson, 2015). The available model was trained on the WikiNER corpus (Nothman et al., 2012) for Named Entities Recognition and on UD Sequoia for POS tagging and parsing. See http://universaldependencies.org/ for the detailed history and different versions. UDPipe (Straka and Straková, 2017) uses a neural network with a Gated Recurrent Unit mechanism to perform 2 5821 both tokenization and sentence segmentation at once. For POS tagging, it generates the possible tags for a word according to its suffix and uses a perceptron to assign the correct one. The transition-based parsing relies on a simple one-layer neural network. UDPipe includes several French models: we used three of them, trained on the UD GSD, Sequoia and ParTUT corpora. Talismane (Urieli and Tanguy, 2013) uses a mix of statistic models and hand-crafted language-specific features and rules incorporating linguistic kno"
2020.lrec-1.713,K18-2001,0,0.0398789,"ed to some extent by a rich linguistic annotation. This has been partially demonstrated in (Tanguy et al., 2015). Our focus here is how the choice of the parser impacts the construction of a distributional model using a frequency-based method. This is related to the wider issue of comparing the efficiency of different available tools and models for dependency parsing. Many efforts have been made in the French Natural Language Processing (NLP) community to compare different parsers: the Easy (Paroubek et al., 2008), Passage (De La Clergerie et al., 2008), SPMRL (Seddah et al., 2013) and CoNLL (Zeman et al., 2018) shared tasks as well as more focused studies like (Candito et al., 2010) or (De La Clergerie, 2014). However, the benchmarks used in these tasks and studies, adopting the kind of diverse, generic corpora on which the tools have been trained, might not be the most relevant option for parsing specialized corpora. Moreover, even though some of these campaigns are recent, all the main tools currently available have not been evaluated on the same benchmarks. In the absence of a benchmark fitting our task and domain, we compare the current main French parsers on an external task, as it was done in"
2021.jeptalnrecital-taln.17,P16-2011,0,0.0386643,"Missing"
2021.jeptalnrecital-taln.17,P08-1030,0,0.176483,"Missing"
2021.jeptalnrecital-taln.17,P13-1008,0,0.0590742,"Missing"
2021.jeptalnrecital-taln.17,P10-1081,0,0.119234,"Missing"
2021.jeptalnrecital-taln.17,P17-1164,0,0.0332215,"Missing"
2021.jeptalnrecital-taln.17,P16-1100,0,0.0777097,"Missing"
2021.jeptalnrecital-taln.17,2020.coling-main.4,0,0.0878685,"Missing"
2021.jeptalnrecital-taln.17,N16-1034,0,0.0464364,"Missing"
2021.jeptalnrecital-taln.17,W16-1618,0,0.0383935,"Missing"
2021.jeptalnrecital-taln.17,P15-2060,0,0.0600112,"Missing"
2021.jeptalnrecital-taln.17,N18-1202,0,0.140932,"Missing"
2021.jeptalnrecital-taln.17,D19-1585,0,0.044561,"Missing"
2021.jeptalnrecital-taln.17,N19-1105,0,0.0225046,"Missing"
2021.jeptalnrecital-taln.17,P19-1522,0,0.0234393,"Missing"
2021.jeptalnrecital-taln.17,P18-2066,0,0.0283046,"Missing"
2021.jeptalnrecital-taln.3,P14-1023,0,0.120661,"Missing"
2021.jeptalnrecital-taln.3,2020.acl-main.431,0,0.0708429,"Missing"
2021.jeptalnrecital-taln.3,J06-1003,0,0.535295,"Missing"
2021.jeptalnrecital-taln.3,2020.conll-1.17,0,0.0456481,"Missing"
2021.jeptalnrecital-taln.3,N19-1423,0,0.0469394,"Missing"
2021.jeptalnrecital-taln.3,D19-1006,0,0.0479675,"Missing"
2021.jeptalnrecital-taln.3,2020.tacl-1.3,0,0.0393945,"Missing"
2021.jeptalnrecital-taln.3,H93-1061,0,0.729994,"Missing"
2021.jeptalnrecital-taln.3,W12-3018,0,0.0324973,"Missing"
2021.jeptalnrecital-taln.3,N18-1202,0,0.130518,"Missing"
2021.jeptalnrecital-taln.3,2018.gwc-1.26,0,0.109844,"Missing"
2021.jeptalnrecital-taln.3,2020.tacl-1.54,0,0.0386859,"Missing"
2021.jeptalnrecital-taln.3,2020.emnlp-main.586,0,0.0644989,"Missing"
2021.jeptalnrecital-taln.3,2020.acl-main.422,0,0.0547331,"Missing"
besancon-etal-2010-lima,carreras-etal-2004-freeling,0,\N,Missing
besancon-etal-2010-lima,villemonte-de-la-clergerie-etal-2008-passage,0,\N,Missing
besancon-etal-2010-lima,W99-0301,0,\N,Missing
besancon-etal-2010-lima,W09-1511,1,\N,Missing
besancon-etal-2010-lima,C94-1097,0,\N,Missing
besancon-etal-2010-lima,W05-0705,1,\N,Missing
besancon-etal-2010-lima,R09-1053,1,\N,Missing
besancon-etal-2010-lima,D09-1098,0,\N,Missing
besancon-etal-2012-evaluation,pouliquen-etal-2006-geocoding,0,\N,Missing
besancon-etal-2012-evaluation,J97-1003,0,\N,Missing
besancon-etal-2012-evaluation,I11-1081,1,\N,Missing
besancon-etal-2012-evaluation,besancon-etal-2010-lima,1,\N,Missing
besancon-etal-2012-evaluation,baroni-etal-2008-cleaneval,0,\N,Missing
besancon-etal-2012-evaluation,C96-1079,0,\N,Missing
besancon-etal-2012-evaluation,evert-2008-lightweight,0,\N,Missing
C02-1033,P98-2243,1,0.875473,"nowledge they use. Most of those that achieve text segmentation only rely on the intrinsic characteristics of texts: word distribution, as in (Hearst, 1997), (Choi, 2000) and (Utiyama and Isahara, 2001), or linguistic cues as in (Passonneau and Litman, 1997). They can be applied without restriction about domains but have low results when a text doesn’t characterize its topical structure by surface clues. Some systems exploit domain-independent knowledge about lexical cohesion: a network of words built from a dictionary in (Kozima, 1993); a large set of collocations collected from a corpus in (Ferret, 1998), (Kaufmann, 1999) and (Choi, 2001). To some extend, this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains. The last main type of systems relies on knowledge about the topics they may encounter in the texts they process. This is typically the kind of approach developed in TDT where this knowledge is automatically built from a set of reference texts. The work of Bigi (Bigi et al., 1998) stands in the same perspective but focuses on much larger topics than TDT. These systems have a limited scope due to their topic repre"
C02-1033,J97-1003,0,0.466294,"ween the resulting segments, has recently raised an important interest. The largest part of it was dedicated to topic segmentation, also called linear text segmentation, and to the TDT (Topic Detection and Tracking) initiative (Fiscus et al., 1999), which addresses all the tasks we have mentioned but from a domain-dependent viewpoint and not necessarily in an integrated way. Systems that implement this work can be categorized according to what kind of knowledge they use. Most of those that achieve text segmentation only rely on the intrinsic characteristics of texts: word distribution, as in (Hearst, 1997), (Choi, 2000) and (Utiyama and Isahara, 2001), or linguistic cues as in (Passonneau and Litman, 1997). They can be applied without restriction about domains but have low results when a text doesn’t characterize its topical structure by surface clues. Some systems exploit domain-independent knowledge about lexical cohesion: a network of words built from a dictionary in (Kozima, 1993); a large set of collocations collected from a corpus in (Ferret, 1998), (Kaufmann, 1999) and (Choi, 2001). To some extend, this knowledge permits these systems to discard some false topical shifts without losing t"
C02-1033,W98-1123,0,0.0569361,"Missing"
C02-1033,P99-1077,0,0.533521,"e. Most of those that achieve text segmentation only rely on the intrinsic characteristics of texts: word distribution, as in (Hearst, 1997), (Choi, 2000) and (Utiyama and Isahara, 2001), or linguistic cues as in (Passonneau and Litman, 1997). They can be applied without restriction about domains but have low results when a text doesn’t characterize its topical structure by surface clues. Some systems exploit domain-independent knowledge about lexical cohesion: a network of words built from a dictionary in (Kozima, 1993); a large set of collocations collected from a corpus in (Ferret, 1998), (Kaufmann, 1999) and (Choi, 2001). To some extend, this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains. The last main type of systems relies on knowledge about the topics they may encounter in the texts they process. This is typically the kind of approach developed in TDT where this knowledge is automatically built from a set of reference texts. The work of Bigi (Bigi et al., 1998) stands in the same perspective but focuses on much larger topics than TDT. These systems have a limited scope due to their topic representations but the"
C02-1033,J97-1005,0,0.0993123,"t of it was dedicated to topic segmentation, also called linear text segmentation, and to the TDT (Topic Detection and Tracking) initiative (Fiscus et al., 1999), which addresses all the tasks we have mentioned but from a domain-dependent viewpoint and not necessarily in an integrated way. Systems that implement this work can be categorized according to what kind of knowledge they use. Most of those that achieve text segmentation only rely on the intrinsic characteristics of texts: word distribution, as in (Hearst, 1997), (Choi, 2000) and (Utiyama and Isahara, 2001), or linguistic cues as in (Passonneau and Litman, 1997). They can be applied without restriction about domains but have low results when a text doesn’t characterize its topical structure by surface clues. Some systems exploit domain-independent knowledge about lexical cohesion: a network of words built from a dictionary in (Kozima, 1993); a large set of collocations collected from a corpus in (Ferret, 1998), (Kaufmann, 1999) and (Choi, 2001). To some extend, this knowledge permits these systems to discard some false topical shifts without losing their independence with regard to domains. The last main type of systems relies on knowledge about the"
C02-1033,W01-0514,0,\N,Missing
C02-1033,J90-1003,0,\N,Missing
C02-1033,C98-2238,1,\N,Missing
C02-1033,P01-1064,0,\N,Missing
C02-1033,P93-1041,0,\N,Missing
C02-1033,P98-1100,0,\N,Missing
C02-1033,C98-1097,0,\N,Missing
C04-1194,E03-1020,0,0.248066,"e discovering of word senses is a side effect of the clustering algorithm, Cluster By Committee, used for building classes of words: as a word can belong to several classes, each of them can be considered as one of its senses. The second main trend, found in (Schütze, 1998), (Pedersen and Bruce, 1997) and (Purandare, 2003), represents each instance of a target word by a set of features that occur in its neighborhood and applies an unsupervised clustering algorithm to all its instances. Each cluster is then considered as a sense of the target word. The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity. Our work takes place in this last trend. 2 Overview The starting point of the method we present in this article is a network of lexical cooccurrences, that is a graph whose vertices are the significant words of a corpus and edges represent the cooccurrences between these words in the corpus. The discovering of word senses is performed word by word and the processing of a word only relies on the subgraph that c"
C04-1194,harabagiu-etal-2002-multidocument,0,0.0133725,"they define these senses. In this article, we present a possible solution to overcome these limits by defining the sense of words from the way they are used. More precisely, we propose to differentiate the senses of a word from a network of lexical cooccurrences built from a large corpus. This method was tested both for French and English and was evaluated for English by comparing its results with WordNet. 1 Introduction Semantic resources have proved to be useful in information retrieval and information extraction for applications such as query expansion (Voorhees, 1998), text summarization (Harabagiu and Maiorano, 2002) or question/answering (Pasca and Harabagiu, 2001). But this work has also shown that these resources must be used with caution: they bring on an improvement of results only if word sense disambiguation is performed with a great accuracy. These findings bring one of the first roles of a semantic resource to light: discriminating and characterizing the senses of a set of words. The main semantic resources with a wide coverage that can be exploited by computers are lexico-semantic networks such as WordNet. Because of the way they were built, mainly by hand, these networks are not fundamentally d"
C04-1194,W99-0501,0,0.0320694,"be used with caution: they bring on an improvement of results only if word sense disambiguation is performed with a great accuracy. These findings bring one of the first roles of a semantic resource to light: discriminating and characterizing the senses of a set of words. The main semantic resources with a wide coverage that can be exploited by computers are lexico-semantic networks such as WordNet. Because of the way they were built, mainly by hand, these networks are not fundamentally different from traditional dictionaries. Hence, it is not very surprising that they were criticized, as in (Harabagiu et al., 1999), for not being suitable for Natural Language Processing. They were criticized both about the nature of the senses they discriminate and the way they characterize them. Their senses are considered as too fine-grained but also incomplete. Moreover, they are generally defined through their relations with synonyms, hyponyms and hyperonyms but not by elements that describe the contexts in which they occur. One of the solutions for solving this problem consists in automatically discovering the senses of words from corpora. Each sense is defined by a list of words that is not restricted to synonyms"
C04-1194,W97-0322,0,0.036003,"ed into three main trends. The first one, represented by (Pantel and Lin, 2002), is not focused on the problem of discovering word senses: its main objective is to build classes of equivalent words from a distributionalist viewpoint, hence to gather words that are mainly synonyms. In the case of (Pantel and Lin, 2002), the discovering of word senses is a side effect of the clustering algorithm, Cluster By Committee, used for building classes of words: as a word can belong to several classes, each of them can be considered as one of its senses. The second main trend, found in (Schütze, 1998), (Pedersen and Bruce, 1997) and (Purandare, 2003), represents each instance of a target word by a set of features that occur in its neighborhood and applies an unsupervised clustering algorithm to all its instances. Each cluster is then considered as a sense of the target word. The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity. Our work takes place in this last trend. 2 Overview The starting point of the method we pre"
C04-1194,N03-3004,0,0.0133155,"first one, represented by (Pantel and Lin, 2002), is not focused on the problem of discovering word senses: its main objective is to build classes of equivalent words from a distributionalist viewpoint, hence to gather words that are mainly synonyms. In the case of (Pantel and Lin, 2002), the discovering of word senses is a side effect of the clustering algorithm, Cluster By Committee, used for building classes of words: as a word can belong to several classes, each of them can be considered as one of its senses. The second main trend, found in (Schütze, 1998), (Pedersen and Bruce, 1997) and (Purandare, 2003), represents each instance of a target word by a set of features that occur in its neighborhood and applies an unsupervised clustering algorithm to all its instances. Each cluster is then considered as a sense of the target word. The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity. Our work takes place in this last trend. 2 Overview The starting point of the method we present in this article i"
C04-1194,2003.mtsummit-papers.42,0,0.0218496,"s a side effect of the clustering algorithm, Cluster By Committee, used for building classes of words: as a word can belong to several classes, each of them can be considered as one of its senses. The second main trend, found in (Schütze, 1998), (Pedersen and Bruce, 1997) and (Purandare, 2003), represents each instance of a target word by a set of features that occur in its neighborhood and applies an unsupervised clustering algorithm to all its instances. Each cluster is then considered as a sense of the target word. The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity. Our work takes place in this last trend. 2 Overview The starting point of the method we present in this article is a network of lexical cooccurrences, that is a graph whose vertices are the significant words of a corpus and edges represent the cooccurrences between these words in the corpus. The discovering of word senses is performed word by word and the processing of a word only relies on the subgraph that contains its coocc"
C04-1194,J98-1004,0,0.653845,"area can be divided into three main trends. The first one, represented by (Pantel and Lin, 2002), is not focused on the problem of discovering word senses: its main objective is to build classes of equivalent words from a distributionalist viewpoint, hence to gather words that are mainly synonyms. In the case of (Pantel and Lin, 2002), the discovering of word senses is a side effect of the clustering algorithm, Cluster By Committee, used for building classes of words: as a word can belong to several classes, each of them can be considered as one of its senses. The second main trend, found in (Schütze, 1998), (Pedersen and Bruce, 1997) and (Purandare, 2003), represents each instance of a target word by a set of features that occur in its neighborhood and applies an unsupervised clustering algorithm to all its instances. Each cluster is then considered as a sense of the target word. The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity. Our work takes place in this last trend. 2 Overview The startin"
C04-1194,2003.jeptalnrecital-long.25,0,0.663366,"nd Lin, 2002), the discovering of word senses is a side effect of the clustering algorithm, Cluster By Committee, used for building classes of words: as a word can belong to several classes, each of them can be considered as one of its senses. The second main trend, found in (Schütze, 1998), (Pedersen and Bruce, 1997) and (Purandare, 2003), represents each instance of a target word by a set of features that occur in its neighborhood and applies an unsupervised clustering algorithm to all its instances. Each cluster is then considered as a sense of the target word. The last trend, explored by (Véronis, 2003), (Dorow and Widdows, 2003) and (Rapp, 2003), starts from the cooccurrents of a word recorded from a corpus and builds its senses by gathering its cooccurrents according to their similarity or their dissimilarity. Our work takes place in this last trend. 2 Overview The starting point of the method we present in this article is a network of lexical cooccurrences, that is a graph whose vertices are the significant words of a corpus and edges represent the cooccurrences between these words in the corpus. The discovering of word senses is performed word by word and the processing of a word only re"
C04-1194,J90-1003,0,\N,Missing
C14-1067,J06-1003,0,0.750653,"ted through experiments and offer significant improvement over the state-of-the-art. 1 Introduction Distributional thesauri are useful for many NLP tasks and their construction is an issue widely discussed for several years (Grefenstette, 1994). However this is still a very active research field, maintained by the increasingly large number of available corpus and by many applications. These thesauri associate each of their entry with a list of words that are desired semantically close to the entry. This notion of proximity varies (synonymy, other paradigmatic relations, syntagmatic relations (Budanitsky and Hirst, 2006; Adam et al., 2013, for a discussion)), but the methods used for the automatic construction of thesauri are often shared. For the most part, these methods rely on the distributional hypothesis of (Firth, 1957): each word is characterized by the set of contexts in which it appears, and the semantic proximity of two words can be inferred from the proximity of their contexts. This hypothesis has been implemented in different ways, and several propositions to improve the results have been explored (see next section for a state of the art). The work presented in this article are part of this frame"
C14-1067,W02-0908,0,0.580345,"9 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 709–720, Dublin, Ireland, August 23-29 2014. 3) Finally, on the basis of this work, we show how to use this confidence score and these probabilities to reorder the list of nearest neighbors (Section 6). To achieve this goal, we model the reranking as an optimization problem of assignments, solved by the Hungarian algorithm (Kuhn and Yaw, 1955). 2 Related work The notion of distributional thesaurus, as it was initially defined by Grefenstette (1994), followed by Lin (1998a) and Curran and Moens (2002), is not often considered specifically, probably because of its strong link with the notion of semantic similarity. As a consequence, the improvement of distributional thesauri has been first a side effect of the improvement of the distributional similarity measures used for their building and more precisely, of the distributional data they rely on. Both the nature of the constituents of distributional contexts and their weighting have been considered in this regard. Concerning their weighting, Broda et al. (2009) proposed to turn the weights of context constituents into ranks to make them les"
C14-1067,P13-1055,1,0.91899,"negative Matrix Factorization (Van de Cruys, 2010) and more recently, lexical representations learnt by neural networks (Huang et al., 2012; Mikolov et al., 2013). The work we present in this article follows a different perspective as our objective is to improve an existing distributional thesaurus by relying on its structure through a reranking of its neighbors. Such approach was adopted to some extent by Zhitomirsky-Geffet and Dagan (2009) as it exploited the neighbors of an entry in an initial thesaurus for reweighting its distributional representation and finally, reranking its neighbors. Ferret (2013) proposed a more indirect method in which the reranking is based on the downgrading of the neighbors that are detected as not similar to their entry through a pseudo word sense disambiguation task: such detection occurs if a certain proportion of the occurrences of a neighbor are not tagged as the entry. Finally, the closest work to ours is (Ferret, 2012), which selects in an unsupervised way a set of examples of semantically similar words from an initial thesaurus for training a classifier whose decision function is used for reranking the neighbors of each entry. Its unsupervised selection of"
C14-1067,W05-0604,0,0.015783,"o build the distributional thesaurus is AQUAINT-2. It is a collection of articles from press containing about 380 million words. The thesaurus entries are all the nouns in the corpus with a frequency > 10. That represents 25,000 entries (i.e. unique nouns), denoted by n in the remaining. The corpus is labeled in parts of speech by TreeTagger (Schmid, 1994). In this way, we can identify the names that form the thesaurus entries and thus compare to existing work. However this information is not used to build the thesaurus, ensuring the portability of the method to other languages, similarly to (Freitag et al., 2005). To evaluate the built thesauri, WordNet 3.0 synonyms (Miller, 1990) and Moby (Ward, 1996) are used as references, either separately, or jointly. These two resources exhibit quite different and complementary characteristics: on the one hand, WordNet indicates strong paradigmatic links between words (synonyms or quasi-synonyms). On the other hand, Moby groups words sharing more extended syntagmatic and paradigmatic relations, including synonymy, hyper/hypo-nymy, meronymy, but also many more complex types such as the composition of co-hyponymy and hyponymy (abolition – annulment, cataclysm – de"
C14-1067,P06-1045,0,0.733345,"fet and Dagan (2009), extended by Yamamoto and Asakura (2010), defined a bootstrapping method for modifying the weights of constituents in the distributional context of a word according to the similarity with its semantic neighbors. The nature of distributional contexts has been first considered through the distinction between windowbased and syntactic co-occurrents (Grefenstette, 1994; Curran and Moens, 2002). However, most of the work related to this issue has focused on the fact that the “traditional” representation of distributional contexts is very sparse and redundant, as illustrated by Hagiwara et al. (2006). Hence, several methods for dimension reduction were tested in this context: from Latent Semantic Analysis (Landauer and Dumais, 1997), extended for syntactic co-occurrents (Pad´o and Lapata, 2007), to Random Indexing (Sahlgren, 2001), Non-negative Matrix Factorization (Van de Cruys, 2010) and more recently, lexical representations learnt by neural networks (Huang et al., 2012; Mikolov et al., 2013). The work we present in this article follows a different perspective as our objective is to improve an existing distributional thesaurus by relying on its structure through a reranking of its neig"
C14-1067,P12-1092,0,0.0442096,"1994; Curran and Moens, 2002). However, most of the work related to this issue has focused on the fact that the “traditional” representation of distributional contexts is very sparse and redundant, as illustrated by Hagiwara et al. (2006). Hence, several methods for dimension reduction were tested in this context: from Latent Semantic Analysis (Landauer and Dumais, 1997), extended for syntactic co-occurrents (Pad´o and Lapata, 2007), to Random Indexing (Sahlgren, 2001), Non-negative Matrix Factorization (Van de Cruys, 2010) and more recently, lexical representations learnt by neural networks (Huang et al., 2012; Mikolov et al., 2013). The work we present in this article follows a different perspective as our objective is to improve an existing distributional thesaurus by relying on its structure through a reranking of its neighbors. Such approach was adopted to some extent by Zhitomirsky-Geffet and Dagan (2009) as it exploited the neighbors of an entry in an initial thesaurus for reweighting its distributional representation and finally, reranking its neighbors. Ferret (2013) proposed a more indirect method in which the reranking is based on the downgrading of the neighbors that are detected as not"
C14-1067,P98-2127,0,0.957837,"enses/by/4.0/ 709 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 709–720, Dublin, Ireland, August 23-29 2014. 3) Finally, on the basis of this work, we show how to use this confidence score and these probabilities to reorder the list of nearest neighbors (Section 6). To achieve this goal, we model the reranking as an optimization problem of assignments, solved by the Hungarian algorithm (Kuhn and Yaw, 1955). 2 Related work The notion of distributional thesaurus, as it was initially defined by Grefenstette (1994), followed by Lin (1998a) and Curran and Moens (2002), is not often considered specifically, probably because of its strong link with the notion of semantic similarity. As a consequence, the improvement of distributional thesauri has been first a side effect of the improvement of the distributional similarity measures used for their building and more precisely, of the distributional data they rely on. Both the nature of the constituents of distributional contexts and their weighting have been considered in this regard. Concerning their weighting, Broda et al. (2009) proposed to turn the weights of context constituen"
C14-1067,N13-1090,0,0.0484281,"ns, 2002). However, most of the work related to this issue has focused on the fact that the “traditional” representation of distributional contexts is very sparse and redundant, as illustrated by Hagiwara et al. (2006). Hence, several methods for dimension reduction were tested in this context: from Latent Semantic Analysis (Landauer and Dumais, 1997), extended for syntactic co-occurrents (Pad´o and Lapata, 2007), to Random Indexing (Sahlgren, 2001), Non-negative Matrix Factorization (Van de Cruys, 2010) and more recently, lexical representations learnt by neural networks (Huang et al., 2012; Mikolov et al., 2013). The work we present in this article follows a different perspective as our objective is to improve an existing distributional thesaurus by relying on its structure through a reranking of its neighbors. Such approach was adopted to some extent by Zhitomirsky-Geffet and Dagan (2009) as it exploited the neighbors of an entry in an initial thesaurus for reweighting its distributional representation and finally, reranking its neighbors. Ferret (2013) proposed a more indirect method in which the reranking is based on the downgrading of the neighbors that are detected as not similar to their entry"
C14-1067,J07-2002,0,0.129938,"Missing"
C14-1067,W10-3906,0,0.806014,"tic similarity. As a consequence, the improvement of distributional thesauri has been first a side effect of the improvement of the distributional similarity measures used for their building and more precisely, of the distributional data they rely on. Both the nature of the constituents of distributional contexts and their weighting have been considered in this regard. Concerning their weighting, Broda et al. (2009) proposed to turn the weights of context constituents into ranks to make them less dependent on a specific weighting function while Zhitomirsky-Geffet and Dagan (2009), extended by Yamamoto and Asakura (2010), defined a bootstrapping method for modifying the weights of constituents in the distributional context of a word according to the similarity with its semantic neighbors. The nature of distributional contexts has been first considered through the distinction between windowbased and syntactic co-occurrents (Grefenstette, 1994; Curran and Moens, 2002). However, most of the work related to this issue has focused on the fact that the “traditional” representation of distributional contexts is very sparse and redundant, as illustrated by Hagiwara et al. (2006). Hence, several methods for dimension"
C14-1067,J09-3004,0,0.724789,"cause of its strong link with the notion of semantic similarity. As a consequence, the improvement of distributional thesauri has been first a side effect of the improvement of the distributional similarity measures used for their building and more precisely, of the distributional data they rely on. Both the nature of the constituents of distributional contexts and their weighting have been considered in this regard. Concerning their weighting, Broda et al. (2009) proposed to turn the weights of context constituents into ranks to make them less dependent on a specific weighting function while Zhitomirsky-Geffet and Dagan (2009), extended by Yamamoto and Asakura (2010), defined a bootstrapping method for modifying the weights of constituents in the distributional context of a word according to the similarity with its semantic neighbors. The nature of distributional contexts has been first considered through the distinction between windowbased and syntactic co-occurrents (Grefenstette, 1994; Curran and Moens, 2002). However, most of the work related to this issue has focused on the fact that the “traditional” representation of distributional contexts is very sparse and redundant, as illustrated by Hagiwara et al. (200"
C14-1067,C98-2122,0,\N,Missing
C98-1062,J97-1003,0,0.419541,"a useful p r o c e s s in many applications, such as text summarization or information extraction task. Approaches that address this problem can be classified in knowledge-based approaches or word-based approaches. K n o w l e d g e - b a s e d systems as G r o s z and Sidner's (1986) require an e x t e n s i v e manual k n o w l e d g e engineering effort to create the knowledge base (semantic network and/or frames) and this is only possible in very limited and well-known domains. To overcome this limitation, and to process a large amount of texts, word-based approaches have been developed. Hearst (1997) and M a s s o n (1995) make use of the word distribution in a text to find a thematic segmentation. These works are well adapted to technical or scientific texts characterized by a specific vocabulary. To process narrative or expository texts such as newspaper articles, Kozima's (1993) and Morris and Hirst's (1991) a p p r o a c h e s are based on lexical cohesion computed from a lexical network. These methods depend on the presence of the text vocabulary inside their network. So, to avoid any restriction about domains in such 392 collocations have been collected is 20 words wide and takes in"
C98-1062,P93-1041,0,0.380973,"Missing"
C98-1062,J91-1002,0,0.373813,"Missing"
C98-1062,C94-2187,0,0.704391,"Missing"
C98-1062,C94-1095,0,0.0228951,"exts, they have to be represented by their significant features from that point of view. So, we only hold for each text the lemmatized form of its nouns, verbs and adjectives. This has been done by combining existing tools. MtSeg from the Multext project presented in V6ronis and Khouri (1995) is used for segmenting the raw texts. As compound nouns are less polysemous than single ones, we have added to MtSeg the ability to identify 2300 compound nouns. We have retained the most frequent compound nouns in 11 years of the French Le Monde newspaper. They have been collected with the INTEX tool of Silberztein (1994). The part of speech tagger TreeTagger of Schmid (1994) is applied to disambiguate the lexical category of the words and to provide their lemmatized form. The selection of the meaningful words, which do not include proper nouns and abbreviations, ends the pre-processing. This one is applied to the texts both for building the collocation network and for their thematic segmentation. 4. /max ---- log2 N 2 ( S w - 1) with N: corpus size and Sw: window size 5. Thematic segmentation lexical network without The first method, based on a numerical analysis of the vocabulary distribution in the text, is"
C98-1062,J90-1003,0,\N,Missing
C98-1062,J86-3001,0,\N,Missing
C98-2238,J90-1003,0,0.0562491,"RD). This article presents a method for thematically segmenting texts by using knowledge about lexical cohesion that has been automatically built. This knowledge takes the form of a network of lexical collocations. We claim that this network is as suitable as a thesaurus or a MRD for segmenting texts. Moreover, building it for a spe2 Method T h e collocation network Our collocation network has been built from 24 months of tile French Lc Monde newspaper. The size of this corpus is around 39 million words. The cohesion between words has been evaluated with the mutual information measure, as in (Church and Hanks, 1990). A large window, 20 words wide, was used to take into account the thematic links. The texts were pre-processed with the probabilistic POS tagger TreeTagger (Schmid, 1994) in order to keep only the lemmatized form of their content words, i.e. nouns, adjectives and verbs. The resulting network is composed of approximatively 31 thousand words and 14 million relations. 2.2 C o m p u t a t i o n of text cohesion As in Kozima&apos;s work, a cohesion value is computed at each position of a window in a text (after pre-processing) from the words in this window. The collocation network is used for determini"
C98-2238,J97-1003,0,0.365699,"e also present the results of an experiment about locating boundaries between a series of concatened texts. The segmentation algorithm we propose includes two steps. First, a computation of the cohesion of the different parts of a text is done by using a collocation network. Second, we locate the major breaks in this cohesion to detect the thematic shifts and build segments. 1 2.1 Introduction Several quantitative methods exist for thematically segmenting texts. Most of them are based on the following assumption: the thematic coherence of a text segment finds expression at tile lexical level. Hearst (1997) and Nomoto and Nitta (1994) detect this coherence through patterns of lexical cooccurrence. Morris and Hirst (1991) and Kozima (1993) find topic boundaries in the texts by using lexical cohesion. The first methods are applied to texts, such as expository texts, whose vocabulary is often very specific. As a concept is always expressed by the same word, word repetitions are thematically significant in these texts. The use of lexical cohesion allows to bypass the problem set by texts, such as narratives, in which a concept is often expressed by different means. However, tMs second approach requi"
C98-2238,P93-1041,0,0.0842272,"we propose includes two steps. First, a computation of the cohesion of the different parts of a text is done by using a collocation network. Second, we locate the major breaks in this cohesion to detect the thematic shifts and build segments. 1 2.1 Introduction Several quantitative methods exist for thematically segmenting texts. Most of them are based on the following assumption: the thematic coherence of a text segment finds expression at tile lexical level. Hearst (1997) and Nomoto and Nitta (1994) detect this coherence through patterns of lexical cooccurrence. Morris and Hirst (1991) and Kozima (1993) find topic boundaries in the texts by using lexical cohesion. The first methods are applied to texts, such as expository texts, whose vocabulary is often very specific. As a concept is always expressed by the same word, word repetitions are thematically significant in these texts. The use of lexical cohesion allows to bypass the problem set by texts, such as narratives, in which a concept is often expressed by different means. However, tMs second approach requires knowledge about the cohesion between words. Morris and Hirst (1991) extract this knowledge from a thesaurus. Kozima (1993) exploit"
C98-2238,J91-1002,0,0.050501,". The segmentation algorithm we propose includes two steps. First, a computation of the cohesion of the different parts of a text is done by using a collocation network. Second, we locate the major breaks in this cohesion to detect the thematic shifts and build segments. 1 2.1 Introduction Several quantitative methods exist for thematically segmenting texts. Most of them are based on the following assumption: the thematic coherence of a text segment finds expression at tile lexical level. Hearst (1997) and Nomoto and Nitta (1994) detect this coherence through patterns of lexical cooccurrence. Morris and Hirst (1991) and Kozima (1993) find topic boundaries in the texts by using lexical cohesion. The first methods are applied to texts, such as expository texts, whose vocabulary is often very specific. As a concept is always expressed by the same word, word repetitions are thematically significant in these texts. The use of lexical cohesion allows to bypass the problem set by texts, such as narratives, in which a concept is often expressed by different means. However, tMs second approach requires knowledge about the cohesion between words. Morris and Hirst (1991) extract this knowledge from a thesaurus. Koz"
C98-2238,C94-2187,0,0.0172342,"results of an experiment about locating boundaries between a series of concatened texts. The segmentation algorithm we propose includes two steps. First, a computation of the cohesion of the different parts of a text is done by using a collocation network. Second, we locate the major breaks in this cohesion to detect the thematic shifts and build segments. 1 2.1 Introduction Several quantitative methods exist for thematically segmenting texts. Most of them are based on the following assumption: the thematic coherence of a text segment finds expression at tile lexical level. Hearst (1997) and Nomoto and Nitta (1994) detect this coherence through patterns of lexical cooccurrence. Morris and Hirst (1991) and Kozima (1993) find topic boundaries in the texts by using lexical cohesion. The first methods are applied to texts, such as expository texts, whose vocabulary is often very specific. As a concept is always expressed by the same word, word repetitions are thematically significant in these texts. The use of lexical cohesion allows to bypass the problem set by texts, such as narratives, in which a concept is often expressed by different means. However, tMs second approach requires knowledge about the cohe"
D14-1199,P04-1056,0,0.0406074,"is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a different domain with less annotated data without reconsidering the design of the features used. An im"
D14-1199,P11-1114,0,0.568475,"asks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is gen"
D14-1199,E12-1029,0,0.32774,"les) whose values are text excerpts. The event extraction task is related to several subtasks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on"
D14-1199,lambert-etal-2012-automatic,0,0.014731,"earning relevant features that requires limited prior knowledge, using a neural model to induce semantic word representations (commonly referred as word embeddings) in an unsupervised fashion, as in (Bengio et al., 2006; Collobert and Weston, 2008). We exploit these word embeddings as features for a supervised event role (multiclass) classifier. This type of approach has been proved efficient for numerous tasks in natural language processing, including named entity recognition (Turian et al., 2010), semantic role labeling (Collobert et al., 2011), machine translation (Schwenk and Koehn, 2008; Lambert et al., 2012), word sense disambiguation (Bordes et al., 2012) or sentiment analysis (Glorot et al., 2011; Socher et al., 2011) but has never been used, to our knowl1852 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics edge, for an event extraction task. Our goal is twofold: (1) to prove that using as only features word vector representations makes the approach competitive in the event extraction task; (2) to show that these word representations are scalable and"
D14-1199,P14-2050,0,0.034226,"Missing"
D14-1199,N13-1090,0,0.0175318,"ifferent parameters, compared to the learning curve of TIER (Huang and Riloff, 2012a). The grey points represent the performances of other IE systems. Figure 1 presents the average F1-score results, computed over the slots PerpInd, PerpOrg, Target, Victim and Weapon. We observe that models relying on word embeddings globally outperform the state-of-the-art results, which demonstrates that the word embeddings capture enough semantic information to perform the task of event newswire corpus 4 W2V-50 are the embeddings induced from the MUC4 data set using the negative sampling training algorithm (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), available at https://code.google.com/ p/word2vec/ role labeling on “String Slots” without using any additional hand-engineered features. Moreover, our representations (DRVR-50) clearly surpass the models based on generic embeddings (C&W-50 and HLBL-50) and obtain better results than W2V50, based the competitive model of (Mikolov et al., 2013a), even if the difference is small. We can also note that the performance of our model is good even with a small amount of training data, which makes it a good candidate to easily develop an event extractio"
D14-1199,D07-1075,0,0.64854,"rmance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a different domain with less annotated data without reconsidering the design of the features used. An important step forwards is TIERlight (Huang and Riloff, 2012a) that targeted the minimization of human supervision with a bootstrapping technique for event roles detection. Also, PIPER (Patwardhan and Riloff, 2007; Patwardhan, 2010) distinguishes between relevant and irrelevant regions and learns domain-relevant extraction patterns using a semantic affinity measure. Another possible approach for dealing with this problem is to combine the use a restricted set of manually annotated data with a much larger set of data extracted in an unsupervised way from a corpus. This approach was experimented for relations in the context of Open Information Extraction (Soderland et al., 2010) but not for extracting events and their participants to our knowledge. In this paper, we propose to approach the task of labeli"
D14-1199,P03-1029,0,0.0356832,"ction, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to"
D14-1199,W06-2207,0,0.0176454,"lefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of the applicative domain (the nature of the events) and it is generally difficult to apply a system on a dif"
D14-1199,P10-1040,0,0.287413,"Missing"
D14-1199,C00-2136,0,0.0908768,"t excerpts. The event extraction task is related to several subtasks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering deployed and a good choice of features and classifiers. Furthermore, the efficiency of the system relies on the a priori knowledge of"
D14-1199,P14-2089,0,0.040448,"Missing"
D14-1199,D09-1016,0,0.558717,"traction constitutes a challenging task. An event is described by a set of participants (i.e. attributes or roles) whose values are text excerpts. The event extraction task is related to several subtasks: event mention detection, candidate rolefiller extraction, relation extraction and event template filling. The problem we address here is the detection of role-filler candidates and their association with specific roles in event templates. For this task, IE systems adopt various ways of extracting patterns or generating rules based on the surrounding context, local context and global context (Patwardhan and Riloff, 2009). Current approaches for learning such patterns include bootstrapping techniques (Huang and Riloff, 2012a; Yangarber et al., 2000), weakly supervised learning algorithms (Huang and Riloff, 2011; Sudo et al., 2003; Surdeanu et al., 2006), fully supervised learning approaches (Chieu et al., 2003; Freitag, 1998; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009) and other variations. All these methods rely on substantial amounts of manually annotated corpora and use a large body of linguistic knowledge. The performance of these approaches is related to the amount of knowledge engineering depl"
D14-1199,I08-2089,0,0.0173342,"roles by automatically learning relevant features that requires limited prior knowledge, using a neural model to induce semantic word representations (commonly referred as word embeddings) in an unsupervised fashion, as in (Bengio et al., 2006; Collobert and Weston, 2008). We exploit these word embeddings as features for a supervised event role (multiclass) classifier. This type of approach has been proved efficient for numerous tasks in natural language processing, including named entity recognition (Turian et al., 2010), semantic role labeling (Collobert et al., 2011), machine translation (Schwenk and Koehn, 2008; Lambert et al., 2012), word sense disambiguation (Bordes et al., 2012) or sentiment analysis (Glorot et al., 2011; Socher et al., 2011) but has never been used, to our knowl1852 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1852–1857, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics edge, for an event extraction task. Our goal is twofold: (1) to prove that using as only features word vector representations makes the approach competitive in the event extraction task; (2) to show that these word represent"
D14-1199,P03-1028,0,\N,Missing
D14-1199,M92-1021,0,\N,Missing
E17-2117,S15-2136,0,0.04979,"creation time from clinical notes. This work, based on a feature engineering approach, obtained competitive results with the current state-of-theart and led to two main conclusions. First, the use of word embeddings in place of lexical features tends to degrade performance. Second, our feature engineering approach can be applied with comparable results to two different languages, English and French in our case. To follow-up with the first conclusion, we would like to test a more integrated approach for using embeddings, either by turning all features into embeddings as in Yang and Eisenstein (2015) or by adopting a neural network architecture as in Chikka (2016). Concerning the CR task, results are separated by a 10 percent gap (0.65 for the MERLOT corpus and 0.53 for the THYME corpus). Results obtained for the THYME corpus are coherent with those obtained by Tourille et al. (2016) on the Clinical TempEval 2016 evaluation corpus2 . We increased the recall value in comparison to their results (from 0.436 to 0.47) but this measure is still the main point to improve. More globally, the best results of the Clinical TempEval shared task were 0.843 (accuracy) for the DR task and 0.573 (F1-Mea"
E17-2117,S16-1201,0,0.0612921,"Missing"
E17-2117,S16-1165,0,0.0337299,"ustejovsky and Stubbs, 2011) can be apprehended as temporal buckets in which several events may be included. These containers are anchored by temporal expressions, medical events or other concepts. Styler IV et al. (2014) argue that the use of narrative containers instead of classical temporal relations (Allen, 1983) yields better annotation while keeping most of the useful temporal information intact. The concept of narrative container is illustrated in Figure 1 and described further in Pustejovsky and Stubbs (2011). val task related to the topic for the past two years (Bethard et al., 2015; Bethard et al., 2016). Its first track focused on extracting clinical events and temporal expressions, while its second track included DR and CR tasks. Different approaches were implemented by the teams, among which SVM classifiers (Lee et al., 2016; Tourille et al., 2016; Cohan et al., 2016; AAl Abdulsalam et al., 2016) and CRF approaches (Caselli and Morante, 2016; AAl Abdulsalam et al., 2016) for the DR task, and CRF, Convolutional neural networks (Chikka, 2016) and SVM classifiers (Tourille et al., 2016; Lee et al., 2016; AAl Abdulsalam et al., 2016) for the CR task. 3 Corpus Presentation The MERLOT corpus is"
E17-2117,S16-1192,0,0.135986,"in Figure 1 and described further in Pustejovsky and Stubbs (2011). val task related to the topic for the past two years (Bethard et al., 2015; Bethard et al., 2016). Its first track focused on extracting clinical events and temporal expressions, while its second track included DR and CR tasks. Different approaches were implemented by the teams, among which SVM classifiers (Lee et al., 2016; Tourille et al., 2016; Cohan et al., 2016; AAl Abdulsalam et al., 2016) and CRF approaches (Caselli and Morante, 2016; AAl Abdulsalam et al., 2016) for the DR task, and CRF, Convolutional neural networks (Chikka, 2016) and SVM classifiers (Tourille et al., 2016; Lee et al., 2016; AAl Abdulsalam et al., 2016) for the CR task. 3 Corpus Presentation The MERLOT corpus is composed of clinical documents written in French from a Gastroenterology, Hepatology and Nutrition department. These documents have been de-identified (Grouin and Névéol, 2014) and annotated with entities, temporal expressions and relations (Deléger et al., 2014). The THYME corpus is a collection of clinical texts written in English from a cancer department that have been released during the Clinical TempEval campaigns. This corpus contains doc"
E17-2117,W11-0419,0,0.458059,"te, F-91191 France. olivier.ferret@cea.fr Xavier Tannier LIMSI, CNRS Univ. Paris-Sud Université Paris-Saclay xavier.tannier@limsi.fr Aurélie Névéol LIMSI, CNRS Université Paris-Saclay aurelie.neveol@limsi.fr Abstract In the DR task, the objective is to temporally locate EVENT entities according to the Document Creation Time of the document in which they occur. Possible tags are Before, Before-Overlap, Overlap and After. In the CR task, the objective is to identify temporal inclusion relations between pairs of entities (EVENT and/or TIMEX3) formalized as narrative container relations following Pustejovsky and Stubbs (2011). In this context, we build on Tourille et al. (2016) and show how this type of model can be applied for extracting temporal relations from clinical texts similarly in two languages. We experimented more specifically on two corpora: the THYME corpus (Styler IV et al., 2014), a corpus of de-identified clinical notes in English from the Mayo Clinic and the MERLOT corpus (Campillos et al., to appear), a comparable corpus in French from a group of French hospitals. In this paper, we present a method for temporal relation extraction from clinical narratives in French and in English. We experiment o"
E17-2117,S16-1194,0,0.117119,"f classical temporal relations (Allen, 1983) yields better annotation while keeping most of the useful temporal information intact. The concept of narrative container is illustrated in Figure 1 and described further in Pustejovsky and Stubbs (2011). val task related to the topic for the past two years (Bethard et al., 2015; Bethard et al., 2016). Its first track focused on extracting clinical events and temporal expressions, while its second track included DR and CR tasks. Different approaches were implemented by the teams, among which SVM classifiers (Lee et al., 2016; Tourille et al., 2016; Cohan et al., 2016; AAl Abdulsalam et al., 2016) and CRF approaches (Caselli and Morante, 2016; AAl Abdulsalam et al., 2016) for the DR task, and CRF, Convolutional neural networks (Chikka, 2016) and SVM classifiers (Tourille et al., 2016; Lee et al., 2016; AAl Abdulsalam et al., 2016) for the CR task. 3 Corpus Presentation The MERLOT corpus is composed of clinical documents written in French from a Gastroenterology, Hepatology and Nutrition department. These documents have been de-identified (Grouin and Névéol, 2014) and annotated with entities, temporal expressions and relations (Deléger et al., 2014). The TH"
E17-2117,deleger-etal-2014-annotation,1,0.896819,"Missing"
E17-2117,S16-1175,1,0.820831,"SI, CNRS Univ. Paris-Sud Université Paris-Saclay xavier.tannier@limsi.fr Aurélie Névéol LIMSI, CNRS Université Paris-Saclay aurelie.neveol@limsi.fr Abstract In the DR task, the objective is to temporally locate EVENT entities according to the Document Creation Time of the document in which they occur. Possible tags are Before, Before-Overlap, Overlap and After. In the CR task, the objective is to identify temporal inclusion relations between pairs of entities (EVENT and/or TIMEX3) formalized as narrative container relations following Pustejovsky and Stubbs (2011). In this context, we build on Tourille et al. (2016) and show how this type of model can be applied for extracting temporal relations from clinical texts similarly in two languages. We experimented more specifically on two corpora: the THYME corpus (Styler IV et al., 2014), a corpus of de-identified clinical notes in English from the Mayo Clinic and the MERLOT corpus (Campillos et al., to appear), a comparable corpus in French from a group of French hospitals. In this paper, we present a method for temporal relation extraction from clinical narratives in French and in English. We experiment on two comparable corpora, the MERLOT corpus for Frenc"
E17-2117,N15-1069,0,0.0152944,"essions and document creation time from clinical notes. This work, based on a feature engineering approach, obtained competitive results with the current state-of-theart and led to two main conclusions. First, the use of word embeddings in place of lexical features tends to degrade performance. Second, our feature engineering approach can be applied with comparable results to two different languages, English and French in our case. To follow-up with the first conclusion, we would like to test a more integrated approach for using embeddings, either by turning all features into embeddings as in Yang and Eisenstein (2015) or by adopting a neural network architecture as in Chikka (2016). Concerning the CR task, results are separated by a 10 percent gap (0.65 for the MERLOT corpus and 0.53 for the THYME corpus). Results obtained for the THYME corpus are coherent with those obtained by Tourille et al. (2016) on the Clinical TempEval 2016 evaluation corpus2 . We increased the recall value in comparison to their results (from 0.436 to 0.47) but this measure is still the main point to improve. More globally, the best results of the Clinical TempEval shared task were 0.843 (accuracy) for the DR task and 0.573 (F1-Mea"
E17-2117,P14-5010,0,\N,Missing
E17-2117,S16-1193,0,\N,Missing
E17-2117,S16-1195,0,\N,Missing
embarek-ferret-2008-learning,C92-2082,0,\N,Missing
embarek-ferret-2008-learning,P99-1016,0,\N,Missing
embarek-ferret-2008-learning,P04-1055,0,\N,Missing
embarek-ferret-2008-learning,W03-0419,0,\N,Missing
embarek-ferret-2008-learning,ayache-etal-2006-equer,0,\N,Missing
F12-2003,besancon-etal-2010-lima,1,0.830322,"Missing"
F12-2003,P11-1098,0,0.0517623,"Missing"
F12-2003,doddington-etal-2004-automatic,0,0.0912155,"Missing"
F12-2003,D07-1088,0,0.0479024,"Missing"
F12-2003,W06-3317,0,0.0268408,"Missing"
F12-2003,C96-1079,0,0.53197,"Missing"
F12-2003,P06-1061,0,0.0714848,"Missing"
F12-2003,I11-1081,1,0.205471,"Missing"
F12-2003,N07-2025,0,0.0628677,"Missing"
F12-2003,P05-1061,0,0.0664266,"Missing"
F12-2003,P04-3020,0,0.0949286,"Missing"
F12-2003,P07-3006,0,0.0607571,"Missing"
F12-2003,D07-1075,0,0.0782967,"Missing"
F12-2003,W06-1671,0,0.0555239,"Missing"
F12-2032,P07-1056,0,0.15035,"Missing"
F12-2032,D09-1062,0,0.0446136,"Missing"
F12-2032,W11-3902,0,0.0322105,"Missing"
F12-2032,esuli-sebastiani-2006-sentiwordnet,0,0.0234556,"Missing"
F12-2032,P10-1060,0,0.0348337,"Missing"
F12-2032,P08-2065,0,0.0407177,"Missing"
F12-2032,J11-2001,0,0.0550062,"Missing"
F13-1004,J06-1003,0,0.203711,"Missing"
F13-1004,W02-0908,0,0.468798,"Missing"
F13-1004,W05-0604,0,0.0580071,"Missing"
F13-1004,P08-3001,0,0.0487387,"Missing"
F13-1004,heylen-etal-2008-modelling,0,0.0306902,"Missing"
F13-1004,P10-1026,0,0.0526075,"Missing"
F13-1004,P98-2127,0,0.509021,"Missing"
F13-1004,J07-2002,0,0.193453,"Missing"
F13-1004,ramisch-etal-2010-mwetoolkit,0,0.0409773,"Missing"
F13-1004,N10-1013,0,0.0471479,"Missing"
F13-1004,W10-3906,0,0.0324763,"Missing"
F13-1004,J09-3004,0,0.519557,"Missing"
F13-1026,I05-2045,0,0.0197361,"Missing"
F13-1026,C04-1051,0,0.176404,"Missing"
F13-1026,eichler-etal-2008-unsupervised,0,0.0406469,"Missing"
F13-1026,D11-1142,0,0.0816668,"Missing"
F13-1026,ferret-2010-testing,1,0.895539,"Missing"
F13-1026,W12-0702,0,0.0548574,"Missing"
F13-1026,P04-1053,0,0.0805501,"Missing"
F13-1026,D12-1094,0,0.0426977,"Missing"
F13-1026,P09-1113,0,0.124317,"Missing"
F13-1026,N10-1047,0,0.0686786,"Missing"
F13-1026,D11-1048,0,0.0534093,"Missing"
F13-1026,P06-2094,0,0.0679576,"Missing"
F13-1026,N06-1039,0,0.0533402,"Missing"
F13-1026,wang-etal-2012-evaluation,1,0.888778,"Missing"
F13-1026,D11-1135,0,0.0503165,"Missing"
F14-1003,P04-1056,0,0.107549,"Missing"
F14-1003,P03-1028,0,0.0808192,"Missing"
F14-1003,W09-1407,0,0.0413785,"Missing"
F14-1003,C96-1079,0,0.0659003,"Missing"
F14-1003,P10-1029,0,0.0367011,"Missing"
F14-1003,P11-1114,0,0.0314191,"Missing"
F14-1003,M92-1008,0,0.174587,"Missing"
F14-1003,D07-1075,0,0.0400329,"Missing"
F14-1003,D09-1016,0,0.0412256,"Missing"
F14-1003,strassel-etal-2008-linguistic,0,0.0214512,"Missing"
F14-1003,P03-1029,0,0.122251,"Missing"
F14-1003,W06-2207,0,0.0344801,"Missing"
F14-1003,P10-1040,0,0.019682,"Missing"
F14-1003,C00-2136,0,0.143887,"Missing"
F14-1020,W02-0908,0,0.1159,"Missing"
F14-1020,P13-1055,1,0.853386,"Missing"
F14-1020,W05-0604,0,0.0542489,"Missing"
F14-1020,P06-1045,0,0.0591494,"Missing"
F14-1020,P12-1092,0,0.091586,"Missing"
F14-1020,P98-2127,0,0.592531,"Missing"
F14-1020,N13-1090,0,0.0509347,"Missing"
F14-1020,J07-2002,0,0.0318426,"Missing"
F14-1020,J09-3004,0,0.0373963,"Missing"
ferret-2006-building,J90-1003,0,\N,Missing
ferret-2006-building,E99-1013,0,\N,Missing
ferret-2006-building,C02-1167,0,\N,Missing
ferret-2006-building,C02-1033,1,\N,Missing
ferret-2006-building,C00-1072,0,\N,Missing
ferret-2006-building,P94-1002,0,\N,Missing
ferret-2006-building,P98-2180,0,\N,Missing
ferret-2006-building,C98-2175,0,\N,Missing
ferret-2006-building,harabagiu-etal-2002-multidocument,0,\N,Missing
ferret-2006-building,W99-0501,0,\N,Missing
ferret-2010-testing,W02-0908,0,\N,Missing
ferret-2010-testing,W05-0604,0,\N,Missing
ferret-2010-testing,C04-1194,1,\N,Missing
ferret-2010-testing,P98-2127,0,\N,Missing
ferret-2010-testing,C98-2122,0,\N,Missing
ferret-2010-testing,W09-0201,0,\N,Missing
ferret-2010-testing,D09-1098,0,\N,Missing
ferret-2010-testing,J09-3004,0,\N,Missing
ferret-etal-2002-building,J97-1003,0,\N,Missing
ferret-etal-2002-building,W01-0907,0,\N,Missing
ferret-etal-2002-building,C96-1083,0,\N,Missing
ferret-etal-2002-building,W01-0909,1,\N,Missing
ferret-etal-2002-building,C92-2082,0,\N,Missing
ferret-etal-2002-building,P98-1082,0,\N,Missing
ferret-etal-2002-building,C98-1079,0,\N,Missing
garcia-fernandez-etal-2014-evaluation,J11-2001,0,\N,Missing
garcia-fernandez-etal-2014-evaluation,P10-1060,0,\N,Missing
garcia-fernandez-etal-2014-evaluation,P08-2065,0,\N,Missing
garcia-fernandez-etal-2014-evaluation,D09-1062,0,\N,Missing
garcia-fernandez-etal-2014-evaluation,P07-1056,0,\N,Missing
garcia-fernandez-etal-2014-evaluation,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
garcia-fernandez-etal-2014-evaluation,W11-3902,0,\N,Missing
grappy-etal-2010-corpus,cramer-etal-2006-building,0,\N,Missing
grappy-etal-2010-corpus,rosset-petel-2006-ritel,0,\N,Missing
grappy-etal-2010-corpus,varasai-etal-2008-building,0,\N,Missing
I11-1081,besancon-etal-2010-lima,1,0.709543,"Missing"
I11-1081,N07-2025,0,0.139366,"elation confidence, a symmetric notion in our case, for filling templates. Figure 3 shows an example of such entity graph. Note that we assume that all entity mentions having the same value are equivalent (as the two mentions of Chino Hills) since they are located in the same event segment. Similarly, we consider all event mentions as equivalent (as earthquake in the first sentence and quake in the second sentence). The presence of a relation between two entities in a sentence is classically determined by a statistical classifier. Following the standard approach of (McDonald et al., 2005) or (Liu et al., 2007), the weight of a relation is evaluated by the confidence score of this classifier and ranges in [0,1] in all the experiments of Section 6.3. In most previous works, such classifier mainly relies on a set of lexicalized features, without any syntactic feature (Afzal, 2009; Gu and Cercone, 2006; Wick et al., 2006). In (Liu et al., 2007), syntactic features are used in addition to lexicalized features. In con• FEAT-BASE: same feature set as (Afzal, 2009), based on lexicalized features; • FEAT-LEX: a feature set that contains lexicalized features, and syntactic features inspired by (Liu et al., 2"
I11-1081,P11-1098,0,0.0877529,"Missing"
I11-1081,P05-1061,0,0.669988,"egment-based HMM (Hidden Markov Model) approach. It relies on a first HMM model for identifying text units (sentences) that are relevant for the extraction of template fillers and on another HMM to extract the fillers from the retrieved sentences. Similarly, Patwardhan and Riloff (2007) proposed first to identify relevant sentences by using a self-trained SVM (Support Vector Machine) and then, to apply extraction patterns (primary and secondary patterns) to find the template fillers. One of the first successful approach for the extraction of n-ary relations came from the biomedical community (McDonald et al., 2005) and was later applied to the domain of corporate management successions (Afzal, 2009). Other works tackled the complex relation problem in the context of database record extraction. They proposed to focus on the compatibility of a set of entities rather than on the compatibility of pairs of entities, which led them to take into account intersentential relations between entities (Wick et al., 2006; Mansuri and Sarawagi, 2006; Feng et al., 2007). the second one involves 4 entities. Similarly to (McDonald et al., 2005), we refer to such relations as complex relations, namely any n-ary relation a"
I11-1081,D07-1088,0,0.499976,"terns) to find the template fillers. One of the first successful approach for the extraction of n-ary relations came from the biomedical community (McDonald et al., 2005) and was later applied to the domain of corporate management successions (Afzal, 2009). Other works tackled the complex relation problem in the context of database record extraction. They proposed to focus on the compatibility of a set of entities rather than on the compatibility of pairs of entities, which led them to take into account intersentential relations between entities (Wick et al., 2006; Mansuri and Sarawagi, 2006; Feng et al., 2007). the second one involves 4 entities. Similarly to (McDonald et al., 2005), we refer to such relations as complex relations, namely any n-ary relation among n typed entities. In this context, each event can be seen as a complex relation where the arity of the relation n is equal to the number of entity types that should be filled in the template (n=5 in the previous example). Several methods were proposed for extracting complex relations such as graph-based methods (McDonald et al., 2005; Wick et al., 2006) or inference-based methods (Goertzel et al., 2006). In this article, we tackle the comp"
I11-1081,P04-3020,0,0.170834,"Missing"
I11-1081,W06-3317,0,0.382641,"Missing"
I11-1081,P07-3006,0,0.123972,"templates from relations: since event segments go beyond the sentence level, they are even more likely to contain complex relations than sentences. Therefore, we have to verify which entities mentioned in these segments are eligible to be part of the complex relation. 4 Segmenting Texts into Events The goal of our segmentation of texts is to delimit segment units in relation with a target event. In previous work such as (Gu and Cercone, 2006; Patwardhan and Riloff, 2007), the methods for identifying such segments relied on fully lexicalized models that were learned using word surface forms. (Naughton, 2007) adopted a more generic approach by exploiting text structure. Our proposal is based on the idea that using temporal cues can help discriminate events, in particular similar events. In the example of Figure 2 for instance, two kinds of temporal cues can be used for this task: date values and verb tenses. {MAIN} An earthquake measuring 5.6 on the Richter scale hit Jayapura, Papua, shortly after midnight on Sunday. {SUB} Previously on Saturday the agency recorded a magnitude 5.6 earthquake had hit Melonguane in North Sulawesi. {OUT} Indonesia, sits on a vulnerable quake-hit zone called the Pacif"
I11-1081,D07-1075,0,0.240846,"account by the linear structure of our CRF model. More details about this segmentation model can be found in (Jean-Louis et al., 2010). • filling event templates from relations: since event segments go beyond the sentence level, they are even more likely to contain complex relations than sentences. Therefore, we have to verify which entities mentioned in these segments are eligible to be part of the complex relation. 4 Segmenting Texts into Events The goal of our segmentation of texts is to delimit segment units in relation with a target event. In previous work such as (Gu and Cercone, 2006; Patwardhan and Riloff, 2007), the methods for identifying such segments relied on fully lexicalized models that were learned using word surface forms. (Naughton, 2007) adopted a more generic approach by exploiting text structure. Our proposal is based on the idea that using temporal cues can help discriminate events, in particular similar events. In the example of Figure 2 for instance, two kinds of temporal cues can be used for this task: date values and verb tenses. {MAIN} An earthquake measuring 5.6 on the Richter scale hit Jayapura, Papua, shortly after midnight on Sunday. {SUB} Previously on Saturday the agency reco"
I11-1081,C96-1079,0,0.907363,"to narrow the span of text to explore in order to link a named entity to an event mention. As time is an important feature for discriminating events, we chose to perform this segmentation by relying on temporal cues. Concerning the second problem, we can observe in Figure 1 that most of the sentences comprise an event mention with more than 2 related entities: the first sentence involves 3 entities while Introduction Information Extraction (IE) is a process that aims at extracting pieces of information from texts. Following the paradigm defined in the Message Understanding Conferences (MUC) (Grishman and Sundheim, 1996), IE systems focus on extracting structured information concerning events to fill predefined templates. These templates make it possible to highlight the information that is specific to a type of events and to discard pieces of information that are not relevant in this respect. Figure 1 gives an example of the filling of a template by information extracted from a news article. Common issues addressed by IE systems for filling a template include identifying named entities, finding relations between these entities, resolving entity coreference, gathering scattered information, etc. (Turmo et al."
I11-1081,P06-1061,0,0.485294,"t types are taken into account by the linear structure of our CRF model. More details about this segmentation model can be found in (Jean-Louis et al., 2010). • filling event templates from relations: since event segments go beyond the sentence level, they are even more likely to contain complex relations than sentences. Therefore, we have to verify which entities mentioned in these segments are eligible to be part of the complex relation. 4 Segmenting Texts into Events The goal of our segmentation of texts is to delimit segment units in relation with a target event. In previous work such as (Gu and Cercone, 2006; Patwardhan and Riloff, 2007), the methods for identifying such segments relied on fully lexicalized models that were learned using word surface forms. (Naughton, 2007) adopted a more generic approach by exploiting text structure. Our proposal is based on the idea that using temporal cues can help discriminate events, in particular similar events. In the example of Figure 2 for instance, two kinds of temporal cues can be used for this task: date values and verb tenses. {MAIN} An earthquake measuring 5.6 on the Richter scale hit Jayapura, Papua, shortly after midnight on Sunday. {SUB} Previous"
I11-1081,W06-1671,0,0.460836,"extraction patterns (primary and secondary patterns) to find the template fillers. One of the first successful approach for the extraction of n-ary relations came from the biomedical community (McDonald et al., 2005) and was later applied to the domain of corporate management successions (Afzal, 2009). Other works tackled the complex relation problem in the context of database record extraction. They proposed to focus on the compatibility of a set of entities rather than on the compatibility of pairs of entities, which led them to take into account intersentential relations between entities (Wick et al., 2006; Mansuri and Sarawagi, 2006; Feng et al., 2007). the second one involves 4 entities. Similarly to (McDonald et al., 2005), we refer to such relations as complex relations, namely any n-ary relation among n typed entities. In this context, each event can be seen as a complex relation where the arity of the relation n is equal to the number of entity types that should be filled in the template (n=5 in the previous example). Several methods were proposed for extracting complex relations such as graph-based methods (McDonald et al., 2005; Wick et al., 2006) or inference-based methods (Goertzel et"
I11-1081,doddington-etal-2004-automatic,0,\N,Missing
I17-1028,P14-1023,0,0.129156,"was defined as a modification of the objective function underlying this model. We have transposed this approach for its application to the adaptation of embeddings after their building, without a specific link to the Skip-Gram model. The general idea is to adapt vectors to minimize sij − sik ∀(i, j, k) ∈ E. The objective to minimize takes more specifically the following form: X f (sik − sij ) The building of our reference distributional thesaurus, Tcnt , was achieved by relying on a classical count-based approach with a set of parameters that were found relevant by several systematic studies (Baroni et al., 2014; Kiela and Clark, 2014; Levy et al., 2015): • distributional contexts: co-occurrents restricted to nouns, verbs and adjectives having at least 10 occurrences in the corpus, collected in a 3 word window, i.e. +/-1 word around the target word; • directional co-occurrents, which were found having a good performance by Bullinaria and Levy (2012); • weighting function of co-occurrents in contexts = Positive Pointwise Mutual Information (PPMI) with the context distribution smoothing factor proposed by (Levy et al., 2015), equal to 0.75; • similarity measure between contexts, for evaluating the sema"
I17-1028,W16-2502,0,0.0189284,"Lin, 1998) or (Curran and Moens, 2002) by extracting the closest semantic neighbors of each of its entries. More precisely, the similarity measure was computed between each entry and its possible neighbors. Both the entries of the thesaurus and their possible neighbors were nouns with at least 10 occurrences in the corpus. These neighbors were then ranked in the decreasing order of the values of this measure. Evaluation of Thesaurus Embedding The evaluation of distributional objects such as thesauri or word embeddings is currently a subject of research as both intrinsic (Faruqui et al., 2016; Batchkarov et al., 2016) and extrinsic (Schnabel et al., 2015) evaluations exhibit insufficiencies that question their reliability. In our case, we perExperimental Framework For testing and evaluating the proposed approach, we needed first to choose a reference corpus and to build a distributional thesaurus from it. We chose 276 Method #eval. words #syn./ word R@100 Rprec MAP P@1 P@2 P@5 Tcnt GloVe SGNS 10,544 2.9 29.0 21.3 22.4 11.3 6.7 8.7 13.1 8.0 10.3 15.7 9.8 12.3 11.4 7.4 8.8 6.6 4.5 5.2 Table 1: Evaluation of the initial thesaurus and two reference models of embeddings (values x 100) formed an intrinsic evalua"
I17-1028,P15-2077,1,0.919168,"ction 3.3. The results of Table 4 clearly validate the benefit of the technique: both retrofitting and counter-rank-fitting significantly improve SGNS embeddings. As in Section 3.3, the results of retrofitting and counterrank-fitting are rather close, with a global advantage for counter-rank-fitting. We can also note that the improved versions of SGNS embeddings are still far from the best results of our thesaurus embedding method (SVD + Retrofit). 4.2 resulting from the second step of the fusion process. The significant difference with the results of Tcnt and SGNS confirms the conclusions of Ferret (2015) about the interest of merging thesauri built differently. The Embretrof (fusion T-S) line shows the evaluation of the word embeddings produced by the global fusion process. In a similar way to the findings of Section 3.3, the embeddings built from the Fusion T-S thesaurus are less effective than the thesaurus itself but the difference is small here too. Moreover, we can note that these embeddings have significantly higher results than SGNS, the input embeddings, but also higher results than the input thesaurus Tcnt , once again without any external knowledge. Fusion of Heterogeneous Represent"
I17-1028,J06-1003,0,0.0721755,"be interesting to transpose the improvements obtained in such a way to distributional contexts, as illustrated by Figure 1. Hence, we propose in this article to investigate the problem of turning a distributional thesaurus into word embeddings, that is to say embedding a thesaurus. We will show that such process can Introduction Early work about distributional semantics (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) was strongly focused on the notion of distributional thesaurus. Recent work in this domain has been more concerned by the notions of semantic similarity and relatedness (Budanitsky and Hirst, 2006) and by the representation of distributional data. This trend has been strengthened even more recently with all work about distributed word representations and embeddings, whether they are built by neural networks (Mikolov et al., 2013) or not (Pennington et al., 2014). From a more global perspective, distributional thesauri and distributional data, i.e. distributional contexts of words, can be considered as dual representations of the same semantic similarity information. Distributional data are an intensional form of this information that can take an extensional form as distributional thesau"
I17-1028,W14-1503,0,0.0198667,"fication of the objective function underlying this model. We have transposed this approach for its application to the adaptation of embeddings after their building, without a specific link to the Skip-Gram model. The general idea is to adapt vectors to minimize sij − sik ∀(i, j, k) ∈ E. The objective to minimize takes more specifically the following form: X f (sik − sij ) The building of our reference distributional thesaurus, Tcnt , was achieved by relying on a classical count-based approach with a set of parameters that were found relevant by several systematic studies (Baroni et al., 2014; Kiela and Clark, 2014; Levy et al., 2015): • distributional contexts: co-occurrents restricted to nouns, verbs and adjectives having at least 10 occurrences in the corpus, collected in a 3 word window, i.e. +/-1 word around the target word; • directional co-occurrents, which were found having a good performance by Bullinaria and Levy (2012); • weighting function of co-occurrents in contexts = Positive Pointwise Mutual Information (PPMI) with the context distribution smoothing factor proposed by (Levy et al., 2015), equal to 0.75; • similarity measure between contexts, for evaluating the semantic similarity of two"
I17-1028,C14-1067,1,0.891035,"Missing"
I17-1028,W02-1029,0,0.0695201,"tion 3.3, the embeddings built from the Fusion T-S thesaurus are less effective than the thesaurus itself but the difference is small here too. Moreover, we can note that these embeddings have significantly higher results than SGNS, the input embeddings, but also higher results than the input thesaurus Tcnt , once again without any external knowledge. Fusion of Heterogeneous Representations Being able to turn a distributional thesaurus into word embeddings also makes it possible to fusion different types of distributional data. In the case of thesaurus, fusion processes were early proposed by Curran (2002) and more recently by Ferret (2015). In the case of word embeddings, the recent work of Yin and Sch¨utze (2016) applied ensemble methods to several word embeddings. By exploiting the possibility to change from one type of representation to another, we propose a new kind of fusion, performed between a thesaurus and word embeddings and leading to improve both the input thesaurus and the embeddings. The first step of this fusion process consists in turning the input word embeddings into a distributional thesaurus. Then, the resulting thesaurus is merged with the input thesaurus, which consists in"
I17-1028,W02-0908,0,0.531215,", which has led to an important improvement of distributional thesauri. Conversely, work such as (Claveau et al., 2014) has focused on methods for improving thesauri themselves. It would clearly be interesting to transpose the improvements obtained in such a way to distributional contexts, as illustrated by Figure 1. Hence, we propose in this article to investigate the problem of turning a distributional thesaurus into word embeddings, that is to say embedding a thesaurus. We will show that such process can Introduction Early work about distributional semantics (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) was strongly focused on the notion of distributional thesaurus. Recent work in this domain has been more concerned by the notions of semantic similarity and relatedness (Budanitsky and Hirst, 2006) and by the representation of distributional data. This trend has been strengthened even more recently with all work about distributed word representations and embeddings, whether they are built by neural networks (Mikolov et al., 2013) or not (Pennington et al., 2014). From a more global perspective, distributional thesauri and distributional data, i.e. distributional contexts of words, can be cons"
I17-1028,P98-2127,0,0.940081,"parameters, which has led to an important improvement of distributional thesauri. Conversely, work such as (Claveau et al., 2014) has focused on methods for improving thesauri themselves. It would clearly be interesting to transpose the improvements obtained in such a way to distributional contexts, as illustrated by Figure 1. Hence, we propose in this article to investigate the problem of turning a distributional thesaurus into word embeddings, that is to say embedding a thesaurus. We will show that such process can Introduction Early work about distributional semantics (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) was strongly focused on the notion of distributional thesaurus. Recent work in this domain has been more concerned by the notions of semantic similarity and relatedness (Budanitsky and Hirst, 2006) and by the representation of distributional data. This trend has been strengthened even more recently with all work about distributed word representations and embeddings, whether they are built by neural networks (Mikolov et al., 2013) or not (Pennington et al., 2014). From a more global perspective, distributional thesauri and distributional data, i.e. distributional conte"
I17-1028,N15-1184,0,0.104462,"Missing"
I17-1028,P15-1145,0,0.0140541,"corpus, already used for various evaluations, a middle-size corpus of around 380 million words made of news articles in English. The main preprocessing of the corpus was the application of lemmatization and the removal of function words. According to (Bullinaria and Levy, 2012), the lemmatization of words leads to only a small improvement in terms of results but it is also a way to obtain the same results with a smaller corpus. The last method we have used for improving the embeddings built from the initial thesaurus, called rank-fitting hereafter, is a transposition of the method proposed by Liu et al. (2015). The objective of this method is to integrate into embeddings order constraints coming from external knowledge with the following form: similarity(wi , wj ) &gt; similarity(wi , wk ), abbreviated sij &gt; sik in what follows. This kind of constraints particularly fits our context as the semantic neighbors of an entry in a distributional thesaurus are ranked and can be viewed as a set of such constraints. More precisely, i corresponds in this case to an entry and j and k to two of its neighbors such that rank(j) &gt; rank(k). However, the method of Liu et al. (2015) is linked to the Skip-Gram model and"
I17-1028,W16-2506,0,0.0219706,"was performed as in (Lin, 1998) or (Curran and Moens, 2002) by extracting the closest semantic neighbors of each of its entries. More precisely, the similarity measure was computed between each entry and its possible neighbors. Both the entries of the thesaurus and their possible neighbors were nouns with at least 10 occurrences in the corpus. These neighbors were then ranked in the decreasing order of the values of this measure. Evaluation of Thesaurus Embedding The evaluation of distributional objects such as thesauri or word embeddings is currently a subject of research as both intrinsic (Faruqui et al., 2016; Batchkarov et al., 2016) and extrinsic (Schnabel et al., 2015) evaluations exhibit insufficiencies that question their reliability. In our case, we perExperimental Framework For testing and evaluating the proposed approach, we needed first to choose a reference corpus and to build a distributional thesaurus from it. We chose 276 Method #eval. words #syn./ word R@100 Rprec MAP P@1 P@2 P@5 Tcnt GloVe SGNS 10,544 2.9 29.0 21.3 22.4 11.3 6.7 8.7 13.1 8.0 10.3 15.7 9.8 12.3 11.4 7.4 8.8 6.6 4.5 5.2 Table 1: Evaluation of the initial thesaurus and two reference models of embeddings (values x 100)"
I17-1028,N16-1018,0,0.0298944,"Missing"
I17-1028,D12-1111,0,0.0408326,"Missing"
I17-1028,D14-1162,0,0.0821073,"thesaurus. We will show that such process can Introduction Early work about distributional semantics (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) was strongly focused on the notion of distributional thesaurus. Recent work in this domain has been more concerned by the notions of semantic similarity and relatedness (Budanitsky and Hirst, 2006) and by the representation of distributional data. This trend has been strengthened even more recently with all work about distributed word representations and embeddings, whether they are built by neural networks (Mikolov et al., 2013) or not (Pennington et al., 2014). From a more global perspective, distributional thesauri and distributional data, i.e. distributional contexts of words, can be considered as dual representations of the same semantic similarity information. Distributional data are an intensional form of this information that can take an extensional form as distributional thesauri by applying a similarity measure to them. Going from an intensional to an extensional representation corresponds to the 273 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 273–283, c Taipei, Taiwan, November 27 – Decem"
I17-1028,P16-1128,0,0.0474052,"Missing"
I17-1028,P14-2089,0,0.0355109,"om this thesaurus by the graph embedding methods we consider. Such adaptation has already been tackled by some work in the context of the injection of external knowledge made of semantic relations into embeddings built mainly by neural methods such as the Skip-Gram model (Mikolov et al., 2013). Methods for performing such injection can roughly be divided into two categories: those operating during the building of the embeddings, generally by modifying the objective function supporting this building (Yih et al., 2012; Zhang et al., 2014), and those applied after the building of the embeddings (Yu and Dredze, 2014; Xu et al., 2014). We have more particularly used or adapted two methods from the second category N X X τ (dist(ˆ qi , qˆj ) − dist(qi , qj )) i =1 j ∈N (i) + X (3) τ (dist(ˆ qi , qˆj )) (i,j) ∈E with dist(x, y) = 1 − cos(x, y) and τ (x) = max(0, x). As in equation 2, the first term tends to preserve the initial vectors. In this case, this preservation does not focus on the vectors themselves but on the pairwise distances between a vector and its nearest neighbors (N (i)). The second term is quite similar to the second term of equation 2 with the use of a distance derived from the Cosine simi"
I17-1028,D14-1161,0,0.0247306,"of the initial thesaurus as constraints for adapting the embeddings built from this thesaurus by the graph embedding methods we consider. Such adaptation has already been tackled by some work in the context of the injection of external knowledge made of semantic relations into embeddings built mainly by neural methods such as the Skip-Gram model (Mikolov et al., 2013). Methods for performing such injection can roughly be divided into two categories: those operating during the building of the embeddings, generally by modifying the objective function supporting this building (Yih et al., 2012; Zhang et al., 2014), and those applied after the building of the embeddings (Yu and Dredze, 2014; Xu et al., 2014). We have more particularly used or adapted two methods from the second category N X X τ (dist(ˆ qi , qˆj ) − dist(qi , qj )) i =1 j ∈N (i) + X (3) τ (dist(ˆ qi , qˆj )) (i,j) ∈E with dist(x, y) = 1 − cos(x, y) and τ (x) = max(0, x). As in equation 2, the first term tends to preserve the initial vectors. In this case, this preservation does not focus on the vectors themselves but on the pairwise distances between a vector and its nearest neighbors (N (i)). The second term is quite similar to the seco"
I17-1028,P06-2111,0,0.0899949,"Missing"
I17-1028,D15-1036,0,0.0245602,"y extracting the closest semantic neighbors of each of its entries. More precisely, the similarity measure was computed between each entry and its possible neighbors. Both the entries of the thesaurus and their possible neighbors were nouns with at least 10 occurrences in the corpus. These neighbors were then ranked in the decreasing order of the values of this measure. Evaluation of Thesaurus Embedding The evaluation of distributional objects such as thesauri or word embeddings is currently a subject of research as both intrinsic (Faruqui et al., 2016; Batchkarov et al., 2016) and extrinsic (Schnabel et al., 2015) evaluations exhibit insufficiencies that question their reliability. In our case, we perExperimental Framework For testing and evaluating the proposed approach, we needed first to choose a reference corpus and to build a distributional thesaurus from it. We chose 276 Method #eval. words #syn./ word R@100 Rprec MAP P@1 P@2 P@5 Tcnt GloVe SGNS 10,544 2.9 29.0 21.3 22.4 11.3 6.7 8.7 13.1 8.0 10.3 15.7 9.8 12.3 11.4 7.4 8.8 6.6 4.5 5.2 Table 1: Evaluation of the initial thesaurus and two reference models of embeddings (values x 100) formed an intrinsic evaluation relying on the synonyms of WordNe"
I17-1028,ferret-2010-testing,1,\N,Missing
I17-1028,Q15-1016,0,\N,Missing
I17-1028,C98-2122,0,\N,Missing
I17-2035,D15-1220,0,0.0439515,"Missing"
I17-2035,P16-1036,0,0.0168509,"rity task (Sultan et al., 2014). We found it unusable in our set-up due to its computational complexity as we calculate about 5 million sentence pair similarities for some datasets while the SemEval 2014 corpus, for instance, gathers only 3,750 sentence pairs. Given this constraint, we chose a similarity measure relying on low dimensional word vectors from word embeddings. In fact, simply averaging word embeddings of all words in a sentence has proven to produce a sentence vector encoding its meaning and has shown a good performance in multiple tasks and particularly in text similarity tasks (Das et al., 2016; White et al., 2015; Gershman and Tenenbaum, 2015; Hill et al., 2016). We adopted this method to represent sentences and used only the embeddings of unigrams since bigrams and phrases are generally not well covered by the existing pre-trained embeddings1 . Before building the sentence vectors, we did not perform any normalization of the words in documents (unigrams) as words in pre-trained embeddings are not normalized. Finally, we classically defined the similarity of two sentences as the cosine similarity of their vectors. Clustering method For performing the semantic clustering of sentence"
I17-2035,E12-1022,0,0.030238,"@cea.fr Olivier Ferret CEA, LIST, Gif-sur-Yvette, F-91191 France. olivier.ferret@cea.fr Abstract its update variant. Having two or more sets of documents ordered chronologically, the update summarization task consists in summarizing the newer documents under the assumption that a user has already read the older documents. Hence, the work done for tackling this task has mainly extended the work done for MDS by taking into account the notion of novelty through different means. Wan (2012) integrates this notion in the framework of graph-based methods for computing the salience of sentences while Delort and Alfonseca (2012) achieve this integration in the context of hierarchical topic models. Li et al. (2012) go one step further in hierarchical Bayesian models by applying the paradigm of Hierarchical Dirichlet Processes to the update task. Another interesting way to consider the problem of AS is to formalize it as a constraint optimization problem based on Integer Linear Programming (ILP). ILP-based approaches are very flexible as they allow to jointly optimize several constraints and were found very successful for MDS, as illustrated by the ICSISumm system of Gillick and Favre (2009). They have also been develo"
I17-2035,C04-1051,0,0.259116,"Missing"
I17-2035,C12-1098,0,0.0162597,"ts update variant. Having two or more sets of documents ordered chronologically, the update summarization task consists in summarizing the newer documents under the assumption that a user has already read the older documents. Hence, the work done for tackling this task has mainly extended the work done for MDS by taking into account the notion of novelty through different means. Wan (2012) integrates this notion in the framework of graph-based methods for computing the salience of sentences while Delort and Alfonseca (2012) achieve this integration in the context of hierarchical topic models. Li et al. (2012) go one step further in hierarchical Bayesian models by applying the paradigm of Hierarchical Dirichlet Processes to the update task. Another interesting way to consider the problem of AS is to formalize it as a constraint optimization problem based on Integer Linear Programming (ILP). ILP-based approaches are very flexible as they allow to jointly optimize several constraints and were found very successful for MDS, as illustrated by the ICSISumm system of Gillick and Favre (2009). They have also been developed for the update summarization task, with work such as (Li et al., 2015) about the we"
I17-2035,W04-1013,0,0.01367,"imize : X 3 3.1 X wi .ci sj .lj ≤ L (1) j sj .Occij ≤ ci , ∀i, j X sj .Occij ≥ ci , ∀i Evaluation Setup For our experiments, we used the DUC 2007 update corpus and TAC 2008 and 2009 update corpora. The 3 datasets are composed respectively of 10, 48 and 44 topics. They gather respectively about 25, 20 and 20 news articles per topic. The articles are ordered chronologically and partitioned into 3 sets, A to C, for DUC 2007 and two sets, A to B, for both TAC 2008 and 2009. We only considered sets A and B for all the datasets. To evaluate our approach, we classically adopted the ROUGE2 framework (Lin, 2004), which estimates a summary score by its n-gram overlap with several reference summaries (Rn). Although our method is unsupervised, we had to tune two parameters: the similarity threshold in the clustering step (for sparcifying the input similarity matrix) and the penalization factor α in the sentence selection. As training data, we used for each dataset the two other datasets. To set up these parameters, we followed a greedy sequential approach for optimizing ROUGE on each training set. We maximized the ROUGE-2 recall score (bigrams overlap) particularly since it has shown the best agreement"
I17-2035,P09-3012,0,0.0823937,"Missing"
I17-2035,D14-1162,0,0.0817815,"whose frequency in the chronologically first set of documents (A) is greater than their frequency in the more recent document set (B). • MCL-W2V-ICSISumm. This version relies on 3 million vectors (300 dimensions) trained with the CBOW model of Word2Vec (Mikolov et al., 2013) on 100 billion words from a Google News dataset. The last two baselines, which do not include our semantic clustering of sentences, are tested to check how effective is this clustering and to what extent it is needed. • MCL-GLOVE-ICSISumm. In this run, we used 2.2 million word vectors (300 dimensions) trained with GloVe (Pennington et al., 2014) on the 840 billion tokens from the Common Crawl repository. State-of-the-art systems • Topic Modeling. This system uses topic probability distributions for salience determination and a dynamic modeling approach for redundancy control (Wang and Zhou, 2012). • MCL-ConceptNet-ICSISumm. This version computes similarities with the ConceptNet 207 Dataset MSRpara SemEvaL STS 2014 SemEvaL STS 2015 SemEvaL STS 2016 Precision Recall 91.44 88.00 90.60 88.28 17.69 14.17 11.46 25.98 the gold standard similarity is higher than 34 . We present in Table 2, the evaluation of our similarity measure using the G"
I17-2035,W09-1802,0,0.24519,"ience of sentences while Delort and Alfonseca (2012) achieve this integration in the context of hierarchical topic models. Li et al. (2012) go one step further in hierarchical Bayesian models by applying the paradigm of Hierarchical Dirichlet Processes to the update task. Another interesting way to consider the problem of AS is to formalize it as a constraint optimization problem based on Integer Linear Programming (ILP). ILP-based approaches are very flexible as they allow to jointly optimize several constraints and were found very successful for MDS, as illustrated by the ICSISumm system of Gillick and Favre (2009). They have also been developed for the update summarization task, with work such as (Li et al., 2015) about the weighting of concepts. However, the most obvious weakness of such methods, particularly the one proposed by Gillick and Favre (2009), is their implicit way of modeling information redundancy. This prevents them from exploiting work about textual entailment or paraphrase detection, which could be especially relevant in the context of MDS. In this article, we aim at extending the update summarization frameFollowing Gillick and Favre (2009), a lot of work about extractive summarization"
I17-2035,S14-2039,0,0.0203813,"he sub-sentence level is more efficient since the number of sentences is much lower, which ensures less calculations of pairwise similarities. Semantic similarity measure Sentence semantic similarity has gained a lot of interest recently, especially in the context of SemEval evaluations (Agirre et al., 2016). However, in practice, most proposed similarity measures for AS are subject to a time efficiency problem which tends to increase with the quality of the similarity measure. This is the case of the lexical word alignement based similarity that won the SemEval 2014 sentence similarity task (Sultan et al., 2014). We found it unusable in our set-up due to its computational complexity as we calculate about 5 million sentence pair similarities for some datasets while the SemEval 2014 corpus, for instance, gathers only 3,750 sentence pairs. Given this constraint, we chose a similarity measure relying on low dimensional word vectors from word embeddings. In fact, simply averaging word embeddings of all words in a sentence has proven to produce a sentence vector encoding its meaning and has shown a good performance in multiple tasks and particularly in text similarity tasks (Das et al., 2016; White et al.,"
I17-2035,N16-1162,0,0.0493411,"Missing"
I17-2035,C12-2126,0,0.0241475,"-91191 France. gael.de-chalendar@cea.fr Maˆali Mnasri CEA, LIST, Univ. Paris-Sud, Universit´e Paris-Saclay. maali.mnasri@cea.fr Olivier Ferret CEA, LIST, Gif-sur-Yvette, F-91191 France. olivier.ferret@cea.fr Abstract its update variant. Having two or more sets of documents ordered chronologically, the update summarization task consists in summarizing the newer documents under the assumption that a user has already read the older documents. Hence, the work done for tackling this task has mainly extended the work done for MDS by taking into account the notion of novelty through different means. Wan (2012) integrates this notion in the framework of graph-based methods for computing the salience of sentences while Delort and Alfonseca (2012) achieve this integration in the context of hierarchical topic models. Li et al. (2012) go one step further in hierarchical Bayesian models by applying the paradigm of Hierarchical Dirichlet Processes to the update task. Another interesting way to consider the problem of AS is to formalize it as a constraint optimization problem based on Integer Linear Programming (ILP). ILP-based approaches are very flexible as they allow to jointly optimize several constrai"
I17-2035,E17-1037,0,0.018432,"traints: one aims at maximizing the coverage of the summary with respect to its information content while the other represents its size limit. In this context, the notion of redundancy is only implicitly taken into account. In this article, we extend the framework defined by Gillick and Favre (2009) by examining how and to what extent integrating semantic sentence similarity into an update summarization system can improve its results. We show more precisely the impact of this strategy through evaluations performed on DUC 2007 and TAC 2008 and 2009 datasets. 1 Introduction As recently shown by Hirao et al. (2017) from a theoretical viewpoint, there is still room for improvements in the extractive approach of Automatic Summarization (AS), which is the framework in which our work takes place. In this context, many methods have been developed for selecting sentences according to various features and aggregating the results of this selection for building summaries. All these methods aim at selecting the most informative sentences and minimizing their redundancy while not exceeding a maximum length. In this article, we focus on Multi-Document Summarization (MDS) and more particularly on 204 Proceedings of"
I17-2035,hong-etal-2014-repository,0,0.128844,"ry score by its n-gram overlap with several reference summaries (Rn). Although our method is unsupervised, we had to tune two parameters: the similarity threshold in the clustering step (for sparcifying the input similarity matrix) and the penalization factor α in the sentence selection. As training data, we used for each dataset the two other datasets. To set up these parameters, we followed a greedy sequential approach for optimizing ROUGE on each training set. We maximized the ROUGE-2 recall score (bigrams overlap) particularly since it has shown the best agreement with manual evaluations (Hong et al., 2014). Yet, we report in what follows three variants of ROUGE: ROUGE-1, which computes the overlap with reference summaries in terms of unigrams, ROUGE-2, described previously and ROUGE-SU4, which computes the overlap of skip-grams with a skip distance of 4 words at most. Again following (Hong et al., 2014), we only report the recall values of the ROUGE metrics because their precision and fmeasure values are very close to them. i Subject to : Experiments (2) (3) j ci ∈ {0, 1} ∀i and sj ∈ {0, 1} ∀j where ci is a variable indicating the presence of the concept i in the summary; wi refers to the weigh"
I17-2035,C16-1102,0,0.0474917,"Missing"
I17-2035,N15-1145,0,\N,Missing
L16-1307,W14-2907,0,0.0454247,"Missing"
L16-1307,P11-1098,0,0.379792,"by the task of template filling. The objective of this task is to assign event roles to individual textual mentions. A template defines a specific type of events (e.g. earthquakes), associated with semantic roles (or slots) hold by entities (for earthquakes, typically their location, date, magnitude and the damages they caused (Jean-Louis et al., 2011)). This kind of structures is comparable to the schemas of (Schank and Abelson, 1977). Schema induction is the task of learning these structures with no supervision from unlabeled texts. We focus here more specifically on event schema induction (Chambers and Jurafsky, 2011; Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015). The idea is to group entities corresponding to the same role into an event template. Figure 1 illustrates this process. Previous work on event schema induction was evaluated on the MUC-4 corpus (Grishman and Sundheim, 1996). However, this corpus raises two main issues: • It was annotated with templates describing all events with the same set of slots. • It doesn’t contain redundancy. The first issue is clearly a limitation due to the fact that all the considered types of events in the MUC-4 corpus are close to each other while the se"
L16-1307,D13-1185,0,0.223806,"ng. The objective of this task is to assign event roles to individual textual mentions. A template defines a specific type of events (e.g. earthquakes), associated with semantic roles (or slots) hold by entities (for earthquakes, typically their location, date, magnitude and the damages they caused (Jean-Louis et al., 2011)). This kind of structures is comparable to the schemas of (Schank and Abelson, 1977). Schema induction is the task of learning these structures with no supervision from unlabeled texts. We focus here more specifically on event schema induction (Chambers and Jurafsky, 2011; Chambers, 2013; Cheung et al., 2013; Nguyen et al., 2015). The idea is to group entities corresponding to the same role into an event template. Figure 1 illustrates this process. Previous work on event schema induction was evaluated on the MUC-4 corpus (Grishman and Sundheim, 1996). However, this corpus raises two main issues: • It was annotated with templates describing all events with the same set of slots. • It doesn’t contain redundancy. The first issue is clearly a limitation due to the fact that all the considered types of events in the MUC-4 corpus are close to each other while the second issue is mo"
L16-1307,N13-1104,0,0.122402,"Missing"
L16-1307,C96-1079,0,0.58605,"comings. We make use of Wikinews to select the data inside the category Laws & Justice, and query Google search engine to retrieve different documents on the same events. Only Wikinews documents are manually annotated and can be used for evaluation, while the others can be used for unsupervised learning. We detail the methodology used for building the corpus and evaluate some existing systems on this new data. Keywords: Event extraction, corpus creation, unsupervised methods. 1. Introduction ... Information Extraction has been defined by the Message Understanding Conference (MUC) evaluations (Grishman and Sundheim, 1996) and its successors, i.e. the Automatic Content Extraction (ACE) (Doddington et al., 2004) and Text Analysis Conference (TAC) (Ellis et al., 2014) evaluations, specifically by the task of template filling. The objective of this task is to assign event roles to individual textual mentions. A template defines a specific type of events (e.g. earthquakes), associated with semantic roles (or slots) hold by entities (for earthquakes, typically their location, date, magnitude and the damages they caused (Jean-Louis et al., 2011)). This kind of structures is comparable to the schemas of (Schank and Ab"
L16-1307,I11-1081,1,0.894816,"Missing"
L16-1307,P15-1019,1,0.914048,"Missing"
L16-1307,W15-0812,0,0.0138618,"pes to broader domains, such as LIFE, TRANSACTION and JUSTICE, with specific roles for each of them. Moreover, in ACE, mentions of the same entity in a document are also grouped together. In relation to this last issue, the TAC KBP evaluation includes an entity linking task to match different mentions of the same entity across documents through their link to a knowledge base. However, the second issue could not be resolved solely by adding entity linking information. A more in-depth comparison and discussion of the different event annotation schemas can be found in (Aguilar et al., 2014) and (Song et al., 2015). 3. ASTRE Corpus In order to remedy the shortcomings described in the previous section, we propose a corpus with the following Figure 3: Framework for building and using the ASTRE corpus. Figure 3 gives an overview of the framework defined for building and using the ASTRE corpus. A collection of documents was first selected from a Wikinews category in English. The Google search engine was then used to retrieve documents from the Web that were similar to these seed documents, with specific time ranges. These retrieved documents were then used for inducing event schemas. At the same time, the W"
L16-1307,E12-2021,0,0.0198236,"Missing"
L16-1307,doddington-etal-2004-automatic,0,\N,Missing
P06-1036,ferret-2006-building,1,0.672118,"t that they rely too heavily on WordNet and some of its more sophisticated features (such as the definitions associated with the synsets). While often being exploited by acquisition methods, these features are generally lacking in similar lexico-semantic networks. Moreover, these methods attempt to learn topical knowledge from a lexical network rather than topical relations. Since our goal is different, we have chosen not to rely on any significant resource, all the more as we would like our method to be applicable to a wide array of languages. In consequence, we took an incremental approach (Ferret, 2006): starting from a network of lexical co-occurrences7 collected from a large corpus, we used these latter to select potential topical relations by using a topical analyzer. 4.2 From a network of co-occurrences to a set of Topical Units We start by extracting lexical co-occurrences from a corpus to build a network. To this end we follow the method introduced by (Church and Hanks, 1990), i.e. by sliding a window of a given size over some texts. The parameters of this extraction were set in such a way as to catch the most obvious topical relations: the window was fairly large (20-words wide), and"
P06-1036,C02-1033,1,0.933224,"must contain many kinds of relations on the syntagmatic and paradigmatic axis to allow for natural and flexible access of words. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al., 1998) contain them. However, as various researchers have pointed out (Harabagiu et al., 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario. Word-pairs like doctor–hospital, burglar–policeman or plane–airport, are examples in case. The lack of such topical relations in resources like WordNet has been dubbed as the tennis problem (Roger Chaffin, cited in Fellbaum, 1998). Some of these links have been introduced more recently in WordNet via the domain relation. Yet their number remains still very small. For instance, WordNet 2.1 does not contain any of the three associations mentioned here above, despite their high frequency."
P06-1036,W99-0501,0,0.0573433,"Missing"
P06-1036,magnini-cavaglia-2000-integrating,0,0.0305485,"eir high frequency. The lack of systematicity of these topical relations makes their extraction and typing very difficult on a large scale. This is why some researchers have proposed to use automatic learning techniques to extend lexical networks like WordNet. In (Harabagiu & Moldovan, 1998), this was done by extracting topical relations from the glosses associated to the synsets. Other researchers used external sources: Mandala et al. (1999) integrated co-occurrences and a thesaurus to WordNet for query expansion; Agirre et al. (2001) built topic signatures from texts in relation to synsets; Magnini and Cavagliá (2000) annotated the synsets with Subject Field Codes. This last idea has been taken up and extended by (Avancini et al., 2003) who expanded the domains built from this annotation. Despite the improvements, all these approaches are limited by the fact that they rely too heavily on WordNet and some of its more sophisticated features (such as the definitions associated with the synsets). While often being exploited by acquisition methods, these features are generally lacking in similar lexico-semantic networks. Moreover, these methods attempt to learn topical knowledge from a lexical network rather th"
P06-1036,P98-2180,0,0.0907252,"e some associations are too complex to be extracted automatically by machine, others are clearly within reach. We will illustrate in the next section how this can be achieved. 4 Automatic extraction of topical relations 4.1 Definition of the problem We have argued in the previous sections that dictionaries must contain many kinds of relations on the syntagmatic and paradigmatic axis to allow for natural and flexible access of words. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al., 1998) contain them. However, as various researchers have pointed out (Harabagiu et al., 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario. Word-pairs like doctor–hospital, burglar–policeman or plane–airport, are examples in case. The lack of such topical relations in resources like WordNet has been dubbed as the tennis problem (Ro"
P06-1036,W02-1118,1,0.806365,"et al., 1999) evaluates the probability that a randomly chosen pair of words, separated by k words, is wrongly classified, i.e. they are found in the same segment by TOPICOLL, while they are actually in different ones (miss of a document break), or they are found in different segments, while they are actually in the same one (false alarm). method is an effective way of selecting topical relations by preference. 5 Discussion and conclusion We have raised and partially answered the question of how a dictionary should be indexed in order to support word access, a question initially addressed in (Zock, 2002) and (Zock and Bilac, 2004). We were particularly concerned with the language producer, as his needs (and knowledge at the onset) are quite different from the ones of the language receiver (listener/reader). It seems that, in order to achieve our goal, we need to do two things: add to an existing electronic dictionary information that people tend to associate with a word, that is, build and enrich a semantic network, and provide a tool to navigate in it. To this end we have suggested to label the links, as this would reduce the graph complexity and allow for type-based navigation. Actually our"
P06-1036,W04-2105,1,0.893887,"in the previous sections that dictionaries must contain many kinds of relations on the syntagmatic and paradigmatic axis to allow for natural and flexible access of words. Synonymy, hypernymy or meronymy fall clearly in this latter category, and well known resources like WordNet (Miller, 1995), EuroWordNet (Vossen, 1998) or MindNet (Richardson et al., 1998) contain them. However, as various researchers have pointed out (Harabagiu et al., 1999), these networks lack information, in particular with regard to syntagmatic associations, which are generally unsystematic. These latter, called TIORA (Zock and Bilac, 2004) or topical relations (Ferret, 2002) account for the fact that two words refer to the same topic, or take part in the same situation or scenario. Word-pairs like doctor–hospital, burglar–policeman or plane–airport, are examples in case. The lack of such topical relations in resources like WordNet has been dubbed as the tennis problem (Roger Chaffin, cited in Fellbaum, 1998). Some of these links have been introduced more recently in WordNet via the domain relation. Yet their number remains still very small. For instance, WordNet 2.1 does not contain any of the three associations mentioned here"
P06-1036,J90-1003,0,\N,Missing
P06-1036,E99-1013,0,\N,Missing
P06-1036,C98-2175,0,\N,Missing
P07-1061,W01-0514,0,0.184021,"systems do not require external knowledge, they are not sensitive to domains but they are limited by the type of documents they can be applied to: lexical reiteration is reliable only if concepts are not too frequently ex480 pressed by several means (synonyms, etc.) and discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in (Kozima, 1993); a thesaurus in (Morris and Hirst, 1991); a large set of lexical cooccurrences collected from a corpus in (Choi et al., 2001). To a certain extent, these lexical networks enable topic segmenters to exploit a sort of concept reiteration. However, their lack of any explicit topical structure makes this kind of knowledge difficult to use when lexical ambiguity is high. The most simple solution to this problem is to exploit knowledge about the topics that may occur in documents. Such topic models are generally built from a large set of example documents as in (Yamron et al., 1998), (Blei and Moreno, 2001) or in one component of (Beeferman et al., 1999). These statistical topic models enable segmenters to improve their p"
P07-1061,A00-2004,0,0.638253,"uation done both for French and English the interest of the method we propose. 1 Introduction In this article, we address the problem of linear topic segmentation, which consists in segmenting documents into topically homogeneous segments that does not overlap each other. This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as (Hearst, 1994). One criterion for classifying topic segmentation systems is the kind of knowledge they depend on. Most of them only rely on surface features of documents: word reiteration in (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003) or discourse cues in (Passonneau and Litman, 1997; Galley et al., 2003). As such systems do not require external knowledge, they are not sensitive to domains but they are limited by the type of documents they can be applied to: lexical reiteration is reliable only if concepts are not too frequently ex480 pressed by several means (synonyms, etc.) and discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictio"
P07-1061,W06-1320,0,0.0403401,"Missing"
P07-1061,P94-1002,0,0.860805,"ethod for discovering the topics of a text. Then, we detail how these topics are used by segmentation for finding topical similarities between text segments. Finally, we show through the results of an evaluation done both for French and English the interest of the method we propose. 1 Introduction In this article, we address the problem of linear topic segmentation, which consists in segmenting documents into topically homogeneous segments that does not overlap each other. This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as (Hearst, 1994). One criterion for classifying topic segmentation systems is the kind of knowledge they depend on. Most of them only rely on surface features of documents: word reiteration in (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003) or discourse cues in (Passonneau and Litman, 1997; Galley et al., 2003). As such systems do not require external knowledge, they are not sensitive to domains but they are limited by the type of documents they can be applied to: lexical reiteration is reliable only if concepts are not too frequently ex480 pressed by several means (synonyms, etc.)"
P07-1061,P98-1100,0,0.648556,"nd of knowledge difficult to use when lexical ambiguity is high. The most simple solution to this problem is to exploit knowledge about the topics that may occur in documents. Such topic models are generally built from a large set of example documents as in (Yamron et al., 1998), (Blei and Moreno, 2001) or in one component of (Beeferman et al., 1999). These statistical topic models enable segmenters to improve their precision but they also restrict their scope. Hybrid systems that combine the approaches we have presented were also developed and illustrated the interest of such a combination: (Jobbins and Evett, 1998) combined word recurrence, co-occurrences and a thesaurus; (Beeferman et al., 1999) relied on both lexical modeling and discourse cues; (Galley et al., 2003) made use of word reiteration through lexical chains and discourse cues. The work we report in this article takes place in the first category we have presented. It does not rely on any a priori knowledge and exploits word usage rather than discourse cues. More precisely, we present a new method for enhancing the results Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 480–487, c Prague, Czech Re"
P07-1061,P93-1041,0,0.460879,"and Isahara, 2001; Galley et al., 2003) or discourse cues in (Passonneau and Litman, 1997; Galley et al., 2003). As such systems do not require external knowledge, they are not sensitive to domains but they are limited by the type of documents they can be applied to: lexical reiteration is reliable only if concepts are not too frequently ex480 pressed by several means (synonyms, etc.) and discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in (Kozima, 1993); a thesaurus in (Morris and Hirst, 1991); a large set of lexical cooccurrences collected from a corpus in (Choi et al., 2001). To a certain extent, these lexical networks enable topic segmenters to exploit a sort of concept reiteration. However, their lack of any explicit topical structure makes this kind of knowledge difficult to use when lexical ambiguity is high. The most simple solution to this problem is to exploit knowledge about the topics that may occur in documents. Such topic models are generally built from a large set of example documents as in (Yamron et al., 1998), (Blei and More"
P07-1061,J91-1002,0,0.879525,"l., 2003) or discourse cues in (Passonneau and Litman, 1997; Galley et al., 2003). As such systems do not require external knowledge, they are not sensitive to domains but they are limited by the type of documents they can be applied to: lexical reiteration is reliable only if concepts are not too frequently ex480 pressed by several means (synonyms, etc.) and discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in (Kozima, 1993); a thesaurus in (Morris and Hirst, 1991); a large set of lexical cooccurrences collected from a corpus in (Choi et al., 2001). To a certain extent, these lexical networks enable topic segmenters to exploit a sort of concept reiteration. However, their lack of any explicit topical structure makes this kind of knowledge difficult to use when lexical ambiguity is high. The most simple solution to this problem is to exploit knowledge about the topics that may occur in documents. Such topic models are generally built from a large set of example documents as in (Yamron et al., 1998), (Blei and Moreno, 2001) or in one component of (Beeferm"
P07-1061,J97-1005,0,0.0984824,"ntroduction In this article, we address the problem of linear topic segmentation, which consists in segmenting documents into topically homogeneous segments that does not overlap each other. This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as (Hearst, 1994). One criterion for classifying topic segmentation systems is the kind of knowledge they depend on. Most of them only rely on surface features of documents: word reiteration in (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003) or discourse cues in (Passonneau and Litman, 1997; Galley et al., 2003). As such systems do not require external knowledge, they are not sensitive to domains but they are limited by the type of documents they can be applied to: lexical reiteration is reliable only if concepts are not too frequently ex480 pressed by several means (synonyms, etc.) and discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in (Kozima, 1993); a thesaurus in (Morris and Hirst, 1991); a large set of lexical cooccurrenc"
P07-1061,J02-1002,0,0.151698,"Missing"
P07-1061,P06-1003,0,0.0376197,"Missing"
P07-1061,P01-1064,0,0.503721,"both for French and English the interest of the method we propose. 1 Introduction In this article, we address the problem of linear topic segmentation, which consists in segmenting documents into topically homogeneous segments that does not overlap each other. This part of the Discourse Analysis field has received a constant interest since the initial work in this domain such as (Hearst, 1994). One criterion for classifying topic segmentation systems is the kind of knowledge they depend on. Most of them only rely on surface features of documents: word reiteration in (Hearst, 1994; Choi, 2000; Utiyama and Isahara, 2001; Galley et al., 2003) or discourse cues in (Passonneau and Litman, 1997; Galley et al., 2003). As such systems do not require external knowledge, they are not sensitive to domains but they are limited by the type of documents they can be applied to: lexical reiteration is reliable only if concepts are not too frequently ex480 pressed by several means (synonyms, etc.) and discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in (Kozima, 1993); a t"
P07-1061,P03-1071,0,\N,Missing
P07-1061,C98-1097,0,\N,Missing
P13-1055,N07-1026,0,0.245807,"Missing"
P13-1055,heylen-etal-2008-modelling,0,0.0567337,"ors of its entries increases (see Section 4.1 for an illustration), which means in practice that only the first neighbors of an entry can be generally exploited. Markovitch, 2007). The last option is the corpusbased approach, based on the distributional hypothesis (Firth, 1957): each word is characterized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of context elements according to the semanti"
P13-1055,J06-1003,0,0.0929433,"s of each of these neighbors. We evaluate the interest of this method for a large set of English nouns with various frequencies. 1 Following work such as (Grefenstette, 1994), a widespread way to build a thesaurus from a corpus is to use a semantic similarity measure for extracting the semantic neighbors of the entries of the thesaurus. Three main ways of implementing such measures can be distinguished. The first one relies on handcrafted resources in which semantic relations are clearly identified. Work based on WordNet-like lexical networks for building semantic similarity measures such as (Budanitsky and Hirst, 2006) or (Pedersen et al., 2004) falls into this category. These measures typically exploit the hierarchical structure of these networks, based on hypernymy relations. The second approach makes use of a less structured source of knowledge about words such as the definitions of classical dictionaries or the glosses of WordNet. WordNet’s glosses were used to support Lesklike measures in (Banerjee and Pedersen, 2003) and more recently, measures were also defined from Wikipedia or Wiktionaries (Gabrilovich and Introduction The work we present in this article focuses on the automatic building of a thesa"
P13-1055,P12-1092,0,0.104999,"rned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of context elements according to the semantic neighbors found by an initial distributional similarity measure. However, another part of these proposals implies more radical changes. The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pad´o and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al., 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al., 2010) can be classified into this second category. The work we present in this article takes place in the framework defined by (Grefenstette, 1994) for implementing the distributional approach but proposes a new method for improving a thesaurus built in this context based on the identification of its bad semantic neighbors rather than on the adaptation of the weight of their features. 2 The approach we propose for identifying the bad semantic neighbors of a thesaurus entry relies on the distributional h"
P13-1055,P02-1030,0,0.558819,"strongly decreases as the rank of the neighbors of its entries increases (see Section 4.1 for an illustration), which means in practice that only the first neighbors of an entry can be generally exploited. Markovitch, 2007). The last option is the corpusbased approach, based on the distributional hypothesis (Firth, 1957): each word is characterized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of"
P13-1055,W02-0908,0,0.934927,"strongly decreases as the rank of the neighbors of its entries increases (see Section 4.1 for an illustration), which means in practice that only the first neighbors of an entry can be generally exploited. Markovitch, 2007). The last option is the corpusbased approach, based on the distributional hypothesis (Firth, 1957): each word is characterized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of"
P13-1055,P10-1026,0,0.199974,"Missing"
P13-1055,P10-2017,0,0.133523,"09), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of context elements according to the semantic neighbors found by an initial distributional similarity measure. However, another part of these proposals implies more radical changes. The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pad´o and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al., 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al., 2010) can be classified into this second category. The work we present in this article takes place in the framework defined by (Grefenstette, 1994) for implementing the distributional approach but proposes a new method for improving a thesaurus built in this context based on the identification of its bad semantic neighbors rather than on the adaptation of the weight of their features. 2 The approach we propose for identifying the bad semantic neighbors"
P13-1055,W02-1006,0,0.137466,"ive model of the entry has to be applied to occurrences of this neighbor in texts. Hence, the method we propose for improving a distributional thesaurus applies the following process to each of its entries: • building of a classifier for determining whether a word in a sentence corresponds or not to the entry; • selection of a set of examples sentences for each of the neighbors of the entry in the the563 rence in a text of the reference word E: • neighboring words; • Part-of-Speech (POS) of neighboring words; • local collocations. Only features based on syntactic relations are not taken from (Lee and Ng, 2002) since their use would have not been coherent with the window based approach of the building of our initial thesaurus. For the neighboring words features, we consider all plain words (common and proper nouns, verbs and adjectives) and adverbs that are present in the same sentence of an occurrence of E. Each neighboring word is represented under its lemma form as a binary feature whose value is equal to 1 when it is present in the same sentence as E. For the second type of features, we take more precisely the POS of the three words before E and those of the three words after E. Each pair {POS,"
P13-1055,ferret-2010-testing,1,0.894082,"sing texts to part-of-speech tagging and lemmatization to make possible the transposition of our method to a large set of languages. This seems to be a reasonable compromise between the approach of (Freitag et al., 2005), in which none normalization of words is done, and the more widespread use of syntactic parsers in work such as (Lin, 1998). More precisely, we used TreeTagger (Schmid, 1994) for performing the linguistic preprocessing of the AQUAINT-2 corpus. For the extraction of distributional data and the characteristics of the distributional similarity measure, we adopted the options of (Ferret, 2010), resulting from a kind of grid search procedure performed with the extended TOEFL test proposed in (Freitag et al., 2005) as an optimization objective. More precisely, the following characteristics were taken: • distributional contexts made of the cooccurrents collected in a 3 word window centered on each occurrence in the corpus of the target word. These co-occurrents were restricted to nouns, verbs and adjectives; • soft filtering of contexts: removal of cooccurrents with only one occurrence; • weighting function of co-occurrents in conImproving a distributional thesaurus Overview The princ"
P13-1055,P98-2127,0,0.930114,"g to use since the quality of such thesaurus strongly decreases as the rank of the neighbors of its entries increases (see Section 4.1 for an illustration), which means in practice that only the first neighbors of an entry can be generally exploited. Markovitch, 2007). The last option is the corpusbased approach, based on the distributional hypothesis (Firth, 1957): each word is characterized by the set of contexts from a corpus in which it appears and the semantic similarity of two words is computed from the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that pro"
P13-1055,W04-2607,0,0.0656414,"Missing"
P13-1055,W05-0604,0,0.73741,"As in (Lin, 1998) or (Curran and Moens, 2002a), this building is based on the definition of a semantic similarity measure from a corpus. The corpus used for defining this measure was the AQUAINT-2 corpus, a middlesize corpus made of around 380 million words coming from news articles. Although our target language is English, we chose to limit deliberately the level of the tools applied for preprocessing texts to part-of-speech tagging and lemmatization to make possible the transposition of our method to a large set of languages. This seems to be a reasonable compromise between the approach of (Freitag et al., 2005), in which none normalization of words is done, and the more widespread use of syntactic parsers in work such as (Lin, 1998). More precisely, we used TreeTagger (Schmid, 1994) for performing the linguistic preprocessing of the AQUAINT-2 corpus. For the extraction of distributional data and the characteristics of the distributional similarity measure, we adopted the options of (Ferret, 2010), resulting from a kind of grid search procedure performed with the extended TOEFL test proposed in (Freitag et al., 2005) as an optimization objective. More precisely, the following characteristics were tak"
P13-1055,J07-2002,0,0.866154,"Missing"
P13-1055,N04-3012,0,0.028297,"evaluate the interest of this method for a large set of English nouns with various frequencies. 1 Following work such as (Grefenstette, 1994), a widespread way to build a thesaurus from a corpus is to use a semantic similarity measure for extracting the semantic neighbors of the entries of the thesaurus. Three main ways of implementing such measures can be distinguished. The first one relies on handcrafted resources in which semantic relations are clearly identified. Work based on WordNet-like lexical networks for building semantic similarity measures such as (Budanitsky and Hirst, 2006) or (Pedersen et al., 2004) falls into this category. These measures typically exploit the hierarchical structure of these networks, based on hypernymy relations. The second approach makes use of a less structured source of knowledge about words such as the definitions of classical dictionaries or the glosses of WordNet. WordNet’s glosses were used to support Lesklike measures in (Banerjee and Pedersen, 2003) and more recently, measures were also defined from Wikipedia or Wiktionaries (Gabrilovich and Introduction The work we present in this article focuses on the automatic building of a thesaurus from a corpus. As illu"
P13-1055,N10-1013,0,0.0496816,"Missing"
P13-1055,W10-3906,0,0.454427,"(Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of context elements according to the semantic neighbors found by an initial distributional similarity measure. However, another part of these proposals implies more radical changes. The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pad´o and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al., 2012) or the redefinition of the distributional approach in a Bayesian framework (Kazama et al.,"
P13-1055,J09-3004,0,0.445625,"the contexts they share. This perspective was first adopted by (Grefenstette, 1994) and (Lin, 1998) and then, explored in details in (Curran and Moens, 2002b), (Weeds, 2003) or (Heylen et al., 2008). The problem of improving the results of the “classical” implementation of the distributional approach as it can be found in (Curran and Moens, 2002a) for instance was already tackled by some work. A part of these proposals focus on the weighting of the elements that are part of the contexts of words such as (Broda et al., 2009), in which the weights of context elements are turned into ranks, or (Zhitomirsky-Geffet and Dagan, 2009), followed and extended by (Yamamoto and Asakura, 2010), that proposes a bootstrapping method for modifying the weights of context elements according to the semantic neighbors found by an initial distributional similarity measure. However, another part of these proposals implies more radical changes. The use of dimensionality reduction techniques, for instance Latent Semantic Analysis in (Pad´o and Lapata, 2007), the multi-prototype (Reisinger and Mooney, 2010) or examplar-based models (Erk and Pado, 2010), the Deep Learning approach of (Huang et al., 2012) or the redefinition of the distribut"
P13-1055,C98-2122,0,\N,Missing
P15-1019,D13-1185,0,0.698682,"d weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to #2 #3 #4 Attributes [armed:amod] Head man [police:nn] [] [innocent:amod, young:amod] statio"
P15-1019,N13-1104,0,0.493723,"forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to #2 #3 #4 Attributes [armed:amod] Head man [police:nn] [] [innocent:amod, young:amod] station policeman man Triggers"
P15-1019,eichler-etal-2008-unsupervised,0,0.0170829,", 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013"
P15-1019,D13-1178,0,0.515493,"tion extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to #2 #3 #4 Attributes [armed:amod] Head man [police:nn] [] [innocent:amod, young:amod] station policeman man Triggers [attack:nsubj, kill:nsubj] [attack:dobj] [kill:dobj] [wound:dobj] Figure 1: Entity representation as tuples of ([attributes], head, [triggers]"
P15-1019,P06-2027,0,0.200404,"e, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like seman"
P15-1019,P08-1090,0,0.469532,"including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to #2 #3 #4 Attributes [armed:amod] Head man [police:nn] [] [innocent:amod, young:amod] station policeman man Triggers [attack:nsubj, kill:nsubj] [attack:dobj"
P15-1019,D11-1133,0,0.0226058,"rist attacks, entities that are objects of verbs to kill, to attack can be grouped together and characterized by a role 188 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 188–197, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Related Work #1 Despite efforts made for making template filling as generic as possible, it still depends heavily on the type of events. Mixing generic processes with a restrictive number of domainspecific rules (Freedman et al., 2011) or examples (Grishman and He, 2014) is a way to reduce the amount of effort needed for adapting a system to another domain. The approaches of Ondemand information extraction (Hasegawa et al., 2004; Sekine, 2006) and Preemptive Information Extraction (Shinyama and Sekine, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding syst"
P15-1019,P09-1068,0,0.508036,"their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to #2 #3 #4 Attributes [armed:amod] Head man [police:nn] [] [innocent:amod, young:amod] station policeman man Triggers [attack:nsubj, kill:nsubj] [attack:dobj] [kill:dobj] [wound:dobj] Fig"
P15-1019,E14-1006,0,0.205916,"Missing"
P15-1019,P11-1098,0,0.573312,"Missing"
P15-1019,E14-1024,0,0.378934,"the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to #2 #3 #4 Attributes [armed:amod] Head man [police:nn] [] [innocent:amod, young:amod] station policeman man Triggers [attack:nsubj, kill:nsubj] [attack:dobj] [kill:dobj] [wound:dobj] Figure 1: Entity representation as tuples of ([attributes], head, [triggers]). events, such as (Frermann et al., 2014), (Pichotta and Mooney, 2014) or (Modi and Titov, 2014). The work we present in this article is in line with Chambers (2013), which will be described in more details in Section 5, together with a quantitative and qualitative comparison. 3 Entity Representation An entity is represented as a triple containing: a head word h, a list A of attribute relations and a list T of trigger relations. Consider the following example: (1) Two armed men attacked the police station and killed a policeman. An innocent young man was also wounded. As illustrated in Figure 1, four entities, equivalent to four separated triples, are generated"
P15-1019,C96-1079,0,0.464628,"ntities. However, elements other than head words contain useful information. For instance, an armed man is more discriminative than man. Our model takes into account this information and precisely represents it using probabilistic topic distributions. We illustrate that such information plays an important role in parameter estimation. Mostly, it makes topic distributions more coherent and more discriminative. Experimental results on benchmark dataset empirically confirm this enhancement. 1 Introduction Information Extraction was initially defined (and is still defined) by the MUC evaluations (Grishman and Sundheim, 1996) and more specifically by the task of template filling. The objective of this task is to assign event roles to individual textual mentions. A template defines a specific type of events (e.g. earthquakes), associated with semantic roles (or slots) hold by entities (for earthquakes, their location, date, magnitude and the damages they caused (Jean-Louis et al., 2011)). Schema induction is the task of learning these templates with no supervision from unlabeled text. We focus here on event schema induction and continue the trend of generative models proposed earlier for this task. The idea is to g"
P15-1019,I08-1021,0,0.123869,"xtraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All"
P15-1019,P10-1100,0,0.0731721,"ation (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event roles and the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induct"
P15-1019,P04-1053,0,0.0303377,"l Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 188–197, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Related Work #1 Despite efforts made for making template filling as generic as possible, it still depends heavily on the type of events. Mixing generic processes with a restrictive number of domainspecific rules (Freedman et al., 2011) or examples (Grishman and He, 2014) is a way to reduce the amount of effort needed for adapting a system to another domain. The approaches of Ondemand information extraction (Hasegawa et al., 2004; Sekine, 2006) and Preemptive Information Extraction (Shinyama and Sekine, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998)"
P15-1019,I11-1081,1,0.884616,"ns more coherent and more discriminative. Experimental results on benchmark dataset empirically confirm this enhancement. 1 Introduction Information Extraction was initially defined (and is still defined) by the MUC evaluations (Grishman and Sundheim, 1996) and more specifically by the task of template filling. The objective of this task is to assign event roles to individual textual mentions. A template defines a specific type of events (e.g. earthquakes), associated with semantic roles (or slots) hold by entities (for earthquakes, their location, date, magnitude and the damages they caused (Jean-Louis et al., 2011)). Schema induction is the task of learning these templates with no supervision from unlabeled text. We focus here on event schema induction and continue the trend of generative models proposed earlier for this task. The idea is to group together entities corresponding to the same role in an event template based on the similarity of the relations that these entities hold with predicates. For example, in a corpus about terrorist attacks, entities that are objects of verbs to kill, to attack can be grouped together and characterized by a role 188 Proceedings of the 53rd Annual Meeting of the Ass"
P15-1019,P06-2094,0,0.118107,"Missing"
P15-1019,P14-5010,0,0.00472954,"policeman. An innocent young man was also wounded. As illustrated in Figure 1, four entities, equivalent to four separated triples, are generated from the text above. Head words are extracted from noun phrases. A trigger relation is composed of a predicate (attack, kill, wound) and a dependency type (subject, object). An attribute relation is composed of an argument (armed, police, young) and a dependency type (adjectival, nominal or verbal modifier). In the relationship to triggers, a head word is argument, but in the relationship to attributes, it is predicate. We use Stanford NLP toolkit (Manning et al., 2014) for parsing and coreference resolution. A head word is extracted if it is a nominal or proper noun and it is related to at least one trigger; pronouns are omitted. A trigger of an head word is extracted if it is a verb or an eventive noun and the head word serves as its subject, object, or preposition. We use the categories noun.EVENT and noun.ACT in WordNet as a list of eventive nouns. A head word can have more than one trigger. These multiple relations can come from a syntactic coordination inside a single sentence, as it is the case in the first sentence of the illustrating example. They c"
P15-1019,N06-1039,0,0.0167643,"Processing, pages 188–197, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Related Work #1 Despite efforts made for making template filling as generic as possible, it still depends heavily on the type of events. Mixing generic processes with a restrictive number of domainspecific rules (Freedman et al., 2011) or examples (Grishman and He, 2014) is a way to reduce the amount of effort needed for adapting a system to another domain. The approaches of Ondemand information extraction (Hasegawa et al., 2004; Sekine, 2006) and Preemptive Information Extraction (Shinyama and Sekine, 2006) tried to overcome this difficulty in another way by exploiting templates induced from representative documents selected by queries. Event schema induction takes root in work on the acquisition from text of knowledge structures, such as the Memory Organization Packets (Schank, 1980), used by early text understanding systems (DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova"
P15-1019,D12-1094,0,0.0125696,"DeJong, 1982) and more recently by Ferret and Grau (1997). First attempts for applying such processes to schema induction have been made in the fields of Information Extraction (Collier, 1998), Automatic Summarization (Harabagiu, 2004) and event QuestionAnswering (Filatova et al., 2006; Filatova, 2008). More recently, work after (Hasegawa et al., 2004) has developed weakly supervised forms of Information Extraction including schema induction in their objectives. However, they have been mainly applied to binary relation extraction in practice (Eichler et al., 2008; Rosenfeld and Feldman, 2007; Min et al., 2012). In parallel, several approaches were proposed for performing specifically schema induction in already existing frameworks: clause graph clustering (Qiu et al., 2008), event sequence alignment (Regneri et al., 2010) or LDA-based approach relying on FrameNet-like semantic frames (Bejan, 2008). More event-specific generative models were proposed by Chambers (2013) and Cheung et al. (2013). Finally, Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Chambers and Jurafsky (2011), improved by Balasubramanian et al. (2013), and Chambers (2013) focused specifically on the induction of event"
P15-1019,H91-1059,0,0.0570363,"assification. We then compare our results with previous approaches, more particularly with Chambers (2013), from both quantitative and qualitative points of view (Section 5.4). Finally, Section 5.5 is dedicated to error analysis, with a special emphasis on sources of false positives. 5 5.1 Evaluations Experimental Setups 5.1.1 Datasets The MUC-4 corpus contains 1,700 news articles about terrorist incidents happening in Latin America. The corpus is divided into 1,300 documents In order to compare with related work, we evaluated our method on the Message Understanding Conference (MUC-4) corpus (Sundheim, 1991) using precision, recall and F-score as conventional 191 instrument Heads Triggers bomb explode:nsubj fire hear:dobj explosion place:dobj blow cause:nsubj charge set:dobj KIDNAPPING victim Attributes Heads Triggers several:amod people arrest:dobj other:amod person kidnap:dobj responsible:amod man release:dobj military:amod member kill:dobj young:amod leader identify:prep as BOMBING for the development set and four test sets, each containing 100 documents. We follow the rules in the literature to guarantee comparable results (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). The evalua"
P15-1019,W14-1606,0,0.177117,"nd the identification of chains of events for building representations from texts by exploiting coreference resolution or the temporal ordering of events. All this work is also linked to work about the induction of scripts from texts, more or less closely linked to #2 #3 #4 Attributes [armed:amod] Head man [police:nn] [] [innocent:amod, young:amod] station policeman man Triggers [attack:nsubj, kill:nsubj] [attack:dobj] [kill:dobj] [wound:dobj] Figure 1: Entity representation as tuples of ([attributes], head, [triggers]). events, such as (Frermann et al., 2014), (Pichotta and Mooney, 2014) or (Modi and Titov, 2014). The work we present in this article is in line with Chambers (2013), which will be described in more details in Section 5, together with a quantitative and qualitative comparison. 3 Entity Representation An entity is represented as a triple containing: a head word h, a list A of attribute relations and a list T of trigger relations. Consider the following example: (1) Two armed men attacked the police station and killed a policeman. An innocent young man was also wounded. As illustrated in Figure 1, four entities, equivalent to four separated triples, are generated from the text above. Head"
P15-1019,D07-1075,0,0.0392679,"the Message Understanding Conference (MUC-4) corpus (Sundheim, 1991) using precision, recall and F-score as conventional 191 instrument Heads Triggers bomb explode:nsubj fire hear:dobj explosion place:dobj blow cause:nsubj charge set:dobj KIDNAPPING victim Attributes Heads Triggers several:amod people arrest:dobj other:amod person kidnap:dobj responsible:amod man release:dobj military:amod member kill:dobj young:amod leader identify:prep as BOMBING for the development set and four test sets, each containing 100 documents. We follow the rules in the literature to guarantee comparable results (Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). The evaluation focuses on four template types – ARSON, ATTACK, BOMBING , KIDNAPPING – and four slots – Perpetrator, Instrument, Target, and Victim. Perpetrator is merged from Perpetrator Individual and Perpetrator Organization. The matching between system answers and references is based on head word matching. A head word is defined as the rightmost word of the phrase or as the right-most word of the first ‘of’ if the phrase contains any. Optional templates and slots are ignored when calculating recall. Template types are ignored in evaluation: this means that a"
P15-1019,D09-1016,0,0.0517084,"Missing"
P15-1019,C04-1084,0,\N,Missing
P15-2077,P14-1023,0,0.0573016,"corresponds to the measures for our thesaurus of compounds. It should be note that in the case of the A2ST-comp thesaurus, the number of evaluated entries is very small, restricted to 813 entries, with also a very small number of reference synonyms by entry. Hence, the results of the evaluation of A2ST-comp have to be considered with caution even if their high level for the very first semantic neighbors tends to confirm the positive impact of the low level of ambiguity of compounds compared to mono-terms. The two following rows gives the results of the thesauri built from the best models of (Baroni et al., 2014), B14-count for the count model, whose main parameters are close or identical to ours, and B14-predict for the predict model, built from (Mikolov et al., 2013). These results first illustrate the known importance of corpus size, as the (Baroni et al., 2014)’s corpus is more than 7 times larger than ours, and the fact that for building thesauri, the count model is superior to the predict model. This last observation is confirmed by the results of the skip-gram model of (Mikolov et al., 2013) with its best parameters2 for our corpus (5th row), which clearly exhibits worst results than initial. F"
P15-2077,P10-1026,0,0.0273428,"Missing"
P15-2077,P98-2127,0,0.67761,"othesis of compounds, for achieving this selection in a more indirect way and show the interest to combine it with the criterion of (Ferret, 2012) for building a large training set with a reasonable error rate. We address the second issue by following (Hagiwara et al., 2009), which defined a Support Vector Machine (SVM) model for deciding whether two words are similar or not. In our context, a positive example is a pair of nouns that are Introduction The work presented in this article aims at improving thesauri built following the distributional approach as implemented by (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002). A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007"
P15-2077,W02-0908,0,0.517012,"compounds, for achieving this selection in a more indirect way and show the interest to combine it with the criterion of (Ferret, 2012) for building a large training set with a reasonable error rate. We address the second issue by following (Hagiwara et al., 2009), which defined a Support Vector Machine (SVM) model for deciding whether two words are similar or not. In our context, a positive example is a pair of nouns that are Introduction The work presented in this article aims at improving thesauri built following the distributional approach as implemented by (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002). A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisi"
P15-2077,N13-1090,0,0.0881998,"onal contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by se470 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 470–476, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics percentage of the vocabulary of compounds): semantically similar while a negative example is a pa"
P15-2077,W02-1029,0,0.874904,"Missing"
P15-2077,D10-1113,0,0.0261545,"ighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by se470 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 470–476, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics percentage of the vocabulary of compounds): semantically similar while a negative example is a pair of non similar nouns. The features of each pair of nouns are built by summing the"
P15-2077,P10-2017,0,0.0122236,"focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by se470 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 470–476, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics percentage of the vocabulary of com"
P15-2077,J07-2002,0,0.0535921,"Missing"
P15-2077,ferret-2010-testing,1,0.90152,"rank would have guaranteed fewer false negative examples but taking neighbors with a rather small rank for building negative examples is more useful in terms of discrimination. 4 4.1 Experiments and evaluation Building of distributional thesauri The first step of the work we present is the building of two distributional thesauri: the thesaurus of mono-terms to improve (A2ST) and a thesaurus of compounds (A2ST-comp). Similarly to (Ferret, 2012), they were both built from the AQUAINT-2 corpus, a 380 million-word corpus of news articles in English. The building procedure, defined 471 method by (Ferret, 2010), was also identical to (Ferret, 2012), with distributional contexts compared with the Cosine measure and made of window-based lemmatized cooccurrents (1 word before and after) weighted by Positive Pointwise Mutual Information (PPMI). For the thesaurus of compounds, a preprocessing step was added to identify nominal compounds in texts. This identification was done in two steps: first, a set of compounds were extracted from the AQUAINT-2 corpus by relying on a restricted set of morpho-syntactic patterns applied by the Multiword Expression Toolkit (mwetoolkit) (Ramisch et al., 2010); then, the m"
P15-2077,D14-1047,0,0.0235328,"Missing"
P15-2077,P14-1009,0,0.0256069,"Missing"
P15-2077,P13-1055,1,0.71028,"t RRF is clearly superior to condorcet but only weakly superior to borda. Finally, the last row of Table 2 – CS-w-Mik – illustrates one step further the interest of ensemble methods for distributional thesauri: whereas the “Mikolov thesaurus” gets the worst results among all the thesauri of Table 2, adding it to the initial, symmetry and compound thesauri in the CombSum method leads to improve both R-precision and MAP, with a only small decrease of P@1 and P@5. From a more global perspective, it is interesting to note that our best method, CombSum, clearly outperforms the reranking method of (Ferret, 2013) with the same initial starting point. 5 Table 3: Reranking for the entry esteem with the early fusion strategy. Conclusion and perspectives In this article, we have presented a method based on bootstrapping for improving distributional thesauri. More precisely, we have proposed a new criterion, based on the relations of mono-terms in similar compounds, for the unsupervised selection of training examples used for reranking the semantic neighbors of a thesaurus. We have evaluated two different strategies for combining this criterion with an already existing one and showed that a late fusion app"
P15-2077,E14-1025,0,0.0246028,"reasonable error rate. We address the second issue by following (Hagiwara et al., 2009), which defined a Support Vector Machine (SVM) model for deciding whether two words are similar or not. In our context, a positive example is a pair of nouns that are Introduction The work presented in this article aims at improving thesauri built following the distributional approach as implemented by (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002). A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama"
P15-2077,ramisch-etal-2010-mwetoolkit,0,0.0253072,"Missing"
P15-2077,P12-1092,0,0.0595094,"ts of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by se470 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 470–476, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics percentage of the vocabulary of compounds): semantically similar while a n"
P15-2077,N10-1013,0,0.0209983,"Missing"
P15-2077,W10-3906,0,0.0152159,"t are Introduction The work presented in this article aims at improving thesauri built following the distributional approach as implemented by (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002). A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by se470 Proceedings of the 53rd Annual Meeting of the Association for Computatio"
P15-2077,J09-3004,0,0.0216458,"itive example is a pair of nouns that are Introduction The work presented in this article aims at improving thesauri built following the distributional approach as implemented by (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002). A part of the work for improving such thesauri focuses on the filtering of the components of the distributional contexts of words (Padr´o et al., 2014; Polajnar and Clark, 2014) or their reweighting, either by turning the weights of these components into ranks (Broda et al., 2009) or by adapting them through a bootstrapping method from the thesaurus to improve (Zhitomirsky-Geffet and Dagan, 2009; Yamamoto and Asakura, 2010). The other part implies more radical changes, including dimensionality reduction methods such as Latent Semantic Analysis (Pad´o and Lapata, 2007), multi-prototype (Reisinger and Mooney, 2010) or exemplar-based models (Erk and Pado, 2010), neural approaches (Huang et al., 2012; Mikolov et al., 2013) or the adoption of a Bayesian viewpoint (Kazama et al., 2010; Dinu and Lapata, 2010). Our work follows (Ferret, 2012), which proposed a different way from (Zhitomirsky-Geffet and Dagan, 2009) to exploit bootstrapping by se470 Proceedings of the 53rd Annual Meeting of t"
P15-2077,C98-2122,0,\N,Missing
P17-2035,S16-1195,0,0.0149202,"has been offering a shared task related to temporal relation extraction from clinical narratives over the past two years (Bethard et al., 2015, 2016). Relying on the THYME corpus, the task challenged participants to extract EVENT and TIMEX 3 entities and then to extract narrative container relations and document creation time relations. Herein, we focus on the second part of the challenge, temporal relation extraction and more specifically the narrative container relations. Different approaches have been implemented by the participants, including Support Vector Machine (SVM) classifiers (AAl Abdulsalam et al., 2016; Cohan et al., 2016; Lee et al., 2016; Tourille et al., 2016), Conditional Random Fields (CRF) and convolutional neural networks (CNNs) (Chikka, 2016). Beyond the challenges, Leeuwenberg and Moens (2017) propose a model based on a structured perceptron to jointly predict both types of temporal relations. Lin et al. (2016) performs training instance augmentation to increase the number of training examples and implement a SVM based model for containment relation extraction. Dligach et al. (2017) implement models based on CNNs and Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhub"
P17-2035,E17-2118,0,0.5086,"have been implemented by the participants, including Support Vector Machine (SVM) classifiers (AAl Abdulsalam et al., 2016; Cohan et al., 2016; Lee et al., 2016; Tourille et al., 2016), Conditional Random Fields (CRF) and convolutional neural networks (CNNs) (Chikka, 2016). Beyond the challenges, Leeuwenberg and Moens (2017) propose a model based on a structured perceptron to jointly predict both types of temporal relations. Lin et al. (2016) performs training instance augmentation to increase the number of training examples and implement a SVM based model for containment relation extraction. Dligach et al. (2017) implement models based on CNNs and Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to extract containment relations from the THYME corpus. From a more general perspective, relation extraction and classification is a task explored by many approaches, from fully unsupervised to fully supervised. Recent years have seen an increasing interest for the use of neural approaches. Introduction Temporal information extraction from clinical health records allows for a fine-grained analysis of patient health history. Providing medical staff with patient timelines could lead to"
P17-2035,P15-1061,0,0.0148876,"fy temporal relations between pairs of entities formalized as narrative container relations. 224 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365 1,565 Table 1: Descriptive statistics about the train and test parts of the THYME corp"
P17-2035,D13-1137,0,0.0126346,"entities and we focus on containment relation extraction where the objective is to identify temporal relations between pairs of entities formalized as narrative container relations. 224 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365"
P17-2035,S15-2136,0,0.16542,"Missing"
P17-2035,N16-1030,0,0.0615328,"Missing"
P17-2035,S16-1192,0,0.0751139,"n the THYME corpus, the task challenged participants to extract EVENT and TIMEX 3 entities and then to extract narrative container relations and document creation time relations. Herein, we focus on the second part of the challenge, temporal relation extraction and more specifically the narrative container relations. Different approaches have been implemented by the participants, including Support Vector Machine (SVM) classifiers (AAl Abdulsalam et al., 2016; Cohan et al., 2016; Lee et al., 2016; Tourille et al., 2016), Conditional Random Fields (CRF) and convolutional neural networks (CNNs) (Chikka, 2016). Beyond the challenges, Leeuwenberg and Moens (2017) propose a model based on a structured perceptron to jointly predict both types of temporal relations. Lin et al. (2016) performs training instance augmentation to increase the number of training examples and implement a SVM based model for containment relation extraction. Dligach et al. (2017) implement models based on CNNs and Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to extract containment relations from the THYME corpus. From a more general perspective, relation extraction and classification is a task exp"
P17-2035,S16-1201,0,0.0542222,"Missing"
P17-2035,S16-1194,0,0.0194281,"ed task related to temporal relation extraction from clinical narratives over the past two years (Bethard et al., 2015, 2016). Relying on the THYME corpus, the task challenged participants to extract EVENT and TIMEX 3 entities and then to extract narrative container relations and document creation time relations. Herein, we focus on the second part of the challenge, temporal relation extraction and more specifically the narrative container relations. Different approaches have been implemented by the participants, including Support Vector Machine (SVM) classifiers (AAl Abdulsalam et al., 2016; Cohan et al., 2016; Lee et al., 2016; Tourille et al., 2016), Conditional Random Fields (CRF) and convolutional neural networks (CNNs) (Chikka, 2016). Beyond the challenges, Leeuwenberg and Moens (2017) propose a model based on a structured perceptron to jointly predict both types of temporal relations. Lin et al. (2016) performs training instance augmentation to increase the number of training examples and implement a SVM based model for containment relation extraction. Dligach et al. (2017) implement models based on CNNs and Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to extract"
P17-2035,D15-1278,0,0.0130017,"n containment relation extraction where the objective is to identify temporal relations between pairs of entities formalized as narrative container relations. 224 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365 1,565 Table 1: De"
P17-2035,S16-1175,1,0.917526,"xtraction from clinical narratives over the past two years (Bethard et al., 2015, 2016). Relying on the THYME corpus, the task challenged participants to extract EVENT and TIMEX 3 entities and then to extract narrative container relations and document creation time relations. Herein, we focus on the second part of the challenge, temporal relation extraction and more specifically the narrative container relations. Different approaches have been implemented by the participants, including Support Vector Machine (SVM) classifiers (AAl Abdulsalam et al., 2016; Cohan et al., 2016; Lee et al., 2016; Tourille et al., 2016), Conditional Random Fields (CRF) and convolutional neural networks (CNNs) (Chikka, 2016). Beyond the challenges, Leeuwenberg and Moens (2017) propose a model based on a structured perceptron to jointly predict both types of temporal relations. Lin et al. (2016) performs training instance augmentation to increase the number of training examples and implement a SVM based model for containment relation extraction. Dligach et al. (2017) implement models based on CNNs and Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to extract containment relations from the THYME corp"
P17-2035,W16-2914,0,0.403651,"elations. Herein, we focus on the second part of the challenge, temporal relation extraction and more specifically the narrative container relations. Different approaches have been implemented by the participants, including Support Vector Machine (SVM) classifiers (AAl Abdulsalam et al., 2016; Cohan et al., 2016; Lee et al., 2016; Tourille et al., 2016), Conditional Random Fields (CRF) and convolutional neural networks (CNNs) (Chikka, 2016). Beyond the challenges, Leeuwenberg and Moens (2017) propose a model based on a structured perceptron to jointly predict both types of temporal relations. Lin et al. (2016) performs training instance augmentation to increase the number of training examples and implement a SVM based model for containment relation extraction. Dligach et al. (2017) implement models based on CNNs and Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997) to extract containment relations from the THYME corpus. From a more general perspective, relation extraction and classification is a task explored by many approaches, from fully unsupervised to fully supervised. Recent years have seen an increasing interest for the use of neural approaches. Introduction Temporal"
P17-2035,E17-2117,1,0.813839,"Missing"
P17-2035,D15-1206,0,0.0113271,"ort Papers), pages 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365 1,565 Table 1: Descriptive statistics about the train and test parts of the THYME corpus. Data 3.2 Corpus Presentation Preprocessing We preprocessed the corpus using cTAKES (Savova et al., 2010), an open-source natural language processing system for the extraction of in"
P17-2035,P16-1105,0,0.0225042,"ational Linguistics (Short Papers), pages 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365 1,565 Table 1: Descriptive statistics about the train and test parts of the THYME corpus. Data 3.2 Corpus Presentation Preprocessing We preprocessed the corpus using cTAKES (Savova et al., 2010), an open-source natural language processing system for the"
P17-2035,C14-1220,0,0.00961922,"between pairs of entities formalized as narrative container relations. 224 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365 1,565 Table 1: Descriptive statistics about the train and test parts of the THYME corpus. Data 3.2 Corpus"
P17-2035,W11-0419,0,0.048557,"poral expressions are assigned a Class attribute. Possible values for these attributes are presented in Table 2. 4 Narrative containers can be apprehended as temporal buckets in which several events may be included. These containers are anchored by temporal expressions, medical events or other concepts. Styler IV et al. (2014) argue that the use of narrative containers instead of classical temporal relations (Allen, 1983) yields better annotation while keeping most of the useful temporal information intact. The concept of narrative container is illustrated in Figure 1 and described further in Pustejovsky and Stubbs (2011). Task Description The container relation extraction task can be cast as a 3-class classification problem. For each combination of EVENT and/or TIMEX 3 from left to right, three cases are possible: • the first entity temporally contains the second entity, • the first entity is temporally contained by the second entity, • there is no temporal containment relation between the entities. Intra- and inter-sentence relation detection can be seen as two different tasks with specific features. Intra-sentence relations can benefit from intra-sentential clues such as adverbs (e.g. during) or pronouns (e"
P17-2035,P16-2034,0,0.00384149,"s 224–230 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-2035 CONTAINS Recursive neural networks (Socher et al., 2011, 2013) have proved useful for tasks involving longdistance relations, such as semantic relation extraction (Hashimoto et al., 2013; Li et al., 2015). Convolutional networks have also been used (dos Santos et al., 2015; Zeng et al., 2014) and more recently, recurrent networks such as LSTM showed to be more robust for learning long-distance semantic information (Miwa and Bansal, 2016; Xu et al., 2015; Zhou et al., 2016). 3 3.1 CONTAINS CONTAINS The last time the dose was increased was in February 2010 . - TIMEX - EVENT - EVENT - - TIMEX - Figure 1: Examples of intra-sentence narrative container relations. EVENT TIMEX 3 Intra-sentence relations Inter-sentence relations Train Test 49,147 5,791 12,855 4,582 15,503 1,917 4,365 1,565 Table 1: Descriptive statistics about the train and test parts of the THYME corpus. Data 3.2 Corpus Presentation Preprocessing We preprocessed the corpus using cTAKES (Savova et al., 2010), an open-source natural language processing system for the extraction of information from elect"
P17-2035,D11-1014,0,0.090597,"Missing"
P17-2035,D13-1170,0,0.00329242,"Missing"
P17-2035,E17-1108,0,\N,Missing
P18-2056,N16-1162,0,0.0130467,"r et al., 2017). The overall principle of this task is similar to the word similarity task of our first evaluation but at the level of sentences: the similarity of a set of sentence pairs is computed by the system to evaluate and compared with a correlation measure, the Pearson correlation coefficient, against a gold standard produced by human annotators. This framework is interesting for the evaluation of Pseudofit because the computation of the similarity of a pair of sentences can be achieved by unsupervised approaches based on word embeddings in a very competitive way, as demonstrated by (Hill et al., 2016). More precisely, the approach we adopt is a classical baseline that composes the embeddings of the plain words of each sentence to compare by elementwise addition and computes the Cosine measure between the two resulting vectors. For building the representation of a sentence, we compare the use of our initial embeddings with that of the embeddings produced by Pseudofit max+fus-max-pooling, the best variant of Pseudofit. For this experiment, pseudo-senses are distinguished not only for nouns but more generally for all nouns, verbs and adjectives with more than 21 occurrences in the corpus. Tab"
P18-2056,P14-1023,0,0.0943542,"esting variants in terms of representations but were built from half of the corpus only. Experiments Experimental Setup For implementing Pseudofit, we randomly select at the level of sentences a 1 billion word subpart of the Annotated English Gigaword corpus (Napoles et al., 2012). This corpus is made of news articles in English processed by the Stanford CoreNLP toolkit (Manning et al., 2014). We use this corpus under its lemmatized form. The building of the embeddings are performed with word2vecf, the adaptation of word2vec from (Levy and Goldberg, 2014), with the best parameter values from (Baroni et al., 2014): minimal count=5, vector size=300, window size=5, 10 negative examples and 10−5 for the subsampling probability of the most frequent words. For PARAGRAM, we adopt most of the parameter values from (Vuli´c et al., 2017): δ = 0.6 and λ = 10−9 , with the AdaGrad optimizer (Duchi et al., 2011) and 50 epochs1 . Retrofitting and Counter-fitting are used with the parameter values specified respectively in (Faruqui et al., 2015) and (Mrkši´c et al., 2016). 3.2 Evaluation of Pseudofit Our first evaluation of Pseudofit at word level is a classical intrinsic evaluation consisting in measuring for a set"
P18-2056,J15-4004,0,0.0265664,"aGrad optimizer (Duchi et al., 2011) and 50 epochs1 . Retrofitting and Counter-fitting are used with the parameter values specified respectively in (Faruqui et al., 2015) and (Mrkši´c et al., 2016). 3.2 Evaluation of Pseudofit Our first evaluation of Pseudofit at word level is a classical intrinsic evaluation consisting in measuring for a set of word pairs the Spearman’s rank correlation between human judgments and the similarity of these words computed from their embeddings by the Cosine measure. This evaluation is performed for the nouns of three large enough reference datasets: SimLex-999 (Hill et al., 2015), 2 The statistical significance of differences are judged according to a two-tailed Steiger’s test with p-value < 0.01 with the R package cocor (Diedenhofen and Musch, 2015). 3 The TOEFL test, which is close to our task, is considered sometimes as extrinsic and sometimes as intrinsic. 4 The significance of differences are judged according to a paired Wilcoxon test with the following notation: nothing if p <= 0.01, † if 0.01 < p ≤ 0.05 and ‡ if p > 0.05. 1 We used the implementation of PARAGRAM provided by https://github.com/nmrksic/attract-repel. 353 method INITIAL high INITIAL low Pseudofith"
P18-2056,D15-1242,0,0.040288,"Missing"
P18-2056,P14-2050,0,0.425682,"lov et al., 2013; Pennington et al., 2014) but also for specializing them according to a particular viewpoint. This viewpoint generally comes in the form of set of lexical relations. For instance, Kiela et al. (2015) specialize word embeddings towards semantic similarity or relatedness by relying either on synonyms or free lexical associations. Methods such as Retrofitting (Faruqui et al., 2015), Counterfitting (Mrkši´c et al., 2016) or PARAGRAM (Wieting et al., 2015) fall within the same framework. The specialization of word embeddings can also come from the way they are built. For instance, Levy and Goldberg (2014) bring word embeddings towards similarity rather than relatedness by using dependency-based distributional contexts rather than linear bag-of-word contexts. Finally, some methods aim at improving word embeddings 351 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 351–357 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics the pseudo-word. Pseudofit adopts the opposite viewpoint. For each word w, more precisely nouns in our case, it splits arbitrarily its occurrences into two sets: the occurrenc"
P18-2056,D17-1070,0,0.0132267,"ble 5 shows the result of this evaluation for the 1,379 sentence pairs of the test part of the STS Benchmark dataset. As for the two previous evaluations, the use of the embeddings modified by Pseudofit leads to a significant improvement of results6 compared to the initial embeddings, which demonstrates that the improvement at word level can be transposed at a larger scale. Table 5 also shows four reference results from (Cer et al., 2017): the lowest and the best baselines based on averaged word embeddings (Skip-gram and GloVe respectively), which are very close to our approach, and the best (Conneau et al., 2017) and the lowest (Duma and Menzel, 2017) unsupervised systems. Although our goal is not to compete with the best systems, it is interesting to note that our results are in line with the state of the art since they significantly outperform the two baselines and the lowest unsupervised system as well as other unsupervised systems mentioned in (Cer et al., 2017). 4 Conclusion and Perspectives In this article, we presented Pseudofit, a method that specializes word embeddings towards semantic similarity without external knowledge by exploiting the variability of distributional contexts. This method"
P18-2056,P14-5010,0,0.00458571,"to a systematic decrease of results, which emphasizes the importance of the repelling term of PARAGRAM. This term probably prevents the representation of a word from being changed too much by its pseudosenses, which are interesting variants in terms of representations but were built from half of the corpus only. Experiments Experimental Setup For implementing Pseudofit, we randomly select at the level of sentences a 1 billion word subpart of the Annotated English Gigaword corpus (Napoles et al., 2012). This corpus is made of news articles in English processed by the Stanford CoreNLP toolkit (Manning et al., 2014). We use this corpus under its lemmatized form. The building of the embeddings are performed with word2vecf, the adaptation of word2vec from (Levy and Goldberg, 2014), with the best parameter values from (Baroni et al., 2014): minimal count=5, vector size=300, window size=5, 10 negative examples and 10−5 for the subsampling probability of the most frequent words. For PARAGRAM, we adopt most of the parameter values from (Vuli´c et al., 2017): δ = 0.6 and λ = 10−9 , with the AdaGrad optimizer (Duchi et al., 2011) and 50 epochs1 . Retrofitting and Counter-fitting are used with the parameter value"
P18-2056,S17-2024,0,0.0526171,"Missing"
P18-2056,N16-1018,0,0.0419932,"Missing"
P18-2056,N15-1184,0,0.0734027,"Missing"
P18-2056,W12-3018,0,0.0225785,"Missing"
P18-2056,D14-1162,0,0.0785466,"ic similarity properties. The method we propose, Pseudofit, formalizes this approach through the notion of pseudo-sense. This notion is related to the notion of pseudo-word introduced in the field of word sense disambiguation by Gale et al. (1992) and Schütze (1992). A pseudo-word is an artificial word resulting from the clustering of two or more different words, each of them being considered as one pseudo-sense of The interest aroused by word embeddings in Natural Language Processing, especially for neural models, has led to propose methods for creating them from texts (Mikolov et al., 2013; Pennington et al., 2014) but also for specializing them according to a particular viewpoint. This viewpoint generally comes in the form of set of lexical relations. For instance, Kiela et al. (2015) specialize word embeddings towards semantic similarity or relatedness by relying either on synonyms or free lexical associations. Methods such as Retrofitting (Faruqui et al., 2015), Counterfitting (Mrkši´c et al., 2016) or PARAGRAM (Wieting et al., 2015) fall within the same framework. The specialization of word embeddings can also come from the way they are built. For instance, Levy and Goldberg (2014) bring word embedd"
P18-2056,P17-1006,0,0.0282777,"Missing"
P18-2056,P16-1128,0,0.0410587,"Missing"
P19-2041,Q17-1010,0,0.0482427,"ms better than the contextual embeddings alone and improves upon static embeddings trained on a large in-domain corpus; Introduction • we define two ways of combining contextual and static embeddings and conclude that the naive concatenation of vectors is consistently outperformed by the addition of the static representation directly into the internal linear combination of ELMo; Today, the NLP community can enjoy an evergrowing list of embedding techniques that include factorization methods (e.g. GloVe (Pennington et al., 2014)), neural methods (e.g. word2vec (Mikolov et al., 2013), fastText (Bojanowski et al., 2017)) and more recently dynamic methods that take into account the context (e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2018)). The success of these methods can be arguably attributed to the availability of large generaldomain corpora like Wikipedia, Gigaword (Graff et al., 2003) or the BooksCorpus (Zhu et al., 2015). Unfortunately, similar corpora are often unavailable for specialized domains, leaving the NLP practitioner with only two choices: either using • finally, we show that ELMo models can be successfully fine-tuned on a small in-domain corpus, bringing significant improvements t"
P19-2041,W16-4202,0,0.0151782,"tatic forms of combination explored in (Yin and Sch¨utze, 2016; Murom¨agi et al., 2017; Bollegala et al., 2018) and more dynamic modes of combination that can be found in (Peters et al., 2018) and (Kiela et al., 2018). In this work, we show in particular how a combination of general-domain contextual embeddings, fine-tuning, and in-domain static embeddings trained on a small corpus can be employed to reach a similar performance using resources that are available for any domain. 3.1 Data 3 To solve this task, we choose a bi-LSTM-CRF as is usual in entity recognition tasks (Lample et al., 2016; Chalapathy et al., 2016; Habibi et al., 2017). Our particular architecture uses 3 bi-LSTM layers with 256 units, a dropout rate of 0.5 and is implemented using the AllenNLP framework (Gardner et al., 2018). During training, the exact span F1 score is monitored on 5,000 randomly sampled sequences for early-stopping. The data consists of discharge summaries and progress reports from three different institutions: Partners Healthcare, Beth Israel Deaconess Medical Center, and the University of Pittsburgh Medical Center. These documents are labeled and split into 394 training files and 477 test files for a total of 30,94"
P19-2041,D14-1162,0,0.0846276,"extual embeddings (ELMo) from the general domain. We also show that this combination performs better than the contextual embeddings alone and improves upon static embeddings trained on a large in-domain corpus; Introduction • we define two ways of combining contextual and static embeddings and conclude that the naive concatenation of vectors is consistently outperformed by the addition of the static representation directly into the internal linear combination of ELMo; Today, the NLP community can enjoy an evergrowing list of embedding techniques that include factorization methods (e.g. GloVe (Pennington et al., 2014)), neural methods (e.g. word2vec (Mikolov et al., 2013), fastText (Bojanowski et al., 2017)) and more recently dynamic methods that take into account the context (e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2018)). The success of these methods can be arguably attributed to the availability of large generaldomain corpora like Wikipedia, Gigaword (Graff et al., 2003) or the BooksCorpus (Zhu et al., 2015). Unfortunately, similar corpora are often unavailable for specialized domains, leaving the NLP practitioner with only two choices: either using • finally, we show that ELMo models can"
P19-2041,N18-1202,0,0.331675,"main corpus; Introduction • we define two ways of combining contextual and static embeddings and conclude that the naive concatenation of vectors is consistently outperformed by the addition of the static representation directly into the internal linear combination of ELMo; Today, the NLP community can enjoy an evergrowing list of embedding techniques that include factorization methods (e.g. GloVe (Pennington et al., 2014)), neural methods (e.g. word2vec (Mikolov et al., 2013), fastText (Bojanowski et al., 2017)) and more recently dynamic methods that take into account the context (e.g. ELMo (Peters et al., 2018), BERT (Devlin et al., 2018)). The success of these methods can be arguably attributed to the availability of large generaldomain corpora like Wikipedia, Gigaword (Graff et al., 2003) or the BooksCorpus (Zhu et al., 2015). Unfortunately, similar corpora are often unavailable for specialized domains, leaving the NLP practitioner with only two choices: either using • finally, we show that ELMo models can be successfully fine-tuned on a small in-domain corpus, bringing significant improvements to strategies involving contextual embeddings. 2 Related Work Former work by Roberts (2016) analyzed the"
P19-2041,W18-2501,0,0.0232897,"., 2018) and (Kiela et al., 2018). In this work, we show in particular how a combination of general-domain contextual embeddings, fine-tuning, and in-domain static embeddings trained on a small corpus can be employed to reach a similar performance using resources that are available for any domain. 3.1 Data 3 To solve this task, we choose a bi-LSTM-CRF as is usual in entity recognition tasks (Lample et al., 2016; Chalapathy et al., 2016; Habibi et al., 2017). Our particular architecture uses 3 bi-LSTM layers with 256 units, a dropout rate of 0.5 and is implemented using the AllenNLP framework (Gardner et al., 2018). During training, the exact span F1 score is monitored on 5,000 randomly sampled sequences for early-stopping. The data consists of discharge summaries and progress reports from three different institutions: Partners Healthcare, Beth Israel Deaconess Medical Center, and the University of Pittsburgh Medical Center. These documents are labeled and split into 394 training files and 477 test files for a total of 30,946 + 45,404 ⇡ 76,000 sequences 4 . 3.2 Task and Model The goal of the Clinical Concept Detection task is to extract three types of medical entities: problems (e.g. the name of a disea"
P19-2041,W16-4208,0,0.0175127,"ELMo (Peters et al., 2018), BERT (Devlin et al., 2018)). The success of these methods can be arguably attributed to the availability of large generaldomain corpora like Wikipedia, Gigaword (Graff et al., 2003) or the BooksCorpus (Zhu et al., 2015). Unfortunately, similar corpora are often unavailable for specialized domains, leaving the NLP practitioner with only two choices: either using • finally, we show that ELMo models can be successfully fine-tuned on a small in-domain corpus, bringing significant improvements to strategies involving contextual embeddings. 2 Related Work Former work by Roberts (2016) analyzed the trade-off between corpus size and similarity when training word embeddings for a clinical entity recognition task. The author’s conclusion was that while embeddings trained with word2vec on indomain texts performed generally better, a combination of both in-domain and general domain em1 Python code for reproducing our experiments is available at: https://github.com/helboukkouri/ acl_srw_2019 295 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 295–301 c Florence, Italy, July 28 - August 2, 2019. 2019 Associa"
P19-2041,P16-1128,0,0.0482051,"Missing"
P19-2041,D18-1176,0,0.0206433,"representations pre-trained on MIMIC-III, proving once more the value of large in-domain corpora (Si et al., 2019).2 While interesting for the clinical domain, these strategies may not always be applicable to other specialized fields since large in-domain corpora like MIMIC-III will rarely be available. To deal with this issue, we explore embedding combinations3 . In this respect, we consider both static forms of combination explored in (Yin and Sch¨utze, 2016; Murom¨agi et al., 2017; Bollegala et al., 2018) and more dynamic modes of combination that can be found in (Peters et al., 2018) and (Kiela et al., 2018). In this work, we show in particular how a combination of general-domain contextual embeddings, fine-tuning, and in-domain static embeddings trained on a small corpus can be employed to reach a similar performance using resources that are available for any domain. 3.1 Data 3 To solve this task, we choose a bi-LSTM-CRF as is usual in entity recognition tasks (Lample et al., 2016; Chalapathy et al., 2016; Habibi et al., 2017). Our particular architecture uses 3 bi-LSTM layers with 256 units, a dropout rate of 0.5 and is implemented using the AllenNLP framework (Gardner et al., 2018). During tra"
P19-2041,N16-1030,0,0.0412958,"t, we consider both static forms of combination explored in (Yin and Sch¨utze, 2016; Murom¨agi et al., 2017; Bollegala et al., 2018) and more dynamic modes of combination that can be found in (Peters et al., 2018) and (Kiela et al., 2018). In this work, we show in particular how a combination of general-domain contextual embeddings, fine-tuning, and in-domain static embeddings trained on a small corpus can be employed to reach a similar performance using resources that are available for any domain. 3.1 Data 3 To solve this task, we choose a bi-LSTM-CRF as is usual in entity recognition tasks (Lample et al., 2016; Chalapathy et al., 2016; Habibi et al., 2017). Our particular architecture uses 3 bi-LSTM layers with 256 units, a dropout rate of 0.5 and is implemented using the AllenNLP framework (Gardner et al., 2018). During training, the exact span F1 score is monitored on 5,000 randomly sampled sequences for early-stopping. The data consists of discharge summaries and progress reports from three different institutions: Partners Healthcare, Beth Israel Deaconess Medical Center, and the University of Pittsburgh Medical Center. These documents are labeled and split into 394 training files and 477 test f"
P19-2041,W17-0212,0,0.0189816,"omain corpus. Similar corpora will often be available in other specialized domains as it is always possible to build a corpus from the training documents. Concatenation a simple concatenation of vectors coming from two different embeddings. This is denoted X Y (e.g. i2b2 Wikipedia). Mixture in the particular case where ELMo embeddings were combined with word2vec vectors, we can directly add the word2vec embedding in the linear combination of ELMo. We denote this combination strategy X+ +Y (e.g. ELMo small+ +i2b2). Then, we also train embeddings on each of two general-domain corpora: Wikipedia (2017) encyclopedia articles from the 01/10/2017 data dump6 . This is a large (2 billion tokens) corpus from the general domain that has limited coverage of the medical field. The mixture method generalizes the way ELMo representations are combined. Given a word w, if we denote the three internal representations produced by ELMo (i.e. the CharCNN, 1st bi-LSTM and 2nd bi-LSTM representations) by h1 , h2 , h3 , we recall that the model computes the word’s embedding as: Gigaword (2003) newswire text data from many sources including the New York Times. This is a large (2 billion tokens) corpus from the"
P98-1065,J97-1003,0,0.141878,"tion according to a topical criterion is a useful process in many applications, such as text summarization or information extraction task. Approaches that address this problem can be classified in knowledge-based approaches or word-based approaches. Knowledge-based systems as Grosz and Sidner's (1986) require an extensive manual knowledge engineering effort to create the knowledge base (semantic network and/or frames) and this is only possible in very limited and well-known domains. To overcome this limitation, and to process a large amount of texts, word-based approaches have been developed. Hearst (1997) and Masson (1995) make use of the word distribution in a text to find a thematic segmentation. These works are well adapted to technical or scientific texts characterized by a specific vocabulary. To process narrative or expository texts such as newspaper articles, Kozima's (1993) and Morris and Hirst's (1991) approaches are based on lexical cohesion computed from a lexical network. These methods depend on the presence of the text vocabulary inside their network. So, to avoid any restriction about domains in such 392 collocations have been collected is 20 words wide and takes into account the"
P98-1065,P93-1041,0,0.0448345,"Missing"
P98-1065,C94-2187,0,0.0295054,"Missing"
P98-1065,C94-1095,0,0.0259555,"y have to be represented by their significant features from that point of view. So, we only hold for each text the lemmatized form of its nouns, verbs and adjectives. This has been done by combining existing tools. MtSeg from the Multext project presented in V6ronis and Khouri (1995) is used for s e g m e n t i n g the raw texts. As compound nouns are less polysemous than single ones, we have added to MtSeg the ability to identify 2300 compound nouns. We have retained the most frequent compound nouns in 11 years of the French Le Monde newspaper. They have been collected with the INTEX tool of Silberztein (1994). The part of speech tagger TreeTagger of Schmid (1994) is applied to disambiguate the lexical category of the words and to provide their lemmatized form. The selection of the meaningful words, which do not include proper nouns and abbreviations, ends the pre-processing. This one is applied to the texts both for building the collocation network and for their thematic segmentation. 4. /max = log2 N2(Sw - 1) with N: corpus size and Sw: window size 5. Thematic segmentation lexical network without The first method, based on a numerical analysis of the vocabulary distribution in the text, is derive"
P98-1065,J90-1003,0,\N,Missing
P98-1065,J91-1002,0,\N,Missing
P98-1065,J86-3001,0,\N,Missing
P98-2243,J90-1003,0,0.0912317,"ing it for a specific domain or for another language is quick. 2 Method The segmentation algorithm we propose includes two steps. First, a computation of the cohesion of the different parts of a text is done by using a collocation network. Second, we locate the major breaks in this cohesion to detect the thematic shifts and build segments. 2.1 The collocation network Our collocation network has been built from 24 months of the French Le Monde newspaper. The size of this corpus is around 39 million words. The cohesion between words has been evaluated with the mutual information measure, as in (Church and Hanks, 1990). A large window, 20 words wide, was used to take into account the thematic links. The texts were pre-processed with the probabilistic POS tagger TreeTagger (Schmid, 1994) in order to keep only the lemmatized form of their content words, i.e. nouns, adjectives and verbs. The resulting network is composed of approximatively 31 thousand words and 14 million relations. 2.2 Computation of text cohesion As in Kozima's work, a cohesion value is computed at each position of a window in a text (after pre-processing) from the words in this window. The collocation network is used for determining how clo"
P98-2243,J97-1003,0,0.723522,"tract This article outlines a quantitative method for segmenting texts into thematically coherent units. This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words. We also present the results of an experiment about locating boundaries between a series of concatened texts. 1 Introduction Several quantitative methods exist for thematically segmenting texts. Most of them are based on the following assumption: the thematic coherence of a text segment finds expression at the lexical level. Hearst (1997) and Nomoto and Nitta (1994) detect this coherence through patterns of lexical cooccurrence. Morris and Hirst (1991) and Kozima (1993) find topic boundaries in the texts by using lexical cohesion. The first methods are applied to texts, such as expository texts, whose vocabulary is often very specific. As a concept is always expressed by the same word, word repetitions are thematically significant in these texts. The use of lexical cohesion allows to bypass the problem set by texts, such as narratives, in which a concept is often expressed by different means. However, this second approach requ"
P98-2243,P93-1041,0,0.0646153,"rk of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words. We also present the results of an experiment about locating boundaries between a series of concatened texts. 1 Introduction Several quantitative methods exist for thematically segmenting texts. Most of them are based on the following assumption: the thematic coherence of a text segment finds expression at the lexical level. Hearst (1997) and Nomoto and Nitta (1994) detect this coherence through patterns of lexical cooccurrence. Morris and Hirst (1991) and Kozima (1993) find topic boundaries in the texts by using lexical cohesion. The first methods are applied to texts, such as expository texts, whose vocabulary is often very specific. As a concept is always expressed by the same word, word repetitions are thematically significant in these texts. The use of lexical cohesion allows to bypass the problem set by texts, such as narratives, in which a concept is often expressed by different means. However, this second approach requires knowledge about the cohesion between words. Morris and Hirst (1991) extract this knowledge from a thesaurus. Kozima (1993) exploi"
P98-2243,J91-1002,0,0.151211,"his method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words. We also present the results of an experiment about locating boundaries between a series of concatened texts. 1 Introduction Several quantitative methods exist for thematically segmenting texts. Most of them are based on the following assumption: the thematic coherence of a text segment finds expression at the lexical level. Hearst (1997) and Nomoto and Nitta (1994) detect this coherence through patterns of lexical cooccurrence. Morris and Hirst (1991) and Kozima (1993) find topic boundaries in the texts by using lexical cohesion. The first methods are applied to texts, such as expository texts, whose vocabulary is often very specific. As a concept is always expressed by the same word, word repetitions are thematically significant in these texts. The use of lexical cohesion allows to bypass the problem set by texts, such as narratives, in which a concept is often expressed by different means. However, this second approach requires knowledge about the cohesion between words. Morris and Hirst (1991) extract this knowledge from a thesaurus. Ko"
P98-2243,C94-2187,0,0.0188228,"outlines a quantitative method for segmenting texts into thematically coherent units. This method relies on a network of lexical collocations to compute the thematic coherence of the different parts of a text from the lexical cohesiveness of their words. We also present the results of an experiment about locating boundaries between a series of concatened texts. 1 Introduction Several quantitative methods exist for thematically segmenting texts. Most of them are based on the following assumption: the thematic coherence of a text segment finds expression at the lexical level. Hearst (1997) and Nomoto and Nitta (1994) detect this coherence through patterns of lexical cooccurrence. Morris and Hirst (1991) and Kozima (1993) find topic boundaries in the texts by using lexical cohesion. The first methods are applied to texts, such as expository texts, whose vocabulary is often very specific. As a concept is always expressed by the same word, word repetitions are thematically significant in these texts. The use of lexical cohesion allows to bypass the problem set by texts, such as narratives, in which a concept is often expressed by different means. However, this second approach requires knowledge about the coh"
R09-1017,A00-2004,0,0.159267,"of the two approaches and their combination is performed in a reference framework and shows the interest of this combination both for French and English. 1 2 Introduction In this article, we address the problem of linear topic segmentation, which consists in segmenting documents into topically homogeneous non-overlapping segments. This Discourse Analysis problem has received a constant interest since works such as [11]. One criterion for classifying topic segmentation systems is the kind of knowledge they depend on. Most of them only rely on surface features of documents: word reiteration in [11, 4, 20, 10], and more recently [14, 7], or discourse cues in [16, 10]. As they don’t exploit external knowledge, such systems are not domain-dependent but they can be successfully applied only to some types of documents: word reiteration is reliable only if concepts are not expressed by too different means (synonyms, etc.); discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in [13]; a thesaurus in [15]; a large set of lexical cooccurrences collected from"
R09-1017,W01-0514,0,0.263498,"ecently [14, 7], or discourse cues in [16, 10]. As they don’t exploit external knowledge, such systems are not domain-dependent but they can be successfully applied only to some types of documents: word reiteration is reliable only if concepts are not expressed by too different means (synonyms, etc.); discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in [13]; a thesaurus in [15]; a large set of lexical cooccurrences collected from a corpus in [5] or [6]. To some extent, these lexical networks enable segmenters to rely on a sort of concept reiteration. However, their lack of any topical structure makes this kind of knowledge difficult to use when lexical ambiguity is high. The most simple solution to this problem is to exploit knowledge about the topics that may occur in documents. Such topic models are generally built from a large set of example documents as in [21], [2] or in Overview In most of the algorithms in the text segmentation field, documents are represented as sequences of basic discourse units. When they are written texts,"
R09-1017,D08-1035,0,0.0136191,"mbination is performed in a reference framework and shows the interest of this combination both for French and English. 1 2 Introduction In this article, we address the problem of linear topic segmentation, which consists in segmenting documents into topically homogeneous non-overlapping segments. This Discourse Analysis problem has received a constant interest since works such as [11]. One criterion for classifying topic segmentation systems is the kind of knowledge they depend on. Most of them only rely on surface features of documents: word reiteration in [11, 4, 20, 10], and more recently [14, 7], or discourse cues in [16, 10]. As they don’t exploit external knowledge, such systems are not domain-dependent but they can be successfully applied only to some types of documents: word reiteration is reliable only if concepts are not expressed by too different means (synonyms, etc.); discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in [13]; a thesaurus in [15]; a large set of lexical cooccurrences collected from a corpus in [5] or [6]. To"
R09-1017,C04-1194,1,0.774585,"it is interesting to test if more elaborated resources can lead to better results. In this section, we consider word senses that were discriminated from a corpus, a resource that can be seen halfway between co-occurrences and semantic knowledge. mouse-device mouse-animal computer#n, disk#n, pc#n, software#n, user#n, machine#n, screen#n, compatible#a ... hormone#n, tumour#n, immune#a, researcher#n, animal#n, disease#n, gene#n ... Table 1: Two senses of the word “mouse” 5.2.1 Word senses discriminated from texts The word senses we use in this work were built according to the method described in [8]. More precisely, the building process starts from a network of lexical co-occurrences as the ones described in Section 5.1.1. First, the subgraph of the co-occurrents of the target word is delimited and turned into a similarity graph where the similarity between two co-occurrents is equal to their cohesion in the network. Then, a clustering algorithm is applied for detecting high-density areas in this graph. Finally, a word sense is defined from each resulting cluster. An example of such word senses is given by Table 1 with the two senses found for the noun mouse. A set of word senses were bu"
R09-1017,P07-1061,1,0.126169,"on: [12] combined word recurrence, co-occurrences and a thesaurus; [1] relied on both lexical modeling and discourse cues; [10] made use of word reiteration through lexical chains and discourse cues. The work we report in this article takes place in the last category we have presented. More precisely, it first confirms the interest of combining lexical recurrence with an external resource about lexical cohesion of texts. Second, it shows that the improvement brought by the use of a resource about lexical cohesion and the improvement brought by an endogenous method such as the one presented in [9] are complementary and can be fruitfully combined. Topic segmentation was addressed by a large amount of work from which it is not easy to draw conclusions, especially about the need for knowledge. In this article, we propose to combine in the same framework two methods for improving the results of a topic segmenter based on lexical reiteration. The first one is endogenous and exploits the distributional similarity of words in a document for discovering its topics. These topics are then used to facilitate the detection of topical similarity between discourse units. The second approach achieves"
R09-1017,P03-1071,0,0.417223,"of the two approaches and their combination is performed in a reference framework and shows the interest of this combination both for French and English. 1 2 Introduction In this article, we address the problem of linear topic segmentation, which consists in segmenting documents into topically homogeneous non-overlapping segments. This Discourse Analysis problem has received a constant interest since works such as [11]. One criterion for classifying topic segmentation systems is the kind of knowledge they depend on. Most of them only rely on surface features of documents: word reiteration in [11, 4, 20, 10], and more recently [14, 7], or discourse cues in [16, 10]. As they don’t exploit external knowledge, such systems are not domain-dependent but they can be successfully applied only to some types of documents: word reiteration is reliable only if concepts are not expressed by too different means (synonyms, etc.); discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in [13]; a thesaurus in [15]; a large set of lexical cooccurrences collected from"
R09-1017,P94-1002,0,0.605846,"ying on external resources. Two resources are tested: a network of lexical co-occurrences built from a large corpus and a set of word senses induced from this network. An evaluation of the two approaches and their combination is performed in a reference framework and shows the interest of this combination both for French and English. 1 2 Introduction In this article, we address the problem of linear topic segmentation, which consists in segmenting documents into topically homogeneous non-overlapping segments. This Discourse Analysis problem has received a constant interest since works such as [11]. One criterion for classifying topic segmentation systems is the kind of knowledge they depend on. Most of them only rely on surface features of documents: word reiteration in [11, 4, 20, 10], and more recently [14, 7], or discourse cues in [16, 10]. As they don’t exploit external knowledge, such systems are not domain-dependent but they can be successfully applied only to some types of documents: word reiteration is reliable only if concepts are not expressed by too different means (synonyms, etc.); discourse cues are often rare and corpus-specific. To overcome these difficulties, some syste"
R09-1017,P98-1100,0,0.374778,"or Roget’s Thesaurus. Although their coverage is large, these resources can’t obviously cover all domains in depth. As a consequence, it can also be interesting to rely on resources whose relations are less well defined but that can be easily extended to a new domain in an unsupervised way 88 International Conference RANLP 2009 - Borovets, Bulgaria, pages 88–93 from a representative corpus. The cohesion relations that are captured through the lexical co-occurrences extracted from a set of texts fulfill these constraints and were already exploited with success together with word reiteration in [12]. In [9], another way to improve the evaluation of the similarity between two sentences of a text is proposed: the idea is to define each topic of the text as a subset of its vocabulary and to use the implicit relation between each couple of words that are part of the same topic for detecting the topical similarity of sentences. A large repository of topics doesn’t exist and similarly to semantic resources, it couldn’t cover all domains. As a consequence, [9] performs the discovering of the topics of a text in an unsupervised way and its method can be qualified as endogenous as it doesn’t rely"
R09-1017,P93-1041,0,0.470356,"only rely on surface features of documents: word reiteration in [11, 4, 20, 10], and more recently [14, 7], or discourse cues in [16, 10]. As they don’t exploit external knowledge, such systems are not domain-dependent but they can be successfully applied only to some types of documents: word reiteration is reliable only if concepts are not expressed by too different means (synonyms, etc.); discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in [13]; a thesaurus in [15]; a large set of lexical cooccurrences collected from a corpus in [5] or [6]. To some extent, these lexical networks enable segmenters to rely on a sort of concept reiteration. However, their lack of any topical structure makes this kind of knowledge difficult to use when lexical ambiguity is high. The most simple solution to this problem is to exploit knowledge about the topics that may occur in documents. Such topic models are generally built from a large set of example documents as in [21], [2] or in Overview In most of the algorithms in the text segmentation field, doc"
R09-1017,P06-1004,0,0.0280955,"mbination is performed in a reference framework and shows the interest of this combination both for French and English. 1 2 Introduction In this article, we address the problem of linear topic segmentation, which consists in segmenting documents into topically homogeneous non-overlapping segments. This Discourse Analysis problem has received a constant interest since works such as [11]. One criterion for classifying topic segmentation systems is the kind of knowledge they depend on. Most of them only rely on surface features of documents: word reiteration in [11, 4, 20, 10], and more recently [14, 7], or discourse cues in [16, 10]. As they don’t exploit external knowledge, such systems are not domain-dependent but they can be successfully applied only to some types of documents: word reiteration is reliable only if concepts are not expressed by too different means (synonyms, etc.); discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in [13]; a thesaurus in [15]; a large set of lexical cooccurrences collected from a corpus in [5] or [6]. To"
R09-1017,J91-1002,0,0.783734,"features of documents: word reiteration in [11, 4, 20, 10], and more recently [14, 7], or discourse cues in [16, 10]. As they don’t exploit external knowledge, such systems are not domain-dependent but they can be successfully applied only to some types of documents: word reiteration is reliable only if concepts are not expressed by too different means (synonyms, etc.); discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in [13]; a thesaurus in [15]; a large set of lexical cooccurrences collected from a corpus in [5] or [6]. To some extent, these lexical networks enable segmenters to rely on a sort of concept reiteration. However, their lack of any topical structure makes this kind of knowledge difficult to use when lexical ambiguity is high. The most simple solution to this problem is to exploit knowledge about the topics that may occur in documents. Such topic models are generally built from a large set of example documents as in [21], [2] or in Overview In most of the algorithms in the text segmentation field, documents are represente"
R09-1017,J97-1005,0,0.0436909,"ference framework and shows the interest of this combination both for French and English. 1 2 Introduction In this article, we address the problem of linear topic segmentation, which consists in segmenting documents into topically homogeneous non-overlapping segments. This Discourse Analysis problem has received a constant interest since works such as [11]. One criterion for classifying topic segmentation systems is the kind of knowledge they depend on. Most of them only rely on surface features of documents: word reiteration in [11, 4, 20, 10], and more recently [14, 7], or discourse cues in [16, 10]. As they don’t exploit external knowledge, such systems are not domain-dependent but they can be successfully applied only to some types of documents: word reiteration is reliable only if concepts are not expressed by too different means (synonyms, etc.); discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in [13]; a thesaurus in [15]; a large set of lexical cooccurrences collected from a corpus in [5] or [6]. To some extent, these lexical netw"
R09-1017,J02-1002,0,0.0186554,"f lemmas. The sense with the highest similarity is kept. 6 6.1 Evaluation Evaluation methodology Choi proposed in [4] an evaluation methodology for topic segmentation systems that has became a kind of standard. This methodology is based on the building of artificial texts made of segments extracted from different documents. Because it is not well adapted 91 Using external resources First, we evaluated the interest of using external resources in F06 by applying F06C and F06WS to the two evaluation corpora. We classically used the error metric Pk proposed in [1] and its variant WindowDiff (WD ) [17] to measure segmentation accuracy. Pk and WD are given as percentages in the next tables (smallest values are best results). Systems U00 C99 TextTiling* LCseg F06R F06C F06WS Pk 25.91 27.57 21.08 20.55 21.58 16.48 18.17 WD 27.42 35.42 27.43 28.31 27.83 20.94 23.14 Table 2: Evaluation of F06 with external resources for the French corpus Tables 2 and 3 show both the results of our evaluations for F06C and F06WS and the results reported in [9] for F06R (F06 with only word recurrence) and several reference topic segmenters: U00 [20], C99 [4] and LCseg [10]; TextTiling* is a variant of TextTiling i"
R09-1017,P01-1064,0,0.204616,"of the two approaches and their combination is performed in a reference framework and shows the interest of this combination both for French and English. 1 2 Introduction In this article, we address the problem of linear topic segmentation, which consists in segmenting documents into topically homogeneous non-overlapping segments. This Discourse Analysis problem has received a constant interest since works such as [11]. One criterion for classifying topic segmentation systems is the kind of knowledge they depend on. Most of them only rely on surface features of documents: word reiteration in [11, 4, 20, 10], and more recently [14, 7], or discourse cues in [16, 10]. As they don’t exploit external knowledge, such systems are not domain-dependent but they can be successfully applied only to some types of documents: word reiteration is reliable only if concepts are not expressed by too different means (synonyms, etc.); discourse cues are often rare and corpus-specific. To overcome these difficulties, some systems make use of domain-independent knowledge about lexical cohesion: a lexical network built from a dictionary in [13]; a thesaurus in [15]; a large set of lexical cooccurrences collected from"
R09-1017,C98-1097,0,\N,Missing
S16-1175,S16-1165,0,0.0431462,"relations. The latter consists of two subtasks. In the Document Creation Time Relation subtask (DR), participants are challenged to identify relations between the events and the document creation time. For the Container Relation subtask (CR), participants have to identity container relations between entities. Participants may submit either a complete system extracting entities and relations or focus on either the entity extraction or relation extraction (using the gold standard entities provided by the organizers). More details about the task and the definition of each subtask can be found in Bethard et al. (2016). In this paper, we present our submission for the CR and DR subtasks based on gold-standard entities (phase 2). Our global approach, which is illustrated in Figure 1, tackles the identification of temporal relations as a set of supervised classification tasks. We submitted two runs, one using plain lexical features and one using word embeddings computed on a large clinical corpus. We obtained scores well above the median scores in both subtasks. The remainder of this paper is organized as follows. Section 2 presents our system for the DR subtask while Section 3 describes our system for the CR"
S16-1175,P05-1022,0,0.0561315,"tistical feature selection. We used the Scikit-learn machine learning library (Pedregosa et al., 2011) for both implementing our classification models and performing statistical feature selection. 4.3 Corpus Preprocessing We applied a four-step preprocessing on the 440 texts that were provided for the subtasks. First, we used NLTK (Loper and Bird, 2002) to segment the texts into sentences with the Punkt Sentence Tokenizer pre-trained model for English provided within the framework. The second step consisted of parsing the resulting sentences. For this task, we used the BLLIP Reranking Parser (Charniak and Johnson, 2005) and a pre-trained biomedical parsing model (McClosky, 2010). In the third step, we lemmatized the corpus using BioLemmatizer (Liu et al., 2012), a tool built for processing the biomedical literature. We used the Part-Of-Speech tags from the previous step as parameters for the lemmatization. The last step consisted in using Metamap (Aronson and Lang, 2010) to detect biomedical events and linking them, after disambiguation, to their related UMLS® (Unified Medical Language System) concept. We chose to keep biomedical entities that had a 1140 span overlapping with at least one entity of the gold"
S16-1175,W02-0109,0,0.0472586,"mbeddings b Table 2: Machine learning algorithms and parameters used for the final submission The machine learning algorithms used for the final submission are presented in Table 2 together with their parameters and the percentage of the feature space kept after statistical feature selection. We used the Scikit-learn machine learning library (Pedregosa et al., 2011) for both implementing our classification models and performing statistical feature selection. 4.3 Corpus Preprocessing We applied a four-step preprocessing on the 440 texts that were provided for the subtasks. First, we used NLTK (Loper and Bird, 2002) to segment the texts into sentences with the Punkt Sentence Tokenizer pre-trained model for English provided within the framework. The second step consisted of parsing the resulting sentences. For this task, we used the BLLIP Reranking Parser (Charniak and Johnson, 2005) and a pre-trained biomedical parsing model (McClosky, 2010). In the third step, we lemmatized the corpus using BioLemmatizer (Liu et al., 2012), a tool built for processing the biomedical literature. We used the Part-Of-Speech tags from the previous step as parameters for the lemmatization. The last step consisted in using Me"
S16-1175,N10-1004,0,0.0122909,"brary (Pedregosa et al., 2011) for both implementing our classification models and performing statistical feature selection. 4.3 Corpus Preprocessing We applied a four-step preprocessing on the 440 texts that were provided for the subtasks. First, we used NLTK (Loper and Bird, 2002) to segment the texts into sentences with the Punkt Sentence Tokenizer pre-trained model for English provided within the framework. The second step consisted of parsing the resulting sentences. For this task, we used the BLLIP Reranking Parser (Charniak and Johnson, 2005) and a pre-trained biomedical parsing model (McClosky, 2010). In the third step, we lemmatized the corpus using BioLemmatizer (Liu et al., 2012), a tool built for processing the biomedical literature. We used the Part-Of-Speech tags from the previous step as parameters for the lemmatization. The last step consisted in using Metamap (Aronson and Lang, 2010) to detect biomedical events and linking them, after disambiguation, to their related UMLS® (Unified Medical Language System) concept. We chose to keep biomedical entities that had a 1140 span overlapping with at least one entity of the gold standard. 5 Results and Discussion In Table 3, we present th"
S16-1175,P06-4018,0,\N,Missing
S16-1175,Q14-1012,0,\N,Missing
S17-2098,S16-1195,0,0.0310096,"to extract containment (CONTAINS) relations between EVENT and/or TIMEX3 as well as Document Creation Time (DCT) relations between EVENT entities and documents in which they are embedded. The novelty of the 2017 edition lies in the difference of domains between train and test corpora. More details about the task and the definition of each subtask can be found in Bethard et al. (2017). The task has been offered by SemEval over the past two years. Concerning the first group of subtasks, different approaches have been implemented by the participants including Conditional Random Fields (CRF) (AAl Abdulsalam et al., 2016; Caselli and Morante, 2016; Chikka, 2016; Cohan et al., 2016; Grouin and Moriceau, 2016; Hansart et al., 2016) and deep learning models (Fries, 2016; Chikka, 2016; Li and Huang, 2016). Similarly, CRF and neural networks models have been used for the second group of subtasks (AAl Abdulsalam et al., 2016; Cohan et al., 597 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 597–602, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics sentence relations that do not span over more than three sentences. By doing so, we ob"
S17-2098,S16-1194,0,0.0219105,"TIMEX3 as well as Document Creation Time (DCT) relations between EVENT entities and documents in which they are embedded. The novelty of the 2017 edition lies in the difference of domains between train and test corpora. More details about the task and the definition of each subtask can be found in Bethard et al. (2017). The task has been offered by SemEval over the past two years. Concerning the first group of subtasks, different approaches have been implemented by the participants including Conditional Random Fields (CRF) (AAl Abdulsalam et al., 2016; Caselli and Morante, 2016; Chikka, 2016; Cohan et al., 2016; Grouin and Moriceau, 2016; Hansart et al., 2016) and deep learning models (Fries, 2016; Chikka, 2016; Li and Huang, 2016). Similarly, CRF and neural networks models have been used for the second group of subtasks (AAl Abdulsalam et al., 2016; Cohan et al., 597 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 597–602, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics sentence relations that do not span over more than three sentences. By doing so, we obtain a manageable training corpus size with less unbalanced c"
S17-2098,S16-1198,0,0.0296996,"n which they are embedded. The novelty of the 2017 edition lies in the difference of domains between train and test corpora. More details about the task and the definition of each subtask can be found in Bethard et al. (2017). The task has been offered by SemEval over the past two years. Concerning the first group of subtasks, different approaches have been implemented by the participants including Conditional Random Fields (CRF) (AAl Abdulsalam et al., 2016; Caselli and Morante, 2016; Chikka, 2016; Cohan et al., 2016; Grouin and Moriceau, 2016; Hansart et al., 2016) and deep learning models (Fries, 2016; Chikka, 2016; Li and Huang, 2016). Similarly, CRF and neural networks models have been used for the second group of subtasks (AAl Abdulsalam et al., 2016; Cohan et al., 597 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 597–602, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics sentence relations that do not span over more than three sentences. By doing so, we obtain a manageable training corpus size with less unbalanced classes while keeping a good coverage. 3 Corpus Preprocessing We preprocessed the corpus"
S17-2098,S16-1190,0,0.0273623,"cument Creation Time (DCT) relations between EVENT entities and documents in which they are embedded. The novelty of the 2017 edition lies in the difference of domains between train and test corpora. More details about the task and the definition of each subtask can be found in Bethard et al. (2017). The task has been offered by SemEval over the past two years. Concerning the first group of subtasks, different approaches have been implemented by the participants including Conditional Random Fields (CRF) (AAl Abdulsalam et al., 2016; Caselli and Morante, 2016; Chikka, 2016; Cohan et al., 2016; Grouin and Moriceau, 2016; Hansart et al., 2016) and deep learning models (Fries, 2016; Chikka, 2016; Li and Huang, 2016). Similarly, CRF and neural networks models have been used for the second group of subtasks (AAl Abdulsalam et al., 2016; Cohan et al., 597 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 597–602, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics sentence relations that do not span over more than three sentences. By doing so, we obtain a manageable training corpus size with less unbalanced classes while keeping a good"
S17-2098,S16-1200,0,0.0644888,"Missing"
S17-2098,S15-2136,0,0.0813531,"Missing"
S17-2098,S17-2093,0,0.0294291,"us editions of the challenge (Bethard et al., 2015, 2016), the first group of subtasks concerns medical event (EVENT) and temporal expression (TIMEX3) extraction from raw text. In a second group of subtasks, participants are challenged to extract containment (CONTAINS) relations between EVENT and/or TIMEX3 as well as Document Creation Time (DCT) relations between EVENT entities and documents in which they are embedded. The novelty of the 2017 edition lies in the difference of domains between train and test corpora. More details about the task and the definition of each subtask can be found in Bethard et al. (2017). The task has been offered by SemEval over the past two years. Concerning the first group of subtasks, different approaches have been implemented by the participants including Conditional Random Fields (CRF) (AAl Abdulsalam et al., 2016; Caselli and Morante, 2016; Chikka, 2016; Cohan et al., 2016; Grouin and Moriceau, 2016; Hansart et al., 2016) and deep learning models (Fries, 2016; Chikka, 2016; Li and Huang, 2016). Similarly, CRF and neural networks models have been used for the second group of subtasks (AAl Abdulsalam et al., 2016; Cohan et al., 597 Proceedings of the 11th International W"
S17-2098,N16-1030,0,0.0591663,"Missing"
S17-2098,S16-1201,0,0.0752453,"Missing"
S17-2098,S16-1197,0,0.0220144,"The novelty of the 2017 edition lies in the difference of domains between train and test corpora. More details about the task and the definition of each subtask can be found in Bethard et al. (2017). The task has been offered by SemEval over the past two years. Concerning the first group of subtasks, different approaches have been implemented by the participants including Conditional Random Fields (CRF) (AAl Abdulsalam et al., 2016; Caselli and Morante, 2016; Chikka, 2016; Cohan et al., 2016; Grouin and Moriceau, 2016; Hansart et al., 2016) and deep learning models (Fries, 2016; Chikka, 2016; Li and Huang, 2016). Similarly, CRF and neural networks models have been used for the second group of subtasks (AAl Abdulsalam et al., 2016; Cohan et al., 597 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 597–602, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics sentence relations that do not span over more than three sentences. By doing so, we obtain a manageable training corpus size with less unbalanced classes while keeping a good coverage. 3 Corpus Preprocessing We preprocessed the corpus using cTAKES 3.2.2 (Savova et al.,"
S17-2098,D15-1063,0,0.0303953,"es while keeping a good coverage. 3 Corpus Preprocessing We preprocessed the corpus using cTAKES 3.2.2 (Savova et al., 2010), an open-source natural language processing system for the extraction of information from electronic health records. We extracted sentence and token boundaries, as well as token types and semantic types of the entities that have a span overlap with a least one gold standard EVENT entity of the THYME corpus. This information was added to the set of gold standard attributes available for EVENT entities in the corpus. We also preprocessed the corpus using HeidelTime 2.2.1 (Strötgen and Gertz, 2015), a multilingual domain-sensitive temporal tagger, and used the results to further extend our feature set. 4 4.1 Figure 1: Neural model for EVENT extraction. – – – – EVENT type attribute, EVENT plain lexical form, EVENT position within the document, POS tags of the verbs within the right and left contexts of the considered entity, – EVENT POS tag, – type or class of the other entities that are present within the left and right contexts, – token unigrams and bigrams within a window around the entity. Models Entity Extraction Our approach relies on Long Short-Term Memory Networks (LSTMs) (Hochre"
S17-2098,S16-1175,1,0.858559,"roblem. For each combination E1 – E2 of EVENT and/or TIMEX3 from left to right, three cases are possible: – E1 temporally contains E2, – E1 is temporally contained by E2, – there is no relation between E1 and E2. Intra- and inter-sentence relation detection can be seen as two different tasks with specific features. Intra-sentence relations can benefit from intra-sentential clues such as adverbs (e.g. during) or pronouns (e.g. which) which are not available at the inter-sentence level. Furthermore, past work on the topic seems to indicate that this differentiation improves overall performance (Tourille et al., 2016). We have adopted this approach by building two separate classifiers, one for intra-sentence relations and one for inter-sentence relations. If we were to consider all combinations of entities within documents for inter-sentence relations, it would result in a very large training corpus with very few positive examples. In order to cope with this issue, we limit our experiments to interIntroduction SemEval 2017 Task 12 offers 6 subtasks addressing medical event recognition and temporal reasoning in the clinical domain using the THYME corpus (Styler IV et al., 2014). Similarly to the two previou"
W01-0909,P93-1041,0,0.0201086,"ion of their mutual information to capture semantic and pragmatic relations between them, computed from their co-occurrence count. In order to build class of words linked to a same topic, we first realize a topic segmentation of the texts in thematic units (TU) whose words refer to the same topic, and learning is applied on these thematic units. Text segmentation is based on the use of the collocation network. A topic is detected by computing a cohesion value for each word resulting from the relations found in the network between these words and their neighbors in a text. As in Kozima’s work (Kozima, 1993), this computation operates on words belonging to a focus window that is moved all over the text.The cohesion values lead to build a graph and by successive transformations applied to it, texts are automatically divided in discourse segments. Such a method leads to delimit small segments, whose size is equivalent to a paragraph, i. e. capable of retrieving topic variations in short texts, as newswires for example. Table 1 shows an extract of the words belonging to a cohesive segment about a dedication of a book. 2.2 Semantic Domain learning in SEGAPSITH Learning a semantic domain consists of a"
W01-1207,C00-1039,1,0.815335,"es for multi-word term and named entity extraction with a specific concern for term conflation through variant recognition. Since named entity recognition has already been described extensively in other publications (Baluja 1999), we present the contribution of terminological variants to adding knowledge to our system. The two main activities involving terminology in NLP are term acquisition and term recognition. Basically, terms can be viewed as a particular type of lexical data. Term variation may involve structural, morphological, and semantic transformations of single or multiwords terms (Fabre and Jacquemin, 2000). In this paper, we describe how QALC uses high level indexes, made of terms and variants, to select among documents the most relevant ones with regard to a question, and then to match candidate answers with this question. In the selection process, the documents first retrieved by a search engine, are then postfiltered and ranked through a weighting scheme based on high level indexes, in order to retain the top ranked ones. Similarly, all systems that participated in TREC9 have a search engine component that firstly selects a subset of the provided database of about one million documents. Sinc"
W01-1207,C00-1043,0,0.0368035,"Missing"
W01-1207,P99-1044,1,0.904337,"uestion relies on a shallow parser which spots discriminating patterns and assigns categories to the question. The categories correspond to the types of entities that are likely to constitute the answer to the question. In order to select the best documents from the results given by the search engine and to locate the answers inside them, we work with terms and their variants, i.e. morphologic, syntactic and semantic equivalent expressions. A term extractor has been developed, based on syntactic patterns which describe complex nominal phrases and their subparts. These terms are used by FASTR (Jacquemin 1999), a shallow transformational natural language analyzer that recognizes their occurrences and their variants. Each occurrence or variant constitutes an index that is subsequently used in the processes of document ranking and question/document matching. Documents are ordered according to a weight computed thanks to the number and the quality of the terms and variants they contain. For example, original terms with proper names are considered more reliable than semantic variants. An analysis of the weight graph enables the system to select a relevant subpart of the documents, whose size varies alo"
W14-6603,ayache-etal-2006-equer,0,0.028995,"Missing"
W14-6603,P14-1023,0,0.0600442,"Missing"
W14-6603,P12-1015,0,0.0872796,"Missing"
W14-6603,W02-0908,0,0.0177046,"Missing"
W14-6603,D10-1113,0,0.0699009,"Missing"
W14-6603,ferret-2010-testing,1,0.890607,"Missing"
W14-6603,S07-1029,0,0.0565992,"Missing"
W14-6603,W06-2907,0,0.0262464,"Missing"
W14-6603,W11-1705,0,0.0217181,"Missing"
W14-6603,W13-0112,0,0.0374173,"Missing"
W14-6603,W12-3401,0,0.0513052,"Missing"
W14-6603,P12-1092,0,0.115115,"Missing"
W14-6603,S07-1009,0,0.0873129,"Missing"
W14-6603,N13-1090,0,0.0366116,"Missing"
W14-6603,D12-1094,0,0.0223099,"Missing"
W14-6603,W04-2607,0,0.0149917,"Missing"
W14-6603,D09-1098,0,0.03478,"Missing"
W14-6603,D11-1094,0,0.0290383,"Missing"
W14-6603,P12-1063,0,0.0223485,"Missing"
W17-4211,N16-1034,0,0.0452228,"ST, Gif-sur-Yvette, F-91191 France. olivier.ferret@cea.fr Xavier Tannier LIMSI, CNRS Univ. Paris-Sud Universit´e Paris-Saclay xavier.tannier@limsi.fr Abstract event detection. It is also an important subtask of larger NLP applications such as document summarization and event schema induction. Several approaches have been used to tackle the different aspects of this task, particularly in an unsupervised fashion, from linguistic pipelines (Filatova et al., 2006; Huang et al., 2016) to topic modeling approaches (Chambers and Jurafsky, 2011; Cheung et al., 2013) and more recently neural networks (Nguyen et al., 2016). While the definition and granularity of an event varies with the task and objectives at hand, most event identification systems exploit mentions to produce type-level representations. We propose to address the unsupervised event extraction task through two subtasks: first, unsupervised event instance extraction and second, event type extraction. This paper will focus on our efforts regarding the first step, e.g. unsupervised event instance extraction. In this perspective, we present a method based on clustering algorithms leveraging news data from different sources. We believe that this firs"
W17-4211,P11-1098,0,0.0795391,"Missing"
W17-4211,N13-1104,0,0.0210226,"aris-Saclay swen.ribeiro@limsi.fr Olivier Ferret CEA, LIST, Gif-sur-Yvette, F-91191 France. olivier.ferret@cea.fr Xavier Tannier LIMSI, CNRS Univ. Paris-Sud Universit´e Paris-Saclay xavier.tannier@limsi.fr Abstract event detection. It is also an important subtask of larger NLP applications such as document summarization and event schema induction. Several approaches have been used to tackle the different aspects of this task, particularly in an unsupervised fashion, from linguistic pipelines (Filatova et al., 2006; Huang et al., 2016) to topic modeling approaches (Chambers and Jurafsky, 2011; Cheung et al., 2013) and more recently neural networks (Nguyen et al., 2016). While the definition and granularity of an event varies with the task and objectives at hand, most event identification systems exploit mentions to produce type-level representations. We propose to address the unsupervised event extraction task through two subtasks: first, unsupervised event instance extraction and second, event type extraction. This paper will focus on our efforts regarding the first step, e.g. unsupervised event instance extraction. In this perspective, we present a method based on clustering algorithms leveraging new"
W17-4211,cybulska-vossen-2014-using,0,0.0277963,"t. We leverage press agency newswire and monolingual word alignment techniques to build meaningful and linguistically varied clusters of articles from the Web in the perspective of a broader event type detection task. We validate our approach on a manually annotated corpus of Web articles. 1 Introduction In the context of news production, an event is the characterization of a significant enough change in a space-time context to be reported as newsworthy content. This definition fits with definitions proposed in other contexts such as the ACE 2005 and TAC KBP Event evaluations or work such as (Cybulska and Vossen, 2014; Mitamura et al., 2015), which generally view each event as “something that happens at a particular place and time”, implying changes in the state of the world and involving participants. In accordance with ontologies about events such as the Simple Event Model (SEM) ontology (van Hage et al., 2011), events can be categorized into different types, for example “elections” or “earthquakes”, gathering multiple real-life instances, for example the “2017 UK General Election” or the “2012 French Presidential Election”. These instances are reported by journalists through varying textual mentions. Ev"
W17-4211,P06-2027,0,0.0413434,"and Aggregation from Newswire and Web Articles Swen Ribeiro LIMSI, CNRS Univ. Paris-Sud Universit´e Paris-Saclay swen.ribeiro@limsi.fr Olivier Ferret CEA, LIST, Gif-sur-Yvette, F-91191 France. olivier.ferret@cea.fr Xavier Tannier LIMSI, CNRS Univ. Paris-Sud Universit´e Paris-Saclay xavier.tannier@limsi.fr Abstract event detection. It is also an important subtask of larger NLP applications such as document summarization and event schema induction. Several approaches have been used to tackle the different aspects of this task, particularly in an unsupervised fashion, from linguistic pipelines (Filatova et al., 2006; Huang et al., 2016) to topic modeling approaches (Chambers and Jurafsky, 2011; Cheung et al., 2013) and more recently neural networks (Nguyen et al., 2016). While the definition and granularity of an event varies with the task and objectives at hand, most event identification systems exploit mentions to produce type-level representations. We propose to address the unsupervised event extraction task through two subtasks: first, unsupervised event instance extraction and second, event type extraction. This paper will focus on our efforts regarding the first step, e.g. unsupervised event instan"
W17-4211,P13-2139,0,0.0605657,"Missing"
W17-4211,P16-1025,0,0.0613311,"Missing"
W17-4211,W15-0809,0,0.0307778,"newswire and monolingual word alignment techniques to build meaningful and linguistically varied clusters of articles from the Web in the perspective of a broader event type detection task. We validate our approach on a manually annotated corpus of Web articles. 1 Introduction In the context of news production, an event is the characterization of a significant enough change in a space-time context to be reported as newsworthy content. This definition fits with definitions proposed in other contexts such as the ACE 2005 and TAC KBP Event evaluations or work such as (Cybulska and Vossen, 2014; Mitamura et al., 2015), which generally view each event as “something that happens at a particular place and time”, implying changes in the state of the world and involving participants. In accordance with ontologies about events such as the Simple Event Model (SEM) ontology (van Hage et al., 2011), events can be categorized into different types, for example “elections” or “earthquakes”, gathering multiple real-life instances, for example the “2017 UK General Election” or the “2012 French Presidential Election”. These instances are reported by journalists through varying textual mentions. Event extraction is a chal"
W17-4211,Q14-1018,0,\N,Missing
W18-5622,S15-2136,0,0.0739526,"Missing"
W18-5622,S17-2093,0,0.0512896,"Missing"
W18-5622,Q17-1010,0,0.193683,"#4, IOB format) and two categorical features (cols. #2 and #3), the partof-speech tag and a chunk label (IOB format). Tool Overview In this section, we present a general overview of YASET. First we describe input data formats (sequences and pre-trained word embeddings). Then we present the input pipeline, the network training phase, the implemented evaluation metrics and the management of its parameters. 4.1 4.2 Word Embeddings YASET supports embeddings in the word2vec (Mikolov et al., 2013b) or Genˇ uˇrek and Sojka, 2010) formats. Other sim (Reh˚ formats of embeddings, for instance FastText (Bojanowski et al., 2017) or Glove (Pennington et al., 2014), must first be converted to either accepted format. Input and Output Data YASET takes CoNLL-like10 formatted files as input. Sequences are separated by empty lines and there must be one token per line. For each token, 10 http://universaldependencies.org/ docs/format.html 195 the maximum number of iterations n and the patience criterion p. Training will stop if the maximum number n is reached or if there are p iterations without performance improvement on the development instances. Users can also set several parameters related to the learning algorithm such a"
W18-5622,W12-2411,0,0.0720674,"Missing"
W18-5622,N18-1131,0,0.0732005,"Missing"
W18-5622,N18-1079,0,0.0142593,"e of the total training set indicated on the first row). Standard errors appear between parentheses. We observe that the performance improves logarithmically with every chunk added in the training data as shown in Table 4. This finding is similar to the observation of Sun et al. (2017) for vision tasks. Further addition of data will slightly improve the performance as the maximum performance plateau is almost reached. 200 overlapping clinical entities at each layer. Ju et al. (2018) present a dynamic end-to-end neural network model capable of handling an undetermined number of nesting levels. Katiyar and Cardie (2018) model the task as an hypergraph whose structure is learned with an LSTM network. Future research will focus on the influence of word embedding models which were shown to significantly impact on performance. Specifically, models taking into account sub-token information (Bojanowski et al., 2017) or emphasizing context (Peters et al., 2018) should be further explored. Moreover, other neural network models for NER such as the ones proposed by Rei et al. (2016) and Ma and Hovy (2016) will be investigated and implemented in YASET. Having a centralized implementation of different NER models will al"
W18-5622,P17-1194,0,0.0178373,"put format and from the output format to the brat format. The tool produces several plots during training for performance analysis. It is implemented in Python and makes use of the TensorFlow library. Both implementations of the Bi-LSTM model suffer from a very long training time which makes them cumbersome to use. YASET offers a faster implementation of the model by allowing mini-batch training and by using the pipeline API of TensorFlow. Rei and Yannakoudakis (2016) released a Python implementation5 of different models presented in their works (Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). One major difference with YASET resides in the possibility to use a language modeling objective during training. Recently, Yang and Zhang (2018) introduced NCRF++6 , a tool presented as the neural version of CRF++7 and implemented with PyTorch (Paszke et al., 2017). The tool is very close to YASET, with the possibility to define handcrafted word features and to perform nbest decoding. Another type of neural network model, based on Convolutional Neural Networks (CNNs) for character level representation, is presented in Ma and Hovy (2016). The authors implemented the architecture with PyTorch."
W18-5622,C16-1030,0,0.112027,"., 2012) to the input format and from the output format to the brat format. The tool produces several plots during training for performance analysis. It is implemented in Python and makes use of the TensorFlow library. Both implementations of the Bi-LSTM model suffer from a very long training time which makes them cumbersome to use. YASET offers a faster implementation of the model by allowing mini-batch training and by using the pipeline API of TensorFlow. Rei and Yannakoudakis (2016) released a Python implementation5 of different models presented in their works (Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). One major difference with YASET resides in the possibility to use a language modeling objective during training. Recently, Yang and Zhang (2018) introduced NCRF++6 , a tool presented as the neural version of CRF++7 and implemented with PyTorch (Paszke et al., 2017). The tool is very close to YASET, with the possibility to define handcrafted word features and to perform nbest decoding. Another type of neural network model, based on Convolutional Neural Networks (CNNs) for character level representation, is presented in Ma and Hovy (2016). The authors implemented the architecture w"
W18-5622,N16-1030,0,0.193553,"e, we report distributions over 30 runs and different sizes of training datasets. YASET provides stateof-the-art performance on the CoNLL 2003 NER dataset (F1=0.87), MEDPOST corpus (F1=0.97), MERLoT corpus (F1=0.99) and NCBI disease corpus (F1=0.81). We believe that YASET is a versatile and efficient tool that can be used for sequence tagging in biomedical and clinical texts. 1 • a fast and accurate implementation of a state-of-the-art sequence tagging model based on Long Short-Term Memory Networks (LSTMs) (Hochreiter and Schmidhuber, 1997). The architecture is similar to the one described in Lample et al. (2016) and is able to process mini-batches for faster training. Furthermore, YASET supports the use of handcrafted features in combination with word and character embeddings; • an easy-to-use interface based on a central configuration file that is used to setup experiments, with default parameters that are suitable for most sequence tagging tasks; • an evaluation on various biomedical corpora and on the CoNLL 2003 corpus, studying the stability of our model and the effect of training data size. We compare YASET performance with state-of-the-art results published in the literature. Introduction Many"
W18-5622,P16-1112,0,0.0675502,"TM model. The authors intended to make the tool easy to use by providing automatic format conversion from the brat format (Stenetorp et al., 2012) to the input format and from the output format to the brat format. The tool produces several plots during training for performance analysis. It is implemented in Python and makes use of the TensorFlow library. Both implementations of the Bi-LSTM model suffer from a very long training time which makes them cumbersome to use. YASET offers a faster implementation of the model by allowing mini-batch training and by using the pipeline API of TensorFlow. Rei and Yannakoudakis (2016) released a Python implementation5 of different models presented in their works (Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). One major difference with YASET resides in the possibility to use a language modeling objective during training. Recently, Yang and Zhang (2018) introduced NCRF++6 , a tool presented as the neural version of CRF++7 and implemented with PyTorch (Paszke et al., 2017). The tool is very close to YASET, with the possibility to define handcrafted word features and to perform nbest decoding. Another type of neural network model, based on Convolutional Neural Netw"
W18-5622,D17-1035,0,0.0682807,"ning. Recently, Yang and Zhang (2018) introduced NCRF++6 , a tool presented as the neural version of CRF++7 and implemented with PyTorch (Paszke et al., 2017). The tool is very close to YASET, with the possibility to define handcrafted word features and to perform nbest decoding. Another type of neural network model, based on Convolutional Neural Networks (CNNs) for character level representation, is presented in Ma and Hovy (2016). The authors implemented the architecture with PyTorch. The code is freely available online8 . The same type of architecture is implemented in the tool released by Reimers and Gurevych (2017). It is implemented with Keras (Chollet et al., 2015) and is freely available online9 . Outside the peer-reviewed scientific environment, many other implementations are freely available online. However, we will not review them in this paper. 3 Neural Network Model There is currently one neural network model implemented in YASET. This model is mostly based on Lample et al. (2016). However, similar architectures are presented in other work (Collobert et al., 2011; Ma and Hovy, 2016; Rei and Yannakoudakis, 2016; Rei et al., 2016). Other network architectures will be implemented in the future. 3.1"
W18-5622,P16-1101,0,0.305832,"in their works (Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). One major difference with YASET resides in the possibility to use a language modeling objective during training. Recently, Yang and Zhang (2018) introduced NCRF++6 , a tool presented as the neural version of CRF++7 and implemented with PyTorch (Paszke et al., 2017). The tool is very close to YASET, with the possibility to define handcrafted word features and to perform nbest decoding. Another type of neural network model, based on Convolutional Neural Networks (CNNs) for character level representation, is presented in Ma and Hovy (2016). The authors implemented the architecture with PyTorch. The code is freely available online8 . The same type of architecture is implemented in the tool released by Reimers and Gurevych (2017). It is implemented with Keras (Chollet et al., 2015) and is freely available online9 . Outside the peer-reviewed scientific environment, many other implementations are freely available online. However, we will not review them in this paper. 3 Neural Network Model There is currently one neural network model implemented in YASET. This model is mostly based on Lample et al. (2016). However, similar architec"
W18-5622,E12-2021,0,0.11951,"Missing"
W18-5622,D14-1162,0,0.0823245,"features (cols. #2 and #3), the partof-speech tag and a chunk label (IOB format). Tool Overview In this section, we present a general overview of YASET. First we describe input data formats (sequences and pre-trained word embeddings). Then we present the input pipeline, the network training phase, the implemented evaluation metrics and the management of its parameters. 4.1 4.2 Word Embeddings YASET supports embeddings in the word2vec (Mikolov et al., 2013b) or Genˇ uˇrek and Sojka, 2010) formats. Other sim (Reh˚ formats of embeddings, for instance FastText (Bojanowski et al., 2017) or Glove (Pennington et al., 2014), must first be converted to either accepted format. Input and Output Data YASET takes CoNLL-like10 formatted files as input. Sequences are separated by empty lines and there must be one token per line. For each token, 10 http://universaldependencies.org/ docs/format.html 195 the maximum number of iterations n and the patience criterion p. Training will stop if the maximum number n is reached or if there are p iterations without performance improvement on the development instances. Users can also set several parameters related to the learning algorithm such as the initial learning rate, and gr"
W18-5622,N18-1202,0,0.0648615,"Missing"
W18-5622,P18-4013,0,0.0139859,"ented in Python and makes use of the TensorFlow library. Both implementations of the Bi-LSTM model suffer from a very long training time which makes them cumbersome to use. YASET offers a faster implementation of the model by allowing mini-batch training and by using the pipeline API of TensorFlow. Rei and Yannakoudakis (2016) released a Python implementation5 of different models presented in their works (Rei and Yannakoudakis, 2016; Rei et al., 2016; Rei, 2017). One major difference with YASET resides in the possibility to use a language modeling objective during training. Recently, Yang and Zhang (2018) introduced NCRF++6 , a tool presented as the neural version of CRF++7 and implemented with PyTorch (Paszke et al., 2017). The tool is very close to YASET, with the possibility to define handcrafted word features and to perform nbest decoding. Another type of neural network model, based on Convolutional Neural Networks (CNNs) for character level representation, is presented in Ma and Hovy (2016). The authors implemented the architecture with PyTorch. The code is freely available online8 . The same type of architecture is implemented in the tool released by Reimers and Gurevych (2017). It is im"
wang-etal-2012-evaluation,P04-1053,0,\N,Missing
wang-etal-2012-evaluation,P08-1004,0,\N,Missing
wang-etal-2012-evaluation,N06-1039,0,\N,Missing
