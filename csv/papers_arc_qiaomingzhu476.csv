2021.findings-emnlp.100,Winnowing Knowledge for Multi-choice Question Answering,2021,-1,-1,6,0,6659,yeqiu li,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"We tackle multi-choice question answering. Acquiring related commonsense knowledge to the question and options facilitates the recognition of the correct answer. However, the current reasoning models suffer from the noises in the retrieved knowledge. In this paper, we propose a novel encoding method which is able to conduct interception and soft filtering. This contributes to the harvesting and absorption of representative information with less interference from noises. We experiment on CommonsenseQA. Experimental results illustrate that our method yields substantial and consistent improvements compared to the strong Bert, RoBERTa and Albert-based baselines."
2021.emnlp-main.187,Not Just Classification: Recognizing Implicit Discourse Relation on Joint Modeling of Classification and Generation,2021,-1,-1,5,1,9020,feng jiang,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Implicit discourse relation recognition (IDRR) is a critical task in discourse analysis. Previous studies only regard it as a classification task and lack an in-depth understanding of the semantics of different relations. Therefore, we first view IDRR as a generation task and further propose a method joint modeling of the classification and generation. Specifically, we propose a joint model, CG-T5, to recognize the relation label and generate the target sentence containing the meaning of relations simultaneously. Furthermore, we design three target sentence forms, including the question form, for the generation model to incorporate prior knowledge. To address the issue that large discourse units are hardly embedded into the target sentence, we also propose a target sentence construction mechanism that automatically extracts core sentences from those large discourse units. Experimental results both on Chinese MCDTB and English PDTB datasets show that our model CG-T5 achieves the best performance against several state-of-the-art systems."
2021.acl-short.70,More than Text: Multi-modal {C}hinese Word Segmentation,2021,-1,-1,5,1,9446,dong zhang,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Chinese word segmentation (CWS) is undoubtedly an important basic task in natural language processing. Previous works only focus on the textual modality, but there are often audio and video utterances (such as news broadcast and face-to-face dialogues), where textual, acoustic and visual modalities normally exist. To this end, we attempt to combine the multi-modality (mainly the converted text and actual voice information) to perform CWS. In this paper, we annotate a new dataset for CWS containing text and audio. Moreover, we propose a time-dependent multi-modal interactive model based on Transformer framework to integrate multi-modal information for word sequence labeling. The experimental results on three different training sets show the effectiveness of our approach with fusing text and audio."
2020.emnlp-main.291,Multi-modal Multi-label Emotion Detection with Modality and Label Dependence,2020,-1,-1,5,1,9446,dong zhang,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"As an important research issue in the natural language processing community, multi-label emotion detection has been drawing more and more attention in the last few years. However, almost all existing studies focus on one modality (e.g., textual modality). In this paper, we focus on multi-label emotion detection in a multi-modal scenario. In this scenario, we need to consider both the dependence among different labels (label dependence) and the dependence between each predicting label and different modalities (modality dependence). Particularly, we propose a multi-modal sequence-to-set approach to effectively model both kinds of dependence in multi-modal multi-label emotion detection. The detailed evaluation demonstrates the effectiveness of our approach."
2020.coling-main.506,{C}hinese Paragraph-level Discourse Parsing with Global Backward and Local Reverse Reading,2020,-1,-1,5,1,9020,feng jiang,Proceedings of the 28th International Conference on Computational Linguistics,0,"Discourse structure tree construction is the fundamental task of discourse parsing and most previous work focused on English. Due to the cultural and linguistic differences, existing successful methods on English discourse parsing cannot be transformed into Chinese directly, especially in paragraph level suffering from longer discourse units and fewer explicit connectives. To alleviate the above issues, we propose two reading modes, i.e., the global backward reading and the local reverse reading, to construct Chinese paragraph level discourse trees. The former processes discourse units from the end to the beginning in a document to utilize the left-branching bias of discourse structure in Chinese, while the latter reverses the position of paragraphs in a discourse unit to enhance the differentiation of coherence between adjacent discourse units. The experimental results on Chinese MCDTB demonstrate that our model outperforms all strong baselines."
2020.ccl-1.18,"èåå\
¨å±åå±é¨ä¿¡æ¯çæ±è¯­å®è§ç¯ç« ç»æè¯å«(Combining Global and Local Information to Recognize {C}hinese Macro Discourse Structure)",2020,-1,-1,5,0,9021,yaxin fan,Proceedings of the 19th Chinese National Conference on Computational Linguistics,0,"ä½ä¸ºå®è§ç¯ç« åæä¸­çåºç¡ä»»å¡,ç¯ç« ç»æè¯å«ä»»å¡çç®çæ¯è¯å«ç¸é»ç¯ç« åå
ä¹é´çç»æ,å¹¶å±æ¬¡åæå»ºç¯ç« ç»ææ ãå·²æçå·¥ä½åªèèå±é¨çç»æåè¯­ä¹ä¿¡æ¯æåªèèå
¨å±ä¿¡æ¯ãå æ­¤,æ¬ææåºäºä¸ç§èåå
¨å±åå±é¨ä¿¡æ¯çæéç½ç»æ¨¡å,è¯¥æ¨¡åå¨èèå
¨å±çè¯­ä¹ä¿¡æ¯åæ¶,åèèå±é¨æ®µè½é´çè¯­ä¹å
³ç³»å¯åç¨åº¦,ä»èææå°æé«å®è§ç¯ç« ç»æè¯å«çè½åãå¨æ±è¯­å®è§ç¯ç« æ åº(MCDTB)çå®éªç»æè¡¨æ,æ¬æææåºçæ¨¡åæ§è½ä¼äºç®åæ§è½æå¥½çæ¨¡åã"
2020.ccl-1.35,"åºäºé\
è¯»çè§£æ¡æ¶çä¸­æäºä»¶è®ºå\
æ½å({C}hinese Event Argument Extraction using Reading Comprehension Framework)",2020,-1,-1,5,0,22042,min chen,Proceedings of the 19th Chinese National Conference on Computational Linguistics,0,"ä¼ ç»çäºä»¶è®ºå
æ½åæ¹æ³æè¯¥ä»»å¡å½ä½å¥å­ä¸­å®ä½æåçå¤åç±»æåºåæ æ³¨ä»»å¡,è®ºå
è§è²çç±»å«å¨è¿äºæ¹æ³ä¸­åªè½ä½ä¸ºåéè¡¨ç¤º,èå¿½ç¥äºè®ºå
è§è²çå
éªä¿¡æ¯ãå®é
ä¸,è®ºå
è§è²çè¯­ä¹åè®ºå
æ¬èº«æå¾å¤§å
³ç³»ãå¯¹æ­¤,æ¬ææè®®å°å
¶å½ä½æºå¨é
è¯»çè§£ä»»å¡,æè®ºå
è§è²è¡¨è¿°ä¸ºèªç¶è¯­è¨æè¿°çé®é¢,éè¿å¨ä¸ä¸æä¸­åç­è¿äºé®é¢æ¥æ½åè®ºå
ãè¯¥æ¹æ³æ´å¥½å°å©ç¨äºè®ºå
è§è²ç±»å«çå
éªä¿¡æ¯,å¨ACE2005ä¸­æè¯­æä¸çå®éªè¯æäºè¯¥æ¹æ³çæææ§ã"
2020.ccl-1.59,åºäºåçç£å­¦ä¹ çä¸­æç¤¾äº¤ææ¬äºä»¶èç±»æ¹æ³(Semi-supervised Method to Cluster {C}hinese Events on Social Streams),2020,-1,-1,4,0,22082,hengrui guo,Proceedings of the 19th Chinese National Conference on Computational Linguistics,0,"é¢åç¤¾äº¤åªä½çäºä»¶èç±»æ¨å¨æ ¹æ®äºä»¶ç¹å¾å¯¹ç­ææ¬èç±»ãç®å,äºä»¶èç±»æ¨¡åä¸»è¦åä¸ºæ çç£æ¨¡ååæçç£æ¨¡åãæ çç£æ¨¡åèç±»ææè¾å·®,æçç£æ¨¡åä¾èµå¤§éæ æ³¨æ°æ®ãåºäºæ­¤,æ¬ææåºäºä¸ç§åçç£äºä»¶èç±»æ¨¡å(SemiEC),è¯¥æ¨¡åå¨å°è§æ¨¡æ æ³¨æ°æ®çåºç¡ä¸,å©ç¨LSTMè¡¨å¾äºä»¶,å©ç¨çº¿æ§æ¨¡åè®¡ç®ææ¬ç¸ä¼¼åº¦,è¿è¡å¢éèç±»,å©ç¨å¢éèç±»äº§ççæ æ³¨æ°æ®å¯¹æ¨¡ååè®­ç»,ç»æåå¯¹ä¸ç¡®å®æ ·æ¬åèç±»ãå®éªè¡¨æ,SemiECçæ§è½ç¸æ¯å
¶ä»æ¨¡ååæææé«ã"
P19-1058,Topic Tensor Network for Implicit Discourse Relation Recognition in {C}hinese,2019,0,0,4,1,21288,sheng xu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"In the literature, most of the previous studies on English implicit discourse relation recognition only use sentence-level representations, which cannot provide enough semantic information in Chinese due to its unique paratactic characteristics. In this paper, we propose a topic tensor network to recognize Chinese implicit discourse relations with both sentence-level and topic-level representations. In particular, besides encoding arguments (discourse units) using a gated convolutional network to obtain sentence-level representations, we train a simplified topic model to infer the latent topic-level representations. Moreover, we feed the two pairs of representations to two factored tensor networks, respectively, to capture both the sentence-level interactions and topic-level relevance using multi-slice tensors. Experimentation on CDTB, a Chinese discourse corpus, shows that our proposed model significantly outperforms several state-of-the-art baselines in both micro and macro F1-scores."
N19-1287,Document-Level Event Factuality Identification via Adversarial Neural Network,2019,0,0,3,0,26216,zhong qian,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Document-level event factuality identification is an important subtask in event factuality and is crucial for discourse understanding in Natural Language Processing (NLP). Previous studies mainly suffer from the scarcity of suitable corpus and effective methods. To solve these two issues, we first construct a corpus annotated with both document- and sentence-level event factuality information on both English and Chinese texts. Then we present an LSTM neural network based on adversarial training with both intra- and inter-sequence attentions to identify document-level event factuality. Experimental results show that our neural network model can outperform various baselines on the constructed corpus."
D19-1230,Negative Focus Detection via Contextual Attention Mechanism,2019,0,0,5,0,22069,longxiang shen,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Negation is a universal but complicated linguistic phenomenon, which has received considerable attention from the NLP community over the last decade, since a negated statement often carries both an explicit negative focus and implicit positive meanings. For the sake of understanding a negated statement, it is critical to precisely detect the negative focus in context. However, how to capture contextual information for negative focus detection is still an open challenge. To well address this, we come up with an attention-based neural network to model contextual information. In particular, we introduce a framework which consists of a Bidirectional Long Short-Term Memory (BiLSTM) neural network and a Conditional Random Fields (CRF) layer to effectively encode the order information and the long-range context dependency in a sentence. Moreover, we design two types of attention mechanisms, word-level contextual attention and topic-level contextual attention, to take advantage of contextual information across sentences from both the word perspective and the topic perspective, respectively. Experimental results on the SEM{'}12 shared task corpus show that our approach achieves the best performance on negative focus detection, yielding an absolute improvement of 2.11{\%} over the state-of-the-art. This demonstrates the great effectiveness of the two types of contextual attention mechanisms."
P18-1048,Self-regulation: Employing a Generative Adversarial Network to Improve Event Detection,2018,0,7,5,0.660086,6661,yu hong,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Due to the ability of encoding and mapping semantic information into a high-dimensional latent feature space, neural networks have been successfully used for detecting events to a certain extent. However, such a feature space can be easily contaminated by spurious features inherent in event detection. In this paper, we propose a self-regulated learning approach by utilizing a generative adversarial network to generate spurious features. On the basis, we employ a recurrent network to eliminate the fakes. Detailed experiments on the ACE 2005 and TAC-KBP 2015 corpora show that our proposed method is highly effective and adaptable."
L18-1302,Building a Macro {C}hinese {D}iscourse {T}reebank,2018,0,1,4,1,9022,xiaomin chu,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-1044,Employing Text Matching Network to Recognise Nuclearity in {C}hinese Discourse,2018,0,2,4,1,21288,sheng xu,Proceedings of the 27th International Conference on Computational Linguistics,0,"The task of nuclearity recognition in Chinese discourse remains challenging due to the demand for more deep semantic information. In this paper, we propose a novel text matching network (TMN) that encodes the discourse units and the paragraphs by combining Bi-LSTM and CNN to capture both global dependency information and local n-gram information. Moreover, it introduces three components of text matching, the Cosine, Bilinear and Single Layer Network, to incorporate various similarities and interactions among the discourse units. Experimental results on the Chinese Discourse TreeBank show that our proposed TMN model significantly outperforms various strong baselines in both micro-F1 and macro-F1."
C18-1045,Joint Modeling of Structure Identification and Nuclearity Recognition in Macro {C}hinese {D}iscourse {T}reebank,2018,0,2,5,1,9022,xiaomin chu,Proceedings of the 27th International Conference on Computational Linguistics,0,"Discourse parsing is a challenging task and plays a critical role in discourse analysis. This paper focus on the macro level discourse structure analysis, which has been less studied in the previous researches. We explore a macro discourse structure presentation schema to present the macro level discourse structure, and propose a corresponding corpus, named Macro Chinese Discourse Treebank. On these bases, we concentrate on two tasks of macro discourse structure analysis, including structure identification and nuclearity recognition. In order to reduce the error transmission between the associated tasks, we adopt a joint model of the two tasks, and an Integer Linear Programming approach is proposed to achieve global optimization with various kinds of constraints."
C18-1203,Stance Detection with Hierarchical Attention Network,2018,0,11,3,0,30857,qingying sun,Proceedings of the 27th International Conference on Computational Linguistics,0,"Stance detection aims to assign a stance label (for or against) to a post toward a specific target. Recently, there is a growing interest in using neural models to detect stance of documents. Most of these works model the sequence of words to learn document representation. However, much linguistic information, such as polarity and arguments of the document, is correlated with the stance of the document, and can inspire us to explore the stance. Hence, we present a neural model to fully employ various linguistic information to construct the document representation. In addition, since the influences of different linguistic information are different, we propose a hierarchical attention network to weigh the importance of various linguistic information, and learn the mutual attention between the document and the linguistic information. The experimental results on two datasets demonstrate the effectiveness of the proposed hierarchical attention neural model."
C18-1296,{MCDTB}: A Macro-level {C}hinese Discourse {T}ree{B}ank,2018,0,3,5,1,9020,feng jiang,Proceedings of the 27th International Conference on Computational Linguistics,0,"In view of the differences between the annotations of micro and macro discourse rela-tionships, this paper describes the relevant experiments on the construction of the Macro Chinese Discourse Treebank (MCDTB), a higher-level Chinese discourse corpus. Fol-lowing RST (Rhetorical Structure Theory), we annotate the macro discourse information, including discourse structure, nuclearity and relationship, and the additional discourse information, including topic sentences, lead and abstract, to make the macro discourse annotation more objective and accurate. Finally, we annotated 720 articles with a Kappa value greater than 0.6. Preliminary experiments on this corpus verify the computability of MCDTB."
D16-1078,Speculation and Negation Scope Detection via Convolutional Neural Networks,2016,24,16,3,0,26216,zhong qian,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1137,Global Inference to {C}hinese Temporal Relation Extraction,2016,12,0,2,1,9023,peifeng li,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Previous studies on temporal relation extraction focus on mining sentence-level information or enforcing coherence on different temporal relation types among various event mentions in the same sentence or neighboring sentences, largely ignoring those discourse-level temporal relations in nonadjacent sentences. In this paper, we propose a discourse-level global inference model to mine those temporal relations between event mentions in document-level, especially in nonadjacent sentences. Moreover, we provide various kinds of discourse-level constraints, which derived from event semantics, to further improve our global inference model. Evaluation on a Chinese corpus justifies the effectiveness of our discourse-level global inference model over two strong baselines."
P15-1064,Negation and Speculation Identification in {C}hinese Language,2015,19,10,2,1,6517,bowei zou,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Identifying negative or speculative narrative fragments from fact is crucial for natural language processing (NLP) applications. Previous studies on negation and speculation identification in Chinese language suffers much from two problems: corpus scarcity and the bottleneck in fundamental Chinese information processing. To resolve these problems, this paper constructs a Chinese corpus which consists of three sub-corpora from different resources. In order to detect the negative and speculative cues, a sequence labeling model is proposed. Moreover, a bilingual cue expansion method is proposed to increase the coverage in cue detection. In addition, this paper presents a new syntactic structure-based framework to identify the linguistic scope of a cue, instead of the traditional chunking-based framework. Experimental results justify the usefulness of our Chinese corpus and the appropriateness of our syntactic structure-based framework which obtained significant improvement over the stateof-the-art on negation and speculation identification in Chinese language. *"
D15-1187,Unsupervised Negation Focus Identification with Word-Topic Graph Model,2015,11,1,3,1,6517,bowei zou,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Due to the commonality in natural language, negation focus plays a critical role in deep understanding of context. However, existing studies for negation focus identification major on supervised learning which is timeconsuming and expensive due to manual preparation of annotated corpus. To address this problem, we propose an unsupervised word-topic graph model to represent and measure the focus candidates from both lexical and topic perspectives. Moreover, we propose a document-sensitive biased PageRank algorithm to optimize the ranking scores of focus candidates. Evaluation on the *SEM 2012 shared task corpus shows that our proposed method outperforms the state of the art on negation focus identification."
P14-1049,Negation Focus Identification with Contextual Discourse Information,2014,21,3,3,1,6517,bowei zou,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Negative expressions are common in natural language text and play a critical role in information extraction. However, the performances of current systems are far from satisfaction, largely due to its focus on intrasentence information and its failure to consider inter-sentence information. In this paper, we propose a graph model to enrich intrasentence features with inter-sentence features from both lexical and topic perspectives. Evaluation on the *SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information."
P14-1055,Bilingual Active Learning for Relation Classification via Pseudo Parallel Corpora,2014,42,6,5,0.9965,27118,longhua qian,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Active learning (AL) has been proven effective to reduce human annotation efforts in NLP. However, previous studies on AL are limited to applications in a single language. This paper proposes a bilingual active learning paradigm for relation classification, where the unlabeled instances are first jointly chosen in terms of their prediction uncertainty scores in two languages and then manually labeled by an oracle. Instead of using a parallel corpus, labeled and unlabeled instances in one language are translated into ones in the other language and all instances in both languages are then fed into a bilingual active learning engine as pseudo parallel corpora. Experimental results on the ACE RDC 2005 Chinese and English corpora show that bilingual active learning for relation classification significantly outperforms monolingual active learning."
C14-1176,Synchronous Constituent Context Model for Inducing Bilingual Synchronous Structures,2014,21,0,3,0,8136,xiangyu duan,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Traditional Statistical Machine Translation (SMT) systems heuristically extract synchronous structures from word alignments, while synchronous grammar induction provides better solutions that can discard heuristic method and directly obtain statistically sound bilingual synchronous structures. This paper proposes Synchronous Constituent Context Model (SCCM) for synchronous grammar induction. The SCCM is different to all previous synchronous grammar induction systems in that the SCCM does not use the Context Free Grammars to model the bilingual parallel corpus, but models bilingual constituents and contexts directly. The experiments show that valuable synchronous structures can be found by the SCCM, and the end-to-end machine translation experiment shows that the SCCM improves the quality of SMT results."
C14-1204,Employing Event Inference to Improve Semi-Supervised {C}hinese Event Extraction,2014,29,3,2,1,9023,peifeng li,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Although semi-supervised model can extract the event mentions matching frequent event patterns, it suffers much from those event mentions, which match infrequent patterns or have no matching pattern. To solve this issue, this paper introduces various kinds of linguistic knowledge-driven event inference mechanisms to semi-supervised Chinese event extraction. These event inference mechanisms can capture linguistic knowledge from four aspects, i.e. semantics of argument role, compositional semantics of trigger, consistency on coreference events and relevant events, to further recover missing event mentions from unlabeled texts. Evaluation on the ACE 2005 Chinese corpus shows that our event inference mechanisms significantly outperform the refined state-of-the-art semi-supervised Chinese event extraction system in F1-score by 8.5%."
P13-1145,Argument Inference from Relevant Event Mentions in {C}hinese Argument Extraction,2013,33,12,2,1,9023,peifeng li,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"As a paratactic language, sentence-level argument extraction in Chinese suffers much from the frequent occurrence of ellipsis with regard to inter-sentence arguments. To resolve such problem, this paper proposes a novel global argument inference model to explore specific relationships, such as Coreference, Sequence and Parallel, among relevant event mentions to recover those intersentence arguments in the sentence, discourse and document layers which represent the cohesion of an event or a topic. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our global argument inference model over a state-of-the-art baseline."
D13-1099,Tree Kernel-based Negation and Speculation Scope Detection with Structured Syntactic Parse Features,2013,21,23,3,1,6517,bowei zou,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Scope detection is a key task in information extraction. This paper proposes a new approach for tree kernel-based scope detection by using the structured syntactic parse information. In addition, we have explored the way of selecting compatible features for different part-of-speech cues. Experiments on the BioScope corpus show that both constituent and dependency structured syntactic parse features have the advantage in capturing the potential relationships between cues and their scopes. Compared with the state of the art scope detection systems, our system achieves substantial improvement."
D12-1092,Employing Compositional Semantics and Discourse Consistency in {C}hinese Event Extraction,2012,25,28,3,1,9023,peifeng li,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Current Chinese event extraction systems suffer much from two problems in trigger identification: unknown triggers and word segmentation errors to known triggers. To resolve these problems, this paper proposes two novel inference mechanisms to explore special characteristics in Chinese via compositional semantics inside Chinese triggers and discourse consistency between Chinese trigger mentions. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline."
C12-2130,A Unified Framework for Discourse Argument Identification via Shallow Semantic Parsing,2012,29,6,2,0,22049,fan xu,Proceedings of {COLING} 2012: Posters,0,"This paper deals with Discourse Argument Identification (DAI) from both intra-sentence and inter-sentence perspectives. For intra-sentence cases, we approach it via a simplified shallow semantic parsing framework, which recasts the discourse connective as the predicate and its scope into several constituents as the argument of the predicate. Different from state-of-the-art chunking approaches, our parsing approach extends DAI from the chunking level to the parse tree level, where rich syntactic information is available, and focuses on determining whether a constituent, rather than a token, is an argument or not. For inter-sentence cases, we present a lightweight heuristic rule-based solution. Evaluation using Penn Discourse Treebank (PDTB) shows that the current researchxe2x80x99s parsing approach significantly outperforms the state-of-the-art chunking alternatives."
C12-1100,Joint Modeling of Trigger Identification and Event Type Determination in {C}hinese Event Extraction,2012,31,8,2,1,9023,peifeng li,Proceedings of {COLING} 2012,0,"Currently, Chinese event extraction systems suffer much from the low quality of annotated event corpora and the high ratio of pseudo trigger mentions to true ones. To resolve these two issues, this paper proposes a joint model of trigger identification and event type determination. Besides, several trigger filtering schemas are introduced to filter out those pseudo trigger mentions as many as possible. Evaluation on the ACE 2005 Chinese corpus justifies the effectiveness of our approach over a strong baseline."
C12-1139,Bilingual Lexicon Construction from Comparable Corpora via Dependency Mapping,2012,25,6,4,0.9965,27118,longhua qian,Proceedings of {COLING} 2012,0,"Bilingual lexicon construction (BLC) from comparable corpora is based on the idea that bilingual similar words tend to occur in similar contexts, usually of words. This, however, introduces noise and leads to low performance. This paper proposes a bilingual dependency mapping model for BLC which encodes a wordxe2x80x99s context as a combination of its dependent words and their relationships. This combination can provide more reliable clues than mere context words for bilingual translation words. We further demonstrate that this kind of bilingual dependency mappings can be successfully generated and maximally exploited without human intervention. The experiments on BLC from English to Chinese show that, by mapping context words and their dependency relationships simultaneously when calculating the similarity between bilingual words, our approach significantly outperforms a state-of-the-art one by ~14 units in accuracy for frequently occurring noun pairs and similarly, though in a less degree, for nouns and verbs in a wide frequency range. This justifies the effectiveness of our dependency mapping model for BLC. TITLE AND ABSTRACT IN ANOTHER LANGUAGE, CHINESE xe5xbax94xe7x94xa8xe4xbex9dxe5xadx98xe6x98xa0xe5xb0x84xe4xbbx8exe5x8fxafxe6xafx94xe8xbex83xe8xafxadxe6x96x99xe5xbax93xe4xb8xadxe6x8axbdxe5x8fx96xe5x8fx8cxe8xafxadxe8xafx8dxe8xa1xa8 xe4xbbx8exe5x8fxafxe6xafx94xe8xbex83xe8xafxadxe6x96x99xe5xbax93xe4xb8xadxe6x8axbdxe5x8fx96xe5x8fx8cxe8xafxadxe8xafx8dxe8xa1xa8xe7x9ax84xe5x9fxbaxe6x9cxacxe6x80x9dxe6x83xb3xe6x98xaf,xe5x8fx8cxe8xafxadxe7x9bxb8xe4xbcxbcxe7x9ax84xe8xafx8dxe8xafxadxe5x87xbaxe7x8exb0xe5x9cxa8xe7x9bxb8xe5x90x8cxe7x9ax84xe8xafxadxe8xafx8dxe4xb8x8axe4xb8x8bxe6x96x87 xe4xb8xadxe3x80x82xe4xb8x8dxe8xbfx87,xe8xbfx99xe7xa7x8dxe6x96xb9xe6xb3x95xe5xbcx95xe5x85xa5xe4xbax86xe5x99xaaxe5xa3xb0,xe4xbbx8exe8x80x8cxe5xafxbcxe8x87xb4xe4xbax86xe4xbdx8exe7x9ax84xe6x8axbdxe5x8fx96xe6x80xa7xe8x83xbdxe3x80x82xe6x9cxacxe6x96x87xe6x8fx90xe5x87xbaxe4xbax86xe4xb8x80xe7xa7x8dxe7x94xa8xe4xbax8exe5x8fx8cxe8xafxadxe8xafx8d xe8xa1xa8xe6x8axbdxe5x8fx96xe7x9ax84xe5x8fx8cxe8xafxadxe4xbex9dxe5xadx98xe6x98xa0xe5xb0x84xe6xa8xa1xe5x9ex8b,xe5x9cxa8xe8xafxa5xe6xa8xa1xe5x9ex8bxe4xb8xadxe4xb8x80xe4xb8xaaxe8xafx8dxe8xafxadxe7x9ax84xe4xb8x8axe4xb8x8bxe6x96x87xe7xbbx93xe5x90x88xe4xbax86xe4xbex9dxe5xadx98xe8xafx8dxe8xafxadxe5x8fx8axe5x85xb6xe4xbex9dxe5xadx98xe5x85xb3 xe7xb3xbbxe3x80x82xe8xbfx99xe7xa7x8dxe7xbbx93xe5x90x88xe6x96xb9xe6xb3x95xe4xb8xbaxe5x8fx8cxe8xafxadxe8xafx8dxe8xa1xa8xe6x9ex84xe5xbbxbaxe6x8fx90xe4xbex9bxe4xbax86xe6xafx94xe5x8dx95xe4xb8x80xe7x9ax84xe8xafx8dxe8xafxadxe4xb8x8axe4xb8x8bxe6x96x87xe6x9bxb4xe4xb8xbaxe5x8fxafxe9x9dxa0xe7x9ax84xe4xbfxa1xe6x81xafxe3x80x82xe6x88x91xe4xbbxacxe8xbfx98xe8xbfx9b xe4xb8x80xe6xadxa5xe5xb1x95xe7xa4xbaxe4xbax86xe5x9cxa8xe6xb2xa1xe6x9cx89xe4xbaxbaxe5xb7xa5xe5xb9xb2xe9xa2x84xe7x9ax84xe6x83x85xe5x86xb5xe4xb8x8bxe5x8fxafxe4xbbxa5xe4xbaxa7xe7x94x9fxe5x92x8cxe5x88xa9xe7x94xa8xe8xbfx99xe7xa7x8dxe5x8fx8cxe8xafxadxe4xbex9dxe5xadx98xe5x85xb3xe7xb3xbbxe3x80x82xe4xbbx8exe8x8bxb1xe6x96x87xe5x88xb0xe4xb8xadxe6x96x87xe7x9ax84 xe5x8fx8cxe8xafxadxe8xafx8dxe8xa1xa8xe6x9ex84xe5xbbxbaxe5xaex9exe9xaax8cxe8xa1xa8xe6x98x8e,xe9x80x9axe8xbfx87xe5x9cxa8xe8xaexa1xe7xaex97xe5x8fx8cxe8xafxadxe8xafx8dxe8xafxadxe7x9bxb8xe4xbcxbcxe5xbaxa6xe6x97xb6xe5x90x8cxe6x97xb6xe6x98xa0xe5xb0x84xe8xafx8dxe8xafxadxe5x8fx8axe5x85xb6xe4xbex9dxe5xadx98xe5x85xb3xe7xb3xbb,xe5x90x8cxe7x9bxae xe5x89x8dxe6x80xa7xe8x83xbdxe6x9cx80xe5xa5xbdxe7x9ax84xe7xb3xbbxe7xbbx9fxe7x9bxb8xe6xafx94,xe6x88x91xe4xbbxacxe7x9ax84xe6x96xb9xe6xb3x95xe6x98xbexe8x91x97xe6x8fx90xe9xabx98xe4xbax86xe7xb2xbexe5xbaxa6xe3x80x82xe5xafxb9xe4xbax8exe7xbbx8fxe5xb8xb8xe5x87xbaxe7x8exb0xe7x9ax84xe5x90x8dxe8xafx8d,xe7xb2xbexe5xbaxa6xe6x8fx90xe9xabx98xe4xbax86 14xe4xb8xaaxe7x99xbexe5x88x86xe7x82xb9;xe5xafxb9xe4xbax8exe8xbex83xe5xa4xa7xe9xa2x91xe7x8ex87xe8x8cx83xe5x9bxb4xe5x86x85xe7x9ax84xe5x90x8dxe8xafx8dxe5x92x8cxe5x8axa8xe8xafx8d,xe6x80xa7xe8x83xbdxe4xb9x9fxe6x8fx90xe9xabx98xe4xbax86,xe5xb0xbdxe7xaexa1xe7xa8x8bxe5xbaxa6xe8xbex83xe5xb0x8fxe3x80x82xe8xbfx99xe8xafxb4xe6x98x8e xe4xbax86xe4xbex9dxe5xadx98xe6x98xa0xe5xb0x84xe6xa8xa1xe5x9ex8bxe5xafxb9xe5x8fx8cxe8xafxadxe8xafx8dxe8xa1xa8xe6x9ex84xe5xbbxbaxe7x9ax84xe6x9cx89xe6x95x88xe6x80xa7xe3x80x82"
P11-1113,Using Cross-Entity Inference to Improve Event Extraction,2011,11,89,6,1,6661,yu hong,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"Event extraction is the task of detecting certain specified types of events that are mentioned in the source language data. The state-of-the-art research on the task is transductive inference (e.g. cross-event inference). In this paper, we propose a new method of event extraction by well using cross-entity inference. In contrast to previous inference methods, we regard entity-type consistency as key feature to predict event mentions. We adopt this inference method to improve the traditional sentence-level event extraction system. Experiments show that we can get 8.6% gain in trigger (event) identification, and more than 11.8% gain for argument (role) classification in ACE event extraction."
I11-1118,Using Context Inference to Improve Sentence Ordering for Multi-document Summarization,2011,9,6,3,1,9023,peifeng li,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"In this paper, we propose a novel context inference-based approach for sentences ordering in mult i-document summarization application. Our method first detects whether or not two summarizat ion sentences should be adjacent according to the similarity between one summarizat ion sentence and the context of the other one, and then it co mputes the reliability of the adjacent summarization sentences based on the similarity and their relative position. To be specific, the first sentence will be selected according to features of sentence, and the second sentence will be selected if and only if it has the maximu m reliability with previous sentence. Evaluation result shows that our method outperforms the state-of-the-art ones on DUC2004 corpus."
W10-4158,Jumping Distance based {C}hinese Person Name Disambiguation,2010,-1,-1,5,1,6661,yu hong,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
D10-1070,A Unified Framework for Scope Learning via Simplified Shallow Semantic Parsing,2010,24,21,1,1,6662,qiaoming zhu,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"This paper approaches the scope learning problem via simplified shallow semantic parsing. This is done by regarding the cue as the predicate and mapping its scope into several constituents as the arguments of the cue. Evaluation on the BioScope corpus shows that the structural information plays a critical role in capturing the relationship between a cue and its dominated arguments. It also shows that our parsing approach significantly outperforms the state-of-the-art chunking ones. Although our parsing approach is only evaluated on negation and speculation scope learning here, it is portable to other kinds of scope learning."
C10-2034,A Novel Method for Bilingual Web Page Acquisition from Search Engine Web Records,2010,21,3,5,0,46440,yanhui feng,Coling 2010: Posters,0,"A new approach has been developed for acquiring bilingual web pages from the result pages of search engines, which is composed of two challenging tasks. The first task is to detect web records embedded in the result pages automatically via a clustering method of a sample page. Identifying these useful records through the clustering method allows the generation of highly effective features for the next task which is high-quality bilingual web page acquisition. The task of high-quality bilingual web page acquisition is a classification problem. One advantage of our approach is that it is search engine and domain independent. The test is based on 2516 records extracted from six search engines automatically and annotated manually, which gets a high precision of 81.3% and a recall of 94.93%. The experimental results indicate that our approach is very effective."
C10-2050,Negative Feedback: The Forsaken Nature Available for Re-ranking,2010,10,3,5,1,6661,yu hong,Coling 2010: Posters,0,"Re-ranking for Information Retrieval aims to elevate relevant feedbacks and depress negative ones in initial retrieval result list. Compared to relevance feedback-based re-ranking method widely adopted in the literature, this paper proposes a new method to well use three features in known negative feedbacks to identify and depress unknown negative feedbacks. The features include: 1) the minor (lower-weighted) terms in negative feedbacks; 2) hierarchical distance (HD) among feedbacks in a hierarchical clustering tree; 3) obstinateness strength of negative feedbacks. We evaluate the method on the TDT4 corpus, which is made up of news topics and their relevant stories. And experimental results show that our new scheme substantially outperforms its counterparts."
C10-1068,Dependency-driven Anaphoricity Determination for Coreference Resolution,2010,29,21,4,1,6709,fang kong,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"This paper proposes a dependency-driven scheme to dynamically determine the syntactic parse tree structure for tree kernel-based anaphoricity determination in coreference resolution. Given a full syntactic parse tree, it keeps the nodes and the paths related with current mention based on constituent dependencies from both syntactic and semantic perspectives, while removing the noisy information, eventually leading to a dependency-driven dynamic syntactic parse tree (D-DSPT). Evaluation on the ACE 2003 corpus shows that the D-DSPT outperforms all previous parse tree structures on anaphoricity determination, and that applying our anaphoricity determination module in coreference resolution achieves the so far best performance."
C10-1076,Learning the Scope of Negation via Shallow Semantic Parsing,2010,19,25,4,1,9182,junhui li,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"In this paper we present a simplified shallow semantic parsing approach to learning the scope of negation (SoN). This is done by formulating it as a shallow semantic parsing problem with the negation signal as the predicate and the negation scope as its arguments. Our parsing approach to SoN learning differs from the state-of-the-art chunking ones in two aspects. First, we extend SoN learning from the chunking level to the parse tree level, where structured syntactic information is available. Second, we focus on determining whether a constituent, rather than a word, is negated or not, via a simplified shallow semantic parsing framework. Evaluation on the BioScope corpus shows that structured syntactic information is effective in capturing the domination relationship between a negation signal and its dominated arguments. It also shows that our parsing approach much outperforms the state-of-the-art chunking ones."
D09-1103,Employing the Centering Theory in Pronoun Resolution from the Semantic Perspective,2009,32,17,3,1,6709,fang kong,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we employ the centering theory in pronoun resolution from the semantic perspective. First, diverse semantic role features with regard to different predicates in a sentence are explored. Moreover, given a pronominal anaphor, its relative ranking among all the pronouns in a sentence, according to relevant semantic role information and its surface position, is incorporated. In particular, the use of both the semantic role features and the relative pronominal ranking feature in pronoun resolution is guided by extending the centering theory from the grammatical level to the semantic level in tracking the local discourse focus. Finally, detailed pronominal subcategory features are incorporated to enhance the discriminative power of both the semantic role features and the relative pronominal ranking feature. Experimental results on the ACE 2003 corpus show that the centering-motivated features contribute much to pronoun resolution."
D09-1133,Improving Nominal {SRL} in {C}hinese Language with Verbal {SRL} Information and Automatic Predicate Recognition,2009,23,27,4,1,9182,junhui li,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"This paper explores Chinese semantic role labeling (SRL) for nominal predicates. Besides those widely used features in verbal SRL, various nominal SRL-specific features are first included. Then, we improve the performance of nominal SRL by integrating useful features derived from a state-of-the-art verbal SRL system. Finally, we address the issue of automatic predicate recognition, which is essential for a nominal SRL system. Evaluation on Chinese NomBank shows that our research in integrating various features derived from verbal SRL significantly improves the performance. It also shows that our nominal SRL system much outperforms the state-of-the-art ones."
D09-1149,Semi-Supervised Learning for Semantic Relation Classification using Stratified Sampling Strategy,2009,20,10,4,1,27118,longhua qian,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a new approach to selecting the initial seed set using stratified sampling strategy in bootstrapping-based semi-supervised learning for semantic relation classification. First, the training data is partitioned into several strata according to relation types/subtypes, then relation instances are randomly sampled from each stratum to form the initial seed set. We also investigate different augmentation strategies in iteratively adding reliable instances to the labeled set, and find that the bootstrapping procedure may stop at a reasonable point to significantly decrease the training time without degrading too much in performance. Experiments on the ACE RDC 2003 and 2004 corpora show the stratified sampling strategy contributes more than the bootstrapping procedure itself. This suggests that a proper sampling strategy is critical in semi-supervised learning."
W08-2137,Dependency Tree-based {SRL} with Proper Pruning and Extensive Feature Engineering,2008,10,1,4,1,35757,hongling wang,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"This paper proposes a dependency tree-based SRL system with proper pruning and extensive feature engineering. Official evaluation on the CoNLL 2008 shared task shows that our system achieves 76.19 in labeled macro F1 for the overall task, 84.56 in labeled attachment score for syntactic dependencies, and 67.12 in labeled F1 for semantic dependencies on combined test set, using the standalone MaltParser. Besides, this paper also presents our unofficial system by 1) applying a new effective pruning algorithm; 2) including additional features; and 3) adopting a better dependency parser, MSTParser. Unofficial evaluation on the shared task shows that our system achieves 82.53 in labeled macro F1, 86.39 in labeled attachment score, and 78.64 in labeled F1, using MSTParser on combined test set. This suggests that proper pruning and extensive feature engineering contributes much in dependency tree-based SRL."
I08-1004,Context-Sensitive Convolution Tree Kernel for Pronoun Resolution,2008,20,17,3,0.333323,6702,guodong zhou,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"This paper proposes a context-sensitive convolution tree kernel for pronoun resolution. It resolves two critical problems in previous researches in two ways. First, given a parse tree and a pair of an anaphor and an antecedent candidate, it implements a dynamic-expansion scheme to automatically determine a proper tree span for pronoun resolution by taking predicateand antecedent competitor-related information into consideration. Second, it applies a context-sensitive convolution tree kernel, which enumerates both context-free and context-sensitive sub-trees by considering their ancestor node paths as their contexts. Evaluation on the ACE 2003 corpus shows that our dynamic-expansion tree span scheme can well cover necessary structured information in the parse tree for pronoun resolution and the context-sensitive tree kernel much outperforms previous tree kernels."
I08-1005,Semi-Supervised Learning for Relation Extraction,2008,22,11,4,0.333323,6702,guodong zhou,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"This paper proposes a semi-supervised learning method for relation extraction. Given a small amount of labeled data and a large amount of unlabeled data, it first bootstraps a moderate number of weighted support vectors via SVM through a co-training procedure with random feature projection and then applies a label propagation (LP) algorithm via the bootstrapped support vectors. Evaluation on the ACE RDC 2003 corpus shows that our method outperforms the normal LP algorithm via all the available labeled data without SVM bootstrapping. Moreover, our method can largely reduce the computational burden. This suggests that our proposed method can integrate the advantages of both SVM bootstrapping and label propagation."
C08-1088,Exploiting Constituent Dependencies for Tree Kernel-Based Semantic Relation Extraction,2008,13,91,4,1,27118,longhua qian,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper proposes a new approach to dynamically determine the tree span for tree kernel-based semantic relation extraction. It exploits constituent dependencies to keep the nodes and their head children along the path connecting the two entities, while removing the noisy information from the syntactic parse tree, eventually leading to a dynamic syntactic parse tree. This paper also explores entity features and their combined features in a unified parse and semantic tree, which integrates both structured syntactic parse information and entity-related semantic information. Evaluation on the ACE RDC 2004 corpus shows that our dynamic syntactic parse tree outperforms all previous tree spans, and the composite kernel combining this tree kernel with a linear state-of-the-art feature-based kernel, achieves the so far best performance."
D07-1076,Tree Kernel-Based Relation Extraction with Context-Sensitive Structured Parse Tree Information,2007,11,161,4,0.333323,6702,guodong zhou,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"This paper proposes a tree kernel with contextsensitive structured parse tree information for relation extraction. It resolves two critical problems in previous tree kernels for relation extraction in two ways. First, it automatically determines a d ynamic context-sensitive tree span for relation extraction by extending the widely -used Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT. Second, it pr oposes a context -sensitive convolution tree kernel, which enumerates both context-free and contextsensitive sub-trees by consid ering their ancestor node paths as their contexts. Moreover, this paper evaluates the complementary nature between our tree kernel and a state -of-the-art linear kernel. Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state-of-the-art Collins and Duffyxe2x80x99s convolution tree kernel. It also shows that our tree kernel achieves much better performance than the state-of-the-art linear kernels . Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features."
Y06-1055,A Visualization method for machine translation evaluation results,2006,6,0,3,0,12358,jianmin yao,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"To make it easier to understand the machine translation evaluation results, a curve is utilized to stand for the performance of a machine translation system. The position of the curve in the graph depicts the quality of the system. The upper left curve stands for higher translation quality. System clustering is made and its dendrogram illustrates the quality difference between systems. These two methods visualize the machine translation evaluation results."
O06-4006,Performance Analysis and Visualization of Machine Translation Evaluation,2006,9,0,4,0,12358,jianmin yao,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 11, Number 3, September 2006: Special Issue on Selected Papers from {ROCLING} {XVII}",0,"Automatic translation evaluation is popular in development of MT systems, but further research is necessary for better evaluation methods and selection of an appropriate evaluation suite. This paper is an attempt for an in-depth analysis of the performance of MT evaluation methods. Difficulty, discriminability and reliability characteristics are proposed and tested in experiments. Visualization of the evaluation scores, which is more intuitional, is proposed to see the translation quality and is shown as a natural way to assemble different evaluation methods."
