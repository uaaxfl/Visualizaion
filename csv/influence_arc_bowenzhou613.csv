2009.iwslt-papers.2,W06-3123,0,0.227225,"Missing"
2009.iwslt-papers.2,W07-0403,0,0.0643152,"search space of this method is huge, so Birch et - 136 - Proceedings of IWSLT 2009, Tokyo - Japan linking sub-translations generated chunk-by-chunk: X → hX1 X2 , X1 X2 i. (5) X is also the start symbol. All rules in R are paired with statistical parameters (i.e., weighted SCFG), which combines with other features to form the models using a log-linear framework. The decoder tries to maximize: P (D) ∝ PLM (e)λLM × Q Q i Figure 1: Enriching SCFG rules from bilingual chart parsing. al. [5] reduce it by using only concepts that match the highconfidence GIZA++ alignments. Similarly, Cherry and Lin [6] use ITG for pruning. May and Knight [7] use EM algorithm to train tree-tostring rule probabilities, and use the Viterbi derivations to re-align the training data. Huang and Zhou [8] use EM to estimate conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Contextfree Grammar. We further improve their approach by using additional rules in the bilingual parsing and EM training. Galley et al. [9] define minimal rules for tree-to-string translation, and (similarly to our rule arithmetic) merge them into composed rules. The EM is used to estimate rule weights. While in their method, wo"
2009.iwslt-papers.2,D07-1038,0,0.0916274,"Birch et - 136 - Proceedings of IWSLT 2009, Tokyo - Japan linking sub-translations generated chunk-by-chunk: X → hX1 X2 , X1 X2 i. (5) X is also the start symbol. All rules in R are paired with statistical parameters (i.e., weighted SCFG), which combines with other features to form the models using a log-linear framework. The decoder tries to maximize: P (D) ∝ PLM (e)λLM × Q Q i Figure 1: Enriching SCFG rules from bilingual chart parsing. al. [5] reduce it by using only concepts that match the highconfidence GIZA++ alignments. Similarly, Cherry and Lin [6] use ITG for pruning. May and Knight [7] use EM algorithm to train tree-tostring rule probabilities, and use the Viterbi derivations to re-align the training data. Huang and Zhou [8] use EM to estimate conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Contextfree Grammar. We further improve their approach by using additional rules in the bilingual parsing and EM training. Galley et al. [9] define minimal rules for tree-to-string translation, and (similarly to our rule arithmetic) merge them into composed rules. The EM is used to estimate rule weights. While in their method, word alignments are used to define all rul"
2009.iwslt-papers.2,P06-1121,0,0.107956,"e)λLM × Q Q i Figure 1: Enriching SCFG rules from bilingual chart parsing. al. [5] reduce it by using only concepts that match the highconfidence GIZA++ alignments. Similarly, Cherry and Lin [6] use ITG for pruning. May and Knight [7] use EM algorithm to train tree-tostring rule probabilities, and use the Viterbi derivations to re-align the training data. Huang and Zhou [8] use EM to estimate conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Contextfree Grammar. We further improve their approach by using additional rules in the bilingual parsing and EM training. Galley et al. [9] define minimal rules for tree-to-string translation, and (similarly to our rule arithmetic) merge them into composed rules. The EM is used to estimate rule weights. While in their method, word alignments are used to define all rules, our method proposes new rules independently of word alignments. 1.2. Formally syntax-based models Our baseline model follows the Chiang’s hierarchical model [2, 10, 11] based on Synchronous Context-free Grammar. The rules have form X → hγ, α, ∼i, (4) φi (X →&lt; γ, α &gt;)λi , (6) where the set of φi (X →&lt; γ, α &gt;) are features defined over given production rule, and PL"
2009.iwslt-papers.2,P05-1033,0,0.397447,"conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Contextfree Grammar. We further improve their approach by using additional rules in the bilingual parsing and EM training. Galley et al. [9] define minimal rules for tree-to-string translation, and (similarly to our rule arithmetic) merge them into composed rules. The EM is used to estimate rule weights. While in their method, word alignments are used to define all rules, our method proposes new rules independently of word alignments. 1.2. Formally syntax-based models Our baseline model follows the Chiang’s hierarchical model [2, 10, 11] based on Synchronous Context-free Grammar. The rules have form X → hγ, α, ∼i, (4) φi (X →&lt; γ, α &gt;)λi , (6) where the set of φi (X →&lt; γ, α &gt;) are features defined over given production rule, and PLM (e) is the language model score on hypothesized output, the λi is the feature weight. The baseline model follows Chiang’s hierarchical model [2]: conditional probabilities P (γ|α) and P (α|γ); lexical weights [13] Pw (γ|α) and Pw (α|γ); word counts |e|; rule counts |D|; abstraction penalty (to account for the accumulated number of non-terminals in D); target n-gram language model PLM (e); and the g"
2009.iwslt-papers.2,W08-0403,1,0.916574,"conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Contextfree Grammar. We further improve their approach by using additional rules in the bilingual parsing and EM training. Galley et al. [9] define minimal rules for tree-to-string translation, and (similarly to our rule arithmetic) merge them into composed rules. The EM is used to estimate rule weights. While in their method, word alignments are used to define all rules, our method proposes new rules independently of word alignments. 1.2. Formally syntax-based models Our baseline model follows the Chiang’s hierarchical model [2, 10, 11] based on Synchronous Context-free Grammar. The rules have form X → hγ, α, ∼i, (4) φi (X →&lt; γ, α &gt;)λi , (6) where the set of φi (X →&lt; γ, α &gt;) are features defined over given production rule, and PLM (e) is the language model score on hypothesized output, the λi is the feature weight. The baseline model follows Chiang’s hierarchical model [2]: conditional probabilities P (γ|α) and P (α|γ); lexical weights [13] Pw (γ|α) and Pw (α|γ); word counts |e|; rule counts |D|; abstraction penalty (to account for the accumulated number of non-terminals in D); target n-gram language model PLM (e); and the g"
2009.iwslt-papers.2,P00-1056,0,0.217366,"r, γ and α are source and target strings with both terminals and nonterminals, subject to the constraint that there is always a oneto-one correspondence ∼ between those non-terminals. The ∼ is often represented by co-indexing corresponding nonterminals. Rules with terminals only are called phrasal rules, while rules with non-terminals are abstract rules. We limit the number of non-terminals in each rule to no more than two, thus ensuring the rank of SCFG is two. The set of rules, denoted as R, are automatically extracted from a parallel corpus [2, 10] with word-alignments obtained from GIZA++ [12]. Finally, an implicit glue rule is embedded with decoder to allow for translations that can be achieved by sequentially X→&lt;γ,α&gt;∈D 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. - 137 - RSpans := precompute(R, e, f ) for i, j, k, l in bottom-up order, such that 1 ≤ i ≤ j ≤ M, 1≤k≤l≤N for ρ ∈ RSpans(i, j, k, l) switch ρ.n case 0: tijkl .push(ρ) case 1: if (f illed(tρ.bp1 )) tijkl .push(ρ) case 2: if (f illed(tρ.bp1 )&f illed(tρ.bp2 )) tijkl .push(ρ) Figure 2: Bilingual chart parser for SCFG. Proceedings of IWSLT 2009, Tokyo - Japan The chart T is a set of cells tijkl such that 1 ≤ i ≤ j ≤ M and 1 ≤ k"
2009.iwslt-papers.2,N03-1017,0,0.0603151,"ed to define all rules, our method proposes new rules independently of word alignments. 1.2. Formally syntax-based models Our baseline model follows the Chiang’s hierarchical model [2, 10, 11] based on Synchronous Context-free Grammar. The rules have form X → hγ, α, ∼i, (4) φi (X →&lt; γ, α &gt;)λi , (6) where the set of φi (X →&lt; γ, α &gt;) are features defined over given production rule, and PLM (e) is the language model score on hypothesized output, the λi is the feature weight. The baseline model follows Chiang’s hierarchical model [2]: conditional probabilities P (γ|α) and P (α|γ); lexical weights [13] Pw (γ|α) and Pw (α|γ); word counts |e|; rule counts |D|; abstraction penalty (to account for the accumulated number of non-terminals in D); target n-gram language model PLM (e); and the glue rule penalty to learn preference of non-terminal rewriting over serial combination through Eq. (5). We note, however, that these parameters are often poorly estimated due to the scarceness of data and the usage of inaccurate heuristics. We try to alleviate this problem by EM training in Section 3. 2. Bilingual chart parsing In this section, we describe the algorithm for bilingual parsing and present our i"
2009.iwslt-papers.2,2001.mtsummit-papers.68,0,0.0430334,"by EM, and uses rule arithmetic to propose rules that better explain the training data. The modular architecture allows for different training setups. In the first setup (EMcosts), only rule probabilities/costs of abstract rules were estimated by 10 iterations of EM. In the second setup (EMpropose), the new rules were proposed after each iteration of EM. In the third setup (EM-propose&costs), the proposed rules were merged with the baseline and ITG rules and the rule costs were estimated by EM. In the following, we are presenting more details about the three setups and present the BLEU scores [14] in the Table 2. 6.3. Baseline The baseline in our experiments is a formal syntax-based translation model [11]. We train GIZA++ [15] word alignments on the sentence-aligned data, and extract phrase pairs using heuristics grow-diag-final [16]. The phrases were up to 6 and 8 words long on the source and target sides, respectively. The method of extracting abstract rules is similar to [2]. The log-linear model combines 9 features, as described in Section 1.2. 6.4. Using BCP and EM to estimate rule costs In the first experiment, we were trying to better estimate features of the baseline abstract r"
2009.iwslt-papers.2,2005.mtsummit-papers.11,0,0.0615178,"istics usually apply constraints, such as limitations of the phrase length or non-terminal span, sometimes too restrictive to extract some good rules. Another reason is the deterministic nature of those heuristics that does not allow to recover from errors in the word alignment. In this work, we learn rules for hierarchical phrase based MT systems directly from the parallel data. The main contribution of this paper is a new method for proposing translation rules which is independent of bilingual word alignments. Let us have an example of a German-English sentence pair from the Europarl corpus [1]. (1) make) are swapped. It would be nice to generate rules that can handle long distance reorderings, still with a reasonably low number of terminals, for example: X → hbesteht darin, is i X → hX1 zu X2 , to X2 X1 i to get the rule (2). Our approach, as shown in Figure 1, consists of bilingual chart parsing (BCP) of the training data, combining rules found in the chart using a rule arithmetic to propose new rules, and using EM to estimate rule probabilities. The paper is structured as follows: In Section 1, we explain our main motivation, summarize previous work, and briefly introduce the for"
2009.iwslt-papers.2,W99-0604,0,0.255074,"t training setups. In the first setup (EMcosts), only rule probabilities/costs of abstract rules were estimated by 10 iterations of EM. In the second setup (EMpropose), the new rules were proposed after each iteration of EM. In the third setup (EM-propose&costs), the proposed rules were merged with the baseline and ITG rules and the rule costs were estimated by EM. In the following, we are presenting more details about the three setups and present the BLEU scores [14] in the Table 2. 6.3. Baseline The baseline in our experiments is a formal syntax-based translation model [11]. We train GIZA++ [15] word alignments on the sentence-aligned data, and extract phrase pairs using heuristics grow-diag-final [16]. The phrases were up to 6 and 8 words long on the source and target sides, respectively. The method of extracting abstract rules is similar to [2]. The log-linear model combines 9 features, as described in Section 1.2. 6.4. Using BCP and EM to estimate rule costs In the first experiment, we were trying to better estimate features of the baseline abstract rules. As discussed in Section 4, to increase the parsability of the corpus we had to provide additional ITG rules. We added all word"
2009.iwslt-papers.2,J07-2003,0,0.279565,"nd for this method, and initial experimental results on German-English translations of Europarl data. 1. Introduction (2) GER: die herausforderung besteht darin diese systeme zu den besten der welt zu machen ENG: the challenge is to make the system the very best We can see that the pairs of long sequences (diese systeme ... der welt, the system ... best) and (zu machen, to X → hbesteht darin X1 zu X2 , is to X2 X1 i, There are 127 sentence pairs out of 300K of the training data that contain this pattern, but this rule was not extracted into the baseline ruleset using the conventional approach [2]: either because of word alignment errors, or because the maximum span for rule extraction is lower than 11 words. We want to learn new rules by combining existing rule usages. Thus we might combine: (3) Statistical machine translation has dramatically improved over the last few decades. Phrase-based and syntax-based systems are probably the most commonly adopted approaches. Although they implement various modeling techniques to improve performance on different languages, domains, or user scenarios, they usually share the same basic pattern for generating rules: starting with word alignments,"
2009.iwslt-papers.2,P07-2045,0,0.00951543,"ed by 10 iterations of EM. In the second setup (EMpropose), the new rules were proposed after each iteration of EM. In the third setup (EM-propose&costs), the proposed rules were merged with the baseline and ITG rules and the rule costs were estimated by EM. In the following, we are presenting more details about the three setups and present the BLEU scores [14] in the Table 2. 6.3. Baseline The baseline in our experiments is a formal syntax-based translation model [11]. We train GIZA++ [15] word alignments on the sentence-aligned data, and extract phrase pairs using heuristics grow-diag-final [16]. The phrases were up to 6 and 8 words long on the source and target sides, respectively. The method of extracting abstract rules is similar to [2]. The log-linear model combines 9 features, as described in Section 1.2. 6.4. Using BCP and EM to estimate rule costs In the first experiment, we were trying to better estimate features of the baseline abstract rules. As discussed in Section 4, to increase the parsability of the corpus we had to provide additional ITG rules. We added all word pairs that had an entry in at least one of the GIZA++ tables of lexical translation probabilities, but were"
2009.iwslt-papers.2,P02-1040,0,\N,Missing
2009.iwslt-papers.2,2006.amta-papers.2,0,\N,Missing
2020.acl-main.125,N16-1012,0,0.325277,"some importance words, such as “obama” and “nominees”. Introduction The explosion of information has expedited the rapid development of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summ"
2020.acl-main.125,D19-1383,0,0.0257256,"atural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) that combines extractive and abstractive techniques with a pointer (Vinyals et al., 2015) enabling the model to copy words from the source text directly. Although, copy mechanism has been widely used in summarization task, how to guarantee that important tokens in the source are copied remains a challenge. In our experiments, we find that the transformer-based summarization model with the copy mechanism may miss some"
2020.acl-main.125,N19-1423,0,0.187679,"sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) that combines extractive and abstractive techniques with a pointer (Vinyals et al., 2015) enabling the model to copy words from the source text directly. Although, copy mechanism has been widely used in summarization task, how to guarantee that important tokens in the source are copied remains a challenge. In our experiments, we find that the transformer-based summarization model with the copy mec"
2020.acl-main.125,D18-1443,0,0.184695,"f-the-art on the public text summarization dataset. 2 (2) where ct is a context vector generated based on the attention distribution (Bahdanau et al., 2015): Pvocab (w) = softmax(Wa st + Va ct ) • We propose a centrality-aware attention and a guidance loss to encourage the model to pay attention to important source words. (1) P (yt ) = pgen Pvocab (yt ) + (1 − pgen )Pcopy (yt ) (7) pgen = sigmoid(waT ct + uTa st + vaT yt−1 ) (8) Related Work Neural network based models (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Nallapati et al., 2017; Zhou et al., 2017; Tan et al., 2017; Gehrmann et al., 2018; Zhu et al., 2019; Li et al., 2020b,a) achieve promising results for the abstractive text summarization. Copy mechanism (Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017; Zhou et al., 2018) enables the summarizers with the ability to copy from the source into the target via pointing (Vinyals et al., 2015). Recently, pre-training based methods (Devlin et al., 2019; Copy distribution Pcopy determines where to attend in time step t. In the most previous work, encoder-decoder attention weight αt is serves as the copy distribution (See et al., 2017): X Pcopy (w) = αt,i (9) i:xi =w The loss"
2020.acl-main.125,P16-1154,0,0.201178,"s, such as “obama” and “nominees”. Introduction The explosion of information has expedited the rapid development of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is"
2020.acl-main.125,P16-1014,1,0.867966,"et al., 2015): Pvocab (w) = softmax(Wa st + Va ct ) • We propose a centrality-aware attention and a guidance loss to encourage the model to pay attention to important source words. (1) P (yt ) = pgen Pvocab (yt ) + (1 − pgen )Pcopy (yt ) (7) pgen = sigmoid(waT ct + uTa st + vaT yt−1 ) (8) Related Work Neural network based models (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Nallapati et al., 2017; Zhou et al., 2017; Tan et al., 2017; Gehrmann et al., 2018; Zhu et al., 2019; Li et al., 2020b,a) achieve promising results for the abstractive text summarization. Copy mechanism (Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017; Zhou et al., 2018) enables the summarizers with the ability to copy from the source into the target via pointing (Vinyals et al., 2015). Recently, pre-training based methods (Devlin et al., 2019; Copy distribution Pcopy determines where to attend in time step t. In the most previous work, encoder-decoder attention weight αt is serves as the copy distribution (See et al., 2017): X Pcopy (w) = αt,i (9) i:xi =w The loss function L is the average negative log likelihood of the ground-truth target word yt for each timestep t: 1 XT L=− logP (yt ) (10) t=0 T 1356"
2020.acl-main.125,C18-1121,1,0.880781,"edited the rapid development of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) that combines extractive and abstracti"
2020.acl-main.125,W04-1013,0,0.0500472,"nn et al., 2018) is a sequence-to-sequence model augmented with a bottom-up content selector. MASS (Song et al., 2019) is a sequence-tosequence pre-trained model based on the Transformer. ABS (Rush et al., 2015) relies on an CNN encoder and a NNLM decoder. ABS+ (Rush et al., 2015) enhances the ABS model with extractive summarization features. SEASS (Zhou et al., 2017) controls the information flow from the encoder to the decoder with the selective encoding strategy. SeqCopyNet (Zhou et al., 2018) extends the copy mechanism that can copy sequences from the source. We adopt ROUGE (RG) F1 score (Lin, 2004) as the evaluation metric. As shown in Table 2 and Table 3, SAGCopy with both outdegree and indegree centrality based guidance significantly outperform the baseline models, which prove the effectiveness of self-attention guided copy mechanism. The basic indegree centrality (indegree-1) is more favorable, considering the ROUGE score and computation complexity. Besides ROUGE evaluation, we further investigate the guidance from the view of the loss function. For each sample in the Gigaword test set, we measure the KL divergence between the centrality score and the copy distribution, and we calcul"
2020.acl-main.125,W04-3252,0,0.103894,"20 Association for Computational Linguistics We propose a Self-Attention Guided Copy mechanism (SAGCopy) that aims to encourage the summarizer to copy important source words. Selfattention layer in the Transformer (Vaswani et al., 2017) builds a directed graph whose vertices represent the source words and edges are defined in terms of the relevance score between each pair of source words by dot-product attention (Vaswani et al., 2017) between the query Q and the key K. We calculate the centrality of each source words based on the adjacency matrices. A straightforward method is using TextRank (Mihalcea and Tarau, 2004) algorithm that assumes a word receiving more relevance score from others are more likely to be important. This measure is known as the indegree centrality. We also adopt another measure assuming that a word sends out more relevance score to others is likely to be more critical, namely outdegree centrality, to calculate the source word centrality. We utilize the centrality score as guidance for copy distribution. Specifically, we extend the dotproduct attention to a centrality-aware function. Furthermore, we introduce an auxiliary loss computed by the divergence between the copy distribution a"
2020.acl-main.125,K16-1028,1,0.938984,"” and “nominees”. Introduction The explosion of information has expedited the rapid development of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Netwo"
2020.acl-main.125,D15-1044,0,0.335781,"-copying switch pgen ∈ [0, 1], the final probability distribution of the ground-truth target word yt is: • We achieve state-of-the-art on the public text summarization dataset. 2 (2) where ct is a context vector generated based on the attention distribution (Bahdanau et al., 2015): Pvocab (w) = softmax(Wa st + Va ct ) • We propose a centrality-aware attention and a guidance loss to encourage the model to pay attention to important source words. (1) P (yt ) = pgen Pvocab (yt ) + (1 − pgen )Pcopy (yt ) (7) pgen = sigmoid(waT ct + uTa st + vaT yt−1 ) (8) Related Work Neural network based models (Rush et al., 2015; Nallapati et al., 2016; Chopra et al., 2016; Nallapati et al., 2017; Zhou et al., 2017; Tan et al., 2017; Gehrmann et al., 2018; Zhu et al., 2019; Li et al., 2020b,a) achieve promising results for the abstractive text summarization. Copy mechanism (Gulcehre et al., 2016; Gu et al., 2016; See et al., 2017; Zhou et al., 2018) enables the summarizers with the ability to copy from the source into the target via pointing (Vinyals et al., 2015). Recently, pre-training based methods (Devlin et al., 2019; Copy distribution Pcopy determines where to attend in time step t. In the most previous work, e"
2020.acl-main.125,P17-1099,0,0.761152,"The explosion of information has expedited the rapid development of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) th"
2020.acl-main.125,P16-1162,0,0.0341165,"encoder-decoder attention. Thus, we adopt a centrality-aware auxiliary loss to encourage the consistency between the overall copy distribution and the word centrality distribution based on the Kullback-Leibler (KL) divergence: L=− 5 5.1 1X 1X logP (yt ) + λKL( αt , score) t t T T (18) Experiments Experimental Setting We evaluate our model in CNN/Daily Mail dataset (Hermann et al., 2015) and Gigaword dataset (Rush et al., 2015). Our experiments are conducted with 4 NVIDIA P40 GPU. We adopt 6 layer encoder and 6 layers decoder with 12 attention heads, and hmodel = 768. Byte Pair Encoding (BPE) (Sennrich et al., 2016) word segmentation is used for data pre-processing. We warm-start the model parameter with MASS pre-trained base model1 and trains about 10 epoches for convergence. During decoding, we use beam search with a beam size of 5. 5.2 Experimental Results We compare our proposed Self-Attention Guided Copy (SAGCopy) model with the following comparative models. Lead-3 uses the first three sentences of the article as its summary. PGNet (See et al., 2017) is the PointerGenerator Network. Bottom-Up (Gehrmann et al., 2018) is a sequence-to-sequence model augmented with a bottom-up content selector. MASS (S"
2020.acl-main.125,P17-1108,0,0.040341,"Missing"
2020.acl-main.125,P19-1499,0,0.0130405,"current neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) that combines extractive and abstractive techniques with a pointer (Vinyals et al., 2015) enabling the model to copy words from the source text directly. Although, copy mechanism has been widely used in summarization task, how to guarantee that important tokens in the source are copied remains a challenge. In our experiments, we find that the transformer-based summarization model with the copy mechanism may miss some important words. As shown in Table 1, words like “nominees” and"
2020.acl-main.125,P19-1628,0,0.0165674,"eman, 1978; Bonacich, 1987; Borgatti and Everett, 2006; Kiss and Bichler, 2008; Li et al., 2011). Degree centrality is one of the simplest centrality measures that can be distinguished as indegree centrality and outdegree centrality (Freeman, 1978), which are determined based on the edges coming into and leaving a node, respectively. Indegree centrality of a word is proportional to the number of relevance scores incoming from other words, which can be measured by the sum of the indegree scores or by graph-based extractive summarization methods (Mihalcea and Tarau, 2004; Erkan and Radev, 2004; Zheng and Lapata, 2019). Outdegree centrality of a word is proportional to the number of relevance scores outgoing to other words, which can be computed by the sum of the outdegree scores. Formally, let G = (V, D) be a directed graph representing self-attention, where vertices V is the word set and edge Di,j is represented by the encoder self-attention P weight from the word xi to the word xj , where i Di,j = 1. Next, we introduce the approaches to calculate the word centrality with the graph G. We first construct a transition probability matrix T as follows: X Ti,j = Di,j / Di,j . (13) j We introduce two approaches"
2020.acl-main.125,P17-1101,0,0.39448,"information has expedited the rapid development of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) that combines extract"
2020.acl-main.125,D19-1302,0,0.0678277,"elopment of text summarization technology, which can help us to grasp the key points from miscellaneous information quickly. There are broadly two types of summarization methods: extractive and abstractive. Extractive approaches select the original text segments in the input to form a summary, while abstractive approaches “create” novel sentences based on natural language generation techniques. In the past few years, recurrent neural networks (RNNs) based architectures (Chopra et al., 2016; Gu et al., 2016; Nallapati et al., 2016, 2017; See et al., 2017; Zhou et al., 2017; Li et al., 2018b,a; Zhu et al., 2019) have obtained state-of-the-art results for text summarization. Benefit from longterm dependency and high scalability, transformerbased networks have shown superiority over RNNs ∗ Equal contribution. on many NLP tasks, including machine translation (Vaswani et al., 2017; Dehghani et al., 2019), sentence classification (Devlin et al., 2019; Cohan et al., 2019), and text summarization (Song et al., 2019; Zhang et al., 2019). One of the most successful frameworks for the summarization task is Pointer-Generator Network (See et al., 2017) that combines extractive and abstractive techniques with a p"
2020.acl-main.241,D19-1522,0,0.0929269,"t al., 2013) is the first and most representative translational distance model. A series of work is conducted along this line such as TransH (Wang et al., 2014), TransR (Lin et al., 2015) and TransD (Ji et al., 2015) etc. RotatE (Sun et al., 2019) further extends the computation into complex domain and is currently the state-of-art in this category. On the other hand, Semantic matching models usually take multiplicative score functions to compute the plausibility of the given triple, such as DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), TuckER (Balazevic et al., 2019) and QuatE (Zhang et al., 2019). ConvKB (Nguyen et al., 2017) and CapsE (Nguyen et al., 2019) further took the triple as a whole, and fed head, relation and tail embeddings into convolutional models or capsule networks. The above knowledge graph embedding methods focused on modeling individual triples. However, they ignored knowledge graph structure and did not take advantage of context from neighbouring nodes and edges. This issue inspired the usage of graph neural networks (Kipf and Welling, 2016; Veliˇckovi´c et al., 2017) for graph context modeling. Encoder-decoder framework was adopted in"
2020.acl-main.241,P19-1431,0,0.133392,"al., 2017) and CapsE (Nguyen et al., 2019) further took the triple as a whole, and fed head, relation and tail embeddings into convolutional models or capsule networks. The above knowledge graph embedding methods focused on modeling individual triples. However, they ignored knowledge graph structure and did not take advantage of context from neighbouring nodes and edges. This issue inspired the usage of graph neural networks (Kipf and Welling, 2016; Veliˇckovi´c et al., 2017) for graph context modeling. Encoder-decoder framework was adopted in (Schlichtkrull et al., 2017; Shang et al., 2019; Bansal et al., 2019). The knowledge graph structure is first encoded via graph neural networks and the output with rich structure information is passed to the following graph embedding model for prediction. The graph model and the scoring model could be end-to-end trained together, or the graph encoder output was only used to initialize the entity embedding (Nathani et al., 2019). We take another approach in this paper: we integrate the graph context directly into the distance scoring function. 2.2 Orthogonal Transform make the training faster and more stable in different tasks. On the other hand, some work has b"
2020.acl-main.241,P15-1067,0,0.409122,"ledge graph embedding could be roughly categorized into two classes (Wang et al., 2017): distance-based models and semantic matching models. Distance-based model is also known as additive models, since it projects head and tail enti2714 ties into the same embedding space and the distance scoring between two entity embeddings is used to measure the plausibility of the given triple. TransE (Bordes et al., 2013) is the first and most representative translational distance model. A series of work is conducted along this line such as TransH (Wang et al., 2014), TransR (Lin et al., 2015) and TransD (Ji et al., 2015) etc. RotatE (Sun et al., 2019) further extends the computation into complex domain and is currently the state-of-art in this category. On the other hand, Semantic matching models usually take multiplicative score functions to compute the plausibility of the given triple, such as DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), TuckER (Balazevic et al., 2019) and QuatE (Zhang et al., 2019). ConvKB (Nguyen et al., 2017) and CapsE (Nguyen et al., 2019) further took the triple as a whole, and fed head, relation and tail embeddings into convolutional m"
2020.acl-main.241,P19-1466,0,0.0266288,"es and edges. This issue inspired the usage of graph neural networks (Kipf and Welling, 2016; Veliˇckovi´c et al., 2017) for graph context modeling. Encoder-decoder framework was adopted in (Schlichtkrull et al., 2017; Shang et al., 2019; Bansal et al., 2019). The knowledge graph structure is first encoded via graph neural networks and the output with rich structure information is passed to the following graph embedding model for prediction. The graph model and the scoring model could be end-to-end trained together, or the graph encoder output was only used to initialize the entity embedding (Nathani et al., 2019). We take another approach in this paper: we integrate the graph context directly into the distance scoring function. 2.2 Orthogonal Transform make the training faster and more stable in different tasks. On the other hand, some work has been done to achieve strict orthogonal during optimization by applying special gradient update scheme. Harandi and Fernando (2016) proposed a Stiefel layer to guarantee fully connected layers to be orthogonal by using Reimannian gradients. Huang et al. (2017) consider the estimation of orthogonal matrix as an optimization over multiple dependent stiefel manifol"
2020.acl-main.241,N19-1226,0,0.058281,"is conducted along this line such as TransH (Wang et al., 2014), TransR (Lin et al., 2015) and TransD (Ji et al., 2015) etc. RotatE (Sun et al., 2019) further extends the computation into complex domain and is currently the state-of-art in this category. On the other hand, Semantic matching models usually take multiplicative score functions to compute the plausibility of the given triple, such as DistMult (Yang et al., 2014), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), TuckER (Balazevic et al., 2019) and QuatE (Zhang et al., 2019). ConvKB (Nguyen et al., 2017) and CapsE (Nguyen et al., 2019) further took the triple as a whole, and fed head, relation and tail embeddings into convolutional models or capsule networks. The above knowledge graph embedding methods focused on modeling individual triples. However, they ignored knowledge graph structure and did not take advantage of context from neighbouring nodes and edges. This issue inspired the usage of graph neural networks (Kipf and Welling, 2016; Veliˇckovi´c et al., 2017) for graph context modeling. Encoder-decoder framework was adopted in (Schlichtkrull et al., 2017; Shang et al., 2019; Bansal et al., 2019). The knowledge graph s"
2020.acl-main.241,W15-4007,0,0.375974,"ive explanation for the success of GC-OTE. Optimization Self-adversarial negative sampling loss (Sun et al., 2019) is used to optimize the embedding in this work, 2717 L = − ∑ p(h , r, t ) log σ(dall (h , r, t ) − γ) ′ ′ − log σ(γ − dall (h, r, t)) ′ ′ (13) where γ is a fixed margin, σ is sigmoid function, ′ ′ ′ ′ (h , r, t ) is negative triple, and p(h , r, t ) is the negative sampling weight defined in (Sun et al., 2019). 4 4.1 Experiments Datasets Two commonly used benchmark datasets (FB15k237 and WN18RR) are employed in this study to evaluate the performance of link prediction. FB15k-237 (Toutanova and Chen, 2015) dataset contains knowledge base relation triples and textual mentions of Freebase entity pairs. The knowledge base triples are a subset of the FB15K (Bordes et al., 2013), originally derived from Freebase. The inverse relations are removed in FB15k-237. WN18RR (Dettmers et al., 2018) is derived from WN18 (Bordes et al., 2013), which is a subset of WordNet. WN18 consists of 18 relations and 40,943 entities. However, many text triples obtained by inverting triples from the training set. Thus WN18RR (Dettmers et al., 2018) is created to ensure that the evaluation dataset does not have test leaka"
2020.coling-main.502,N16-1012,0,0.0675896,"with text that contains the most valuable information about products, which is of practical value to address the problem of information overload. In the e-commerce scenarios, unfaithful product summaries that are inconsistent with the corresponding product attributes, e.g., generating “cotton” for a “silk dress”, mislead the users and decrease public credibility of the e-commerce platform. Thus, the faithfulness is a basic requirement for product summarization. Recently, sequence-to-sequence (seq2seq) methods show promising performances for general text summarization tasks (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Li et al., 2018; Zhang et al., 2018; Li et al., 2020b), which has been adopted to text generation tasks in the field of ecommerce (Khatri et al., 2018; Yang et al., 2018; Daultani et al., 2019; Chen et al., 2019; Li et al., 2020a). Although applicable, they do not attempt to improve the faithfulness for product summarization. In this paper, we aim to produce faithful product summaries with heterogeneous data, i.e., textual product description and product attribute table, as shown in Figure 1. First, as the words in product attribute tables are explicit indicators for the p"
2020.coling-main.502,P16-1154,0,0.0370054,"den state at timestep t, and cxt is the source sequence context vector that is generated by attention (Bahdanau et al., 2015) mechanism as follows: x αt,i = softmax(uTa tanh(Wa hxi + Va st−1 )) X x x cxt = αt,i hi i 5713 (2) (3) x is the attention for i-th word in the source at timestep t. Similarly, we can get the attention where αt,i v . We calculate the attribute attention and attribute context vector as follows: over each attribute word αt,i,j k αt,i = softmax(uTa tanh(Wa X X k hki,j αt,i ckt = i X j hki,j + Va st−1 )) (4) (5) j Our model is based on the pointer-generator network (PGNet) (Gu et al., 2016; See et al., 2017) that predicts words based on the probability distributions of the generator and the pointer (Vinyals et al., 2015). The generator produces vocabulary distribution Pgen over a fixed target vocabulary as follows: Pgen (w) = softmax(Wb st + Vb cxt ) (6) The dual-pointer copy the word w from both the source sequence and attribute table. The copy distribution from the source sequence is obtained by the attention distribution over the source sequence: X x αt,i (7) Pcopy (w) = i:xi =w We adopt a coarse-to-fine attention (Liu et al., 2019) to calculate the final copy distribution f"
2020.coling-main.502,C18-1121,1,0.838642,"e information about products, which is of practical value to address the problem of information overload. In the e-commerce scenarios, unfaithful product summaries that are inconsistent with the corresponding product attributes, e.g., generating “cotton” for a “silk dress”, mislead the users and decrease public credibility of the e-commerce platform. Thus, the faithfulness is a basic requirement for product summarization. Recently, sequence-to-sequence (seq2seq) methods show promising performances for general text summarization tasks (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Li et al., 2018; Zhang et al., 2018; Li et al., 2020b), which has been adopted to text generation tasks in the field of ecommerce (Khatri et al., 2018; Yang et al., 2018; Daultani et al., 2019; Chen et al., 2019; Li et al., 2020a). Although applicable, they do not attempt to improve the faithfulness for product summarization. In this paper, we aim to produce faithful product summaries with heterogeneous data, i.e., textual product description and product attribute table, as shown in Figure 1. First, as the words in product attribute tables are explicit indicators for the product attributes, we propose a dual"
2020.coling-main.502,D15-1044,0,0.0835946,"-commerce platform with text that contains the most valuable information about products, which is of practical value to address the problem of information overload. In the e-commerce scenarios, unfaithful product summaries that are inconsistent with the corresponding product attributes, e.g., generating “cotton” for a “silk dress”, mislead the users and decrease public credibility of the e-commerce platform. Thus, the faithfulness is a basic requirement for product summarization. Recently, sequence-to-sequence (seq2seq) methods show promising performances for general text summarization tasks (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Li et al., 2018; Zhang et al., 2018; Li et al., 2020b), which has been adopted to text generation tasks in the field of ecommerce (Khatri et al., 2018; Yang et al., 2018; Daultani et al., 2019; Chen et al., 2019; Li et al., 2020a). Although applicable, they do not attempt to improve the faithfulness for product summarization. In this paper, we aim to produce faithful product summaries with heterogeneous data, i.e., textual product description and product attribute table, as shown in Figure 1. First, as the words in product attribute tables are explicit"
2020.coling-main.502,P17-1099,0,0.0613729,"step t, and cxt is the source sequence context vector that is generated by attention (Bahdanau et al., 2015) mechanism as follows: x αt,i = softmax(uTa tanh(Wa hxi + Va st−1 )) X x x cxt = αt,i hi i 5713 (2) (3) x is the attention for i-th word in the source at timestep t. Similarly, we can get the attention where αt,i v . We calculate the attribute attention and attribute context vector as follows: over each attribute word αt,i,j k αt,i = softmax(uTa tanh(Wa X X k hki,j αt,i ckt = i X j hki,j + Va st−1 )) (4) (5) j Our model is based on the pointer-generator network (PGNet) (Gu et al., 2016; See et al., 2017) that predicts words based on the probability distributions of the generator and the pointer (Vinyals et al., 2015). The generator produces vocabulary distribution Pgen over a fixed target vocabulary as follows: Pgen (w) = softmax(Wb st + Vb cxt ) (6) The dual-pointer copy the word w from both the source sequence and attribute table. The copy distribution from the source sequence is obtained by the attention distribution over the source sequence: X x αt,i (7) Pcopy (w) = i:xi =w We adopt a coarse-to-fine attention (Liu et al., 2019) to calculate the final copy distribution from attribute word"
2020.coling-main.502,C18-1095,0,0.0213421,"t summaries that are inconsistent with the corresponding product attributes, e.g., generating “cotton” for a “silk dress”, mislead the users and decrease public credibility of the e-commerce platform. Thus, the faithfulness is a basic requirement for product summarization. Recently, sequence-to-sequence (seq2seq) methods show promising performances for general text summarization tasks (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Li et al., 2018; Zhang et al., 2018; Li et al., 2020b), which has been adopted to text generation tasks in the field of ecommerce (Khatri et al., 2018; Yang et al., 2018; Daultani et al., 2019; Chen et al., 2019; Li et al., 2020a). Although applicable, they do not attempt to improve the faithfulness for product summarization. In this paper, we aim to produce faithful product summaries with heterogeneous data, i.e., textual product description and product attribute table, as shown in Figure 1. First, as the words in product attribute tables are explicit indicators for the product attributes, we propose a dual-copy mechanism that can selectively copy the tokens in textual product descriptions and product attribute words into the summaries. Second, for the produ"
2020.coling-main.502,P17-1101,0,0.0583451,"ns the most valuable information about products, which is of practical value to address the problem of information overload. In the e-commerce scenarios, unfaithful product summaries that are inconsistent with the corresponding product attributes, e.g., generating “cotton” for a “silk dress”, mislead the users and decrease public credibility of the e-commerce platform. Thus, the faithfulness is a basic requirement for product summarization. Recently, sequence-to-sequence (seq2seq) methods show promising performances for general text summarization tasks (Rush et al., 2015; Chopra et al., 2016; Zhou et al., 2017; Li et al., 2018; Zhang et al., 2018; Li et al., 2020b), which has been adopted to text generation tasks in the field of ecommerce (Khatri et al., 2018; Yang et al., 2018; Daultani et al., 2019; Chen et al., 2019; Li et al., 2020a). Although applicable, they do not attempt to improve the faithfulness for product summarization. In this paper, we aim to produce faithful product summaries with heterogeneous data, i.e., textual product description and product attribute table, as shown in Figure 1. First, as the words in product attribute tables are explicit indicators for the product attributes,"
2020.coling-main.510,D17-1181,0,0.0649095,"Missing"
2020.coling-main.510,P17-1152,0,0.228905,"urther proposes a hybrid attention scheme which includes an instance-level attention and a feature-level attention, where the former is used to highlight the crucial support sentences in calculating the prototype, and the latter is to select more efficient features when calculating distances. MLMAN Different from the Proto and Proto-HATT, MLMAN encodes each query and the supporting set in an interactive way by considering their matching information on multiple levels. At local level, the representations of an instance and a supporting set are matched following the sentence matching framework (Chen et al., 2017b) and aggregated by max and average pooling. At instance level, the matching degree is first calculated via a multi-layer perception (MLP). Then, taking the matching degrees as weights, the instances in a supporting set are aggregated to obtain the class prototype for final classification. BERT-PAIR This model is based on the sentence classification model in BERT. The sentence to be classified is first paired with all the supporting instances, and then each pair is concatenated to a sequence. BERT takes this sequence as input and returns a relevance score, which is used to measure whether the"
2020.coling-main.510,P19-1012,0,0.0489319,"Missing"
2020.coling-main.510,D19-1649,0,0.305434,"ng the relations with insufficient instances. Therefore, making the RC models capable of identifying relations with few training instances becomes a crucial challenge. Inspired by the success of few-shot learning methods in the computer vision community (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016) and some other natural language processing tasks (Chen et al., 2016; Qin et al., 2020; Zhou et al., 2019), Han et al. (2018) first introduce the few-shot learning to RC task and propose the FewRel dataset. Recently, many works focus on this task and achieve remarkable performance (Gao et al., 2019a; Snell et al., 2017; Ye and Ling, 2019). Previous few-shot relation classifiers perform well on sentences with only one relation of a single entity pair. However, in real natural language, a sentence usually jointly describes multiple relations of different entity pairs. Since these relations usually keep high co-occurrence in the same context, previous few-shot RC models struggle to distinguish them with few annotated instances. For example, Table 1 shows three instances from the FewRel dataset, where each sentence describes multiple relations with corresponding keyphrases highlighted (colo"
2020.coling-main.510,D18-1514,0,0.25064,"on between two specified entities in a sentence. Previous supervised approaches on this task heavily depend on human-annotated data, which limit their performance on classifying the relations with insufficient instances. Therefore, making the RC models capable of identifying relations with few training instances becomes a crucial challenge. Inspired by the success of few-shot learning methods in the computer vision community (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016) and some other natural language processing tasks (Chen et al., 2016; Qin et al., 2020; Zhou et al., 2019), Han et al. (2018) first introduce the few-shot learning to RC task and propose the FewRel dataset. Recently, many works focus on this task and achieve remarkable performance (Gao et al., 2019a; Snell et al., 2017; Ye and Ling, 2019). Previous few-shot relation classifiers perform well on sentences with only one relation of a single entity pair. However, in real natural language, a sentence usually jointly describes multiple relations of different entity pairs. Since these relations usually keep high co-occurrence in the same context, previous few-shot RC models struggle to distinguish them with few annotated i"
2020.coling-main.510,P19-1135,0,0.0302267,"/oPos” CAT and w/o relative position and the syntax position bring significant improvements. In addition, compared with our full model, the performance of “w/o CAT” proves that the CAT help to decouple the confusing relations. 4 Related Work Few-shot Relation Classification Relation classification (RC) aims to identify the semantic relation between two entities in a sentence, which is the basis of many natural language processing task, such as question answering (Yu et al., 2017) and knowledge graph completion (Shang et al., 2019). It has attracted more and more attention over past few years (Jia et al., 2019; Feng et al., 2018; Vinyals et al., 2018; Adel and Sch¨utze, 2017; Yang et al., 2016a). Previous supervised approaches on this task heavily rely on labeled data for training, that limits their ability to classify the relations with insufficient instances. To address this problem, Han et al. (2018) first introduce few-shot learning to RC task, which has been proved effective in the computer vision community and has many applications (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016). Earlier works on few-shot RC are based on the widely used model prototypical network (Snell et al."
2020.coling-main.510,2020.acl-main.512,0,0.0425707,"re-trained LM BERT (Devlin et al., 2018) to few-shot RC, and their work shows that BERT brings significant improvements on classification performance. Furthermore, the approach proposed by Soares et al. (2019) are also based on BERT and achieve the state-of-art result on the few-shot RC task. Syntactic Relation Previous RC models usually use the relative position information to identify which words are the entities in a sentence, e.g., Zeng et al. (2015b). In addition, the syntax information of the sentences is proved useful in many natural language processing tasks (Fale´nska and Kuhn, 2019; Ma et al., 2020; Chen et al., 2017a). Inspired by Yang et al. (2016b), which adopt the dependency parse tree for RC (Ma et al., 2020), we also introduce the dependency relation as another type of position to emphasize the specific entities, and propose a novel application of the syntax positions. 5 Conclusions In this paper, we propose CTEG equipped with two novel mechanisms, namely the Entity-Guided Attention (EGA) and the Confusion-Aware Training (CAT), to address the relation confusion problem in few-shot relation classification (RC). We conduct extensive experiments on benchmark dataset FewRel, and exper"
2020.coling-main.510,P09-1113,0,0.13173,"d attention (EGA) and confusion-aware training (CAT) through the ablation studies in Section 3.4. In order to more intuitively and clearly show the role of EGA and CAT, we show their visualized examples in case study in Section 3.5. Furthermore, we verify that our model is capable of addressing the relation confusion problem to some extent in Section 3.6. 3.1 Implementation Details Dataset The FewRel dataset (Han et al., 2018) contains 100 relations, which are split up into 64 for training, 16 for validation and 20 for testing. Each relation has 700 instances generated by distant supervision (Mintz et al., 2009). All the instances are annotated with a specified entity pair. Settings The dimension of word embedding is set to 768 for consistency with the base model of BERT (Devlin et al., 2018). The max length of the input is set to 100. Following BERT, the layer number M of the transformer encoder with EGA is 12, and all parameters in it is initialized with the pretrained BERT model. The relative position and syntactic relation embedding dimensions are both set to 50, and the transformer encoder for obtaining entity-guided gates is set up with hidden size as 230, head number of self-attention as 2. In"
2020.coling-main.510,P19-1279,0,0.0191409,"on community and has many applications (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016). Earlier works on few-shot RC are based on the widely used model prototypical network (Snell et al., 2017; Ye and Ling, 2019). Recently, the pre-trained language models (LM) has shown significant power in many natural language processing tasks. To this end, Gao et al. (2019c) adopt the most representative pre-trained LM BERT (Devlin et al., 2018) to few-shot RC, and their work shows that BERT brings significant improvements on classification performance. Furthermore, the approach proposed by Soares et al. (2019) are also based on BERT and achieve the state-of-art result on the few-shot RC task. Syntactic Relation Previous RC models usually use the relative position information to identify which words are the entities in a sentence, e.g., Zeng et al. (2015b). In addition, the syntax information of the sentences is proved useful in many natural language processing tasks (Fale´nska and Kuhn, 2019; Ma et al., 2020; Chen et al., 2017a). Inspired by Yang et al. (2016b), which adopt the dependency parse tree for RC (Ma et al., 2020), we also introduce the dependency relation as another type of position to e"
2020.coling-main.510,D18-1250,0,0.0221617,"and the syntax position bring significant improvements. In addition, compared with our full model, the performance of “w/o CAT” proves that the CAT help to decouple the confusing relations. 4 Related Work Few-shot Relation Classification Relation classification (RC) aims to identify the semantic relation between two entities in a sentence, which is the basis of many natural language processing task, such as question answering (Yu et al., 2017) and knowledge graph completion (Shang et al., 2019). It has attracted more and more attention over past few years (Jia et al., 2019; Feng et al., 2018; Vinyals et al., 2018; Adel and Sch¨utze, 2017; Yang et al., 2016a). Previous supervised approaches on this task heavily rely on labeled data for training, that limits their ability to classify the relations with insufficient instances. To address this problem, Han et al. (2018) first introduce few-shot learning to RC task, which has been proved effective in the computer vision community and has many applications (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016). Earlier works on few-shot RC are based on the widely used model prototypical network (Snell et al., 2017; Ye and Ling, 2019). Recently, the"
2020.coling-main.510,D16-1007,0,0.330763,"vements. In addition, compared with our full model, the performance of “w/o CAT” proves that the CAT help to decouple the confusing relations. 4 Related Work Few-shot Relation Classification Relation classification (RC) aims to identify the semantic relation between two entities in a sentence, which is the basis of many natural language processing task, such as question answering (Yu et al., 2017) and knowledge graph completion (Shang et al., 2019). It has attracted more and more attention over past few years (Jia et al., 2019; Feng et al., 2018; Vinyals et al., 2018; Adel and Sch¨utze, 2017; Yang et al., 2016a). Previous supervised approaches on this task heavily rely on labeled data for training, that limits their ability to classify the relations with insufficient instances. To address this problem, Han et al. (2018) first introduce few-shot learning to RC task, which has been proved effective in the computer vision community and has many applications (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016). Earlier works on few-shot RC are based on the widely used model prototypical network (Snell et al., 2017; Ye and Ling, 2019). Recently, the pre-trained language models (LM) has shown"
2020.coling-main.510,P19-1277,0,0.48141,"ances. Therefore, making the RC models capable of identifying relations with few training instances becomes a crucial challenge. Inspired by the success of few-shot learning methods in the computer vision community (Vinyals et al., 2016; Sung et al., 2017; Santoro et al., 2016) and some other natural language processing tasks (Chen et al., 2016; Qin et al., 2020; Zhou et al., 2019), Han et al. (2018) first introduce the few-shot learning to RC task and propose the FewRel dataset. Recently, many works focus on this task and achieve remarkable performance (Gao et al., 2019a; Snell et al., 2017; Ye and Ling, 2019). Previous few-shot relation classifiers perform well on sentences with only one relation of a single entity pair. However, in real natural language, a sentence usually jointly describes multiple relations of different entity pairs. Since these relations usually keep high co-occurrence in the same context, previous few-shot RC models struggle to distinguish them with few annotated instances. For example, Table 1 shows three instances from the FewRel dataset, where each sentence describes multiple relations with corresponding keyphrases highlighted (colored) as evidence. When specified two enti"
2020.coling-main.510,D15-1203,0,0.314328,"inspired by the success of pre-trained language models, our approaches are based on BERT (Devlin et al., 2018), which has been proved effective especially for few-shot learning tasks. Specifically, the backbone of the encoder of our model is a transformer equipped with the proposed EGA which guides the calculation of self-attention distributions by weighting the attention logits with entity-guided gates. The gates are used to measure the relevance between each word and the given two entities. Two types of information for each word are used to calculate its gate. One is the relative position (Zeng et al., 2015a) information, which is the relative distance between a word and an entity in the input sequence. The other is syntactic relation which is proposed in this paper, defined as the dependency relations between each word and the entities. Based on these information, the entity-guided gates in EGA are able to select those important words and control the contribution of each word in self-attention. We also propose CAT to explicitly force the model to asynchronously learn the classification from an instance to its true relation and its confusing relation. After each training step, the CAT first sele"
2020.emnlp-main.166,N18-2118,0,0.0293538,"the textual product description, and we label the values by exactly matching. We denote this subset of the MAE dataset as MAE-text and the rest as MAE-image (values can be only inferred by the images). 4 Experiment We compare our proposed methods with the following baselines: WSM is the method that uses attribute values in the training set to retrieve the attribute values in the testing set by word matching. Sep-BERT is the pretrained BERT model with feed-forward layers to perform these two subtasks separately. RNN-LSTM (Hakkani-T¨ur et al., 2016), Attn-BiRNN (Liu and Lane, 2016), SlotGated (Goo et al., 2018), and Joint-BERT (Chen et al., 2019) are the models to address intent classification and slot filling tasks, which are similar to the attribute prediction and value extraction, and Value 768 2048 49 (7*7) 200 46 0.0001 sigmoid 0.5 128 50 112M 1x NVIDIA Tesla P40 50 minutes Table 2: Details about hyper-parameters. Model WSM Sep-BERT RNN-LSTM (Hakkani-T¨ur et al., 2016) Attn-BiRNN (Liu and Lane, 2016) Slot-Gated (Goo et al., 2018) Joint-BERT (Chen et al., 2019) ScalingUp (Xu et al., 2019) JAVE (LSTM based) JAVE (BERT based) M-JAVE (LSTM based) M-JAVE (BERT based) Attribute 77.20 86.34 85.76 86.1"
2020.emnlp-main.166,N19-1423,0,0.0311235,"the corresponding values for ecommerce products. The input of the task is a “textual product description, product image” pair, and the outputs are the product attributes (there may be more than one attribute in the descriptions) and the corresponding values. We model the product attribute prediction task as a sequence-level multilabel classification task and the value extraction task as a sequence labeling task. The framework of our proposed Multimodal Joint Attribute Prediction and Value Extraction model (M-JAVE) is shown in Figure 2. The input sentence is encoded by a pretrained BERT model (Devlin et al., 2019), and the image is encoded by a pretrained ResNet model (He et al., 2016). The global-gated cross-modality attention layer encodes text and image into the multimodal hidden representations. Then, the M-JAVE model predicts the product attributes based on the multimodal representations. Next, the model extracts the values based on the previously predicted product attributes and the multimodal representations obtained through the regional-gated cross-modality attention layer. We apply the multitask learning framework to jointly model the product attribute prediction and value extraction. Consider"
2020.emnlp-main.166,D18-1329,0,0.028579,"M-JAVE w/o Regional Visual Gate underperform the models thoroughly removing visual-related modules. To sum up, using the visual product information indiscriminately poses detrimental effects on the model, and selectively utilizing visual product information with global and regional visual gates are essential for our tasks. 2135 4.3 Adversarial Evaluation of Attribute Prediction and Value Extraction Value Attribute To further verify whether the visual product information can improve the performance of product attribute prediction and value extraction, we adopt an adversarial evaluation method (Elliott, 2018) that measures the performance variation when our model is presented with a random incongruent image. The awareness score of a model M on an evaluation dataset D is defined as follows: |D| ∆Awareness = 1 X aM (xi , yi , vi , v¯i ) |D| (15) i Where ∆Awareness denotes the image awareness. x, y denote the the text and the values of the product, respectively. v, ¯v denote the congruent image and the incongruent image, respectively. We use the F1 score to calculate awareness score for a single instance: aM = F1 (xi , yi , vi ) − F1 (xi , yi , v¯i ) (16) Under this definition, the output of the eval"
2020.emnlp-main.166,D17-1114,1,0.821444,"d with black shoes”, the term “golden” can be ambiguous for predicting the product attributes. While by viewing the product image, we can easily recognize the attribute corresponding to “golden” is “Color” instead of “Material”. Moreover, the product image can indicate that the term “black” is not an attribute value of the current product; thus, it should not be extracted. This may be tricky for the model based on purely textual descriptions, but leveraging the visual information can make it easier. In addition, multimodal information shows promising efficiency on many tasks (Lu et al., 2016; Li et al., 2017; Anderson et al., 2018; Li et al., 2018; Yu et al., 2019; Li et al., 2019; Tan and Bansal, 2019; Liu et al., 2019; Su et al., 2020; Li et al., 2020). Therefore, we propose to incorporate visual information into our task. First, we selectively enhance the semantic representation of the textual product descriptions with a global-gated cross-modality attention module that is anticipated to benefit attribute prediction task with visually grounded semantics. Moreover, for different values, our model selectively utilizes visual information with a regional-gated cross-modality attention module to im"
2020.emnlp-main.166,N19-2005,0,0.0281588,"he product image, we can easily recognize the attribute corresponding to “golden” is “Color” instead of “Material”. Moreover, the product image can indicate that the term “black” is not an attribute value of the current product; thus, it should not be extracted. This may be tricky for the model based on purely textual descriptions, but leveraging the visual information can make it easier. In addition, multimodal information shows promising efficiency on many tasks (Lu et al., 2016; Li et al., 2017; Anderson et al., 2018; Li et al., 2018; Yu et al., 2019; Li et al., 2019; Tan and Bansal, 2019; Liu et al., 2019; Su et al., 2020; Li et al., 2020). Therefore, we propose to incorporate visual information into our task. First, we selectively enhance the semantic representation of the textual product descriptions with a global-gated cross-modality attention module that is anticipated to benefit attribute prediction task with visually grounded semantics. Moreover, for different values, our model selectively utilizes visual information with a regional-gated cross-modality attention module to improve the accuracy of values extraction. Our main contributions are threefold: 2130 • We propose an end-to-end mod"
2020.emnlp-main.166,D11-1144,0,0.229489,"to our statistics on a mainstream e-commerce platform in China, there are over 40 attributes for the products in clothing category, but the average count of attributes present for each product is fewer than 8. The absence of the product attributes seriously affects customers’ shopping experience and reduces the potential of successful trading. In this paper, we propose a method to jointly predict product attributes and extract the corresponding values with multimodal product information, as shown in Figure 1. Though plenty of systems have been proposed to supplement product attribute values (Putthividhya and Hu, 2011; More, 2016; Shinzato and Sekine, 2013; Zheng et al., 2018; Xu et al., 2019), the relationship between product attributes and values are not sufficiently explored, and most of these approaches primarily focus on the text information. Attributes and values are, however, known to strongly depend on each other, and vision can play a particularly essential role for this task. 2129 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2129–2139, c November 16–20, 2020. 2020 Association for Computational Linguistics Attribute Prediction Value Extraction Mater"
2020.emnlp-main.166,I13-1190,0,0.0235928,"mmerce platform in China, there are over 40 attributes for the products in clothing category, but the average count of attributes present for each product is fewer than 8. The absence of the product attributes seriously affects customers’ shopping experience and reduces the potential of successful trading. In this paper, we propose a method to jointly predict product attributes and extract the corresponding values with multimodal product information, as shown in Figure 1. Though plenty of systems have been proposed to supplement product attribute values (Putthividhya and Hu, 2011; More, 2016; Shinzato and Sekine, 2013; Zheng et al., 2018; Xu et al., 2019), the relationship between product attributes and values are not sufficiently explored, and most of these approaches primarily focus on the text information. Attributes and values are, however, known to strongly depend on each other, and vision can play a particularly essential role for this task. 2129 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2129–2139, c November 16–20, 2020. 2020 Association for Computational Linguistics Attribute Prediction Value Extraction Material, Collar Type ya Value of Material O"
2020.emnlp-main.166,D19-1514,0,0.0200863,"es. While by viewing the product image, we can easily recognize the attribute corresponding to “golden” is “Color” instead of “Material”. Moreover, the product image can indicate that the term “black” is not an attribute value of the current product; thus, it should not be extracted. This may be tricky for the model based on purely textual descriptions, but leveraging the visual information can make it easier. In addition, multimodal information shows promising efficiency on many tasks (Lu et al., 2016; Li et al., 2017; Anderson et al., 2018; Li et al., 2018; Yu et al., 2019; Li et al., 2019; Tan and Bansal, 2019; Liu et al., 2019; Su et al., 2020; Li et al., 2020). Therefore, we propose to incorporate visual information into our task. First, we selectively enhance the semantic representation of the textual product descriptions with a global-gated cross-modality attention module that is anticipated to benefit attribute prediction task with visually grounded semantics. Moreover, for different values, our model selectively utilizes visual information with a regional-gated cross-modality attention module to improve the accuracy of values extraction. Our main contributions are threefold: 2130 • We propose"
2020.emnlp-main.166,P19-1514,0,0.230766,"ributes for the products in clothing category, but the average count of attributes present for each product is fewer than 8. The absence of the product attributes seriously affects customers’ shopping experience and reduces the potential of successful trading. In this paper, we propose a method to jointly predict product attributes and extract the corresponding values with multimodal product information, as shown in Figure 1. Though plenty of systems have been proposed to supplement product attribute values (Putthividhya and Hu, 2011; More, 2016; Shinzato and Sekine, 2013; Zheng et al., 2018; Xu et al., 2019), the relationship between product attributes and values are not sufficiently explored, and most of these approaches primarily focus on the text information. Attributes and values are, however, known to strongly depend on each other, and vision can play a particularly essential role for this task. 2129 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 2129–2139, c November 16–20, 2020. 2020 Association for Computational Linguistics Attribute Prediction Value Extraction Material, Collar Type ya Value of Material O Bm O Value of Collar Type O O O Bc O"
2020.emnlp-main.166,P15-1128,1,0.735642,"l results on this dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset are available at https://github. com/jd-aig/JAVE. 1 shirt can be paired with black shoes …… ” Product attribute values that provide details of the product are crucial parts of e-commerce, which help customers to make purchasing decisions and facilitate retailers on many applications, such as question answering system (Yih et al., 2015; Yu et al., 2017), product recommendations (Gong, Corresponding author. Value Collar Type lapel Color golden Figure 1: An example of predicting attributes and extracting values from the textual product description with the aid of the visual product information. Introduction ∗ Attribute 2009; Cao et al., 2018), and product retrieval (Liao et al., 2018; Magnani et al., 2019). While product attribute values are pervasively incomplete for a massive number of products on the e-commerce platform. According to our statistics on a mainstream e-commerce platform in China, there are over 40 attributes"
2020.emnlp-main.166,P17-1053,1,0.843333,"dataset demonstrate that explicitly modeling the relationship between attributes and values facilitates our method to establish the correspondence between them, and selectively utilizing visual product information is necessary for the task. Our code and dataset are available at https://github. com/jd-aig/JAVE. 1 shirt can be paired with black shoes …… ” Product attribute values that provide details of the product are crucial parts of e-commerce, which help customers to make purchasing decisions and facilitate retailers on many applications, such as question answering system (Yih et al., 2015; Yu et al., 2017), product recommendations (Gong, Corresponding author. Value Collar Type lapel Color golden Figure 1: An example of predicting attributes and extracting values from the textual product description with the aid of the visual product information. Introduction ∗ Attribute 2009; Cao et al., 2018), and product retrieval (Liao et al., 2018; Magnani et al., 2019). While product attribute values are pervasively incomplete for a massive number of products on the e-commerce platform. According to our statistics on a mainstream e-commerce platform in China, there are over 40 attributes for the products i"
2020.lrec-1.58,P96-1009,0,0.175808,"e uncertainties make dialogue task extremely different from traditional machine learning tasks which usually have explicit targets and clearly defined evaluation metrics. To tackle this challenging problem, constructing a dialogue dataset is the most essential work. Especially for popular deep learning based approaches, large scale of training corpus in real scenario becomes decisive. However, existing datasets are still deficient. Datasets with structured annotations (e.g., slots and corresponding values) are often smallscale and in a limited capacity. Either traditional domainspecific ones (Allen et al., 1996; Petukhova et al., 2014; Bordes et al., 2016; Dodge et al., 2015) or recent multidomain ones (Budzianowski et al., 2018; Shah et al., 2018; El Asri et al., 2017) are usually built for task-oriented dialogue systems. Another typical branch of works collects the dialogue corpus from movie subtitles, such as OpenSubtitles (Tiedemann, 2009) and Cornell (Danescu-NiculescuMizil and Lee, 2011), which contain long sessions (over 100 turns) and some expressions like individual monologues may be not suitable for dialogue systems. More recently, some researchers construct dialogue datasets from social m"
2020.lrec-1.58,D18-1547,0,0.0642854,"Missing"
2020.lrec-1.58,W11-0609,0,0.0813466,"Missing"
2020.lrec-1.58,N19-1423,0,0.084852,"veral candidate answers (10 candidate answers for Challenge Set I and II, and 3 for Challenge Set III), as for dialogue task, the groundtruth is usually not limited to one. What’s more, different weights are provided for each candidate answer, so evaluation metrics (e.g., BLEU score) can be calculated more accurately. We hope these 3 challenge sets can help evaluate the dialogue systems on a fine-grained level. 4. In this section, we conduct experiments on the JDDC dataset. We focus on two categories of models used in datadriven dialogue systems: retrieval-based models based on BM25 and BERT (Devlin et al., 2019) and generative models (Gu et al., 2016). We will introduce some empirical settings, including dataset preparation, baseline methods, parameter settings. Then we introduce the experimental results on this dataset. 4.1. Figure 3: The explanation of our 3 challenge sets. The responses (r) in red color are required to be answered by the dialogue system. http://jddc.jd.com/ Experimental Setup We first divide the around 1 million conversation sessions into training, validation and testing set. Then we construct I-R pairs from each set into the {I, R} = {q1 , r1 , q2 , r2 , Q, R} format, where I = {"
2020.lrec-1.58,W17-5526,0,0.0668821,"Missing"
2020.lrec-1.58,P16-1154,0,0.171491,"s for Challenge Set I and II, and 3 for Challenge Set III), as for dialogue task, the groundtruth is usually not limited to one. What’s more, different weights are provided for each candidate answer, so evaluation metrics (e.g., BLEU score) can be calculated more accurately. We hope these 3 challenge sets can help evaluate the dialogue systems on a fine-grained level. 4. In this section, we conduct experiments on the JDDC dataset. We focus on two categories of models used in datadriven dialogue systems: retrieval-based models based on BM25 and BERT (Devlin et al., 2019) and generative models (Gu et al., 2016). We will introduce some empirical settings, including dataset preparation, baseline methods, parameter settings. Then we introduce the experimental results on this dataset. 4.1. Figure 3: The explanation of our 3 challenge sets. The responses (r) in red color are required to be answered by the dialogue system. http://jddc.jd.com/ Experimental Setup We first divide the around 1 million conversation sessions into training, validation and testing set. Then we construct I-R pairs from each set into the {I, R} = {q1 , r1 , q2 , r2 , Q, R} format, where I = {C, Q} stands for input, C = {q1 , r1 , q"
2020.lrec-1.58,P17-4012,0,0.0116607,"e and negative sample is 1:1. Then the constructed positive and negative I-R pairs are used to fine-tune the BERT model. Our model implementation for BERT is based on Google’s work (Devlin et al., 2019) and follows the hyperparameter settings in the original model. For generative models, we first clean the training set to decrease the portion of short responses (shorter than 3 Chinese characters) and generic responses (e.g., “What else can I do for you?”). Then all remaining I-R pairs are used for training the model. Our code implementation is based on the machine translation toolkit OpenNMT (Klein et al., 2017). In all generative experiments, we set 100,000 for vocabulary size and 200 for word embedding dimension. The source length is 128 and target length is decreased to 40 to avoid generating too long response. Other training parameters are set as default. 4.2. Comparable Models In this subsection, we will introduce the detailed information on retrieval-based models and generative models used for our experiment. 4.2.1. Retrieval-based Models BM25 To make the retrieval baseline more efficient, we firstly index all the Input-Response pairs in the training set using ElasticSearch4 . Then we use BM25"
2020.lrec-1.58,N16-1014,0,0.226314,"(Gu et al., 2016) to the attention-based Seq2Seq baseline (Seq2Seq-Copy). The copy mechanism can explicitly extract words or phrases like certain entities from the input. 4.3. In order to provide comparable baseline results for future research, we use some quantitative metrics for automatic evaluation. BLEU and ROUGE scores, which are widely used in NLP and multi-turn dialogue generation tasks (Tian et al., 2017; Luo et al., 2018; Shen et al., 2019), are used to measure the quality of generated responses via the comparison with the ground truths. The recently proposed Distinct (Distinct-1/2) (Li et al., 2016), is used to evaluate the degree of diversity by calculating the ratio of unique unigrams and bigrams in the generated responses. 4.4. 4.4.1. Automatic Evaluation Results BM25 BERTRetrieval i 4.2.2. Generative Models Vanilla Seq2Seq We implement the vanilla Sequence-toSequence (Seq2Seq) model (Shang et al., 2015b) with 512unit 4-layer Bi-LSTMs for both the encoder and decoder. The input is the concatenated context and query, while the output is the response. Attention-based Seq2Seq To improve our baseline, we applied attention mechanism (Luong et al., 2015) in the Experimental Results In this"
2020.lrec-1.58,W15-4640,0,0.236256,"systems. Another typical branch of works collects the dialogue corpus from movie subtitles, such as OpenSubtitles (Tiedemann, 2009) and Cornell (Danescu-NiculescuMizil and Lee, 2011), which contain long sessions (over 100 turns) and some expressions like individual monologues may be not suitable for dialogue systems. More recently, some researchers construct dialogue datasets from social media networks (e.g., Twitter Dialogue Corpus (Ritter et al., 2011) and Chinese Weibo dataset (Wang et al., 2013)), or online forums (e.g., Chinese Douban dataset (Wu et al., 2017) and Ubuntu Dialogue Corpus (Lowe et al., 2015)). Although in large scale, they are different from real scenario conversations, as posts and replies are informal, single-turn or short-term related. In this work, we construct a large-scale multi-turn Chinese dialogue dataset, namely JDDC (Jing Dong Dialogue Corpus), with more than 1 million multi-turn dialogues, 20 million utterances, and 150 million words, which contains conversations about after-sales topics between users and customer service staffs in E-commerce scenario. Different from existing datasets mentioned above, the JDDC dataset illustrates the complexity of conversations in E-c"
2020.lrec-1.58,D18-1075,0,0.0374738,"Missing"
2020.lrec-1.58,D15-1166,0,0.0164212,"ently proposed Distinct (Distinct-1/2) (Li et al., 2016), is used to evaluate the degree of diversity by calculating the ratio of unique unigrams and bigrams in the generated responses. 4.4. 4.4.1. Automatic Evaluation Results BM25 BERTRetrieval i 4.2.2. Generative Models Vanilla Seq2Seq We implement the vanilla Sequence-toSequence (Seq2Seq) model (Shang et al., 2015b) with 512unit 4-layer Bi-LSTMs for both the encoder and decoder. The input is the concatenated context and query, while the output is the response. Attention-based Seq2Seq To improve our baseline, we applied attention mechanism (Luong et al., 2015) in the Experimental Results In this section, we analyze different baselines’ performance based on automatic evaluation measures and present indepth case study. Wi · S1 (wi , Idoc ) · S2 (wi , Itest ) (1) where Itest stands for the test input including context C and query Q, wi is the i-th word in the Itest , Idoc is the document input in the repository, Wi represents the weight of wi (such as inverse document frequency), and S(·) calculates the relevance score of the two elements. Therefore, S(Itest , Idoc ) is the similarity score between the test input and the existing I-R pairs in the repo"
2020.lrec-1.58,petukhova-etal-2014-dbox,0,0.0203159,"dialogue task extremely different from traditional machine learning tasks which usually have explicit targets and clearly defined evaluation metrics. To tackle this challenging problem, constructing a dialogue dataset is the most essential work. Especially for popular deep learning based approaches, large scale of training corpus in real scenario becomes decisive. However, existing datasets are still deficient. Datasets with structured annotations (e.g., slots and corresponding values) are often smallscale and in a limited capacity. Either traditional domainspecific ones (Allen et al., 1996; Petukhova et al., 2014; Bordes et al., 2016; Dodge et al., 2015) or recent multidomain ones (Budzianowski et al., 2018; Shah et al., 2018; El Asri et al., 2017) are usually built for task-oriented dialogue systems. Another typical branch of works collects the dialogue corpus from movie subtitles, such as OpenSubtitles (Tiedemann, 2009) and Cornell (Danescu-NiculescuMizil and Lee, 2011), which contain long sessions (over 100 turns) and some expressions like individual monologues may be not suitable for dialogue systems. More recently, some researchers construct dialogue datasets from social media networks (e.g., Twi"
2020.lrec-1.58,N10-1020,0,0.311105,"has kept active for decades. The growth of this field has been consistently supported by the development of new datasets. We briefly review existing dialogue datasets, and roughly divide them into three categories according to data features: 1) large scale data extracted from social media or forums, 2) artificial dialogue corpus constructed from crowd workers, and 3) corpus collected from real human-human coversation scenario. A list of related large-scale datasets discussed is provided in Table 2. Traditional methods tend to extract conversation alike information from social media or forum (Ritter et al., 2010; Shang et al., 2015a; Wu et al., 2017; Lowe et al., 2015; Li et al., 2018; Al-Rfou et al., 2016). Despite of the massive number of utterances included in these datasets, they usually provide ambiguous dialogue flows. It’s due to the fact that these datasets mainly comprise post-reply pairs on social networks or forums where people interact with others more freely (often more than two speakers are involved in the conversation). Moreover, the replies in these datasets are most or only related to the post and there are very few context information provided for query understanding. To imitate the"
2020.lrec-1.58,D11-1054,0,0.0539628,"Dodge et al., 2015) or recent multidomain ones (Budzianowski et al., 2018; Shah et al., 2018; El Asri et al., 2017) are usually built for task-oriented dialogue systems. Another typical branch of works collects the dialogue corpus from movie subtitles, such as OpenSubtitles (Tiedemann, 2009) and Cornell (Danescu-NiculescuMizil and Lee, 2011), which contain long sessions (over 100 turns) and some expressions like individual monologues may be not suitable for dialogue systems. More recently, some researchers construct dialogue datasets from social media networks (e.g., Twitter Dialogue Corpus (Ritter et al., 2011) and Chinese Weibo dataset (Wang et al., 2013)), or online forums (e.g., Chinese Douban dataset (Wu et al., 2017) and Ubuntu Dialogue Corpus (Lowe et al., 2015)). Although in large scale, they are different from real scenario conversations, as posts and replies are informal, single-turn or short-term related. In this work, we construct a large-scale multi-turn Chinese dialogue dataset, namely JDDC (Jing Dong Dialogue Corpus), with more than 1 million multi-turn dialogues, 20 million utterances, and 150 million words, which contains conversations about after-sales topics between users and custo"
2020.lrec-1.58,P15-1152,0,0.141836,"decades. The growth of this field has been consistently supported by the development of new datasets. We briefly review existing dialogue datasets, and roughly divide them into three categories according to data features: 1) large scale data extracted from social media or forums, 2) artificial dialogue corpus constructed from crowd workers, and 3) corpus collected from real human-human coversation scenario. A list of related large-scale datasets discussed is provided in Table 2. Traditional methods tend to extract conversation alike information from social media or forum (Ritter et al., 2010; Shang et al., 2015a; Wu et al., 2017; Lowe et al., 2015; Li et al., 2018; Al-Rfou et al., 2016). Despite of the massive number of utterances included in these datasets, they usually provide ambiguous dialogue flows. It’s due to the fact that these datasets mainly comprise post-reply pairs on social networks or forums where people interact with others more freely (often more than two speakers are involved in the conversation). Moreover, the replies in these datasets are most or only related to the post and there are very few context information provided for query understanding. To imitate the natural conversatio"
2020.lrec-1.58,P19-1549,1,0.835626,"ng and contains a lot of rare terminologies like “京东白条” (Jing Dong IOU(I owe you)), which may be OOV (out of vocabulary) words. Therefore, we add the copy mechanism (Gu et al., 2016) to the attention-based Seq2Seq baseline (Seq2Seq-Copy). The copy mechanism can explicitly extract words or phrases like certain entities from the input. 4.3. In order to provide comparable baseline results for future research, we use some quantitative metrics for automatic evaluation. BLEU and ROUGE scores, which are widely used in NLP and multi-turn dialogue generation tasks (Tian et al., 2017; Luo et al., 2018; Shen et al., 2019), are used to measure the quality of generated responses via the comparison with the ground truths. The recently proposed Distinct (Distinct-1/2) (Li et al., 2016), is used to evaluate the degree of diversity by calculating the ratio of unique unigrams and bigrams in the generated responses. 4.4. 4.4.1. Automatic Evaluation Results BM25 BERTRetrieval i 4.2.2. Generative Models Vanilla Seq2Seq We implement the vanilla Sequence-toSequence (Seq2Seq) model (Shang et al., 2015b) with 512unit 4-layer Bi-LSTMs for both the encoder and decoder. The input is the concatenated context and query, while th"
2020.lrec-1.58,P17-2036,0,0.0199818,"The context-query input is usually long and contains a lot of rare terminologies like “京东白条” (Jing Dong IOU(I owe you)), which may be OOV (out of vocabulary) words. Therefore, we add the copy mechanism (Gu et al., 2016) to the attention-based Seq2Seq baseline (Seq2Seq-Copy). The copy mechanism can explicitly extract words or phrases like certain entities from the input. 4.3. In order to provide comparable baseline results for future research, we use some quantitative metrics for automatic evaluation. BLEU and ROUGE scores, which are widely used in NLP and multi-turn dialogue generation tasks (Tian et al., 2017; Luo et al., 2018; Shen et al., 2019), are used to measure the quality of generated responses via the comparison with the ground truths. The recently proposed Distinct (Distinct-1/2) (Li et al., 2016), is used to evaluate the degree of diversity by calculating the ratio of unique unigrams and bigrams in the generated responses. 4.4. 4.4.1. Automatic Evaluation Results BM25 BERTRetrieval i 4.2.2. Generative Models Vanilla Seq2Seq We implement the vanilla Sequence-toSequence (Seq2Seq) model (Shang et al., 2015b) with 512unit 4-layer Bi-LSTMs for both the encoder and decoder. The input is the co"
2020.lrec-1.58,D13-1096,0,0.0214639,"(Budzianowski et al., 2018; Shah et al., 2018; El Asri et al., 2017) are usually built for task-oriented dialogue systems. Another typical branch of works collects the dialogue corpus from movie subtitles, such as OpenSubtitles (Tiedemann, 2009) and Cornell (Danescu-NiculescuMizil and Lee, 2011), which contain long sessions (over 100 turns) and some expressions like individual monologues may be not suitable for dialogue systems. More recently, some researchers construct dialogue datasets from social media networks (e.g., Twitter Dialogue Corpus (Ritter et al., 2011) and Chinese Weibo dataset (Wang et al., 2013)), or online forums (e.g., Chinese Douban dataset (Wu et al., 2017) and Ubuntu Dialogue Corpus (Lowe et al., 2015)). Although in large scale, they are different from real scenario conversations, as posts and replies are informal, single-turn or short-term related. In this work, we construct a large-scale multi-turn Chinese dialogue dataset, namely JDDC (Jing Dong Dialogue Corpus), with more than 1 million multi-turn dialogues, 20 million utterances, and 150 million words, which contains conversations about after-sales topics between users and customer service staffs in E-commerce scenario. Dif"
2020.lrec-1.58,P17-1046,0,0.185368,"are usually built for task-oriented dialogue systems. Another typical branch of works collects the dialogue corpus from movie subtitles, such as OpenSubtitles (Tiedemann, 2009) and Cornell (Danescu-NiculescuMizil and Lee, 2011), which contain long sessions (over 100 turns) and some expressions like individual monologues may be not suitable for dialogue systems. More recently, some researchers construct dialogue datasets from social media networks (e.g., Twitter Dialogue Corpus (Ritter et al., 2011) and Chinese Weibo dataset (Wang et al., 2013)), or online forums (e.g., Chinese Douban dataset (Wu et al., 2017) and Ubuntu Dialogue Corpus (Lowe et al., 2015)). Although in large scale, they are different from real scenario conversations, as posts and replies are informal, single-turn or short-term related. In this work, we construct a large-scale multi-turn Chinese dialogue dataset, namely JDDC (Jing Dong Dialogue Corpus), with more than 1 million multi-turn dialogues, 20 million utterances, and 150 million words, which contains conversations about after-sales topics between users and customer service staffs in E-commerce scenario. Different from existing datasets mentioned above, the JDDC dataset ill"
2020.lrec-1.58,P19-1369,0,0.102398,"ber of utterances included in these datasets, they usually provide ambiguous dialogue flows. It’s due to the fact that these datasets mainly comprise post-reply pairs on social networks or forums where people interact with others more freely (often more than two speakers are involved in the conversation). Moreover, the replies in these datasets are most or only related to the post and there are very few context information provided for query understanding. To imitate the natural conversation flows in real life, some datasets are collected with pre-defined prompts or guided schema. The DuConv (Wu et al., 2019) and PERSONACHAT (Zhang et al., 2018a) datasets are collected with Wizard-of-Oz technique (Kelley, 1984). The former one is collected during knowledge-driven conversation with one person playing as the conversation leader and the other one playing as the follower. However, the conversation goal is defined in advance. The later one collects data from two crowd workers with different persona information provided during conversation. Apart from the WOZ, the SGD (Rastogi et al., 2019) dataset is constructed by firstly generating dialogue outlines by simulator, then uses a crowd-sourcing procedure"
2020.lrec-1.58,N16-1174,1,0.430747,"Missing"
2020.lrec-1.58,P18-1205,0,0.366805,"e datasets, they usually provide ambiguous dialogue flows. It’s due to the fact that these datasets mainly comprise post-reply pairs on social networks or forums where people interact with others more freely (often more than two speakers are involved in the conversation). Moreover, the replies in these datasets are most or only related to the post and there are very few context information provided for query understanding. To imitate the natural conversation flows in real life, some datasets are collected with pre-defined prompts or guided schema. The DuConv (Wu et al., 2019) and PERSONACHAT (Zhang et al., 2018a) datasets are collected with Wizard-of-Oz technique (Kelley, 1984). The former one is collected during knowledge-driven conversation with one person playing as the conversation leader and the other one playing as the follower. However, the conversation goal is defined in advance. The later one collects data from two crowd workers with different persona information provided during conversation. Apart from the WOZ, the SGD (Rastogi et al., 2019) dataset is constructed by firstly generating dialogue outlines by simulator, then uses a crowd-sourcing procedure to paraphrase the outlines to natura"
2020.lrec-1.58,C18-1317,0,0.380799,"e datasets, they usually provide ambiguous dialogue flows. It’s due to the fact that these datasets mainly comprise post-reply pairs on social networks or forums where people interact with others more freely (often more than two speakers are involved in the conversation). Moreover, the replies in these datasets are most or only related to the post and there are very few context information provided for query understanding. To imitate the natural conversation flows in real life, some datasets are collected with pre-defined prompts or guided schema. The DuConv (Wu et al., 2019) and PERSONACHAT (Zhang et al., 2018a) datasets are collected with Wizard-of-Oz technique (Kelley, 1984). The former one is collected during knowledge-driven conversation with one person playing as the conversation leader and the other one playing as the follower. However, the conversation goal is defined in advance. The later one collects data from two crowd workers with different persona information provided during conversation. Apart from the WOZ, the SGD (Rastogi et al., 2019) dataset is constructed by firstly generating dialogue outlines by simulator, then uses a crowd-sourcing procedure to paraphrase the outlines to natura"
2021.emnlp-main.336,2020.emnlp-main.700,0,0.0296039,"Missing"
2021.emnlp-main.336,N18-1150,1,0.904233,"Missing"
2021.emnlp-main.336,P18-1063,0,0.0140842,"ith a small margin, indicating that general pre-training with selected data is not effective, and correlational copying is essential for pretraining. Fourth, we study the effectiveness of semantic and positional correlation between source words (i.e., SemCorrelation and PosCorrelation, respectively), we can observe that semantic and positional correlation are both useful, and depriving positional correlation decreases the performance larger. 4.3.2 Results on SAMSum The results on the SAMSum dataset are shown in Table 3. • Longest-3 takes three longest utterances as the summary. • Fast Abs RL (Chen and Bansal, 2018) is a hybrid extractive-abstractive model with the policy-based reinforcement learning. • TransformerABS (Vaswani et al., 2017) is the basic Transformer-based Seq2Seq model without pre-training. • DynamicConv (Wu et al., 2018) is a dynamic convolution model based on lightweight convolutions. • D-HGN (Feng et al., 2020) is a dialogue heterogeneous graph network modeling the utterance and commonsense knowledge. • TGDGA (Zhao et al., 2020) is a topic-word guided dialogue method based on the graph attention model. Models RG-1 RG-2 RG-L 32.46 37.27 41.03 42.37 45.41 42.03 43.11 51.53 51.58 10.27 14"
2021.emnlp-main.336,N16-1012,0,0.0290636,"tly, the sequence-to-sequence (Seq2Seq) framework has become the mainstream for performing abstractive summarization tasks. However, it suffers from handling out-of-vocabulary words (OOV). As it has been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step indepenout reading the full text. There are two types of dently of the former ones, neglecting the guidance summarizers: ext"
2021.emnlp-main.336,D18-1443,0,0.0276445,"Missing"
2021.emnlp-main.336,D19-5409,0,0.0146854,"pan to 128 and 32, respectively, After ranking with the ROUGE score, we select the top 20M samples as our final pre-training data. We believe this data selection strategy towards pre-training can make sure that there are enough output words that can be generated by copying from the input, which resembles the downstream task and learns our proposed correlational copying mechanism better. 4 4.1 Experiments Dataset sentence summaries. We use the non-anonymized version used in See et al. (2017), which has 287,226 training samples, 13,368 validation samples and 11,490 test samples. SAMSum dataset (Gliwa et al., 2019) contains 16K chat dialogues with manually annotated summaries, splited into 14,732 training samples, 818 validation samples, and 819 test samples. We use the version of the dataset with artificial separator (Gliwa et al., 2019), in which utterances are separated with “|”. 4.2 Experimental Settings For simplicity, we warm-start the model parameters with the publicly released pre-trained BART (large) model1 with 12 layers in both the encoder and decoder, and the hidden size is 1024. The learning rate is set to 3e-5, and learning decay is applied. We use Adam optimizer with β1 = 0.9, β1 = 0.999,"
2021.emnlp-main.336,P16-1154,0,0.126663,"ns (such as“into a red Honda”). how humans would summarize a text, but it is far more challenging to achieve. Currently, the sequence-to-sequence (Seq2Seq) framework has become the mainstream for performing abstractive summarization tasks. However, it suffers from handling out-of-vocabulary words (OOV). As it has been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step inde"
2021.emnlp-main.336,P16-1014,1,0.927021,"lowing copying operations (such as“into a red Honda”). how humans would summarize a text, but it is far more challenging to achieve. Currently, the sequence-to-sequence (Seq2Seq) framework has become the mainstream for performing abstractive summarization tasks. However, it suffers from handling out-of-vocabulary words (OOV). As it has been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at ea"
2021.emnlp-main.336,2020.acl-main.703,0,0.50856,"rent from Yang et al. (2018), we do not apply the predicted central position, because we argue that the information of relative position is strongly associated with the word correlations. In addition, following Shaw et al. (2018), we perform a relative distance clipping to improve the generalization of our model. 3.5 Correlational Copying Pre-training (CoCoPretrain) Pre-training with self-supervised objectives on raw text corpora has demonstrated the effectiveness of attCopy coCopy a broad range of text generation tasks (Song et al., = gt · Pt (w) + (1 − gt ) · Pt (w) 2019; Dong et al., 2019; Lewis et al., 2020; Zhang (14) X et al., 2020). In this paper, we enhance CoCoNet gt = sigmoid(Wg (αt,i + PtcoCopy (xi )) · Vi )) through correlational copying pre-training (CoCoi (15) Pretrain) on text span generation. The process of 4094 Ptf inalCopy (w) Original Text t1 : The formation of the Central American Isthmus closed the Central American Seaway Output Span Top-K Pre-training Data … Input Span si sj sk s1 … Overlap Score s1 Figure 2: The process of constructing the pre-training data. Given a piece of text, we divide it into an input span and an output span, and we calculate the overlap score of them by"
2021.emnlp-main.336,C18-1121,1,0.89257,"Missing"
2021.emnlp-main.336,W04-1013,0,0.286925,"pretraining with a self-supervised objective of text span generation with copying on the raw text corpora. Motivated by the work of Zhang et al. (2020), which has proven that pre-training resembling the downstream task leads to better and faster fine-tuning performances, we make sure our pretraining simulates the copying behaviors desired for the downstream summarization tasks. We divide each sequence in the corpora into two spans with some overlapping words, and the first span is used to generate the second in pre-training. We measure the overlap between the two spans based on ROUGE scores (Lin, 2004) to ensure that there are enough words to be generated by copying. Our main contributions are as follows: • We propose a Correlational Copying Network (CoCoNet) for abstractive summarization. It tracks the copying history and copies the next word from the input based on its relevance with the previously copied one. • We further enhance CoCoNet’s learning of copying through self-supervised pre-training on text span generation with copying. summarization tasks, and experimental results show that CoCoNet can copy more accurately. 2 2.1 Related work Copying Mechanism The copying mechanism is widel"
2021.emnlp-main.336,D19-1387,0,0.015656,"eported) BART (Our implement) BART + Cont. Pre-train Pre-trained Models + Copying BART + AttnCopy BART + SAGCopy CoCoNet CoCoNet - SemCorrelation CoCoNet - PosCorrelation CoCoNet + CoCoPretrain Table 2: ROUGE F1 scores on the CNN/DailyMail dataset. For a fair comparison, we continue pretraining BART with the same pre-training data but without copying mechanism (i.e., BART + Cont. Pretrain). • Lead-3 baseline that simply selects the first three sentences in the input document. • PGNet (See et al., 2017) is a hybrid pointergenerator model applying an attentional copy mechanism. • BERTSUMEXTABS (Liu and Lapata, 2019) applies BERT in text summarization. It is a two-stage fine-tuned model that first finetunes the encoder on the extractive summarization task and then on the abstractive summarization task. • SAGCopy (Xu et al., 2020b) fine-tunes MASS by incorporating the importance score for source words into the copying module. • PEGASUS (Zhang et al., 2020) adopts gapsentence generation as the pre-training objective. • T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) are models with denoising Seq2Seq pre-training. • ProphetNet (Qi et al., 2020) proposes to simultaneously predict the future n-gram at e"
2021.emnlp-main.336,P15-1002,0,0.0232746,"rd from the input based on its relevance with the previously copied one. • We further enhance CoCoNet’s learning of copying through self-supervised pre-training on text span generation with copying. summarization tasks, and experimental results show that CoCoNet can copy more accurately. 2 2.1 Related work Copying Mechanism The copying mechanism is widely used in abstractive summarization. It allows models to directly copy words from the input to the output. Vinyals et al. (2015) present the pointer network that uses attention distribution to select tokens in the input sequence as the output. Luong et al. (2015) propose to copy source words to the target sentence by a fixed-size softmax layer over a relative copying range. Gulcehre et al. (2016) leverage the attention mechanism to predict the location of the word to copy and apply a copying gate to determine whether to copy or not. Gu et al. (2016) propose to predict output words by combining copying and generating modes through a shared softmax function. See et al. (2017) introduce a copying probability to incorporate copying and generating distributions dynamically. Bi et al. (2020) adopt the copy mechanism in the language model pre-training. Exist"
2021.emnlp-main.336,P16-1008,0,0.0238253,"not attempt to calculate the copying distributions based on the copying history, which is our focus. 2.2 Temporal Attention Mechanism Our proposed copying mechanism is partially inspired by the temporal attention mechanism (Sankaran et al., 2016) that keeps track of previous attention scores and adjusts the future attention distribution by normalization with historical attention scores. This model has been proven effective in the text summarization task (Nallapati et al., 2016). Similar ideas are also adopted by the coverage mechanism for image caption (Xu et al., 2015), machine translation (Tu et al., 2016), and text summarization (See et al., 2017), maintaining a coverage vector to record the attention history to compute future attention distributions. Temporal attention mechanism is designed to avoid repetitive or insufficient attentions. While our work aims to learn a better copying mechanism from the copying history. 3 3.1 Model Overview The input of the text summarization task is a longer • CoCoNet achieves new state-of-the-art perfor- text, x = (x1 , x2 , ..., xS ) of S tokens, and the outmances on news summarization and dialogue put is a condensed summary, y = (y1 , y2 , ..., yT ) 4092 in"
2021.emnlp-main.336,K16-1028,1,0.927465,"ng and generating distributions dynamically. Bi et al. (2020) adopt the copy mechanism in the language model pre-training. Existing works do not attempt to calculate the copying distributions based on the copying history, which is our focus. 2.2 Temporal Attention Mechanism Our proposed copying mechanism is partially inspired by the temporal attention mechanism (Sankaran et al., 2016) that keeps track of previous attention scores and adjusts the future attention distribution by normalization with historical attention scores. This model has been proven effective in the text summarization task (Nallapati et al., 2016). Similar ideas are also adopted by the coverage mechanism for image caption (Xu et al., 2015), machine translation (Tu et al., 2016), and text summarization (See et al., 2017), maintaining a coverage vector to record the attention history to compute future attention distributions. Temporal attention mechanism is designed to avoid repetitive or insufficient attentions. While our work aims to learn a better copying mechanism from the copying history. 3 3.1 Model Overview The input of the text summarization task is a longer • CoCoNet achieves new state-of-the-art perfor- text, x = (x1 , x2 , ..."
2021.emnlp-main.336,2020.findings-emnlp.217,0,0.0220871,"Missing"
2021.emnlp-main.336,D15-1044,0,0.0533162,"summarize a text, but it is far more challenging to achieve. Currently, the sequence-to-sequence (Seq2Seq) framework has become the mainstream for performing abstractive summarization tasks. However, it suffers from handling out-of-vocabulary words (OOV). As it has been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step indepenout reading the full text. There are two types"
2021.emnlp-main.336,P17-1099,0,0.6045,"but it is far more challenging to achieve. Currently, the sequence-to-sequence (Seq2Seq) framework has become the mainstream for performing abstractive summarization tasks. However, it suffers from handling out-of-vocabulary words (OOV). As it has been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step indepenout reading the full text. There are two types of dently of the fo"
2021.emnlp-main.336,N18-2074,0,0.0232705,"ias, which considers the relative distances between different source words and range of local context suitable for copying: −(pstj −psti )2 1 2δ 2 j e pj,i = √ 2πδj |x| δj = sigmoid(Wδ Qj ) 2 (20) (21) where pstj and psti denote the positions for source word xj and xi , respectively. δj denotes the standard deviation that conditions on the length of the source sequence, i.e., |x|. Different from Yang et al. (2018), we do not apply the predicted central position, because we argue that the information of relative position is strongly associated with the word correlations. In addition, following Shaw et al. (2018), we perform a relative distance clipping to improve the generalization of our model. 3.5 Correlational Copying Pre-training (CoCoPretrain) Pre-training with self-supervised objectives on raw text corpora has demonstrated the effectiveness of attCopy coCopy a broad range of text generation tasks (Song et al., = gt · Pt (w) + (1 − gt ) · Pt (w) 2019; Dong et al., 2019; Lewis et al., 2020; Zhang (14) X et al., 2020). In this paper, we enhance CoCoNet gt = sigmoid(Wg (αt,i + PtcoCopy (xi )) · Vi )) through correlational copying pre-training (CoCoi (15) Pretrain) on text span generation. The proce"
2021.emnlp-main.336,2020.acl-main.125,1,0.167147,"been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step indepenout reading the full text. There are two types of dently of the former ones, neglecting the guidance summarizers: extractive and abstractive. Extractive of the copying history. Our work demonstrates that methods produce a summary by taking important the copying history can provide crucial clues of the sentence"
2021.emnlp-main.336,D18-1475,0,0.129476,"n the Transformer-based Seq2Seq architecture (Vaswani et al., 2017) , which has shown superiority in various text generation tasks, such as machine translation and text summarization. More specifically, CoCoNet copies from the input text at each time step by selecting what is relevant to the previously copied word. It keeps track of the prior copying distribution and explicitly models the correlation between different source words by integrating semantic and positional correlations. We obtain the semantic correlations based on the encoder selfattention matrix as Xu et al. (2020b). Inspired by Yang et al. (2018), we represent positional correlations as a Gaussian bias, which considers the relative distances between source words and the scope of the local context when copying. The framework of our model is shown in Figure 1. Furthermore, we enhance CoCoNet through pretraining with a self-supervised objective of text span generation with copying on the raw text corpora. Motivated by the work of Zhang et al. (2020), which has proven that pre-training resembling the downstream task leads to better and faster fine-tuning performances, we make sure our pretraining simulates the copying behaviors desired fo"
2021.emnlp-main.336,2020.coling-main.502,1,0.806885,"t some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step indepenout reading the full text. There are two types of dently of the former ones, neglecting the guidance summarizers: extractive and abstractive. Extractive of the copying history. Our work demonstrates that methods produce a summary by taking important the copying history can provide crucial clues of the sentences from the original t"
2021.emnlp-main.336,2020.coling-main.39,0,0.0269741,". 4.3.2 Results on SAMSum The results on the SAMSum dataset are shown in Table 3. • Longest-3 takes three longest utterances as the summary. • Fast Abs RL (Chen and Bansal, 2018) is a hybrid extractive-abstractive model with the policy-based reinforcement learning. • TransformerABS (Vaswani et al., 2017) is the basic Transformer-based Seq2Seq model without pre-training. • DynamicConv (Wu et al., 2018) is a dynamic convolution model based on lightweight convolutions. • D-HGN (Feng et al., 2020) is a dialogue heterogeneous graph network modeling the utterance and commonsense knowledge. • TGDGA (Zhao et al., 2020) is a topic-word guided dialogue method based on the graph attention model. Models RG-1 RG-2 RG-L 32.46 37.27 41.03 42.37 45.41 42.03 43.11 51.53 51.58 10.27 14.42 16.93 18.44 20.65 18.07 19.15 26.48 26.49 29.92 34.36 39.05 39.27 41.45 39.56 40.49 47.22 47.11 52.03 52.12 52.28 52.21 52.16 52.68 26.69 26.82 26.97 26.87 26.79 27.89 47.55 47.80 48.14 48.01 47.94 48.67 Baseline Methods Longest-3 PGNet Fast Abs RL TransformerABS DynamicConv D-HGN TGDGA BART (Our implement) BART + Cont. Pre-train Pre-trained Models + Copying BART + AttnCopy BART + SAGCopy CoCoNet CoCoNet - SemCorrelation CoCoNet - P"
2021.emnlp-main.336,P17-1101,0,0.0217972,"sequence (Seq2Seq) framework has become the mainstream for performing abstractive summarization tasks. However, it suffers from handling out-of-vocabulary words (OOV). As it has been observed that some words in the input text reappear in the summary, one way of coping with the OOV issue is by extracting words from the input text and incorporating them into the abstractive summary. Following this strategy, exist1 Introduction ing works (Gulcehre et al., 2016; Gu et al., 2016; Text summarization techniques (Rush et al., 2015; See et al., 2017) propose the copying mechanism, Chopra et al., 2016; Zhou et al., 2017; Li et al., which copies words from the input sequence to 2018; Zhang et al., 2018; Li et al., 2019, 2020a,b; form part of the summary. These models generally Xu et al., 2020a; Yuan et al., 2020) aim to gener- regard the encoder-decoder attention as the copying ate a condensed and cohesive version of the input distribution, which we call “attentional copying”. text, enabling readers to grasp the main points with- They perform copying at each time step indepenout reading the full text. There are two types of dently of the former ones, neglecting the guidance summarizers: extractive and abstrac"
2021.findings-emnlp.1,K16-1028,1,0.825257,"-2 (RG-2), and ROUGE-L (RG-L) F1 scores (Lin and Hovy, 2003). K-PLUG clearly performs better than other text-based methods. E-commerce knowledge plays a significant role in the abstractive product summarization task, and domain-specific pre-training data and knowledge-injected pre-training objectives both enhance the model. K-PLUG achieves 4.2.2 Abstractive Product Summarization Task Definition. Abstractive product summarization task aims to capture the most attractive information of a product that resonates with potential purchasers. Similar to the text summarization task (Rush et al., 2015; Nallapati et al., 2016; Li 6 Model Home Applications RG-1 RG-2 RG-L RG-1 Clothing RG-2 RG-L Cases&Bags RG-1 RG-2 RG-L LexRank Seq2seq MASS PG 24.06 21.57 28.19 31.11 10.01 7.18 8.02 10.93 18.19 17.61 18.73 21.11 26.87 23.05 26.73 29.11 9.01 6.84 8.03 9.24 17.76 16.82 17.72 19.92 27.09 23.18 27.19 31.31 9.87 6.94 9.03 10.27 18.03 17.29 18.17 21.79 Aspect MMPG* 34.36 12.52 22.35 31.93 11.09 21.54 33.78 12.51 22.43 C-PLUG E-PLUG K-PLUG 32.75 33.11 33.56 11.62 12.07 12.50 21.76 22.01 22.15 31.73 32.61 33.00 10.86 11.03 11.24 20.37 20.98 21.43 32.04 32.37 33.87 10.75 11.14 11.83 21.85 21.98 22.35 Table 2: Experimental r"
2021.findings-emnlp.1,2020.acl-main.703,0,0.182112,"D AI Research University of California, Berkeley 3 Renmin University of China {xusong28, lihaoran24, yuanpeng29}@jd.com 2 Abstract language understanding (NLU) tasks, including text classification, reading comprehension, and natural language inference. These models are trained on large-scale text corpora with self-supervision based on either bi-directional or auto-regressive pre-training. Equally promising performances have been achieved in natural language generation (NLG) tasks, such as machine translation and text summarization, by MASS (Song et al., 2019), UniLM (Dong et al., 2019), BART (Lewis et al., 2020), T5 (Raffel et al., 2020), PEGASUS (Zhang et al., 2020), and ProphetNet (Qi et al., 2020). In contrast, these approaches adopt Transformerbased sequence-to-sequence models to jointly pretrain for both the encoder and the decoder. While these PLMs can learn rich semantic patterns from raw text data and thereby enhance downstream NLP applications, many of them do not explicitly model domain-specific knowledge. As a result, they may not be as sufficient for capturing human-curated or domain-specific knowledge that is necessary for tasks in a certain domain, such as tasks in e-commerce scenarios."
2021.findings-emnlp.1,N18-1202,0,0.261068,"roduct entities, and unique selling propositions of product entities. K-PLUG achieves new state-of-the-art results on a suite of domain-specific NLP tasks, including product knowledge base completion, abstractive product summarization, and multiturn dialogue, significantly outperforms baselines across the board, which demonstrates that the proposed method effectively learns a diverse set of domain-specific knowledge for both language understanding and generation tasks. Our code is available at https:// github.com/xu-song/k-plug. 1 Introduction Pre-trained language models (PLMs), such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019), have made remarkable breakthroughs in many natural ∗ Equal contribution. 1 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1–17 November 7–11, 2021. ©2021 Association for Computational Linguistics K-PLUG integrates knowledge into pre-training for both the encoder and the decoder, and thus K-PLUG can be adopted to both downstream knowledge-driven NLU and NLG tasks. We verify the performance of the proposed method in various e-commerce scenarios. In th"
2021.findings-emnlp.1,D19-1005,0,0.0507863,"Missing"
2021.findings-emnlp.1,2020.findings-emnlp.217,0,0.0116008,"oran24, yuanpeng29}@jd.com 2 Abstract language understanding (NLU) tasks, including text classification, reading comprehension, and natural language inference. These models are trained on large-scale text corpora with self-supervision based on either bi-directional or auto-regressive pre-training. Equally promising performances have been achieved in natural language generation (NLG) tasks, such as machine translation and text summarization, by MASS (Song et al., 2019), UniLM (Dong et al., 2019), BART (Lewis et al., 2020), T5 (Raffel et al., 2020), PEGASUS (Zhang et al., 2020), and ProphetNet (Qi et al., 2020). In contrast, these approaches adopt Transformerbased sequence-to-sequence models to jointly pretrain for both the encoder and the decoder. While these PLMs can learn rich semantic patterns from raw text data and thereby enhance downstream NLP applications, many of them do not explicitly model domain-specific knowledge. As a result, they may not be as sufficient for capturing human-curated or domain-specific knowledge that is necessary for tasks in a certain domain, such as tasks in e-commerce scenarios. In order to overcome this limitation, several recent studies have proposed to enrich PLMs"
2021.findings-emnlp.1,C18-1121,1,0.880096,"Missing"
2021.findings-emnlp.1,N03-1020,0,0.132811,"Missing"
2021.findings-emnlp.1,J85-2015,0,0.808299,"onal Linguistics: EMNLP 2021, pages 1–17 November 7–11, 2021. ©2021 Association for Computational Linguistics K-PLUG integrates knowledge into pre-training for both the encoder and the decoder, and thus K-PLUG can be adopted to both downstream knowledge-driven NLU and NLG tasks. We verify the performance of the proposed method in various e-commerce scenarios. In the proposed K-PLUG, we formulate the learning of four types of domainspecific knowledge: e-commerce domain-specific knowledge-bases, aspects of product entities, categories of product entities, and unique selling propositions (USPs) (Reeves, 1961) of product entities. Specifically, e-commerce KB stores standardized product attribute information, product aspects are features that play a crucial role in understanding product information, product categories are the backbones for constructing taxonomies for organization, and USPs are the essence of what differentiates a product from its competitors. K-PLUG learns these types of knowledge into a unified PLM, enhancing performances for various language understanding and generation tasks. To effectively learn these four types of valuable domain-specific knowledge in K-PLUG, we proposed five n"
2021.findings-emnlp.1,D15-1044,0,0.0322804,"UGE-1 (RG-1), ROUGE-2 (RG-2), and ROUGE-L (RG-L) F1 scores (Lin and Hovy, 2003). K-PLUG clearly performs better than other text-based methods. E-commerce knowledge plays a significant role in the abstractive product summarization task, and domain-specific pre-training data and knowledge-injected pre-training objectives both enhance the model. K-PLUG achieves 4.2.2 Abstractive Product Summarization Task Definition. Abstractive product summarization task aims to capture the most attractive information of a product that resonates with potential purchasers. Similar to the text summarization task (Rush et al., 2015; Nallapati et al., 2016; Li 6 Model Home Applications RG-1 RG-2 RG-L RG-1 Clothing RG-2 RG-L Cases&Bags RG-1 RG-2 RG-L LexRank Seq2seq MASS PG 24.06 21.57 28.19 31.11 10.01 7.18 8.02 10.93 18.19 17.61 18.73 21.11 26.87 23.05 26.73 29.11 9.01 6.84 8.03 9.24 17.76 16.82 17.72 19.92 27.09 23.18 27.19 31.31 9.87 6.94 9.03 10.27 18.03 17.29 18.17 21.79 Aspect MMPG* 34.36 12.52 22.35 31.93 11.09 21.54 33.78 12.51 22.43 C-PLUG E-PLUG K-PLUG 32.75 33.11 33.56 11.62 12.07 12.50 21.76 22.01 22.15 31.73 32.61 33.00 10.86 11.03 11.24 20.37 20.98 21.43 32.04 32.37 33.87 10.75 11.14 11.83 21.85 21.98 22.35"
2021.findings-emnlp.1,P19-1139,0,0.291579,"quence-to-sequence models to jointly pretrain for both the encoder and the decoder. While these PLMs can learn rich semantic patterns from raw text data and thereby enhance downstream NLP applications, many of them do not explicitly model domain-specific knowledge. As a result, they may not be as sufficient for capturing human-curated or domain-specific knowledge that is necessary for tasks in a certain domain, such as tasks in e-commerce scenarios. In order to overcome this limitation, several recent studies have proposed to enrich PLMs with explicit knowledge, including knowledge base (KB) (Zhang et al., 2019; Peters et al., 2019; Xiong et al., 2020; Wang et al., 2019, 2020), lexical relation (Lauscher et al., 2019; Wang et al., 2020), word sense (Levine et al., 2020), part-of-speech tag (Ke et al., 2020), and sentiment polarity (Ke et al., 2020; Tian et al., 2020). However, these methods only integrate knowledge into the encoder, and the decoding process in many NLG tasks benefits little from these knowledge. To mitigate this problem, we propose a Knowledge-injected Pre-trained Language model that is suitable for both Natural Language Understanding and Generation (K-PLUG). Different from existing"
2021.findings-emnlp.160,D18-1241,0,0.280843,"he length constraint. For examusually split into multiple chunks that are independently read. It results in the reading ple, each instance in open-domain MRC usually field being limited to individual chunks withconsists of a collection of passages, such as Triviout information collaboration for long docuaQA (Joshi et al., 2017), one of the most popular ment machine reading comprehension. To adopen-domain MRC datasets, containing 6,589 todress this problem, we propose RoR, a readkens on average. In addition, for conversational over-read method, which expands the reading MRC task, such as QuAC (Choi et al., 2018), exfield from chunk to document. Specifically, isting methods incorporate conversation history by RoR includes a chunk reader and a document reader. The former first predicts a set of reprepending the previous utterances to the current gional answers for each chunk, which are then question, which is packed with the document into compacted into a highly-condensed version of a length input (707 tokens on average). the original document, guaranteeing to be enTo handle a long document that exceeds the coded once. The latter further predicts the length constraint, a commonly used approach is globa"
2021.findings-emnlp.160,N19-1423,0,0.0602507,"Missing"
2021.findings-emnlp.160,N18-1202,0,0.0254128,"Missing"
2021.findings-emnlp.160,2020.coling-main.247,0,0.0181694,"l., 2015; Trischler et al., 2017; Rajpurkar et al., 2016, 2018). The best performing models in various MRC tasks are commonly based on the pre-trained language models (PLMs) within the typical encoding limit of 512 tokens. However, the input sequence in some MRC tasks usually exceeds the length limit, such as conversational MRC and open-domain MRC. Conversational MRC, which extends the traditional single-turn MRC, requires the models 3 Approach to additionally understand the conversation his3.1 Task Formulation tory (Reddy et al., 2019; Choi et al., 2018; Gao et al., 2018; Huang et al., 2019; Gupta et al., 2020) Given a document P , a question q, the task of MRC as dialog and conversational recommendation sys- is to predict an answer span y from P based on the tems (Lu et al., 2021). A straightforward but effec- comprehension of P and q. If q is an unanswerable tive approach of modeling the history is to prepend question, the QuAC dataset requires the model to the previous dialogs to the current question, which give an unanswerable tag as the final answer. To will compose a lengthy input sequence with the model the dialog history in QuAC, we prepend relatively long document (Gong et al., 2020). previ"
2021.findings-emnlp.160,P17-1147,0,0.336832,"els consist of a stack of transformer blocks machine reading comprehension. However, that only encode a length-limited sequence (e.g., due to the constraint of encoding length (e.g., 512). However, the input sequences in some MRC 512 WordPiece tokens), a long document is tasks may exceed the length constraint. For examusually split into multiple chunks that are independently read. It results in the reading ple, each instance in open-domain MRC usually field being limited to individual chunks withconsists of a collection of passages, such as Triviout information collaboration for long docuaQA (Joshi et al., 2017), one of the most popular ment machine reading comprehension. To adopen-domain MRC datasets, containing 6,589 todress this problem, we propose RoR, a readkens on average. In addition, for conversational over-read method, which expands the reading MRC task, such as QuAC (Choi et al., 2018), exfield from chunk to document. Specifically, isting methods incorporate conversation history by RoR includes a chunk reader and a document reader. The former first predicts a set of reprepending the previous utterances to the current gional answers for each chunk, which are then question, which is packed wi"
2021.findings-emnlp.160,Q19-1026,0,0.0128756,"cument reading limitation in existing models. • We propose a voting strategy to rerank the answers from regional chunks and a condensed document, overcoming the major drawback in aggregating the answers from different sources. • Extensive experiments on long document benchmarks are conducted to verify the effectiveness of our model. Especially on the QuAC dataset, our model achieves state-of-the-art results over all evaluation metrics on the leaderboard. 2 Related Work Open-domain MRC is a task of answering questions using a large collection of passages (Joshi et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019). The main challenge of this task is that the sequence length of multiple passages relevant to each question far exceeds the length limit of 512 tokens. For example, documents in TriviaQA (Joshi et al., 2017) contain 6,589 tokens on average. To enable the PLMs to encode long documents, a common approach is to chunk the document into overlapping chunks of length 512, then process each chunk separately, which inevitably causes the two problems aforementioned. Another intuitive approach is to increase the encoding length of the PLMs. For example, the recently proposed PLMs Longformer (Beltagy et"
2021.findings-emnlp.160,2021.ccl-1.108,0,0.0358678,"Missing"
2021.findings-emnlp.160,2021.findings-acl.99,1,0.825043,"Missing"
2021.findings-emnlp.160,P18-2124,0,0.0189942,".7 73.4 17.8 78.2 65.0 90.0 Table 2: Results on the development set of QuAC. After the operations above, the longest condensed document contains 471 tokens in TriviaQA and 184 tokens in QuAC. When RoR is adapted to other datasets, the length of the condensed documents can be guaranteed to be shorter than 512 as long as the parameters in the above four operations are adjusted correspondingly. In order to improve the model performance, some data augmentations are applied to better train the model. Specifically, ELECTRA is finetuned on other MRC datasets before fine-tuned on QuAC, such as SQuAD (Rajpurkar et al., 2018) and CoQA (Reddy et al., 2019), hoping to transfer the knowledge in other datasets to our model. Experimental results show that CoQA has a much higher lifting effect than SQuAD. This is because both CoQA and QuAC are conversational MRC datasets, while SQuAD is a single-turn MRC dataset. The answers in CoQA are free-form and generally short (average answer length = 2.7), which is quite different from QuAC (average answer length = 15.1). As a result, we choose the rationale sentence of the gold span in CoQA as the prediction target. Model F1 HEQ-Q HEQ-D Human 81.1 100 100 ELECTRA-RoR 74.9 72.2 1"
2021.findings-emnlp.160,D16-1264,0,0.322438,"f the answers are not comparable as they are not 1 Introduction globally normalized over chunks. To address these problems, we propose RoR, a The task of machine reading comprehension read-over-read pipeline, which is able to expand (MRC), which requires machines to answer questhe reading field from chunk-level to documenttions through reading and understanding a given document, has been a growing research field in nat- level. RoR contains a chunk reader and a document ural language understanding (Hermann et al., 2015; reader, both of which are based on the pre-trained Trischler et al., 2017; Rajpurkar et al., 2016, 2018; model. Specifically, the chunk reader first predicts the regional answers from each chunk. These anJoshi et al., 2017; Choi et al., 2018). swers are then compacted into a new document Transformer-based pre-trained models have been widely proven to be effective in a range of natu- through a minimum span coverage algorithm guaranteeing that its sequence length is shorter than the ∗ Corresponding Author: baojunwei001@gmail.com limitation (i.e., 512). By this means, all regional 1 https://quac.ai/ 2 answers can be normalized in one document. This Our code is available at https://github.com"
2021.findings-emnlp.160,Q19-1016,0,0.0732638,"the correct answers to questions after reading a given passage (Hermann et al., 2015; Trischler et al., 2017; Rajpurkar et al., 2016, 2018). The best performing models in various MRC tasks are commonly based on the pre-trained language models (PLMs) within the typical encoding limit of 512 tokens. However, the input sequence in some MRC tasks usually exceeds the length limit, such as conversational MRC and open-domain MRC. Conversational MRC, which extends the traditional single-turn MRC, requires the models 3 Approach to additionally understand the conversation his3.1 Task Formulation tory (Reddy et al., 2019; Choi et al., 2018; Gao et al., 2018; Huang et al., 2019; Gupta et al., 2020) Given a document P , a question q, the task of MRC as dialog and conversational recommendation sys- is to predict an answer span y from P based on the tems (Lu et al., 2021). A straightforward but effec- comprehension of P and q. If q is an unanswerable tive approach of modeling the history is to prepend question, the QuAC dataset requires the model to the previous dialogs to the current question, which give an unanswerable tag as the final answer. To will compose a lengthy input sequence with the model the dialog h"
2021.findings-emnlp.160,W17-2623,0,0.167266,"on (May 17th, 2021)2 . of the answers are not comparable as they are not 1 Introduction globally normalized over chunks. To address these problems, we propose RoR, a The task of machine reading comprehension read-over-read pipeline, which is able to expand (MRC), which requires machines to answer questhe reading field from chunk-level to documenttions through reading and understanding a given document, has been a growing research field in nat- level. RoR contains a chunk reader and a document ural language understanding (Hermann et al., 2015; reader, both of which are based on the pre-trained Trischler et al., 2017; Rajpurkar et al., 2016, 2018; model. Specifically, the chunk reader first predicts the regional answers from each chunk. These anJoshi et al., 2017; Choi et al., 2018). swers are then compacted into a new document Transformer-based pre-trained models have been widely proven to be effective in a range of natu- through a minimum span coverage algorithm guaranteeing that its sequence length is shorter than the ∗ Corresponding Author: baojunwei001@gmail.com limitation (i.e., 512). By this means, all regional 1 https://quac.ai/ 2 answers can be normalized in one document. This Our code is availab"
2021.findings-emnlp.160,P18-1178,0,0.0158503,"Longformer (Beltagy et al., 2020) and Big bird (Zaheer et al., 2021), specifically for long document modeling, have extended the encoding length from 512 to 4,096. However, their encoding length is fixed. The two problems caused by chunking still exist when encoding the sequences longer than 4,096. In contrast, our proposed model RoR is flexible which is able to encode sequences of arbitrary length. Moreover, RoR is assembleable and its encoder can be replaced with any PLMs, such as BERT and Longformer. Theoretically, hierarchical models can be adapted to long document MRC (Yang et al., 2016; Wang et al., 2018; Yang et al., 2020). However, deploying the large transformer-based PLMs as the encoders of hierarchical models can be prohibitively costly. Typically, hierarchical models parallelly encode the splitted chunks of a long document with multiple transformers, which requires extremely large GPU support. In contrast, RoR only needs to read a chunk at each encoding process, then gradually predict all answers from chunk to document. Therefore, RoR is able to deal with a long document without consuming too much computing resources and can be more widely used than hierarchical models. MRC is a fundame"
2021.findings-emnlp.160,N16-1174,1,0.439032,"ntly proposed PLMs Longformer (Beltagy et al., 2020) and Big bird (Zaheer et al., 2021), specifically for long document modeling, have extended the encoding length from 512 to 4,096. However, their encoding length is fixed. The two problems caused by chunking still exist when encoding the sequences longer than 4,096. In contrast, our proposed model RoR is flexible which is able to encode sequences of arbitrary length. Moreover, RoR is assembleable and its encoder can be replaced with any PLMs, such as BERT and Longformer. Theoretically, hierarchical models can be adapted to long document MRC (Yang et al., 2016; Wang et al., 2018; Yang et al., 2020). However, deploying the large transformer-based PLMs as the encoders of hierarchical models can be prohibitively costly. Typically, hierarchical models parallelly encode the splitted chunks of a long document with multiple transformers, which requires extremely large GPU support. In contrast, RoR only needs to read a chunk at each encoding process, then gradually predict all answers from chunk to document. Therefore, RoR is able to deal with a long document without consuming too much computing resources and can be more widely used than hierarchical model"
2021.findings-emnlp.160,D19-5812,0,0.0315881,"Missing"
2021.naacl-main.229,D17-1047,0,0.0227603,"e graph. • Our GraphMerge RGAT model outperforms recent state-of-the-art work on three benchmark datasets (Laptop and Restaurant reviews from SemEval 2014 and the ACL 14 Twitter dataset). It also outperforms its single-parse counterparts as well as other ensemble techniques. 2 Related Work Much recent work on aspect-level sentiment classification has focused on applying attention mechanisms (e.g., co-attention, self attention, and hierarchical attention) to sequence models such recurrent neural networks (RNNs) (Tang et al., 2015, 2016; Liu and Zhang, 2017; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). In a similar vein, pretrained transformer language models such as BERT (Devlin et al., 2018) have also been applied to this task, which operates directly on word sequences (Song et al., 2019; Xu et al., 2019; Rietzler et al., 2019). In parallel, researchers have also found syntactic information to be helpful for this task, and incorporated it into aspect-level sentiment classification models in the form of dependency trees (Dong et al., 2014; He et al., 2018) as well as constituency trees (Nguyen and Shirai, 2015). More recently, r"
2021.naacl-main.229,P14-2009,0,0.248697,"(RNNs) (Tang et al., 2015, 2016; Liu and Zhang, 2017; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). In a similar vein, pretrained transformer language models such as BERT (Devlin et al., 2018) have also been applied to this task, which operates directly on word sequences (Song et al., 2019; Xu et al., 2019; Rietzler et al., 2019). In parallel, researchers have also found syntactic information to be helpful for this task, and incorporated it into aspect-level sentiment classification models in the form of dependency trees (Dong et al., 2014; He et al., 2018) as well as constituency trees (Nguyen and Shirai, 2015). More recently, researchers have developed robust dependency-based models with the help of GNNs that operate either directly on dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019), as well as reshaped dependency trees that center around aspect terms (Wang et al., 2020b). While most recent work stack GNNs on top of BERT models, Tang et al. (2020) have also reported gains by jointly learning the two with a mutual biaffine attention mechanism. Despite the success of these dependency-based models"
2021.naacl-main.229,D18-1380,0,0.0312535,"Missing"
2021.naacl-main.229,N19-1259,0,0.146758,"Missing"
2021.naacl-main.229,P17-1044,0,0.0266605,"which aims to iden- of-the-art dependency parsers usually struggle to predict flawless parse trees especially in out-oftify the sentiment polarity (e.g., positive, negative or neutral) of a specific aspect term in a sentence. domain settings. This poses great challenge to dependency-based methods that rely on these parse For example, in “The exterior, unlike the food, is unwelcoming.”, the polarities of aspect terms “exte- trees—the added benefit from syntactic structure rior” and “food” are negative and positive, respec- does not always prevail the noise introduced by model-predicted parses (He et al., 2017; Sachan tively. This task has many applications, such as et al., 2021). assisting customers to filter online reviews or make purchase decisions on e-commerce websites. In this paper, we propose GraphMerge, a graph Recent studies have shown that syntactic infor- ensemble technique to help dependency-based modmation such as dependency trees is very effec- els mitigate the effect of parsing errors. Our techtive in capturing long-range syntactic relations that nique is based on the observation that different are obscure from the surface form (Zhang et al., parsers, especially ones with different"
2021.naacl-main.229,C18-1096,0,0.0161964,"., 2015, 2016; Liu and Zhang, 2017; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). In a similar vein, pretrained transformer language models such as BERT (Devlin et al., 2018) have also been applied to this task, which operates directly on word sequences (Song et al., 2019; Xu et al., 2019; Rietzler et al., 2019). In parallel, researchers have also found syntactic information to be helpful for this task, and incorporated it into aspect-level sentiment classification models in the form of dependency trees (Dong et al., 2014; He et al., 2018) as well as constituency trees (Nguyen and Shirai, 2015). More recently, researchers have developed robust dependency-based models with the help of GNNs that operate either directly on dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019), as well as reshaped dependency trees that center around aspect terms (Wang et al., 2020b). While most recent work stack GNNs on top of BERT models, Tang et al. (2020) have also reported gains by jointly learning the two with a mutual biaffine attention mechanism. Despite the success of these dependency-based models, they are usually"
2021.naacl-main.229,D19-1549,0,0.177889,"The exterior , unlike the food , is unwelcoming . Figure 1: An example where an incorrect parse (above the sentence) can mislead aspect-level sentiment classification for the term “food” by connecting it to the negative sentiment word “unwelcoming” by mistake. Although having its own issues, the parse below correctly captures the main syntactic structure between the aspect terms “exterior”, “food” and the sentiment word, and is more likely to lead to a correct prediction. graph neural network (GNN) (Kipf and Welling, 2016) model over dependency trees to aspect-level sentiment classification (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020b), which demonstrate that syntactic information is helpful for associating the aspect term with relevant opinion words more directly for increased robustness in sentiment classification. However, existing approaches are vulnerable to parsing errors (Wang et al., 2020b). For example, in Figure 1, the blue parse above the sentence can mislead models to predict negative sentiment for the aspect term “food” with its direct association to 1 Introduction “unwelcoming”. Despite their high edge-wise parsAspect-level sentiment classification is"
2021.naacl-main.229,P18-1249,0,0.0464693,"Missing"
2021.naacl-main.229,K18-1018,0,0.015169,"ng errors does not require any additional computational cost, since we are still applying GNNs to a single graph with the same number of nodes. Last but not least, GraphMerge helps prevent GNNs from overfitting by limiting over-parameterization. Aside from keeping the GNN computation over a single graph to avoid separate parameterization for each parse tree, GraphMerge also introduces more edges in the graph when parses differ, which reduces the diameter of graphs. As a result, fewer layers of GNNs are needed to learn good representations from the graph, alleviating the oversmoothing problem (Li et al., 2018b). To summarize, the main contribution of our work are the following: ensemble graph enables the model to learn from noisy graph and select correct edges among nodes at no additional computational cost. • We retain the syntactic dependency information in the original trees by parameterizing parent-tochildren and children-to-parent edges separately, which improves the performance of the RGAT model on the ensemble graph. • Our GraphMerge RGAT model outperforms recent state-of-the-art work on three benchmark datasets (Laptop and Restaurant reviews from SemEval 2014 and the ACL 14 Twitter dataset"
2021.naacl-main.229,P18-1087,0,0.125201,"Missing"
2021.naacl-main.229,E17-2091,0,0.0221694,"improves the performance of the RGAT model on the ensemble graph. • Our GraphMerge RGAT model outperforms recent state-of-the-art work on three benchmark datasets (Laptop and Restaurant reviews from SemEval 2014 and the ACL 14 Twitter dataset). It also outperforms its single-parse counterparts as well as other ensemble techniques. 2 Related Work Much recent work on aspect-level sentiment classification has focused on applying attention mechanisms (e.g., co-attention, self attention, and hierarchical attention) to sequence models such recurrent neural networks (RNNs) (Tang et al., 2015, 2016; Liu and Zhang, 2017; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). In a similar vein, pretrained transformer language models such as BERT (Devlin et al., 2018) have also been applied to this task, which operates directly on word sequences (Song et al., 2019; Xu et al., 2019; Rietzler et al., 2019). In parallel, researchers have also found syntactic information to be helpful for this task, and incorporated it into aspect-level sentiment classification models in the form of dependency trees (Dong et al., 2014; He et al., 2018) as well as consti"
2021.naacl-main.229,P14-5010,0,0.00438897,"Missing"
2021.naacl-main.229,D15-1298,0,0.0597109,"Missing"
2021.naacl-main.229,2020.acl-demos.14,1,0.889502,"Missing"
2021.naacl-main.229,2021.eacl-main.228,1,0.823451,"Missing"
2021.naacl-main.229,D19-1569,0,0.376313,"ming . Figure 1: An example where an incorrect parse (above the sentence) can mislead aspect-level sentiment classification for the term “food” by connecting it to the negative sentiment word “unwelcoming” by mistake. Although having its own issues, the parse below correctly captures the main syntactic structure between the aspect terms “exterior”, “food” and the sentiment word, and is more likely to lead to a correct prediction. graph neural network (GNN) (Kipf and Welling, 2016) model over dependency trees to aspect-level sentiment classification (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020b), which demonstrate that syntactic information is helpful for associating the aspect term with relevant opinion words more directly for increased robustness in sentiment classification. However, existing approaches are vulnerable to parsing errors (Wang et al., 2020b). For example, in Figure 1, the blue parse above the sentence can mislead models to predict negative sentiment for the aspect term “food” with its direct association to 1 Introduction “unwelcoming”. Despite their high edge-wise parsAspect-level sentiment classification is a fine- ing performance on standard be"
2021.naacl-main.229,C16-1311,0,0.0749951,"Missing"
2021.naacl-main.229,D16-1021,0,0.0445682,"Missing"
2021.naacl-main.229,2020.acl-main.588,0,0.206746,"tactic information to be helpful for this task, and incorporated it into aspect-level sentiment classification models in the form of dependency trees (Dong et al., 2014; He et al., 2018) as well as constituency trees (Nguyen and Shirai, 2015). More recently, researchers have developed robust dependency-based models with the help of GNNs that operate either directly on dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019), as well as reshaped dependency trees that center around aspect terms (Wang et al., 2020b). While most recent work stack GNNs on top of BERT models, Tang et al. (2020) have also reported gains by jointly learning the two with a mutual biaffine attention mechanism. Despite the success of these dependency-based models, they are usually vulnerable to parse errors since they rely on a single parser. Tu et al. • We propose a GraphMerge technique to combine (2012) used a dependency forest to combine muldependency parsing trees from different parsers to tiple dependency trees, however they tackled the improve model robustness to parsing errors. The sentence-level sentiment analysis task instead, and 2885 pos neu neg c Classification d a b e Pooling Edge Union b c"
2021.naacl-main.229,2020.acl-main.295,0,0.814831,"n example where an incorrect parse (above the sentence) can mislead aspect-level sentiment classification for the term “food” by connecting it to the negative sentiment word “unwelcoming” by mistake. Although having its own issues, the parse below correctly captures the main syntactic structure between the aspect terms “exterior”, “food” and the sentiment word, and is more likely to lead to a correct prediction. graph neural network (GNN) (Kipf and Welling, 2016) model over dependency trees to aspect-level sentiment classification (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020b), which demonstrate that syntactic information is helpful for associating the aspect term with relevant opinion words more directly for increased robustness in sentiment classification. However, existing approaches are vulnerable to parsing errors (Wang et al., 2020b). For example, in Figure 1, the blue parse above the sentence can mislead models to predict negative sentiment for the aspect term “food” with its direct association to 1 Introduction “unwelcoming”. Despite their high edge-wise parsAspect-level sentiment classification is a fine- ing performance on standard benchmarks, stategrai"
2021.naacl-main.229,P18-1088,0,0.0126494,"ance of the RGAT model on the ensemble graph. • Our GraphMerge RGAT model outperforms recent state-of-the-art work on three benchmark datasets (Laptop and Restaurant reviews from SemEval 2014 and the ACL 14 Twitter dataset). It also outperforms its single-parse counterparts as well as other ensemble techniques. 2 Related Work Much recent work on aspect-level sentiment classification has focused on applying attention mechanisms (e.g., co-attention, self attention, and hierarchical attention) to sequence models such recurrent neural networks (RNNs) (Tang et al., 2015, 2016; Liu and Zhang, 2017; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). In a similar vein, pretrained transformer language models such as BERT (Devlin et al., 2018) have also been applied to this task, which operates directly on word sequences (Song et al., 2019; Xu et al., 2019; Rietzler et al., 2019). In parallel, researchers have also found syntactic information to be helpful for this task, and incorporated it into aspect-level sentiment classification models in the form of dependency trees (Dong et al., 2014; He et al., 2018) as well as constituency trees (Nguye"
2021.naacl-main.229,2020.emnlp-main.292,0,0.0421324,"Missing"
2021.naacl-main.229,N19-1242,0,0.0399926,"Missing"
2021.naacl-main.229,D19-1464,0,0.411246,"Missing"
2021.naacl-main.229,D18-1244,1,0.828178,"Missing"
2021.naacl-main.455,D09-1027,0,0.316718,"significantly outperforms the strong baselines for both present and absent keyphrases generation. Furthermore, we extend SGG to a title generation task which indicates its extensibility in natural language generation tasks. 1 Select <Present keyphrase&gt; collaborative filtering </Present keyphrase&gt; Selection-Guided Generation <Absent keyphrase&gt; customer relationship management </Absent keyphrase&gt; Figure 1: An example of keyphrase prediction by SGG. Introduction Automatic keyphrase prediction recommends a set of representative phrases that are related to the main topics discussed in a document (Liu et al., 2009). Since keyphrases can provide a high-level topic description of a document, they are beneficial for a wide range of natural language processing (NLP) tasks, such as information extraction (Wan and Xiao, 2008), text summarization (Wang and Cardie, 2013) and question generation (Subramanian et al., 2018). Existing methods for keyphrase prediction can be categorized into extraction and generation approaches. Specifically, keyphrase extraction methods identify important consecutive words from a given document as keyphrases, which means that the extracted keyphrases (denoted as present keyphrases)"
2021.naacl-main.455,D09-1137,0,0.0192803,", and the experiment results indicate the extensibility and effectiveness of our SGG approach on generation tasks. 2 Related Work As mentioned in Section 1, the extraction and generation methods are two different research directions in the field of keyphrase prediction. The existing extraction methods can be broadly classified into supervised and unsupervised approaches. The supervised approaches treat keyphrase extraction as a binary classification task, which train the models with the features of labeled keyphrases to determine whether a candidate phrase is a keyphrase (Witten et al., 1999; Medelyan et al., 2009; Gollapalli et al., 2017). In contrast, the unsupervised approaches treat keyphrase extraction as a ranking task, scoring each candidate using some different ranking metrics, such as clustering (Liu et al., 2009), or graph-based ranking (Mihalcea and Tarau, 2004; Wang et al., 2014; Gollapalli and Caragea, 2014; Zhang et al., 2017). This work is mainly related to keyphrase generation approaches which have demonstrated good performance on keyphrase prediction task. Following CopyRNN (Meng et al., 2017), several extensions have been proposed to boost the generation capability. In CopyRNN, model"
2021.naacl-main.455,P17-1054,0,0.538445,"me from the given document. However, some keyphrases (denoted as absent keyphrases) of a given document do not match any contiguous subsequence but are highly semantically related to the source text. The extraction methods fail to predict these absent keyphrases. Therefore, generation methods have been proposed to produce a keyphrase verbatim from a predefined vocabulary, no matter whether the generated keyphrase appears in the source text. Compared with conventional extraction methods, generation methods have the ability of generating absent keyphrases as well as present keyphrases. CopyRNN (Meng et al., 2017) is the first to employ the sequence-to-sequence (Seq2Seq) framework (Sutskever et al., 2014) with the copying mechanism (Gu et al., 2016) to generate keyphrases for the given documents. Following the CopyRNN, several Seq2Seq-based keyphrase generation approaches have been proposed to improve the generation performance (Chen et al., 2018; Ye and Wang, 2018; Chen et al., 2019; Zhao and Zhang, 2019; Wang et al., 2019; Yuan et al., 2020). All these existing methods generate present and absent keyphrases synchronously without ex5717 Proceedings of the 2021 Conference of the North American Chapter"
2021.naacl-main.455,D18-1439,0,0.450895,"nput, ignoring the leading role of the title. To address this deficiency, Chen et al. (2019) proposed a title-guided Seq2Seq network to sufficiently utilize the already summarized information in title. In addition, some research attempts to introduce external knowledge into keyphrase generation, such as syntactic constraints (Zhao and Zhang, 2019) and latent topics (Wang et al., 2019). These approaches do not consider the one-tomany relationship between the input text and target keyphrases, and thus fail to model the correlation among the multiple target keyphrases. To overcome this drawback, Chen et al. (2018) incorporated the review mechanism into keyphrase generation and proposed a model CorrRNN with correlation constraints. Similarly, SGG separately models one-to-many relationship between the input text and present keyphrases and absent keyphrases. To avoid generating duplicate keyphrases, Chen et al. (2020) proposed an exclusive hierarchical decoding framework that includes a hierarchical decoding process and either a soft or a hard exclusion mechanism. For the same purpose, our method deploys a guider to avoid the generator generating duplicate present keyphrases. Last but most important, all"
2021.naacl-main.455,2020.acl-main.103,0,0.361639,"Missing"
2021.naacl-main.455,P16-1154,0,0.66081,"nce but are highly semantically related to the source text. The extraction methods fail to predict these absent keyphrases. Therefore, generation methods have been proposed to produce a keyphrase verbatim from a predefined vocabulary, no matter whether the generated keyphrase appears in the source text. Compared with conventional extraction methods, generation methods have the ability of generating absent keyphrases as well as present keyphrases. CopyRNN (Meng et al., 2017) is the first to employ the sequence-to-sequence (Seq2Seq) framework (Sutskever et al., 2014) with the copying mechanism (Gu et al., 2016) to generate keyphrases for the given documents. Following the CopyRNN, several Seq2Seq-based keyphrase generation approaches have been proposed to improve the generation performance (Chen et al., 2018; Ye and Wang, 2018; Chen et al., 2019; Zhao and Zhang, 2019; Wang et al., 2019; Yuan et al., 2020). All these existing methods generate present and absent keyphrases synchronously without ex5717 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5717–5726 June 6–11, 2021. ©2021 Association for Comp"
2021.naacl-main.455,P06-1068,0,0.642878,"llowing equations: cat = L X αia,t hi (9) i=1 αa,t = softmax(ua,t ) ua,t i = (10) VaT tanh(Wa [sat ; hi ; r] + ba ) (11) contains a title and an abstract of a scientific publication as source text, and author-assigned keywords as target keyphrases. We randomly select the example which contains at least one present keyphrase to construct the training set. Then, a validation set containing 500 samples will be selected from the remaining examples. In order to evaluate our proposed model comprehensively, we test models on four widely used public datasets from the scientific domain, namely Inspec (Hulth and Megyesi, 2006), Krapivin (Krapivin et al., 2009), SemEval2010 (Kim et al., 2010) and NUS (Nguyen and Kan, 2007), the statistic information of which are summarized in Table 2. Dataset where Va , Wa and ba are learnable parameters. r is a vector produced by the guider. The generation probability pgen at time step t is computed as: Test a pgen = σ(Wgen [cat ; sat ; emb(yt−1 )] + bgen ) (12) where Wgen and bgen are learnable parameters, a ) σ(·) represents a sigmoid function and emb(yt−1 a . In addition, p is the embedding of yt−1 gen in formula (7) is used as a soft switch to choose either generating words ove"
2021.naacl-main.455,S10-1004,0,0.252658,"i = (10) VaT tanh(Wa [sat ; hi ; r] + ba ) (11) contains a title and an abstract of a scientific publication as source text, and author-assigned keywords as target keyphrases. We randomly select the example which contains at least one present keyphrase to construct the training set. Then, a validation set containing 500 samples will be selected from the remaining examples. In order to evaluate our proposed model comprehensively, we test models on four widely used public datasets from the scientific domain, namely Inspec (Hulth and Megyesi, 2006), Krapivin (Krapivin et al., 2009), SemEval2010 (Kim et al., 2010) and NUS (Nguyen and Kan, 2007), the statistic information of which are summarized in Table 2. Dataset where Va , Wa and ba are learnable parameters. r is a vector produced by the guider. The generation probability pgen at time step t is computed as: Test a pgen = σ(Wgen [cat ; sat ; emb(yt−1 )] + bgen ) (12) where Wgen and bgen are learnable parameters, a ) σ(·) represents a sigmoid function and emb(yt−1 a . In addition, p is the embedding of yt−1 gen in formula (7) is used as a soft switch to choose either generating words over vocabulary or copying words from source text based on distributi"
2021.naacl-main.455,W04-3252,0,0.257251,"rediction. The existing extraction methods can be broadly classified into supervised and unsupervised approaches. The supervised approaches treat keyphrase extraction as a binary classification task, which train the models with the features of labeled keyphrases to determine whether a candidate phrase is a keyphrase (Witten et al., 1999; Medelyan et al., 2009; Gollapalli et al., 2017). In contrast, the unsupervised approaches treat keyphrase extraction as a ranking task, scoring each candidate using some different ranking metrics, such as clustering (Liu et al., 2009), or graph-based ranking (Mihalcea and Tarau, 2004; Wang et al., 2014; Gollapalli and Caragea, 2014; Zhang et al., 2017). This work is mainly related to keyphrase generation approaches which have demonstrated good performance on keyphrase prediction task. Following CopyRNN (Meng et al., 2017), several extensions have been proposed to boost the generation capability. In CopyRNN, model training heavily relies on large amount of labeled data, which is often unavailable especially for the new domains. To address this problem, Ye and Wang (2018) proposed 5718 a semi-supervised keyphrase generation model that utilizes both abundant unlabeled data a"
2021.naacl-main.455,P17-1099,0,0.706576,"to guide the generation. Specifically, our SGG is implemented with a hierarchical neural network which performs Seq2Seq learning by applying a multi-task learning strategy. This network consists of a selector at low layer, a generator at high layer, and a guider at middle layer for information transfer. The selector generates present keyphrases through a pointing mechanism (Vinyals et al., 2015), which adopts attention distributions to select a sequence of words from the source text as output. The generator further generates the absent keyphrases through a pointing-generating (PG) mechanism (See et al., 2017). Since present keyphrases have already been generated by the selector, they should not be generated again by the generator. Therefore, a guider is designed to memorize the generated present keyphrases from the selector, and then fed into the attention module of the generator to constrain it to focus on generating absent keyphrases. We summarize our main contributions as follows: • We propose a SGG approach which models present and absent keyphrase generation separately in different stages, i.e., select, guide, and generate, without sacrificing the end-to-end training through back-propagation."
2021.naacl-main.455,W18-2609,0,0.0190146,"e&gt; Selection-Guided Generation <Absent keyphrase&gt; customer relationship management </Absent keyphrase&gt; Figure 1: An example of keyphrase prediction by SGG. Introduction Automatic keyphrase prediction recommends a set of representative phrases that are related to the main topics discussed in a document (Liu et al., 2009). Since keyphrases can provide a high-level topic description of a document, they are beneficial for a wide range of natural language processing (NLP) tasks, such as information extraction (Wan and Xiao, 2008), text summarization (Wang and Cardie, 2013) and question generation (Subramanian et al., 2018). Existing methods for keyphrase prediction can be categorized into extraction and generation approaches. Specifically, keyphrase extraction methods identify important consecutive words from a given document as keyphrases, which means that the extracted keyphrases (denoted as present keyphrases) must exactly come from the given document. However, some keyphrases (denoted as absent keyphrases) of a given document do not match any contiguous subsequence but are highly semantically related to the source text. The extraction methods fail to predict these absent keyphrases. Therefore, generation me"
2021.naacl-main.455,P16-1008,0,0.240195,"roposed SGG which is implemented with a hierarchical neural network. r= M X αp,t (6) t=1 The final hidden representation hi of the i-th source word is the concatenation of forward and backward → − ← − hidden states, i.e., hi = [hi ; hi ]. where M is the length of present keyphrase sequence. r is an unnormalized distribution over the source words. As the attention distribution of selector is equal to the probability distribution over the source words, r represents the possibility that these words have been generated by the selector. The calculation of guider is inspired by the coverage vector (Tu et al., 2016) that is sequentially updated during the decoding process. In contrast to this, the guider here is a static vector which is capable of memorizing a global information. 3.4 3.6 two directions and outputs a sequence of forward → − hidden states {hi }L i=1 and backward hidden states ← − L {hi }i=1 by iterating the following equations: → − hi = LSTM(xi , hi−1 ) ← − hi = LSTM(xi , hi+1 ) (1) (2) Selector A selector is designed to generate present keyphrase sequences through the pointer mechanism (Vinyals et al., 2015), which adopts the attention distribution as a pointer to select words from the so"
2021.naacl-main.455,P13-1137,0,0.111034,"rase&gt; collaborative filtering </Present keyphrase&gt; Selection-Guided Generation <Absent keyphrase&gt; customer relationship management </Absent keyphrase&gt; Figure 1: An example of keyphrase prediction by SGG. Introduction Automatic keyphrase prediction recommends a set of representative phrases that are related to the main topics discussed in a document (Liu et al., 2009). Since keyphrases can provide a high-level topic description of a document, they are beneficial for a wide range of natural language processing (NLP) tasks, such as information extraction (Wan and Xiao, 2008), text summarization (Wang and Cardie, 2013) and question generation (Subramanian et al., 2018). Existing methods for keyphrase prediction can be categorized into extraction and generation approaches. Specifically, keyphrase extraction methods identify important consecutive words from a given document as keyphrases, which means that the extracted keyphrases (denoted as present keyphrases) must exactly come from the given document. However, some keyphrases (denoted as absent keyphrases) of a given document do not match any contiguous subsequence but are highly semantically related to the source text. The extraction methods fail to predic"
2021.naacl-main.455,P19-1240,0,0.0137029,"ress this problem, Ye and Wang (2018) proposed 5718 a semi-supervised keyphrase generation model that utilizes both abundant unlabeled data and limited labeled data. CopyRNN uses the concatenation of article title and abstract as input, ignoring the leading role of the title. To address this deficiency, Chen et al. (2019) proposed a title-guided Seq2Seq network to sufficiently utilize the already summarized information in title. In addition, some research attempts to introduce external knowledge into keyphrase generation, such as syntactic constraints (Zhao and Zhang, 2019) and latent topics (Wang et al., 2019). These approaches do not consider the one-tomany relationship between the input text and target keyphrases, and thus fail to model the correlation among the multiple target keyphrases. To overcome this drawback, Chen et al. (2018) incorporated the review mechanism into keyphrase generation and proposed a model CorrRNN with correlation constraints. Similarly, SGG separately models one-to-many relationship between the input text and present keyphrases and absent keyphrases. To avoid generating duplicate keyphrases, Chen et al. (2020) proposed an exclusive hierarchical decoding framework that in"
2021.naacl-main.455,D18-1447,0,0.0152981,"ng some different ranking metrics, such as clustering (Liu et al., 2009), or graph-based ranking (Mihalcea and Tarau, 2004; Wang et al., 2014; Gollapalli and Caragea, 2014; Zhang et al., 2017). This work is mainly related to keyphrase generation approaches which have demonstrated good performance on keyphrase prediction task. Following CopyRNN (Meng et al., 2017), several extensions have been proposed to boost the generation capability. In CopyRNN, model training heavily relies on large amount of labeled data, which is often unavailable especially for the new domains. To address this problem, Ye and Wang (2018) proposed 5718 a semi-supervised keyphrase generation model that utilizes both abundant unlabeled data and limited labeled data. CopyRNN uses the concatenation of article title and abstract as input, ignoring the leading role of the title. To address this deficiency, Chen et al. (2019) proposed a title-guided Seq2Seq network to sufficiently utilize the already summarized information in title. In addition, some research attempts to introduce external knowledge into keyphrase generation, such as syntactic constraints (Zhao and Zhang, 2019) and latent topics (Wang et al., 2019). These approaches"
2021.naacl-main.455,2020.acl-main.710,0,0.206226,"ne respectively. † indicates that the model is reimplemented. Method Inspec Krapivin NUS SemEval CopyRNN CopyTrans† 10.0 5.6 20.2 16.9 11.6 8.9 6.7 4.1 CorrRNN† CatSeq 8.5 2.9 15.2 7.4 8.0 3.1 3.5 2.5 SGG 11.0 23.5 12.4 4.9 Table 4: Recall@50 results of predicting absent keyphrases of different models on four datasets. The CorrRNN is retrained following the implementation details in Chen et al. (2018) as they did not report the Recall@50 results. • CorrRNN(one-to-many) (Chen et al., 2018) is an extension of CopyRNN incorporating the coverage mechanism (Tu et al., 2016). • CatSeq(one-to-many) (Yuan et al., 2020) has the same model structure as CopyRNN. The difference is CatSeq is trained by one-to-many. The baseline CopyTrans has not been reported in existing papers and thus is retrained. The implementation of Transformer is base on open source tool OpenNMT 1 . For our experiments of absent keyphrase generation, only generation methods are chosen as baselines. The copying mechanism used in all reimplemented generation models is based on the version (See et al., 2017), which is slightly different from the implementations by version (Meng et al., 2017; Gu et al., 2016). SGG indicates the full version o"
2021.naacl-main.455,P19-1515,1,0.652642,"le especially for the new domains. To address this problem, Ye and Wang (2018) proposed 5718 a semi-supervised keyphrase generation model that utilizes both abundant unlabeled data and limited labeled data. CopyRNN uses the concatenation of article title and abstract as input, ignoring the leading role of the title. To address this deficiency, Chen et al. (2019) proposed a title-guided Seq2Seq network to sufficiently utilize the already summarized information in title. In addition, some research attempts to introduce external knowledge into keyphrase generation, such as syntactic constraints (Zhao and Zhang, 2019) and latent topics (Wang et al., 2019). These approaches do not consider the one-tomany relationship between the input text and target keyphrases, and thus fail to model the correlation among the multiple target keyphrases. To overcome this drawback, Chen et al. (2018) incorporated the review mechanism into keyphrase generation and proposed a model CorrRNN with correlation constraints. Similarly, SGG separately models one-to-many relationship between the input text and present keyphrases and absent keyphrases. To avoid generating duplicate keyphrases, Chen et al. (2020) proposed an exclusive h"
2021.textgraphs-1.8,D17-1047,0,0.0223931,"r each head, and concatenate the GCN outputs w.r.t. different heads as the final word representation for sentiment analysis. The main contributions of this work are summarized as the following: 2 Related Work Capturing the interaction between the aspect term and opinion words is essential for predicting the sentiment polarity towards the aspect term. In recent work, various attention mechanisms, such as co-attention, self-attention and hierarchical attention, were utilized to learn this interaction (Tang et al., 2016; Liu and Zhang, 2017; Li et al., 2018c; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). Specifically, they first encoded the context and the aspect term by recurrent neural networks (RNNs), and then stacked several attention layers to learn the aspect term representations from important context words. After the success of the pre-trained BERT model (Devlin et al., 2018), Song et al. (2019) utilized the pre-trained BERT as the encoder. In the study by (Xu et al., 2019), the task was considered as a review reading comprehension (RRC) problem. RRC datasets were post trained on BERT and then fine-tuned to the aspect-level"
2021.textgraphs-1.8,P14-2009,0,0.0262342,"RT model (Devlin et al., 2018), Song et al. (2019) utilized the pre-trained BERT as the encoder. In the study by (Xu et al., 2019), the task was considered as a review reading comprehension (RRC) problem. RRC datasets were post trained on BERT and then fine-tuned to the aspect-level sentiment classification. Rietzler et al. (2019) utilized millions of extra data based on BERT to help sentiment analysis. The above approaches mainly considered the semantic information. Recent approaches attempted to incorporate the syntactic knowledge to learn the syntax-aware representation of the aspect term. Dong et al. (2014) proposed AdaRNN, which adaptively propagated the sentiments of words to target along the dependency tree in a bottom-up manner. Nguyen and Shirai (2015) extended RNN to obtain the representation of the target aspect by aggregating the syntactic information from the dependency and constituent tree of the sentence. He et al. (2018) proposed to use the distance between the context word and the aspect term along the dependency tree as the attention weight. Some re• We propose a selective attention based GCN (SA84 where {wτ , wτ +1 ..., wτ +m−1 } stand for the aspect term containing m words. First"
2021.textgraphs-1.8,P18-1087,0,0.262817,"passing. In some cases, the most important context words, i.e. opinion words, are more than two-hops away from the aspect term words on the dependency tree. As indicated by Figure 1, there are four hops between the target “Mac OS” and the opinion words “easily picked up” on the dependency tree. This type of cases requires more than two layers of GCN to learn interactions between them. However, previous works show that GCN models with two layers often achieve the best performance (Zhang et al., 2018; Xu et al., 2018), deeper GCNs do not bring additional gain due to the over-smoothing problem (Li et al., 2018b), which makes different nodes have similar representations and lose the distinction among nodes. In order to solve the above problem, we propose a novel selective attention based GCN (SA-GCN) model, which combines the GCN model over dependency trees with a self-attention based sequence model over the sentence. On one hand, the selfattention sequence model enables the direct interaction between an aspect term and its context so that it can take care of the situation where the term is far away from the opinion words on the dependency tree. On the other hand, a top-k attention selection module"
2021.textgraphs-1.8,D18-1380,0,0.0467062,"Missing"
2021.textgraphs-1.8,E17-2091,0,0.0226572,"pply a GCN layer again to integrate information from such sparse graph(s) for each head, and concatenate the GCN outputs w.r.t. different heads as the final word representation for sentiment analysis. The main contributions of this work are summarized as the following: 2 Related Work Capturing the interaction between the aspect term and opinion words is essential for predicting the sentiment polarity towards the aspect term. In recent work, various attention mechanisms, such as co-attention, self-attention and hierarchical attention, were utilized to learn this interaction (Tang et al., 2016; Liu and Zhang, 2017; Li et al., 2018c; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). Specifically, they first encoded the context and the aspect term by recurrent neural networks (RNNs), and then stacked several attention layers to learn the aspect term representations from important context words. After the success of the pre-trained BERT model (Devlin et al., 2018), Song et al. (2019) utilized the pre-trained BERT as the encoder. In the study by (Xu et al., 2019), the task was considered as a review reading comprehension (RRC) problem. RRC"
2021.textgraphs-1.8,N19-1259,0,0.0783644,"(16) Finally, the total training loss is: L = Ls + αLo (17) where α ≥ 0 represents the weight of opinion extraction task. 4 Experiments Data Sets. We evaluate our SA-GCN model on four datasets: Laptop reviews from SemEval 2014 Task 4 (14Lap), Restaurant reviews from SemEval 2014 Task 4 (Pontiki et al., 2014), SemEval 2015 Task 12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016) (14Rest, 15Rest and 16Rest). We remove several examples with “conflict” labels. The statistics of these datasets are listed in Table 1. The opinion words labeling for these four datasets come from (Fan et al., 2019). Baselines. Since BERT(Devlin et al., 2018) model shows significant improvements over many NLP tasks, we directly implement SA-GCN based on BERT and compare with following BERT-based baseline models: 1. BERT-SPC (Song et al., 2019) feeds the sentence and term pair into the BERT model and the BERT outputs are used for prediction. 4.1 2. AEN-BERT (Song et al., 2019) uses BERT as the encoder and employs several attention layers. Experimental Results We present results of the SA-GCN model in two aspects: classification performance and qualitative case study. Classification. Table 2 shows comparis"
2021.textgraphs-1.8,D15-1298,0,0.0660808,"Missing"
2021.textgraphs-1.8,P19-1024,0,0.3478,"elects the most important context words, which effectively sparsifies the fully-connected graph from self-attention. Then we apply another GCN layer on top of this new sparsified graph, such that each of those important context words is directly reachable by the aspect term and the interaction between them could be learned. 3 3.1 3.3 With words representations X as node features and dependency tree as the graph, we employ a GCN to capture syntactic relations between the term node and its neighboring nodes. GCNs have shown to be effective for many NLP applications, such as relation extraction (Guo et al., 2019; Zhang et al., 2018), reading comprehension (Kundu et al., 2019; Tu et al., 2019), and aspect-level sentiment analysis (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019). In each GCN layer, a node aggregates the information from its one-hop neighbors and update its representation. In our case, the graph is represented by the dependency tree, where each word is treated as a single node and its representation is denoted as the node feature. The message passing on the graph can be formulated as follows: Proposed Model H (l) = σ(AH (l−1) W (l−1) ) Overview of the Model The goal of our"
2021.textgraphs-1.8,C18-1096,0,0.0237842,"tilized millions of extra data based on BERT to help sentiment analysis. The above approaches mainly considered the semantic information. Recent approaches attempted to incorporate the syntactic knowledge to learn the syntax-aware representation of the aspect term. Dong et al. (2014) proposed AdaRNN, which adaptively propagated the sentiments of words to target along the dependency tree in a bottom-up manner. Nguyen and Shirai (2015) extended RNN to obtain the representation of the target aspect by aggregating the syntactic information from the dependency and constituent tree of the sentence. He et al. (2018) proposed to use the distance between the context word and the aspect term along the dependency tree as the attention weight. Some re• We propose a selective attention based GCN (SA84 where {wτ , wτ +1 ..., wτ +m−1 } stand for the aspect term containing m words. First, we construct the input as “[CLS] + sentence + [SEP] + term + [SEP]” and feed it into BERT. This input format enables explicit interactions between the whole sentence and the term such that the obtained word representations are term-attended. Then, we use average pooling to summarize the information carried by sub-words from BERT"
2021.textgraphs-1.8,D19-1549,0,0.583811,"mm1994@gmail.com Abstract in identifying the sentiment polarity towards the given term. Most approaches consider the semantic information from the context words and utilize the attention mechanism to learn such interactions. However, it has been shown that syntactic information obtained from dependency parsing is very effective in capturing long-range syntactic relations that are obscure from the surface form (Zhang et al., 2018). A recent popular approach to learn syntaxaware representations is employing graph convolutional networks (GCN) (Kipf and Welling, 2017) model over dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020; Tang et al., 2020), which introduces syntactic inductive biases into the message passing. In some cases, the most important context words, i.e. opinion words, are more than two-hops away from the aspect term words on the dependency tree. As indicated by Figure 1, there are four hops between the target “Mac OS” and the opinion words “easily picked up” on the dependency tree. This type of cases requires more than two layers of GCN to learn interactions between them. However, previous works show that GCN models with two layers often achie"
2021.textgraphs-1.8,S15-2082,0,0.0808747,"ew reading comprehension (RRC) problem. RRC datasets were post trained on BERT and then fine-tuned to the aspect-level sentiment classification. Rietzler et al. (2019) utilized millions of extra data based on BERT to help sentiment analysis. The above approaches mainly considered the semantic information. Recent approaches attempted to incorporate the syntactic knowledge to learn the syntax-aware representation of the aspect term. Dong et al. (2014) proposed AdaRNN, which adaptively propagated the sentiments of words to target along the dependency tree in a bottom-up manner. Nguyen and Shirai (2015) extended RNN to obtain the representation of the target aspect by aggregating the syntactic information from the dependency and constituent tree of the sentence. He et al. (2018) proposed to use the distance between the context word and the aspect term along the dependency tree as the attention weight. Some re• We propose a selective attention based GCN (SA84 where {wτ , wτ +1 ..., wτ +m−1 } stand for the aspect term containing m words. First, we construct the input as “[CLS] + sentence + [SEP] + term + [SEP]” and feed it into BERT. This input format enables explicit interactions between the"
2021.textgraphs-1.8,P19-1263,0,0.0269068,"rsifies the fully-connected graph from self-attention. Then we apply another GCN layer on top of this new sparsified graph, such that each of those important context words is directly reachable by the aspect term and the interaction between them could be learned. 3 3.1 3.3 With words representations X as node features and dependency tree as the graph, we employ a GCN to capture syntactic relations between the term node and its neighboring nodes. GCNs have shown to be effective for many NLP applications, such as relation extraction (Guo et al., 2019; Zhang et al., 2018), reading comprehension (Kundu et al., 2019; Tu et al., 2019), and aspect-level sentiment analysis (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019). In each GCN layer, a node aggregates the information from its one-hop neighbors and update its representation. In our case, the graph is represented by the dependency tree, where each word is treated as a single node and its representation is denoted as the node feature. The message passing on the graph can be formulated as follows: Proposed Model H (l) = σ(AH (l−1) W (l−1) ) Overview of the Model The goal of our proposed SA-GCN model is to predict the sentiment polarity of a"
2021.textgraphs-1.8,S14-2004,0,0.11841,"Missing"
2021.textgraphs-1.8,2020.acl-demos.14,1,0.817008,"BERT+SA-GCN is our proposed SA-GCN model with BERT encoder. Joint SA-GCN refers to joint training of sentiment classification and opinion extraction tasks. Evaluation metrics. We train the model on training set, and evaluate the performance on test set in terms of accuracy and macro-F1 scores which are commonly-used in sentiment analysis (Sun et al., 2019; Tang et al., 2016; Wang et al., 2020). Parameter Setting. During training, we set the learning rate to 10−5 . The batch size is 4. We train the model up to 5 epochs with Adam optimizer. We obtain dependency trees using the Stanford Stanza (Qi et al., 2020). The dimension of BERT output dB is 768. The hidden dimensions are selected from {128, 256, 512}. We apply dropout (Srivastava et al., 2014) and the dropout rate range is [0.1, 0.4]. The L2 regularization is set to 10−6 . We use 1 or 2 SA-GCN blocks in our experiments. We choose k in top-k selection module from {2, 3} to achieve the best performance. For joint training, the weight range of opinion extraction loss is [0.05, 0.15].2 algorithm in the decoding phase. And the loss for opinion extraction task is defined as: ˆ o )) Lo = −log(p(yo |H (16) Finally, the total training loss is: L = Ls +"
2021.textgraphs-1.8,P18-1088,0,0.0132798,"ormation from such sparse graph(s) for each head, and concatenate the GCN outputs w.r.t. different heads as the final word representation for sentiment analysis. The main contributions of this work are summarized as the following: 2 Related Work Capturing the interaction between the aspect term and opinion words is essential for predicting the sentiment polarity towards the aspect term. In recent work, various attention mechanisms, such as co-attention, self-attention and hierarchical attention, were utilized to learn this interaction (Tang et al., 2016; Liu and Zhang, 2017; Li et al., 2018c; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). Specifically, they first encoded the context and the aspect term by recurrent neural networks (RNNs), and then stacked several attention layers to learn the aspect term representations from important context words. After the success of the pre-trained BERT model (Devlin et al., 2018), Song et al. (2019) utilized the pre-trained BERT as the encoder. In the study by (Xu et al., 2019), the task was considered as a review reading comprehension (RRC) problem. RRC datasets were post trained on BERT an"
2021.textgraphs-1.8,N19-1242,0,0.0285329,"ical attention, were utilized to learn this interaction (Tang et al., 2016; Liu and Zhang, 2017; Li et al., 2018c; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). Specifically, they first encoded the context and the aspect term by recurrent neural networks (RNNs), and then stacked several attention layers to learn the aspect term representations from important context words. After the success of the pre-trained BERT model (Devlin et al., 2018), Song et al. (2019) utilized the pre-trained BERT as the encoder. In the study by (Xu et al., 2019), the task was considered as a review reading comprehension (RRC) problem. RRC datasets were post trained on BERT and then fine-tuned to the aspect-level sentiment classification. Rietzler et al. (2019) utilized millions of extra data based on BERT to help sentiment analysis. The above approaches mainly considered the semantic information. Recent approaches attempted to incorporate the syntactic knowledge to learn the syntax-aware representation of the aspect term. Dong et al. (2014) proposed AdaRNN, which adaptively propagated the sentiments of words to target along the dependency tree in a b"
2021.textgraphs-1.8,D19-1569,0,0.626666,"sentiment polarity towards the given term. Most approaches consider the semantic information from the context words and utilize the attention mechanism to learn such interactions. However, it has been shown that syntactic information obtained from dependency parsing is very effective in capturing long-range syntactic relations that are obscure from the surface form (Zhang et al., 2018). A recent popular approach to learn syntaxaware representations is employing graph convolutional networks (GCN) (Kipf and Welling, 2017) model over dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020; Tang et al., 2020), which introduces syntactic inductive biases into the message passing. In some cases, the most important context words, i.e. opinion words, are more than two-hops away from the aspect term words on the dependency tree. As indicated by Figure 1, there are four hops between the target “Mac OS” and the opinion words “easily picked up” on the dependency tree. This type of cases requires more than two layers of GCN to learn interactions between them. However, previous works show that GCN models with two layers often achieve the best performance (Zhang et al.,"
2021.textgraphs-1.8,D19-1464,0,0.178173,"Missing"
2021.textgraphs-1.8,D18-1244,1,0.918203,"Graph Convolutional Networks for Aspect-Level Sentiment Classification Xiaochen Hou∗ , Jing Huang, Guangtao Wang, Peng Qi, Xiaodong He, Bowen Zhou JD AI Research, Mountain View, CA ∗ xclmm1994@gmail.com Abstract in identifying the sentiment polarity towards the given term. Most approaches consider the semantic information from the context words and utilize the attention mechanism to learn such interactions. However, it has been shown that syntactic information obtained from dependency parsing is very effective in capturing long-range syntactic relations that are obscure from the surface form (Zhang et al., 2018). A recent popular approach to learn syntaxaware representations is employing graph convolutional networks (GCN) (Kipf and Welling, 2017) model over dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020; Tang et al., 2020), which introduces syntactic inductive biases into the message passing. In some cases, the most important context words, i.e. opinion words, are more than two-hops away from the aspect term words on the dependency tree. As indicated by Figure 1, there are four hops between the target “Mac OS” and the opinion words “easily picked up”"
2021.textgraphs-1.8,C16-1311,0,0.1644,"raph. Finally, we apply a GCN layer again to integrate information from such sparse graph(s) for each head, and concatenate the GCN outputs w.r.t. different heads as the final word representation for sentiment analysis. The main contributions of this work are summarized as the following: 2 Related Work Capturing the interaction between the aspect term and opinion words is essential for predicting the sentiment polarity towards the aspect term. In recent work, various attention mechanisms, such as co-attention, self-attention and hierarchical attention, were utilized to learn this interaction (Tang et al., 2016; Liu and Zhang, 2017; Li et al., 2018c; Wang et al., 2018; Fan et al., 2018; Chen et al., 2017; Zheng and Xia, 2018; Wang and Lu, 2018; Li et al., 2018a,c). Specifically, they first encoded the context and the aspect term by recurrent neural networks (RNNs), and then stacked several attention layers to learn the aspect term representations from important context words. After the success of the pre-trained BERT model (Devlin et al., 2018), Song et al. (2019) utilized the pre-trained BERT as the encoder. In the study by (Xu et al., 2019), the task was considered as a review reading comprehensio"
2021.textgraphs-1.8,2020.acl-main.588,0,0.321309,"term. Most approaches consider the semantic information from the context words and utilize the attention mechanism to learn such interactions. However, it has been shown that syntactic information obtained from dependency parsing is very effective in capturing long-range syntactic relations that are obscure from the surface form (Zhang et al., 2018). A recent popular approach to learn syntaxaware representations is employing graph convolutional networks (GCN) (Kipf and Welling, 2017) model over dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020; Tang et al., 2020), which introduces syntactic inductive biases into the message passing. In some cases, the most important context words, i.e. opinion words, are more than two-hops away from the aspect term words on the dependency tree. As indicated by Figure 1, there are four hops between the target “Mac OS” and the opinion words “easily picked up” on the dependency tree. This type of cases requires more than two layers of GCN to learn interactions between them. However, previous works show that GCN models with two layers often achieve the best performance (Zhang et al., 2018; Xu et al., 2018), deeper GCNs do"
2021.textgraphs-1.8,2020.acl-main.295,0,0.675681,"y towards the given term. Most approaches consider the semantic information from the context words and utilize the attention mechanism to learn such interactions. However, it has been shown that syntactic information obtained from dependency parsing is very effective in capturing long-range syntactic relations that are obscure from the surface form (Zhang et al., 2018). A recent popular approach to learn syntaxaware representations is employing graph convolutional networks (GCN) (Kipf and Welling, 2017) model over dependency trees (Huang and Carley, 2019; Zhang et al., 2019; Sun et al., 2019; Wang et al., 2020; Tang et al., 2020), which introduces syntactic inductive biases into the message passing. In some cases, the most important context words, i.e. opinion words, are more than two-hops away from the aspect term words on the dependency tree. As indicated by Figure 1, there are four hops between the target “Mac OS” and the opinion words “easily picked up” on the dependency tree. This type of cases requires more than two layers of GCN to learn interactions between them. However, previous works show that GCN models with two layers often achieve the best performance (Zhang et al., 2018; Xu et al., 2"
C10-2021,D07-1038,0,0.0156719,"c. 4. The experimental setup is described in Sec. 5. Results are thoroughly discussed in Sec. 6. Finally, we conclude in Sec. 7. 1.1 Related work Many previous works use the EM algorithm to estimate probabilities of translation rules: Wu (1997) uses EM to directly estimate joint word alignment probabilities of Inversion Transduction Grammar (ITG). Marcu and Wong (2002) use EM to estimate joint phrasal translation model (JPTM). Birch et al. (2006) reduce its complexity by using only concepts that match the high-confidence GIZA++ alignments. Similarly, Cherry and Lin (2007) use ITG for pruning. May and Knight (2007) use EM algorithm to train treeto-string rule probabilities, and use the Viterbi derivations to re-align the training data. Huang and Zhou (2009) use EM to estimate conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Context-free Grammar. Others try to overcome the deterministic nature of using bilingual alignments for rule extraction by sampling techniques (Blunsom et al., 2009; DeNero et al., 2008). Galley et al. (2006) define minimal rules for tree-to-string translation, merge them into composed rules (similarly to the rule arithmetic), and train weights by EM. While in their"
C10-2021,P00-1056,0,0.100984,"the chart. The proposed method is again independent of bilingual alignment, but at the same time utilizes the information obtained from the bilingual chart parsing. 5 Experiments We carried out experiments on two language pairs, German-English and Farsi-English. The German-English data is a subset (297k sentence pairs) of the Europarl (Koehn, 2005) corpus. Since we are focused on speech-to-speech translation, the punctuation was removed, and the text was lowercased. The dev set and test set contain each 1k sentence pairs with one reference. The word alignments were trained by GIZA++ toolkit (Och and Ney, 2000). Phrase pairs were 183 extracted using grow-diag-final (Koehn et al., 2007). The baseline ruleset was obtained as in (Chiang, 2007). The maximum phrase length for rule extraction was set to 10, the minimum required non-terminal span was 2. Additional rules for insertion, deletion, and swap were added to improve the parsability of the data, and to help EM training and rule arithmetic. However, these rules are not used by the decoder, since they would degrade the performance. New rules were proposed after the first iteration of EM1 , either by rule arithmetic or directly from the chart. Only no"
C10-2021,P09-1088,0,0.0480757,"phrasal translation model (JPTM). Birch et al. (2006) reduce its complexity by using only concepts that match the high-confidence GIZA++ alignments. Similarly, Cherry and Lin (2007) use ITG for pruning. May and Knight (2007) use EM algorithm to train treeto-string rule probabilities, and use the Viterbi derivations to re-align the training data. Huang and Zhou (2009) use EM to estimate conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Context-free Grammar. Others try to overcome the deterministic nature of using bilingual alignments for rule extraction by sampling techniques (Blunsom et al., 2009; DeNero et al., 2008). Galley et al. (2006) define minimal rules for tree-to-string translation, merge them into composed rules (similarly to the rule arithmetic), and train weights by EM. While in their method, word alignments are used to define all rules, rule arithmetic proposes new rules independently of word alignments. Similarly, Liu and Gildea (2009) identify matching long sequences (“big templates”) using word alignments and “liberate” matching small subtrees based on chart probabilities. Our method of proposing rules directly from the chart does not use word alignment at all. 1.2 For"
C10-2021,2001.mtsummit-papers.68,0,0.0658183,"Missing"
C10-2021,W07-0403,0,0.041256,"Missing"
C10-2021,J97-3002,0,0.306858,"uctured as follows: In Sec. 1, we explain our main motivation, summarize previous work, and briefly introduce the formalism of hierarchical phrase-based translation. In Sec. 2, we describe the bilingual chart parsing and the EM algorithm. The rule arithmetic is introduced in Sec. 3. The new method for proposing new rules directly from the chart is described in Sec. 4. The experimental setup is described in Sec. 5. Results are thoroughly discussed in Sec. 6. Finally, we conclude in Sec. 7. 1.1 Related work Many previous works use the EM algorithm to estimate probabilities of translation rules: Wu (1997) uses EM to directly estimate joint word alignment probabilities of Inversion Transduction Grammar (ITG). Marcu and Wong (2002) use EM to estimate joint phrasal translation model (JPTM). Birch et al. (2006) reduce its complexity by using only concepts that match the high-confidence GIZA++ alignments. Similarly, Cherry and Lin (2007) use ITG for pruning. May and Knight (2007) use EM algorithm to train treeto-string rule probabilities, and use the Viterbi derivations to re-align the training data. Huang and Zhou (2009) use EM to estimate conditional rule probabilities P (α|γ) and P (γ|α) for Syn"
C10-2021,P05-1033,0,0.0953716,"rge them into composed rules (similarly to the rule arithmetic), and train weights by EM. While in their method, word alignments are used to define all rules, rule arithmetic proposes new rules independently of word alignments. Similarly, Liu and Gildea (2009) identify matching long sequences (“big templates”) using word alignments and “liberate” matching small subtrees based on chart probabilities. Our method of proposing rules directly from the chart does not use word alignment at all. 1.2 Formally syntax-based models Our baseline model follows the Chiang’s hierarchical model (Chiang, 2007; Chiang, 2005; Zhou et al., 2008) based on Synchronous Context-free Grammar (SCFG). The rules have form X → hγ, α, ∼i, (4) where X is the only non-terminal in the grammar, γ and α are source and target strings with terminals and up to two non-terminals, ∼ is the correspondence between the non-terminals. Corresponding non-terminals have to be expanded at the same time. 2 Bilingual chart parsing and EM algorithm In this section, we briefly overview the algorithm for bilingual chart parsing and EM estimation of SCFG rule features. N Let e = eM 1 and f = f1 of source and target sentences. For each sentence pai"
C10-2021,W08-0403,1,0.855203,"composed rules (similarly to the rule arithmetic), and train weights by EM. While in their method, word alignments are used to define all rules, rule arithmetic proposes new rules independently of word alignments. Similarly, Liu and Gildea (2009) identify matching long sequences (“big templates”) using word alignments and “liberate” matching small subtrees based on chart probabilities. Our method of proposing rules directly from the chart does not use word alignment at all. 1.2 Formally syntax-based models Our baseline model follows the Chiang’s hierarchical model (Chiang, 2007; Chiang, 2005; Zhou et al., 2008) based on Synchronous Context-free Grammar (SCFG). The rules have form X → hγ, α, ∼i, (4) where X is the only non-terminal in the grammar, γ and α are source and target strings with terminals and up to two non-terminals, ∼ is the correspondence between the non-terminals. Corresponding non-terminals have to be expanded at the same time. 2 Bilingual chart parsing and EM algorithm In this section, we briefly overview the algorithm for bilingual chart parsing and EM estimation of SCFG rule features. N Let e = eM 1 and f = f1 of source and target sentences. For each sentence pair e, f , the ’E’ ste"
C10-2021,J07-2003,0,0.798832,"these approaches are very successful in handling local linguistic phenomena, handling longer distance reorderings can be more difficult. To avoid the combinatorial explosion, various restrictions, such as limitations of the phrase length or non-terminal span are used, that sometimes prevent from extracting good rules. Another reason is the deterministic nature of those heuristics that does not easily recover from errors in the word alignment. There are 127 sentence pairs out of 300K of the training data that contain this pattern, but this rule was not learned using the conventional approach (Chiang, 2007). There are three potential risks: (1) alignment errors (the first zu aligned to to, or der welt (of the world) aligned to null); (2) maximum phrase length for extracting rules lower than 11 words; (3) requirement of non-terminals spanning at least 2 words. The rule arithmetic (Cmejrek et al., 2009) constructs the new rule (2) as a combination of good rule usages: (3) X → hbesteht darin, is i X → hX1 zu X2 , to X2 X1 i 180 Coling 2010: Poster Volume, pages 180–188, Beijing, August 2010 The approach consists of bilingual chart parsing (BCP) of the training data, combining rules found in the cha"
C10-2021,2009.iwslt-papers.2,1,0.279966,"from extracting good rules. Another reason is the deterministic nature of those heuristics that does not easily recover from errors in the word alignment. There are 127 sentence pairs out of 300K of the training data that contain this pattern, but this rule was not learned using the conventional approach (Chiang, 2007). There are three potential risks: (1) alignment errors (the first zu aligned to to, or der welt (of the world) aligned to null); (2) maximum phrase length for extracting rules lower than 11 words; (3) requirement of non-terminals spanning at least 2 words. The rule arithmetic (Cmejrek et al., 2009) constructs the new rule (2) as a combination of good rule usages: (3) X → hbesteht darin, is i X → hX1 zu X2 , to X2 X1 i 180 Coling 2010: Poster Volume, pages 180–188, Beijing, August 2010 The approach consists of bilingual chart parsing (BCP) of the training data, combining rules found in the chart using a rule arithmetic to propose new rules, and using EM to estimate rule probabilities. In this paper, we study the behavior of the rule arithmetic on two different language pairs: German-English and Farsi-English. We also propose an additional method for constructing new rules directly from t"
C10-2021,D08-1033,0,0.1236,"Missing"
C10-2021,P06-1121,0,0.0353186,"l. (2006) reduce its complexity by using only concepts that match the high-confidence GIZA++ alignments. Similarly, Cherry and Lin (2007) use ITG for pruning. May and Knight (2007) use EM algorithm to train treeto-string rule probabilities, and use the Viterbi derivations to re-align the training data. Huang and Zhou (2009) use EM to estimate conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Context-free Grammar. Others try to overcome the deterministic nature of using bilingual alignments for rule extraction by sampling techniques (Blunsom et al., 2009; DeNero et al., 2008). Galley et al. (2006) define minimal rules for tree-to-string translation, merge them into composed rules (similarly to the rule arithmetic), and train weights by EM. While in their method, word alignments are used to define all rules, rule arithmetic proposes new rules independently of word alignments. Similarly, Liu and Gildea (2009) identify matching long sequences (“big templates”) using word alignments and “liberate” matching small subtrees based on chart probabilities. Our method of proposing rules directly from the chart does not use word alignment at all. 1.2 Formally syntax-based models Our baseline model"
C10-2021,P07-2045,0,0.00694909,"Missing"
C10-2021,2005.mtsummit-papers.11,0,0.00749185,"or outside of the rule span. The scoring function for rules with one nonterminal is just a special case of 15. Again, the candidates can be scored efficiently, taking into account only those combinations of non-terminal spans that correspond to filled cells in the chart. The proposed method is again independent of bilingual alignment, but at the same time utilizes the information obtained from the bilingual chart parsing. 5 Experiments We carried out experiments on two language pairs, German-English and Farsi-English. The German-English data is a subset (297k sentence pairs) of the Europarl (Koehn, 2005) corpus. Since we are focused on speech-to-speech translation, the punctuation was removed, and the text was lowercased. The dev set and test set contain each 1k sentence pairs with one reference. The word alignments were trained by GIZA++ toolkit (Och and Ney, 2000). Phrase pairs were 183 extracted using grow-diag-final (Koehn et al., 2007). The baseline ruleset was obtained as in (Chiang, 2007). The maximum phrase length for rule extraction was set to 10, the minimum required non-terminal span was 2. Additional rules for insertion, deletion, and swap were added to improve the parsability of"
C10-2021,D09-1136,0,0.0132043,"hou (2009) use EM to estimate conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Context-free Grammar. Others try to overcome the deterministic nature of using bilingual alignments for rule extraction by sampling techniques (Blunsom et al., 2009; DeNero et al., 2008). Galley et al. (2006) define minimal rules for tree-to-string translation, merge them into composed rules (similarly to the rule arithmetic), and train weights by EM. While in their method, word alignments are used to define all rules, rule arithmetic proposes new rules independently of word alignments. Similarly, Liu and Gildea (2009) identify matching long sequences (“big templates”) using word alignments and “liberate” matching small subtrees based on chart probabilities. Our method of proposing rules directly from the chart does not use word alignment at all. 1.2 Formally syntax-based models Our baseline model follows the Chiang’s hierarchical model (Chiang, 2007; Chiang, 2005; Zhou et al., 2008) based on Synchronous Context-free Grammar (SCFG). The rules have form X → hγ, α, ∼i, (4) where X is the only non-terminal in the grammar, γ and α are source and target strings with terminals and up to two non-terminals, ∼ is th"
C10-2021,W02-1018,0,0.0608278,"ormalism of hierarchical phrase-based translation. In Sec. 2, we describe the bilingual chart parsing and the EM algorithm. The rule arithmetic is introduced in Sec. 3. The new method for proposing new rules directly from the chart is described in Sec. 4. The experimental setup is described in Sec. 5. Results are thoroughly discussed in Sec. 6. Finally, we conclude in Sec. 7. 1.1 Related work Many previous works use the EM algorithm to estimate probabilities of translation rules: Wu (1997) uses EM to directly estimate joint word alignment probabilities of Inversion Transduction Grammar (ITG). Marcu and Wong (2002) use EM to estimate joint phrasal translation model (JPTM). Birch et al. (2006) reduce its complexity by using only concepts that match the high-confidence GIZA++ alignments. Similarly, Cherry and Lin (2007) use ITG for pruning. May and Knight (2007) use EM algorithm to train treeto-string rule probabilities, and use the Viterbi derivations to re-align the training data. Huang and Zhou (2009) use EM to estimate conditional rule probabilities P (α|γ) and P (γ|α) for Synchronous Context-free Grammar. Others try to overcome the deterministic nature of using bilingual alignments for rule extractio"
C10-2021,P02-1040,0,\N,Missing
C10-2021,W06-3123,0,\N,Missing
C10-2021,2006.amta-papers.2,0,\N,Missing
C10-2095,P06-1067,0,0.0155807,"0.1064 0.1028 0.1256 0.1214 0.1378 0.1062 Test 0.0941 0.0894 0.1091 0.1094 0.1209 0.0897 Table 2: E2F BLEU: PM Alignment Combination Based MT Model Comparision We built a standard phrase-based translation system (Koehn et al., 2003) that utilizes a stackbased decoder based on an A∗ search. Based on the combined alignments, we extracted phrase tables with a maximum phrase length of 6 for English and 8 for Pashto, respectively. We then trained the lexicalized reordering model that produced distortion costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006). Our training sentences are a compilation of sentences from various domains collected by DARPA, and hence we were able to build interpolated language model which weights the domains differently. We built an interpolated LM for both 833 Type I H GDF PM PMn U English and Pashto, but for English we had significantly more monolingual sentences (1.4 million in total) compared to slightly more than 100K sentences for Pashto. We tuned our MT model using minimum error rate (Och, 2003) training. F2E I H GDF PM PMn U Dev 0.1145 0.1262 0.1115 0.1201 0.1198 0.1111 Test 0.1101 0.1193 0.1204 0.1155 0.1196"
C10-2095,ayan-etal-2004-multi,0,0.0392993,"Missing"
C10-2095,J93-2003,0,0.0206783,"timal alignment lying between the intersection and union of multiple alignment tables by optimizing the parameter p: the affinely extended real number defining the order of the power mean function. The combination approach produces better alignment tables in terms of both F-measure and BLEU scores. 1 2 Introduction Machine Translation (MT) systems are trained on bi-text parallel corpora. One of the first steps involved in training a MT system is obtaining alignments between words of source and target languages. This is typically done using some form of Expectation Maximization (EM) algorithm (Brown et al., 1993), (Och and Ney, 2003), (Vogel et al., 1996). These unsupervised algorithms provide alignment links between english words ei and the foreign words fj for a given e−f sentence pair. The alignment pairs are then used to extract phrases tables (Koehn et al., 2003), hierarchical rules (Chiang, 2005), or tree-to-string mappings (Yamada and Knight, 2001). Thus, the Related Work Most existing methods for alignment combination (symmetrization) rely on heuristics to identify reliable links (Och and Ney, 2003), (Koehn et al., 2003). The method proposed in (Och and Ney, 2003), for example, interpolates th"
C10-2095,P05-1033,0,0.0505771,"2 Introduction Machine Translation (MT) systems are trained on bi-text parallel corpora. One of the first steps involved in training a MT system is obtaining alignments between words of source and target languages. This is typically done using some form of Expectation Maximization (EM) algorithm (Brown et al., 1993), (Och and Ney, 2003), (Vogel et al., 1996). These unsupervised algorithms provide alignment links between english words ei and the foreign words fj for a given e−f sentence pair. The alignment pairs are then used to extract phrases tables (Koehn et al., 2003), hierarchical rules (Chiang, 2005), or tree-to-string mappings (Yamada and Knight, 2001). Thus, the Related Work Most existing methods for alignment combination (symmetrization) rely on heuristics to identify reliable links (Och and Ney, 2003), (Koehn et al., 2003). The method proposed in (Och and Ney, 2003), for example, interpolates the intersection and union of two asymmetric alignment tables by adding links that are adjacent to intersection links, and connect at least one previously unaligned word. Another example is the method in (Koehn et al., 2003), which adds links to the intersection of two alignment tables that are t"
C10-2095,J07-3002,0,0.0309814,"Missing"
C10-2095,N03-1017,0,0.282151,"ining technique involves obtaining two sets of alignment tables A1 and A2 for the same sentence pair e − f , and producing a new set based on union A∪ = A1 ∪ A2 or intersection A∩ = A1 ∩ A2 or some optimal combination Ao such that it is subset of A1 ∪ A2 but a superset of A1 ∩ A2 . How to find this optimal Ao is a key question. A∪ has high precision but low recall producing fewer alignments and A∩ has high recall but low precision. Most existing techniques for combining multiple alignment tables can combine only two alignment tables at a time, and are based on heuristics (Och and Ney, 2003), (Koehn et al., 2003). In this paper, we propose a novel mathematical formulation for combining an arbitrary number of alignment tables using their power mean. The method frames the combination task as an optimization problem, and finds the optimal alignment lying between the intersection and union of multiple alignment tables by optimizing the parameter p: the affinely extended real number defining the order of the power mean function. The combination approach produces better alignment tables in terms of both F-measure and BLEU scores. 1 2 Introduction Machine Translation (MT) systems are trained on bi-text paral"
C10-2095,N06-1014,0,0.108703,"and Ney, 2003), (Koehn et al., 2003). The method proposed in (Och and Ney, 2003), for example, interpolates the intersection and union of two asymmetric alignment tables by adding links that are adjacent to intersection links, and connect at least one previously unaligned word. Another example is the method in (Koehn et al., 2003), which adds links to the intersection of two alignment tables that are the diagonal neighbors of existing links, optionally requiring that any added links connect two previously unaligned words. Other methods try to combine the tables during alignment training. In (Liang et al., 2006), asymmetric models are jointly trained to maximize the similarity of their alignments, by opti828 Coling 2010: Poster Volume, pages 828–836, Beijing, August 2010 mizing an EM-like objective function based on agreement heuristics. In (Ayan et al., 2004), the authors present a technique for combining alignments based on various linguistic resources such as parts of speech, dependency parses, or bilingual dictionaries, and use machine learning techniques to do alignment combination. One of the main disadvantages of (Ayan et al., 2004)’s method, however, is that the algorithm is a supervised lear"
C10-2095,C04-1032,0,0.0168509,"eement heuristics. In (Ayan et al., 2004), the authors present a technique for combining alignments based on various linguistic resources such as parts of speech, dependency parses, or bilingual dictionaries, and use machine learning techniques to do alignment combination. One of the main disadvantages of (Ayan et al., 2004)’s method, however, is that the algorithm is a supervised learning method, and so requires human-annotated data. Recently, (Xiang et al., 2010) proposed a method that can handle multiple alignments with soft links which are defined by confidence scores of alignment links. (Matusov et al., 2004) on the other hand, frame symmetrization as finding a set with minimal cost using use a graph based algorithm where costs are associated with local alignment probabilities. In summary, most existing alignment combination methods try to find an optimal alignment set Ao that lies between A∩ and A∪ using heuristics. The main problems with methods based on heuristics are: 1. they may not generalize well across language pairs 2. they typically do not have any parameters to optimize the words ei and fj , where i ≤ I and j ≤ J. In this paper we will use the convention that when aij = 1, words ei and"
C10-2095,J03-1002,0,0.267618,"Ney, 2003). This combining technique involves obtaining two sets of alignment tables A1 and A2 for the same sentence pair e − f , and producing a new set based on union A∪ = A1 ∪ A2 or intersection A∩ = A1 ∩ A2 or some optimal combination Ao such that it is subset of A1 ∪ A2 but a superset of A1 ∩ A2 . How to find this optimal Ao is a key question. A∪ has high precision but low recall producing fewer alignments and A∩ has high recall but low precision. Most existing techniques for combining multiple alignment tables can combine only two alignment tables at a time, and are based on heuristics (Och and Ney, 2003), (Koehn et al., 2003). In this paper, we propose a novel mathematical formulation for combining an arbitrary number of alignment tables using their power mean. The method frames the combination task as an optimization problem, and finds the optimal alignment lying between the intersection and union of multiple alignment tables by optimizing the parameter p: the affinely extended real number defining the order of the power mean function. The combination approach produces better alignment tables in terms of both F-measure and BLEU scores. 1 2 Introduction Machine Translation (MT) systems are tr"
C10-2095,P03-1021,0,0.0151973,"n costs based on the number of words that are skipped on the target side, in a manner similar to (Al-Onaizan and Papineni, 2006). Our training sentences are a compilation of sentences from various domains collected by DARPA, and hence we were able to build interpolated language model which weights the domains differently. We built an interpolated LM for both 833 Type I H GDF PM PMn U English and Pashto, but for English we had significantly more monolingual sentences (1.4 million in total) compared to slightly more than 100K sentences for Pashto. We tuned our MT model using minimum error rate (Och, 2003) training. F2E I H GDF PM PMn U Dev 0.1145 0.1262 0.1115 0.1201 0.1198 0.1111 Test 0.1101 0.1193 0.1204 0.1155 0.1196 0.1155 PT Size (100K) 182.17 30.73 27.65 60.87 25.67 24.54 Table 4: E2F Phrase Table Size Table 3: F2E BLEU : PM Alignment Combination Based MT Model Comparision We built five different MT models based on Intersection (I), Union (U), (Koehn et al., 2003) Grow Diagonal Final (GDF), (Och and Ney, 2003) H refined heuristics and Power Mean (PMn ) alignment sets where n = 5. We obtained BLEU (Papineni et al., 2002) scores for E2F direction as shown in Table 2. As expected MT model b"
C10-2095,P02-1040,0,0.0806439,"than 100K sentences for Pashto. We tuned our MT model using minimum error rate (Och, 2003) training. F2E I H GDF PM PMn U Dev 0.1145 0.1262 0.1115 0.1201 0.1198 0.1111 Test 0.1101 0.1193 0.1204 0.1155 0.1196 0.1155 PT Size (100K) 182.17 30.73 27.65 60.87 25.67 24.54 Table 4: E2F Phrase Table Size Table 3: F2E BLEU : PM Alignment Combination Based MT Model Comparision We built five different MT models based on Intersection (I), Union (U), (Koehn et al., 2003) Grow Diagonal Final (GDF), (Och and Ney, 2003) H refined heuristics and Power Mean (PMn ) alignment sets where n = 5. We obtained BLEU (Papineni et al., 2002) scores for E2F direction as shown in Table 2. As expected MT model based on I alignment has the low BLEU score of 0.1064 on the dev set and 0.0941 on the test set on E2F direction. Intersection, though, has higher precision, but throws away many alignments, so the overall number of alignments is too small to produce a good phrase translation table. Similarly the U alignment also has low scores (0.1062 and 0.0897) on the dev and test sets, respectively. The best scores for E2F direction for both dev and test set is obtained using the model based on PMn algorithm. We obtained BLEU scores of 0.1"
C10-2095,C96-2141,0,0.678172,"ion and union of multiple alignment tables by optimizing the parameter p: the affinely extended real number defining the order of the power mean function. The combination approach produces better alignment tables in terms of both F-measure and BLEU scores. 1 2 Introduction Machine Translation (MT) systems are trained on bi-text parallel corpora. One of the first steps involved in training a MT system is obtaining alignments between words of source and target languages. This is typically done using some form of Expectation Maximization (EM) algorithm (Brown et al., 1993), (Och and Ney, 2003), (Vogel et al., 1996). These unsupervised algorithms provide alignment links between english words ei and the foreign words fj for a given e−f sentence pair. The alignment pairs are then used to extract phrases tables (Koehn et al., 2003), hierarchical rules (Chiang, 2005), or tree-to-string mappings (Yamada and Knight, 2001). Thus, the Related Work Most existing methods for alignment combination (symmetrization) rely on heuristics to identify reliable links (Och and Ney, 2003), (Koehn et al., 2003). The method proposed in (Och and Ney, 2003), for example, interpolates the intersection and union of two asymmetric"
C10-2095,P10-2005,1,0.806453,"rity of their alignments, by opti828 Coling 2010: Poster Volume, pages 828–836, Beijing, August 2010 mizing an EM-like objective function based on agreement heuristics. In (Ayan et al., 2004), the authors present a technique for combining alignments based on various linguistic resources such as parts of speech, dependency parses, or bilingual dictionaries, and use machine learning techniques to do alignment combination. One of the main disadvantages of (Ayan et al., 2004)’s method, however, is that the algorithm is a supervised learning method, and so requires human-annotated data. Recently, (Xiang et al., 2010) proposed a method that can handle multiple alignments with soft links which are defined by confidence scores of alignment links. (Matusov et al., 2004) on the other hand, frame symmetrization as finding a set with minimal cost using use a graph based algorithm where costs are associated with local alignment probabilities. In summary, most existing alignment combination methods try to find an optimal alignment set Ao that lies between A∩ and A∪ using heuristics. The main problems with methods based on heuristics are: 1. they may not generalize well across language pairs 2. they typically do no"
C10-2095,N09-1028,0,0.0252149,"Missing"
C10-2095,P01-1067,0,0.044285,"stems are trained on bi-text parallel corpora. One of the first steps involved in training a MT system is obtaining alignments between words of source and target languages. This is typically done using some form of Expectation Maximization (EM) algorithm (Brown et al., 1993), (Och and Ney, 2003), (Vogel et al., 1996). These unsupervised algorithms provide alignment links between english words ei and the foreign words fj for a given e−f sentence pair. The alignment pairs are then used to extract phrases tables (Koehn et al., 2003), hierarchical rules (Chiang, 2005), or tree-to-string mappings (Yamada and Knight, 2001). Thus, the Related Work Most existing methods for alignment combination (symmetrization) rely on heuristics to identify reliable links (Och and Ney, 2003), (Koehn et al., 2003). The method proposed in (Och and Ney, 2003), for example, interpolates the intersection and union of two asymmetric alignment tables by adding links that are adjacent to intersection links, and connect at least one previously unaligned word. Another example is the method in (Koehn et al., 2003), which adds links to the intersection of two alignment tables that are the diagonal neighbors of existing links, optionally re"
C16-1164,D13-1160,0,0.766093,"entities as subject are then the fact search space for this question. CharCNN and word-CNN decompose each question-fact match into an entity-mention surface-form match and a predicate-pattern semantic match. Our approach has a simple architecture, but it outperforms the state-of-the-art, a system that has a much more complicated structure. 2 Related Work As mentioned in Section 1, factoid QA against Freebase can be categorized into single-relation QA and multi-relation QA. Much work has been done on multi-relation QA in the past decade, especially after the release of benchmark WebQuestions (Berant et al., 2013). Most state-of-the-art approaches (Berant et al., 2013; Yahya et al., 2013; Yao and Van Durme, 2014; Yih et al., 2015) are based on semantic parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing th"
C16-1164,D14-1067,0,0.48883,"work makes two main contributions. (i) A simple and effective entity linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker over SimpleQA task. 1 (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task. 1 Introduction Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)). Answering a question can require reference to multiple related facts in Freebase or reference to a single fact. This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts. Single-relation factual questions are the most common type of question observed in various community QA sites (Fader et al., 2013) and in search query logs. Even though this task is called “simple”, it is in"
C16-1164,P16-1076,0,0.692821,"m by an embedding-based QA system developed under the framework of Memory Networks (Weston et al., 2015; Sukhbaatar et al., 2015). The setting of the SimpleQA corresponds to the elementary operation of performing a single lookup in the memory. They investigate the performance of training on the combination of SimpleQuestions, WebQuestions and Reverb training sets. Golub and He (2016) propose a character-level attention-based encoder-decoder framework to encode the question and subsequently decode into (subject, predicate) tuple. Our model in this work is much simpler than these prior systems. Dai et al. (2016) combine a unified conditional probabilistic framework with deep recurrent neural networks and neural embeddings to get state-of-the-art performance. Treating SimpleQA as fact selection is inspired by work on answer selection (e.g., Yu et al. (2014), Yin et al. (2016b), Santos et al. (2016)) that looks for the correct answer(s) from some candidates for a given question. The answer candidates in those tasks are raw text, not structured information as facts in Freebase are. We are also inspired by work that generates natural language questions given knowledge graph facts (Seyler et al., 2015; Se"
C16-1164,P15-1026,0,0.370013,"form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a word-hashing technique (Huang et al., 2013) for both entity-mention and predicate-pattern matches. Each word"
C16-1164,D11-1142,0,0.0153623,"h the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a word-hashing technique (Huang et al., 2013) for both entity-mention and predicate-pattern matches. Each word is first preprocessed into a count vector of character-trigram vocabulary, then forwarded into the CNN as input. We treat entities and mentions as character sequences. Our char-CNN for entity-mention match is more end-to-end without data preprocessing. (ii)"
C16-1164,P13-1158,0,0.0602297,"eebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)). Answering a question can require reference to multiple related facts in Freebase or reference to a single fact. This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts. Single-relation factual questions are the most common type of question observed in various community QA sites (Fader et al., 2013) and in search query logs. Even though this task is called “simple”, it is in reality not simple at all and far from solved. In SimpleQA, a question, such as “what’s the hometown of Obama?”, asks a single and direct topic of an entity. In this example, the entity is “Obama” and the topic is hometown. So our task is reduced to finding one fact (subject, predicate, object) in Freebase that answers the question, which roughly means the subject and predicate are the best matches for the topical entity “Obama” and for the topic description “what’s the hometown of”, respectively. Thus, we aim to des"
C16-1164,D16-1166,0,0.655535,"system, given a question, is asked to choose the best answer from a list of candidates. In this work, we formulate the SimpleQA task as a fact selection problem and the key issue lies in the system design for how to match a fact candidate to the question. The first obstacle is that Freebase has an overwhelming number of facts. A common and effective way is to first conduct entity linking of a question over Freebase, so that only a small subset of facts remain as candidates. Prior work achieves entity linking by searching word n-grams of a question among all entity names (Bordes et al., 2015; Golub and He, 2016). Then, facts whose subject entities match those n-grams are kept. Our first contribution in this work is to present a simple while effective entity linker ∗ 1 This work was conducted during the first author’s internship at IBM Watson Group. We release our entity linking results at: https://github.com/Gorov/SimpleQuestions-EntityLinking This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1746 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers"
C16-1164,P14-1090,0,0.301761,"Missing"
C16-1164,W14-2416,0,0.0221327,"Missing"
C16-1164,N15-3014,0,0.0599456,"c parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a word-hashin"
C16-1164,P14-2105,0,0.095584,"e based on semantic parsing, where a question is mapped to its formal meaning representation (e.g., logical form) and then translated to a knowledge base (KB) query. The answers to the question can then be retrieved simply 2 Surface-form entity linking has limitations in candidate collection as some entities have the same names. We tried another word-CNN to match the pattern to the entity description provided by Freebase, but no improvement is observed. 1747 by executing the query. Other approaches retrieve a set of candidate answers from KB using relation extraction (Yao and Van Durme, 2014; Yih et al., 2014; Yao, 2015; Bast and Haussmann, 2015) or distributed representations (Bordes et al., 2014; Dong et al., 2015; Xu et al., 2016). Our method in this work explores CNN to learn distributed representations for Freebase facts and questions. SimpleQA was first investigated in (Fader et al., 2013) through PARALEX dataset against knowledge base Reverb (Fader et al., 2011). Yih et al. (2014) also investigate PARALEX by a system with some similarity to ours – they employ CNNs to match entity-mention and predicate-pattern. Our model differs in two-fold. (i) They use the same CNN architecture based on a"
C16-1164,P15-1128,0,0.592067,"y linker over Freebase is proposed. Our entity linker outperforms the state-of-the-art entity linker over SimpleQA task. 1 (ii) A novel attentive maxpooling is stacked over word-CNN, so that the predicate representation can be matched with the predicate-focused question representation more effectively. Experiments show that our system sets new state-of-the-art in this task. 1 Introduction Factoid question answering (QA) over knowledge bases such as Freebase (Bollacker et al., 2008) has been intensively studied recently (e.g., Bordes et al. (2014), Yao et al. (2014), Bast and Haussmann (2015), Yih et al. (2015), Xu et al. (2016)). Answering a question can require reference to multiple related facts in Freebase or reference to a single fact. This work studies simple question answering (SimpleQA) based on the SimpleQuestions benchmark (Bordes et al., 2015) in which answering a question does not require reasoning over multiple facts. Single-relation factual questions are the most common type of question observed in various community QA sites (Fader et al., 2013) and in search query logs. Even though this task is called “simple”, it is in reality not simple at all and far from solved. In SimpleQA, a que"
C16-1164,W16-0103,1,0.856224,"Missing"
C16-1164,Q16-1019,1,0.0271245,"Missing"
C16-1164,N03-1033,0,\N,Missing
C16-1164,Q14-1002,1,\N,Missing
D10-1014,J93-2003,0,0.02048,"particular, the hierarchical model (Chiang, 2007) studied in this paper explores hierarchical structures of natural language and utilize only a unified nonterminal symbol X in the grammar, X → hγ, α, ∼i where ∼ is the one-to-one correspondence between X’s in γ and α, and it can be indicated by underscripted co-indexes. Two example English-toChinese translation rules are represented as follows: X → hgive the pen to me, 钢笔 给 我i (1) X → hgive X1 to me, X1 给 我i (2) The SCFG rules of hierarchical phrase-based models are extracted automatically from corpora of word-aligned parallel sentence pairs (Brown et al., 1993; Och and Ney, 2000). An aligned sentence pair is a tuple (E, F, A), where E = e1 · · · en can be interpreted as an English sentence of length n, F = f1 · · · fm its translation of length m in a foreign language, and A a set of links between words of the two sentences. Figure 1 (a) shows an example of aligned English-to-Chinese sentence pair. Widely adopted in phrase-based models (Och and Ney, 2004), a pair of consecutive sequences of words from E and F is a phrase pair if all words are aligned only within the sequences and not to any word outside. We call a sequence of words a phrase if it co"
D10-1014,P10-1146,0,0.362265,"oding (Mi et al., 2008) or forest-based rule extraction (Mi and Huang, 2008). On the other hand, when properly used, syntactic constraints can provide invaluable benefits to improve translation quality. The tree-to-string models of Mi and Huang (2008) can actually signif138 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138–147, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics icantly outperform hierarchical phrase-based models when using forest-based rule extraction together with forest-based decoding. Chiang (2010) also obtained significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions. In this paper, we augment rules in hierarchical phrase-based translation systems with novel syntactic features. Unlike previous studies (e.g., (Zollmann and Venugopal, 2006)) that directly use explicit treebank categories such as NP, NP/PP (NP missing PP from the right) to annotate phrase pairs, we induce a set of latent categories to capture the syntactic"
D10-1014,N04-1035,0,0.228129,"ur model, each X nonterminal in a SCFG rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories. These feature vectors are utilized at decoding time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints. 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality, thanks to the incorporation of phrasal translation adopted from the widely used phrase-based models (Och and Ney, 2004) to handle local fluency and the engagement of synchronous context-free grammars (SCFG) to handle non-local phrase reordering. Approaches to syntaxbased translation models can be largely categorized On the one hand, hierarchical phrase-based models do not suffer from errors in syntactic constraints that are unavoidable in linguistically syntax-based models. Despite the complete lack of linguist"
D10-1014,W05-1506,0,0.0243644,"plitting followed by several EM iterations to tune model parameters. We consider 16 an appropriate number for latent categories, not too small to differentiate between different syntactic usages and not too large for the extra computational and storage costs. 9 Each binary production rule is now associated with a 3dimensional matrix of probabilities, and each emission rule associated with a 1-dimensional array of probabilities. by a variant of Expectation-Maximization (EM) algorithm. Recall that our decomposition forests are fully binarized (except the root). In the hypergraph representation (Huang and Chiang, 2005), the hyperedges of our forests all have the same format10 h(V, W ), U i, meaning that node U expands to nodes V and W with production rule U → V W . Given a forest F with root node R, we denote e(U ) the emitted syntactic category at node U and LR(U ) (or PL(W ), or PR(V ))11 the set of node pairs (V, W ) (or (U, V ), or (U, W )) such that h(V, W ), U i is a hyperedge of the forest. Now consider node U , which is either S, X, or B, in the forest. Let Ux be the latent syntactic category12 of node U . We define I(Ux ) the part of the forest (includes e(U ) but not Ux ) inside U , and O(Ux ) the"
D10-1014,D09-1087,1,0.700195,"slation. The baseline system is our implementation of the hierarchical phrase-based model of Chiang (2007), and it includes basic features such as rule and lexicalized rule translation probabilities, language model scores, rule counts, etc. We use 4-gram language models in both tasks, and conduct minimumerror-rate training (Och, 2003) to optimize feature weights on the dev set. Our baseline hierarchical model has 8.3M and 9.7M rules for the English-toGerman and English-to-Chinese tasks, respectively. The English side of the parallel data is parsed by our implementation of the Berkeley parser (Huang and Harper, 2009) trained on the combination of Broadcast News treebank from Ontonotes (Weischedel et al., 2008) and a speechified version of the WSJ treebank (Marcus et al., 1999) to achieve higher parsing accuracy (Huang et al., 2010). Our approach introduces a new syntactic feature and its feature weight is tuned in the same way together with the features in the baseline model. In this study, we induce 16 latent categories for both X and B nonterminals. Our approach identifies ∼180k unique tag sequences for the English side of phrase pairs in both tasks. As shown by the examples in Table 2, the syntactic fe"
D10-1014,W06-3601,0,0.141074,"text-free grammars (SCFG) to handle non-local phrase reordering. Approaches to syntaxbased translation models can be largely categorized On the one hand, hierarchical phrase-based models do not suffer from errors in syntactic constraints that are unavoidable in linguistically syntax-based models. Despite the complete lack of linguistic guidance, the performance of hierarchical phrasebased models is competitive when compared to linguistically syntax-based models. As shown in (Mi and Huang, 2008), hierarchical phrase-based models significantly outperform tree-to-string models (Liu et al., 2006; Huang et al., 2006), even when attempts are made to alleviate parsing errors using either forest-based decoding (Mi et al., 2008) or forest-based rule extraction (Mi and Huang, 2008). On the other hand, when properly used, syntactic constraints can provide invaluable benefits to improve translation quality. The tree-to-string models of Mi and Huang (2008) can actually signif138 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138–147, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics icantly outperform hierarchical phrase-bas"
D10-1014,N09-2054,1,0.680169,"imized. The motivation is to let the latent categories learn different preferences of (emitted) syntactic categories as well as structural dependencies along the hierarchy so that they can carry syntactic information. We call them latent syntactic categories. The learned Xi ’s represent syntactically-induced finer-grained categories of phrases and are used as the set of latent syntactic categories C described in Section 3. In related research, Matsuzaki et al. (2005) and Petrov et al. (2006) introduced latent variables to learn finergrained distinctions of treebank categories for parsing, and Huang et al. (2009) used a similar approach to learn finer-grained part-of-speech tags for tagging. Our method is in spirit similar to these approaches. Optimization of grammar parameters to maximize the likelihood of training forests can be achieved 8 • Σ is the set of terminals comprising treebank categories plus the CR tag (the crossing category), 7 The intermediate binarization nodes are also labeled as either X or B based on whether they exactly cover a phrase or not. 143 We incrementally split each nonterminal to 2, 4, 8, and finally 16 categories, with each splitting followed by several EM iterations to t"
D10-1014,D10-1002,1,0.83272,"rule counts, etc. We use 4-gram language models in both tasks, and conduct minimumerror-rate training (Och, 2003) to optimize feature weights on the dev set. Our baseline hierarchical model has 8.3M and 9.7M rules for the English-toGerman and English-to-Chinese tasks, respectively. The English side of the parallel data is parsed by our implementation of the Berkeley parser (Huang and Harper, 2009) trained on the combination of Broadcast News treebank from Ontonotes (Weischedel et al., 2008) and a speechified version of the WSJ treebank (Marcus et al., 1999) to achieve higher parsing accuracy (Huang et al., 2010). Our approach introduces a new syntactic feature and its feature weight is tuned in the same way together with the features in the baseline model. In this study, we induce 16 latent categories for both X and B nonterminals. Our approach identifies ∼180k unique tag sequences for the English side of phrase pairs in both tasks. As shown by the examples in Table 2, the syntactic feature vector representation is able to identify similar and dissimilar tag sequences. For instance, it determines that the sequence of “DT JJ NN” is syntactically very similar to “DT ADJP NN” while very dissimilar to “N"
D10-1014,2005.mtsummit-papers.11,0,0.00552699,"it is unary, but it can be handled similarly. When clear from context, we use the same variable to present both a node and its label. 11 LR stands for the left and right children, PL for the parent and left children, and PR for the parent and right children. 12 We never split the start symbol S, and denote S0 = S. 13 The emission rules can be handled similarly. 144 6 Experiments We conduct experiments on two tasks, English-toGerman and English-to-Chinese, both aimed for speech-to-speech translation. The training data for the English-to-German task is a filtered subset of the Europarl corpus (Koehn, 2005), containing ∼300k parallel bitext with ∼4.5M tokens on each side. The dev and test sets both contain 1k sentences with one reference for each. The training data for the Englishto-Chinese task is collected from transcription and human translation of conversations in travel domain. It consists of ∼500k parallel bitext with ∼3M tokens14 on each side. Both dev and test sets contain ∼1.3k sentences, each with two references. Both 14 The Chinese sentences are automatically segmented into words. However, BLEU scores are computed at character level for tuning and evaluation. corpora are also preproce"
D10-1014,P06-1077,0,0.211823,"erminal in a SCFG rule is decorated with a real-valued feature vector computed based on its distribution of latent syntactic categories. These feature vectors are utilized at decoding time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints. 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality, thanks to the incorporation of phrasal translation adopted from the widely used phrase-based models (Och and Ney, 2004) to handle local fluency and the engagement of synchronous context-free grammars (SCFG) to handle non-local phrase reordering. Approaches to syntaxbased translation models can be largely categorized On the one hand, hierarchical phrase-based models do not suffer from errors in syntactic constraints that are unavoidable in linguistically syntax-based models. Despite the complete lack of linguistic guidance, the pe"
D10-1014,P08-1114,0,0.21816,"JJ NN TO VP JJ WHNP DT NN IN INTJ NP ADJP JJ PDT JJ RB JJ ADJP JJ JJ CC ADJP VB JJ JJ ADVP WHNP JJ ADJP IN NP JJ AUX RB ADJP ADJP VP Table 2: Examples of similar and dissimilar tag sequences. This cost can be reduced, however, by caching the dot-products of the tag sequences that are frequently accessed. There are other successful investigations to impose soft syntactic constraints to hierarchical phrase-based models by either introducing syntaxbased rule features such as the prior derivation model of Zhou et al. (2008) or by imposing constraints on translation spans at decoding time, e.g., (Marton and Resnik, 2008; Xiong et al., 2009; Xiong et al., 2010). These approaches are all orthogonal to ours and it is expected that they can be combined with our approach to achieve greater improvement. This work is an initial effort to investigate latent syntactic categories to enhance hierarchical phrasebased translation models, and there are many directions to continue this line of research. First, while the current approach imposes soft syntactic constraints between the parse structure of the source sentence and the SCFG rules used to derive the translation, the real-valued syntactic feature vectors can also b"
D10-1014,P05-1010,0,0.0580797,"{B1 , · · · , Bn } for B, and then learn a set of rule probabilities9 φ on the latent categories so that the likelihood of the training forests are maximized. The motivation is to let the latent categories learn different preferences of (emitted) syntactic categories as well as structural dependencies along the hierarchy so that they can carry syntactic information. We call them latent syntactic categories. The learned Xi ’s represent syntactically-induced finer-grained categories of phrases and are used as the set of latent syntactic categories C described in Section 3. In related research, Matsuzaki et al. (2005) and Petrov et al. (2006) introduced latent variables to learn finergrained distinctions of treebank categories for parsing, and Huang et al. (2009) used a similar approach to learn finer-grained part-of-speech tags for tagging. Our method is in spirit similar to these approaches. Optimization of grammar parameters to maximize the likelihood of training forests can be achieved 8 • Σ is the set of terminals comprising treebank categories plus the CR tag (the crossing category), 7 The intermediate binarization nodes are also labeled as either X or B based on whether they exactly cover a phrase o"
D10-1014,D08-1022,0,0.292907,"om the widely used phrase-based models (Och and Ney, 2004) to handle local fluency and the engagement of synchronous context-free grammars (SCFG) to handle non-local phrase reordering. Approaches to syntaxbased translation models can be largely categorized On the one hand, hierarchical phrase-based models do not suffer from errors in syntactic constraints that are unavoidable in linguistically syntax-based models. Despite the complete lack of linguistic guidance, the performance of hierarchical phrasebased models is competitive when compared to linguistically syntax-based models. As shown in (Mi and Huang, 2008), hierarchical phrase-based models significantly outperform tree-to-string models (Liu et al., 2006; Huang et al., 2006), even when attempts are made to alleviate parsing errors using either forest-based decoding (Mi et al., 2008) or forest-based rule extraction (Mi and Huang, 2008). On the other hand, when properly used, syntactic constraints can provide invaluable benefits to improve translation quality. The tree-to-string models of Mi and Huang (2008) can actually signif138 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138–147, c MIT, Massachu"
D10-1014,P08-1023,0,0.0462297,"be largely categorized On the one hand, hierarchical phrase-based models do not suffer from errors in syntactic constraints that are unavoidable in linguistically syntax-based models. Despite the complete lack of linguistic guidance, the performance of hierarchical phrasebased models is competitive when compared to linguistically syntax-based models. As shown in (Mi and Huang, 2008), hierarchical phrase-based models significantly outperform tree-to-string models (Liu et al., 2006; Huang et al., 2006), even when attempts are made to alleviate parsing errors using either forest-based decoding (Mi et al., 2008) or forest-based rule extraction (Mi and Huang, 2008). On the other hand, when properly used, syntactic constraints can provide invaluable benefits to improve translation quality. The tree-to-string models of Mi and Huang (2008) can actually signif138 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 138–147, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics icantly outperform hierarchical phrase-based models when using forest-based rule extraction together with forest-based decoding. Chiang (2010) also obta"
D10-1014,P00-1056,0,0.0696837,"rarchical model (Chiang, 2007) studied in this paper explores hierarchical structures of natural language and utilize only a unified nonterminal symbol X in the grammar, X → hγ, α, ∼i where ∼ is the one-to-one correspondence between X’s in γ and α, and it can be indicated by underscripted co-indexes. Two example English-toChinese translation rules are represented as follows: X → hgive the pen to me, 钢笔 给 我i (1) X → hgive X1 to me, X1 给 我i (2) The SCFG rules of hierarchical phrase-based models are extracted automatically from corpora of word-aligned parallel sentence pairs (Brown et al., 1993; Och and Ney, 2000). An aligned sentence pair is a tuple (E, F, A), where E = e1 · · · en can be interpreted as an English sentence of length n, F = f1 · · · fm its translation of length m in a foreign language, and A a set of links between words of the two sentences. Figure 1 (a) shows an example of aligned English-to-Chinese sentence pair. Widely adopted in phrase-based models (Och and Ney, 2004), a pair of consecutive sequences of words from E and F is a phrase pair if all words are aligned only within the sequences and not to any word outside. We call a sequence of words a phrase if it corresponds to either"
D10-1014,J04-4002,0,0.583218,"time to measure the similarity between the syntactic analysis of the source side and the syntax of the SCFG rules that are applied to derive translations. Our approach maintains the advantages of hierarchical phrase-based translation systems while at the same time naturally incorporates soft syntactic constraints. 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality, thanks to the incorporation of phrasal translation adopted from the widely used phrase-based models (Och and Ney, 2004) to handle local fluency and the engagement of synchronous context-free grammars (SCFG) to handle non-local phrase reordering. Approaches to syntaxbased translation models can be largely categorized On the one hand, hierarchical phrase-based models do not suffer from errors in syntactic constraints that are unavoidable in linguistically syntax-based models. Despite the complete lack of linguistic guidance, the performance of hierarchical phrasebased models is competitive when compared to linguistically syntax-based models. As shown in (Mi and Huang, 2008), hierarchical phrase-based models sign"
D10-1014,P03-1021,0,0.012347,"eferences. Both 14 The Chinese sentences are automatically segmented into words. However, BLEU scores are computed at character level for tuning and evaluation. corpora are also preprocessed with punctuation removed and words down-cased to make them suitable for speech translation. The baseline system is our implementation of the hierarchical phrase-based model of Chiang (2007), and it includes basic features such as rule and lexicalized rule translation probabilities, language model scores, rule counts, etc. We use 4-gram language models in both tasks, and conduct minimumerror-rate training (Och, 2003) to optimize feature weights on the dev set. Our baseline hierarchical model has 8.3M and 9.7M rules for the English-toGerman and English-to-Chinese tasks, respectively. The English side of the parallel data is parsed by our implementation of the Berkeley parser (Huang and Harper, 2009) trained on the combination of Broadcast News treebank from Ontonotes (Weischedel et al., 2008) and a speechified version of the WSJ treebank (Marcus et al., 1999) to achieve higher parsing accuracy (Huang et al., 2010). Our approach introduces a new syntactic feature and its feature weight is tuned in the same"
D10-1014,P06-1055,0,0.0697491,"nd then learn a set of rule probabilities9 φ on the latent categories so that the likelihood of the training forests are maximized. The motivation is to let the latent categories learn different preferences of (emitted) syntactic categories as well as structural dependencies along the hierarchy so that they can carry syntactic information. We call them latent syntactic categories. The learned Xi ’s represent syntactically-induced finer-grained categories of phrases and are used as the set of latent syntactic categories C described in Section 3. In related research, Matsuzaki et al. (2005) and Petrov et al. (2006) introduced latent variables to learn finergrained distinctions of treebank categories for parsing, and Huang et al. (2009) used a similar approach to learn finer-grained part-of-speech tags for tagging. Our method is in spirit similar to these approaches. Optimization of grammar parameters to maximize the likelihood of training forests can be achieved 8 • Σ is the set of terminals comprising treebank categories plus the CR tag (the crossing category), 7 The intermediate binarization nodes are also labeled as either X or B based on whether they exactly cover a phrase or not. 143 We incremental"
D10-1014,N09-1027,0,0.289186,"ries based on automatic parse trees. They introduced an extended set of categories (e.g., NP+V for she went and DTNP for great wall, an noun phrase with a missing determiner on the left) to annotate phrase pairs that do not align with syntactic constituents. Their hard syntactic constraint requires that the nonterminals should match exactly to rewrite with a rule, which could rule out potentially correct derivations due to errors in the syntactic parses as well as to data sparsity. For example, NP cannot be instantiated with phrase pairs of type DT+NN, in spite of their syntactic similarity. Venugopal et al. (2009) addressed this problem by directly introducing soft syntactic preferences into SCFG rules using preference grammars, but they had to face the computational challenges of large preference vectors. Chiang (2010) also avoided hard constraints and took a soft alternative that directly models the cost of mismatched rule substitutions. This, however, would require a large number of parameters to be tuned on a generally small-sized heldout set, and it could thus suffer from over-tuning. 3 Approach Overview In this work, we take a different approach to introduce linguistic syntax to hierarchical phra"
D10-1014,J97-3002,0,0.120398,"nter Institute for Advanced Computer Studies Yorktown Heights, NY 10598 University of Maryland {martin.cmejrek,zhou}@us.ibm.com College Park, MD 20742 zqhuang@umiacs.umd.edu Abstract into two classes based on their dependency on annotated corpus (Chiang, 2007). Linguistically syntaxbased models (e.g., (Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006)) utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank) and guide the derivation of SCFG rules with explicit parsing on at least one side of the parallel corpus. Formally syntax-based models (e.g., (Wu, 1997; Chiang, 2007)) extract synchronous grammars from parallel corpora based on the hierarchical structure of natural language pairs without any explicit linguistic knowledge or annotations. In this work, we focus on the hierarchical phrase-based models of Chiang (2007), which is formally syntax-based, and always refer the term SCFG, from now on, to the grammars of this model class. In this paper, we present a novel approach to enhance hierarchical phrase-based machine translation systems with linguistically motivated syntactic features. Rather than directly using treebank categories as in previo"
D10-1014,P09-1036,0,0.0573787,"N IN INTJ NP ADJP JJ PDT JJ RB JJ ADJP JJ JJ CC ADJP VB JJ JJ ADVP WHNP JJ ADJP IN NP JJ AUX RB ADJP ADJP VP Table 2: Examples of similar and dissimilar tag sequences. This cost can be reduced, however, by caching the dot-products of the tag sequences that are frequently accessed. There are other successful investigations to impose soft syntactic constraints to hierarchical phrase-based models by either introducing syntaxbased rule features such as the prior derivation model of Zhou et al. (2008) or by imposing constraints on translation spans at decoding time, e.g., (Marton and Resnik, 2008; Xiong et al., 2009; Xiong et al., 2010). These approaches are all orthogonal to ours and it is expected that they can be combined with our approach to achieve greater improvement. This work is an initial effort to investigate latent syntactic categories to enhance hierarchical phrasebased translation models, and there are many directions to continue this line of research. First, while the current approach imposes soft syntactic constraints between the parse structure of the source sentence and the SCFG rules used to derive the translation, the real-valued syntactic feature vectors can also be used to impose sof"
D10-1014,N10-1016,0,0.0417931,"PDT JJ RB JJ ADJP JJ JJ CC ADJP VB JJ JJ ADVP WHNP JJ ADJP IN NP JJ AUX RB ADJP ADJP VP Table 2: Examples of similar and dissimilar tag sequences. This cost can be reduced, however, by caching the dot-products of the tag sequences that are frequently accessed. There are other successful investigations to impose soft syntactic constraints to hierarchical phrase-based models by either introducing syntaxbased rule features such as the prior derivation model of Zhou et al. (2008) or by imposing constraints on translation spans at decoding time, e.g., (Marton and Resnik, 2008; Xiong et al., 2009; Xiong et al., 2010). These approaches are all orthogonal to ours and it is expected that they can be combined with our approach to achieve greater improvement. This work is an initial effort to investigate latent syntactic categories to enhance hierarchical phrasebased translation models, and there are many directions to continue this line of research. First, while the current approach imposes soft syntactic constraints between the parse structure of the source sentence and the SCFG rules used to derive the translation, the real-valued syntactic feature vectors can also be used to impose soft constraints between"
D10-1014,P01-1067,0,0.35577,"Missing"
D10-1014,C08-1136,0,0.038528,"s between words of the two sentences. Figure 1 (a) shows an example of aligned English-to-Chinese sentence pair. Widely adopted in phrase-based models (Och and Ney, 2004), a pair of consecutive sequences of words from E and F is a phrase pair if all words are aligned only within the sequences and not to any word outside. We call a sequence of words a phrase if it corresponds to either side of a phrase pair, and a non-phrase otherwise. Note that the boundary words of a phrase pair may not be aligned to any other word. We call the phrase pairs with all boundary words aligned tight phrase pairs (Zhang et al., 2008). A tight phrase pair is the minimal phrase pair among all that share the same set of alignment links. Figure 1 (b) highlights the tight phrase pairs in the example sentence pair. 6 5 4 3 2 1 1 (a) 2 3 4 5 (b) Figure 1: An example of word-aligned sentence pair (a) with tight phrase pairs marked in a matrix representation (b). The extraction of SCFG rules proceeds as follows. In the first step, all phrase pairs below a maximum length are extracted as phrasal rules. In the second step, abstract rules are extracted from tight phrase pairs that contain other tight phrase pairs by replacing the sub"
D10-1014,W08-0403,1,0.846488,"NP NN NN CD VP RB NP IN CD VP VB VB RB VB PP VB DT DT NN VP PP JJ NN VB NN NN VB VB RB IN JJ JJ NN TO VP JJ WHNP DT NN IN INTJ NP ADJP JJ PDT JJ RB JJ ADJP JJ JJ CC ADJP VB JJ JJ ADVP WHNP JJ ADJP IN NP JJ AUX RB ADJP ADJP VP Table 2: Examples of similar and dissimilar tag sequences. This cost can be reduced, however, by caching the dot-products of the tag sequences that are frequently accessed. There are other successful investigations to impose soft syntactic constraints to hierarchical phrase-based models by either introducing syntaxbased rule features such as the prior derivation model of Zhou et al. (2008) or by imposing constraints on translation spans at decoding time, e.g., (Marton and Resnik, 2008; Xiong et al., 2009; Xiong et al., 2010). These approaches are all orthogonal to ours and it is expected that they can be combined with our approach to achieve greater improvement. This work is an initial effort to investigate latent syntactic categories to enhance hierarchical phrasebased translation models, and there are many directions to continue this line of research. First, while the current approach imposes soft syntactic constraints between the parse structure of the source sentence and th"
D10-1014,W06-3119,0,0.812005,", USA, 9-11 October 2010. 2010 Association for Computational Linguistics icantly outperform hierarchical phrase-based models when using forest-based rule extraction together with forest-based decoding. Chiang (2010) also obtained significant improvement over his hierarchical baseline by using syntactic parse trees on both source and target sides to induce fuzzy (not exact) tree-to-tree rules and by also allowing syntactically mismatched substitutions. In this paper, we augment rules in hierarchical phrase-based translation systems with novel syntactic features. Unlike previous studies (e.g., (Zollmann and Venugopal, 2006)) that directly use explicit treebank categories such as NP, NP/PP (NP missing PP from the right) to annotate phrase pairs, we induce a set of latent categories to capture the syntactic dependencies inherent in the hierarchical structure of phrase pairs, and derive a real-valued feature vector for each X nonterminal of a SCFG rule based on the distribution of the latent categories. Moreover, we convert the equality test of two sequences of syntactic categories, which are either identical or different, into the computation of a similarity score between their corresponding feature vectors. In ou"
D10-1014,J07-2003,0,\N,Missing
D13-1048,N04-4026,0,\N,Missing
D13-1048,W09-2307,0,\N,Missing
D13-1048,N09-1053,0,\N,Missing
D13-1048,D08-1089,0,\N,Missing
D13-1048,W05-0823,0,\N,Missing
D13-1048,P11-2080,0,\N,Missing
D13-1048,W12-3125,0,\N,Missing
D13-1048,P07-2045,0,\N,Missing
D13-1048,P12-1095,0,\N,Missing
D13-1048,P05-1033,0,\N,Missing
D13-1048,N13-1001,0,\N,Missing
D13-1048,W06-3108,0,\N,Missing
D13-1048,J97-3002,0,\N,Missing
D13-1048,P09-1037,1,\N,Missing
D13-1048,P11-1086,0,\N,Missing
D13-1048,C10-2033,0,\N,Missing
D13-1048,P07-1090,1,\N,Missing
D13-1048,P08-1066,0,\N,Missing
D13-1048,P13-1124,1,\N,Missing
D13-1048,N13-1002,0,\N,Missing
D13-1048,D11-1125,0,\N,Missing
D13-1052,N09-1025,0,0.0609013,"Missing"
D13-1052,P05-1033,0,0.381694,"Missing"
D13-1052,N10-1141,0,0.0524598,"Missing"
D13-1052,N04-1035,0,0.133071,"Missing"
D13-1052,P06-1121,0,0.18684,"Missing"
D13-1052,D08-1011,0,0.0521456,"Missing"
D13-1052,2006.amta-papers.8,0,0.167307,"Missing"
D13-1052,N03-1017,0,0.00858727,"Missing"
D13-1052,P06-1077,0,0.130571,"Missing"
D13-1052,P09-1065,1,0.92946,"Missing"
D13-1052,D08-1022,1,0.907993,"Missing"
D13-1052,P08-1023,1,0.912261,"Missing"
D13-1052,N07-1051,0,0.121787,"Missing"
D13-1052,P07-1040,0,0.062928,"Missing"
D13-1052,P11-1125,0,0.0275882,"Missing"
D13-1052,D09-1108,0,0.354295,"Missing"
D13-1081,2007.mtsummit-papers.3,0,0.0302778,"(SMT) systems, for example (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009) to some extent mitigated this problem. In their work, there are as many as 10,000 features defined on the appearance of certain frequent words and Part of Speech (POS) tags in rules. They provide significant improvement in automatic evaluation metrics. However, these sparse features fire quite randomly and infrequently on each rule. Thus, there is still plenty of space to better model translation rules. In this paper, we will explore the relationship among translation rules. We no longer view rules as discrete or unrelated events. Instead, we view"
D13-1081,N09-1025,0,0.164183,"03; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009) to some extent mitigated this problem. In their work, there are as many as 10,000 features defined on the appearance of certain frequent words and Part of Speech (POS) tags in rules. They provide significant improvement in automatic evaluation metrics. However, these sparse features fire quite randomly and infrequently on each rule. Thus, there is still plenty of space to better model translation rules. In this paper, we will explore the relationship among translation rules. We no longer view rules as discrete or unrelated events. Instead, we view rules, which are observed from training data,"
D13-1081,P05-1033,0,0.0864848,"ion systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation. 1 Introduction Most of the modern Statistical Machine Translation (SMT) systems, for example (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009) to some extent"
D13-1081,D09-1076,0,0.0140727,"uctures surely do not have to be POS tags. For example, an alternative could be unsupervised NT splitting similar to (Huang et al., 2010). The meta-grammar based approach was also motivated by the insight acquired on mono-lingual linguistic grammar generation, especially in the TAG related research (Xia, 2001; Prolo, 2002). Metagrammar was viewed as an effective way to remove redundancy in grammars. The link between Tree Adjoining Grammar (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997) and MT was first introduced in (Shieber and Schabes, 1990), a pioneer work in tree-to-tree translation. (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. On the other 1 http://www.itl.nist.gov/iad/mig/tests/mt/2008/ 843 In this paper, we introduced a novel method to model translation rules as observed generation output of a compact hidden model. As a case study to capitalize this model, we presented three methods to enrich rule modeling with features defined on a hidden model. Preliminary experiments verified gain of one point on TER-BLEU over a strong baseline in Chinese-to-English translation. As for future work, we plan to extend th"
D13-1081,P99-1011,0,0.0425975,"Missing"
D13-1081,D10-1014,1,0.854674,"his shows the limitation of the generative feature. When we use meta-rules as binary sparse features in Type 2, we obtain about one point improvement on T-B on both sets. This shows the advantage of tuning individual meta-rule weights over a generative model. Type 3 (0.01) and Type 2 are at the same level. Proper smoothing is important to Type 3. 5 Discussion In the case study of Section 3, we use POS-based rules as hidden states. However, it should be noted that the hidden structures surely do not have to be POS tags. For example, an alternative could be unsupervised NT splitting similar to (Huang et al., 2010). The meta-grammar based approach was also motivated by the insight acquired on mono-lingual linguistic grammar generation, especially in the TAG related research (Xia, 2001; Prolo, 2002). Metagrammar was viewed as an effective way to remove redundancy in grammars. The link between Tree Adjoining Grammar (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997) and MT was first introduced in (Shieber and Schabes, 1990), a pioneer work in tree-to-tree translation. (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. O"
D13-1081,N03-1017,0,0.0186295,"Missing"
D13-1081,W06-1606,0,0.0284976,"ly on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation. 1 Introduction Most of the modern Statistical Machine Translation (SMT) systems, for example (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009) to some extent mitigated this prob"
D13-1081,D09-1074,0,0.0255345,"Missing"
D13-1081,J04-4002,0,0.0913975,"he machine translation systems rely on a large set of translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation. 1 Introduction Most of the modern Statistical Machine Translation (SMT) systems, for example (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009)"
D13-1081,2001.mtsummit-papers.68,0,0.0986495,"Missing"
D13-1081,C02-1153,0,0.0441994,"e of tuning individual meta-rule weights over a generative model. Type 3 (0.01) and Type 2 are at the same level. Proper smoothing is important to Type 3. 5 Discussion In the case study of Section 3, we use POS-based rules as hidden states. However, it should be noted that the hidden structures surely do not have to be POS tags. For example, an alternative could be unsupervised NT splitting similar to (Huang et al., 2010). The meta-grammar based approach was also motivated by the insight acquired on mono-lingual linguistic grammar generation, especially in the TAG related research (Xia, 2001; Prolo, 2002). Metagrammar was viewed as an effective way to remove redundancy in grammars. The link between Tree Adjoining Grammar (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997) and MT was first introduced in (Shieber and Schabes, 1990), a pioneer work in tree-to-tree translation. (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. On the other 1 http://www.itl.nist.gov/iad/mig/tests/mt/2008/ 843 In this paper, we introduced a novel method to model translation rules as observed generation output of a compact hidden m"
D13-1081,P08-1066,1,0.826463,"translation rules. These rules are treated as discrete and independent events. In this short paper, we propose a novel method to model rules as observed generation output of a compact hidden model, which leads to better generalization capability. We present a preliminary generative model to test this idea. Experimental results show about one point improvement on TER-BLEU over a strong baseline in Chinese-to-English translation. 1 Introduction Most of the modern Statistical Machine Translation (SMT) systems, for example (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009) to some extent mitigated this problem. In their work,"
D13-1081,C90-3045,0,0.383893,"d rules as hidden states. However, it should be noted that the hidden structures surely do not have to be POS tags. For example, an alternative could be unsupervised NT splitting similar to (Huang et al., 2010). The meta-grammar based approach was also motivated by the insight acquired on mono-lingual linguistic grammar generation, especially in the TAG related research (Xia, 2001; Prolo, 2002). Metagrammar was viewed as an effective way to remove redundancy in grammars. The link between Tree Adjoining Grammar (TAG) (Joshi et al., 1975; Joshi and Schabes, 1997) and MT was first introduced in (Shieber and Schabes, 1990), a pioneer work in tree-to-tree translation. (DeNeefe and Knight, 2009) re-visited the use of adjoining operation in the context of Statistical MT, and reported encouraging results. On the other 1 http://www.itl.nist.gov/iad/mig/tests/mt/2008/ 843 In this paper, we introduced a novel method to model translation rules as observed generation output of a compact hidden model. As a case study to capitalize this model, we presented three methods to enrich rule modeling with features defined on a hidden model. Preliminary experiments verified gain of one point on TER-BLEU over a strong baseline in"
D13-1081,2006.amta-papers.25,0,0.0486754,"Missing"
D13-1081,D07-1080,0,0.0203235,"ample (Koehn et al., 2003; Och and Ney, 2004; Chiang, 2005; Marcu et al., 2006; Shen et al., 2008), employ a large rule set that may contain tens of millions of translation rules or even more. In these systems, each translation rule has about 20 dense features, which represent key statistics collected from the training data, such as word translation probability, phrase translation probability etc. Except for these common features, there is no connection among the translation rules. The translation rules are treated as independent events. The use of sparse features as in (Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2009) to some extent mitigated this problem. In their work, there are as many as 10,000 features defined on the appearance of certain frequent words and Part of Speech (POS) tags in rules. They provide significant improvement in automatic evaluation metrics. However, these sparse features fire quite randomly and infrequently on each rule. Thus, there is still plenty of space to better model translation rules. In this paper, we will explore the relationship among translation rules. We no longer view rules as discrete or unrelated events. Instead, we view rules, which are observ"
D13-1081,P02-1040,0,\N,Missing
D13-1081,W90-0102,0,\N,Missing
D13-1083,W09-1114,0,0.0190866,"tences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To avoid the sentence BLEU, the work in (Haddow et al., 2011) proposed to process sentences in small batches. The authors adopted a Gibbs sampling (Arun et al., 2009) technique to search the hope and fear hypotheses, and they did not compare with MIRA. Watanabe (2012) also tuned the parameters with small batches of sentences and optimized a hinge loss not explicitly related to BLEU using stochastic gradient descent. Both approaches introduced additional complexities over baseline MIRA approaches. In contrast, we propose a remarkably simple but efficient batch MIRA approach which exploits the exact corpus-level BLEU to compute model losses. We search for a hope and a fear hypotheses for the corpus with a straightforward approach and minimize the structured"
D13-1083,N12-1047,0,0.542423,"proaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations. 1 Introduction Margin infused relaxed algorithm (MIRA) has been widely adopted for the parameter optimization in SMT with a large feature size (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Eidelman, 2012; Cherry and Foster, 2012). Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To avoid the sentence BLEU, the work in (Haddow et al., 2011) proposed to process"
D13-1083,N09-1025,0,0.0590043,"orpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations. 1 Introduction Margin infused relaxed algorithm (MIRA) has been widely adopted for the parameter optimization in SMT with a large feature size (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Eidelman, 2012; Cherry and Foster, 2012). Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To avoid the sentence BLEU"
D13-1083,D08-1024,0,0.390192,"features. Since the corpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations. 1 Introduction Margin infused relaxed algorithm (MIRA) has been widely adopted for the parameter optimization in SMT with a large feature size (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Eidelman, 2012; Cherry and Foster, 2012). Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To av"
D13-1083,J07-2003,0,0.0257539,"MIRA can moderately improve BLEU by 0.2∼0.4 from MIRA1 and 0.2∼0.6 from MIRA2 . This might indicate that a loss defined on corpus is more accurate than the one defined on sentence. Table 2 lists the running time. Only MIRA2 is fairly faster than c-MIRA because of more epochs in c-MIRA. 4.2 Iterative Batch Training In this experiment, we conduct the batch tuning procedure shown in section 3. We align the FBIS data including about 230K sentence pairs with GIZA++ for extracting grammar, and train a 4-gram language model on the Xinhua portion of Gigaword corpus. A hierarchical phrase-based model (Chiang, 2007) is tuned on NIST MT 2002, which has 878 sentences, and tested on MT 2004, 2005, 2006, and 2008. All features used here, besides eight basic ones in (Chiang, 2007), consists of an extra 220 group features. We design such feature templates to group grammar by the length of source side and target side, (f eat type, a ≤ src side ≤ b, c ≤ tgt side ≤ d) , where f eat type denotes any of relative frequency, reversed relative frequency, lexical probability and reversed lexical probability, and [a, b], [c, d] enumerate all possible subranges of [1, 10], as the maximum 854 Re-ranking Experiments The ba"
D13-1083,W12-3160,0,0.237266,"e, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations. 1 Introduction Margin infused relaxed algorithm (MIRA) has been widely adopted for the parameter optimization in SMT with a large feature size (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Eidelman, 2012; Cherry and Foster, 2012). Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To avoid the sentence BLEU, the work in (Haddow et al.,"
D13-1083,W11-2130,0,0.461368,"Eidelman, 2012; Cherry and Foster, 2012). Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To avoid the sentence BLEU, the work in (Haddow et al., 2011) proposed to process sentences in small batches. The authors adopted a Gibbs sampling (Arun et al., 2009) technique to search the hope and fear hypotheses, and they did not compare with MIRA. Watanabe (2012) also tuned the parameters with small batches of sentences and optimized a hinge loss not explicitly related to BLEU using stochastic gradient descent. Both approaches introduced additional complexities over baseline MIRA approaches. In contrast, we propose a remarkably simple but efficient batch MIRA approach which exploits the exact corpus-level BLEU to compute model losses. We search for"
D13-1083,P07-2045,0,0.0075534,"n 50,000 features. In both experiments, we compare c-MIRA and three baselines: (1) MERT (Och, 2003), (2) Chiang et al.’s MIRA (MIRA1 ) in (Chiang et al., 2008). (3) batch-MIRA (MIRA2 ) in (Cherry and Foster, 2012). Here, we roughly choose C with the best BLEU on dev set, from {0.1, 0.01, 0.001, 0.0001, 0.00001}. We convert Chiang et al.’s MIRA to the batch mode described in section 3.1. So the only difference between MIRA1 and MIRA2 is: MIRA1 obtains multiple constraints before optimization, while MIRA2 only uses one constraint. We implement MERT and MIRA1 , and directly use MIRA2 from Moses (Koehn et al., 2007). We conduct experiments in a server of 8-cores with 2.5GHz Opteron. We set the maximum number of epochs as we generally do not observe an obvious increase on the dev set BLEU. MERT C MIRA1 MIRA2 c-MIRA 0.0001 0.001 0.0001 8 dev 34.80 34.70 34.73 34.70 feat. 04 31.92 31.81 31.73 31.83 05 28.85 28.94 28.71 28.92 0.001 0.001 0.001 35.24 35.14 35.56 C all dev 34.61 feat. 04 31.76 32.25 32.04 32.57+ 05 28.85 29.43 29.37 29.41 06news 30.91 31.43 31.24 31.82+ 06others 27.43 28.01 28.13 28.45 08news 25.62 26.11 26.03 26.40 08others 16.22 16.66 16.46 17.10+ R. T. MIRA1 MIRA2 c-MIRA 25.8min 16.0min 7.3"
D13-1083,P06-1096,0,0.488493,"parameters. The objective for each sentence i is, 2008), c-MIRA repeatedly optimizes, min w min w li (w) = 1 ||w − w0 ||2 + C · li (w) 2 max {b(ei∗ ) − b(eij ) lcorpus (w) = (1) eij −w · [h(ei∗ ) − h(eij )]} (2) where e∗i ∈ ei is a hope candidate, w0 is the parameter vector from the last sentence. Since MIRA defines its objective only based on the current sentence, b(·) is a sentence-level BLEU. Most MIRA algorithms need a deliberate definition of b(·), since BLEU cannot be decomposed into sentences. The types of the sentence BLEU calculation includes: (a) a smoothed version of BLEU for eij (Liang et al., 2006), (b) fit eij into a pseudodocument considering the history (Chiang et al., 2008; Chiang, 2012), (c) use eij to replace the corresponding hypothesis in the oracles (Watanabe et al., 2007). The sentence-level BLEU sometimes perplexes the algorithms and results in a mismatch with the corpus-level BLEU. 3.1 Corpus-level MIRA E −w · [H(E ∗ ) − H(E)]} (4) where B(·) is a corpus-level BLEU. E ∗ is a hope hypothesis. E ∈ L, where L is the hypothesis space of the entire corpus, and |L |= |e1 |· · · |eM |. Algorithm 1 Corpus-Level MIRA Require: {(fi , ei , ri )}M i=1 , w0 , C 1: for t = 1 · · · T do 2:"
D13-1083,C04-1072,0,0.132915,"arch the entire space of L for precise solution E ∗ and E 0 , because MIRA only attempts to separate the hope from the fear by a margin proportional to their BLEU differentials (Cherry and Foster, 2012). We just construct E ∗ and E 0 respectively by, loss of c-MIRA in Eq. 4 is, lcorpus (w) ∝ max 0 E M X [B(ei,kE ∗ ) − B(ei,kE 0 ) i=1 −w·h(ei,kE ∗ ) + w · h(ei,kE 0 )] eE ∗ ,i = max[w · h(ei,j ) + b0 (ei,j )] = ei,j M X i=1 0 max[B(ei,kE ∗ ) − B(eij ) eij eE 0 ,i = max[w · h(ei,j ) − b (ei,j )] ei,j −w·h(ei,kE ∗ ) + w · h(eij )] = M X li (w) i=1 where b0 is simply a BLEU with add one smoothing (Lin and Och, 2004). A smoothed BLEU is good enough to pick up a “satisfying” pair of hope and fear. However, the updating step (Line 11) uses the corpus-level BLEU. 3.2 Justification c-MIRA treats a corpus as one sentence for decoding, while conventional decoders process sentences one by one. We show the optimal solutions from the two methods are equivalent theoretically. We follow the notations in (Och and Ney, 2002). We search a hypothesis on corpus E = {e1 ,k1 , e2 ,k2 , ..., eM ,kM } with the highest probability given the source corpus F = {f1 , f2 , ..., fM }, E = = arg max logP (E|F) E arg max w · E M X h"
D13-1083,P03-1021,0,0.114682,"ss definitions differ fundamentally. Batch MIRA basically uses a sentence-level loss, and they also follow the sentence-by-sentence tuning pattern. In the future work, we will compare structural SVM and c-MIRA under decomposable metrics like WER or SSER (Och and Ney, 2002). 4 Experiments and Analysis We first evaluate c-MIRA in a iterative batch tuning procedure in a Chinese-to-English machine translation system with 228 features. Second, we show cMIRA is also effective in the re-ranking task with more than 50,000 features. In both experiments, we compare c-MIRA and three baselines: (1) MERT (Och, 2003), (2) Chiang et al.’s MIRA (MIRA1 ) in (Chiang et al., 2008). (3) batch-MIRA (MIRA2 ) in (Cherry and Foster, 2012). Here, we roughly choose C with the best BLEU on dev set, from {0.1, 0.01, 0.001, 0.0001, 0.00001}. We convert Chiang et al.’s MIRA to the batch mode described in section 3.1. So the only difference between MIRA1 and MIRA2 is: MIRA1 obtains multiple constraints before optimization, while MIRA2 only uses one constraint. We implement MERT and MIRA1 , and directly use MIRA2 from Moses (Koehn et al., 2007). We conduct experiments in a server of 8-cores with 2.5GHz Opteron. We set the"
D13-1083,P02-1038,0,0.407817,"ei,j )] = ei,j M X i=1 0 max[B(ei,kE ∗ ) − B(eij ) eij eE 0 ,i = max[w · h(ei,j ) − b (ei,j )] ei,j −w·h(ei,kE ∗ ) + w · h(eij )] = M X li (w) i=1 where b0 is simply a BLEU with add one smoothing (Lin and Och, 2004). A smoothed BLEU is good enough to pick up a “satisfying” pair of hope and fear. However, the updating step (Line 11) uses the corpus-level BLEU. 3.2 Justification c-MIRA treats a corpus as one sentence for decoding, while conventional decoders process sentences one by one. We show the optimal solutions from the two methods are equivalent theoretically. We follow the notations in (Och and Ney, 2002). We search a hypothesis on corpus E = {e1 ,k1 , e2 ,k2 , ..., eM ,kM } with the highest probability given the source corpus F = {f1 , f2 , ..., fM }, E = = arg max logP (E|F) E arg max w · E M X h(ei,ki ) − i=1 = {arg max w · h(ei,ki )}M i=1 ei,ki M X ! log(Zi ) (7) i=1 (8) PN (fi ) where Zi = j=1 exp(w · h(ei,j )), which is a constant with respective to E. Eq. 7 shows that the feature vector of E is determined by the sum of each candidate’s feature vectors. Also, the model score can be decomposed into each sentence in Eq. 8, which shows that decoding all sentences together equals to decoding"
D13-1083,P02-1040,0,0.0888733,"toEnglish translation tasks with a moderate margin. 2 Margin Infused Relaxed Algorithm We optimize the model parameters based on N-best lists. Our development (dev) set is a set of triples {(fi , ei , ri )}M i=1 , where fi is a source-language sentence, corresponded by a list of target-language hyN (f ) potheses ei = {eij }j=1i , with a number of references ri . h(e ij ) is a feature vector. Generally, most decoders return a top-1 candidate as the translation result, such that e¯i (w) = arg maxj w · h(eij ), where w are the model parameters. In this paper, we aim at optimizing the BLEU score (Papineni et al., 2002). MIRA is an instance of online learning which assumes an overlap of the decoding procedure and the parameter optimization procedure. For example in (Crammer et al., 2006; Chiang et al., 2008), MIRA 851 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 851–856, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics is performed after an input sentence are decoded, and the next sentence is decoded with the updated parameters. The objective for each sentence i is, 2008), c-MIRA repeatedly optimizes, min w min w l"
D13-1083,N12-1026,0,0.254724,"ch are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm the performance. To avoid the sentence BLEU, the work in (Haddow et al., 2011) proposed to process sentences in small batches. The authors adopted a Gibbs sampling (Arun et al., 2009) technique to search the hope and fear hypotheses, and they did not compare with MIRA. Watanabe (2012) also tuned the parameters with small batches of sentences and optimized a hinge loss not explicitly related to BLEU using stochastic gradient descent. Both approaches introduced additional complexities over baseline MIRA approaches. In contrast, we propose a remarkably simple but efficient batch MIRA approach which exploits the exact corpus-level BLEU to compute model losses. We search for a hope and a fear hypotheses for the corpus with a straightforward approach and minimize the structured hinge loss defined on them. The experiments show that our method consistently outperforms two state-of"
D13-1083,D07-1080,0,0.716354,"with a large number of features. Since the corpus-level BLEU is not decomposable, these MIRA approaches usually define a variety of heuristic-driven sentencelevel BLEUs in their model losses. Instead, we present a new MIRA method, which employs an exact corpus-level BLEU to compute the model loss. Our method is simpler in implementation. Experiments on Chinese-toEnglish translation show its effectiveness over two state-of-the-art MIRA implementations. 1 Introduction Margin infused relaxed algorithm (MIRA) has been widely adopted for the parameter optimization in SMT with a large feature size (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Eidelman, 2012; Cherry and Foster, 2012). Since BLEU is defined on the corpus, and not decomposed into sentences, most MIRA approaches consider a variety of sentence-level BLEUs for the model losses, many of which are heuristic-driven (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009; Chiang, 2012; Cherry and Foster, 2012). The sentence-level BLEU appearing in the objective is generally based on a pseudo-document, which may not precisely reflect the corpus-level BLEU. We believe that this mismatch could potentially harm t"
D16-1223,H94-1010,0,0.121505,"tead, we directly fed the input word into the labeler part with using context window method as explained in Section 2.3. 3 Experiments We report two sets of experiments. First we use the standard ATIS corpus to confirm the improvement by the proposed encoder-labeler LSTM and compare our results with the published results while discussing the related works. Then we use a large-scale data set to confirm the effect of the proposed method in a realistic use-case. 3.1 ATIS Experiment 3.1.1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010). Figure 2 shows an example sentence and its seman2079 tic slot labels in In-Out-Begin (IOB) representation. The slot filling task was to predict the slot label sequences from input word sequences. The performance was measured by the F1 -score: recision×Recall F1 = 2×P P recision+Recall , where precision is the ratio of the correct labels in the system’s output and recall is the ratio of the correct labels in the ground truth of the evaluation data (van Rijsbergen, 1979). The ATIS corpus contains the training data of 4,978 sentences and evaluation data of"
D16-1223,K16-1028,1,0.595295,"Missing"
D16-1223,H90-1020,0,0.139898,"smaller. Instead, we directly fed the input word into the labeler part with using context window method as explained in Section 2.3. 3 Experiments We report two sets of experiments. First we use the standard ATIS corpus to confirm the improvement by the proposed encoder-labeler LSTM and compare our results with the published results while discussing the related works. Then we use a large-scale data set to confirm the effect of the proposed method in a realistic use-case. 3.1 ATIS Experiment 3.1.1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010). Figure 2 shows an example sentence and its seman2079 tic slot labels in In-Out-Begin (IOB) representation. The slot filling task was to predict the slot label sequences from input word sequences. The performance was measured by the F1 -score: recision×Recall F1 = 2×P P recision+Recall , where precision is the ratio of the correct labels in the system’s output and recall is the ratio of the correct labels in the ground truth of the evaluation data (van Rijsbergen, 1979). The ATIS corpus contains the training data of 4,978 sentences and"
D16-1223,P06-2113,0,0.0276978,"ed the input word into the labeler part with using context window method as explained in Section 2.3. 3 Experiments We report two sets of experiments. First we use the standard ATIS corpus to confirm the improvement by the proposed encoder-labeler LSTM and compare our results with the published results while discussing the related works. Then we use a large-scale data set to confirm the effect of the proposed method in a realistic use-case. 3.1 ATIS Experiment 3.1.1 Experimental Setup We used the ATIS corpus, which has been widely used as the benchmark for NLU (Price, 1990; Dahl et al., 1994; Wang et al., 2006; Tur et al., 2010). Figure 2 shows an example sentence and its seman2079 tic slot labels in In-Out-Begin (IOB) representation. The slot filling task was to predict the slot label sequences from input word sequences. The performance was measured by the F1 -score: recision×Recall F1 = 2×P P recision+Recall , where precision is the ratio of the correct labels in the system’s output and recall is the ratio of the correct labels in the ground truth of the evaluation data (van Rijsbergen, 1979). The ATIS corpus contains the training data of 4,978 sentences and evaluation data of 893 sentences. The"
D16-1223,P15-1109,0,0.0100616,"ummarization (Nallapati et al., 2016) and so on. The difference is that the proposed encoder-labeler LSTM accepts the same input sequence twice while the usual encoder-decoder LSTM accepts the input sequence once in the encoder. Note that the LSTMs for encoding and labeling are different in the encoder-labeler LSTM, but the same word embedding matrix is used both for the encoder and labeler since the same input sequence is fed twice. 2.4 Related Work on Considering Sentence-level Information Bi-directional RNN/LSTM have been proposed to capture sentence-level information (Mesnil et al., 2015; Zhou and Xu, 2015; Vu et al., 2016). While the bi-directional RNN/LSTM model the preceding and succeeding contexts at a specific word and O O O O O O B-ToCity I need a ticket to Seattle O &lt;B&gt; O I (a) Labeler LSTM(W). O O need O O O a B-ToCity O ticket O to Seattle to ticket a need I O O &lt;B&gt; O Encoder (backward) LSTM Seattle (b) Labeler LSTM(W+L). O I need O O O B-ToCity to ticket a need I Encoder LSTM (backward) a ticket to Seattle Labeler LSTM(W) to ticket a need I O &lt;B&gt; B-ToCity O O O O O O I Encoder LSTM (backward) (d) Encoder-labeler LSTM(W). O (c) Encoder-decoder LSTM. Seattle Seattle O Decoder LSTM O O O"
K16-1028,P16-1014,1,0.434022,"atures. Pointer networks (Vinyals et al., 2015) have also been used earlier for the problem of rare words in the context of machine translation (Luong et al., 2015), but the novel addition of switch in our model allows it to strike a balance between when to be faithful to the original source (e.g., for named entities and OOV) and when it is allowed to be creative. We believe such a process arguably mimics how human produces summaries. For a more detailed treatment of this model, and experiments on multiple tasks, please refer to the parallel work published by some of the authors of this work (Gulcehre et al., 2016). Hierarchical attention model (Sec. 2.4): Previously proposed hierarchical encoder-decoder models use attention only at sentence-level (Li et al., 2015). The novelty of our approach lies in joint modeling of attention at both sentence and word levels, where the word-level attention is further influenced by sentence-level attention, thus capturing the notion of important sentences and important words within those sentences. Concatenation of positional embeddings with the hidden state at sentence-level is also new. 4 (2015). We used the scripts made available by the authors of this work4 to pre"
K16-1028,H90-1087,0,0.206907,"Missing"
K16-1028,P00-1041,0,0.534682,"UC2003 and DUC-2004 competitions.2 The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans. The best performing system on the DUC-2004 task, called TOPIARY (Zajic et al., 2004), used a combination of linguistically motivated compression techniques, and an unsupervised topic detection algorithm that appends keywords extracted from the article onto the compressed output. Some of the other notable work in the task of abstractive summarization includes using traditional phrase-table based machine translation approaches (Banko et al., 2000), compression using weighted tree-transformation rules (Cohn and Lapata, 2008) and quasi-synchronous grammar approaches (Woodsend et al., 2010). With the emergence of deep learning as a viable alternative for many NLP tasks (Collobert et al., 2011), researchers have started considering this framework as an attractive, fully data-driven alternative to abstractive summarization. In Rush et al. (2015), the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword an"
K16-1028,D15-1229,0,0.137711,"considering this framework as an attractive, fully data-driven alternative to abstractive summarization. In Rush et al. (2015), the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, Chopra et al. (2016) used a similar convolutional model for the encoder, but replaced the decoder with an RNN, producing further improvement in performance on both datasets. In another paper that is closely related to our work, Hu et al. (2015) introduce a large dataset for Chinese short text summarization. They show promising results on their Chinese dataset using an encoder-decoder RNN, but do not report experiments on English corpora. In another very recent work, Cheng and Lapata (2016) used RNN based encoder-decoder for extractive summarization of documents. Our work starts with the same framework as (Hu et al., 2015), but we go beyond the stan, HiddenState Wordlayer HiddenState Sentencelayer OutputLayer where Pwa (j) is the word-level attention weight at j th position of the source document, and s(j) is the ID of the sentence a"
K16-1028,P16-1046,0,0.384532,"network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, Chopra et al. (2016) used a similar convolutional model for the encoder, but replaced the decoder with an RNN, producing further improvement in performance on both datasets. In another paper that is closely related to our work, Hu et al. (2015) introduce a large dataset for Chinese short text summarization. They show promising results on their Chinese dataset using an encoder-decoder RNN, but do not report experiments on English corpora. In another very recent work, Cheng and Lapata (2016) used RNN based encoder-decoder for extractive summarization of documents. Our work starts with the same framework as (Hu et al., 2015), but we go beyond the stan, HiddenState Wordlayer HiddenState Sentencelayer OutputLayer where Pwa (j) is the word-level attention weight at j th position of the source document, and s(j) is the ID of the sentence at j th word position, Psa (l) is the sentence-level attention weight for the lth sentence in the source, Nd is the number of words in the source document, and P a (j) is the re-scaled attention at the j th word position. The re-scaled attention is th"
K16-1028,D16-1053,0,0.0194114,"Missing"
K16-1028,N16-1012,0,0.712569,"on rules (Cohn and Lapata, 2008) and quasi-synchronous grammar approaches (Woodsend et al., 2010). With the emergence of deep learning as a viable alternative for many NLP tasks (Collobert et al., 2011), researchers have started considering this framework as an attractive, fully data-driven alternative to abstractive summarization. In Rush et al. (2015), the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, Chopra et al. (2016) used a similar convolutional model for the encoder, but replaced the decoder with an RNN, producing further improvement in performance on both datasets. In another paper that is closely related to our work, Hu et al. (2015) introduce a large dataset for Chinese short text summarization. They show promising results on their Chinese dataset using an encoder-decoder RNN, but do not report experiments on English corpora. In another very recent work, Cheng and Lapata (2016) used RNN based encoder-decoder for extractive summarization of documents. Our work starts with the same framework as (Hu et a"
K16-1028,P15-1107,0,0.0605365,"Missing"
K16-1028,W08-1404,0,0.0472543,"g token from the source document can be displayed in the summary even when it is not part of the training vocabulary either on the source side or the target side. RNNs on the source side, one at the word level and the other at the sentence level. The attention mechanism operates at both levels simultaneously. The word-level attention is further re-weighted by the corresponding sentence-level attention and renormalized as shown below: P a (j) = Pwa (j)Psa (s(j)) PNd a a k=1 Pw (k)Ps (s(k)) al., 2002; Erkan and Radev, 2004; Wong et al., 2008a; Filippova and Altun, 2013; Colmenares et al., 2015; Litvak and Last, 2008; K. Riedhammer and Hakkani-Tur, 2010; Ricardo Ribeiro, 2013). Humans on the other hand, tend to paraphrase the original story in their own words. As such, human summaries are abstractive in nature and seldom consist of reproduction of original sentences from the document. The task of abstractive summarization has been standardized using the DUC2003 and DUC-2004 competitions.2 The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans. The best performing system on the DUC-2004 task, called TOPIARY (Zajic et al., 2004),"
K16-1028,C08-1018,0,0.00948802,"s stories from various topics with multiple reference summaries per story generated by humans. The best performing system on the DUC-2004 task, called TOPIARY (Zajic et al., 2004), used a combination of linguistically motivated compression techniques, and an unsupervised topic detection algorithm that appends keywords extracted from the article onto the compressed output. Some of the other notable work in the task of abstractive summarization includes using traditional phrase-table based machine translation approaches (Banko et al., 2000), compression using weighted tree-transformation rules (Cohn and Lapata, 2008) and quasi-synchronous grammar approaches (Woodsend et al., 2010). With the emergence of deep learning as a viable alternative for many NLP tasks (Collobert et al., 2011), researchers have started considering this framework as an attractive, fully data-driven alternative to abstractive summarization. In Rush et al. (2015), the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, Chopra et al. (2016) used a simi"
K16-1028,P15-1002,0,0.0271305,"., 2008b), but they are novel in the context of deep learning approaches for abstractive summarization, to the best of our knowledge. Switching generator-pointer model (Sec. 2.3): This model combines extractive and abstractive approaches to summarization in a single end-toend framework. Rush et al. (2015) also used a combination of extractive and abstractive approaches, but their extractive model is a separate log-linear classifier with handcrafted features. Pointer networks (Vinyals et al., 2015) have also been used earlier for the problem of rare words in the context of machine translation (Luong et al., 2015), but the novel addition of switch in our model allows it to strike a balance between when to be faithful to the original source (e.g., for named entities and OOV) and when it is allowed to be creative. We believe such a process arguably mimics how human produces summaries. For a more detailed treatment of this model, and experiments on multiple tasks, please refer to the parallel work published by some of the authors of this work (Gulcehre et al., 2016). Hierarchical attention model (Sec. 2.4): Previously proposed hierarchical encoder-decoder models use attention only at sentence-level (Li et"
K16-1028,N15-1014,0,0.012696,"s known, the corresponding token from the source document can be displayed in the summary even when it is not part of the training vocabulary either on the source side or the target side. RNNs on the source side, one at the word level and the other at the sentence level. The attention mechanism operates at both levels simultaneously. The word-level attention is further re-weighted by the corresponding sentence-level attention and renormalized as shown below: P a (j) = Pwa (j)Psa (s(j)) PNd a a k=1 Pw (k)Ps (s(k)) al., 2002; Erkan and Radev, 2004; Wong et al., 2008a; Filippova and Altun, 2013; Colmenares et al., 2015; Litvak and Last, 2008; K. Riedhammer and Hakkani-Tur, 2010; Ricardo Ribeiro, 2013). Humans on the other hand, tend to paraphrase the original story in their own words. As such, human summaries are abstractive in nature and seldom consist of reproduction of original sentences from the document. The task of abstractive summarization has been standardized using the DUC2003 and DUC-2004 competitions.2 The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans. The best performing system on the DUC-2004 task, called TOPIAR"
K16-1028,D13-1155,0,0.0863677,"he RNN. Once the position is known, the corresponding token from the source document can be displayed in the summary even when it is not part of the training vocabulary either on the source side or the target side. RNNs on the source side, one at the word level and the other at the sentence level. The attention mechanism operates at both levels simultaneously. The word-level attention is further re-weighted by the corresponding sentence-level attention and renormalized as shown below: P a (j) = Pwa (j)Psa (s(j)) PNd a a k=1 Pw (k)Ps (s(k)) al., 2002; Erkan and Radev, 2004; Wong et al., 2008a; Filippova and Altun, 2013; Colmenares et al., 2015; Litvak and Last, 2008; K. Riedhammer and Hakkani-Tur, 2010; Ricardo Ribeiro, 2013). Humans on the other hand, tend to paraphrase the original story in their own words. As such, human summaries are abstractive in nature and seldom consist of reproduction of original sentences from the document. The task of abstractive summarization has been standardized using the DUC2003 and DUC-2004 competitions.2 The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans. The best performing system on the DUC"
K16-1028,D15-1044,0,0.780207,"ed from the article onto the compressed output. Some of the other notable work in the task of abstractive summarization includes using traditional phrase-table based machine translation approaches (Banko et al., 2000), compression using weighted tree-transformation rules (Cohn and Lapata, 2008) and quasi-synchronous grammar approaches (Woodsend et al., 2010). With the emergence of deep learning as a viable alternative for many NLP tasks (Collobert et al., 2011), researchers have started considering this framework as an attractive, fully data-driven alternative to abstractive summarization. In Rush et al. (2015), the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, Chopra et al. (2016) used a similar convolutional model for the encoder, but replaced the decoder with an RNN, producing further improvement in performance on both datasets. In another paper that is closely related to our work, Hu et al. (2015) introduce a large dataset for Chinese short text summarization. They show promising results on their Chinese d"
K16-1028,C08-1124,0,0.733791,"’ token encoded by the RNN. Once the position is known, the corresponding token from the source document can be displayed in the summary even when it is not part of the training vocabulary either on the source side or the target side. RNNs on the source side, one at the word level and the other at the sentence level. The attention mechanism operates at both levels simultaneously. The word-level attention is further re-weighted by the corresponding sentence-level attention and renormalized as shown below: P a (j) = Pwa (j)Psa (s(j)) PNd a a k=1 Pw (k)Ps (s(k)) al., 2002; Erkan and Radev, 2004; Wong et al., 2008a; Filippova and Altun, 2013; Colmenares et al., 2015; Litvak and Last, 2008; K. Riedhammer and Hakkani-Tur, 2010; Ricardo Ribeiro, 2013). Humans on the other hand, tend to paraphrase the original story in their own words. As such, human summaries are abstractive in nature and seldom consist of reproduction of original sentences from the document. The task of abstractive summarization has been standardized using the DUC2003 and DUC-2004 competitions.2 The data for these tasks consists of news stories from various topics with multiple reference summaries per story generated by humans. The best"
K16-1028,D10-1050,0,0.0118157,"er story generated by humans. The best performing system on the DUC-2004 task, called TOPIARY (Zajic et al., 2004), used a combination of linguistically motivated compression techniques, and an unsupervised topic detection algorithm that appends keywords extracted from the article onto the compressed output. Some of the other notable work in the task of abstractive summarization includes using traditional phrase-table based machine translation approaches (Banko et al., 2000), compression using weighted tree-transformation rules (Cohn and Lapata, 2008) and quasi-synchronous grammar approaches (Woodsend et al., 2010). With the emergence of deep learning as a viable alternative for many NLP tasks (Collobert et al., 2011), researchers have started considering this framework as an attractive, fully data-driven alternative to abstractive summarization. In Rush et al. (2015), the authors use convolutional models to encode the source, and a context-sensitive attentional feed-forward neural network to generate the summary, producing state-of-the-art results on Gigaword and DUC datasets. In an extension to this work, Chopra et al. (2016) used a similar convolutional model for the encoder, but replaced the decoder"
K19-1070,D17-1215,0,0.0366705,"ecision. For example, “Northridge earthquake” is mistakenly taken as the answer to the question about what earthquake caused $20 million in damage. Because “$20 billon” is positioned far away from “Northridge earthquake”, it is hard for a model to link these two concepts together and recognize the mismatch of “$20 million” in the question and “$20 billion” in the context. Introduction Ever since the release of many challenging large scale datasets for machine reading comprehension (MRC) (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2016; Yang et al., 2018; Reddy et al., 2018; Jia and Liang, 2017), there have been correspondingly many models for these datasets (Yu et al., 2018; Seo et al., 2017; Liu et al., 2018b; Hu et al., 2017; Xiong et al., 2017; Wang et al., 2018; Liu et al., 2018c; Tay et al., 2018). Knowing what you don’t know (Rajpurkar et al., 2018) is important in real applications of reading comprehension. Unanswerable questions are commonplace in the real world, and SQuAD 2.0 was released specifically to target this problem (see Figure 1 for an example of non-answerable questions). Motivated by exploiting high level semantic relationships in the context, our first step is t"
K19-1070,P17-1147,0,0.0333028,"el semantics in the context are helpful to make better answerable or unanswerable decision. For example, “Northridge earthquake” is mistakenly taken as the answer to the question about what earthquake caused $20 million in damage. Because “$20 billon” is positioned far away from “Northridge earthquake”, it is hard for a model to link these two concepts together and recognize the mismatch of “$20 million” in the question and “$20 billion” in the context. Introduction Ever since the release of many challenging large scale datasets for machine reading comprehension (MRC) (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2016; Yang et al., 2018; Reddy et al., 2018; Jia and Liang, 2017), there have been correspondingly many models for these datasets (Yu et al., 2018; Seo et al., 2017; Liu et al., 2018b; Hu et al., 2017; Xiong et al., 2017; Wang et al., 2018; Liu et al., 2018c; Tay et al., 2018). Knowing what you don’t know (Rajpurkar et al., 2018) is important in real applications of reading comprehension. Unanswerable questions are commonplace in the real world, and SQuAD 2.0 was released specifically to target this problem (see Figure 1 for an example of non-answerable questions). Motivate"
K19-1070,P18-1157,0,0.0301885,"Missing"
K19-1070,P18-1158,0,0.0149095,"ioned far away from “Northridge earthquake”, it is hard for a model to link these two concepts together and recognize the mismatch of “$20 million” in the question and “$20 billion” in the context. Introduction Ever since the release of many challenging large scale datasets for machine reading comprehension (MRC) (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2016; Yang et al., 2018; Reddy et al., 2018; Jia and Liang, 2017), there have been correspondingly many models for these datasets (Yu et al., 2018; Seo et al., 2017; Liu et al., 2018b; Hu et al., 2017; Xiong et al., 2017; Wang et al., 2018; Liu et al., 2018c; Tay et al., 2018). Knowing what you don’t know (Rajpurkar et al., 2018) is important in real applications of reading comprehension. Unanswerable questions are commonplace in the real world, and SQuAD 2.0 was released specifically to target this problem (see Figure 1 for an example of non-answerable questions). Motivated by exploiting high level semantic relationships in the context, our first step is to extract meaningful high-level semantics from question/context. Multi-head self-attentive pooling 747 Proceedings of the 23rd Conference on Computational Natural Language Le"
K19-1070,N18-1202,0,0.0311034,"ule on top of the base reader BERT. In addition to the original startend prediction layers trained from true answers in the base reader, we include a separate start-end prediction layer, with separate parameters, trained specifically on plausible and true answers available in SQuAD 2.0. The context output C from BERT is projected into two hidden state layers S and E, where C, S and E ∈ RL×h , L is the context length and h is the hidden size. The S and E layers are then projected down to a hidden Pretraining embeddings on large unlabelled corpus has been shown to improve many downstream tasks (Peters et al., 2018; Howard and Ruder, 2018; Alec et al., 2018). The recently released 749 dimension of 1, and trained with Cross-Entropy Loss against the plausible and true answer starts and ends. The hidden states S and E of this layer are concatenated with the last context layer output C and projected back to the original dimension to obtain the augmented context vector X, which is fused with start-end span information. S = tanh(CW1 + b1 ) (1) E = tanh(CW2 + b2 ) (2) X = [C; S; E]W (3) Figure 3: Illustration of a Relation Network. The gθ is a MLP to score relationships between pairs term also helps prevent th"
K19-1070,P18-2124,0,0.0457322,"Missing"
K19-1070,D18-1348,0,0.0293522,"herefore our relation module takes all pairs of context objects to score, and use the question objects to guide the scoring function. We use 2 question heads q0 , q1 , so our scoring function is: where W3 ∈ Rh×h , and W4 ∈ Rn×h ; σ is an activation function, such as tanh; n is the number of heads, and h is the hidden dimension. The output O ∈ Rn×h contains the n objects with hidden dimension h that are passed to the next layer. ri = 3.2.1 Object Extraction Regularization In order to help encourage the multiple heads to extract different meaningful semantics in the text, a regularization loss (Xia et al., 2018) is introduced to encourage each head to attend to slightly different sections of the context. Overlapping objects centered on the answer span are expected, due to information fused from S and E, but we do not want the entire weight distribution of the head to be solely focused on the answer span. As we show in later figures, many heads heavily weight the answer span, but also weight information relevant to the answer span needed to make a better non-answerable prediction. Our regularization n X ωi,j ∗ gθ (oi , oj , q0 , q1 ) (7) γi ∗ fφ (ri ) (8) j=0 z= n X i=0 where the outputs ri is the wei"
K19-1070,D18-1259,0,0.016881,"ake better answerable or unanswerable decision. For example, “Northridge earthquake” is mistakenly taken as the answer to the question about what earthquake caused $20 million in damage. Because “$20 billon” is positioned far away from “Northridge earthquake”, it is hard for a model to link these two concepts together and recognize the mismatch of “$20 million” in the question and “$20 billion” in the context. Introduction Ever since the release of many challenging large scale datasets for machine reading comprehension (MRC) (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2016; Yang et al., 2018; Reddy et al., 2018; Jia and Liang, 2017), there have been correspondingly many models for these datasets (Yu et al., 2018; Seo et al., 2017; Liu et al., 2018b; Hu et al., 2017; Xiong et al., 2017; Wang et al., 2018; Liu et al., 2018c; Tay et al., 2018). Knowing what you don’t know (Rajpurkar et al., 2018) is important in real applications of reading comprehension. Unanswerable questions are commonplace in the real world, and SQuAD 2.0 was released specifically to target this problem (see Figure 1 for an example of non-answerable questions). Motivated by exploiting high level semantic relatio"
N13-1034,D08-1064,0,0.196217,"scale up discriminative training of (He and Deng, 2012) to train features with 150 million parameters, which is one order of magnitude higher than previously published effort, and to apply discriminative training to redistribute probability mass that is lost due to model pruning. The experimental results confirm the effectiveness of our proposals on NIST MT06 set over a strong baseline. 1 Introduction State-of-the-art statistical machine translation systems based on a log-linear framework are parameterized by {λ, Φ}, where the feature weights λ are discriminatively trained (Och and Ney, 2002; Chiang et al., 2008b; Simianer et al., 2012) by directly optimizing them against a translation-oriented metric such as BLEU. The feature parameters Φ can be roughly divided into two categories: dense feature that measures the plausibility of each translation rule from a particular aspect, e.g., the rule translation probabilities p(f |e) and p(e|f ); and sparse feature that fires when certain phenomena is observed, e.g., when a frequent word pair co-occured in a rule. In contrast to λ, feature parameters in Φ are usually modeled by generative models for dense features, or by indicator functions for sparse ones. I"
N13-1034,D08-1024,0,0.14835,"Missing"
N13-1034,P11-2080,0,0.0128163,"l phrase-based system as described in (Zhou et al., 2008), trained on six million parallel sentences corpora that are available to the DARPA BOLT Chinese-English task. The training corpora includes a mixed genre of news wire, broadcast news, web-blog and comes from various sources such as LDC, HK Hansard and UN data. In total, there are 50 dense features in our translation system. In addition to the standard features which include the rule translation probabilities, we incorporate features that are found useful for developing a state-of-the-art baseline, e.g. provenancebased lexical features (Chiang et al., 2011). We use a large 6-gram language model, which we train on a 10 billion words monolingual corpus, including the English side of our parallel corpora plus other corpora such as Gigaword (LDC2011T07) and Google News. To prevent possible over-fitting, we only kept the rules that have at most three terminal words (plus up to two nonterminals) on the source side, resulting in a grammar with 167 million rules. Our discriminative training procedure includes updating both λ and θ, and we follow (He and Deng, 2012) to optimize them in an alternate manner. That is, when we optimize θ via EBW, we keep λ f"
N13-1034,N07-2006,0,0.0534373,"Missing"
N13-1034,P12-1031,0,0.11072,"dense feature that measures the plausibility of each translation rule from a particular aspect, e.g., the rule translation probabilities p(f |e) and p(e|f ); and sparse feature that fires when certain phenomena is observed, e.g., when a frequent word pair co-occured in a rule. In contrast to λ, feature parameters in Φ are usually modeled by generative models for dense features, or by indicator functions for sparse ones. It is therefore desirable to train the dense features for each rule in a discriminative fashion to maximize some translation criterion. The maximum expected BLEU training of (He and Deng, 2012) is a recent effort towards this direction, and in this paper, we extend their work to a scaled-up task of discriminative training of the features of a strong hierarchical phrase-based model and confirm its effectiveness empirically. In this work, we further consider the application of discriminative training to pruned model. Various pruning techniques (Johnson et al., 2007; Zens et al., 2012; Eck et al., 2007; Lee et al., 2012; Tomeh et al., 2011) have been proposed recently to filter translation rules. One common consequence of pruning is that the probability distribution P of many surviving"
N13-1034,D11-1125,0,0.0641803,"a 10 billion words monolingual corpus, including the English side of our parallel corpora plus other corpora such as Gigaword (LDC2011T07) and Google News. To prevent possible over-fitting, we only kept the rules that have at most three terminal words (plus up to two nonterminals) on the source side, resulting in a grammar with 167 million rules. Our discriminative training procedure includes updating both λ and θ, and we follow (He and Deng, 2012) to optimize them in an alternate manner. That is, when we optimize θ via EBW, we keep λ fixed and when we optimize λ, we keep λ fixed. We use PRO (Hopkins and May, 2011) to tune λ. For discriminative training of θ, we use a subset of 550 thousands of parallel sentences selected from the entire training data, mainly to allow for faster experimental cycle; they mainly come from news and web-blog domains. For each sentence of this subset, we generate 500-best of unique hypotheses using the baseline model. The 1-best and the oracle BLEU scores for this subset are 40.19 and 47.06 respec337 tively. Following (He and Deng, 2012), we focus on discriminative training of p(f |e) and p(e|f ), which in practice affects around 150 million of parameters; hence the title. F"
N13-1034,D07-1103,0,0.204508,"atures, or by indicator functions for sparse ones. It is therefore desirable to train the dense features for each rule in a discriminative fashion to maximize some translation criterion. The maximum expected BLEU training of (He and Deng, 2012) is a recent effort towards this direction, and in this paper, we extend their work to a scaled-up task of discriminative training of the features of a strong hierarchical phrase-based model and confirm its effectiveness empirically. In this work, we further consider the application of discriminative training to pruned model. Various pruning techniques (Johnson et al., 2007; Zens et al., 2012; Eck et al., 2007; Lee et al., 2012; Tomeh et al., 2011) have been proposed recently to filter translation rules. One common consequence of pruning is that the probability distribution P of many surviving rules become deficient, i.e. f p(f |e) &lt; 1. In practice, others have chosen either to leave the pruned rules as it-is, or simply to re-normalize the probability mass by distributing the pruned mass to surviving rules proportionally. We argue that both approaches are suboptimal, and propose a more principled method to re-distribute the probability mass, i.e. using discrimin"
N13-1034,P07-2045,0,0.00514102,"P simply renormalize ¯ the pruned mass, i.e. θij = θij / j θij . We argue that applying the DT techniques to a pruned grammar, as described in Sec. 2, provides a more principled method to redistribute the mass, i.e. by quantizing how each rule contributes to the expected BLEU score in comparison to other competing rules. To empirically verify this, we consider the significance test based pruning (Johnson et al., 2007), though our general idea can be appllied to any pruning techniques. For our experiments, we use the significance pruning tool that is available as part of Moses decoder package (Koehn et al., 2007). 4 Experiments Our experiments are designed to serve two goals: 1) to show the performance of discriminative training of feature parameters θ in a large-scale task; and 2) to show the effectiveness of DT when applied to pruned grammar. Our baseline system is a state-of-the-art hierarchical phrase-based system as described in (Zhou et al., 2008), trained on six million parallel sentences corpora that are available to the DARPA BOLT Chinese-English task. The training corpora includes a mixed genre of news wire, broadcast news, web-blog and comes from various sources such as LDC, HK Hansard and"
N13-1034,P12-2057,0,0.0534039,"Missing"
N13-1034,P02-1038,0,0.127631,"Missing"
N13-1034,P12-1002,0,0.271064,"Missing"
N13-1034,2011.iwslt-papers.10,0,0.044744,"Missing"
N13-1034,D12-1089,0,0.0354149,"Missing"
N13-1034,W08-0403,1,0.818516,"s, we consider the significance test based pruning (Johnson et al., 2007), though our general idea can be appllied to any pruning techniques. For our experiments, we use the significance pruning tool that is available as part of Moses decoder package (Koehn et al., 2007). 4 Experiments Our experiments are designed to serve two goals: 1) to show the performance of discriminative training of feature parameters θ in a large-scale task; and 2) to show the effectiveness of DT when applied to pruned grammar. Our baseline system is a state-of-the-art hierarchical phrase-based system as described in (Zhou et al., 2008), trained on six million parallel sentences corpora that are available to the DARPA BOLT Chinese-English task. The training corpora includes a mixed genre of news wire, broadcast news, web-blog and comes from various sources such as LDC, HK Hansard and UN data. In total, there are 50 dense features in our translation system. In addition to the standard features which include the rule translation probabilities, we incorporate features that are found useful for developing a state-of-the-art baseline, e.g. provenancebased lexical features (Chiang et al., 2011). We use a large 6-gram language mode"
N16-1063,N15-1011,0,0.011196,"Missing"
N16-1063,D14-1181,0,0.00999026,"e a novel NN initialization method that treats some of the neurons in the final hidden layer as dedicated neurons for each pattern of label co-occurrence. These dedicated neurons are initialized to connect to the corresponding cooccurring labels with stronger weights than to others. While initialization of an NN is an important research topic (Glorot and Bengio, 2010; Sutskever et al., 2013; Le et al., 2015), to the best of our knowledge, there has been no attempt to leverage label cooccurrence for NN initialization. 2 Related Work Along with the recent success in NNs (Collobert et al., 2011; Kim, 2014), NN-based multi-label classification has been proposed. An NN for NLQ classification needs to accept queries with variable length and output their labels. Figure 1 shows a typical NN architecture (Collobert et al., 2011). This NN first transforms words in the input query into word embeddings (Mikolov et al., 2013), then applies Convolutional Neural Network (CNN) and Max-pooling over time to extract fixed-length feature vectors, and feed them into the output layer to predict the label for the query (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014). To take care of multi-la"
N16-1063,P14-2105,0,0.0197249,"in NNs (Collobert et al., 2011; Kim, 2014), NN-based multi-label classification has been proposed. An NN for NLQ classification needs to accept queries with variable length and output their labels. Figure 1 shows a typical NN architecture (Collobert et al., 2011). This NN first transforms words in the input query into word embeddings (Mikolov et al., 2013), then applies Convolutional Neural Network (CNN) and Max-pooling over time to extract fixed-length feature vectors, and feed them into the output layer to predict the label for the query (Collobert and Weston, 2008; Collobert et al., 2011; Yih et al., 2014). To take care of multi-labels, label co-occurrence has been incorporated into loss functions such as pairwise ranking loss (Zhang and Zhou, 2006). More recently, Nam et 521 Proceedings of NAACL-HLT 2016, pages 521–526, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics al. (2014) reported that binary cross entropy can outperform the pairwise ranking loss by leveraging rectified linear units (ReLUs) for nonlinearity (Nair and Hinton, 2010), AdaGrad for optimization (Duchi et al., 2011), and dropout for generalization (Srivastava et al., 2014). Considering"
N18-1109,P07-1056,0,0.373365,"raining for future work. 4 Tasks and Data Sets We test our methods by conducting experiments on two text classification data sets. We used NLTK toolkit3 for tokenization. The task are divided into meta-training tasks and meta-testing tasks (target tasks), where the meta-training tasks are used for clustering and cluster-encoder training. The metatesting tasks are few-shot tasks, which are used for evaluating the method in Eq. (6). 4.1 Amazon Review Sentiment Classification First, following Barzilai and Crammer (2015), we construct multiple tasks with the multi-domain sentiment classification (Blitzer et al., 2007) data set. The dataset consists of Amazon product reviews for 23 types of products (see Appendix D for the details). For each product domain, we construct three binary classification tasks with different thresholds on the ratings: the tasks consider a review as positive if it belongs to one of the following buckets = 5 stars, &gt;= 4 stars or &gt;= 2 stars.4 These buckets then form the basis of the task-setup, giving us 23 ⇥ 3=69 tasks in total. For each domain we distribute the reviews uniformly 3 http://www.nltk.org/ Data downloaded from http://www.cs.jhu.edu/ ˜mdredze/datasets/sentiment/, in whic"
N18-1109,D14-1181,0,0.00578282,"test instance x ˆ and the support set S: y = P (.|ˆ x, S) = |S| X ↵(ˆ x, xi ; ✓)yi , M different from Eq. (1): (1) y = P (.|ˆ x, S) = i=1 B,S⇠D ↵(ˆ x, Si ; ✓)yi . (3) i=1 where we defined ↵(., .) to be a softmax distribution given ⇤(ˆ x, xi ), where xi is a supporting instance, i.e., ↵(ˆ x, xi ; ✓) = T exp(f (ˆ x)T f (xi ))/P|S |exp(f (ˆ x) f (xj )), where ✓ j=1 are the parameters of the encoder f . Thus, y is a valid distribution over the supporting set’s labels |S| {yi }i=1 . To adapt the MNet to text classification, we choose encoder f to be a convolutional neural network (CNN) following (Kim, 2014; Johnson and Zhang, 2016). Figure 1 shows the MNet with the CNN architecture. Following (Collobert et al., 2011; Kim, 2014), the model consists of a convolution layer and a max-pooling operation over the entire sentence. To train the MNets, we first sample the training dataset D for task T from all tasks T , with notation simplified as D ⇠ T . For each class in the sampled dataset D, we sample k random instances in that class to construct a support set S, and sample a batch of training instances B as training examples, i.e., B, S ⇠ D. The training objective is to minimize the prediction error"
N18-1109,D14-1162,0,0.0810149,": a metric-learning based few-shot learning model trained on all training tasks; (5) Prototypical Network: a variation of matching network with different prediction function as Eq. 3; (6) Convex combining all single-task models: training one CNN classifier on each meta-training task individually and taking the encoder, then for each target task training a linear combination of all the above singletask encoders with Eq. (6). This baseline can be viewed as a variation of our method without task clustering. We initialize all models with pretrained 100-dim Glove embeddings (trained on 6B corpus) (Pennington et al., 2014). Hyper-Parameter Tuning In all experiments, we set both p1 and p2 parameters in (4) to 0.5. This strikes a balance between obtaining enough observed entries in Y, and ensuring that most of the retained similarities are consistent with the cluster membership. The window/hidden-layer sizes of CNN and the initialization of embeddings (random or pre-trained) are tuned during the clusterencoder training phase, with the validation sets of meta-training tasks. We have the CNN with window size of 5 and 200 hidden units. The singlemetric FSL baselines have 400 hidden units in the CNN encoders. On sent"
P09-2058,J93-2003,0,0.0205404,"Missing"
P09-2058,P96-1041,0,0.0277227,". Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh (Koehn et al., 2003). Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, lexicalized reordering models, sentence length penalty and other heuristics. These feature weights are tuned on the dev set to achieve optimal translation performance evaluated by automatic metric. The language model is a statistical 4-gram model estimated with Modified Kneser-Ney smoothing (Chen and Goodman, 1996) using only English sentences in the parallel training data. 3.1 Phrase Table Comparison We first study the impact of different word alignment combination methods on phrase translation table, and compare our approaches to heuristic based methods. The same English to Farsi and Farsi to English Model-4 word alignments are used, but we try different combination methods and analysis the final alignment set and the resulting phase translation table. Table 1 presents some statistics. Each row corresponds to a particular combination. The first two are intersection (I) and union (U). The next two meth"
P09-2058,N03-1017,0,0.620136,"nter Yorktown Heights, NY 10598, USA {ydeng,zhou}@us.ibm.com Abstract performance. Symmetrization can also be realized during alignment model training (Liang et al., 2006; Zens et al., 2004). Given two sets of word alignments trained in two translation directions, two extreme combination are intersection and union. While intersection achieves high precision with low recall, union is the opposite. A right balance of these two extreme cases would offer a good coverage with reasonable accuracy. So starting from intersection, gradually adding elements in the union by heuristics is typically used. Koehn et al. (2003) grow the set of word links by appending neighboring points, while Och and Hey (2003) try to avoid both horizontal and vertical neighbors. These heuristicbased combination methods are not driven explicitly by the intended application of the resulting output. Ayan (2005) exploits many advanced machine learning techniques for general word alignment combination problem. However, human annotation is required for supervised training in those techniques. We propose a new combination method. Like heuristics, we aim to find a balance between intersection and union. But unlike heuristics, combination i"
P09-2058,N06-1014,0,0.356028,"Missing"
P09-2058,J03-1002,0,0.0569872,"thod is indeed moving to the desired direction of extracting as many accurate (all boundary words should be aligned) phrase pairs as possible. We still notice that ratio of |P P2 (A) |and |P P0 (A) |(the last column) is high. We suspect that the ratio of this two phrase table size might somewhat be indicative of the phrase table accuracy, which is hard to estimate without manual annotation though. data was provided by the DARPA TransTac program. It consists of around 110K sentence pairs with 850K English words in the military force protection domain. We train IBM Model-4 using GIZA++ toolkit (Och and Ney, 2003) in two translation directions and perform different word alignment combination. The resulting alignment set is used to train a phrase translation table, where Farsi phrases are limited to up to 6 words. The quality of resulting phrase translation table is measured by translation results. Our decoder is a phrase-based multi-stack implementation of the log-linear model similar to Pharaoh (Koehn et al., 2003). Like other log-linear model based decoders, active features in our translation engine include translation models in two directions, lexicon weights in two directions, language model, lexic"
P09-2058,P03-1021,0,0.145495,"Missing"
P09-2058,P02-1040,0,0.0746516,"Missing"
P09-2058,C04-1006,0,0.0416337,"Missing"
P10-2005,P05-1066,0,0.092531,"Missing"
P10-2005,P09-2058,1,0.838923,"nt combination in the past has focused on how to combine the alignments from two different directions, sourceto-target and target-to-source. Usually people start from the intersection of two sets of alignments, and gradually add links in the union based on certain heuristics, as in (Koehn et al., 2003), to achieve a better balance compared to using either intersection (high precision) or union (high recall). In (Ayan and Dorr, 2006) a maximum entropy approach was proposed to combine multiple alignments based on a set of linguistic and alignment features. A different approach was presented in (Deng and Zhou, 2009), which again concentrated on the combination of two sets of alignments, but with a different criterion. It tries to maximize the number of phrases that can be extracted in the combined alignments. A greedy search method was utilized and it achieved higher translation performance than the baseline. More recently, an alignment selection approach was proposed in (Huang, 2009), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold. The alignments used in that work were generated from different aligners (HMM, block model,"
P10-2005,P04-3014,0,0.0534274,"Missing"
P10-2005,D07-1006,0,0.0160649,"guistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better translation performance. 1 Introduction Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system. It has received a significant amount of research over the years, notably in (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007; Hermjakob, 2009). They all focused on the improvement of word alignment models. In this work, we leverage existing aligners and generate multiple sets of word alignments based on complementary information, then combine them to get the final alignment for phrase training. The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example. This makes the approach especially appealing for SMT on low-resource languages. 22 Proceedings of the ACL 2010 Conference Short Papers, pages 22–26, c Uppsala, Sweden, 11-16 July 20"
P10-2005,D09-1024,0,0.0183355,"ology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better translation performance. 1 Introduction Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system. It has received a significant amount of research over the years, notably in (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007; Hermjakob, 2009). They all focused on the improvement of word alignment models. In this work, we leverage existing aligners and generate multiple sets of word alignments based on complementary information, then combine them to get the final alignment for phrase training. The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example. This makes the approach especially appealing for SMT on low-resource languages. 22 Proceedings of the ACL 2010 Conference Short Papers, pages 22–26, c Uppsala, Sweden, 11-16 July 2010. 2010 Associati"
P10-2005,P09-1105,0,0.250249,"on) or union (high recall). In (Ayan and Dorr, 2006) a maximum entropy approach was proposed to combine multiple alignments based on a set of linguistic and alignment features. A different approach was presented in (Deng and Zhou, 2009), which again concentrated on the combination of two sets of alignments, but with a different criterion. It tries to maximize the number of phrases that can be extracted in the combined alignments. A greedy search method was utilized and it achieved higher translation performance than the baseline. More recently, an alignment selection approach was proposed in (Huang, 2009), which computes confidence scores for each link and prunes the links from multiple sets of alignments using a hand-picked threshold. The alignments used in that work were generated from different aligners (HMM, block model, and maximum entropy model). In this work, we use soft voting with weighted confidence scores, where the weights can be tuned with a specific objective function. There is no need for a pre-determined threshold as used in (Huang, 2009). Also, we utilize various knowledge sources to enrich the alignments instead of using different aligners. Our strategy is to diversify and th"
P10-2005,H05-1012,0,0.020907,"erent motivations, such as linguistic knowledge, morphology and heuristics. We demonstrate this approach on an English-to-Pashto translation task by combining the alignments obtained from syntactic reordering, stemming, and partial words. The combined alignment outperforms the baseline alignment, with significantly higher F-scores and better translation performance. 1 Introduction Word alignment usually serves as the starting point and foundation for a statistical machine translation (SMT) system. It has received a significant amount of research over the years, notably in (Brown et al., 1993; Ittycheriah and Roukos, 2005; Fraser and Marcu, 2007; Hermjakob, 2009). They all focused on the improvement of word alignment models. In this work, we leverage existing aligners and generate multiple sets of word alignments based on complementary information, then combine them to get the final alignment for phrase training. The resource required for this approach is little, compared to what is needed to build a reasonable discriminative alignment model, for example. This makes the approach especially appealing for SMT on low-resource languages. 22 Proceedings of the ACL 2010 Conference Short Papers, pages 22–26, c Uppsal"
P10-2005,N03-1017,0,0.116303,"Missing"
P10-2005,P00-1056,0,0.212809,"ght neighboring word is aligned to tk so far; • tk is not aligned and its left or right neighboring word is aligned to sj so far. 3. Repeat scanning all candidate links until no more links can be added. In this way, those alignment links with higher confidence scores have higher priority to be included in the combined alignment. 4 Experiments 4.1 Baseline Our training data contains around 70K EnglishPashto sentence pairs released under the DARPA TRANSTAC project, with about 900K words on the English side. The baseline is a phrase-based MT system similar to (Koehn et al., 2003). We use GIZA++ (Och and Ney, 2000) to generate the baseline alignment for each direction and then 24 gdf. We also notice that its higher F-score is mainly due to the higher precision, which should result from the consideration of confidence scores. Alignment Baseline V S P X B+V B+V B+V B+S B+P B+X B+V+P B+V+S+P B+V+S+P Comb c0 c1 c2 c2 c2 c2 c2 gdf c2 P 0.6923 0.6934 0.7376 0.7665 0.7615 0.7639 0.7645 0.7895 0.7942 0.8006 0.7827 0.7912 0.7238 0.7906 R 0.6414 0.6388 0.6495 0.6643 0.6641 0.6312 0.6373 0.6505 0.6553 0.6612 0.6670 0.6755 0.7042 0.6852 F 0.6659 0.6650 0.6907 0.7118 0.7095 0.6913 0.6951 0.7133 0.7181 0.7242 0.7202"
P10-2005,P03-1021,0,0.0199596,"nce of aijk as Stemming Pashto is one of the morphologically rich languages. In addition to the linguistic knowledge applied in the syntactic reordering described above, we also utilize morphological analysis by applying stemming on both the English and Pashto sides. For English, we use Porter stemming (Porter, c(aijk |S, T ) = 23 q qs2t (aijk |S, T )qt2s (aijk |T, S), (1) where the source-to-target link posterior probability pi (tk |sj ) qs2t (aijk |S, T ) = PK , k ′ =1 pi (tk ′ |sj ) apply grow-diagonal-final (gdf). The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni et al., 2002). There are 2028 sentences in the tuning set and 1019 sentences in the test set, both with one reference. We use another 150 sentence pairs as a heldout hand-aligned set to measure the word alignment quality. The three sets of alignments described in Section 2 are generated on the same training data separately with GIZA++ and enhanced by gdf as for the baseline alignment. The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997). (2) and the target-to-source link posterior probabil"
P10-2005,P02-1040,0,0.078807,"e of the morphologically rich languages. In addition to the linguistic knowledge applied in the syntactic reordering described above, we also utilize morphological analysis by applying stemming on both the English and Pashto sides. For English, we use Porter stemming (Porter, c(aijk |S, T ) = 23 q qs2t (aijk |S, T )qt2s (aijk |T, S), (1) where the source-to-target link posterior probability pi (tk |sj ) qs2t (aijk |S, T ) = PK , k ′ =1 pi (tk ′ |sj ) apply grow-diagonal-final (gdf). The decoding weights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni et al., 2002). There are 2028 sentences in the tuning set and 1019 sentences in the test set, both with one reference. We use another 150 sentence pairs as a heldout hand-aligned set to measure the word alignment quality. The three sets of alignments described in Section 2 are generated on the same training data separately with GIZA++ and enhanced by gdf as for the baseline alignment. The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997). (2) and the target-to-source link posterior probability qt2s (aijk |T, S) is defined similarly. pi ("
P10-2005,W97-0301,0,0.009103,"eights are optimized with minimum error rate training (MERT) (Och, 2003) to maximize BLEU scores (Papineni et al., 2002). There are 2028 sentences in the tuning set and 1019 sentences in the test set, both with one reference. We use another 150 sentence pairs as a heldout hand-aligned set to measure the word alignment quality. The three sets of alignments described in Section 2 are generated on the same training data separately with GIZA++ and enhanced by gdf as for the baseline alignment. The English parse tree used for the syntactic reordering was produced by a maximum entropy based parser (Ratnaparkhi, 1997). (2) and the target-to-source link posterior probability qt2s (aijk |T, S) is defined similarly. pi (tk |sj ) is the lexical translation probability between source word sj and target word tk in the i-th set of alignments. Our alignment combination algorithm is as follows. 1. Each candidate link ajk gets soft votes from N sets of alignments via weighted confidence scores: v(ajk |S, T ) = N X wi ∗ c(aijk |S, T ), 4.2 Improvement in Word Alignment In Table 1 we show the precision, recall and Fscore of each set of word alignments for the 150sentence set. Using partial word provides the highest F-"
P10-2005,D07-1077,0,0.0237779,"nglish-to-Pashto MT task as an example and create three sets of additional alignments on top of the baseline alignment. 2.1 P: they hQvy E’: they are stAsO your VP NP VBP NNS your employees and you kArvAl employees dy Av are and know NP ADVP PRP RB them tAsO hQvy smh you them well well pOZnB know Figure 1: Alignment before/after VP-based reordering. Syntactic Reordering Pashto is a subject-object-verb (SOV) language, which puts verbs after objects. People have proposed different syntactic rules to pre-reorder SOV languages, either based on a constituent parse tree (Dr´abek and Yarowsky, 2004; Wang et al., 2007) or dependency parse tree (Xu et al., 2009). In this work, we apply syntactic reordering for verb phrases (VP) based on the English constituent parse. The VP-based reordering rule we apply in the work is: 1980), a widely applied algorithm to remove the common morphological and inflexional endings from words in English. For Pashto, we utilize a morphological decompostion algorithm that has been shown to be effective for Arabic speech recognition (Xiang et al., 2006). We start from a fixed set of affixes with 8 prefixes and 21 suffixes. The prefixes and suffixes are stripped off from the Pashto"
P10-2005,N09-1028,0,0.0265802,"eate three sets of additional alignments on top of the baseline alignment. 2.1 P: they hQvy E’: they are stAsO your VP NP VBP NNS your employees and you kArvAl employees dy Av are and know NP ADVP PRP RB them tAsO hQvy smh you them well well pOZnB know Figure 1: Alignment before/after VP-based reordering. Syntactic Reordering Pashto is a subject-object-verb (SOV) language, which puts verbs after objects. People have proposed different syntactic rules to pre-reorder SOV languages, either based on a constituent parse tree (Dr´abek and Yarowsky, 2004; Wang et al., 2007) or dependency parse tree (Xu et al., 2009). In this work, we apply syntactic reordering for verb phrases (VP) based on the English constituent parse. The VP-based reordering rule we apply in the work is: 1980), a widely applied algorithm to remove the common morphological and inflexional endings from words in English. For Pashto, we utilize a morphological decompostion algorithm that has been shown to be effective for Arabic speech recognition (Xiang et al., 2006). We start from a fixed set of affixes with 8 prefixes and 21 suffixes. The prefixes and suffixes are stripped off from the Pashto words under the two constraints:(1) Longest"
P10-2005,J93-2003,0,\N,Missing
P10-2005,N06-1013,0,\N,Missing
P13-1081,P03-1055,0,0.106741,"ased system trained on only 60K sentences, while we conduct experiments on more advanced Hiero and tree-to-string systems, trained on 2M sentences in a much larger corpus. We directly take advantage of the augmented parse trees in the tree-to-string grammar, which could have larger impact on the MT system performance. Table 11: BLEU scores in the tree-to-string system with Hiero rules as backoff. 5 Related Work Empty categories have been studied in recent years for several languages, mostly in the context of reference resolution and syntactic processing for English, such as in (Johnson, 2002; Dienes and Dubey, 2003; Gabbard et al., 2006). More recently, EC recovery for Chinese started emerging in literature. In (Guo et al., 2007), non-local dependencies are migrated from English to Chinese for generating proper predicateargument-modifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang"
P13-1081,N06-1024,0,0.0252752,"nly 60K sentences, while we conduct experiments on more advanced Hiero and tree-to-string systems, trained on 2M sentences in a much larger corpus. We directly take advantage of the augmented parse trees in the tree-to-string grammar, which could have larger impact on the MT system performance. Table 11: BLEU scores in the tree-to-string system with Hiero rules as backoff. 5 Related Work Empty categories have been studied in recent years for several languages, mostly in the context of reference resolution and syntactic processing for English, such as in (Johnson, 2002; Dienes and Dubey, 2003; Gabbard et al., 2006). More recently, EC recovery for Chinese started emerging in literature. In (Guo et al., 2007), non-local dependencies are migrated from English to Chinese for generating proper predicateargument-modifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang and Xue, 2010), where"
P13-1081,D07-1027,0,0.0294301,"Missing"
P13-1081,D11-1125,0,0.0346264,"Missing"
P13-1081,P02-1018,0,0.0593778,"y use a phase-based system trained on only 60K sentences, while we conduct experiments on more advanced Hiero and tree-to-string systems, trained on 2M sentences in a much larger corpus. We directly take advantage of the augmented parse trees in the tree-to-string grammar, which could have larger impact on the MT system performance. Table 11: BLEU scores in the tree-to-string system with Hiero rules as backoff. 5 Related Work Empty categories have been studied in recent years for several languages, mostly in the context of reference resolution and syntactic processing for English, such as in (Johnson, 2002; Dienes and Dubey, 2003; Gabbard et al., 2006). More recently, EC recovery for Chinese started emerging in literature. In (Guo et al., 2007), non-local dependencies are migrated from English to Chinese for generating proper predicateargument-modifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a"
P13-1081,N03-1017,0,0.0472565,"Missing"
P13-1081,P11-2037,0,0.434892,"odifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang and Xue, 2010), where each word in the sentence receives a tag indicating whether there is an EC before it. A maximum entropy model is utilized to predict the tags, but different types of ECs are not distinguished. In (Cai et al., 2011), a language-independent method was proposed to integrate the recovery of empty elements into syntactic parsing. As shown in the previous section, our model outperforms the model in (Yang and Xue, 2010) and (Cai et al., 2011) significantly using the same training and test data. (Luo and Zhao, 2011) also tries to predict the existence of an EC 6 Conclusions and Future Work In this paper, we presented a novel structured approach to EC prediction, which utilizes a maximum entropy model with various syntactic features and shows significantly higher accuracy than the state-of-the-art approaches. We"
P13-1081,P05-1033,0,0.300708,"nted Data With the pre-processed MT training corpus, an unsupervised word aligner, such as GIZA++, can be used to generate automatic word alignment, as the first step of a system training pipeline. The effect of inserting ECs is two-fold: first, it can impact the automatic word alignment since now it allows the target-side words, especially the function words, to align to the inserted ECs and fix some errors in the original word alignment; second, new phrases and rules can be extracted from the preprocessed training data. For example, for a hierarchical MT system, some phrase pairs and Hiero (Chiang, 2005) rules can be extracted with recovered *pro* and *PRO* at the Chinese side. In this work we also take advantages of the augmented Chinese parse trees (with ECs projected to the surface) and extract tree-to-string grammar (Liu et al., 2006) for a tree-to-string MT system. Due to the recovered ECs in the source parse trees, the tree-to-string grammar extracted from such trees can be more discriminative, with an increased capability of distinguishing different context. An example of an augmented Chinese parse tree aligned to an English string is shown in Figure 3, in which the incorrect alignment"
P13-1081,D10-1062,0,0.452167,"acted Hiero rules and tree-to-string rules are also listed, which we would not have been able to extract from the original incorrect word alignment when the *pro* was missing. Table 2: List of features. 3 Integrating Empty Categories in Machine Translation In this section, we explore multiple approaches of utilizing recovered ECs in machine translation. 3.1 Explicit Recovery of ECs in MT We conducted some initial error analysis on our MT system output and found that most of the errors that are related to ECs are due to the missing *pro* and *PRO*. This is also consistent with the findings in (Chung and Gildea, 2010). One of the other frequent ECs, *OP*, appears in the Chinese relative clauses, which usually have a Chinese word “De” aligned to the target side “that” or “which”. And the trace, *T*, exists in both Chinese and English sides. For MT we want to focus on the places where there exist mismatches between the source and target languages. A straightforward way of utilizing the recovered *pro* and *PRO* is to pre-process the MT training and test data by inserting ECs into the original source text (i.e. Chinese in this case). As mentioned in the previous section, the output of our EC predictor is a ne"
P13-1081,P05-1066,0,0.0744819,"Missing"
P13-1081,P06-1077,0,0.013018,"ld: first, it can impact the automatic word alignment since now it allows the target-side words, especially the function words, to align to the inserted ECs and fix some errors in the original word alignment; second, new phrases and rules can be extracted from the preprocessed training data. For example, for a hierarchical MT system, some phrase pairs and Hiero (Chiang, 2005) rules can be extracted with recovered *pro* and *PRO* at the Chinese side. In this work we also take advantages of the augmented Chinese parse trees (with ECs projected to the surface) and extract tree-to-string grammar (Liu et al., 2006) for a tree-to-string MT system. Due to the recovered ECs in the source parse trees, the tree-to-string grammar extracted from such trees can be more discriminative, with an increased capability of distinguishing different context. An example of an augmented Chinese parse tree aligned to an English string is shown in Figure 3, in which the incorrect alignment in Figure 1 is fixed. A few examples of the extracted Hiero rules and tree-to-string rules are also listed, which we would not have been able to extract from the original incorrect word alignment when the *pro* was missing. Table 2: List"
P13-1081,P11-1123,1,0.865531,"ith their intended usages. Readers are referred to the documentation (Xue et al., 2005) of CTB for detailed discussions about the characterization of empty categories. EC *T* * *PRO* *pro* *OP* *RNR* Meaning trace of A’-movement trace of A-movement big PRO in control structures pro-drop operator in relative clauses for right node raising Table 1: List of empty categories in the CTB. In this section, we tackle the problem of recovering Chinese ECs. The problem has been studied before in the literature. For instance, Yang and Xue (2010) attempted to predict the existence of an EC before a word; Luo and Zhao (2011) predicted ECs on parse trees, but the position information of some ECs is partially lost in their representation. Furthermore, Luo and Zhao (2011) conducted experiments on gold parse trees only. In 823 Figure 2: Example of tree transformation on training data to encode an empty category and its position information. tactic tree, as opposed to simply attaching it to a neighboring word, as was done in (Yang and Xue, 2010). We believe this is one of the reasons why our model has better accuracy than that of (Yang and Xue, 2010) (cf. Table 7). In summary, a projected tag consists of an EC type (s"
P13-1081,J93-2004,0,0.0430812,"the predicted ECs into a Chinese-to-English machine translation task through multiple approaches, including the extraction of EC-specific sparse features. We show that the recovered empty categories not only improve the word alignment quality, but also lead to significant improvements in a large-scale state-of-the-art syntactic MT system. Figure 1: Example of incorrect word alignment due to missing pronouns on the Chinese side. In order to account for certain language phenomena such as pro-drop and wh-movement, a set of special tokens, called empty categories (EC), are used in Penn Treebanks (Marcus et al., 1993; Bies and Maamouri, 2003; Xue et al., 2005). Since empty categories do not exist in the surface form of a language, they are often deemed elusive and recovering ECs is even figuratively called “chasing the ghost” (Yang and Xue, 2010). In this work we demonstrate that, with the availability of large-scale EC annotations, it is feasible to predict and recover ECs with high accuracy. More importantly, with various approaches of modeling the recovered ECs in SMT, we are able to achieve significant improvements1 . The contributions of this paper include the following: 1 Introduction One of the key"
P13-1081,P00-1056,0,0.0410984,"Missing"
P13-1081,P02-1040,0,0.088566,"Missing"
P13-1081,W97-0301,0,0.0615855,"Missing"
P13-1081,C10-2158,0,0.350208,"quality, but also lead to significant improvements in a large-scale state-of-the-art syntactic MT system. Figure 1: Example of incorrect word alignment due to missing pronouns on the Chinese side. In order to account for certain language phenomena such as pro-drop and wh-movement, a set of special tokens, called empty categories (EC), are used in Penn Treebanks (Marcus et al., 1993; Bies and Maamouri, 2003; Xue et al., 2005). Since empty categories do not exist in the surface form of a language, they are often deemed elusive and recovering ECs is even figuratively called “chasing the ghost” (Yang and Xue, 2010). In this work we demonstrate that, with the availability of large-scale EC annotations, it is feasible to predict and recover ECs with high accuracy. More importantly, with various approaches of modeling the recovered ECs in SMT, we are able to achieve significant improvements1 . The contributions of this paper include the following: 1 Introduction One of the key challenges in statistical machine translation (SMT) is to effectively model inherent differences between the source and the target language. Take the Chinese-English SMT as an example: it is non-trivial to produce correct pronouns on"
P13-1081,D08-1060,0,0.0502522,"Missing"
P13-1081,D07-1057,0,0.0851818,"nce. Table 11: BLEU scores in the tree-to-string system with Hiero rules as backoff. 5 Related Work Empty categories have been studied in recent years for several languages, mostly in the context of reference resolution and syntactic processing for English, such as in (Johnson, 2002; Dienes and Dubey, 2003; Gabbard et al., 2006). More recently, EC recovery for Chinese started emerging in literature. In (Guo et al., 2007), non-local dependencies are migrated from English to Chinese for generating proper predicateargument-modifier structures from surface context free phrase structure trees. In (Zhao and Ng, 2007), a decision tree learning algorithm is presented to identify and resolve Chinese anaphoric zero pronouns. and achieves a performance comparable to a heuristic rule-based approach. Similar to the work in (Dienes and Dubey, 2003), empty detection is formulated as a tagging problem in (Yang and Xue, 2010), where each word in the sentence receives a tag indicating whether there is an EC before it. A maximum entropy model is utilized to predict the tags, but different types of ECs are not distinguished. In (Cai et al., 2011), a language-independent method was proposed to integrate the recovery of"
P13-1124,D10-1014,1,\N,Missing
P13-1124,N10-1016,0,\N,Missing
P13-1124,N04-4026,0,\N,Missing
P13-1124,W09-2307,0,\N,Missing
P13-1124,D09-1105,0,\N,Missing
P13-1124,N09-1053,0,\N,Missing
P13-1124,D08-1089,0,\N,Missing
P13-1124,W09-0435,0,\N,Missing
P13-1124,P11-2080,0,\N,Missing
P13-1124,D08-1024,0,\N,Missing
P13-1124,N09-1027,0,\N,Missing
P13-1124,D09-1008,1,\N,Missing
P13-1124,P07-2045,0,\N,Missing
P13-1124,P12-1095,0,\N,Missing
P13-1124,C10-1043,0,\N,Missing
P13-1124,P08-1114,0,\N,Missing
P13-1124,W06-1609,0,\N,Missing
P13-1124,P05-1033,0,\N,Missing
P13-1124,D11-1045,0,\N,Missing
P13-1124,W06-3108,0,\N,Missing
P13-1124,P09-1037,1,\N,Missing
P13-1124,P07-1090,1,\N,Missing
P13-1124,P08-1066,1,\N,Missing
P13-1124,P06-1090,0,\N,Missing
P13-1124,D11-1125,0,\N,Missing
P15-1061,D09-1149,0,0.164217,"tation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals. Introduction Relation classification is an important Natural Language Processing (NLP) task which is normally used as an intermediate step in many complex NLP applications such as question-answering and automatic knowledge base construction. Since the last decade there has been increasing interest in applying machine learning approaches to this task (Zhang, 2004; Qian et al., 2009; Rink and Harabagiu, 2010). One reason is the availability of benchmark datasets such as the SemEval-2010 626 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 626–634, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics The remainder of the paper is structured as follows. Section 2 details the proposed neural network. In Section 3, we present details about the setup of experimental evaluation, and then describe the results in Section 4. In Sect"
P15-1061,S10-1057,0,0.870625,"icial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals. Introduction Relation classification is an important Natural Language Processing (NLP) task which is normally used as an intermediate step in many complex NLP applications such as question-answering and automatic knowledge base construction. Since the last decade there has been increasing interest in applying machine learning approaches to this task (Zhang, 2004; Qian et al., 2009; Rink and Harabagiu, 2010). One reason is the availability of benchmark datasets such as the SemEval-2010 626 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 626–634, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics The remainder of the paper is structured as follows. Section 2 details the proposed neural network. In Section 3, we present details about the setup of experimental evaluation, and then describe the results in Section 4. In Section 5, we discuss previous"
P15-1061,D12-1110,0,0.777357,"ur experimental results show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals. 1 The [introduction]e1 in the [book]e2 is a summary of what is in the text. Some recent work on relation classification has focused on the use of deep neural networks with the aim of reducing the number of handcrafted features (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014). However, in order to achieve state-ofthe-art results these approaches still use some features derived from lexical resources such as WordNet or NLP tools such as dependency parsers and named entity recognizers (NER). In this work, we propose a new convolutional neural network (CNN), which we name Classification by Ranking CNN (CR-CNN), to tackle the relation classification task. The proposed network learns a distributed vector representation for each relation class. Given an input text segment, the network uses a convolutional layer to produce a distribut"
P15-1061,C14-1008,1,0.41496,"ameters to be chosen by the user. It is important to note that dc corresponds to the size of the sentence representation. Sentence Representation 2.4 The next step in the NN consists in creating the distributed vector representation rx for the input sentence x. The main challenges in this step are the sentence size variability and the fact that important information can appear at any position in the sentence. In recent work, convolutional approaches have been used to tackle these issues when creating representations for text segments of different sizes (Zeng et al., 2014; Hu et al., 2014; dos Santos and Gatti, 2014) and characterlevel representations of words of different sizes (dos Santos and Zadrozny, 2014). Here, we use a convolutional layer to compute distributed vector representations of the sentence. The convolutional layer first produces local features around each word in the sentence. Then, it combines these local features using a max operation to create a fixed-sized vector for the input sentence. Given a sentence x, the convolutional layer applies a matrix-vector operation to each window of size k of successive windows in embx = {rw1 , rw2 , ..., rwN }. Let us define the vector zn ∈ w Rd k as t"
P15-1061,N03-1033,0,0.011564,"used in our experiments are initialized by means of unsupervised pre-training. We perform pre-training using the skip-gram NN architecture (Mikolov et al., 2013) available in the word2vec tool. We use the December 2013 snapshot of the English Wikipedia corpus to train word embeddings with word2vec. We preprocess the Wikipedia text using the steps described in (dos Santos and Gatti, 2014): (1) removal of paragraphs that are not in English; (2) substitution of non-western characters for a special character; (3) tokenization of the text using the tokenizer available with the Stanford POS Tagger (Toutanova et al., 2003); (4) removal of sentences that are less than 20 characters long (including white spaces) or have less than 5 tokens. (5) lowercase all words and substitute each numerical digit by a 0. The resulting clean corpus contains about 1.75 billion tokens. 3.3 Neural Network Hyper-parameter We use 4-fold cross-validation to tune the neural network hyperparameters. Learning rates in the range of 0.03 and 0.01 give relatively similar results. Best results are achieved using between 10 and 15 training epochs, depending on the CR-CNN configuration. In Table 1, we show the selected hyperparameter values. A"
P15-1061,D14-1002,0,0.0174498,"core sθ (x)c− decreases. Training CR-CNN by minimizing the loss function in Equation 1 has the effect of training to give scores greater than m+ for the correct class and (negative) scores smaller than −m− for incorrect classes. In our experiments we set γ to 2, m+ to 2.5 and m− to 0.5. We use L2 regularization by adding the term βkθk2 to Equation 1. In our experiments we set β to 0.001. We use stochastic gradient descent (SGD) to minimize the loss function with respect to θ. Like some other ranking approaches that only update two classes/examples at every training round (Weston et al., 2011; Gao et al., 2014), we can efficiently train the network for tasks which have a very large number of classes. This is an advantage over softmax classifiers. On the other hand, sampling informative negative classes/examples can have a significant impact in the effectiveness of the learned model. In the case of our loss function, more informative negative classes are the ones with a score larger than −m− . The number of classes in the relation classification dataset that we use in our experiments is small. Therefore, in our experiments, given a sentence x with class label y + , the incorrect class c− that we choo"
P15-1061,D14-1194,0,0.0160439,"Missing"
P15-1061,S10-1006,0,0.433515,"Missing"
P15-1061,C14-1220,0,0.131552,"ts show that: (1) our approach is more effective than CNN followed by a softmax classifier; (2) omitting the representation of the artificial class Other improves both precision and recall; and (3) using only word embeddings as input features is enough to achieve state-of-the-art results if we consider only the text between the two target nominals. 1 The [introduction]e1 in the [book]e2 is a summary of what is in the text. Some recent work on relation classification has focused on the use of deep neural networks with the aim of reducing the number of handcrafted features (Socher et al., 2012; Zeng et al., 2014; Yu et al., 2014). However, in order to achieve state-ofthe-art results these approaches still use some features derived from lexical resources such as WordNet or NLP tools such as dependency parsers and named entity recognizers (NER). In this work, we propose a new convolutional neural network (CNN), which we name Classification by Ranking CNN (CR-CNN), to tackle the relation classification task. The proposed network learns a distributed vector representation for each relation class. Given an input text segment, the network uses a convolutional layer to produce a distributed vector represent"
P15-1061,P14-1062,0,0.00694751,"reat it as a multiclass classification problem and apply a variety of machine learning techniques to the task in order to achieve a high accuracy. Recently, deep learning (Bengio, 2009) has become an attractive area for multiple applications, including computer vision, speech recognition and natural language processing. Among the different deep learning strategies, convolutional neural networks have been successfully applied to different NLP task such as part-of-speech tagging (dos Santos and Zadrozny, 2014), sentiment analysis (Kim, 2014; dos Santos and Gatti, 2014), question classification (Kalchbrenner et al., 2014), semantic role labeling (Collobert et al., 2011), hashtag prediction (Weston et al., 2014), sentence completion and response matching (Hu et al., 2014). Some recent work on deep learning for relation classification include Socher et al. (2012), Zeng et al. (2014) and Yu et al. (2014). In (Socher et al., 2012), the authors tackle relation classification using a recursive neural network (RNN) that assigns a matrix-vector representation to every node in a parse tree. The representation for the complete sentence is computed bottom-up by recursively combining the words according to the syntactic s"
P15-1061,D14-1181,0,\N,Missing
P15-2029,E06-1011,0,0.0423355,"Missing"
P15-2029,C04-1121,0,0.176218,"Missing"
P15-2029,D15-1279,0,0.0480131,"but while his sequential CNNs put a word in its sequential context, ours considers a word and its parent, grandparent, great-grand-parent, and siblings on the dependency tree. This way we incorporate longdistance information that are otherwise unavailable on the surface string. Experiments on three classification tasks demonstrate the superior performance of our DCNNs over the baseline sequential CNNs. In particular, our accuracy on the TREC dataset outperforms all previously published results in the literature, including those with heavy hand-engineered features. Independently of this work, Mou et al. (2015, unpublished) reported related efforts; see Sec. 3.3. In sentence modeling and classification, convolutional neural network approaches have recently achieved state-of-the-art results, but all such efforts process word vectors sequentially and neglect long-distance dependencies. To combine deep learning with linguistic structures, we propose a dependency-based convolution approach, making use of tree-based n-grams rather than surface ones, thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achi"
P15-2029,P05-1015,0,0.29457,"Missing"
P15-2029,D14-1070,0,0.0374971,"Missing"
P15-2029,P06-1063,0,0.0715932,"Missing"
P15-2029,P14-1062,0,0.550006,", thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC. 1 Introduction Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al., 2014). In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the latter play an important role in many linguistic phenomena such as negation, subordination, and wh-extraction, all of which might dully a"
P15-2029,D14-1181,0,0.224626,"ements (Gamon, 2004; Matsumoto et al., 2005), while some otherwise (Dave et al., 2003; Kudo and Matsumoto, 2004). As a result, syntactic features have yet to become popular in the sentiment analysis community. We suspect one of the reasons for this is data sparsity (according to our experiments, tree n-grams are significantly sparser than surface n-grams), but this problem has largely been alleviated by the recent advances in word embedding. Can we combine the advantages of both worlds? So we propose a very simple dependency-based convolutional neural networks (DCNNs). Our model is similar to Kim (2014), but while his sequential CNNs put a word in its sequential context, ours considers a word and its parent, grandparent, great-grand-parent, and siblings on the dependency tree. This way we incorporate longdistance information that are otherwise unavailable on the surface string. Experiments on three classification tasks demonstrate the superior performance of our DCNNs over the baseline sequential CNNs. In particular, our accuracy on the TREC dataset outperforms all previously published results in the literature, including those with heavy hand-engineered features. Independently of this work,"
P15-2029,D11-1014,0,0.694798,"being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the latter play an important role in many linguistic phenomena such as negation, subordination, and wh-extraction, all of which might dully affect the sentiment, subjectivity, or other categorization of the sentence. 2 Dependency-based Convolution The original CNN, first proposed by LeCun et al. (1995), applies convolution kernels on a series of continuous areas of given images, and was adapted to NLP by Collobert et al. (2011). Following Kim (2014), one dimensional convolution operates the convolution kernel in sequential order in Equation 1, where xi ∈ Rd represents the d dimensional word representation for the i-th word in ∗ This work was done at both IBM and CUNY, and was supported in part by DARPA FA8750-13-2-0041 (DEFT), and NSF IIS-1449278. We thank Yoon Kim for sharing his code, and James Cross and Kai Zhao for discussions. 174 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages"
P15-2029,P10-1001,0,0.014268,"Missing"
P15-2029,W04-3239,0,0.100597,"Missing"
P15-2029,D13-1170,0,0.0564206,"Missing"
P15-2029,P14-2105,0,0.0131604,"stic structures, we propose a dependency-based convolution approach, making use of tree-based n-grams rather than surface ones, thus utlizing nonlocal interactions between words. Our model improves sequential baselines on all four sentiment and question classification tasks, and achieves the highest published accuracy on TREC. 1 Introduction Convolutional neural networks (CNNs), originally invented in computer vision (LeCun et al., 1995), has recently attracted much attention in natural language processing (NLP) on problems such as sequence labeling (Collobert et al., 2011), semantic parsing (Yih et al., 2014), and search query retrieval (Shen et al., 2014). In particular, recent work on CNN-based sentence modeling (Kalchbrenner et al., 2014; Kim, 2014) has achieved excellent, often state-of-the-art, results on various classification tasks such as sentiment, subjectivity, and question-type classification. However, despite their celebrated success, there remains a major limitation from the linguistics perspective: CNNs, being invented on pixel matrices in image processing, only consider sequential n-grams that are consecutive on the surface string and neglect longdistance dependencies, while the lat"
P15-2029,C02-1150,0,0.030806,"Missing"
P15-2029,P14-5010,0,0.00964441,"Missing"
P16-1014,P16-1154,0,0.655127,"Missing"
P16-1014,P15-1002,0,0.170257,"Missing"
P16-1014,D15-1044,0,0.113512,"Missing"
P16-1014,P15-1001,1,\N,Missing
P16-1014,P16-1162,0,\N,Missing
P16-1044,P15-2114,1,0.424703,"task. The approaches normally pursue the solution on the following directions. First, a joint feature vector is constructed based on both the question and the answer, and then the task can be converted into a classification or ranking problem (Wang and Nyberg, 2015; Hu et al., 2014). Second, recently proposed models for text generation can intrinsically be used for answer selection and generation (Bahdanau et al., 2015; Vinyals and Le, 2015). Finally, the question and answer representations can be learned and then matched by certain similarity metrics (Feng et al., 2015; Yu et al., 2014; dos Santos et al., 2015; Qiu and Huang, 2015). Fundamentally, our proposed models belong to the last category. 3.1 LSTM for Answer Selection Our LSTM implementation is similar to the one in (Graves et al., 2013) with minor modifications. Given an input sequence X = {x(1), x(2), · · · , x(n)}, where x(t) is an Edimension word vector in this paper, the hidden vector h(t) (with size H) at the time step t is updated as follows. it = σ(Wi x(t) + Ui h(t − 1) + bi ) (1) ft = σ(Wf x(t) + Uf h(t − 1) + bf ) (2) ot = σ(Wo x(t) + Uo h(t − 1) + bo ) (3) C˜t = tanh(Wc x(t) + Uc h(t − 1) + bc )(4) Ct = it ∗ C˜t + ft ∗ Ct−1 (5) Me"
P16-1044,N10-1145,0,0.0273977,"use the V1 version of this dataset). 2 The data is obtained from (Yao et al., 2013) http://cs.jhu.edu/˜xuchen/packages/jacana-qa-naacl2013data-results.tar.bz2 465 and Yin et al. (2015) generate interactive attention weights on both inputs by assignment matrices. Yin et al. (2015) use a simple Euclidean distance to compute the interdependence between the two input texts, while dos Santos et al. (2016) resort to attentive parameter matrices. cal matching between the question/answer parse trees. Some work tried to fulfill the matching using minimal edit sequences between dependency parse trees (Heilman and Smith, 2010; Yao et al., 2013). Discriminative tree-edit feature extraction and engineering over parsing trees were automated in (Severyn and Moschitti, 2013). Such methods might suffer from the availability of additional resources, the effort of feature engineering and the systematic complexity introduced by the linguistic tools, such as parse trees and dependency trees. 3 Approaches In this section, we first present our basic discriminative framework for answer selection based on long short-term memory (LSTM), which we call QA-LSTM. Next, we detail the proposed hybrid and attentive neural networks that"
P16-1044,C10-1131,0,0.0659681,"ortant local information. How to combine the merits from both has not been sufficiently explored. Secondly, previous approaches are usually based on independently generated question and answer embeddings; the quality of such representations, however, usually degrades as the answer sequences grow longer. 2 Related work Previous work on answer selection normally used feature engineering, linguistic tools, or external resources. For example, semantic features were constructed based on WordNet in (Yih et al., 2013). This model pairs semantically related words based on word semantic relations. In (Wang and Manning, 2010; Wang et al., 2007), the answer selection problem was transformed to a syntacti1 git clone https://github.com/shuzi/insuranceQA.git (We use the V1 version of this dataset). 2 The data is obtained from (Yao et al., 2013) http://cs.jhu.edu/˜xuchen/packages/jacana-qa-naacl2013data-results.tar.bz2 465 and Yin et al. (2015) generate interactive attention weights on both inputs by assignment matrices. Yin et al. (2015) use a simple Euclidean distance to compute the interdependence between the two input texts, while dos Santos et al. (2016) resort to attentive parameter matrices. cal matching betwee"
P16-1044,P15-2116,0,0.168548,"Missing"
P16-1044,P14-1062,0,0.00708652,"hitecture, we replace the simple pooling layers (average/maxpooling) by a convolutional layer, which allows to capture richer local information by applying a convolution over sequences of LSTM output vectors. The number of output vectors k (context window size) considered by the convolution is a hyper-parameter of the model. The convolution structure adopted in this work is as follows: Z ∈ Rk|h|×L is a matrix where the m-th column is the concatenation of k hidden vectors generated from biLSTM centralized in the m-th word of the sequence, L is the length of the sequence after wide convolution (Kalchbrenner et al., 2014). The output of the convolution with c filters is, Convolutional LSTMs The pooling strategies used in QA-LSTM suffer from the incapability of filtering important local information, especially when dealing with long answer sequences. Also, it is well known that LSTM models successfully keep the useful information from longrange dependency. But the strength has a tradeoff effect of ignoring the local n-gram coherence. This can be partially alleviated with bidirectional architectures. Meanwhile, the convolutional structures have been widely used in the question answering tasks, C = tanh(Wcp Z) (8"
P16-1044,D07-1003,0,0.387562,"tructures to distinguish better between useful and irrelevant pieces presented in questions and answers. Next, by breaking the independence assumption of the question and answer embedding, we introduce an effective attention mechanism to generate answer representations according to the question, such that the embeddings do not overlook informative parts of the answers. We report experimental results for two answer selection datasets: (1) InsuranceQA (Feng et al., 2015) 1 , a recently released large-scale nonfactoid QA dataset from the insurance domain, and (2) TREC-QA 2 , which was created by Wang et al. (2007) based on Text REtrieval Conference (TREC) QA track data. The contribution of this paper is hence threefold: 1) We propose hybrid neural networks, which learn better representations for both questions and answers by combining merits of both RNN and CNN. 2) We prove the effectiveness of attention on the answer selection task, which has not been sufficiently explored in prior work. 3) We achieve the state-of-the-art results on both TRECQA and InsuranceQA datasets. The rest of the paper is organized as follows: Section 2 describes the related work for answer selection; Section 3 provides the deta"
P16-1044,D14-1194,0,0.0139045,"e retrieve the word embeddings (WEs) of both q and a. Then, we separately apply a biLSTM over the two sequences of WEs. Next, 466 we generate a fixed-sized distributed vector representations using one of the following three approaches: (1) the concatenation of the last vectors on both directions of the biLSTM; (2) average pooling over all the output vectors of the biLSTM; (3) max pooling over all the output vectors. Finally, we use cosine similarity sim(q, a) to score the input (q, a) pair. It is important to note that the same biLSTM is applied to both q and a. Similar to (Feng et al., 2015; Weston et al., 2014; Hu et al., 2014), we define the training objective as a hinge loss. such as (Yu et al., 2014; Feng et al., 2015; Hu et al., 2014). Classical convolutional layers usually emphasize the local lexical connections of the n-gram. However, the local pieces are associated with each other only at the pooling step. No longrange dependencies are taken into account during the formulation of convolution vectors. Fundamentally, recurrent and convolutional neural networks have their own pros and cons, due to their different topologies. How to keep both merits motivates our studies of the following two hyb"
P16-1044,N13-1106,0,0.0316526,"representations, however, usually degrades as the answer sequences grow longer. 2 Related work Previous work on answer selection normally used feature engineering, linguistic tools, or external resources. For example, semantic features were constructed based on WordNet in (Yih et al., 2013). This model pairs semantically related words based on word semantic relations. In (Wang and Manning, 2010; Wang et al., 2007), the answer selection problem was transformed to a syntacti1 git clone https://github.com/shuzi/insuranceQA.git (We use the V1 version of this dataset). 2 The data is obtained from (Yao et al., 2013) http://cs.jhu.edu/˜xuchen/packages/jacana-qa-naacl2013data-results.tar.bz2 465 and Yin et al. (2015) generate interactive attention weights on both inputs by assignment matrices. Yin et al. (2015) use a simple Euclidean distance to compute the interdependence between the two input texts, while dos Santos et al. (2016) resort to attentive parameter matrices. cal matching between the question/answer parse trees. Some work tried to fulfill the matching using minimal edit sequences between dependency parse trees (Heilman and Smith, 2010; Yao et al., 2013). Discriminative tree-edit feature extract"
P16-1044,P13-1171,0,0.021363,"l interaction within n-gram, while RNN is designed to capture long range information and forget unimportant local information. How to combine the merits from both has not been sufficiently explored. Secondly, previous approaches are usually based on independently generated question and answer embeddings; the quality of such representations, however, usually degrades as the answer sequences grow longer. 2 Related work Previous work on answer selection normally used feature engineering, linguistic tools, or external resources. For example, semantic features were constructed based on WordNet in (Yih et al., 2013). This model pairs semantically related words based on word semantic relations. In (Wang and Manning, 2010; Wang et al., 2007), the answer selection problem was transformed to a syntacti1 git clone https://github.com/shuzi/insuranceQA.git (We use the V1 version of this dataset). 2 The data is obtained from (Yao et al., 2013) http://cs.jhu.edu/˜xuchen/packages/jacana-qa-naacl2013data-results.tar.bz2 465 and Yin et al. (2015) generate interactive attention weights on both inputs by assignment matrices. Yin et al. (2015) use a simple Euclidean distance to compute the interdependence between the t"
P16-1044,D15-1044,0,0.0149604,"(1), x(2), · · · , x(n)}, where x(t) is an Edimension word vector in this paper, the hidden vector h(t) (with size H) at the time step t is updated as follows. it = σ(Wi x(t) + Ui h(t − 1) + bi ) (1) ft = σ(Wf x(t) + Uf h(t − 1) + bf ) (2) ot = σ(Wo x(t) + Uo h(t − 1) + bo ) (3) C˜t = tanh(Wc x(t) + Uc h(t − 1) + bc )(4) Ct = it ∗ C˜t + ft ∗ Ct−1 (5) Meanwhile, attention-based systems have shown very promising results on a variety of NLP tasks, such as machine translation (Bahdanau et al., 2015; Sutskever et al., 2014), machine reading comprehension (Hermann et al., 2015), text summarization (Rush et al., 2015) and text entailment (Rockt¨aschel et al., 2016). Such models learn to focus their attention to specific parts of their input and most of them are based on a one-way attention, in which the attention is basically performed merely over one type of input based on another (e.g. over target languages based on the source languages for machine translation, or over documents according to queries for reading comprehension). Most recently, several two-way attention mechanisms are proposed, where the information from the two input items can influence the computation of each others representations. Rockt"
P16-1044,D13-1044,0,0.127246,"a-results.tar.bz2 465 and Yin et al. (2015) generate interactive attention weights on both inputs by assignment matrices. Yin et al. (2015) use a simple Euclidean distance to compute the interdependence between the two input texts, while dos Santos et al. (2016) resort to attentive parameter matrices. cal matching between the question/answer parse trees. Some work tried to fulfill the matching using minimal edit sequences between dependency parse trees (Heilman and Smith, 2010; Yao et al., 2013). Discriminative tree-edit feature extraction and engineering over parsing trees were automated in (Severyn and Moschitti, 2013). Such methods might suffer from the availability of additional resources, the effort of feature engineering and the systematic complexity introduced by the linguistic tools, such as parse trees and dependency trees. 3 Approaches In this section, we first present our basic discriminative framework for answer selection based on long short-term memory (LSTM), which we call QA-LSTM. Next, we detail the proposed hybrid and attentive neural networks that are built on top of the QA-LSTM framework. Some recent work has used deep learning methods for the passage-level answer selection task. The approa"
P17-1053,C16-1236,0,0.0667671,"Missing"
P17-1053,D13-1160,0,0.384459,"ames via different levels of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field su"
P17-1053,P14-2012,0,0.113812,"extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of re"
P17-1053,D16-1244,0,0.0119806,"Missing"
P17-1053,S10-1057,0,0.0312402,"m to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks. 2 Related Work Relation Extraction Relation extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2"
P17-1053,P16-1076,0,0.0424625,"embeddings). For relation detection in KBQA, such information is mostly missing because: (1) one question usually contains single argument (the topic entity) and (2) one KB entity could have multiple types (type vocabulary size larger than 1,500). This makes KB entity typing itself a difficult problem so no previous used entity information in the relation detection model.3 Relation Detection in KBQA Systems Relation detection for KBQA also starts with featurerich approaches (Yao and Van Durme, 2014; Bast and Haussmann, 2015) towards usages of deep networks (Yih et al., 2015; Xu et al., 2016; Dai et al., 2016) and attention models (Yin et al., 2016; Golub and He, 2016). Many of the above relation detection research could naturally support large relation vocabulary and open relation sets (especially for QA with OpenIE KB like ParaLex (Fader et al., 2013)), in order to fit the goal of open-domain question answering. Different KBQA data sets have different levels of requirement about the above open-domain capacity. For example, most of the gold test relations in WebQuestions can be observed during training, thus some prior work on this task adopted the close domain assumption like in the general RE re"
P17-1053,D16-1054,0,0.0546844,"Missing"
P17-1053,P15-1061,1,0.455256,"rks on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node. 572 KBP2015 has 74 relations although it considers open-domain Wiki"
P17-1053,P11-1053,0,0.0156809,"art results on both single-relation and multi-relation KBQA tasks. 2 Related Work Relation Extraction Relation extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et a"
P17-1053,P13-1158,0,0.0412673,"ation (WebQSP) QA benchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field such tasks are usually called relation extraction or relation classification. 571 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 571–581 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1053 Question: what episode was mike kelley the writer of Entity Linking Question: what tv show did grant show play on in 2008 Entity Link"
P17-1053,D16-1166,0,0.291728,"tion is mostly missing because: (1) one question usually contains single argument (the topic entity) and (2) one KB entity could have multiple types (type vocabulary size larger than 1,500). This makes KB entity typing itself a difficult problem so no previous used entity information in the relation detection model.3 Relation Detection in KBQA Systems Relation detection for KBQA also starts with featurerich approaches (Yao and Van Durme, 2014; Bast and Haussmann, 2015) towards usages of deep networks (Yih et al., 2015; Xu et al., 2016; Dai et al., 2016) and attention models (Yin et al., 2016; Golub and He, 2016). Many of the above relation detection research could naturally support large relation vocabulary and open relation sets (especially for QA with OpenIE KB like ParaLex (Fader et al., 2013)), in order to fit the goal of open-domain question answering. Different KBQA data sets have different levels of requirement about the above open-domain capacity. For example, most of the gold test relations in WebQuestions can be observed during training, thus some prior work on this task adopted the close domain assumption like in the general RE research. While for data sets like SimpleQuestions and ParaLex"
P17-1053,N16-1065,0,0.032177,"Missing"
P17-1053,D15-1205,1,0.564694,"Missing"
P17-1053,P16-1123,0,0.048317,"Missing"
P17-1053,C16-1164,1,0.738237,"Missing"
P17-1053,N16-1170,0,0.0445311,"Missing"
P17-1053,N16-1117,1,0.8342,"ng capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node. 572 KBP2015 has 74 relations although it considers open-domain Wikipedia relations. All are much fewer than thousands of relations in KBQA. As a result, few work in this field focuses on dealing with large number of relations or unseen relations. Yu et al. (2016) proposed to use relation embeddings in a low-rank tensor method. However their relation embeddings are still trained in supervised way and the number of relations is not large in the experiments. search assumes that the two argument entities are both available. Thus it usually benefits from features (Nguyen and Grishman, 2014; Gormley et al., 2015) or attention mechanisms (Wang et al., 2016) based on the entity information (e.g. entity types or entity embeddings). For relation detection in KBQA, such information is mostly missing because: (1) one question usually contains single argument (the"
P17-1053,C14-1220,0,0.029909,"n this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node. 572 KBP2015 has 74 relations although it c"
P17-1053,P16-1220,0,0.44264,"entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field such tasks are usually called relation extraction or relation classification. 571 Proceedings of the 55"
P17-1053,P15-1049,0,0.0158297,"sts of a Freebase subset with 2M entities (FB2M) (Bordes et al., 2015), in order to compare with previous research. Yin et al. (2016) also evaluated their relation extractor on this data set and released their proposed question-relation pairs, so we run our relation detection model on their data set. For the KBQA evaluation, we also start with their entity linking results6 . Therefore, our results can be compared with their reported results on both tasks. WebQSP (WQ): A multi-relation KBQA task. We use the entire Freebase KB for evaluation purposes. Following Yih et al. (2016), we use S-MART (Yang and Chang, 2015) entity-linking outputs.7 In order to evaluate the relation detection models, we create a new relation detection task from the WebQSP data set.8 For each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length  2) connected to the topic entity, and set the corechain labeled in the parse as the positive label and all the others as the negative examples. We tune the following hyper-parameters on development sets: (1) the size of hidden states for LSTMs ({50, 100, 200, 400})9 ; (2) learning ra"
P17-1053,P05-1053,0,0.147839,"r simple KBQA system to achieve state-of-the-art results on both single-relation and multi-relation KBQA tasks. 2 Related Work Relation Extraction Relation extraction (RE) is an important sub-field of information extraction. General research in this field usually works on a (small) pre-defined relation set, where given a text paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Ta"
P17-1053,P16-2034,0,0.0088906,"t paragraph and two target entities, the goal is to determine whether the text indicates any types of relations between the entities or not. As a result RE is usually formulated as a classification task. Traditional RE methods rely on large amount of hand-crafted features (Zhou et al., 2005; Rink and Harabagiu, 2010; Sun et al., 2011). Recent research benefits a lot from the advancement of deep learning: from word embeddings (Nguyen and Grishman, 2014; Gormley et al., 2015) to deep networks like CNNs and LSTMs (Zeng et al., 2014; dos Santos et al., 2015; Vu et al., 2016) and attention models (Zhou et al., 2016; Wang et al., 2016). The above research assumes there is a fixed (closed) set of relation types, thus no zero-shot learning capability is required. The number of relations is usually not large: The widely used ACE2005 has 11/32 coarse/fine-grained relations; SemEval2010 Task8 has 19 relations; TAC2 Following Yih et al. (2015), here topic entity refers to the root of the (directed) query tree; and core-chain is the directed path of relation from root to the answer node. 572 KBP2015 has 74 relations although it considers open-domain Wikipedia relations. All are much fewer than thousands of rela"
P17-1053,W14-2416,0,0.0230301,"Missing"
P17-1053,P14-1090,0,0.128408,"Missing"
P17-1053,P15-1128,0,0.342367,"m that integrates entity linking and our proposed relation detector to make the two components enhance each other. Our experimental results show that our approach not only achieves outstanding relation detection performance, but more importantly, it helps our KBQA system achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field such tasks are usually called relation extraction or relation classification. 571 Pro"
P17-1053,P14-2105,0,0.0490471,"nchmarks. 1 Introduction Knowledge Base Question Answering (KBQA) systems answer questions by obtaining information from KB tuples (Berant et al., 2013; Yao et al., 2014; Bordes et al., 2015; Bast and Haussmann, 2015; Yih et al., 2015; Xu et al., 2016). For an input question, these systems typically generate a KB query, which can be executed to retrieve the answers from a KB. Figure 1 illustrates the process used to parse two sample questions in a KBQA system: (a) a single-relation question, which can be answered with a single &lt;head-entity, relation, tail-entity&gt; KB tuple (Fader et al., 2013; Yih et al., 2014; Bordes et al., 2015); and (b) a more complex case, where some constraints need to be handled 1 In the information extraction field such tasks are usually called relation extraction or relation classification. 571 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 571–581 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1053 Question: what episode was mike kelley the writer of Entity Linking Question: what tv show did grant show play on in 2008 Entity Linking Relation Detec"
P17-1053,P16-2033,0,0.0532749,"ion KBQA task. The KB we use consists of a Freebase subset with 2M entities (FB2M) (Bordes et al., 2015), in order to compare with previous research. Yin et al. (2016) also evaluated their relation extractor on this data set and released their proposed question-relation pairs, so we run our relation detection model on their data set. For the KBQA evaluation, we also start with their entity linking results6 . Therefore, our results can be compared with their reported results on both tasks. WebQSP (WQ): A multi-relation KBQA task. We use the entire Freebase KB for evaluation purposes. Following Yih et al. (2016), we use S-MART (Yang and Chang, 2015) entity-linking outputs.7 In order to evaluate the relation detection models, we create a new relation detection task from the WebQSP data set.8 For each question and its labeled semantic parse: (1) we first select the topic entity from the parse; and then (2) select all the relations and relation chains (length  2) connected to the topic entity, and set the corechain labeled in the parse as the positive label and all the others as the negative examples. We tune the following hyper-parameters on development sets: (1) the size of hidden states for LSTMs ({"
P17-2053,1993.eamt-1.1,0,0.647058,"Missing"
P17-2053,P15-2029,1,0.851551,"ictionary learning to CNN, we first develop a neural version of SGL, Group Sparse Autoencoders (GSAs), which to the best of our knowledge, is the first full neural model with group sparse constraints. The encoding matrix of GSA (like the dictionary in SGL) is grouped into different categories. The bases in different groups can be either initialized randomly or by Introduction Question classification has applications in many domains ranging from question answering to dialog systems, and has been increasingly popular in recent years. Several recent efforts (Kim, 2014; Kalchbrenner et al., 2014; Ma et al., 2015) treat questions as general sentences and employ Convolutional Neural Networks (CNNs) to achieve remarkably strong performance in the TREC question classification task. We argue, however, that those general sentence modeling frameworks neglect two unique properties of question classification. First, different from the flat and coarse categories in most sentence classification tasks (i.e. sentimental classification), question classes often have a hierarchical structure such as those from the New York State DMV FAQ1 (see Fig. 1). Another unique aspect of question classification is the well prepa"
P17-2053,P14-1062,0,0.101124,"n et al., 2013). To apply dictionary learning to CNN, we first develop a neural version of SGL, Group Sparse Autoencoders (GSAs), which to the best of our knowledge, is the first full neural model with group sparse constraints. The encoding matrix of GSA (like the dictionary in SGL) is grouped into different categories. The bases in different groups can be either initialized randomly or by Introduction Question classification has applications in many domains ranging from question answering to dialog systems, and has been increasingly popular in recent years. Several recent efforts (Kim, 2014; Kalchbrenner et al., 2014; Ma et al., 2015) treat questions as general sentences and employ Convolutional Neural Networks (CNNs) to achieve remarkably strong performance in the TREC question classification task. We argue, however, that those general sentence modeling frameworks neglect two unique properties of question classification. First, different from the flat and coarse categories in most sentence classification tasks (i.e. sentimental classification), question classes often have a hierarchical structure such as those from the New York State DMV FAQ1 (see Fig. 1). Another unique aspect of question classification"
P17-2053,D14-1181,0,0.151024,"(SGL) (Simon et al., 2013). To apply dictionary learning to CNN, we first develop a neural version of SGL, Group Sparse Autoencoders (GSAs), which to the best of our knowledge, is the first full neural model with group sparse constraints. The encoding matrix of GSA (like the dictionary in SGL) is grouped into different categories. The bases in different groups can be either initialized randomly or by Introduction Question classification has applications in many domains ranging from question answering to dialog systems, and has been increasingly popular in recent years. Several recent efforts (Kim, 2014; Kalchbrenner et al., 2014; Ma et al., 2015) treat questions as general sentences and employ Convolutional Neural Networks (CNNs) to achieve remarkably strong performance in the TREC question classification task. We argue, however, that those general sentence modeling frameworks neglect two unique properties of question classification. First, different from the flat and coarse categories in most sentence classification tasks (i.e. sentimental classification), question classes often have a hierarchical structure such as those from the New York State DMV FAQ1 (see Fig. 1). Another unique aspect"
P19-1260,D17-1209,0,0.0222336,"a sets (Sun et al., 2018; Wu et al., 2019). However, these data sets require multi-hop reasoning over multiple sentences or multiple common knowledge while the problem 1 By May 30th 2019, http://qangaroo.cs.ucl. ac.uk/leaderboard.html 2705 Final scores we want to solve in this paper requires collecting evidences across multiple documents. GNN for NLP: Recently, there is considerable amount of interest in applying GNN to NLP tasks and great success has been achieved. For example, in neural machine translation, GNN has been employed to integrate syntactic and semantic information into encoders (Bastings et al., 2017; Marcheggiani et al., 2018); Zhang et al. (2018) applied GNN to relation extraction over pruned dependency trees; the study by Yao et al. (2018) employed GNN over a heterogeneous graph to do text classification, which inspires our idea of the HDE graph; Liu et al. (2018) proposed a new contextualized neural network for sequence learning by leveraging various types of non-local contextual information in the form of information passing over GNN. These studies are related to our work in the sense that we both use GNN to improve the information interaction over long context or across documents. 3"
P19-1260,D14-1179,0,0.0273333,"Missing"
P19-1260,N19-1240,0,0.276926,"Missing"
P19-1260,N18-2007,0,0.0324694,"t is not enough to find the correct answer. To promote the study for multi-hop RC over multiple documents, two data sets are recently proposed: W IKI H OP (Welbl et al., 2018) and HotpotQA (Yang et al., 2018). These two data sets require multi-hop reasoning over multiple supporting documents to find the answer. In Figure 1, we show an excerpt from one sample in W IKI H OP development set to illustrate the need for multi-hop reasoning. Two types of approaches have been proposed on the multi-hop multi-document RC problem. The first is based on previous neural RC models. The earliest attempt in (Dhingra et al., 2018) concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener2704 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2704–2713 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics ated coreference annotations. Adding this layer to the neural RC models improved performance on multi-hop tasks. Recently, an attention based system (Zhong et al., 2019) utilizing both documentlevel and entity-level information achieved stateo"
P19-1260,D17-1206,0,0.032038,"-GCN(De Cao et al., 2018) DynSAN* Proposed 68.5 70.9 Single models Model Full model - HDE graph - different edge types - candidate nodes scores - entity nodes scores - candidate nodes - document nodes - entity nodes 71.2 73.8 74.3 Table 2: Ablation results on the W IKI H OP dev set. Table 1: Performance comparison among different models on W IKI H OP development and test set. The results of “BiDAF” are presented in the paper by Welbl et al. (2018). Models annotated with “*” are unpublished but available on W IKI H OP leaderboard. “-” indicates unavailable numbers. character n-gram embeddings (Hashimoto et al., 2017) are used to convert words into 400dimensional vector representations. Out of vocabulary words are initialized with random vectors. The embedding matrices are not updated during training. The proposed model is implemented with PyTorch (Paszke et al., 2017). More details about experimental and hyperparameter settings can be found in supplementary materials. The performance on development set is measured after each training epoch, and the model with the highest accuracy is saved and submitted to be evaluated on the blind test set. We will make our code publicly available after the review process"
P19-1260,P16-1145,0,0.0240712,"tput dimension is 1. We directly sum the scores from candidate nodes and entity nodes as the final scores over multiple candidates. Thus, the output score vector a ∈ RC×1 gives a distribution over all candidates. Since the task is multi-class classification, we use cross-entropy loss as training objective which takes a and the labels as input. 4 4.1 Experiments Dataset We use W IKI H OP (Welbl et al., 2018) to validate the effectiveness of our proposed model. The query of W IKI H OP is constructed with entities and relations from W IKI DATA, while supporting documents are from W IKI R EADING (Hewlett et al., 2016). A bipartite graph connecting entities and documents is first built and the answer for each query is located by traversal on this graph. Candidates that are type-consistent with the answer and share the same relation in query with the answer are included, resulting in a set of candidates. Thus, W IKI H OP is a multi-choice style reading comprehension data set. There are totally about 43K samples in training set, 5K samples in development set and 2.5K samples in test set. The test set is not provided and can only be evaluated on blindly. The task is to predict the correct answer given a query"
P19-1260,N18-1023,0,0.0313722,"how the effectiveness of attention mechanisms. Our model is very different from the other two studies (Dhingra et al., 2018; Kundu et al., 2018): these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs. Besides these studies, our work is also related to the following research directions. Multi-hop RC: There exist several different data sets that require reasoning in multiple steps in literature, for example bAbI (Weston et al., 2015), MultiRC (Khashabi et al., 2018) and OpenBookQA (Mihaylov et al., 2018). A lot of systems have been proposed to solve the multi-hop RC problem with these data sets (Sun et al., 2018; Wu et al., 2019). However, these data sets require multi-hop reasoning over multiple sentences or multiple common knowledge while the problem 1 By May 30th 2019, http://qangaroo.cs.ucl. ac.uk/leaderboard.html 2705 Final scores we want to solve in this paper requires collecting evidences across multiple documents. GNN for NLP: Recently, there is considerable amount of interest in applying GNN to NLP tasks and great success has been achieved. For"
P19-1260,Q18-1023,0,0.0477443,"Missing"
P19-1260,W02-0109,0,0.394884,"Missing"
P19-1260,N18-2078,0,0.0298225,"8; Wu et al., 2019). However, these data sets require multi-hop reasoning over multiple sentences or multiple common knowledge while the problem 1 By May 30th 2019, http://qangaroo.cs.ucl. ac.uk/leaderboard.html 2705 Final scores we want to solve in this paper requires collecting evidences across multiple documents. GNN for NLP: Recently, there is considerable amount of interest in applying GNN to NLP tasks and great success has been achieved. For example, in neural machine translation, GNN has been employed to integrate syntactic and semantic information into encoders (Bastings et al., 2017; Marcheggiani et al., 2018); Zhang et al. (2018) applied GNN to relation extraction over pruned dependency trees; the study by Yao et al. (2018) employed GNN over a heterogeneous graph to do text classification, which inspires our idea of the HDE graph; Liu et al. (2018) proposed a new contextualized neural network for sequence learning by leveraging various types of non-local contextual information in the form of information passing over GNN. These studies are related to our work in the sense that we both use GNN to improve the information interaction over long context or across documents. 3 Cand score 1 Context encodi"
P19-1260,D18-1260,0,0.0128476,"anisms. Our model is very different from the other two studies (Dhingra et al., 2018; Kundu et al., 2018): these two studies both explicitly score the possible reasoning paths with extra NER or coreference resolution systems while our method does not require these modules and we do multi-hop reasoning over graphs. Besides these studies, our work is also related to the following research directions. Multi-hop RC: There exist several different data sets that require reasoning in multiple steps in literature, for example bAbI (Weston et al., 2015), MultiRC (Khashabi et al., 2018) and OpenBookQA (Mihaylov et al., 2018). A lot of systems have been proposed to solve the multi-hop RC problem with these data sets (Sun et al., 2018; Wu et al., 2019). However, these data sets require multi-hop reasoning over multiple sentences or multiple common knowledge while the problem 1 By May 30th 2019, http://qangaroo.cs.ucl. ac.uk/leaderboard.html 2705 Final scores we want to solve in this paper requires collecting evidences across multiple documents. GNN for NLP: Recently, there is considerable amount of interest in applying GNN to NLP tasks and great success has been achieved. For example, in neural machine translation,"
P19-1260,D14-1162,0,0.0843767,"xtual information in the form of information passing over GNN. These studies are related to our work in the sense that we both use GNN to improve the information interaction over long context or across documents. 3 Cand score 1 Context encoding Given a query q with the form of (s, r, ?) which represents subject, relation and unknown object respectively, a set of support documents Sq and a set of candidates Cq , the task is to predict the correct answer a∗ to the query. To encode information including in the text of query, candidates and support documents, we use a pretrained embedding matrix (Pennington et al., 2014) to convert word sequences to sequences of vectors. Let Xq ∈ j i Rlq ×d , Xis ∈ Rls ×d and Xjc ∈ Rlc ×d represent the embedding matrices of query, i-th supporting document and j-th candidate of a sample, where lq , lsi and lcj are the numbers of words in query, i-th supporting document and j-th candidate respectively. d is the dimension of the word embedding. We use bidirectional recurrent neural networks (RNN) Entity scores FC Cand nodes Self-attn FC Doc nodes Entity nodes Self-attn Self-attn coattn Methodology In this section, we describe different modules of the proposed Heterogeneous Docum"
P19-1260,N18-1202,0,0.10753,"Missing"
P19-1260,P18-2124,0,0.0525314,"Missing"
P19-1260,D16-1264,0,0.0530475,"ph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance. 1 Figure 1: A W IKI H OP example. Words with different colors indicate the evidences across documents. Introduction Being able to comprehend a document and output correct answer given a query/question about content in the document, often referred as machine reading comprehension (RC) or question answering (QA), is an important and challenging task in natural language processing (NLP). Plenty of data sets have been constructed to facilitate research on this topic, such as SQuAD (Rajpurkar et al., 2016, 2018), NarrativeQA (Koˇcisk`y et al., 2018) and CoQA (Reddy et al., 2018). Many neural models have been proposed to tackle the machine RC/QA problem (Seo et al., 2016; Xiong et al., 2016; Tay et al., 2018), and great success has been achieved, especially after the release of the BERT (Devlin et al., 2018). However, current research mainly focuses on machine RC/QA on a single document or paragraph, and still lacks the ability to do reasoning across multiple documents when a single document is not enough to find the correct answer. To promote the study for multi-hop RC over multiple documents,"
P19-1260,Q18-1021,0,0.106312,"CoQA (Reddy et al., 2018). Many neural models have been proposed to tackle the machine RC/QA problem (Seo et al., 2016; Xiong et al., 2016; Tay et al., 2018), and great success has been achieved, especially after the release of the BERT (Devlin et al., 2018). However, current research mainly focuses on machine RC/QA on a single document or paragraph, and still lacks the ability to do reasoning across multiple documents when a single document is not enough to find the correct answer. To promote the study for multi-hop RC over multiple documents, two data sets are recently proposed: W IKI H OP (Welbl et al., 2018) and HotpotQA (Yang et al., 2018). These two data sets require multi-hop reasoning over multiple supporting documents to find the answer. In Figure 1, we show an excerpt from one sample in W IKI H OP development set to illustrate the need for multi-hop reasoning. Two types of approaches have been proposed on the multi-hop multi-document RC problem. The first is based on previous neural RC models. The earliest attempt in (Dhingra et al., 2018) concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gene"
P19-1260,D18-1259,0,0.0327061,"ural models have been proposed to tackle the machine RC/QA problem (Seo et al., 2016; Xiong et al., 2016; Tay et al., 2018), and great success has been achieved, especially after the release of the BERT (Devlin et al., 2018). However, current research mainly focuses on machine RC/QA on a single document or paragraph, and still lacks the ability to do reasoning across multiple documents when a single document is not enough to find the correct answer. To promote the study for multi-hop RC over multiple documents, two data sets are recently proposed: W IKI H OP (Welbl et al., 2018) and HotpotQA (Yang et al., 2018). These two data sets require multi-hop reasoning over multiple supporting documents to find the answer. In Figure 1, we show an excerpt from one sample in W IKI H OP development set to illustrate the need for multi-hop reasoning. Two types of approaches have been proposed on the multi-hop multi-document RC problem. The first is based on previous neural RC models. The earliest attempt in (Dhingra et al., 2018) concatenated all supporting documents and designed a recurrent layer to explicitly exploit the skip connections between entities given automatically gener2704 Proceedings of the 57th Ann"
P19-1260,D18-1244,0,0.0426428,", these data sets require multi-hop reasoning over multiple sentences or multiple common knowledge while the problem 1 By May 30th 2019, http://qangaroo.cs.ucl. ac.uk/leaderboard.html 2705 Final scores we want to solve in this paper requires collecting evidences across multiple documents. GNN for NLP: Recently, there is considerable amount of interest in applying GNN to NLP tasks and great success has been achieved. For example, in neural machine translation, GNN has been employed to integrate syntactic and semantic information into encoders (Bastings et al., 2017; Marcheggiani et al., 2018); Zhang et al. (2018) applied GNN to relation extraction over pruned dependency trees; the study by Yao et al. (2018) employed GNN over a heterogeneous graph to do text classification, which inspires our idea of the HDE graph; Liu et al. (2018) proposed a new contextualized neural network for sequence learning by leveraging various types of non-local contextual information in the form of information passing over GNN. These studies are related to our work in the sense that we both use GNN to improve the information interaction over long context or across documents. 3 Cand score 1 Context encoding Given a query q wi"
Q16-1019,D12-1050,0,0.00810263,"tworks to model sentence pairs for AS, PI and TE. For AS, Yu et al. (2014) present a bigram CNN to model question and answer candidates. Yang et al. (2015) extend this method and get state-of-the-art performance on the WikiQA dataset (Section 5.1). Feng et al. (2015) test various setups of a bi-CNN architecture on an insurance domain QA dataset. Tan et al. (2016) explore bidirectional LSTMs on the same dataset. Our approach is different because we do not model the sentences by two independent neural networks in parallel, but instead as an interdependent sentence pair, using attention. For PI, Blacoe and Lapata (2012) form sentence representations by summing up word embeddings. Socher et al. (2011) use recursive autoencoders (RAEs) to model representations of local phrases in sentences, then pool similarity values of phrases from the two sentences as features for binary classification. Yin and Sch¨utze (2015a) similarly replace an RAE with a CNN. In all three papers, the representation of one sentence is not influenced by the other – in contrast to our attention-based model. For TE, Bowman et al. (2015b) use recursive neural networks to encode entailment on SICK (Marelli et al., 2014b). Rockt¨aschel et al."
Q16-1019,D15-1075,0,0.242137,"when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need for an architecture tha"
Q16-1019,W15-4002,0,0.0821956,"when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need for an architecture tha"
Q16-1019,N10-1066,0,0.0141601,"is on exploiting syntactic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, synonym and antonym relations. Yao et al. (2013a) extend word-to-word alignment to phraseto-phrase alignment by a semi-Markov CRF. However, such approaches often require more computational resources. In addition, employing syntactic or semantic parsers – which produce errors on many sentences – to find the best match between the structured representations of two sentences is not trivial. DL on Sentence Pair Modeling. To address some of the challenges of non-DL work, mu"
Q16-1019,C04-1051,0,0.18671,"Missing"
Q16-1019,D15-1181,0,0.40273,"Missing"
Q16-1019,N10-1145,0,0.0633371,"attracted lots of attention in the past decades. Many tasks can be reduced to a semantic text matching problem. Due to the variety of word choices and inherent ambiguities in natural language, bag-of-word approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting syntactic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, synonym and antonym relations. Yao et al. (2013a) extend word-to-word alignment to phraseto-phra"
Q16-1019,D13-1090,0,0.0125067,"Missing"
Q16-1019,S14-2131,0,0.0262344,"Missing"
Q16-1019,P14-1062,0,0.0950702,"nize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need for an architecture that computes different representations of si for different s1−i (i ∈ {0, 1}). 259 Transactions of the Association for Computational Linguistics, vol. 4, pp. 259–272, 2016. Action Editor: Brian Roark. Submission batch: 12/2015; Revision batch: 3/2016; Published 6/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Convolutional Neural Networks (CNNs) (LeCun et al., 1998) are widely used to model sentences (Kalchbrenner et al., 2014; Kim, 2014) and sentence pairs (Socher et al., 2011; Yin and Sch¨utze, 2015a), especially in classification tasks. CNNs are supposed to be good at extracting robust and abstract features of input. This work presents the ABCNN, an attention-based convolutional neural network, that has a powerful mechanism for modeling a sentence pair by taking into account the interdependence between the two sentences. The ABCNN is a general architecture that can handle a wide variety of sentence pair modeling tasks. Some prior work proposes simple mechanisms that can be interpreted as controlling varying atte"
Q16-1019,D14-1181,0,0.0350541,"and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need for an architecture that computes different representations of si for different s1−i (i ∈ {0, 1}). 259 Transactions of the Association for Computational Linguistics, vol. 4, pp. 259–272, 2016. Action Editor: Brian Roark. Submission batch: 12/2015; Revision batch: 3/2016; Published 6/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Convolutional Neural Networks (CNNs) (LeCun et al., 1998) are widely used to model sentences (Kalchbrenner et al., 2014; Kim, 2014) and sentence pairs (Socher et al., 2011; Yin and Sch¨utze, 2015a), especially in classification tasks. CNNs are supposed to be good at extracting robust and abstract features of input. This work presents the ABCNN, an attention-based convolutional neural network, that has a powerful mechanism for modeling a sentence pair by taking into account the interdependence between the two sentences. The ABCNN is a general architecture that can handle a wide variety of sentence pair modeling tasks. Some prior work proposes simple mechanisms that can be interpreted as controlling varying attention; e.g.,"
Q16-1019,S14-2055,0,0.00838841,"ctic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, synonym and antonym relations. Yao et al. (2013a) extend word-to-word alignment to phraseto-phrase alignment by a semi-Markov CRF. However, such approaches often require more computational resources. In addition, employing syntactic or semantic parsers – which produce errors on many sentences – to find the best match between the structured representations of two sentences is not trivial. DL on Sentence Pair Modeling. To address some of the challenges of non-DL work, much recent work uses neural ne"
Q16-1019,P15-1107,0,0.199045,"Missing"
Q16-1019,W04-1013,0,0.0122377,"Missing"
Q16-1019,D15-1166,0,0.137021,"only processing the selected regions at high resolution. Gregor et al. (2015) combine a spatial attention mechanism with RNNs for image generation. Ba et al. (2015) investigate attention-based RNNs for recognizing multiple objects in images. Chorowski et al. (2014) and Chorowski et al. (2015) use attention in RNNs for speech recognition. Attention-Based DL in NLP. Attention-based DL systems have been applied to NLP after their success in computer vision and speech recognition. They mainly rely on RNNs and end-to-end encoderdecoders for tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text reconstruction (Li et al., 2015; Rush et al., 2015). Our work takes the lead in exploring attention mechanisms in CNNs for NLP tasks. 3 BCNN: Basic Bi-CNN We now introduce our basic (non-attention) CNN that is based on the Siamese architecture (Bromley et al., 1993), i.e., it consists of two weightsharing CNNs, each processing one of the two sentences, and a final layer that solves the sentence pair task. See Figure 2. We refer to this architecture as the BCNN. The next section will then introduce the ABCNN, an attention architecture that extends the BCNN. Table 1 gives our notationa"
Q16-1019,N12-1019,0,0.00988376,"al influence of the two sentences in the context of the task. It also contradicts what humans do when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors"
Q16-1019,S14-2001,0,0.275524,"tradicts what humans do when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need f"
Q16-1019,marelli-etal-2014-sick,0,0.153021,"tradicts what humans do when comparing two sentences. We usually focus on key parts of one sentence by extracting parts from the other sentence that are related by identity, synonymy, antonymy and other relations. Thus, human beings model the two sentences together, using the content of one sentence to guide the representation of the other. Introduction How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS) (Yu et al., 2014; Feng et al., 2015), paraphrase identification (PI) (Madnani et al., 2012; Yin and Sch¨utze, 2015a), textual entailment (TE) (Marelli et al., 2014a; Bowman et al., 2015a) etc. Figure 1 demonstrates that each sentence of a pair partially determines which parts of the other sentence we must focus on. For AS, correctly answering s0 requires attention on “gross”: s+ 1 contains a corresponding unit (“earned”) while s− 1 does not. For PI, focus should be removed from “today” to correctly recognize &lt; s0 , s+ 1 &gt; as paraphrases and &lt; s0 , s− &gt; as non-paraphrases. For TE, we need 1 to focus on “full of people” (to recognize TE for &lt; s0 , s+ 1 &gt;) and on “outdoors” / “indoors” (to recognize non-TE for &lt; s0 , s− 1 &gt;). These examples show the need f"
Q16-1019,D15-1044,0,0.195677,"r et al. (2015) combine a spatial attention mechanism with RNNs for image generation. Ba et al. (2015) investigate attention-based RNNs for recognizing multiple objects in images. Chorowski et al. (2014) and Chorowski et al. (2015) use attention in RNNs for speech recognition. Attention-Based DL in NLP. Attention-based DL systems have been applied to NLP after their success in computer vision and speech recognition. They mainly rely on RNNs and end-to-end encoderdecoders for tasks such as machine translation (Bahdanau et al., 2015; Luong et al., 2015) and text reconstruction (Li et al., 2015; Rush et al., 2015). Our work takes the lead in exploring attention mechanisms in CNNs for NLP tasks. 3 BCNN: Basic Bi-CNN We now introduce our basic (non-attention) CNN that is based on the Siamese architecture (Bromley et al., 1993), i.e., it consists of two weightsharing CNNs, each processing one of the two sentences, and a final layer that solves the sentence pair task. See Figure 2. We refer to this architecture as the BCNN. The next section will then introduce the ABCNN, an attention architecture that extends the BCNN. Table 1 gives our notational conventions. In our implementation and also in the mathemat"
Q16-1019,D07-1002,0,0.00848433,"atures are used. 2 Related Work Non-DL on Sentence Pair Modeling. Sentence pair modeling has attracted lots of attention in the past decades. Many tasks can be reduced to a semantic text matching problem. Due to the variety of word choices and inherent ambiguities in natural language, bag-of-word approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting syntactic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, s"
Q16-1019,D07-1003,0,0.344451,"sks can be reduced to a semantic text matching problem. Due to the variety of word choices and inherent ambiguities in natural language, bag-of-word approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting syntactic and semantic structure. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007), tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasi-synchronous grammars (Wang et al., 2007) that match the dependency parse trees of the two sentences. Instead of focusing on the high-level semantic representation, Yih et al. (2013) turn their attention to improving the shallow semantic component, lexical semantics, by performing semantic matching based on a latent word-alignment structure (cf. Chang et al. (2010)). Lai and Hockenmaier (2014) explore finer-grained word overlap and alignment between two sentences using negation, hypernym, synonym and antonym relations. Yao et al. (2013a) extend word-to-word alignment to phraseto-phrase alignment by a semi-Markov CRF. However, such ap"
Q16-1019,D15-1237,0,0.717817,"Missing"
Q16-1019,D13-1056,0,0.00563054,"Missing"
Q16-1019,P13-2029,0,0.00630854,"Missing"
Q16-1019,P13-1171,0,0.0672204,"and sentence pairs (Socher et al., 2011; Yin and Sch¨utze, 2015a), especially in classification tasks. CNNs are supposed to be good at extracting robust and abstract features of input. This work presents the ABCNN, an attention-based convolutional neural network, that has a powerful mechanism for modeling a sentence pair by taking into account the interdependence between the two sentences. The ABCNN is a general architecture that can handle a wide variety of sentence pair modeling tasks. Some prior work proposes simple mechanisms that can be interpreted as controlling varying attention; e.g., Yih et al. (2013) employ word alignment to match related parts of the two sentences. In contrast, our attention scheme based on CNNs models relatedness between two parts fully automatically. Moreover, attention at multiple levels of granularity, not only at word level, is achieved as we stack multiple convolution layers that increase abstraction. Prior work on attention in deep learning (DL) mostly addresses long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997). LSTMs achieve attention usually in a word-to-word scheme, and word representations mostly encode the whole context within the sen"
Q16-1019,N15-1091,1,0.587059,"Missing"
Q16-1019,P15-1007,1,0.520976,"Missing"
Q16-1019,S14-2044,0,0.0779176,"Missing"
W06-3711,1983.tc-1.13,0,0.56224,"ient languages. The IBM MASTOR speech-to-speech translation system has been developed for the DARPA CAST and Transtac programs whose mission is to develop technologies that enable rapid deployment of real-time S2S translation of low-resource languages on portable devices. It originated from the IBM MARS S2S system handling the air travel reservation domain described in [1], which was later significantly improved in all components, including ASR, MT and TTS, and later evolved into the MASTOR multilingual S2S system that covers much broader domains such as medical treatment and force protection [2,3]. More recently, we have further broadened our experience and efforts to very rapidly develop systems for under-studied languages, such as regional dialects of Arabic. The intent of this program is to provide language support to military, medical and humanitarian personnel during operations in foreign territories, by deciphering possibly critical language communications with a two-way real-time speech-to-speech translation system designed for specific tasks such as medical triage and force protection. The initial data collection effort for the project has shown that the domain of force protect"
W08-0403,J07-2003,0,0.270841,"dels that utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank), and SCFG rules are derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models (Och and Ney, 2004) to handle local fluency, and the engagement of synchronous context-free grammars (SCFG), which enhances the generative capacity of the underlying model that is limited by finite-state machinery. Approaches to syntax-based tr"
W08-0403,P05-1066,0,0.0614724,"Missing"
W08-0403,N04-1035,0,0.201986,"improved models based on tree kernel methods. Experiments on an English-to-Chinese task demonstrate that our proposed models outperformed the baseline formally syntaxbased models, while both of them achieved significant improvements over a state-of-theart phrase-based SMT system. • Linguistically syntax-based: models that utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank), and SCFG rules are derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translat"
W08-0403,P06-1121,0,0.210961,"oding. Recent work by (Zhang et al., 2006) shows a practically efficient approach that binarizes linguistically SCFG rules when possible. 19 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 19–27, c ACL-08: HLT, Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics language model, which is a key element to ensure translation output quality. On the other hand, available linguistic theory and annotations could provide invaluable benefits in grammar induction and scoring, as shown by recent progress on such models (Galley et al., 2006). In contrast, formally syntax-based grammars often lack explicit linguistic constraints. In this paper, we propose a scheme to enrich formally syntax-based models with linguistically syntactic knowledge. In other words, we maintain our grammar to be based on formal syntax on surface, but incorporate linguistic knowledge into our models to leverage syntax theory and annotations. Our goal is two-fold. First, how to score SCFG rules whose general abstraction forms are unseen in the training data is an important question to answer. In hierarchical models, Chiang (Chiang, 2007) utilizes heuristics"
W08-0403,P03-1011,0,0.0217745,"tion. The remainder of the paper is organized as follows. We start with a brief review of some related work in Sec. 2. In Sec. 3, we describe our formally syntax-based models and decoder implementation, that is established as our baseline system. Sec. 4 presents the approach to score formal SCFG rules using kernel methods. Experimental results are provided in Sec. 5. Finally, Sec. 6 summarized our contributions with discussions and future work. 2 Related Work Syntax-based translation models engaged with SCFG have been actively investigated in the literature (Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Galley et al., 2004; Satta and Peserico, 2005). Recent work by (Chiang, 2007; Galley et al., 2006) shows promising improvements compared to phrase-based models for large-scale tasks. However, few previous work directly applied linguistically syntactic information into a formally syntaxbased models, which is explored in this paper. Kernel methods leverage the fact that the only operation in a procedure is the evaluation of inner dot products between pairs of observations, where the inner product is thus replaced with a Mercer kernel that provides an efficient way to carry out computation when"
W08-0403,N03-1017,0,0.0318403,"n source and target sides are identical). By “optimal”, it indicates that the derivation D maximizes following log-linear models over all possible derivations: P (D) ∝ PLM (e)λLM × Q Q λi i X→<γ,α>∈D φi (X →< γ, α >) , (4) where the set of φi (X →< γ, α >) are features defined over given production rule, and P LM (e) is the language model score on hypothesized output, the λi is the feature weight. Our baseline model follows Chiang’s hierarchical model (Chiang, 2007) in conjunction with additional features: • conditional probabilities in both directions: P (γ|α) and P (α|γ); • lexical weights (Koehn et al., 2003) in both directions: Pw (γ|α) and Pw (α|γ); • word counts |e|; • rule counts |D|; • target n-gram language model PLM (e); • glue rule penalty to learn preference of nonterminal rewriting over serial combination through Eq. 3; Moreover, we propose an additional feature, namely the abstraction penalty, to account for the accumulated number of nonterminals applied in D: • abstraction penalty exp(−Na ), where Na = P X→<γ,α>∈D n(γ) where n(γ) is the number of nonterminals in γ. This feature aims to learn the preference among phrasal rules, and abstract rules with one or two nonterminals. This makes"
W08-0403,P06-1077,0,0.0443672,"re derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models (Och and Ney, 2004) to handle local fluency, and the engagement of synchronous context-free grammars (SCFG), which enhances the generative capacity of the underlying model that is limited by finite-state machinery. Approaches to syntax-based translation models using SCFG can be further categorized into two classes, based on their dependency on annotated corWhil"
W08-0403,E06-1015,0,0.201305,"y, we propose a linguisticallymotivated method to train prior derivation models for formally syntax-based translation. In this framework, prior derivation models can be viewed as a smoothing of rule translation models, addressing the weakness of the baseline model estimation that relies on relative counts obtained from heuristics. First, we apply automatic parsers to obtain syntax annotations on the English side of the parallel corpus. Next, we extract tree fragments associated with phrase pairs, and measure similarity between such tree fragments using kernel methods (Collins and Duffy, 2002; Moschitti, 2006). Finally, we score 20 and rank rules based on their minimal cluster similarity of their nonterminals, which is used to compute the prior distribution of hypothesis derivations during decoding for improved translation. The remainder of the paper is organized as follows. We start with a brief review of some related work in Sec. 2. In Sec. 3, we describe our formally syntax-based models and decoder implementation, that is established as our baseline system. Sec. 4 presents the approach to score formal SCFG rules using kernel methods. Experimental results are provided in Sec. 5. Finally, Sec. 6 s"
W08-0403,P00-1056,0,0.208906,"rminal symbol X in the grammar, X → hγ, α, ∼i, (1) where ∼ is the one-to-one correspondence between X’s in γ and α, which is indicated by underscripted co-indices on both sides. For example, some English-to-Chinese production rules can be represented as follows: X → hX1 enjoy readingX2 , (2) X1 xihuan(enjoy) yuedu(reading)X2 i X → hX1 enjoy readingX2 , X1 xihuan(enjoy)X2 yuedu(reading)i The set of rules, denoted as R, are automatically extracted from sentence-aligned parallel corpus (Chiang, 2007). First, bidirectional word-level alignment is carried out on the parallel corpus running GIZA++ (Och and Ney, 2000). Based on the resulting Viterbi alignments Ae2f and Af 2e , the union, AU = Ae2f ∪ Af 2e , is taken as the symmetrized word-level alignment. Next, bilingual phrase pairs consistent with word alignments are extracted from AU (Och and Ney, 2004). Specifically, any pair of consecutive sequences of words below a maximum length M is considered to be a phrase pair if its component words are aligned only within the phrase pair and not to any words outside. The resulting bilingual phrase pair inventory is denoted as BP. Each phrase pair PP ∈ BP is represented as a production rule X → hfij , elk i, wh"
W08-0403,J04-4002,0,0.55546,"ctures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models (Och and Ney, 2004) to handle local fluency, and the engagement of synchronous context-free grammars (SCFG), which enhances the generative capacity of the underlying model that is limited by finite-state machinery. Approaches to syntax-based translation models using SCFG can be further categorized into two classes, based on their dependency on annotated corWhile these two often resemble in appearance, from practical viewpoints, there are some distinctions in training and decoding procedures differentiating formally syntax-based models from linguistically syntax-based models. First, the former has no dependency o"
W08-0403,P03-1021,0,0.0136676,"from transcription and human translation of conversations. The vocabulary size is 37K for English and 44K for Chinese after segmentation. Our evaluation data is a held out data set of 2755 sentences pairs. We extracted every one out of two sentence pairs into the dev-set, and left the remainder as the test-set. We thereby obtained a dev-set of 1378 sentence pairs, and a test-set with 1377 sentence pairs. In both cases, there are about 15K running words on English side. All Chinese sentences in training, dev and test sets are all automatically segmented into words. Minimum-error-rate training (Och, 2003) are conducted on dev-set to optimize feature weights maximizing the BLEU score up to 4grams, and the obtained feature weights are blindly applied on the test-set. To compare performances excluding tokenization effects, all BLEU scores are optimized (on dev-set) and reported (on test-set) at Chinese character-level. From training data, we extracted an initial phrase pair set with 3.7M entries for phrases up to 8 words 25 on Chinese side. We trained a 4-gram language model for Chinese at word level, which is shared by all translation systems reported in this paper, using the Chinese side of the"
W08-0403,H05-1101,0,0.0123004,"organized as follows. We start with a brief review of some related work in Sec. 2. In Sec. 3, we describe our formally syntax-based models and decoder implementation, that is established as our baseline system. Sec. 4 presents the approach to score formal SCFG rules using kernel methods. Experimental results are provided in Sec. 5. Finally, Sec. 6 summarized our contributions with discussions and future work. 2 Related Work Syntax-based translation models engaged with SCFG have been actively investigated in the literature (Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Galley et al., 2004; Satta and Peserico, 2005). Recent work by (Chiang, 2007; Galley et al., 2006) shows promising improvements compared to phrase-based models for large-scale tasks. However, few previous work directly applied linguistically syntactic information into a formally syntaxbased models, which is explored in this paper. Kernel methods leverage the fact that the only operation in a procedure is the evaluation of inner dot products between pairs of observations, where the inner product is thus replaced with a Mercer kernel that provides an efficient way to carry out computation when original feature dimension is large or even inf"
W08-0403,J97-3002,0,0.344192,"ed SMT system. • Linguistically syntax-based: models that utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank), and SCFG rules are derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models (Och and Ney, 2004) to handle local fluency, and the engagement of synchronous context-free grammars (SCFG), which enhances the generative capacity of the underlying model that is limited by f"
W08-0403,P01-1067,0,0.321147,"ing algorithm to achieve such improved models based on tree kernel methods. Experiments on an English-to-Chinese task demonstrate that our proposed models outperformed the baseline formally syntaxbased models, while both of them achieved significant improvements over a state-of-theart phrase-based SMT system. • Linguistically syntax-based: models that utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank), and SCFG rules are derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorpo"
W08-0403,N06-1033,0,0.0196192,"s in training and decoding procedures differentiating formally syntax-based models from linguistically syntax-based models. First, the former has no dependency on available linguistic theory and annotations for targeting language pairs, and thus the training and rule extraction are more efficient. Secondly, the decoding complexity of the former is lower 1 , especially when integrating a n-gram based 1 The complexity is dominated by synchronous parsing and boundary words keeping. Thus binary SCFG employed in formally syntax-based systems help to maintain efficient CKY decoding. Recent work by (Zhang et al., 2006) shows a practically efficient approach that binarizes linguistically SCFG rules when possible. 19 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 19–27, c ACL-08: HLT, Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics language model, which is a key element to ensure translation output quality. On the other hand, available linguistic theory and annotations could provide invaluable benefits in grammar induction and scoring, as shown by recent progress on such models (Galley et al., 2006). In contrast, formall"
