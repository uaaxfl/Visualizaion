2020.lrec-1.674,Cross-lingual and Cross-domain Evaluation of Machine Reading Comprehension with Squad and {CALOR}-Quest Corpora,2020,-1,-1,3,0.603605,17730,delphine charlet,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Machine Reading received recently a lot of attention thanks to both the availability of very large corpora such as SQuAD or MS MARCO containing triplets (document, question, answer), and the introduction of Transformer Language Models such as BERT which obtain excellent results, even matching human performance according to the SQuAD leaderboard. One of the key features of Transformer Models is their ability to be jointly trained across multiple languages, using a shared subword vocabulary, leading to the construction of cross-lingual lexical representations. This feature has been used recently to perform zero-shot cross-lingual experiments where a multilingual BERT model fine-tuned on a machine reading comprehension task exclusively for English was directly applied to Chinese and French documents with interesting performance. In this paper we study the cross-language and cross-domain capabilities of BERT on a Machine Reading Comprehension task on two corpora: SQuAD and a new French Machine Reading dataset, called CALOR-QUEST. The semantic annotation available on CALOR-QUEST allows us to give a detailed analysis on the kinds of questions that are properly handled through the cross-language process. We will try to answer this question: which factor between language mismatch and domain mismatch has the strongest influence on the performances of a Machine Reading Comprehension task?"
2020.jeptalnrecital-taln.28,Analyse automatique en cadres s{\\'e}mantiques pour l{'}apprentissage de mod{\\`e}les de compr{\\'e}hension de texte (Semantic Frame Parsing for training Machine Reading Comprehension models),2020,-1,-1,4,1,184,gabriel marzinotto,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"Dans le cadre de la compr{\'e}hension automatique de documents, cet article propose une {\'e}valuation intrins{\`e}que et extrins{\`e}que d{'}un mod{\`e}le d{'}analyse automatique en cadres s{\'e}mantiques (Frames). Le mod{\`e}le propos{\'e} est un mod{\`e}le {\'e}tat de l{'}art {\`a} base de GRU bi-directionnel, enrichi par l{'}utilisation d{'}embeddings contextuels. Nous montrons qu{'}un mod{\`e}le de compr{\'e}hension de documents appris sur un corpus de triplets g{\'e}n{\'e}r{\'e}s {\`a} partir d{'}un corpus analys{\'e} automatiquement avec l{'}analyseur en cadre s{\'e}mantique pr{\'e}sente des performances inf{\'e}rieures de seulement 2.5{\%} en relatif par rapport {\`a} un mod{\`e}le appris sur un corpus de triplets g{\'e}n{\'e}r{\'e}s {\`a} partir d{'}un corpus analys{\'e} manuellement."
2020.jeptalnrecital-demos.19,Analyse s{\\'e}mantique robuste par apprentissage antagoniste pour la g{\\'e}n{\\'e}ralisation de domaine (Robust Semantic Parsing with Adversarial Learning for Domain Generalization ),2020,-1,-1,3,1,184,gabriel marzinotto,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 4 : D{\\'e}monstrations et r{\\'e}sum{\\'e}s d'articles internationaux",0,"Nous pr{\'e}sentons des r{\'e}sum{\'e}s en fran{\c{c}}ais et en anglais de l{'}article (Marzinotto et al., 2019) pr{\'e}sent{\'e} {\`a} la conf{\'e}rence North American Chapter of the Association for Computational Linguistics : Human Language Technologies en 2019."
N19-2021,Robust Semantic Parsing with Adversarial Learning for Domain Generalization,2019,22,3,3,1,184,gabriel marzinotto,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Industry Papers)",0,"This paper addresses the issue of generalization for Semantic Parsing in an adversarial framework. Building models that are more robust to inter-document variability is crucial for the integration of Semantic Parsing technologies in real applications. The underlying question throughout this study is whether adversarial learning can be used to train models on a higher level of abstraction in order to increase their robustness to lexical and stylistic variations. We propose to perform Semantic Parsing with a domain classification adversarial task, covering various use-cases with or without explicit knowledge of the domain. The strategy is first evaluated on a French corpus of encyclopedic documents, annotated with FrameNet, in an information retrieval perspective. This corpus constitutes a new public benchmark, gathering documents from various thematic domains and various sources. We show that adversarial learning yields improved results when using explicit domain classification as the adversarial task. We also propose an unsupervised domain discovery approach that yields equivalent improvements. The latter is also evaluated on a PropBank Semantic Role Labeling task on the CoNLL-2005 benchmark and is shown to increase the model{'}s generalization capabilities on out-of-domain data."
D19-5803,{CALOR}-{QUEST} : generating a training corpus for Machine Reading Comprehension models from shallow semantic annotations,2019,0,0,1,1,17997,frederic bechet,Proceedings of the 2nd Workshop on Machine Reading for Question Answering,0,"Machine reading comprehension is a task related to Question-Answering where questions are not generic in scope but are related to a particular document. Recently very large corpora (SQuAD, MS MARCO) containing triplets (document, question, answer) were made available to the scientific community to develop supervised methods based on deep neural networks with promising results. These methods need very large training corpus to be efficient, however such kind of data only exists for English and Chinese at the moment. The aim of this study is the development of such resources for other languages by proposing to generate in a semi-automatic way questions from the semantic Frame analysis of large corpora. The collect of natural questions is reduced to a validation/test set. We applied this method on the CALOR-Frame French corpus to develop the CALOR-QUEST resource presented in this paper."
2019.jeptalnrecital-court.4,{CALOR}-{QUEST} : un corpus d{'}entra{\\^\\i}nement et d{'}{\\'e}valuation pour la compr{\\'e}hension automatique de textes (Machine reading comprehension is a task related to Question-Answering where questions are not generic in scope but are related to a particular document),2019,-1,-1,1,1,17997,frederic bechet,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"La compr{\'e}hension automatique de texte est une t{\^a}che faisant partie de la famille des syst{\`e}mes de Question/R{\'e}ponse o{\`u} les questions ne sont pas {\`a} port{\'e}e g{\'e}n{\'e}rale mais sont li{\'e}es {\`a} un document particulier. R{\'e}cemment de tr{\`e}s grand corpus (SQuAD, MS MARCO) contenant des triplets (document, question, r{\'e}ponse) ont {\'e}t{\'e} mis {\`a} la disposition de la communaut{\'e} scientifique afin de d{\'e}velopper des m{\'e}thodes supervis{\'e}es {\`a} base de r{\'e}seaux de neurones profonds en obtenant des r{\'e}sultats prometteurs. Ces m{\'e}thodes sont cependant tr{\`e}s gourmandes en donn{\'e}es d{'}apprentissage, donn{\'e}es qui n{'}existent pour le moment que pour la langue anglaise. Le but de cette {\'e}tude est de permettre le d{\'e}veloppement de telles ressources pour d{'}autres langues {\`a} moindre co{\^u}t en proposant une m{\'e}thode g{\'e}n{\'e}rant de mani{\`e}re semi-automatique des questions {\`a} partir d{'}une analyse s{\'e}mantique d{'}un grand corpus. La collecte de questions naturelle est r{\'e}duite {\`a} un ensemble de validation/test. L{'}application de cette m{\'e}thode sur le corpus CALOR-Frame a permis de d{\'e}velopper la ressource CALOR-QUEST pr{\'e}sent{\'e}e dans cet article."
L18-1014,Handling Normalization Issues for Part-of-Speech Tagging of Online Conversational Text,2018,0,1,6,0.458952,188,geraldine damnati,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"For the purpose of POS tagging noisy user-generated text, should normalization be handled as a preliminary task or is it possiblen to handle misspelled words directly in the POS tagging model? We propose in this paper a combined approach where some errorsn are normalized before tagging, while a Gated Recurrent Unit deep neural network based tagger handles the remaining errors. Wordn embeddings are trained on a large corpus in order to address both normalization and POS tagging. Experiments are run on Contactn Center chat conversations, a particular type of formal Computer Mediated Communication data."
L18-1159,Semantic Frame Parsing for Information Extraction : the {CALOR} corpus,2018,0,4,3,1,184,gabriel marzinotto,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"This paper presents a publicly available corpus of French encyclopedic history texts annotated according to the Berkeley FrameNet formalism. The main difference in our approach compared to previous works on semantic parsing with FrameNet is that we are not interested here in full text parsing but rather on partial parsing. The goal is to select from the FrameNet resources the minimal set of frames that are going to be useful for the applicative framework targeted, in our case Information Extraction from encyclopedic documents. Such an approach leverages the manual annotation of larger corpora than those obtained through full text parsing and therefore opens the door to alternative methods for Frame parsing than those used so far on the FrameNet 1.5 benchmark corpus. The approaches compared in this study rely on an integrated sequence labeling model which jointly optimizes frame identification and semantic role segmentation and identification. The models compared are CRFs and multitasks bi-LSTMs."
L18-1716,Adding Syntactic Annotations to Flickr30k Entities Corpus for Multimodal Ambiguous Prepositional-Phrase Attachment Resolution,2018,0,1,3,1,30320,sebastien delecraz,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,We propose in this paper to add to the captions of the Flickr30k Entities corpus some syntactic annotations in order to study the joint processing of image and language features for the Preposition-Phrase attachment disambiguation task. The annotation has been performed on the English version of the captions and automatically projected on their French and German translations.
2018.jeptalnrecital-long.13,Correction automatique d{'}attachements pr{\\'e}positionnels par utilisation de traits visuels ({PP}-attachement resolution using visual features),2018,-1,-1,5,1,30320,sebastien delecraz,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"La d{\'e}sambigu{\""\i}sation des rattachements pr{\'e}positionnels est une t{\^a}che syntaxique qui demande des connaissances s{\'e}mantiques, pouvant {\^e}tre extraites d{'}une image associ{\'e}e au texte trait{\'e}. Nous pr{\'e}sentons et analysons les difficult{\'e}s de cette t{\^a}che pour laquelle nous construisons un syst{\`e}me complet entra{\^\i}n{\'e} sur une version {\'e}tendue des annotations du corpus Flickr30k Entities. Lorsque la s{\'e}mantique lexicale n{'}est pas disponible, l{'}information visuelle apporte 3 {\%} d{'}am{\'e}lioration."
2018.jeptalnrecital-court.4,Evaluation automatique de la satisfaction client {\\`a} partir de conversations de type {``}chat{''} par r{\\'e}seaux de neurones r{\\'e}currents avec m{\\'e}canisme d{'}attention (Customer satisfaction prediction with attention-based {RNN}s from a chat contact center corpus),2018,-1,-1,5,0,27341,jeremy auguste,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,Cet article pr{\'e}sente des m{\'e}thodes permettant l{'}{\'e}valuation de la satisfaction client {\`a} partir de tr{\`e}s vastes corpus de conversation de type {``}chat{''} entre des clients et des op{\'e}rateurs. Extraire des connaissances dans ce contexte demeure un d{\'e}fi pour les m{\'e}thodes de traitement automatique des langues de par la dimension interactive et les propri{\'e}t{\'e}s de ce nouveau type de langage {\`a} l{'}intersection du langage {\'e}crit et parl{\'e}. Nous pr{\'e}sentons une {\'e}tude utilisant des r{\'e}ponses {\`a} des sondages utilisateurs comme supervision faible permettant de pr{\'e}dire la satisfaction des usagers d{'}un service en ligne d{'}assistance technique et commerciale.
2018.jeptalnrecital-court.5,D{\\'e}tection d{'}erreurs dans des transcriptions {OCR} de documents historiques par r{\\'e}seaux de neurones r{\\'e}currents multi-niveau (Combining character level and word level {RNN}s for post-{OCR} error detection),2018,-1,-1,2,0,30983,thibault magallon,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"Le traitement {\`a} posteriori de transcriptions OCR cherche {\`a} d{\'e}tecter les erreurs dans les sorties d{'}OCR pour tenter de les corriger, deux t{\^a}ches {\'e}valu{\'e}es par la comp{\'e}tition ICDAR-2017 Post-OCR Text Correction. Nous pr{\'e}senterons dans ce papier un syst{\`e}me de d{\'e}tection d{'}erreurs bas{\'e} sur un mod{\`e}le {\`a} r{\'e}seaux r{\'e}currents combinant une analyse du texte au niveau des mots et des caract{\`e}res en deux temps. Ce syst{\`e}me a {\'e}t{\'e} class{\'e} second dans trois cat{\'e}gories {\'e}valu{\'e}es parmi 11 candidats lors de la comp{\'e}tition."
W17-6311,Correcting prepositional phrase attachments using multimodal corpora,2017,7,1,3,1,30320,sebastien delecraz,Proceedings of the 15th International Conference on Parsing Technologies,0,"PP-attachments are an important source of errors in parsing natural language. We propose in this article to use data coming from a multimodal corpus, combining textual, visual and conceptual information, as well as a correction strategy, to propose alternative attachments in the output of a parser."
2017.jeptalnrecital-demo.5,Apprentissage d{'}agents conversationnels pour la gestion de relations clients (Training chatbots for customer relation management),2017,-1,-1,2,0.473141,14954,benoit favre,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 3 - D{\\'e}monstrations,0,"Ce travail d{\'e}montre la faisabilit{\'e} d{'}entra{\^\i}ner des chatbots sur des traces de conversations dans le domaine de la relation client. Des syst{\`e}mes {\`a} base de mod{\`e}les de langage, de recherche d{'}information et de traduction sont compar{\'e}s pour la t{\^a}che."
2017.jeptalnrecital-court.6,Analyse automatique {F}rame{N}et : une {\\'e}tude sur un corpus fran{\\c{c}}ais de textes encyclop{\\'e}diques ({F}rame{N}et automatic analysis : a study on a {F}rench corpus of encyclopedic texts),2017,-1,-1,3,1,184,gabriel marzinotto,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 2 - Articles courts,0,"Cet article pr{\'e}sente un syst{\`e}me d{'}analyse automatique en cadres s{\'e}mantiques {\'e}valu{\'e} sur un corpus de textes encyclop{\'e}diques d{'}histoire annot{\'e}s selon le formalisme FrameNet. L{'}approche choisie repose sur un mod{\`e}le int{\'e}gr{\'e} d{'}{\'e}tiquetage de s{\'e}quence qui optimise conjointement l{'}identification des cadres, la segmentation et l{'}identification des r{\^o}les s{\'e}mantiques associ{\'e}s. Nous cherchons dans cette {\'e}tude {\`a} analyser la complexit{\'e} de la t{\^a}che selon plusieurs dimensions. Une analyse d{\'e}taill{\'e}e des performances du syst{\`e}me est ainsi propos{\'e}e, {\`a} la fois selon l{'}angle des param{\`e}tres du mod{\`e}le et de la nature des donn{\'e}es."
W16-3621,Syntactic parsing of chat language in contact center conversation corpus,2016,12,3,4,0,5812,alexis nasr,Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Chat language is often referred to as Computer-mediated communication (CMC). Most of the previous studies on chat language has been dedicated to collecting  chat room  data as it is the kind of data which is the most accessible on the WEB. This kind of data falls under the informal register whereas we are interested in this paper in understanding the mechanisms of a more formal kind of CMC: dialog chat in contact centers. The particularities of this type of dialogs and the type of language used by customers and agents is the focus of this paper towards understanding this new kind of CMC data. The challenges for processing chat data comes from the fact that Natural Language Processing tools such as syntactic parsers and part of speech taggers are typically trained on mismatched conditions, we describe in this study the impact of such a mismatch for a syntactic parsing task."
L16-1166,Enhancing The {RATP}-{DECODA} Corpus With Linguistic Annotations For Performing A Large Range Of {NLP} Tasks,2016,9,3,3,0,18649,carole lailler,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this article, we present the RATP-DECODA Corpus which is composed by a set of 67 hours of speech from telephone conversations of a Customer Care Service (CCS). This corpus is already available on line at http://sldr.org/sldr000847/fr in its first version. However, many enhancements have been made in order to allow the development of automatic techniques to transcript conversations and to capture their meaning. These enhancements fall into two categories: firstly, we have increased the size of the corpus with manual transcriptions from a new operational day; secondly we have added new linguistic annotations to the whole corpus (either manually or through an automatic processing) in order to perform various linguistic tasks from syntactic and semantic parsing to dialog act tagging and dialog summarization."
L16-1701,Summarizing Behaviours: An Experiment on the Annotation of Call-Centre Conversations,2016,0,1,5,0,2753,morena danieli,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Annotating and predicting behavioural aspects in conversations is becoming critical in the conversational analytics industry. In this paper we look into inter-annotator agreement of agent behaviour dimensions on two call center corpora. We find that the task can be annotated consistently over time, but that subjectivity issues impacts the quality of the annotation. The reformulation of some of the annotated dimensions is suggested in order to improve agreement."
2016.jeptalnrecital-long.5,D{\\'e}tection de concepts pertinents pour le r{\\'e}sum{\\'e} automatique de conversations par recombinaison de patrons (Relevant concepts detection for the automatic summary of conversations using patterns recombination ),2016,-1,-1,3,1,35943,jeremy trione,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"automatique de conversations par recombinaison de patrons J{\'e}r{\'e}my Trione Benoit Favre Fr{\'e}d{\'e}ric B{\'e}chet Aix-Marseille Universit{\'e}, CNRS, LIF UMR 7279, 13000, Marseille, France pr{\'e}nom.nom@lif.univ-mrs.fr R {\'E}SUM{\'E} Ce papier d{\'e}crit une approche pour cr{\'e}er des r{\'e}sum{\'e}s de conversations parl{\'e}es par remplissage de patrons. Les patrons sont g{\'e}n{\'e}r{\'e}s automatiquement {\`a} partir de fragments g{\'e}n{\'e}ralis{\'e}s depuis un corpus de r{\'e}sum{\'e}s d{'}apprentissage. Les informations n{\'e}cessaires pour remplir les patrons sont d{\'e}tect{\'e}es dans les transcriptions des conversations et utilis{\'e}es pour s{\'e}lectionner les fragments candidats. L{'}approche obtient un score ROUGE-2 de 0.116 sur le corpus RATP-DECODA. Les r{\'e}sultats obtenus montrent que cette approche abstractive est plus performante que les approches extractives utilis{\'e}es habituellement dans le domaine du r{\'e}sum{\'e} automatique."
2016.jeptalnrecital-jep.41,Fusion d{'}espaces de repr{\\'e}sentations multimodaux pour la reconnaissance du r{\\^o}le du locuteur dans des documents t{\\'e}l{\\'e}visuels (Multimodal embedding fusion for robust speaker role recognition in video broadcast ),2016,-1,-1,2,1,30320,sebastien delecraz,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 1 : JEP,0,"L{'}identification du r{\^o}le d{'}un locuteur dans des {\'e}missions de t{\'e}l{\'e}vision est un probl{\`e}me de classification de personne selon une liste de r{\^o}les comme pr{\'e}sentateur, journaliste, invit{\'e}, etc. {\`A} cause de la nonsynchronie entre les modalit{\'e}s, ainsi que par le manque de corpus de vid{\'e}os annot{\'e}es dans toutes les modalit{\'e}s, seulement une des modalit{\'e}s est souvent utilis{\'e}e. Nous pr{\'e}sentons dans cet article une fusion multimodale des espaces de repr{\'e}sentations de l{'}audio, du texte et de l{'}image pour la reconnaissance du r{\^o}le du locuteur pour des donn{\'e}es asynchrones. Les espaces de repr{\'e}sentations monomodaux sont entra{\^\i}n{\'e}s sur des corpus de donn{\'e}es exog{\`e}nes puis ajust{\'e}s en utilisant des r{\'e}seaux de neurones profonds sur un corpus d{'}{\'e}missions fran{\c{c}}aises pour notre t{\^a}che de classification. Les exp{\'e}riences r{\'e}alis{\'e}es sur le corpus de donn{\'e}es REPERE ont mis en {\'e}vidence les gains d{'}une fusion au niveau des espaces de repr{\'e}sentations par rapport aux m{\'e}thodes de fusion tardive standard."
W15-4633,Call Centre Conversation Summarization: A Pilot Task at Multiling 2015,2015,11,5,4,0.482035,14954,benoit favre,Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"This paper describes the results of the Call Centre Conversation Summarization task at Multilingxe2x80x9915. The CCCS task consists in generating abstractive synopses from call centre conversations between a caller and an agent. Synopses are summaries of the problem of the caller, and how it is solved by the agent. Generating them is a very challenging task given that deep analysis of the dialogs and text generation are necessary. Three languages were addressed: French, Italian and English translations of conversations from those two languages. The official evaluation metric was ROUGE-2. Two participants submitted a total of four systems which had trouble beating the extractive baselines. The datasets released for the task will allow more research on abstractive dialog summarization."
W15-0212,Rapid {F}rame{N}et annotation of spoken conversation transcripts,2015,8,3,2,1,35943,jeremy trione,Proceedings of the 11th Joint {ACL}-{ISO} Workshop on Interoperable Semantic Annotation ({ISA}-11),0,None
S15-2095,{L}sislif: Feature Extraction and Label Weighting for Sentiment Analysis in {T}witter,2015,16,23,3,1,32337,hussam hamdan,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes our sentiment analysis systems which have been built for SemEval2015 Task 10 Subtask B and E. For subtask B, a Logistic Regression classifier has been trained after extracting several groups of features including lexical, syntactic, lexiconbased, Z score and semantic features. A weighting schema has been adapted for positive and negative labels in order to take into account the unbalanced distribution of tweets between the positive and negative classes. This system is ranked third over 40 participants, it achieves average F1 64.27 on Twitter data set 2015 just 0.57% less than the first system. We also present our participation in Subtask E in which our system has got the second rank with Kendall metric but the first one with Spearman for ranking twitter terms according to their association with the positive sentiment."
S15-2128,{L}sislif: {CRF} and Logistic Regression for Opinion Target Extraction and Sentiment Polarity Analysis,2015,19,20,3,1,32337,hussam hamdan,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"This paper describes our contribution in Opinion Target Extraction OTE and Sentiment Polarity sub tasks of SemEval 2015 ABSA task. A CRF model with IOB notation has been adopted for OTE with several groups of features including syntactic, lexical, semantic, sentiment lexicon features. Our submission for OTE is ranked fifth over twenty submissions. A Logistic Regression model with a weighting schema of positive and negative labels have been used for sentiment polarity; several groups of features (lexical, syntactic, semantic, lexicon and Z score) are extracted. Our submission for Sentiment Polarity is ranked third over ten submissions on the restaurant data set, third over thirteen on the laptops data set, but the first over eleven on the hotel data set that is out-of-domain set."
S14-2104,Supervised Methods for Aspect-Based Sentiment Analysis,2014,15,4,3,1,32337,hussam hamdan,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"In this paper, we present our contribution in SemEval2014 ABSA task, some supervised methods for Aspect-Based Sentiment Analysis of restaurant and laptop reviews are proposed, implemented and evaluated. We focus on determining the aspect terms existing in each sentence, finding out their polarities, detecting the categories of the sentence and the polarity of each category. The evaluation results of our proposed methods exhibit a significant improvement in terms of accuracy and f-measure over all four subtasks regarding to the baseline proposed by SemEval organisers."
S14-2113,The Impact of Z{\\_}score on {T}witter Sentiment Analysis,2014,22,3,3,1,32337,hussam hamdan,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"Twitter has become more and more an important resource of user-generated data. Sentiment Analysis in Twitter is interesting for many applications and objectives. In this paper, we propose to exploit some features which can be useful for this task; the main contribution is the use of Z-scores as features for sentiment classification in addition to pre-polarity and POS tags features. Our experiments have been evaluated using the test data provided by SemEval 2013 and 2014. The evaluation demonstrates that Z_scores features can significantly improve the prediction performance."
nasr-etal-2014-automatically,Automatically enriching spoken corpora with syntactic information for linguistic studies,2014,7,7,2,0,5812,alexis nasr,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,Syntactic parsing of speech transcriptions faces the problem of the presence of disfluencies that break the syntactic structure of the utterances. We propose in this paper two solutions to this problem. The first one relies on a disfluencies predictor that detects disfluencies and removes them prior to parsing. The second one integrates the disfluencies in the syntactic structure of the utterances and train a disfluencies aware parser.
benkoussas-etal-2014-collection,A Collection of Scholarly Book Reviews from the Platforms of electronic sources in Humanities and Social Sciences {O}pen{E}dition.org,2014,10,0,4,0,39895,chahinez benkoussas,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we present our contribution for the automatic construction of the Scholarly Book Reviews corpora from two different sources, the OpenEdition platform which is dedicated to electronic resources in the humanities and social sciences, and the Web. The main target is the collect of reviews in order to provide automatic links between each review and its potential book in the future. For these purposes, we propose different document representations and we apply some supervised approaches for binary genre classification before evaluating their impact."
F14-2021,Impact of the nature and size of the training set on performance in the automatic detection of named entities (Impact de la nature et de la taille des corpus d{'}apprentissage sur les performances dans la d{\\'e}tection automatique des entit{\\'e}s nomm{\\'e}es) [in {F}rench],2014,0,0,4,0,40012,anais ollagnier,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
S13-2075,"Experiments with {DB}pedia, {W}ord{N}et and {S}enti{W}ord{N}et as resources for sentiment analysis in micro-blogging",2013,19,25,2,1,32337,hussam hamdan,"Second Joint Conference on Lexical and Computational Semantics (*{SEM}), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation ({S}em{E}val 2013)",0,"Sentiment Analysis in Twitter has become an important task due to the huge user-generated content published over such media. Such analysis could be useful for many domains such as Marketing, Finance, Politics, and Social. We propose to use many features in order to improve a trained classifier of Twitter messages; these features extend the feature vector of uni-gram model by the concepts extracted from DBpedia, the verb groups and the similar adjectives extracted from WordNet, the Sentifeatures extracted using SentiWordNet and some useful domain specific features. We also built a dictionary for emotion icons, abbreviation and slang words in tweets which is useful before extending the tweets with different features. Adding these features has improved the f-measure accuracy 2% with SVM and 4% with NaiveBayes."
W12-0508,A Joint Named Entity Recognition and Entity Linking System,2012,16,6,3,0,42263,rosa stern,Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data,0,"We present a joint system for named entity recognition (NER) and entity linking (EL), allowing for named entities mentions extracted from textual data to be matched to uniquely identifiable entities. Our approach relies on combined NER modules which transfer the disambiguation step to the EL component, where referential knowledge about entities can be used to select a correct entity reading. Hybridation is a main feature of our system, as we have performed experiments combining two types of NER, based respectively on symbolic and statistical techniques. Furthermore, the statistical EL module relies on entity knowledge acquired over a large news corpus using a simple rule-base disambiguation tool. An implementation of our system is described, along with experiments and evaluation results on French news wires. Linking accuracy reaches up to 87%, and the NER F-score up to 83%."
bazillon-etal-2012-syntactic,Syntactic annotation of spontaneous speech: application to call-center conversation data,2012,10,8,3,1,39875,thierry bazillon,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper describes the syntactic annotation process of the DECODA corpus. This corpus contains manual transcriptions of spoken conversations recorded in the French call-center of the Paris Public Transport Authority (RATP). Three levels of syntactic annotation have been performed with a semi-supervised approach: POS tags, Syntactic Chunks and Dependency parses. The main idea is to use off-the-shelf NLP tools and models, originaly developped and trained on written text, to perform a first automatic annotation on the manually transcribed corpus. At the same time a fully manual annotation process is performed on a subset of the original corpus, called the GOLD corpus. An iterative process is then applied, consisting in manually correcting errors found in the automatic annotations, retraining the linguistic models of the NLP tools on this corrected corpus, then checking the quality of the adapted models on the fully manual annotations of the GOLD corpus. This process iterates until a certain error rate is reached. This paper describes this process, the main issues raising when adapting NLP tools to process speech transcriptions, and presents the first evaluations performed with these new adapted tools."
bechet-etal-2012-decoda,{DECODA}: a call-centre human-human spoken conversation corpus,2012,12,35,1,1,17997,frederic bechet,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The goal of the DECODA project is to reduce the development cost of Speech Analytics systems by reducing the need for manual annotat ion. This project aims to propose robust speech data mining tools in the framework of call-center monitoring and evaluation, by means of weakl y supervised methods. The applicative framework of the project is the call-center of the RATP (Paris public transport authority). This project tackles two very important open issues in the development of speech mining methods from spontaneous speech recorded in call-centers : robus tness (how to extract relevant information from very noisy and spontaneous speech messages) and weak supervision (how to reduce the annotation effort needed to train and adapt recognition and classification models). This paper describes the DECODA corpus collected at the RATP during the project. We present the different annotation levels performed on the corpus, the methods used to obtain them, as well as some evaluation o f the quality of the annotations produced."
F12-1070,Percol0 - un syst{\\`e}me multimodal de d{\\'e}tection de personnes dans des documents vid{\\'e}o (Percol0 - A multimodal person detection system in video documents) [in {F}rench],2012,-1,-1,1,1,17997,frederic bechet,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 1: JEP",0,None
P11-4015,{MACAON} An {NLP} Tool Suite for Processing Word Lattices,2011,17,30,2,0.64122,5812,alexis nasr,Proceedings of the {ACL}-{HLT} 2011 System Demonstrations,0,"MACAON is a tool suite for standard NLP tasks developed for French. MACAON has been designed to process both human-produced text and highly ambiguous word-lattices produced by NLP tools. MACAON is made of several native modules for common tasks such as a tokenization, a part-of-speech tagging or syntactic parsing, all communicating with each other through XML files. In addition, exchange protocols with external tools are easily definable. MACAON is a fast, modular and open tool, distributed under GNU Public License."
2011.jeptalnrecital-long.8,Qui {\\^e}tes-vous ? Cat{\\'e}goriser les questions pour d{\\'e}terminer le r{\\^o}le des locuteurs dans des conversations orales (Who are you? Categorize questions to determine the role of speakers in oral conversations),2011,-1,-1,4,1,39875,thierry bazillon,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"La fouille de donn{\'e}es orales est un domaine de recherche visant {\`a} caract{\'e}riser un flux audio contenant de la parole d{'}un ou plusieurs locuteurs, {\`a} l{'}aide de descripteurs li{\'e}s {\`a} la forme et au contenu du signal. Outre la transcription automatique en mots des paroles prononc{\'e}es, des informations sur le type de flux audio trait{\'e} ainsi que sur le r{\^o}le et l{'}identit{\'e} des locuteurs sont {\'e}galement cruciales pour permettre des requ{\^e}tes complexes telles que : Â« chercher des d{\'e}bats sur le th{\`e}me X Â», Â« trouver toutes les interviews de Y Â», etc. Dans ce cadre, et en traitant des conversations enregistr{\'e}es lors d{'}{\'e}missions de radio ou de t{\'e}l{\'e}vision, nous {\'e}tudions la mani{\`e}re dont les locuteurs expriment des questions dans les conversations, en partant de l{'}intuition initiale que la forme des questions pos{\'e}es est une signature du r{\^o}le du locuteur dans la conversation (pr{\'e}sentateur, invit{\'e}, auditeur, etc.). En proposant une classification du type des questions et en utilisant ces informations en compl{\'e}ment des descripteurs g{\'e}n{\'e}ralement utilis{\'e}s dans la litt{\'e}rature pour classer les locuteurs par r{\^o}le, nous esp{\'e}rons am{\'e}liorer l{'}{\'e}tape de classification, et valider par la m{\^e}me occasion notre intuition initiale."
2011.jeptalnrecital-court.4,Coop{\\'e}ration de m{\\'e}thodes statistiques et symboliques pour l{'}adaptation non-supervis{\\'e}e d{'}un syst{\\`e}me d{'}{\\'e}tiquetage en entit{\\'e}s nomm{\\'e}es (Statistical and symbolic methods cooperation for the unsupervised adaptation of a named entity recognition system),2011,-1,-1,1,1,17997,frederic bechet,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"La d{\'e}tection et le typage des entit{\'e}s nomm{\'e}es sont des t{\^a}ches pour lesquelles ont {\'e}t{\'e} d{\'e}velopp{\'e}s {\`a} la fois des syst{\`e}mes symboliques et probabilistes. Nous pr{\'e}sentons les r{\'e}sultats d{'}une exp{\'e}rience visant {\`a} faire interagir le syst{\`e}me {\`a} base de r{\`e}gles NP, d{\'e}velopp{\'e} sur des corpus provenant de l{'}AFP, int{\'e}grant la base d{'}entit{\'e}s Aleda et qui a une bonne pr{\'e}cision, et le syst{\`e}me LIANE, entra{\^\i}n{\'e} sur des transcriptions de l{'}oral provenant du corpus ESTER et qui a un bon rappel. Nous montrons qu{'}on peut adapter {\`a} un nouveau type de corpus, de mani{\`e}re non supervis{\'e}e, un syst{\`e}me probabiliste tel que LIANE gr{\^a}ce {\`a} des corpus volumineux annot{\'e}s automatiquement par NP. Cette adaptation ne n{\'e}cessite aucune annotation manuelle suppl{\'e}mentaire et illustre la compl{\'e}mentarit{\'e} des m{\'e}thodes num{\'e}riques et symboliques pour la r{\'e}solution de t{\^a}ches linguistiques."
esteve-etal-2010-epac,The {EPAC} Corpus: Manual and Automatic Annotations of Conversational Speech in {F}rench Broadcast News,2010,10,39,4,0,5785,yannick esteve,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper presents the EPAC corpus which is composed by a set of 100 hours of conversational speech manually transcribed and by the outputs of automatic tools (automatic segmentation, transcription, POS tagging, etc.) applied on the entire French ESTER 1 audio corpus: this concerns about 1700 hours of audio recordings from radiophonic shows. This corpus was built during the EPAC project funded by the French Research Agency (ANR) from 2007 to 2010. This corpus increases significantly the amount of French manually transcribed audio recordings easily available and it is now included as a part of the ESTER 1 corpus in the ELRA catalog without additional cost. By providing a large set of automatic outputs of speech processing tools, the EPAC corpus should be useful to researchers who want to work on such data without having to develop and deal with such tools. These automatic annotations are various: segmentation and speaker diarization, one-best hypotheses from the LIUM automatic speech recognition system with confidence measures, but also word-lattices and confusion networks, named entities, part-of-speech tags, chunks, etc. The 100 hours of speech manually transcribed were split into three data sets in order to get an official training corpus, an official development corpus and an official test corpus. These data sets were used to develop and to evaluate some automatic tools which have been used to process the 1700 hours of audio recording. For example, on the EPAC test data set our ASR system yields a word error rate equals to 17.25{\%}."
2010.jeptalnrecital-demonstration.16,{MACAON} Une cha{\\^\\i}ne linguistique pour le traitement de graphes de mots,2010,-1,-1,2,0.837448,5812,alexis nasr,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. D{\\'e}monstrations,0,
2009.jeptalnrecital-long.3,Analyse syntaxique en d{\\'e}pendances de l{'}oral spontan{\\'e},2009,-1,-1,2,0.837448,5812,alexis nasr,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article d{\'e}crit un mod{\`e}le d{'}analyse syntaxique de l{'}oral spontan{\'e} ax{\'e} sur la reconnaissance de cadres valenciels verbaux. Le mod{\`e}le d{'}analyse se d{\'e}compose en deux {\'e}tapes : une {\'e}tape g{\'e}n{\'e}rique, bas{\'e}e sur des ressources g{\'e}n{\'e}riques du fran{\c{c}}ais et une {\'e}tape de r{\'e}ordonnancement des solutions de l{'}analyseur r{\'e}alis{\'e} par un mod{\`e}le sp{\'e}cifique {\`a} une application. Le mod{\`e}le est {\'e}valu{\'e} sur le corpus MEDIA."
oger-etal-2008-local,Local Methods for On-Demand Out-of-Vocabulary Word Retrieval,2008,10,3,3,0,45933,stanislas oger,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Most of the Web-based methods for lexicon augmenting consist in capturing global semantic features of the targeted domain in order to collect relevant documents from the Web. We suggest that the local context of the out-of-vocabulary (OOV) words contains relevant information on the OOV words. With this information, we propose to use the Web to build locally-augmented lexicons which are used in a final local decoding pass. First, an automatic web based OOV word detection method is proposed. Then, we demonstrate the relevance of the Web for the OOV word retrieval. Different methods are proposed to retrieve the hypothesis words. We finally retrieve about 26{\%} of the OOV words with a lexicon increase of less than 1000 words using the reference context."
meurs-etal-2008-semantic,Semantic Frame Annotation on the {F}rench {MEDIA} corpus,2008,6,7,3,0,17834,mariejean meurs,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper introduces a knowledge representation formalism used for annotation of the French MEDIA dialogue corpus in terms of high level semantic structures. The semantic annotation, worked out according to the Berkeley FrameNet paradigm, is incremental and partially automated. We describe an automatic interpretation process for composing semantic structures from basic semantic constituents using patterns involving words and constituents. This process contains procedures which provide semantic compositions and generating frame hypotheses by inference. The MEDIA corpus is a French dialogue corpus recorded using a Wizard of Oz system simulating a telephone server for tourist information and hotel booking. It had been manually transcribed and annotated at the word and semantic constituent levels. These levels support the automatic interpretation process which provides a high level semantic frame annotation. The Frame based Knowledge Source we composed contains Frame definitions and composition rules. We finally provide some results obtained on the automatically-derived annotation."
2008.jeptalnrecital-court.21,Annotation en Frames S{\\'e}mantiques du corpus de dialogue {MEDIA},2008,-1,-1,3,0,17834,mariejean meurs,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Cet article pr{\'e}sente un formalisme de repr{\'e}sentation des connaissances qui a {\'e}t{\'e} utilis{\'e} pour fournir des annotations s{\'e}mantiques de haut niveau pour le corpus de dialogue oral MEDIA. Ces annotations en structures s{\'e}mantiques, bas{\'e}es sur le paradigme FrameNet, sont obtenues de mani{\`e}re incr{\'e}mentale et partiellement automatis{\'e}e. Nous d{\'e}crivons le processus d{'}interpr{\'e}tation automatique qui permet d{'}obtenir des compositions s{\'e}mantiques et de g{\'e}n{\'e}rer des hypoth{\`e}ses de frames par inf{\'e}rence. Le corpus MEDIA est un corpus de dialogues en langue fran{\c{c}}aise dont les tours de parole de l{'}utilisateur ont {\'e}t{\'e} manuellement transcrits et annot{\'e}s (niveaux mots et constituants s{\'e}mantiques de base). Le processus propos{\'e} utilise ces niveaux pour produire une annotation de haut niveau en frames s{\'e}mantiques. La base de connaissances d{\'e}velopp{\'e}e (d{\'e}finitions des frames et r{\`e}gles de composition) est pr{\'e}sent{\'e}e, ainsi que les r{\'e}sultats de l{'}annotation automatique."
W07-0307,Experiments on the {F}rance Telecom 3000 Voice Agency corpus: academic research on an industrial spoken dialog system,2007,11,1,2,0.344471,188,geraldine damnati,Proceedings of the Workshop on Bridging the Gap: Academic and Industrial Research in Dialog Technologies,0,"The recent advances in speech recognition technologies, and the experience acquired in the development of WEB or Interactive Voice Response interfaces, have facilitated the integration of speech modules in robust Spoken Dialog Systems (SDS), leading to the deployment on a large scale of speech-enabled services. With these services it is possible to obtain very large corpora of human-machine interactions by collecting system logs. This new kinds of systems and dialogue corpora offer new opportunities for academic research while raising two issues: How can academic research take profit of the system logs of deployed SDS in order to build the next generation of SDS, although the dialogues collected have a dialogue flow constrained by the previous SDS generation? On the other side, what immediate benefits can academic research offer for the improvement of deployed system? This paper addresses these aspects in the framework of the deployed France Telecom 3000 Voice Agency service."
2007.jeptalnrecital-poster.6,Analyse automatique de sondages t{\\'e}l{\\'e}phoniques d{'}opinion,2007,-1,-1,2,0,17732,nathalie camelin,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Cette {\'e}tude pr{\'e}sente la probl{\'e}matique de l{'}analyse automatique de sondages t{\'e}l{\'e}phoniques d{'}opinion. Cette analyse se fait en deux {\'e}tapes : tout d{'}abord extraire des messages oraux les expressions subjectives relatives aux opinions de utilisateurs sur une dimension particuli{\`e}re (efficacit{\'e}, accueil, etc.) ; puis s{\'e}lectionner les messages fiables, selon un ensemble de mesures de confiance, et estimer la distribution des diverses opinions sur le corpus de test. Le but est d{'}estimer une distribution aussi proche que possible de la distribution de r{\'e}f{\'e}rence. Cette {\'e}tude est men{\'e}e sur un corpus de messages provenant de vrais utilisateurs fournis par France T{\'e}l{\'e}com R{\&}D."
2007.jeptalnrecital-long.24,R{\\'e}solution de la r{\\'e}f{\\'e}rence dans des dialogues homme-machine : {\\'e}valuation sur corpus de deux approches symbolique et probabiliste,2007,-1,-1,2,0,39028,alexandre denis,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article d{\'e}crit deux approches, l{'}une num{\'e}rique, l{'}autre symbolique, traitant le probl{\`e}me de la r{\'e}solution de la r{\'e}f{\'e}rence dans un cadre de dialogue homme-machine. L{'}analyse des r{\'e}sultats obtenus sur le corpus MEDIA montre la compl{\'e}mentarit{\'e} des deux syst{\`e}mes d{\'e}velopp{\'e}s : robustesse aux erreurs et hypoth{\`e}ses multiples pour l{'}approche num{\'e}rique ; mod{\'e}lisation de ph{\'e}nom{\`e}nes complexes et interpr{\'e}tation compl{\`e}te pour l{'}approche symbolique."
2007.iwslt-1.22,{MISTRAL}: a lattice translation system for {IWSLT} 2007,2007,17,4,3,0,44357,alexandre patry,Proceedings of the Fourth International Workshop on Spoken Language Translation,0,"This paper describes MISTRAL, the lattice translation system that we developed for the Italian-English track of the International Workshop on Spoken Language Translation 2007. MISTRAL is a discriminative phrase-based system that translates a source word lattice in two passes. The first pass extracts a list of top ranked sentence pairs from the lattice and the second pass rescores this list with more complex features. Our experiments show that our system, when translating pruned lattices, is at least as good as a fair baseline that translates the first ranked sentences returned by a speech recognition system."
2006.jeptalnrecital-long.30,D{\\'e}codage conceptuel et apprentissage automatique : application au corpus de dialogue Homme-Machine {MEDIA},2006,9,3,2,0,5281,christophe servan,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cette {\'e}tude pr{\'e}sente les travaux du LIA effectu{\'e}s sur le corpus de dialogue homme-machine MEDIA et visant {\`a} proposer des m{\'e}thodes d{'}analyse robuste permettant d{'}extraire d{'}un message audio une s{\'e}quence de concepts {\'e}l{\'e}mentaires. Le mod{\`e}le de d{\'e}codage conceptuel pr{\'e}sent{\'e} est bas{\'e} sur une approche stochastique qui int{\`e}gre directement le processus de compr{\'e}hension au processus de Reconnaissance Automatique de la Parole (RAP). Cette approche permet de garder l{'}espace probabiliste des phrases produit en sortie du module de RAP et de le projeter vers un espace probabiliste de s{\'e}quences de concepts. Les exp{\'e}riences men{\'e}es sur le corpus MEDIA montrent que les performances atteintes par notre mod{\`e}le sont au niveau des meilleurs syst{\`e}mes ayant particip{\'e} {\`a} l{'}{\'e}valuation sur des transcriptions manuelles de dialogues. En d{\'e}taillant les performances du syst{\`e}me en fonction de la taille du corpus d{'}apprentissage on peut mesurer le nombre minimal ainsi que le nombre optimal de dialogues n{\'e}cessaires {\`a} l{'}apprentissage des mod{\`e}les. Enfin nous montrons comment des connaissances a priori peuvent {\^e}tre int{\'e}gr{\'e}es dans nos mod{\`e}les afin d{'}augmenter significativement leur couverture en diminuant, {\`a} performance {\'e}gale, l{'}effort de constitution et d{'}annotation du corpus d{'}apprentissage."
H05-1062,Robust Named Entity Extraction from Large Spoken Archives,2005,15,37,2,0.41714,14954,benoit favre,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Traditional approaches to Information Extraction (IE) from speech input simply consist in applying text based methods to the output of an Automatic Speech Recognition (ASR) system. If it gives satisfaction with low Word Error Rate (WER) transcripts, we believe that a tighter integration of the IE and ASR modules can increase the IE performance in more difficult conditions. More specifically this paper focuses on the robust extraction of Named Entities from speech input where a temporal mismatch between training and test corpora occurs. We describe a Named Entity Recognition (NER) system, developed within the French Rich Broadcast News Transcription program ESTER, which is specifically optimized to process ASR transcripts and can be integrated into the search process of the ASR modules. Finally we show how some metadata information can be collected in order to adapt NER and ASR models to new conditions and how they can be used in a task of Named Entity indexation of spoken archives."
W04-3218,Mining Spoken Dialogue Corpora for System Evaluation and Modelin,2004,1,19,1,1,17997,frederic bechet,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,None
W04-2321,On the Use of Confidence for Statistical Decision in Dialogue Strategies,2004,9,8,2,0,27857,christian raymond,Proceedings of the 5th {SIG}dial Workshop on Discourse and Dialogue at {HLT}-{NAACL} 2004,0,"This paper describes an interpretation and decision strategy that minimizes interpretation errors and perform dialogue actions which may not depend on the hypothesized concepts only, but also on confidence of what has been recognized. The concepts introduced here are applied in a system which integrates language and interpretation models into Stochastic Finite State Transducers (SFST). Furthermore, acoustic, linguistic and semantic confidence measures on the hypothesized word sequences are made available to the dialogue strategy. By evaluating predicates related to these confidence measures, a decision tree automatically learn a decision strategy for rescoring a n-best list of candidates representing a userxe2x80x99s utterance. The different actions that can be then performed are chosen according to the confidence scores given by the tree."
devillers-etal-2004-french,The {F}rench {MEDIA}/{EVALDA} Project: the Evaluation of the Understanding Capability of Spoken Language Dialogue Systems,2004,9,30,11,0,39952,laurence devillers,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,The aim of the MEDIA project is to design and test a methodology for the evaluat ion of context-dependent and independent spoken dialogue systems. We propose an evaluation paradigm based on the use of test suites from real-world corpora and a common semantic representation and common metrics. This paradigm should allow us to diagnose the context-sensitive understanding capability of dialogue system s. This paradigm will be used within an evaluation campaign involving several si tes all of which will carry out the task of querying information from a database .
C04-1082,Tagging with Hidden {M}arkov Models Using Ambiguous Tags,2004,10,3,2,0.591137,5812,alexis nasr,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"Part of speech taggers based on Hidden Markov Models rely on a series of hypotheses which make certain errors inevitable. The idea developed in this paper consists in allowing a limited, controlled ambiguity in the output of the tagger in order to avoid a number of errors. The ambiguity takes the form of ambiguous tags which denote subsets of the tagset. These tags are used when the tagger hesitates between the different components of the ambiguous tags. They are introduced in an existing lexicon and 3-gram database. Their lexical and syntactic counts are computed on the basis of the lexical and syntactic counts of their constituents, using impurity functions. The tagging process itself, based on the Viterbi algorithm, is unchanged. Experiments conducted on the Brown corpus show a recall of 0.982, for an ambiguity rate of 1.233 which is to be compared with a baseline recall of 0.978 for an ambiguity rate of 1.414 using the same ambiguous tags and with a recall of 0.955 corresponding to the one best solution of standard tagging (without ambiguous tags)."
W03-0702,Conceptual Language Models for Dialog Systems,2003,3,0,2,0,35963,renato mori,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Research Directions in Dialogue Processing,0,The purpose of computer speech understanding is to find conceptual representations from signs coded into the speech signal.
2001.jeptalnrecital-poster.1,Mod{\\`e}les de langage hi{\\'e}rarchiques pour les applications de dialogue en parole spontan{\\'e}e,2001,3,0,1,1,17997,frederic bechet,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Le cadre de cette {\'e}tude concerne les syst{\`e}mes de dialogue via le t{\'e}l{\'e}phone entre un serveur de donn{\'e}es et un utilisateur. Nous nous int{\'e}resserons au cas de dialogues non contraints o{\`u} l{'}utilisateur {\`a} toute libert{\'e} pour formuler ses requ{\^e}tes. G{\'e}n{\'e}ralement, le module de Reconnaissance Automatique de la Parole (RAP) de tels serveurs utilise un seul Mod{\`e}le de Langage (ML) de type bigramme ou trigramme pour mod{\'e}liser l{'}ensemble des interventions possibles de l{'}utilisateur. Ces ML sont appris sur des corpus de phrases retranscrites {\`a} partir de sessions entre le serveur et plusieurs utilisateurs. Nous proposons dans cette {\'e}tude une m{\'e}thode de segmentation de corpus d{'}apprentissage de dialogue utilisant une strat{\'e}gie mixte bas{\'e}e {\`a} la fois sur des connaissances explicites mais aussi sur l{'}optimisation d{'}un crit{\`e}re statistique. Nous montrons qu{'}un gain en terme de perplexit{\'e} et de taux d{'}erreurs/mot peut {\^e}tre constat{\'e} en utilisant un ensemble de sous mod{\`e}les de langage issus de la segmentation plut{\^o}t qu{'}un mod{\`e}le unique appris sur l{'}ensemble du corpus."
P00-1011,Tagging Unknown Proper Names Using Decision Trees,2000,10,49,1,1,17997,frederic bechet,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes a supervised learning method to automatically select from a set of noun phrases, embedding proper names of different semantic classes, their most distinctive features. The result of the learning process is a decision tree which classifies an unknown proper name on the basis of its context of occurrence. This classifier is used to estimate the probability distribution of an out of vocabulary proper name over a tagset. This probability distribution is itself used to estimate the parameters of a stochastic part of speech tagger."
W97-0605,Automatic Lexicon Enhancement by Means of Corpus Tagging,1997,6,1,1,1,17997,frederic bechet,Interactive Spoken Dialog Systems: Bringing Speech and {NLP} Together in Real Applications,0,"Using specialised text corpus to automatically enhance a general lexicon is the aim of this study. Indeed, having lexicons which offer maximal cover on a specific topic is an important benefit in many applications of Automatic Speech and Natural Language Processing. The enhancement of these lexicons can be made automatic as big corpora of specialised texts are available.n n A syntactic tagging process, based on 3-class and 3-gram language models, allows us to automatically allocate possible syntactic categories to the Out-Of-Vocabulary (OOV) words which are found in the corpus processed. These OOV words generally occur several times in the corpus, and a number of these occurrences can be important. By taking into account all the occurrences of an OOV word in a given text as a whole, we propose here a method for automatically extracting a specialised lexicon from a text corpus which is representative of a specific topic."
