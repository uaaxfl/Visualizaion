2009.mtsummit-tutorials.2,2006.iwslt-papers.3,0,\N,Missing
2009.mtsummit-tutorials.2,niessen-etal-2000-evaluation,0,\N,Missing
2009.mtsummit-tutorials.2,J99-4005,0,\N,Missing
2009.mtsummit-tutorials.2,J93-2003,0,\N,Missing
2009.mtsummit-tutorials.2,J04-3001,0,\N,Missing
2009.mtsummit-tutorials.2,E06-1005,0,\N,Missing
2009.mtsummit-tutorials.2,C02-1117,0,\N,Missing
2009.mtsummit-tutorials.2,W03-0310,0,\N,Missing
2009.mtsummit-tutorials.2,P02-1040,0,\N,Missing
2009.mtsummit-tutorials.2,P95-1026,0,\N,Missing
2009.mtsummit-tutorials.2,N09-1047,1,\N,Missing
2009.mtsummit-tutorials.2,D07-1090,0,\N,Missing
2009.mtsummit-tutorials.2,W07-0737,0,\N,Missing
2009.mtsummit-tutorials.2,D08-1024,0,\N,Missing
2009.mtsummit-tutorials.2,P04-1023,0,\N,Missing
2009.mtsummit-tutorials.2,J07-1003,0,\N,Missing
2009.mtsummit-tutorials.2,J94-2001,0,\N,Missing
2009.mtsummit-tutorials.2,N06-2018,0,\N,Missing
2009.mtsummit-tutorials.2,P08-1112,0,\N,Missing
2009.mtsummit-tutorials.2,P07-1004,1,\N,Missing
2009.mtsummit-tutorials.2,W06-1607,0,\N,Missing
2009.mtsummit-tutorials.2,P08-1098,0,\N,Missing
2009.mtsummit-tutorials.2,P09-1021,1,\N,Missing
2009.mtsummit-tutorials.2,N09-1025,0,\N,Missing
2009.mtsummit-tutorials.2,P06-1097,0,\N,Missing
2009.mtsummit-tutorials.2,C04-1046,0,\N,Missing
2009.mtsummit-tutorials.2,N06-1003,0,\N,Missing
2009.mtsummit-tutorials.2,W07-0724,0,\N,Missing
2009.mtsummit-tutorials.2,P07-1078,0,\N,Missing
2009.mtsummit-tutorials.2,N03-1017,0,\N,Missing
2009.mtsummit-tutorials.2,P02-1038,0,\N,Missing
2009.mtsummit-tutorials.2,P96-1042,0,\N,Missing
2009.mtsummit-tutorials.2,J03-1002,0,\N,Missing
2009.mtsummit-tutorials.2,N07-1029,0,\N,Missing
2009.mtsummit-tutorials.2,P08-1066,0,\N,Missing
2009.mtsummit-tutorials.2,P07-1092,0,\N,Missing
2009.mtsummit-tutorials.2,D08-1112,0,\N,Missing
2009.mtsummit-tutorials.2,2005.mtsummit-papers.11,0,\N,Missing
2009.mtsummit-tutorials.2,J07-2003,0,\N,Missing
2009.mtsummit-tutorials.2,W99-0613,0,\N,Missing
2009.mtsummit-tutorials.2,P02-1046,0,\N,Missing
2009.mtsummit-tutorials.2,D07-1080,0,\N,Missing
2009.mtsummit-tutorials.2,W08-0305,0,\N,Missing
2009.mtsummit-tutorials.2,P03-1021,0,\N,Missing
2009.mtsummit-tutorials.2,J04-3004,0,\N,Missing
2009.mtsummit-tutorials.2,P06-1091,0,\N,Missing
2012.amta-papers.16,P09-1088,0,0.134955,"del performs better than simple filtering approaches both in terms of BLEU and model size. Alternately, some of the recent works have employed Bayesian techniques for inducing SCFG. Blunsom et al. (2008) proposed a generative model for deriving a sentence pair through a series of terminal and ITG-style non-terminal rules and used Variational Bayes for learning the SCFG rules. Their goal of learning a SCFG is at variance with our objective of extracting a compact Hiero grammar. A non-parametric Bayesian model using a Gibbs sampler to reason over the space of derivations has also been proposed (Blunsom et al., 2009). Though the model specifically uses priors to bias the grammar to be small, they do not compare the resulting grammar size. Additionally, the model suffered from weaker reordering ability and involve an additional step of extracting the SCFG rules using Hiero rule extraction algorithm on the sampled hierarchical alignments. However both these approaches use small datasets that range between 33K-300K sentence pairs. In contrast, our experiments use large datasets having 1.1M and 1.7M sentence pairs respectively for Ar-En and En-Es, with 2.2M-2.7M thresholded phrase pairs. More recently Sankara"
2012.amta-papers.16,J07-2003,0,0.928443,"ting phrase pairs from the Hiero grammar. We use Variational Bayes (VB) for inference. The Bayesian model induces a compact Hiero grammar that has comparable performance to the original Hiero grammar in terms of the translation quality, and even improves on the full Hiero grammar when faced with a small amount of bilingual training data. On different datasets, the VB method achieves a significant reduction in the grammar size. We analyze the different extracted grammars and explain why the Bayesian model works better. 2 1 Introduction Hierarchical phrase-based statistical machine translation (Chiang, 2007) has been shown to perform competitively with phrase-based and syntax-based models in several language pairs. A major issue with hierarchical phrase-based translation has been the size of the trained translation model, which is typically several times larger than the phrase-based counterpart trained from the same dataset. This leads to over-generation, search errors and a slower decoder (de Gispert et al., 2010). In this paper we propose two alternative approaches to induce compact Hiero grammars. Similar to the original Hiero rule extraction (Chiang, 2007), we consider the phrase pairs that a"
2012.amta-papers.16,D09-1075,0,0.0243409,"., 2008) has further details. 5 Experiments Corpora. We use three language pairs in our experiments: Arabic-English and English-Spanish (large bilingual data conditions), and KoreanEnglish (small bilingual data condition). Table 1 summarizes the statistics for the bilingual corpora used in this paper. For the language model, we use English Gigaword corpus (v4) for the ArabicEnglish and Korean-English translation tasks, and the WMT10 training data together with the UN data for the English-Spanish translation task and use 5gram models for all language pairs. We used the University of Rochester (Chung and Gildea, 2009) corpus for our Korean-English experiments without changing the tuning or test set splits, so our results are directly comparable to theirs. We also used the same rule-based morphological analyzer5 as Chung and Gildea (2009) to segment the Korean side of the bitext. SMT Models. We use our in-house implementation of Hiero (Chiang, 2007) with the standard features such as forward and reverse translation probabilities and lexical weights, phrase and word penalties, glue penalty and language model feature. For each experiment, we use MERT (Och, 2003) to optimize the feature weights on a tuning set"
2012.amta-papers.16,J10-3008,0,0.0439681,"Missing"
2012.amta-papers.16,P06-1121,0,0.25849,"the rules in contrast to two non-terminals allowed by Hiero. However, we note that our model does capture reordering as well as discontiguous phrases (a key feature of Hiero). In terms of the reordering abilities, our model lies between the hierarchical phrasebased and phrase-based models. Our model allows the unaligned source words to be attached at all possible positions in the derivation tree. This results in multiple interpretations of the unaligned words reflecting through large number of derivations, which include wider and richer rule contexts. This is analogous to the method used in (Galley et al., 2006) for context-rich syntactic translation models and we hope this to be useful in the Hiero models as well. In contrast the original Hiero grammar extraction restricts the unaligned words to be attached only to the top most position and so it can participate in just a single derivation. To make VB inference practical, we need to efficiently enumerate all the derivations for a phrase pair such that they are consistent with the given word alignments. We use the factorization algorithm proposed by (Zhang et al., 2008) which encodes wordaligned phrase pairs as a compact alignment tree. (Zhang et al."
2012.amta-papers.16,E09-1044,0,0.155811,"Missing"
2012.amta-papers.16,D07-1103,0,0.0640563,"an the corresponding Hiero 1NT rules. Related Works Some earlier works have focussed on reducing the Hiero grammar size by eliminating rule redundancies in some form such as by discarding rules that can be obtained by monotonically composing the smaller rules (He et al., 2009) or by filtering the grammar, based on certain patterns of hierarchical rules in which the useful patterns were identified in a greedy fashion (Iglesias et al., 2009). Yang and Zheng (2009) applied the Fisher’s exact significance test for pruning the translation model, which has been earlier used for phrase-based models (Johnson et al., 2007). As we showed in our ArabicEnglish experiments, our Bayesian model performs better than simple filtering approaches both in terms of BLEU and model size. Alternately, some of the recent works have employed Bayesian techniques for inducing SCFG. Blunsom et al. (2008) proposed a generative model for deriving a sentence pair through a series of terminal and ITG-style non-terminal rules and used Variational Bayes for learning the SCFG rules. Their goal of learning a SCFG is at variance with our objective of extracting a compact Hiero grammar. A non-parametric Bayesian model using a Gibbs sampler"
2012.amta-papers.16,J04-4002,0,0.0787864,"with phrase-based and syntax-based models in several language pairs. A major issue with hierarchical phrase-based translation has been the size of the trained translation model, which is typically several times larger than the phrase-based counterpart trained from the same dataset. This leads to over-generation, search errors and a slower decoder (de Gispert et al., 2010). In this paper we propose two alternative approaches to induce compact Hiero grammars. Similar to the original Hiero rule extraction (Chiang, 2007), we consider the phrase pairs that are consistent with the word alignments (Och and Ney, 2004) as the starting point in this work. Our first approach learns a minimal grammar by solving a combinatorial optimization problem over a tripartite graph consisting of three types of nodes: phrase pairs, Anoop Sarkar Simon Fraser University Burnaby BC. Canada anoop@cs.sfu.ca Motivation Hierarchical phrase-based translation (Chiang, 2007) model uses a particular type of synchronous context-free grammar (SCFG) over the source and target languages. Unlike typical SCFGs, the rules are lexicalized on the right hand side with at least one aligned word pair in source and target and the grammar has one"
2012.amta-papers.16,P03-1021,0,0.00756982,"used the University of Rochester (Chung and Gildea, 2009) corpus for our Korean-English experiments without changing the tuning or test set splits, so our results are directly comparable to theirs. We also used the same rule-based morphological analyzer5 as Chung and Gildea (2009) to segment the Korean side of the bitext. SMT Models. We use our in-house implementation of Hiero (Chiang, 2007) with the standard features such as forward and reverse translation probabilities and lexical weights, phrase and word penalties, glue penalty and language model feature. For each experiment, we use MERT (Och, 2003) to optimize the feature weights on a tuning set, and evaluate using the corresponding optimal weights on the test set. To ensure robustness in Korean-English small data condition, we run MERT three times. The official NIST BLEU script6 is used for computing the case-insensitive BLEU scores. Evaluation. We compare our two translation grammar induction methods, based on variational Bayes (VB) and combinatorial optimization (Greedy), against the following grammars: • Original Hiero (2NT). The grammar as extracted by the original rule extraction algorithm (Chiang, 2007) with two non-terminals, •"
2012.amta-papers.16,W11-2167,1,0.943817,"xr of rule r. The base measure of a translation rule p0 (r) is the arithmetic mean of the two alignment scores above. p0 (r) ∝ (lfxr + lbxr )/2. Let lx be the geometric mean4 of the forward and backward alignment score over an initial phrase pair  1 2 x ∈ P, lx ∝ lfx lbx . We place a Beta(lx , 0.5) prior over the Bernoulli distribution that decides the derivation type zd and this is normalized by the sum of lexical weights from all phrase pairs. The Beta prior prefers to consolidate a phrase pair fragment (within a larger phrase-pair) having a higher lx as a single rule. This is similar to (Sankaran et al., 2011) and we discuss the differences between the two models in Section 6. 4.2 Variational Inference Variational inference (Ghahramani and Beal, 2000; Attias, 2000) is an approximation technique typically used in Bayesian settings. It is used for approximating an intractable posterior distribution p(Φ; θ) 3 If there are multiple alignments for xr (based on multiple initial phrase pairs), we take the union of these alignments as a. 4 The reason for picking arithmetic mean for p0 and geometric mean for lx is explained in (Sankaran et al., 2011). by finding a tractable variational distribution q(Φ; θ)"
2012.amta-papers.16,P09-2060,0,0.444988,"on variational Bayes (VB) and combinatorial optimization (Greedy), against the following grammars: • Original Hiero (2NT). The grammar as extracted by the original rule extraction algorithm (Chiang, 2007) with two non-terminals, • Original Hiero (1NT). A variant of the original 5 6 http://nlp.kookmin.ac.kr/HAM/eng/main-e.html ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl Lang Pair Ar-En En-Es Ko-En Dataset ISI Ar-En corpus WMT10: no UN URochester data Train/ Tune/ Test Grammar BLEU Model Size Speed (sent/min) 1.1 M/ 1982/ 987 1.7 M/ 5061/ 2489 59218/ 1118/ 1118 Original Hiero (2NT) - Yang and Zheng (2009) - Iglesias et al. (2009) filtered - Pruned (mincount 1.0) 33.11 32.84 32.52 31.68 4.82 4.70 3.59 2.24 3.62 3.73 4.99 5.57 Original Hiero (1NT) - Yang and Zheng (2009) - Iglesias et al. (2009) filtered - Pruned (mincount 1.0) 33.08 32.80 32.40 31.64 3.71 3.59 3.43 2.28 4.43 4.87 5.36 5.70 Greedy Approach 31.20 1.88 6.53 Variational Bayes - Pruned (mincount 1.0) - Pruned (mincount 1.5) 33.13 33.05 32.44 3.75 2.90 1.84 4.62 4.87 5.33 Table 1: Corpus Statistics in # of sentences rule extraction algorithm where the number of nonterminals is restricted to one7 . We compare the model size and BLEU s"
2012.amta-papers.16,D12-1089,0,0.0288271,"Missing"
2012.amta-papers.16,C08-1136,0,0.285114,"include wider and richer rule contexts. This is analogous to the method used in (Galley et al., 2006) for context-rich syntactic translation models and we hope this to be useful in the Hiero models as well. In contrast the original Hiero grammar extraction restricts the unaligned words to be attached only to the top most position and so it can participate in just a single derivation. To make VB inference practical, we need to efficiently enumerate all the derivations for a phrase pair such that they are consistent with the given word alignments. We use the factorization algorithm proposed by (Zhang et al., 2008) which encodes wordaligned phrase pairs as a compact alignment tree. (Zhang et al., 2008) has further details. 5 Experiments Corpora. We use three language pairs in our experiments: Arabic-English and English-Spanish (large bilingual data conditions), and KoreanEnglish (small bilingual data condition). Table 1 summarizes the statistics for the bilingual corpora used in this paper. For the language model, we use English Gigaword corpus (v4) for the ArabicEnglish and Korean-English translation tasks, and the WMT10 training data together with the UN data for the English-Spanish translation task a"
2012.amta-papers.16,C08-1144,0,0.0618312,"oy pruning based on fisher significance test (Yang and Zheng, 2009) to reduce the Hiero model. We also provide results for the pattern-based filtering (Iglesias et al., 2009) that filters the grammar extracted by the original rule extraction algorithm based on certain patterns that are found to be least useful in translation or in improving the quality. And finally, we apply a fixed count cut-off on the pseudo counts of the grammar rules and eliminate all rules having pseudo counts fewer than 1.0 (we call this parameter mincount). This is somewhat similar to the pruning of hierarchical rules (Zollmann et al., 2008) based on a threshold, except that here 7 The Hiero rule extraction algorithm can be trivially modified to limit to 1 NT grammar by replacing only one sub-phrase pair (in a larger phrase pair) with non-terminal X. Other rule extraction constraints are still applied. 8 Following Sankaran et al. (2011), we add the coverage phrase pairs (those with non-decomposable source-target alignments) without the threshold limit to avoid OOVs (in training). Table 2: Arabic-English (Threshold-3): Results. Model sizes is in millions. Boldface indicate the best setting of high BLEU, model size and decoding spe"
2020.acl-main.275,D18-1461,0,0.0381001,"prohibitive training and inference complexity. While learning can be sped up using sampling techniques (Jean et al., 2015), word based NMT models have a difficult time handling rare words, especially in morphologically rich languages such as Romanian, Estonian, and Finnish. The size of the word vocabulary should increase dramatically to capture the compositionality of morphemes in such languages. More recently, many NMT models have been developed based on characters and a combination of characters and words (Ling et al., 2015; Luong and Manning, 2016; Vylomova et al., 2017; Lee et al., 2017; Cherry et al., 2018). Fully character based models (Lee et al., 2017; Cherry et al., 2018) demonstrate a significant improvement over word 1 code and corpora: https://github.com/xlhex/dpe based models on morphologically rich languages. Nevertheless, owing to the lack of morphological information, deeper models are often required to obtain a good translation quality. Moreover, elongated sequences brought by a character representation drastically increases the inference latency. In order to maintain a good balance between the vocabulary size and decoding speed, subword units are introduced in NMT (Sennrich et al.,"
2020.acl-main.275,N19-1423,0,0.133244,"r segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences. DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English ↔ (German, Romanian, Estonian, Finnish, Hungarian). 1 Introduction The segmentation of rare words into subword units (Sennrich et al., 2016; Wu et al., 2016) has become a critical component of neural machine translation (Vaswani et al., 2017) and natural language understanding (Devlin et al., 2019). Subword units enable open vocabulary text processing with a negligible pre-processing cost and help maintain a desirable balance between the vocabulary size and decoding speed. Since subword vocabularies are built in an unsupervised manner (Sennrich et al., 2016; Wu et al., 2016), they are easily applicable to any language. Given a fixed vocabulary of subword units, rare words can be segmented into a sequence of Mohammad Norouzi Google Research mnorouzi@google.com subword units in different ways. For instance, “un+conscious” and “uncon+scious” are both suitable segmentations for the word “un"
2020.acl-main.275,P15-1001,0,0.0443386,"et sentences. DPE achieves an average improvement of 0.9 BLEU over greedy BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over stochastic BPE dropout (Provilkov et al., 2019)1 . 2 Related Work Neural networks have revolutionized machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014). Early neural machine translation (NMT) systems used words as the atomic element of sentences. They used vocabularies with tens of thousands words, resulting in prohibitive training and inference complexity. While learning can be sped up using sampling techniques (Jean et al., 2015), word based NMT models have a difficult time handling rare words, especially in morphologically rich languages such as Romanian, Estonian, and Finnish. The size of the word vocabulary should increase dramatically to capture the compositionality of morphemes in such languages. More recently, many NMT models have been developed based on characters and a combination of characters and words (Ling et al., 2015; Luong and Manning, 2016; Vylomova et al., 2017; Lee et al., 2017; Cherry et al., 2018). Fully character based models (Lee et al., 2017; Cherry et al., 2018) demonstrate a significant improv"
2020.acl-main.275,P18-1007,0,0.313908,"are both suitable segmentations for the word “unconscious”. This paper studies the impact of subword segmentation on neural machine translation, given a fixed subword vocabulary, and presents a new algorithm called Dynamic Programming Encoding (DPE). We identify three families of subword segmentation algorithms in neural machine translation: 1. Greedy algorithms: Wu et al. (2016) segment words by recursively selecting the longest subword prefix. Sennrich et al. (2016) recursively combine adjacent word fragments that co-occur most frequently, starting from characters. 2. Stochastic algorithms (Kudo, 2018; Provilkov et al., 2019) draw multiple segmentations for source and target sequences resorting to randomization to improve robustness and generalization of translation models. 3. Dynamic programming algorithms, studied here, enable exact marginalization of subword segmentations for certain sequence models. We view the subword segmentation of output sentences in machine translation as a latent variable that should be marginalized out to obtain the probability of the output sentence given the input. On the other hand, the segmentation of source sentences can be thought of as input features and"
2020.acl-main.275,Q17-1026,0,0.070523,"ords, resulting in prohibitive training and inference complexity. While learning can be sped up using sampling techniques (Jean et al., 2015), word based NMT models have a difficult time handling rare words, especially in morphologically rich languages such as Romanian, Estonian, and Finnish. The size of the word vocabulary should increase dramatically to capture the compositionality of morphemes in such languages. More recently, many NMT models have been developed based on characters and a combination of characters and words (Ling et al., 2015; Luong and Manning, 2016; Vylomova et al., 2017; Lee et al., 2017; Cherry et al., 2018). Fully character based models (Lee et al., 2017; Cherry et al., 2018) demonstrate a significant improvement over word 1 code and corpora: https://github.com/xlhex/dpe based models on morphologically rich languages. Nevertheless, owing to the lack of morphological information, deeper models are often required to obtain a good translation quality. Moreover, elongated sequences brought by a character representation drastically increases the inference latency. In order to maintain a good balance between the vocabulary size and decoding speed, subword units are introduced in"
2020.acl-main.275,D18-1149,0,0.0236883,"ax{j∈[k−m,k−1] |yj,k ∈V } βj + log Pθ (yj,k |y1 , .., yj ) end for . backtrace the best segmentation using b z ← backtrace(b1 , .., bT ) Number of sentences train dev test En-Hu WMT09 En-De WMT14 En-Fi WMT15 En-Ro WMT16 En-Et WMT18 0.6M 4.2M 1.7M 0.6M 1.9M 2,051 3000 1,500 1,999 2,000 2,525 3003 1,370 1,999 2,000 Vocab size 32K 32K 32K 32K 32K Table 1: Statistics of the corpora. 5 Experiments Dataset We use WMT09 for En-Hu, WMT14 for En-De, WMT15 for En-Fi, WMT16 for En-Ro and WMT18 for En-Et. We utilize Moses toolkit2 to pre-process all corpora, and preserve the true case of the text. Unlike Lee et al. (2018), we retain diacritics for En-Ro to retain the morphological richness. We use all of the sentence pairs where the length of either side is less than 80 tokens for. training. Byte pair encoding (BPE) (Sennrich et al., 2016) is applied to all language pairs to construct a subword vocabulary and provide a baseline segmentation algorithm. The statistics of all corpora is summarized in Table 1. Training with BPE Dropout. We apply BPE dropout (Provilkov et al., 2019) to each mini-batch. For each complete word, during the BPE merge operation, we randomly drop a particular merge with a probability of"
2020.acl-main.275,D15-1176,0,0.0383553,"Missing"
2020.acl-main.275,P16-1100,0,0.041991,"Missing"
2020.acl-main.275,2020.emnlp-main.83,1,0.88669,"Missing"
2020.acl-main.275,P16-1162,0,0.565548,"A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability. DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming. Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences. DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English ↔ (German, Romanian, Estonian, Finnish, Hungarian). 1 Introduction The segmentation of rare words into subword units (Sennrich et al., 2016; Wu et al., 2016) has become a critical component of neural machine translation (Vaswani et al., 2017) and natural language understanding (Devlin et al., 2019). Subword units enable open vocabulary text processing with a negligible pre-processing cost and help maintain a desirable balance between the vocabulary size and decoding spee"
2020.acl-main.275,W17-4115,1,0.861996,"ith tens of thousands words, resulting in prohibitive training and inference complexity. While learning can be sped up using sampling techniques (Jean et al., 2015), word based NMT models have a difficult time handling rare words, especially in morphologically rich languages such as Romanian, Estonian, and Finnish. The size of the word vocabulary should increase dramatically to capture the compositionality of morphemes in such languages. More recently, many NMT models have been developed based on characters and a combination of characters and words (Ling et al., 2015; Luong and Manning, 2016; Vylomova et al., 2017; Lee et al., 2017; Cherry et al., 2018). Fully character based models (Lee et al., 2017; Cherry et al., 2018) demonstrate a significant improvement over word 1 code and corpora: https://github.com/xlhex/dpe based models on morphologically rich languages. Nevertheless, owing to the lack of morphological information, deeper models are often required to obtain a good translation quality. Moreover, elongated sequences brought by a character representation drastically increases the inference latency. In order to maintain a good balance between the vocabulary size and decoding speed, subword units"
2020.acl-main.530,W18-6311,1,0.853431,"tage of different pronoun types. Table 1: Train/dev/test statistics: number of sentences (K: thousands, M: millions), and average document length (in sentences). The #Documents can be obtained by dividing the #Sentences by the Document Length. Context-Aware NMT Multiple works have successfully demonstrated the advantages of using larger context in NMT, where the context comprises few previous source sentences (Wang et al., 2017; Zhang et al., 2018), few previous source and target sentences (Miculicich et al., 2018), or both past and future source and target sentences (Maruf and Haffari, 2018; Maruf et al., 2018, 2019). Further, context-aware NMT has demonstrated improvements in pronoun translation using past context, through concatenating source sentences (Tiedemann and Scherrer, 2017) or through an additional context encoder (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2018). Miculicich et al. (2018) observed reasonable improvements in generic and pronoun-focused translation using three previous sentences as context. Voita et al. (2018) observed improvements using the previous sentence as context, but report decreased BLEU when using the following sentence. We, on the other hand, observe"
2020.acl-main.530,N19-1313,1,0.861951,"ificant gains in BLEU when using the following sentence as context on the same data domain. 3 Contextual Analysis of Corpora To motivate our use of the future context for improving the translation of cataphoric pronouns in particular and NMT in general, we first analyse the distribution of coreferences for anaphoric and cataphoric pronouns over three different corpora OpenSubtitles20181 (Lison and Tiedemann, 2016), Europarl (Koehn, 2005) and TED Talks (Cettolo et al., 2012) - for English-German. For Europarl and TED Talks, we use the publicly available document-aligned version of the corpora (Maruf et al., 2019). For Subtitles, we align the English and German subtitles at the document-level using publicly available alignment links.2 To control for the length and coherency of documents, we keep 1 http://www.opensubtitles.org/ 2 http://opus.nlpl.eu/OpenSubtitles2018.php 40 Anaphora Cataphora 30 20 10 0 1 2 3 4 5 6 7 8 9 10 Figure 1: Plot showing proportion of intersentential English pronouns versus size of coreference resolution window for the Subtitles corpus (plots for Europarl and TED Talks are in the appendix). subtitles with a run-time less than 50 minutes (for English) and those with number of se"
2020.acl-main.530,D18-1325,0,0.100226,"HAN(k = -1) Table 3: BLEU for the Transformer baseline and Transformer-HAN with following sentence (k = +1) and previous sentence (k = -1). ♠: Statistically significantly better than HAN (k = -1). Model Baseline +HAN(k = +1) +HAN(k = -1) the Moses toolkit (Koehn et al., 2007) and split into subword units using a joint BPE model with 30K merge operations (Sennrich et al., 2016).5 Description of the NMT systems As our baseline, we use the DyNet (Neubig et al., 2017) implementation of Transformer (Vaswani et al., 2017).6 For the context-dependent NMT model, we choose the Transformer-HAN encoder (Miculicich et al., 2018), which has demonstrated reasonable performance for anaphoric pronoun translation on Subtitles. We extend its DyNet implementation (Maruf et al., 2019) to a single context sentence.78 For training, Transformer-HAN is initialised with the baseline Transformer and then the parameters of the whole network are optimised in a second stage as in Miculicich et al. (2018) (details of model configuration are in Appendix A.1). For evaluation, we compute BLEU (Papineni et al., 2002) on tokenised truecased text and measure statistical significance with p &lt; 0.005 (Clark et al., 2011). 4.1 Results We consid"
2020.acl-main.530,W17-4802,0,0.462384,"ble 4: Pronoun-focused evaluation on generic test set for models trained on different types of context. identify if this discrepancy is due to the languagepair or the model, we conduct experiments with English→Russian in the same data setting as Voita et al. (2018) and find that HAN (k = +1) still significantly outperforms the Transformer and is comparable to HAN (k = -1) (more details in Appendix A.2). 4.2 Analysis Pronoun-Focused Automatic Evaluation For the models in Table 3, we employ three types of pronoun-focused automatic evaluation: 1. Accuracy of Pronoun Translation (APT) (Miculicich Werlen and Popescu-Belis, 2017)9 . This measures the degree of overlapping pronouns between the output and reference translations obtained via word-alignments. 2. Precision, Recall and F1 scores. We use a variation of AutoPRF (Hardmeier and Federico, 2010) to calculate precision, recall and F1-scores. For each source pronoun, we compute the clipped count (Papineni et al., 2002) of overlap between candidate and reference translations. To eliminate word alignment errors, we compare this overlap over the set of dictionarymatched target pronouns, in contrast to the set of target words aligned to a given source pronoun as done b"
2020.acl-main.530,P02-1040,0,0.107388,"ation of Transformer (Vaswani et al., 2017).6 For the context-dependent NMT model, we choose the Transformer-HAN encoder (Miculicich et al., 2018), which has demonstrated reasonable performance for anaphoric pronoun translation on Subtitles. We extend its DyNet implementation (Maruf et al., 2019) to a single context sentence.78 For training, Transformer-HAN is initialised with the baseline Transformer and then the parameters of the whole network are optimised in a second stage as in Miculicich et al. (2018) (details of model configuration are in Appendix A.1). For evaluation, we compute BLEU (Papineni et al., 2002) on tokenised truecased text and measure statistical significance with p &lt; 0.005 (Clark et al., 2011). 4.1 Results We consider two versions of the Transformer-HAN respectively trained with the following and previous source sentence as context. From Table 3, we note both context-dependent models to significantly outperform the Transformer across all language-pairs in terms of BLEU. Further, HAN (k = +1) demonstrates statistically significant improvements over the HAN (k = -1) when translating to English. These results are quite surprising as Voita et al. (2018) report decreased translation qual"
2020.acl-main.530,N18-1202,0,0.0179005,"+1) 37.68♠ 37.19♠ 39.51 39.45 English→Portuguese Model Cataphora DET PROPN NOUN Baseline 36.29 35.91 37.91 37.60 +HAN(k = +1) 37.08♠ 36.70♠ 38.49 38.19 Portuguese→English Model Cataphora DET PROPN NOUN Baseline 40.74 40.12 42.77 42.63 +HAN(k = +1) 41.63♠ 41.06♠ 43.60♠ 43.42♠ Table 5: Evaluation on English→German generic test set for HAN trained with k = {-2,-1,+1,+2} but decoded with varying context. ♠: Statistically significantly better than HAN with no context (k = ∅). two measures which rely on computing pronoun overlap between the target and reference translation, we employ an ELMo-based (Peters et al., 2018) evaluation framework that distinguishes between a good and a bad translation via pairwise ranking (Jwalapuram et al., 2019). We use the CRC setting of this metric which considers the same reference context (one previous and one next sentence) for both reference and system translations. However, this measure is limited to evaluation only on the English target-side.10 The results using the aforementioned pronoun evaluation metrics are reported in Table 4. We observe improvements for all metrics with both HAN models in comparison to the baseline. Further, we observe that the HAN (k = +1) is eith"
2020.acl-main.530,P16-1162,0,0.0990698,"seline HAN(k = +1) HAN(k = -1) English→German 31.87 32.53 32.48 German→English 35.92 36.64♠ 36.32 English→Portuguese 35.45 36.04 36.21 Portuguese→English 39.34 39.96♠ 39.69 Model Baseline +HAN(k = +1) +HAN(k = -1) Model Baseline +HAN(k = +1) +HAN(k = -1) Table 3: BLEU for the Transformer baseline and Transformer-HAN with following sentence (k = +1) and previous sentence (k = -1). ♠: Statistically significantly better than HAN (k = -1). Model Baseline +HAN(k = +1) +HAN(k = -1) the Moses toolkit (Koehn et al., 2007) and split into subword units using a joint BPE model with 30K merge operations (Sennrich et al., 2016).5 Description of the NMT systems As our baseline, we use the DyNet (Neubig et al., 2017) implementation of Transformer (Vaswani et al., 2017).6 For the context-dependent NMT model, we choose the Transformer-HAN encoder (Miculicich et al., 2018), which has demonstrated reasonable performance for anaphoric pronoun translation on Subtitles. We extend its DyNet implementation (Maruf et al., 2019) to a single context sentence.78 For training, Transformer-HAN is initialised with the baseline Transformer and then the parameters of the whole network are optimised in a second stage as in Miculicich et"
2020.acl-main.530,W17-4811,0,0.0622218,"Documents can be obtained by dividing the #Sentences by the Document Length. Context-Aware NMT Multiple works have successfully demonstrated the advantages of using larger context in NMT, where the context comprises few previous source sentences (Wang et al., 2017; Zhang et al., 2018), few previous source and target sentences (Miculicich et al., 2018), or both past and future source and target sentences (Maruf and Haffari, 2018; Maruf et al., 2018, 2019). Further, context-aware NMT has demonstrated improvements in pronoun translation using past context, through concatenating source sentences (Tiedemann and Scherrer, 2017) or through an additional context encoder (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2018). Miculicich et al. (2018) observed reasonable improvements in generic and pronoun-focused translation using three previous sentences as context. Voita et al. (2018) observed improvements using the previous sentence as context, but report decreased BLEU when using the following sentence. We, on the other hand, observe significant gains in BLEU when using the following sentence as context on the same data domain. 3 Contextual Analysis of Corpora To motivate our use of the future context for imp"
2020.acl-main.530,D18-1049,0,0.222851,"Missing"
2020.acl-main.530,P18-1117,0,0.244315,"ve successfully demonstrated the advantages of using larger context in NMT, where the context comprises few previous source sentences (Wang et al., 2017; Zhang et al., 2018), few previous source and target sentences (Miculicich et al., 2018), or both past and future source and target sentences (Maruf and Haffari, 2018; Maruf et al., 2018, 2019). Further, context-aware NMT has demonstrated improvements in pronoun translation using past context, through concatenating source sentences (Tiedemann and Scherrer, 2017) or through an additional context encoder (Jean et al., 2017; Bawden et al., 2018; Voita et al., 2018). Miculicich et al. (2018) observed reasonable improvements in generic and pronoun-focused translation using three previous sentences as context. Voita et al. (2018) observed improvements using the previous sentence as context, but report decreased BLEU when using the following sentence. We, on the other hand, observe significant gains in BLEU when using the following sentence as context on the same data domain. 3 Contextual Analysis of Corpora To motivate our use of the future context for improving the translation of cataphoric pronouns in particular and NMT in general, we first analyse the d"
2020.acl-main.530,D17-1301,0,0.0573884,"Missing"
2020.acl-main.530,P07-2045,0,\N,Missing
2020.acl-main.530,E12-3001,0,\N,Missing
2020.acl-main.530,P11-2031,0,\N,Missing
2020.acl-main.530,W10-1737,0,\N,Missing
2020.acl-main.530,2010.iwslt-papers.10,0,\N,Missing
2020.acl-main.530,2005.mtsummit-papers.11,0,\N,Missing
2020.acl-main.530,L16-1147,0,\N,Missing
2020.acl-main.530,P18-1118,1,\N,Missing
2020.acl-main.530,N18-1118,0,\N,Missing
2020.acl-main.530,D19-1294,0,\N,Missing
2020.acl-main.530,D18-1334,0,\N,Missing
2020.acl-main.530,2012.eamt-1.60,0,\N,Missing
2020.alta-1.1,blanco-etal-2008-causal,0,0.0546153,"tual causality extraction can be divided into two main subtasks, causality identification and causality localisation. The former subtask focuses on identifying whether a sentence carries any causal information or not, which can be seen as classification problem. The objective of the latter subtask is to extract text spans related to cause and effect, subject to existence. The automatic identification and localisation of causal relations in textual data is considered a nontrivial task (Dasgupta et al., 2018). Causal relations in text can be categorised as marked/unmarked and explicit/implicit (Blanco et al., 2008; Hendrickx et al., 2009). Marked causality refers to the case where a causal linguistic feature, such as “because of”, is stated in the sentence. For example, in “His OCD is because of genetic factors.”, because of is a causal marker, whereas in unmarked causality there is no such indicator. For instance, in “Don’t take these medications before driving. you might feel sleepy.” the cause and effect relationship is spread between two sentences without a marker. On the other hand, explicit causality refers to the case where both cause and effect are mentioned in text. However, in implicit causal"
2020.alta-1.1,W18-5035,0,0.0967934,"), 1 https://github.com/farhadmfar/ace and natural language inference (Roemmele et al., 2011). The task of textual causality extraction can be divided into two main subtasks, causality identification and causality localisation. The former subtask focuses on identifying whether a sentence carries any causal information or not, which can be seen as classification problem. The objective of the latter subtask is to extract text spans related to cause and effect, subject to existence. The automatic identification and localisation of causal relations in textual data is considered a nontrivial task (Dasgupta et al., 2018). Causal relations in text can be categorised as marked/unmarked and explicit/implicit (Blanco et al., 2008; Hendrickx et al., 2009). Marked causality refers to the case where a causal linguistic feature, such as “because of”, is stated in the sentence. For example, in “His OCD is because of genetic factors.”, because of is a causal marker, whereas in unmarked causality there is no such indicator. For instance, in “Don’t take these medications before driving. you might feel sleepy.” the cause and effect relationship is spread between two sentences without a marker. On the other hand, explicit"
2020.alta-1.1,D11-1027,0,0.0389375,"no causality. 2 Related Works The projection of causal relation in textual data can be in various forms, depending on the type of causality. The categorisation mentioned in Section 1 can indicate the relation between pairs of events, phrases, concepts, named entities or a mixture of the aforementioned text spans (Hashimoto, 2019). Some works in the area have endeavoured to extract and present the textual information between concepts or events. Causal relations are a component of SemEval task (Hendrickx et al., 2009), but it involves a limited set of causal relations between pairs of nominals. Do et al. (2011) developed a framework based on combining semantic association and supervised causal discourse classification in order to identify causal relations between pairs of events. They expand the patterns in pdtb (Lin et al., 2009) using a self-training approach. Other methods (Riaz and Girju, 2014, 2013) leveraged linguistic features such as part-of-speech information, alongside with discourse markers, for identifying causal relations between events. An et al. (2019) used the syntactic patterns and word vectors to develop an unsupervised method for constructing causal graphs. To expand the repositor"
2020.alta-1.1,D19-1296,0,0.0147036,"ation. • To fill the gap of the current causality datasets on encompassing different types of causality, we introduce M ED C AUS, a dataset of 15,000 labelled sentences, retrieved from medical articles. This dataset consists of sentences with labels of explicit, implicit, nested, and no causality. 2 Related Works The projection of causal relation in textual data can be in various forms, depending on the type of causality. The categorisation mentioned in Section 1 can indicate the relation between pairs of events, phrases, concepts, named entities or a mixture of the aforementioned text spans (Hashimoto, 2019). Some works in the area have endeavoured to extract and present the textual information between concepts or events. Causal relations are a component of SemEval task (Hendrickx et al., 2009), but it involves a limited set of causal relations between pairs of nominals. Do et al. (2011) developed a framework based on combining semantic association and supervised causal discourse classification in order to identify causal relations between pairs of events. They expand the patterns in pdtb (Lin et al., 2009) using a self-training approach. Other methods (Riaz and Girju, 2014, 2013) leveraged lingu"
2020.alta-1.1,W09-2415,0,0.128019,"Missing"
2020.alta-1.1,P16-1135,0,0.0912791,"combining semantic association and supervised causal discourse classification in order to identify causal relations between pairs of events. They expand the patterns in pdtb (Lin et al., 2009) using a self-training approach. Other methods (Riaz and Girju, 2014, 2013) leveraged linguistic features such as part-of-speech information, alongside with discourse markers, for identifying causal relations between events. An et al. (2019) used the syntactic patterns and word vectors to develop an unsupervised method for constructing causal graphs. To expand the repository of causal syntactic patterns, Hidey and McKeown (2016) built a parallel corpus between English Wikipedia and Simple Wikipedia, where the same causal relation might be in different syntactic markers in two parallel sentences. A supervised method was adapted by Mirza and Tonelli (2016) using lexical, semantic, and syntactic features within a sentence to address this task. Using Hidey and McKeown (2016)’s method, Mart´ınez-C´amara et al. (2017) created a set of labelled sentences, assuming all of the sentences include a causal relation, and presented a neural model based on LSTM to identify causality. Dasgupta et al. (2018) collected a dataset and d"
2020.alta-1.1,D17-1292,0,0.0156486,"Tonelli (2016) using lexical, semantic, and syntactic features within a sentence to address this task. Using Hidey and McKeown (2016)’s method, Mart´ınez-C´amara et al. (2017) created a set of labelled sentences, assuming all of the sentences include a causal relation, and presented a neural model based on LSTM to identify causality. Dasgupta et al. (2018) collected a dataset and developed a model using BiLSTM for extracting causal relation within a sentence2 . Other approaches in event prediction applied Granger causality (Granger, 1988) to identify causal relations in time series of events (Kang et al., 2017). Rojas-Carulla et al. (2017) defined a proxy variable, which may carry some information about cause and effect, to identify causal relationship between static entities. Zhao et al. (2017) developed a causality network embedding for event prediction. De Silva et al. (2017) proposed a convolutional neural network model for identifying causality. Liang et al. (2019) also deployed a self-attentive neural model to address the task of causality identification, however, the extraction of causal information is not addressed by their model. In more recent works, a dataset of counterfactual 2 The sourc"
2020.alta-1.1,W19-5031,0,0.0166396,"ataset was used for both causality identification and causality localisation task (Hendrickx et al., 2009). 3 4 http://www.qwamci.com/ http://wp.lancs.ac.uk/cfie/fincausal2020/ BioCausal-Small The dataset is a part of larger dataset 5 , consisting of 2,000 biomedical sentences from which 1,113 have causal relations. The sentences from this dataset have been collected from biomedical articles of PubMed 6 . Since this dataset only includes information about whether a sentence has causal relation (regardless of the position of the cause and effect), it has been used for causality identification (Kyriakakis et al., 2019). 4.2 Experimental Details For both GCE and ACE, we use Stanford CoreNLP (Manning et al., 2014) to generate the dependency parsing tree for each sentence. We use the pre-trained 300-dimensional Glove vectors (Pennington et al., 2014) to initialise the embedding layer of our model. The hidden size for LSTM and the output feedforward layers is set to 100. We use the standard max pooling function for the pooling layer. Also, for all non-linearities in our model, we use Tanh function. A dropout ratio of p = 0.5 has been applied to all layers except for the last layer of GCN, for regularisation pur"
2020.alta-1.1,N16-1030,0,0.0595329,"e prediction of causal relations. in terms of extracting cause and effect from textual data. M ED C AUS and FinCausal are used in this task for evaluation purposes. Each dataset are split into train/test/validation with the ratio of 60:20:20. For comparison, we report the results of the performance of each model in labelling each token with the proper tag. For this purpose, precision, recall, and F1 score are reported. Similar to the Task1, we compare our model to bi-LSTM (Mart´ınez-C´amara et al., 2017), GCN, and C-GCN (Zhang et al., 2018). Also, we compare our model to the proposed model of Lample et al. (2016), with two variations of using S-LSTM and ELMO (Peters et al., 2018) for contextual embedding. The results of causality localisation are reported in Table 3. The experiments on M ED C AUS show that while bi-LSTM-CRF achieves better results in precision, it fails to gain high recall. On the hand C-GCN-CRF achieves highest recall, followed closely by our model. However, in F1 score, our model, outperforms the baselines. On FinCausal, bi-LSTM-CRF achieves the highest precision. However, our model achieves better recall and F1 score. 4.5 4.4 Task2: Causality Localisation This section covers the re"
2020.alta-1.1,D09-1036,0,0.0427973,"of events, phrases, concepts, named entities or a mixture of the aforementioned text spans (Hashimoto, 2019). Some works in the area have endeavoured to extract and present the textual information between concepts or events. Causal relations are a component of SemEval task (Hendrickx et al., 2009), but it involves a limited set of causal relations between pairs of nominals. Do et al. (2011) developed a framework based on combining semantic association and supervised causal discourse classification in order to identify causal relations between pairs of events. They expand the patterns in pdtb (Lin et al., 2009) using a self-training approach. Other methods (Riaz and Girju, 2014, 2013) leveraged linguistic features such as part-of-speech information, alongside with discourse markers, for identifying causal relations between events. An et al. (2019) used the syntactic patterns and word vectors to develop an unsupervised method for constructing causal graphs. To expand the repository of causal syntactic patterns, Hidey and McKeown (2016) built a parallel corpus between English Wikipedia and Simple Wikipedia, where the same causal relation might be in different syntactic markers in two parallel sentence"
2020.alta-1.1,P14-5010,0,0.0107692,"sarial learning strategy for adapting the model to new domains. Figure 1 illustrates the high level overview of our approach. 3.1 Given a sentence X = [x1 , . . . , xn ], where xi is the vector representation of the i-th token of the sentence, the goal of our model is two-fold: identifying whether or not the causal relation exists, and locating the position of cause and effect in the given sentence. The core part of our causal identification model consists of an L-layer graph convolutional network (GCN) which takes as input the dependency tree of a sentence, obtained through Stanford CoreNLP (Manning et al., 2014). The dependency tree can be represented with an n × n adjacency matrix A, where n is the number of nodes in the graph. In the adjacency matrix, Aji = Aij = 1 if an edge connects i to j, and zero otherwise. Given (l−1) hi as the representation of the node i at layer l − 1, GCN updates the node representation at layer l as follows (Zhang et al., 2018; Kipf and Welling, 2016): (l) n X (l−1) A˜ij W(l) hj /di + b(l) ) hGCN (X) = f pool (GCN(BiLSTM(X)) R (1) j=1 ˜ = A+I, with I the identity matrix, f actv where A an activation function (i.e., element-wise RELU), (l) b(l) the Pnbias ˜vector, W the w"
2020.alta-1.1,W17-6927,0,0.0319091,"Missing"
2020.alta-1.1,C16-1007,0,0.1327,"use and effect are mentioned in text. However, in implicit causality, either cause or effect are directly mentioned in the text. In a more complex case called nested causality, multiple causal relations may exist in one sentence (e.g., “Procaine can also cause allergic reactions causing individuals to have problems with breathing”). All of these ambiguities contribute to the challenging nature of this task. Traditional approaches to address the problem of causality extraction mainly relied on predefined linguistic patterns and rules to identify the existence of causal relations in a sentence (Mirza and Tonelli, 2016). More advanced approaches combined pattern-based methods with machine learning techniques (Zhao et al., 2018), and as such they require heavy manual feature engineering to perform reasonably. To overcome this problem, the recent approaches have adopted deep learning techniques to extract meaningful features from the text (Liang et al., 2019; Mart´ınez-C´amara et al., 2017). However, all the aformentioned approaches suffer from the problem of domain shift, where there is a distribution difference between the training and the test data. More specifically, the existing approaches perform poorly"
2020.alta-1.1,P18-1212,0,0.0184813,"he text. 1 Introduction Causality is the basis for reasoning and decision making. While human-beings use this psychological tool to choreograph their environment into a mental model to act accordingly (Pearl and Mackenzie, 2018), the inability to identify causal relationships is one of the drawbacks of current Artificial Intelligence systems (Lake et al., 2015). The projection of causal relations in natural language enables machines to develop a better understanding of the surrounding context and helps downstream tasks such as question answering (Hassanzadeh et al., 2019), text summarisation (Ning et al., 2018), 1 https://github.com/farhadmfar/ace and natural language inference (Roemmele et al., 2011). The task of textual causality extraction can be divided into two main subtasks, causality identification and causality localisation. The former subtask focuses on identifying whether a sentence carries any causal information or not, which can be seen as classification problem. The objective of the latter subtask is to extract text spans related to cause and effect, subject to existence. The automatic identification and localisation of causal relations in textual data is considered a nontrivial task (D"
2020.alta-1.1,2020.semeval-1.55,0,0.0790352,"Missing"
2020.alta-1.1,D14-1162,0,0.0835385,"5 , consisting of 2,000 biomedical sentences from which 1,113 have causal relations. The sentences from this dataset have been collected from biomedical articles of PubMed 6 . Since this dataset only includes information about whether a sentence has causal relation (regardless of the position of the cause and effect), it has been used for causality identification (Kyriakakis et al., 2019). 4.2 Experimental Details For both GCE and ACE, we use Stanford CoreNLP (Manning et al., 2014) to generate the dependency parsing tree for each sentence. We use the pre-trained 300-dimensional Glove vectors (Pennington et al., 2014) to initialise the embedding layer of our model. The hidden size for LSTM and the output feedforward layers is set to 100. We use the standard max pooling function for the pooling layer. Also, for all non-linearities in our model, we use Tanh function. A dropout ratio of p = 0.5 has been applied to all layers except for the last layer of GCN, for regularisation purposes. For training of GCE, we split the data into train, development, and test set with the ratio of 60:20:20. For both models, we use batches of size 50. We train the model for 100 epochs, using Adamax optimiser. We use a decay rat"
2020.alta-1.1,N18-1202,0,0.109062,"Keown, 2016) bi-LSTM (Mart´ınez-C´amara et al., 2017) GCN (Zhang et al., 2018) C-GCN (Zhang et al., 2018) GCE M ED C AUS P R F1 74.4 74.4 74.4 84.2 97.8 90.5 90.8 94.6 92.7 91.2 94.9 93.0 92.5 94.0 93.2 FinCausal P R F1 54.0 54.0 54.0 81.3 77.0 79.1 85 74.8 79.6 86.1 68.7 76.4 84.8 83.3 84 Table 2: Results of our proposed method on Task1, causality identification, compared with the baseline approaches on the M ED C AUS and FinCausal dataset. Model bi-LSTM-CRF (Mart´ınez-C´amara et al., 2017) GCN-CRF (Zhang et al., 2018) C-GCN-CRF (Zhang et al., 2018) S-LSTM-CRF (Lample et al., 2016) ELMO-CRF (Peters et al., 2018) GCE M ED C AUS P R F1 77.4 69.9 73.4 31.9 46.8 37.9 72.5 75.9 74.1 58.6 64.0 61.2 48.5 78.9 60.1 76.3 73.6 74.9 FinCausal P R F1 82.4 65.0 72.7 66.1 55.5 60.3 76.3 68.8 72.3 61.5 29.7 40.0 71.8 61.3 66.1 79.2 69.8 74.2 Table 3: Results of our proposed method on Task2, causality localisation, compared with the baseline approaches on the M ED C AUS and FinCausal dataset. for causality identification. We divide the dataset into train/test/validation sets based on the ratio 60:20:20. We compare our model to the P-Wiki (Hidey and McKeown, 2016), which is a rule-based method, and bi-LSTM (Mart´ınez"
2020.alta-1.1,W09-1119,0,0.0321769,"nd strengthen the causality classifier. 3.3 Tagging Scheme The objective of the task of causality localisation is to assign a label to each token in a sentence to locate the position of cause and effect. Cause and effect of a causal relation may span several tokens in a sentence. Therefore, the labels of a sentence usually are represented in the IOB-format (Inside, outside, and beginning). In this format, B-label indicates beginning of the span label, I-label shows a token inside the label but not the first token, and O-label represents the token as an outsider of label. However, inspired by (Ratinov and Roth, 2009) and (Dai et al., 2015), we use IOBES, an extended version of IOB, which also accounts for singleton labels and end of the label span token. Furthermore, to keep the tags consistent with the Equation 5, we add a start and end label to the set of tags. 4 Experiments In this section, we first describe the datasets that have been used for the evaluation of our models, including our collected dataset. Then we present results of our proposed models on both (adaptive) causality identification and causality localisation. 4.1 Datasets M ED C AUS We introduce our medical causality dataset with 15,000 s"
2020.alta-1.1,W13-4004,0,0.0420915,"Missing"
2020.alta-1.1,W14-0707,0,0.0222346,"aforementioned text spans (Hashimoto, 2019). Some works in the area have endeavoured to extract and present the textual information between concepts or events. Causal relations are a component of SemEval task (Hendrickx et al., 2009), but it involves a limited set of causal relations between pairs of nominals. Do et al. (2011) developed a framework based on combining semantic association and supervised causal discourse classification in order to identify causal relations between pairs of events. They expand the patterns in pdtb (Lin et al., 2009) using a self-training approach. Other methods (Riaz and Girju, 2014, 2013) leveraged linguistic features such as part-of-speech information, alongside with discourse markers, for identifying causal relations between events. An et al. (2019) used the syntactic patterns and word vectors to develop an unsupervised method for constructing causal graphs. To expand the repository of causal syntactic patterns, Hidey and McKeown (2016) built a parallel corpus between English Wikipedia and Simple Wikipedia, where the same causal relation might be in different syntactic markers in two parallel sentences. A supervised method was adapted by Mirza and Tonelli (2016) using"
2020.alta-1.1,D18-1244,0,0.245485,"ition of cause and effect in the given sentence. The core part of our causal identification model consists of an L-layer graph convolutional network (GCN) which takes as input the dependency tree of a sentence, obtained through Stanford CoreNLP (Manning et al., 2014). The dependency tree can be represented with an n × n adjacency matrix A, where n is the number of nodes in the graph. In the adjacency matrix, Aji = Aij = 1 if an edge connects i to j, and zero otherwise. Given (l−1) hi as the representation of the node i at layer l − 1, GCN updates the node representation at layer l as follows (Zhang et al., 2018; Kipf and Welling, 2016): (l) n X (l−1) A˜ij W(l) hj /di + b(l) ) hGCN (X) = f pool (GCN(BiLSTM(X)) R (1) j=1 ˜ = A+I, with I the identity matrix, f actv where A an activation function (i.e., element-wise RELU), (l) b(l) the Pnbias ˜vector, W the weight matrix, and di = j=1 Aij the degree of node i. (2) R where f pool : d×n → d is a pooling function generating the representations for the n tokens of the sentence. The final sentence representation is obtained by a feed forward network (FFNN) whose input is the concatenation of hGCN (X) and hBiLSTM (X). Note that hBiLSTM (X) is the contextualis"
2020.coling-main.119,P17-1042,0,0.0201093,"res (PS1 , and PS2 ) which are normalized using (2) and, additionally, used as features during classification. It should be noted that using phonetic vectors and their similarity scores has already been proposed in the previous literature (Rama, 2016) for a cognate detection task, and we do not claim this approach to be our novel contribution. 4.3 Cross-lingual Vectors & Similarity As described above, we train cross-lingual embedding models by aligning two disjoint monolingual vector spaces through linear transformations, using a small bilingual dictionary for supervision (Doval et al., 2018; Artetxe et al., 2017). The first two approaches for training cross-lingual methods use this dictionary for supervision. In our novel approach, we propose the use of vectors from the cross-lingual embedding models trained on Indian language pairs. We obtain vectors for word-pairs (W VS and W VT ) and averaged context vectors (CVS and CVT ) for the context dictionary, to create feature sets. We obtain vectors for each candidate pair and their context using all the three cross-lingual methodologies. Additionally, we use angular cosine similarity (Cer et al., 2018) scores for word pairs and their contexts. Angular sim"
2020.coling-main.119,P18-1073,0,0.0179705,"imedia Dumps; as on April 22, 2020 Additional Monolingual Corpus JNU Sanskrit Proses Corpus Indic NLP Library FastText - GitHub 1387 uses the supervised method named MUSE (Conneau et al., 2017)10 which utilizes a manually curated bilingual lexicon11 for alignments. We use Hindi as a pivot language due to the ease of computation and availability of resources (Corpora and WordNet size). We use the monolingual models described above and train 13 cross-lingual word embedding models (thirteen language pairs over 100 dimensions) using this approach. The second cross-lingual methodology uses VecMap (Artetxe et al., 2018), which utilizes the monolingual models created above. VecMap uses an optional normalization feature while it builds the mappings between any two monolingual models. It performs orthogonal transformation and maps semantically related words, similar to MUSE, which was used in our first approach for building cross-lingual models. Additionally, it also reduces the dimensions of the embeddings models, which, is optional. We train it using the same hyperparameters as described above, for consistency while evaluating. We used the supervised approach for training these models as well, and the trainin"
2020.coling-main.119,D16-1162,0,0.0258029,"an optimal number of merge operations. We observe that performing 2500 merge operations provided us with best BLEU (Papineni et al., 2002) scores, for most of the language pairs. We report the best results here, and a complete set of merge operation results in the supplementary material. We call this the NMT-BPE Baseline. To validate our hypothesis that our approach can help the NMT task, we inject the cognates detected using our approach to the parallel corpus for their respective language pairs, as single word sentences. Lexical Dictionaries have previously been used to improve the MT task (Arthur et al., 2016; Han et al., 2019). However, a decent improvement in their BLEU scores is observed when their lexicon sizes are approximately around 1M tokens (Arthur et al., 2016). Our detected cognate list size varies from 930 cognates (Hi-Te) to 15834 (Hi-Mr). Due to the addition of more parallel instances to the corpus, the vocabulary size for NMT increases. Hence, we experiment further by varying the BPE merges, in a close range, to the optimal merge point obtained earlier. We report the results of the best optimal merge setting, for both NMT-BPE Baseline model and the cognate injected NMT-BPE model, in"
2020.coling-main.119,N09-3008,0,0.0125337,"nza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verification and segratation of their output into cognates and non-cognates. Kanojia et al. (2019b) utilize recurrent neural networks to harness the character sequence among cognates and non-cognates for Indian languages, but employ monolingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation equivalents affects bilingual word re"
2020.coling-main.119,Q17-1010,0,0.0429109,"any other script to the Devanagari script. We perform Unicode transliteration using Indic NLP Library8 to convert scripts for Bn, As, Or, Gu, Pa, Ml, Ta, Kn and Te to Devanagari for standardization. Hi, Mr, Ko, Ne, and Sa are already based on the Devanagari script. We perform this for script transliteration for both the cognate dataset (Table 1) and the corpus (Table 2). We describe the creation of cross-lingual word embeddings below. 3.2 Cross-lingual Word Embedding Methodologies Using the monolingual corpora described above, we build monolingual word embeddings using the FastText library9 (Bojanowski et al., 2017) since it takes sub-word information into account, which is beneficial for a task such as ours where sub-words play an important role, and spelling variations can lead to different meanings. We do not use BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), or MBERT (Pires et al., 2019) for word embeddings as their pre-trained models are not trained on transliterated corpora. We choose FastText to train Skipgram word embedding models (100 dimensions) for each language using the following hyperparameters - 15 epochs with 0.1 as the learning rate. We use two characters (bi-gram) as the size o"
2020.coling-main.119,bojar-etal-2014-hindencorp,0,0.028968,"Missing"
2020.coling-main.119,D18-2029,0,0.0251243,"l dictionary for supervision (Doval et al., 2018; Artetxe et al., 2017). The first two approaches for training cross-lingual methods use this dictionary for supervision. In our novel approach, we propose the use of vectors from the cross-lingual embedding models trained on Indian language pairs. We obtain vectors for word-pairs (W VS and W VT ) and averaged context vectors (CVS and CVT ) for the context dictionary, to create feature sets. We obtain vectors for each candidate pair and their context using all the three cross-lingual methodologies. Additionally, we use angular cosine similarity (Cer et al., 2018) scores for word pairs and their contexts. Angular similarity distinguishes nearly parallel vectors much better as small changes in vector values yield considerable distances. For each word pair vector and its context vectors, we compute the ‘word-pair similarity’ and ‘contextual similarity’. We use arccos to obtain angular cosine similarity (asim) among vectors ‘u’ and ‘v’, as shown below:     u.v asim(u, v) = 1 − arccos /π (3) kukkvk Each candidate word-pair generates a score i.e., score1, and the average of scores among all words in the context dictionary generates another score i.e., s"
2020.coling-main.119,P14-2017,0,0.106733,"a. Link: Data, code and models This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details are on this link. 3 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 1384 Proceedings of the 28th International Conference on Computational Linguistics, pages 1384–1395 Barcelona, Spain (Online), December 8-13, 2020 The task of cognate detection across languages requires one to detect word pairs which are etymologically related, and carry the same meaning. Previous approaches to the task use orhtographic (Ciobanu and Dinu, 2014), phonetic (Rama, 2016) and semantic (Kondrak, 2001) features. However, these methods have a limitation since they do not take into consideration the notion of semantic similarity across languages. A key question that we try to answer in this paper is, “Can semantic information be leveraged from Cross-lingual models to improve cognate detection amongst low-resource languages?” We hypothesize that utilizing cross-lingual features by employing existing resources such as wordnets and cross-lingual embeddings should help improve cognate detection. In this paper, we utilize the semantic information"
2020.coling-main.119,P15-2071,0,0.0199277,"(Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verification and segratation of their output into cognates and non-cognates. Kanojia et al. (2019b) utilize recurrent neural networks to harness the character sequence among cognates and non-cognates for Indian languages, but employ monolingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation e"
2020.coling-main.119,P19-4007,0,0.0420076,"Missing"
2020.coling-main.119,D18-1027,0,0.0152337,"e two similarity scores (PS1 , and PS2 ) which are normalized using (2) and, additionally, used as features during classification. It should be noted that using phonetic vectors and their similarity scores has already been proposed in the previous literature (Rama, 2016) for a cognate detection task, and we do not claim this approach to be our novel contribution. 4.3 Cross-lingual Vectors & Similarity As described above, we train cross-lingual embedding models by aligning two disjoint monolingual vector spaces through linear transformations, using a small bilingual dictionary for supervision (Doval et al., 2018; Artetxe et al., 2017). The first two approaches for training cross-lingual methods use this dictionary for supervision. In our novel approach, we propose the use of vectors from the cross-lingual embedding models trained on Indian language pairs. We obtain vectors for word-pairs (W VS and W VT ) and averaged context vectors (CVS and CVT ) for the context dictionary, to create feature sets. We obtain vectors for each candidate pair and their context using all the three cross-lingual methodologies. Additionally, we use angular cosine similarity (Cer et al., 2018) scores for word pairs and thei"
2020.coling-main.119,E17-1113,0,0.0425306,"Missing"
2020.coling-main.119,jha-2010-tdil,0,0.0142001,"Missing"
2020.coling-main.119,N10-1103,0,0.043284,"be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or"
2020.coling-main.119,2019.gwc-1.51,1,0.808977,", Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verification and segratation of their output into cognates and non-cognates. Kanojia et al. (2019b) utilize recurrent neural networks to harness the character sequence among cognates and non-cognates for Indian languages, but employ monolingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation equivalents affects bilingual word recognition, even in tasks manually performed by humans. They discuss how the need for recognizing semantic simil"
2020.coling-main.119,2020.lrec-1.378,1,0.798533,"ages as shown in this work. This paper discusses the quantitative and qualitative results using our approach and then, applies our output to different neural machine translation architectures. Language Pair Hi-Bn Hi-Gu Hi-Mr Hi-Pa Hi-Sa Hi-Ml Hi-Ta Hi-Te Hi-As Hi-Kn Hi-Or Hi-Ne* Hi-Ko* Cognates 15312 17021 15726 14097 21710 9235 3363 936 3478 4103 11894 2560 11295 Non-Cognates 16119 15057 15983 15166 23029 8976 4005 1084 4101 3810 13027 1918 9826 Table 1: Number of cognates and non-cognates for each language pair in the dataset. Hi-Ne* and Hi-Ko* were generated via replicating their approach (Kanojia et al., 2020). Language Hi Bn Gu Mr Pa Sa Ml Ta Te Ne As Kn Ko Or Corpus Size 48142K 1564K 439K 520K 505K 553K 495K 909K 1023K 706K 504K 159K 214K 744K STTR (n=1000) 0.5821 0.5437 0.4587 0.6108 0.4314 0.5350 0.7339 0.6411 0.4950 0.4883 0.5968 0.5338 0.5614 0.4160 Table 2: Corpus Statistics where corpus size is the approximate number of lines, and STTR is the moving average type-token ratio on a windows of 1000 sentences. 3 Dataset and Experimental Setup In this section, we describe our primary dataset for the cognate detection task. We also describe the datasets used for building cross-lingual word embeddi"
2020.coling-main.119,P17-4012,0,0.0362403,"once the learning rate falls below 0.001. We perform our experiments with the feature sets (Orthographic (WLS), Phonetic (PVS), and three different cross-lingual embeddings based feature sets) described above for all the thirteen language pairs. We also perform an ablation test with various feature sets and report the results for the best feature combination in the next section. The results of our classification task can be seen in Table 3 and are discussed in the next section, in detail. 4.5 Cognate-aware Neural Machine Translation (NMT) Task For the NMT task, we use the OpenNMT-Py toolkit (Klein et al., 2017) to perform our experiments. We use a Bidirectional RNN Encoder-Decoder architecture with attention (Bahdanau et al., 2014). We choose three stacked LSTM (Hochreiter and Schmidhuber, 1997) layers in the encoder and decoder. The hidden-size of the model was 500 units. We optimize using stochastic gradient descent at an initial learning rate of 1, and a batch-size of 1024 units. Training is done for 150,000 steps of which the initial 8,000 steps are for learning rate warm-up. We use Byte-pair encoding (BPE) (Sennrich et al., 2015) merge operations, initially, in an endeavour to find the best bas"
2020.coling-main.119,N03-2016,0,0.112806,"ginated from Sanskrit, Persian, and English. While, in many cases, one might argue that such occurrences do not belong to an Indian language, the frequency of such usage indicates a wide acceptance of these foreign language words as Indian language words. In numerous cases, these words also are morphologically altered as per the Indian language morphological rules to generate new variants of existing words. Detection of such variants or ‘Cognates’ across languages helps Cross-lingual Information Retrieval (CLIR) (Makin et al., 2008; Meng et al., 2001), Machine Translation (MT) (Kondrak, 2005; Kondrak et al., 2003; Al-Onaizan et al., 1999), and Computational Phylogenetics (Rama et al., 2018). Cognates are etymologically related words across two languages (Crystal, 2011). However, NLP applications are concerned with the set of cognate words which have similarities in their spelling and their meaning. For example, the French and English word pair, Libert´e - Liberty, reveals itself to be a true cognate through orthographic similarity. In some cases, similar words have a common meaning only in some contexts; such words are called partial cognates. For example, the word “police” in French can translate to"
2020.coling-main.119,A00-2038,0,0.178355,"logies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (20"
2020.coling-main.119,N01-1014,0,0.277746,"Creative Commons Attribution 4.0 International Licence. Licence details are on this link. 3 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 1384 Proceedings of the 28th International Conference on Computational Linguistics, pages 1384–1395 Barcelona, Spain (Online), December 8-13, 2020 The task of cognate detection across languages requires one to detect word pairs which are etymologically related, and carry the same meaning. Previous approaches to the task use orhtographic (Ciobanu and Dinu, 2014), phonetic (Rama, 2016) and semantic (Kondrak, 2001) features. However, these methods have a limitation since they do not take into consideration the notion of semantic similarity across languages. A key question that we try to answer in this paper is, “Can semantic information be leveraged from Cross-lingual models to improve cognate detection amongst low-resource languages?” We hypothesize that utilizing cross-lingual features by employing existing resources such as wordnets and cross-lingual embeddings should help improve cognate detection. In this paper, we utilize the semantic information from cross-lingual word embeddings. Cross-lingual w"
2020.coling-main.119,2005.mtsummit-papers.40,0,0.203812,"s that have originated from Sanskrit, Persian, and English. While, in many cases, one might argue that such occurrences do not belong to an Indian language, the frequency of such usage indicates a wide acceptance of these foreign language words as Indian language words. In numerous cases, these words also are morphologically altered as per the Indian language morphological rules to generate new variants of existing words. Detection of such variants or ‘Cognates’ across languages helps Cross-lingual Information Retrieval (CLIR) (Makin et al., 2008; Meng et al., 2001), Machine Translation (MT) (Kondrak, 2005; Kondrak et al., 2003; Al-Onaizan et al., 1999), and Computational Phylogenetics (Rama et al., 2018). Cognates are etymologically related words across two languages (Crystal, 2011). However, NLP applications are concerned with the set of cognate words which have similarities in their spelling and their meaning. For example, the French and English word pair, Libert´e - Liberty, reveals itself to be a true cognate through orthographic similarity. In some cases, similar words have a common meaning only in some contexts; such words are called partial cognates. For example, the word “police” in Fr"
2020.coling-main.119,W12-0216,0,0.0171469,"tion methodologies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Prev"
2020.coling-main.119,N01-1020,0,0.147349,"ative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language"
2020.coling-main.119,J99-1003,0,0.0606347,"ion 4 presents the approaches used in terms of feature sets and classification methodologies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection"
2020.coling-main.119,K19-1011,0,0.0223535,"Missing"
2020.coling-main.119,mulloni-pekar-2006-automatic,0,0.0637471,"the approaches used in terms of feature sets and classification methodologies. The results obtained are described in Section 5 along with a discussion on the qualitative analysis of our output. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly i"
2020.coling-main.119,W97-1102,0,0.46133,"entations for each token. 4 Approaches We use various approaches to perform the cognate detection task viz. baseline cognate detection approaches like orthographic similarity-based, phonetic similarity-based, phonetic vectors with SiameseCNN based proposed by Rama (2016), and Recurrent neural network-based approach proposed by Kanojia et al. (2019b). We use the same hyperparameters and architectures, as discussed in these papers. We describe each of these feature sets in this section. 4.1 Weighted Lexical Similarity (WLS) The Normalized Edit Distance (NED) approach computes the edit distance (Nerbonne and Heeringa, 1997) for all word pairs in our dataset. Each of the operations has unit cost (except that substitution of a character by itself has zero cost), so NED is equal to the minimum number of operations to transform ‘word a’ to ‘word b’. We use a similarity score provided by NED, which is calculated as (1 - NED Score). We combine NED with q-gram distance (Shannon, 1948) for a better similarity score. The qgrams (‘n-grams’) are simply substrings of length q. This distance measure has been applied previously for various spelling correction approaches (Owolabi and McGregor, 1988; Kohonen, 1978). Kanojia et"
2020.coling-main.119,P02-1040,0,0.108173,"e three stacked LSTM (Hochreiter and Schmidhuber, 1997) layers in the encoder and decoder. The hidden-size of the model was 500 units. We optimize using stochastic gradient descent at an initial learning rate of 1, and a batch-size of 1024 units. Training is done for 150,000 steps of which the initial 8,000 steps are for learning rate warm-up. We use Byte-pair encoding (BPE) (Sennrich et al., 2015) merge operations, initially, in an endeavour to find the best baseline model with an optimal number of merge operations. We observe that performing 2500 merge operations provided us with best BLEU (Papineni et al., 2002) scores, for most of the language pairs. We report the best results here, and a complete set of merge operation results in the supplementary material. We call this the NMT-BPE Baseline. To validate our hypothesis that our approach can help the NMT task, we inject the cognates detected using our approach to the parallel corpus for their respective language pairs, as single word sentences. Lexical Dictionaries have previously been used to improve the MT task (Arthur et al., 2016; Han et al., 2019). However, a decent improvement in their BLEU scores is observed when their lexicon sizes are approx"
2020.coling-main.119,N18-1202,0,0.0510587,"pt. We perform this for script transliteration for both the cognate dataset (Table 1) and the corpus (Table 2). We describe the creation of cross-lingual word embeddings below. 3.2 Cross-lingual Word Embedding Methodologies Using the monolingual corpora described above, we build monolingual word embeddings using the FastText library9 (Bojanowski et al., 2017) since it takes sub-word information into account, which is beneficial for a task such as ours where sub-words play an important role, and spelling variations can lead to different meanings. We do not use BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), or MBERT (Pires et al., 2019) for word embeddings as their pre-trained models are not trained on transliterated corpora. We choose FastText to train Skipgram word embedding models (100 dimensions) for each language using the following hyperparameters - 15 epochs with 0.1 as the learning rate. We use two characters (bi-gram) as the size of each sub-word for capturing the maximum number of sub-words. We use three different methodologies for training the cross-lingual word embedding models on all the language pairs with Hindi as a pivot language (Hi-Mr, Hi-Bn and so on). The first methodology 5"
2020.coling-main.119,P19-1493,0,0.0254159,"ransliteration for both the cognate dataset (Table 1) and the corpus (Table 2). We describe the creation of cross-lingual word embeddings below. 3.2 Cross-lingual Word Embedding Methodologies Using the monolingual corpora described above, we build monolingual word embeddings using the FastText library9 (Bojanowski et al., 2017) since it takes sub-word information into account, which is beneficial for a task such as ours where sub-words play an important role, and spelling variations can lead to different meanings. We do not use BERT (Devlin et al., 2018), ELMo (Peters et al., 2018), or MBERT (Pires et al., 2019) for word embeddings as their pre-trained models are not trained on transliterated corpora. We choose FastText to train Skipgram word embedding models (100 dimensions) for each language using the following hyperparameters - 15 epochs with 0.1 as the learning rate. We use two characters (bi-gram) as the size of each sub-word for capturing the maximum number of sub-words. We use three different methodologies for training the cross-lingual word embedding models on all the language pairs with Hindi as a pivot language (Hi-Mr, Hi-Bn and so on). The first methodology 5 Link: Link: 7 Link: 8 Link: 9"
2020.coling-main.119,N18-2063,0,0.0356884,"Missing"
2020.coling-main.119,C16-1097,0,0.0946439,"work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details are on this link. 3 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 1384 Proceedings of the 28th International Conference on Computational Linguistics, pages 1384–1395 Barcelona, Spain (Online), December 8-13, 2020 The task of cognate detection across languages requires one to detect word pairs which are etymologically related, and carry the same meaning. Previous approaches to the task use orhtographic (Ciobanu and Dinu, 2014), phonetic (Rama, 2016) and semantic (Kondrak, 2001) features. However, these methods have a limitation since they do not take into consideration the notion of semantic similarity across languages. A key question that we try to answer in this paper is, “Can semantic information be leveraged from Cross-lingual models to improve cognate detection amongst low-resource languages?” We hypothesize that utilizing cross-lingual features by employing existing resources such as wordnets and cross-lingual embeddings should help improve cognate detection. In this paper, we utilize the semantic information from cross-lingual wor"
2020.coling-main.119,W99-0626,0,0.196426,"put. Section 6 concludes this article with possible future work in the area. 2 Related Work The two main existing approaches for the detection of cognates belong to the generative and discriminative paradigms. The first set of approaches is based on the computation of a similarity score between potential candidate pairs. This score can be based on orthographic similarity (J¨ager et al., 2017; Melamed, 1999; Mulloni and Pekar, 2006), phonetic similarity (Rama, 2016; List, 2012; Kondrak, 2000), or a distance measure with the scores learned from an existing parallel set (Mann and Yarowsky, 2001; Tiedemann, 1999). The discriminative paradigm uses standard approaches to machine learning, which are 4 Compounding means when two or more words or signs are joined to make a longer word or sign. 1385 based on (1) extracting features, e.g., character n-grams, and (2) learning to predict the transformations of the source word needed to (Jiampojamarn et al., 2010; Frunza and Inkpen, 2009). Cognate Detection has been explored vastly in terms of classification methodologies. Previously, Rama (2016) employ a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for c"
2020.coling-main.119,W19-4720,0,0.0231209,"lingual embeddings for the task. Dijkstra et al. (2010) show how cross-linguistic similarity of translation equivalents affects bilingual word recognition, even in tasks manually performed by humans. They discuss how the need for recognizing semantic similarity arises for non-identical cognates, based on the reaction time from human annotators. Similarly, Merlo and Andueza Rodriguez (2019) show that cross-lingual models exhibit the semantic properties of for bilingual lexicons despite their structural simplicities, which leads us to perform our investigation for low-resource Indian languages. Uban et al. (2019) discuss the semantic change in languages by studying the change in cognate words across Romance languages using cross-lingual similarity. All of the previous approaches discussed above, lack the use of an appropriate cross-lingual similarity-based measure and do not work well for Indian languages as shown in this work. This paper discusses the quantitative and qualitative results using our approach and then, applies our output to different neural machine translation architectures. Language Pair Hi-Bn Hi-Gu Hi-Mr Hi-Pa Hi-Sa Hi-Ml Hi-Ta Hi-Te Hi-As Hi-Kn Hi-Or Hi-Ne* Hi-Ko* Cognates 15312 1702"
2020.coling-main.226,N16-1181,0,0.0191664,"nd Neubig, 2018; Guo et al., 2019b). At each time step, the decoder of these methods emits either a parse action or a production rule, leading to a grammatically valid MR at the end. these works produce derivations by varying grammars. NSM (Liang et al., 2016) uses a subset of Lisp syntax. T RAN X (Yin and Neubig, 2018) defines the grammars in Abstract Syntax Description Language, while IRN ET (Guo et al., 2019b) considers the context-free grammar of a language called SemQL. There are also neural-symbolic approaches adopting neural architectures other than S EQ 2S EQ. One of such examples is (Andreas et al., 2016), which adopts a dynamic neural module network (DNMN) to generate MRs. 2.2 Evaluation In semantic parsing, exact match accuracy is the most commonly used evaluation metric. With exact match accuracy, the parsing results are considered correct only when the output MR/denotations exactly match the string of the ground truth MR/denotations. One flaw of the evaluation metric is that some types of MRs (e.g., SQL) do not hold order constraints. Yu et al. (2018) proposed a metric set match accuracy to evaluate the semantic parsing performance over SQLs, which treats each SQL statement as a set of cla"
2020.coling-main.226,W13-2322,0,0.0453578,"ally, we cover open research problems in §5, and conclude by providing a roadmap for future research in this area. 2 Background C I SP aims to learn a mapping πθ : X → Y, which translates an NL utterance x ∈ X into an MR y ∈ Y. An MR y can be executed in a programming environment (e.g. databases, knowledge graphs, etc.) to yield a result z, namely denotation. The structure of an MR takes a form of either a tree or graph, depending on its underlying formal language. The languages of MRs are categorized into three types of formalism : logic based (e.g. first order logic), graph based (e.g. AMR (Banarescu et al., 2013)), and programming languages (e.g. Java, Python) (Kamath and Das, 2018). Some semantic parsers explicitly apply a production grammar to yield MRs from utterances. Such a grammar consists of a set of production rules, which define a list of candidate derivations for each NL utterance. Each derivation deterministically produces a grammatically valid MR. 2.1 Semantic Parsing Models Given an utterance x ∈ X and its paired MR y ∈ Y, a C I SP model can form a conditional distribution p(y|x). The model learning can be supervised by either utterance-MR pairs or merely utterancedenotation pairs. If onl"
2020.coling-main.226,2020.acl-main.187,0,0.0378577,"Missing"
2020.coling-main.226,P14-1134,0,0.0255603,"es of a pair of utterance and derivation, and G(x) be the set of candidate derivations based on x. A widely used scoring model is the log linear model (Zettlemoyer and Collins, 2012; Kamath and Das, 2018). exp(θΦ(x, d)) 0 d0 ∈G(x) exp(θΦ(x, d )) p(d|x) = P (1) where θ denotes the model parameters. If only utterance-denotation pairs are provided at training time, a model marginalizes over all possible derivations yielding the same denotations by p(z|x) = P d p(z, d|x) (Krishnamurthy and Mitchell, 2012; Liang, 2016). Those corresponding parsers further differentiate between graph-based parsers (Flanigan et al., 2014; Zettlemoyer and Collins, 2012) and shift-reduce parsers (Zhao and Huang, 2014) due to the adopted parsing algorithms and the ways to generate derivations. From a machine learning perspective, these approaches are also linked to a structured prediction problem. 2510 Neural Approaches Neural approaches apply neural networks to translate NL utterances into MRs without using production grammars. These approaches formulate semantic parsing as a machine translation problem by viewing NL as the source language and the formal language of MRs as the target language. Most work in this category adopts"
2020.coling-main.226,P19-1082,0,0.0731315,"alization. In the second stage, another S EQ 2S EQ model is applied to fill the slot variables with the corresponding entities. Neural-Symbolic Approaches In order to ensure the generated MRs to be syntactically valid without compromising the generalization power of neural networks, neural-symbolic approaches fuse both symbolic and neural approaches by applying production grammars to the generated MRs; then the derivations are scored by neural networks. The majority of these methods linearize derivations such that they are able to leverage S EQ 2S EQ (Liang et al., 2016; Yin and Neubig, 2018; Guo et al., 2019b). At each time step, the decoder of these methods emits either a parse action or a production rule, leading to a grammatically valid MR at the end. these works produce derivations by varying grammars. NSM (Liang et al., 2016) uses a subset of Lisp syntax. T RAN X (Yin and Neubig, 2018) defines the grammars in Abstract Syntax Description Language, while IRN ET (Guo et al., 2019b) considers the context-free grammar of a language called SemQL. There are also neural-symbolic approaches adopting neural architectures other than S EQ 2S EQ. One of such examples is (Andreas et al., 2016), which adop"
2020.coling-main.226,P19-1444,0,0.0214372,"Missing"
2020.coling-main.226,P17-1097,0,0.0155623,"ly used by the neural dialogue models (Zhang et al., 2018), thus suffer from the same 2513 problems caused by composition complexity. As a result, the trained models are found insensitive to utterance order and word order in context (Sankar et al., 2019). MR encoders construct a neural context representation at time t based on the MRs predicted before t. As MRs are expressed in a formal language, MR encoders also apply RNNs to encode each MR or segments of MRs into embedding vectors. Then MR encoders build context representations of historical MRs in the same spirit as utterance encoders. In (Guu et al., 2017), they only concatenate the embeddings of k most recent history MR tokens as they assume current MR is always an extension of previous MRs. In (Suhr et al., 2018), a bidirectional RNN is applied to construct a vector for each segment, which is extracted from historical MRs. Soft attention is also applied in (Zhang et al., 2019) for building context vectors, which uses the current hidden state of their decoder as the query vector to attend over the token embeddings of the previous MR. Context-aware Decoders Decoders in C D SP models produce MRs based on the neural representations provided by th"
2020.coling-main.226,U19-1013,1,0.791014,"uhr et al., 2018; Suhr and Artzi, 2018). As a result, decoders have access to information in at most k utterances. However, this method fails to access information beyond the k utterances. In addition, it is computationally expensive because if an utterance belongs to multiple contexts, it would be repeatedly encoded for modelling all the contexts. • To overcome the above weakness, an alternative method is to treat a sub-sequence of utterances up to time t as a sequence of vectors, and project them into a discourse state vector by using a turn-level RNN (Suhr et al., 2018; Zhang et al., 2019; He et al., 2019). In another word, those models apply hierarchical RNNs to map each context into a fixed-size vector. In this method, each utterance is encoded only once and reused for modelling different contexts. However, this approach often leads to significant information loss (Pascanu et al., 2013; Khandelwal et al., 2018) due to the challenges imposed by encoding sequences of utterances into single vectors. • In order to focus on history utterances most relevant to current decoder states or utterances, soft attention (Bahdanau et al., 2014) is applied to construct context vectors. The query vectors are"
2020.coling-main.226,N18-2115,0,0.0118832,"f designated actions to copy entities, predicates and action subsequences from the memory respectively. Instead of decisively copying from memory, each type of action probabilistically selects the corresponding segments conditioning on the symbolic representations, which are later integrated into the generated action sequences. Guo et al. (2019a) employs the same neural-symbolic models as in (Guo et al., 2018) to capture contextual information. Different from other approaches, Guo et al. (2019a) adopts the meta-learning approach to improve the generation ability of C D SP models. Inspired by (Huang et al., 2018), Guo et al. (2019a) utilize the context from other interactions to guide the learning of C D SP over utterances within current interactions via MAML. Guo et al. (2019a) considers an input utterance xi and its context Ci as an instance. A context-aware retriever would retrieve instances, which are semantically close to the current instances, from other interactions. When learning model parameters, the retrieved instances and the current instances are considered as the support set and test set, respectively, and grouped as tasks as in the common MAML paradigm. 3.4 Comparison between Different C"
2020.coling-main.226,P17-1089,0,0.0588529,"Missing"
2020.coling-main.226,D18-1192,0,0.0159158,"beyond this utterance. With this definition, there are two types of context for semantic parsing, local context and global context. The local context for an utterance is the text and multimedia content surrounding it, which is meaningful only for this utterance. In plain texts, the concept of local context is also quite close to discourse, which is defined as a group of collocated, structured, coherent sentences (Parsing, 2009). In contrast, its global context is the information accessible to more than one utterance, including databases and external text corpora, images or class environment (Iyer et al., 2018). The content of local context varies for each NL utterance 2511 while the global context is always static. The work in our survey is only concerned with local context. Therefore, we always refer to ”local context” as ”context” in the following sections. Context provides additional information to resolve ambiguity and vagueness in current utterances. For semantic parsing, one type of ambiguity is caused by references in current utterances, which need to be resolved to previously mentioned objects and relations. References may include explicit or implicit lexical triggers, such as those in ”For"
2020.coling-main.226,P17-1167,0,0.188353,"rning. They take advantages from both the good context representation obtained by neural nets and reduced complexity of decoding due to the constraints introduced by grammars. In existing work, those approaches regard the generation of an MR as the prediction of a sequence of actions. Neural-symbolic methods normally take the same methods as the neural approaches to encode the contextual information. What differentiate them is the neural-symbolic could handle context by i) designing specific actions, and ii) utilizing symbolic context representations. The context specific actions proposed in (Iyyer et al., 2017; Sun et al., 2019; Liu et al., 2020) adopt copy mechanism to reuse the previous MRs. CAMP (Sun et al., 2019) include three actions to copy three different SQL clauses from precedent queries. Liu et al. (2020) allows copying of any actions or subtrees from precedent SQL queries. The subsequent action in (Iyyer et al., 2017) adds SQL conditions from the previous query into the current semantic parse to address the ellipsis problem. Different from other approaches, Iyyer et al. (2017) uses a DYN SP, which is in a similar neural network structure as the DNMN, instead of the S EQ 2S EQ to generate"
2020.coling-main.226,P18-1027,0,0.0148315,"tedly encoded for modelling all the contexts. • To overcome the above weakness, an alternative method is to treat a sub-sequence of utterances up to time t as a sequence of vectors, and project them into a discourse state vector by using a turn-level RNN (Suhr et al., 2018; Zhang et al., 2019; He et al., 2019). In another word, those models apply hierarchical RNNs to map each context into a fixed-size vector. In this method, each utterance is encoded only once and reused for modelling different contexts. However, this approach often leads to significant information loss (Pascanu et al., 2013; Khandelwal et al., 2018) due to the challenges imposed by encoding sequences of utterances into single vectors. • In order to focus on history utterances most relevant to current decoder states or utterances, soft attention (Bahdanau et al., 2014) is applied to construct context vectors. The query vectors are either the hidden state of an decoder (Suhr et al., 2018; He et al., 2019; Suhr and Artzi, 2018) or an utterance vector (Liu et al., 2020; Zhang et al., 2019). To differentiate between positional information, token embeddings of history utterance are concatenated with their position embeddings (Suhr et al., 2018"
2020.coling-main.226,D12-1069,0,0.145719,"histories in dialogues. The surrounding text varies significantly across different application scenarios. In a piece of free text, we refer to the surrounding text of a current utterance as its context. The context is different with respect to different utterances. In our sequel, we differentiate between context independent semantic parsing (C I SP) and context dependent semantic parsing (C D SP) by whether a corresponding parser utilizes context information. A knowledge base or a database (on which a MR is executed for the purpose of question answering) can be considered as context as well (Krishnamurthy and Mitchell, 2012; Liang, 2016). This type of context does not change with respect to the utterances. In this survey, we only consider the former kind of context which does vary with different utterances. D Q1 S1 Q2 S2 Q3 S3 Database about pets What are the different pet types? SELECT DISTINCT pettype FROM pets For each of those, what is the maximum age? SELECT max(pet age), pettype FROM pets GROUP BY pettype What about the average age? SELECT avg(pet age), pettype FROM pets GROUP BY pettype Table 1: An example of C D SP from SPAR C (Yu et al., 2019b), where each SQL query Si is the MR of the question Qi . The"
2020.coling-main.226,P14-1135,0,0.0190448,"nces within each interaction are around a topic described by the provided text. To collect SPAR C and S EQUENTIAL QA, crowd-workers are asked to raise questions to obtain the information that answers the questions sampled from other corpora (Pasupat and Liang, 2015; Yu et al., 2018). But the assumption for S EQUENTIAL QA is the answers of the current question must be the subset of answers from the last turn. In ATIS (Price, 1990), crowd-workers raise questions around the detailed scripts describing air travel planning scenarios. T EMP S TRUCTURE (Chen and Bunescu, 2019) and T IME E XPRESSION (Lee et al., 2014) particularly focused on addressing the temporal-related dependency. In T EMP S TRUCTURE, human users or the simulators raise natural language questions chronologically towards a knowledge base. The facts in the knowledge base are organized in time series. Therefore, the questions in T EMP S TRUCTURE are rich with time expressions. T IME E XPRESSION only annotate temporal mentions (text segments that describe time expressions) instead of complete questions. All the mentions are from the time expression-rich corpora. 2516 SCONE (Long et al., 2016) and CSQA (Saha et al., 2018) use semi-automatic"
2020.coling-main.226,P16-1138,0,0.322353,"arding the history utterances, MRs, denotations. Such a parser learns a mapping from a current utterance xi to an MR yi by πθ (xi , Ci ). 3.1 Symbolic Approaches Existing symbolic approaches formulate C D SP as a structured prediction problem by including contextual information into their feature models. Their models capture p(di |xi , Ci ) by including context as a condition. Both Zettlemoyer and Collins (2009) and Srivastava et al. (2017) divide the parsing process into two steps: i) generate initial parses using C I SP; ii) complete initial parses using contextual information. In contrast, Long et al. (2016) parses a sequence of utterances in one step. In all those work, symbolic features are used to represent contexts. In two-step approaches, Zettlemoyer and Collins (2009) and Srivastava et al. (2017) differ in the details of individual steps. In the first step, Zettlemoyer and Collins (2009) extends MRs with predicates representing references, while Srivastava et al. (2017) generates a set of context independent parses for each utterance. In the second step, Zettlemoyer and Collins (2009) collects possible derivations by applying three heuristic rules to replace references with entities in cont"
2020.coling-main.226,P15-1142,0,0.018358,"luding MRs, denotations, and dialogue acts. The system responses are usually annotated with the dialogue acts. We especially highlight those annotations that explicitly reflect contextual dependencies of utterances in the sequel. 4.1 Scenarios Single-party Scenarios In SPAR C (Yu et al., 2019b), S EQUENTIAL QA (Iyyer et al., 2017) and ATIS, the user utterances within each interaction are around a topic described by the provided text. To collect SPAR C and S EQUENTIAL QA, crowd-workers are asked to raise questions to obtain the information that answers the questions sampled from other corpora (Pasupat and Liang, 2015; Yu et al., 2018). But the assumption for S EQUENTIAL QA is the answers of the current question must be the subset of answers from the last turn. In ATIS (Price, 1990), crowd-workers raise questions around the detailed scripts describing air travel planning scenarios. T EMP S TRUCTURE (Chen and Bunescu, 2019) and T IME E XPRESSION (Lee et al., 2014) particularly focused on addressing the temporal-related dependency. In T EMP S TRUCTURE, human users or the simulators raise natural language questions chronologically towards a knowledge base. The facts in the knowledge base are organized in time"
2020.coling-main.226,H90-1020,0,0.405981,"ntextual dependencies of utterances in the sequel. 4.1 Scenarios Single-party Scenarios In SPAR C (Yu et al., 2019b), S EQUENTIAL QA (Iyyer et al., 2017) and ATIS, the user utterances within each interaction are around a topic described by the provided text. To collect SPAR C and S EQUENTIAL QA, crowd-workers are asked to raise questions to obtain the information that answers the questions sampled from other corpora (Pasupat and Liang, 2015; Yu et al., 2018). But the assumption for S EQUENTIAL QA is the answers of the current question must be the subset of answers from the last turn. In ATIS (Price, 1990), crowd-workers raise questions around the detailed scripts describing air travel planning scenarios. T EMP S TRUCTURE (Chen and Bunescu, 2019) and T IME E XPRESSION (Lee et al., 2014) particularly focused on addressing the temporal-related dependency. In T EMP S TRUCTURE, human users or the simulators raise natural language questions chronologically towards a knowledge base. The facts in the knowledge base are organized in time series. Therefore, the questions in T EMP S TRUCTURE are rich with time expressions. T IME E XPRESSION only annotate temporal mentions (text segments that describe tim"
2020.coling-main.226,P19-1004,0,0.0494361,"Missing"
2020.coling-main.226,D19-1248,0,0.0464578,"Missing"
2020.coling-main.226,P18-1193,0,0.0721188,"ations based on on historical MRs. Utterance encoders aim to embed rich information hidden in utterances into fixed-length representations, which provide contextual information in addition to current utterances for decoders. They apply first an RNN to map each utterance into a continuous vector of fixed-size. Then there are three ways to encode utterances in context into a fixed-size neural representation. • For each utterance in a dialogue, a straightforward method is to concatenate its previous k − 1 utterances with current utterance in order and encode them with the RNN (Suhr et al., 2018; Suhr and Artzi, 2018). As a result, decoders have access to information in at most k utterances. However, this method fails to access information beyond the k utterances. In addition, it is computationally expensive because if an utterance belongs to multiple contexts, it would be repeatedly encoded for modelling all the contexts. • To overcome the above weakness, an alternative method is to treat a sub-sequence of utterances up to time t as a sequence of vectors, and project them into a discourse state vector by using a turn-level RNN (Suhr et al., 2018; Zhang et al., 2019; He et al., 2019). In another word, thos"
2020.coling-main.226,N18-1203,0,0.121403,"ational Conference on Computational Linguistics, pages 2509–2521 Barcelona, Spain (Online), December 8-13, 2020 age”. This example shows also another challenge caused by elliptical (incomplete) utterances. The sentence “What about the average age?” alone misses information about the database table and the column pettype. The incomplete meaning needs to be complemented by the discourse context. Compared with C I SP, which usually assumes that the information within the utterance is complete, C D SP is expected to tackle challenges posed by involving context in the parsing process (Liang, 2016; Suhr et al., 2018; Zhang et al., 2019; Liu et al., 2020). In addition, tackling the above challenges provides us with more opportunities to inspect the linguistic phenomena which could influence semantic parsing. Our survey on C D SP fills the gap in the literature, as the recent surveys in the semantic parsing research mainly focus on C I SP (Kamath and Das, 2018; Zhu et al., 2019). This paper is organised as follows. We start with providing a brief and fundamental understanding of C I SP in §2. We then present a comprehensive organization of the recent advances in C D SP in §3. We discuss current C I SP task"
2020.coling-main.226,Q14-1042,0,0.314899,"fy the questions such that the sequence of questions would include contextual linguistic properties such as ambiguity, underspecification or coreference. It is worth mentioning that, with such method, CSQA includes the largest number of interactions until now, which is over 200k. Multi-party Scenarios Similar to the scenario of SPAR C, to obtain the answers to the questions sampled from S PIDER (Yu et al., 2018), the conversations in C O SQL (Yu et al., 2019a) are conducted between two human interlocutors, who play the roles of user and system, respectively. The dialogues in the S PACE B OOK (Vlachos and Clark, 2014) are under the scenarios formed by the routing requests. One human interlocutor pretends to be a tourist walking around Edinburgh while another interlocutor plays the role of a system responding to the tourist. The conversations in E MAIL D IALOGUE are between the human agent and an email assistant instead of two humans. 4.2 Context and Annotations The contextual linguistic phenomena in the C D SP corpora is quite close to the phenomena in the corpora of tasks such as document-level machine translation, question answering, dialogue system, etc.. However, in C D SP datasets, the contextual ling"
2020.coling-main.226,D19-1547,0,0.04139,"Missing"
2020.coling-main.226,D18-2002,0,0.0144356,"for a high-level generalization. In the second stage, another S EQ 2S EQ model is applied to fill the slot variables with the corresponding entities. Neural-Symbolic Approaches In order to ensure the generated MRs to be syntactically valid without compromising the generalization power of neural networks, neural-symbolic approaches fuse both symbolic and neural approaches by applying production grammars to the generated MRs; then the derivations are scored by neural networks. The majority of these methods linearize derivations such that they are able to leverage S EQ 2S EQ (Liang et al., 2016; Yin and Neubig, 2018; Guo et al., 2019b). At each time step, the decoder of these methods emits either a parse action or a production rule, leading to a grammatically valid MR at the end. these works produce derivations by varying grammars. NSM (Liang et al., 2016) uses a subset of Lisp syntax. T RAN X (Yin and Neubig, 2018) defines the grammars in Abstract Syntax Description Language, while IRN ET (Guo et al., 2019b) considers the context-free grammar of a language called SemQL. There are also neural-symbolic approaches adopting neural architectures other than S EQ 2S EQ. One of such examples is (Andreas et al.,"
2020.coling-main.226,D18-1425,0,0.347895,"e called SemQL. There are also neural-symbolic approaches adopting neural architectures other than S EQ 2S EQ. One of such examples is (Andreas et al., 2016), which adopts a dynamic neural module network (DNMN) to generate MRs. 2.2 Evaluation In semantic parsing, exact match accuracy is the most commonly used evaluation metric. With exact match accuracy, the parsing results are considered correct only when the output MR/denotations exactly match the string of the ground truth MR/denotations. One flaw of the evaluation metric is that some types of MRs (e.g., SQL) do not hold order constraints. Yu et al. (2018) proposed a metric set match accuracy to evaluate the semantic parsing performance over SQLs, which treats each SQL statement as a set of clauses and ignore their orders. Due to the variety of domains and languages over different datasets, it is difficult to measure all semantic parsing methods in a unified framework. To address this issue, Yu et al. (2018), Yu et al. (2019b) and Yu et al. (2019a) built different shared-task platforms with leaderboard for semantic parsing evaluation on the common datasets and consistent evaluation metrics. 3 Context Dependent Semantic Parsing Context dependent"
2020.coling-main.226,P19-1443,0,0.124237,"ering) can be considered as context as well (Krishnamurthy and Mitchell, 2012; Liang, 2016). This type of context does not change with respect to the utterances. In this survey, we only consider the former kind of context which does vary with different utterances. D Q1 S1 Q2 S2 Q3 S3 Database about pets What are the different pet types? SELECT DISTINCT pettype FROM pets For each of those, what is the maximum age? SELECT max(pet age), pettype FROM pets GROUP BY pettype What about the average age? SELECT avg(pet age), pettype FROM pets GROUP BY pettype Table 1: An example of C D SP from SPAR C (Yu et al., 2019b), where each SQL query Si is the MR of the question Qi . The utilization of context in semantic parsing imposes both challenges and opportunities. As shown in Table 1, one challenge is to resolve references, such as those in “For each of those, what is the maximum This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 2509 Proceedings of the 28th International Conference on Computational Linguistics, pages 2509–2521 Barcelona, Spain (Online), December 8-13, 2020 age”. This example shows also another"
2020.coling-main.226,P09-1110,0,0.150241,"ated utterances with the union set of their context as one interaction, I = (x, C), where x = [x1 , ..., xi , ..., xT ] and C = ∪Ti=1 Ci . Currently, most C D SP work focus on the research problems of context Ci regarding the history utterances, MRs, denotations. Such a parser learns a mapping from a current utterance xi to an MR yi by πθ (xi , Ci ). 3.1 Symbolic Approaches Existing symbolic approaches formulate C D SP as a structured prediction problem by including contextual information into their feature models. Their models capture p(di |xi , Ci ) by including context as a condition. Both Zettlemoyer and Collins (2009) and Srivastava et al. (2017) divide the parsing process into two steps: i) generate initial parses using C I SP; ii) complete initial parses using contextual information. In contrast, Long et al. (2016) parses a sequence of utterances in one step. In all those work, symbolic features are used to represent contexts. In two-step approaches, Zettlemoyer and Collins (2009) and Srivastava et al. (2017) differ in the details of individual steps. In the first step, Zettlemoyer and Collins (2009) extends MRs with predicates representing references, while Srivastava et al. (2017) generates a set of co"
2020.coling-main.226,C18-1206,0,0.0197215,"e et al., 2019; Suhr and Artzi, 2018) or an utterance vector (Liu et al., 2020; Zhang et al., 2019). To differentiate between positional information, token embeddings of history utterance are concatenated with their position embeddings (Suhr et al., 2018; He et al., 2019), which encode the positions of history utterances relative to the current utterances. This method reflects the observation that similar utterances tend to share relevant information, such as references of the same entities. Both discourse states and attended representations are also widely used by the neural dialogue models (Zhang et al., 2018), thus suffer from the same 2513 problems caused by composition complexity. As a result, the trained models are found insensitive to utterance order and word order in context (Sankar et al., 2019). MR encoders construct a neural context representation at time t based on the MRs predicted before t. As MRs are expressed in a formal language, MR encoders also apply RNNs to encode each MR or segments of MRs into embedding vectors. Then MR encoders build context representations of historical MRs in the same spirit as utterance encoders. In (Guu et al., 2017), they only concatenate the embeddings of"
2020.coling-main.302,D18-1103,0,0.0166509,"2018; Saleh et al., 2019; Kim et al., 2019). However, this is a one-to-one approach, which is not able to exploit models trained for multiple high-resource language-pairs for the target language-pair of interest. Furthermore, models transferred from different high-resource language-pairs may have complementary syntactic and/or semantic strengths, hence using a single model may be sub-optimal. Another appealing approach is multilingual NMT, whereby a single NMT model is trained by combining data from multiple high-resource and low-resource language-pairs (Johnson et al., 2017; Ha et al., 2016; Neubig and Hu, 2018). However, the performance of a multilingual NMT model is highly dependent on the types of languages used to train the model. Indeed, if languages are from very distant language families, they lead to negative transfer, causing low translation quality in the multilingual system compared to the counterparts trained on the individual language-pairs (Tan et al., 2019a; Oncevay et al., 2020). To address this problem, (Tan et al., 2019b) has proposed a knowledge distillation approach to effectively train a multilingual model, by selectively distilling the knowledge from individual teacher models to"
2020.coling-main.302,2020.emnlp-main.187,0,0.0109949,"timal. Another appealing approach is multilingual NMT, whereby a single NMT model is trained by combining data from multiple high-resource and low-resource language-pairs (Johnson et al., 2017; Ha et al., 2016; Neubig and Hu, 2018). However, the performance of a multilingual NMT model is highly dependent on the types of languages used to train the model. Indeed, if languages are from very distant language families, they lead to negative transfer, causing low translation quality in the multilingual system compared to the counterparts trained on the individual language-pairs (Tan et al., 2019a; Oncevay et al., 2020). To address this problem, (Tan et al., 2019b) has proposed a knowledge distillation approach to effectively train a multilingual model, by selectively distilling the knowledge from individual teacher models to the multilingual student model. However, still all the language pairs are trained in a single model with a blind contribution during training. This work is licensed under a Creative Commons Attribution 4.0 International Licence. //creativecommons.org/licenses/by/4.0/. Licence details: http: 3413 Proceedings of the 28th International Conference on Computational Linguistics, pages 3413–34"
2020.coling-main.302,N19-4009,0,0.0313717,"memory and train a single low-resource model (student) from scratch while using the weighted average of teachers’ probabilities based on their contribution weight. In order to make clear how different teachers contribute during training the student, we illustrate contribution weights of all teachers for first 30 iterations of different mini-batches during the training in Figure 2. Model configuration. All models are trained with Transformer architecture (Vaswani et al., 2017), with the model hidden size of 256, feed-forward hidden size of 1024, and 2 layers, implemented in Fairseq framework (Ott et al., 2019). We use the Adam optimizer (Kingma and Ba, 2015) and an inverse square root schedule with warm-up (maximum LR 0.0005). We apply dropout and label smoothing with a rate of 0.3 and 0.1 respectively. The source and target embeddings are shared and tied with the last layer. We train with half-precision floats on one V100 GPU, with at most 4028 tokens per batch. 1 http://www.loc.gov/standards/iso639-2/php/English list.php 3415 MT Task x!en Individual student ru 10.58 26.38 13.87 6.50 10.15 10.36 32.24 11.88 9.54 12.18 sl nb gl eu et Transfer Learning from (x!en) model de it es pl nl 14.09 32.77 17"
2020.coling-main.302,N18-2084,0,0.0623738,"started from 0.5 and gradually increased to 3 following the annealing function of (Bowman et al., 2015) in our experiments. Our approach is summarized in Algorithm 1 and Figure 1. 3 Experiments 3.1 Settings Data. We conduct our experiments on the European languages of IWSLT and TED datasets. The language pairs with more than 100K training data are considered as high-resource and the ones less than 15k are assumed as low-resource. The high-resource models are trained on IWSLT2014 (ru,de,it,pl,nl,esen). IWSLT 2014 MT task data (sl-en) (Cettolo et al., 2014), and TED talk data (gl/et/nb/eu-en) (Qi et al., 2018) are used as low-resource languages. Detail about the preprocessing step and the statistics of data and language codes based on ISO 639-1 standard1 are listed in Section 1.1 of Appendix A. Training configuration. Individual low-resource and high-resource NMT models are trained on the low-resource data. The first trained from scratch and the later by finetuning with the vanilla transformer architecture. For multilingual NMT, we train a single model with all high-resource and the up-sampled of low-resource language pairs by using a decoder language embedding layer to identify the type of languag"
2020.coling-main.302,D19-5631,1,0.8879,"Missing"
2020.coling-main.302,D19-1089,0,0.0481278,"Missing"
2020.coling-main.302,P16-1162,0,0.201138,"Missing"
2020.coling-main.395,2010.iwslt-papers.10,0,0.0357535,"earst (1997) has proposed the Texttiling algorithm which computes the cosine distance between the bag-of-word (BoW) vectors of adjacent sentences. Foltz et al. (1998) have proposed to replace the BoW vectors with topic vectors. Li et al. (2017) have learned topic embeddings with a self-supervised neural network. There is also a third group of COH metrics that are based solely in syntactic regularities (Smith et al., 2016) that have also shown to be effective at modelling textual coherence. Other metrics have been proposed to measure different discourse properties such as grammatical cohesion (Hardmeier and Federico, 2010; Miculicich and Popescu-Belis, 2017) and discourse connectives (Hajlaoui and PopescuBelis, 2013). 2.3 Reinforcement learning in NMT Researchers in NMT and other natural language generation tasks have used reinforcement learning (Sutton and Barto, 2018) techniques to train the models to maximize discrete sentence-level and documentlevel metrics as an alternative or a complement to the NLL. For example, Ranzato et al. (2016) have proposed training NMT systems targeting the BLEU score, showing consistent improvements with respect to strong baselines. In addition to training the model directly wi"
2020.coling-main.395,J97-1003,0,0.242264,".g. hyponyms, hypernyms) by using WordNet (Miller, 1998). Gong et al. (2015) have proposed a similar metric that uses lexical chains. For COH, mainly two types of metrics have been proposed: entity-based and topic-based. The former follow the Centering Theory (Grosz et al., 1995) which states that documents with a high frequency of the same salient entities are more coherent. An entity-based coherence metric was proposed by Barzilay and Lapata (2008). At their turn, topic-based metrics assume that a document is coherent when adjacent sentences are similar in topic and vocabulary. Accordingly, Hearst (1997) has proposed the Texttiling algorithm which computes the cosine distance between the bag-of-word (BoW) vectors of adjacent sentences. Foltz et al. (1998) have proposed to replace the BoW vectors with topic vectors. Li et al. (2017) have learned topic embeddings with a self-supervised neural network. There is also a third group of COH metrics that are based solely in syntactic regularities (Smith et al., 2016) that have also shown to be effective at modelling textual coherence. Other metrics have been proposed to measure different discourse properties such as grammatical cohesion (Hardmeier an"
2020.coling-main.395,P17-4012,0,0.0844076,"Missing"
2020.coling-main.395,C18-1050,0,0.0186773,"standard encoder-decoder architecture to more effectively account for the context from surrounding sentences. Jean et al. (2017) have introduced a dedicated attention mechanism for the previous source sentences. Multi-encoder approaches with hierarchical attention networks have been proposed to separately encode each of the context sentences before they are merged back into a single context vector in the decoder (Miculicich et al., 2018; Maruf et al., 2019a; Wang et al., 2017). These models have shown significant improvements over sentence-level NMT baselines on many different language pairs. Kuang et al. (2018) and Tu et al. (2018) have proposed using an external cache to store, respectively, a set of topical words or a set of previous hidden vectors. This information has proved to benefit the decoding step at limited additional computational cost. In turn, Maruf and Haffari (2018) have presented a model that incorporates two memory networks, one for the source and one for the target, to capture document-level interdependencies. For the inference stage, they have proposed an iterative decoding algorithm that incrementally refines the predicted translation. However, all the aforementioned models assu"
2020.coling-main.395,D17-1019,0,0.0413416,"Missing"
2020.coling-main.395,C00-1072,0,0.0212859,"ystems targeting the BLEU score, showing consistent improvements with respect to strong baselines. In addition to training the model directly with the evaluation function, they claim that this approach mollifies the exposure bias problem (Bengio et al., 2015). Expected risk minimization has been proposed as an alternative reinforcement learning-style training to maximize the sentence-level (Edunov et al., 2018; Shen et al., 2016) and the document-level (Saunders et al., 2020) BLEU scores. Paulus et al. (2018) have proposed a similar approach for summarization using ROUGE as the training loss (Lin and Hovy, 2000). Tebbifakhr et al. (2019) have used a similar objective function to improve the sentiment classification of translated sentences. Finally, Edunov et al. (2018) have presented a comprehensive comparison of reinforcement learning and structured prediction losses for NMT model training. 3 Baseline Models This section describes the baseline NMT models used in the experiments. In detail, subsection 3.1 recaps the standard sentence-level translation model while subsection 3.2 describes the recent, strong hierarchical baseline that we have augmented with discourse rewards. 3.1 Sentence-level NMT Our"
2020.coling-main.395,D15-1166,0,0.0523766,"nd three translation domains have shown that our training approach has been able to achieve more cohesive and coherent document translations than other competitive approaches, yet without compromising the faithfulness to the reference translation. In the case of the Zh-En language pair, our method has achieved an improvement of 2.46 percentage points (pp) in LC and 1.17 pp in COH over the runner-up, while at the same time improving 0.63 pp in BLEU score and 0.47 pp in FBERT . 1 Introduction The recent advances in neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) have provided the research community and the commercial landscape with effective translation models that can at times achieve near-human performance. However, this usually holds at phrase or sentence level. When using these models in larger units of text, such as paragraphs or documents, the quality of the translation may drop considerably in terms of discourse attributes such as lexical and stylistic consistency. In fact, document-level translation is still a very open and challenging problem. The sentences that make up a document are not unrelated pieces of text that"
2020.coling-main.395,P18-1118,1,0.833695,"ks have been proposed to separately encode each of the context sentences before they are merged back into a single context vector in the decoder (Miculicich et al., 2018; Maruf et al., 2019a; Wang et al., 2017). These models have shown significant improvements over sentence-level NMT baselines on many different language pairs. Kuang et al. (2018) and Tu et al. (2018) have proposed using an external cache to store, respectively, a set of topical words or a set of previous hidden vectors. This information has proved to benefit the decoding step at limited additional computational cost. In turn, Maruf and Haffari (2018) have presented a model that incorporates two memory networks, one for the source and one for the target, to capture document-level interdependencies. For the inference stage, they have proposed an iterative decoding algorithm that incrementally refines the predicted translation. However, all the aforementioned models assume that the model can implicitly learn the occurring discourse patterns. Moreover, the training objective is the standard negative log-likelihood (NLL) loss, 4468 which simply maximizes the probability of the reference target words in the sentence. Only one work these authors"
2020.coling-main.395,N19-1313,1,0.903288,"ve near-human performance. However, this usually holds at phrase or sentence level. When using these models in larger units of text, such as paragraphs or documents, the quality of the translation may drop considerably in terms of discourse attributes such as lexical and stylistic consistency. In fact, document-level translation is still a very open and challenging problem. The sentences that make up a document are not unrelated pieces of text that can be predicted independently; rather, a set of sequences linked together by complex underlying linguistics aspects, also known as the discourse (Maruf et al., 2019b; Jurafsky and Martin, 2019). The discourse of a document includes several properties such as grammatical cohesion (Halliday and Hasan, 2014), lexical cohesion (Halliday and Hasan, 2014), document coherence (Hobbs, 1979) and the use of discourse connectives (Kalajahi et al., 2012). Ensuring that the translation retain such linguistic properties is expected to significantly improve its overall readability and flow. However, due to the limitations of current decoder technology, NMT models are still bound to translate at sentence level. In order to capture the discourse properties of the source"
2020.coling-main.395,W17-4802,0,0.0127196,"Texttiling algorithm which computes the cosine distance between the bag-of-word (BoW) vectors of adjacent sentences. Foltz et al. (1998) have proposed to replace the BoW vectors with topic vectors. Li et al. (2017) have learned topic embeddings with a self-supervised neural network. There is also a third group of COH metrics that are based solely in syntactic regularities (Smith et al., 2016) that have also shown to be effective at modelling textual coherence. Other metrics have been proposed to measure different discourse properties such as grammatical cohesion (Hardmeier and Federico, 2010; Miculicich and Popescu-Belis, 2017) and discourse connectives (Hajlaoui and PopescuBelis, 2013). 2.3 Reinforcement learning in NMT Researchers in NMT and other natural language generation tasks have used reinforcement learning (Sutton and Barto, 2018) techniques to train the models to maximize discrete sentence-level and documentlevel metrics as an alternative or a complement to the NLL. For example, Ranzato et al. (2016) have proposed training NMT systems targeting the BLEU score, showing consistent improvements with respect to strong baselines. In addition to training the model directly with the evaluation function, they clai"
2020.coling-main.395,D18-1325,0,0.0724286,"ed an embedding of the entire document to the input, and shown promising results in English-French. Conversely, other document-level NMT approaches have proposed modifications to the standard encoder-decoder architecture to more effectively account for the context from surrounding sentences. Jean et al. (2017) have introduced a dedicated attention mechanism for the previous source sentences. Multi-encoder approaches with hierarchical attention networks have been proposed to separately encode each of the context sentences before they are merged back into a single context vector in the decoder (Miculicich et al., 2018; Maruf et al., 2019a; Wang et al., 2017). These models have shown significant improvements over sentence-level NMT baselines on many different language pairs. Kuang et al. (2018) and Tu et al. (2018) have proposed using an external cache to store, respectively, a set of topical words or a set of previous hidden vectors. This information has proved to benefit the decoding step at limited additional computational cost. In turn, Maruf and Haffari (2018) have presented a model that incorporates two memory networks, one for the source and one for the target, to capture document-level interdependen"
2020.coling-main.395,P02-1040,0,0.107364,"contextual information from surrounding sentences. Most document-level NMT approaches augment the model with multiple encoders, extra attention layers and memory caches to encode the surrounding sentences, and leave the model to implicitly learn the discourse attributes by simply minimizing a conventional NLL objective. The hope is that the model will spontaneously identify and retain the discourse patterns within the source document. Conversely, very little work has attempted to model the discourse attributes explicitly. Even the evaluation metrics typically used in translation such as BLEU (Papineni et al., 2002) are not designed to assess the discourse quality of the translated documents. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 4467 Proceedings of the 28th International Conference on Computational Linguistics, pages 4467–4482 Barcelona, Spain (Online), December 8-13, 2020 For these reasons, in this paper we propose training an NMT model by directly targeting two specific discourse metrics: lexical cohesion (LC) and coherence (COH). LC is a measure of the frequency of semantically-similar words"
2020.coling-main.395,W17-4702,0,0.0248128,"and Barto, 2018) which allows using any evaluation metric as a reward without having to differentiate it. By combining different types of rewards, the model can be trained to simultaneously achieve more lexicallycohesive and more coherent document translations, while at the same time retaining faithfulness to the reference translation. 2 2.1 Related Work Document-level NMT Many document-level NMT models have proposed taking the context into account by concatenating surrounding sentences or extra features to the current input sentence, with otherwise no modifications to the model. For example, Rios et al. (2017) have trained an NMT model that learns to disambiguate words given the context semantic landscape by simply extracting lexical chains from the source document, and using them as additional features. Other researchers have proposed concatenating previous source and target sentences to the current source sentence, so that the decoder can observe a proper amount of context (Agrawal et al., 2018; Tiedemann and Scherrer, 2017; Scherrer et al., 2019). Their work has shown that concatenating even just one or two previous sentences can result in a noticeable improvement. Mac´e and Servan (2019) have a"
2020.coling-main.395,2020.acl-main.693,0,0.020576,"-level and documentlevel metrics as an alternative or a complement to the NLL. For example, Ranzato et al. (2016) have proposed training NMT systems targeting the BLEU score, showing consistent improvements with respect to strong baselines. In addition to training the model directly with the evaluation function, they claim that this approach mollifies the exposure bias problem (Bengio et al., 2015). Expected risk minimization has been proposed as an alternative reinforcement learning-style training to maximize the sentence-level (Edunov et al., 2018; Shen et al., 2016) and the document-level (Saunders et al., 2020) BLEU scores. Paulus et al. (2018) have proposed a similar approach for summarization using ROUGE as the training loss (Lin and Hovy, 2000). Tebbifakhr et al. (2019) have used a similar objective function to improve the sentiment classification of translated sentences. Finally, Edunov et al. (2018) have presented a comprehensive comparison of reinforcement learning and structured prediction losses for NMT model training. 3 Baseline Models This section describes the baseline NMT models used in the experiments. In detail, subsection 3.1 recaps the standard sentence-level translation model while"
2020.coling-main.395,D19-6506,0,0.0353223,"Missing"
2020.coling-main.395,P16-1159,0,0.0161732,"in the models to maximize discrete sentence-level and documentlevel metrics as an alternative or a complement to the NLL. For example, Ranzato et al. (2016) have proposed training NMT systems targeting the BLEU score, showing consistent improvements with respect to strong baselines. In addition to training the model directly with the evaluation function, they claim that this approach mollifies the exposure bias problem (Bengio et al., 2015). Expected risk minimization has been proposed as an alternative reinforcement learning-style training to maximize the sentence-level (Edunov et al., 2018; Shen et al., 2016) and the document-level (Saunders et al., 2020) BLEU scores. Paulus et al. (2018) have proposed a similar approach for summarization using ROUGE as the training loss (Lin and Hovy, 2000). Tebbifakhr et al. (2019) have used a similar objective function to improve the sentiment classification of translated sentences. Finally, Edunov et al. (2018) have presented a comprehensive comparison of reinforcement learning and structured prediction losses for NMT model training. 3 Baseline Models This section describes the baseline NMT models used in the experiments. In detail, subsection 3.1 recaps the s"
2020.coling-main.395,W16-3407,0,0.0247027,"tric was proposed by Barzilay and Lapata (2008). At their turn, topic-based metrics assume that a document is coherent when adjacent sentences are similar in topic and vocabulary. Accordingly, Hearst (1997) has proposed the Texttiling algorithm which computes the cosine distance between the bag-of-word (BoW) vectors of adjacent sentences. Foltz et al. (1998) have proposed to replace the BoW vectors with topic vectors. Li et al. (2017) have learned topic embeddings with a self-supervised neural network. There is also a third group of COH metrics that are based solely in syntactic regularities (Smith et al., 2016) that have also shown to be effective at modelling textual coherence. Other metrics have been proposed to measure different discourse properties such as grammatical cohesion (Hardmeier and Federico, 2010; Miculicich and Popescu-Belis, 2017) and discourse connectives (Hajlaoui and PopescuBelis, 2013). 2.3 Reinforcement learning in NMT Researchers in NMT and other natural language generation tasks have used reinforcement learning (Sutton and Barto, 2018) techniques to train the models to maximize discrete sentence-level and documentlevel metrics as an alternative or a complement to the NLL. For"
2020.coling-main.395,stefanescu-etal-2014-latent,0,0.0230556,"aum, 2012) has been used to classify the relationships between words. Note that this reward function is unsupervised since it does not require a ground-truth reference translation. LC = # of cohesion devices in document # of words in document (5) COHdoc : To calculate COH, we have used the approach proposed by Foltz et al. (1998). This approach first uses a trained LSA model to infer topic vectors (ti ) for each sentence in the document, and then computes the average cosine distance between adjacent sentences (Eq. 6). For the topic vectors, we have used the pre-trained LSA model (Wiki-6) from Stefanescu et al. (2014), which was trained over Wikipedia. Note that COH also does not require a ground-truth reference translation. k COH = 1 X cos(ti , ti−1 ) k−1 (6) i=2 BLEUdoc : In addition to the LC and COH rewards, we have decided to use a reference-based metric such as BLEU (Papineni et al., 2002). Due to the unsupervised nature of LC and COH, the model could trivially boost them by only repeating words and creating very similar sentences. However, this will come at the expense of producing translations that are increasingly unrelated to the reference translation (low adequacy) and grammatically incorrect (l"
2020.coling-main.395,D19-1140,0,0.0196964,"BLEU score, showing consistent improvements with respect to strong baselines. In addition to training the model directly with the evaluation function, they claim that this approach mollifies the exposure bias problem (Bengio et al., 2015). Expected risk minimization has been proposed as an alternative reinforcement learning-style training to maximize the sentence-level (Edunov et al., 2018; Shen et al., 2016) and the document-level (Saunders et al., 2020) BLEU scores. Paulus et al. (2018) have proposed a similar approach for summarization using ROUGE as the training loss (Lin and Hovy, 2000). Tebbifakhr et al. (2019) have used a similar objective function to improve the sentiment classification of translated sentences. Finally, Edunov et al. (2018) have presented a comprehensive comparison of reinforcement learning and structured prediction losses for NMT model training. 3 Baseline Models This section describes the baseline NMT models used in the experiments. In detail, subsection 3.1 recaps the standard sentence-level translation model while subsection 3.2 describes the recent, strong hierarchical baseline that we have augmented with discourse rewards. 3.1 Sentence-level NMT Our first baseline is a stand"
2020.coling-main.395,W17-4811,0,0.0242903,"oposed taking the context into account by concatenating surrounding sentences or extra features to the current input sentence, with otherwise no modifications to the model. For example, Rios et al. (2017) have trained an NMT model that learns to disambiguate words given the context semantic landscape by simply extracting lexical chains from the source document, and using them as additional features. Other researchers have proposed concatenating previous source and target sentences to the current source sentence, so that the decoder can observe a proper amount of context (Agrawal et al., 2018; Tiedemann and Scherrer, 2017; Scherrer et al., 2019). Their work has shown that concatenating even just one or two previous sentences can result in a noticeable improvement. Mac´e and Servan (2019) have added an embedding of the entire document to the input, and shown promising results in English-French. Conversely, other document-level NMT approaches have proposed modifications to the standard encoder-decoder architecture to more effectively account for the context from surrounding sentences. Jean et al. (2017) have introduced a dedicated attention mechanism for the previous source sentences. Multi-encoder approaches wi"
2020.coling-main.395,Q18-1029,0,0.0197168,"architecture to more effectively account for the context from surrounding sentences. Jean et al. (2017) have introduced a dedicated attention mechanism for the previous source sentences. Multi-encoder approaches with hierarchical attention networks have been proposed to separately encode each of the context sentences before they are merged back into a single context vector in the decoder (Miculicich et al., 2018; Maruf et al., 2019a; Wang et al., 2017). These models have shown significant improvements over sentence-level NMT baselines on many different language pairs. Kuang et al. (2018) and Tu et al. (2018) have proposed using an external cache to store, respectively, a set of topical words or a set of previous hidden vectors. This information has proved to benefit the decoding step at limited additional computational cost. In turn, Maruf and Haffari (2018) have presented a model that incorporates two memory networks, one for the source and one for the target, to capture document-level interdependencies. For the inference stage, they have proposed an iterative decoding algorithm that incrementally refines the predicted translation. However, all the aforementioned models assume that the model can"
2020.coling-main.395,D17-1301,0,0.0206396,"input, and shown promising results in English-French. Conversely, other document-level NMT approaches have proposed modifications to the standard encoder-decoder architecture to more effectively account for the context from surrounding sentences. Jean et al. (2017) have introduced a dedicated attention mechanism for the previous source sentences. Multi-encoder approaches with hierarchical attention networks have been proposed to separately encode each of the context sentences before they are merged back into a single context vector in the decoder (Miculicich et al., 2018; Maruf et al., 2019a; Wang et al., 2017). These models have shown significant improvements over sentence-level NMT baselines on many different language pairs. Kuang et al. (2018) and Tu et al. (2018) have proposed using an external cache to store, respectively, a set of topical words or a set of previous hidden vectors. This information has proved to benefit the decoding step at limited additional computational cost. In turn, Maruf and Haffari (2018) have presented a model that incorporates two memory networks, one for the source and one for the target, to capture document-level interdependencies. For the inference stage, they have"
2020.coling-main.395,D12-1097,0,0.443218,"he model by explicitly learning discourse attributes. Inspired by recent work in text generation (Bosselut et al., 2018), Xiong et al. (2019) have proposed automatically learning neural rewards that can encourage translation coherence at document level. However, it is not clear whether the learned rewards would be in good correspondence with human judgment. For this reason, in our work we prefer to rely on established discourse metrics as rewards. 2.2 Discourse evaluation metrics As a matter of fact, several metrics have been proposed in the literature to measure discourse properties. For LC, Wong and Kit (2012) have proposed a metric that looks for repetitions of words and their related terms (e.g. hyponyms, hypernyms) by using WordNet (Miller, 1998). Gong et al. (2015) have proposed a similar metric that uses lexical chains. For COH, mainly two types of metrics have been proposed: entity-based and topic-based. The former follow the Centering Theory (Grosz et al., 1995) which states that documents with a high frequency of the same salient entities are more coherent. An entity-based coherence metric was proposed by Barzilay and Lapata (2008). At their turn, topic-based metrics assume that a document"
2020.coling-main.467,P19-1470,0,0.326094,"obable answer. Knowledge-based approaches to such commonsense question-answering (QA) require an inferential knowledge base. ATOMIC (Sap et al., 2019a) is the largest commonsense knowledge base of this kind, which contains 300,000 short textual description of events and 877,000 typed if-then relations between events, categorised into 9 dimensions. For instance, IF the event “X puts trust in Y” occurs and the target relation is “xWant”, THEN “X wants to develop a relationship”. Prior work utilizes knowledge in ATOMIC by formulating the learning problem as event prediction in if-then relations (Bosselut et al., 2019). In particular, they encode the textual description of an event and a relation into an embedding, and maximise the probability of predicting the description of the associated event or characteristic of an involved person. However, due to the nature of commonsense knowledge, given an event and a relation, there are multiple plausible associated events. Moreover, these models fail to predict all associated events This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http:// creativecommons.org/licenses/by/4.0/. 5347 Proceedings of the 28th Intern"
2020.coling-main.467,W14-3346,0,0.0110222,"of our proposed model. As a quantitative evaluation, for a set of clause generated by the model denoted as {ˆ y }M m=1 , we use div bleu and div ngram (He et al., 2018b), which are defined as follow: 5353 M X M X div bleu = 1 − ( BLEU(ˆ yi , yˆj ))/M (M − 1)/2 (8) i=1 j=i+1 div ngram = 1 − |∩M m )| m=1 ngram(yˆ M |∪m=1 ngram(yˆm )| (9) where ngram(y) indicates the set of unique ngrams in a sequence y. For each model the top-50 clause generated by beam search is considered for evaluation purposes. For div bleu we report the average result of BLEU-1, BLEU-2, BLEU-3, and BLEU-4, with Smoothing1 (Chen and Cherry, 2014). For div ngram, we report the average results of 1-gram, 2-gram, 3-gram, and 4-gram. 4.4 Experimental Details For training our proposed Knowledge Graph Neuralisation model, C OS M O 2 , we finetune the S EQ 2S EQ model using the pretrained model of Yan (2020). Our implementation is based on FAIRSEQ 3 . The model consists of 12 layers of encoder and 12 layers of decoder. The embedding size and batch size are set to 1024 and 512, respectively. The number of future ngram is set to 2. We use Adam optimiser (Kingma and Ba, 2014) with a peak learning rate of 1 × 10−4 . For the question answering mo"
2020.coling-main.467,D19-1308,0,0.0169978,"encoder and the decoder of this model utilize n-stream self-attention mechanism and future n-gram prediction in order to encourage planning for the future tokens and prevent overfitting on strong local correlations. Training The goal of training is to learn the parameters of the following model on ATOMIC, Y max θB K X P r(z|hk , x, r; θB )P r(hk |x). (5) r(x,z)∈B k=1 More specifically, we train the model on each if-then relation of the form “if x and r then z” in ATOMIC by taking x and r as input and predicting z. Prior work on diverse machine translation (He et al., 2018a; Shen et al., 2019; Cho et al., 2019) suggests to apply online hard EM by interleaving the following two steps for each mini-batch. • E-step: estimate the value of the latent variable through kˆ = arg maxk P r(z|hk , x, r; θB ) using the current parameters θB . • M-step: The model parameters θB are then updated by minimising the cross-entropy loss on P r(z|hkˆ , x; θB ). However, the greedy search in the E-step may still assign the same latent variable value to different target sequences. To eliminate the problem, we modify and constrain the E-Step by requiring that, different target sequences of the same input need to be assigne"
2020.coling-main.467,K18-1056,1,0.90097,"tural language generation tasks. The encoder and the decoder of this model utilize n-stream self-attention mechanism and future n-gram prediction in order to encourage planning for the future tokens and prevent overfitting on strong local correlations. Training The goal of training is to learn the parameters of the following model on ATOMIC, Y max θB K X P r(z|hk , x, r; θB )P r(hk |x). (5) r(x,z)∈B k=1 More specifically, we train the model on each if-then relation of the form “if x and r then z” in ATOMIC by taking x and r as input and predicting z. Prior work on diverse machine translation (He et al., 2018a; Shen et al., 2019; Cho et al., 2019) suggests to apply online hard EM by interleaving the following two steps for each mini-batch. • E-step: estimate the value of the latent variable through kˆ = arg maxk P r(z|hk , x, r; θB ) using the current parameters θB . • M-step: The model parameters θB are then updated by minimising the cross-entropy loss on P r(z|hkˆ , x; θB ). However, the greedy search in the E-step may still assign the same latent variable value to different target sequences. To eliminate the problem, we modify and constrain the E-Step by requiring that, different target sequenc"
2020.coling-main.467,P19-1484,0,0.0178417,"estion Answering (Teney and Hengel, 2016) adopted a zero-shot learning approach to extract features from unseen text description about given images. Lewis (2019) proposed an unsupervised extractive question answering model by using unsupervised data generation for converting the QA task to a cloze translation task. 5349 Puri (2020) improved the quality of unsupervised extractive question answering systems by introducing an approach involving answer generation, question generation and roundtrip filtration. In addition, Li et al. (2020) used Wikipedia’s data in order to overcome the drawback of Lewis et al. (2019). Compared to the previous question answering model, our model shows the novelty in generation new clauses from given contexts. 3 Our Approach In this section, we present our model C OS M O for question answering in the zero-shot setting. In this setting, there is no training data for the QA task, thus we train our model only on ATOMIC to acquire inferential knowledge. We apply the trained model to answer multi-choice questions on everyday inferential knowledge by augmenting the trained model with a non-parametric answer scoring module. Formally, given a narrative describing a context c, a com"
2020.coling-main.467,2020.acl-main.600,0,0.0206055,"ach has inspired many works in question answering systems as well. Visual Question Answering (Teney and Hengel, 2016) adopted a zero-shot learning approach to extract features from unseen text description about given images. Lewis (2019) proposed an unsupervised extractive question answering model by using unsupervised data generation for converting the QA task to a cloze translation task. 5349 Puri (2020) improved the quality of unsupervised extractive question answering systems by introducing an approach involving answer generation, question generation and roundtrip filtration. In addition, Li et al. (2020) used Wikipedia’s data in order to overcome the drawback of Lewis et al. (2019). Compared to the previous question answering model, our model shows the novelty in generation new clauses from given contexts. 3 Our Approach In this section, we present our model C OS M O for question answering in the zero-shot setting. In this setting, there is no training data for the QA task, thus we train our model only on ATOMIC to acquire inferential knowledge. We apply the trained model to answer multi-choice questions on everyday inferential knowledge by augmenting the trained model with a non-parametric a"
2020.coling-main.467,D19-1282,0,0.0142791,"on transfer learning, where a large scale pre-trained model (Lan et al., 2019; Devlin et al., 2018) is finetuned on the target task(Sap et al., 2019b). On the other hand, with the release of new commonsense KGs, such as ATOMIC (Sap et al., 2019a) and ConceptNet (Speer et al., 2017), the possibility of enriching language models with these KGs has been investigated vastly (Lv et al., 2020; Mitra et al., 2019; Banerjee and Baral, 2020). Most of these approaches map the context of a question to an entity/event in KG and perform reasoning on the KG (Weissenborn et al., 2017; Paul and Frank, 2019; Lin et al., 2019). While these methods enable the system to perform multi-hop reasoning, they are limited to the set of entities in KG. Other works deployed generative models to generate context-aware information to answer the questions (Shwartz et al., 2020; Bosselut and Choi, 2019; Banerjee and Baral, 2020). These approaches overcome the limitation of static KGs, but they lack diversity in generating new relations, which makes their inference path limited. Furthermore, for scoring the answer choices they have solely relied on the conditional likelihood of the generative model, which lack to perform when the"
2020.coling-main.467,N19-1368,0,0.0281387,"e approaches are based on transfer learning, where a large scale pre-trained model (Lan et al., 2019; Devlin et al., 2018) is finetuned on the target task(Sap et al., 2019b). On the other hand, with the release of new commonsense KGs, such as ATOMIC (Sap et al., 2019a) and ConceptNet (Speer et al., 2017), the possibility of enriching language models with these KGs has been investigated vastly (Lv et al., 2020; Mitra et al., 2019; Banerjee and Baral, 2020). Most of these approaches map the context of a question to an entity/event in KG and perform reasoning on the KG (Weissenborn et al., 2017; Paul and Frank, 2019; Lin et al., 2019). While these methods enable the system to perform multi-hop reasoning, they are limited to the set of entities in KG. Other works deployed generative models to generate context-aware information to answer the questions (Shwartz et al., 2020; Bosselut and Choi, 2019; Banerjee and Baral, 2020). These approaches overcome the limitation of static KGs, but they lack diversity in generating new relations, which makes their inference path limited. Furthermore, for scoring the answer choices they have solely relied on the conditional likelihood of the generative model, which lack t"
2020.coling-main.467,2020.emnlp-main.468,0,0.0251336,"Missing"
2020.coling-main.467,D19-1454,0,0.430909,"Choi, 2019). Such systems are often evaluated by answering questions based on narratives. As illustrated in Figure 1, given a narrative “Austin often spends her weekend at the lake fishing with friends” and a question regarding the intention of Austin, an AI system is supposed to associate this event to relevant events in an inferential knowledge base or a web-scale corpus, find plausible reasons of those events, and conclude that “wanted to relax” is the most probable answer. Knowledge-based approaches to such commonsense question-answering (QA) require an inferential knowledge base. ATOMIC (Sap et al., 2019a) is the largest commonsense knowledge base of this kind, which contains 300,000 short textual description of events and 877,000 typed if-then relations between events, categorised into 9 dimensions. For instance, IF the event “X puts trust in Y” occurs and the target relation is “xWant”, THEN “X wants to develop a relationship”. Prior work utilizes knowledge in ATOMIC by formulating the learning problem as event prediction in if-then relations (Bosselut et al., 2019). In particular, they encode the textual description of an event and a relation into an embedding, and maximise the probability"
2020.coling-main.467,2020.emnlp-main.373,0,0.104474,"Missing"
2020.coling-main.467,2020.findings-emnlp.217,0,0.0549383,"Missing"
2020.coling-main.467,D18-1009,0,0.0236037,"(2020) developed a model which takes both structural and semantic characteristics of the nodes in a KG to address this task. On the other hand, some works have developed generative models on top of pre-trained language models to extract new commonsense information (Bosselut et al., 2019; Malaviya et al., 2019). However, when adapting to the KG, the previously acquired knowledge of these models is forgotten. Unlike these approaches, our neural model ensures both diversity and memorisation. Commonsense Question Answering Recent surge of commonsense question answering dataset (Sap et al., 2019b; Zellers et al., 2018) has led to many supervised approaches to address this task. Most of these approaches are based on transfer learning, where a large scale pre-trained model (Lan et al., 2019; Devlin et al., 2018) is finetuned on the target task(Sap et al., 2019b). On the other hand, with the release of new commonsense KGs, such as ATOMIC (Sap et al., 2019a) and ConceptNet (Speer et al., 2017), the possibility of enriching language models with these KGs has been investigated vastly (Lv et al., 2020; Mitra et al., 2019; Banerjee and Baral, 2020). Most of these approaches map the context of a question to an entit"
2020.coling-main.467,P19-1150,0,0.014726,"hese approaches overcome the limitation of static KGs, but they lack diversity in generating new relations, which makes their inference path limited. Furthermore, for scoring the answer choices they have solely relied on the conditional likelihood of the generative model, which lack to perform when the distribution of KG and QA differ. Zero-shot Question Answering In recent years, zero-shot learning has become a popular method to conquer the inability of machine learning system to perform on unobserved data (Wang et al., 2019). While this method has been thoroughly researched in other fields (Zhao et al., 2019), the necessity of using such approach has inspired many works in question answering systems as well. Visual Question Answering (Teney and Hengel, 2016) adopted a zero-shot learning approach to extract features from unseen text description about given images. Lewis (2019) proposed an unsupervised extractive question answering model by using unsupervised data generation for converting the QA task to a cloze translation task. 5349 Puri (2020) improved the quality of unsupervised extractive question answering systems by introducing an approach involving answer generation, question generation and"
2020.emnlp-main.469,D13-1160,0,0.151782,"Missing"
2020.emnlp-main.469,P19-1082,0,0.0781258,"oups. Therefore, action sequences relevant to different groups may have significant deviations, and it is hard to learn a one-size-fits-all model that could adapt to varied types of questions. An exception is (Guo et al., 2019), which proposes a few-shot learning approach, i.e., S2A, to solve the CQA problem with a retriever and a meta-learner. The retriever selects similar instances from the training dataset to form tasks, and the meta-learner is trained on these tasks to learn how to quickly adapt to a new task created by the target question of interest at the test time. However, Guo et al. (2019) make use of teacher forcing within the learning by demonstration approach, which suffers from the aforementioned drawbacks. Also, though S2A is the most similar to ours, the tasks are very different. S2A aims to answer context-dependent questions, where each question is part of a multiple-turn conversation. On the contrary, we consider the different task where the questions are single-turn and have no context. Thus, a novel challenge arises in retrieving accurate support sets without conversationbased context information. In this paper, we propose a Meta-RL approach for CQA (MRL-CQA), where t"
2020.emnlp-main.469,P17-1097,0,0.0196079,"ced by θ 0 . The reward of K 0 trajectories are considered as the evaluation of the adapted policy θ 0 for the given task Tpse ; thus we have the objective, def J(θ 0 ) = Eτ 0 ∼π(τ 0 |qmeta ;θ0 ) [R(τ 0 )] (4) The parameter of the generic policy θ are then trained by maximising the objective J(θ 0 ), θ ← θ + η2 ∇θ J(θ 0 ) (5) In each VPG step, since we have N samples in sqmeta , we use N policy gradient adaptation steps to update θ 0 . Meanwhile, we use one policy gradient step to optimize θ based on the evaluation of θ 0 . Monte Carlo integration is used as the approximation strategy in VPG (Guu et al., 2017). We summarise the meta-learning approach in Alg.1. When making inferences, for each question qtest , the retriever creates a pseudo-task, similar to the meta-learning process. The top-N similar questions to qtest form the support set sqtest , and are 0 used to obtain the adapted model θ ∗ , starting from the meta learned policy θ ∗ . The adapted model is then used to generate the program and compute the target question’s final answer. 5830 pervised way. Suppose there is a set of i words {w11 , ..., w1i } in q1 and j words {w21 , ..., w2j } in q2 , and word similarity sim(wi , wj ) is calculat"
2020.emnlp-main.469,N18-2115,0,0.0182885,"y Antoinette Sandbach? Program Induction from Terminal Rewards (CIPITR) (Saha et al., 2019) relies on auxiliary awards, KB schema, and inferred answer types for training an NPI model to solve the CQA task. However, CIPITR separately trains one model for each category of questions with a different difficulty level. Compared with the NPI models, our model can flexibly adapt to the question under processing. Meta-learning. Meta-learning, aka learningto-learn, aims to make learning a new task more effective based on the inductive biases that are meta-learned in learning similar tasks in the past. Huang et al. (2018) use MAML to learn a Seq2Seq model to convert questions in WikiSQL into SQL queries. More closely related to our work, Guo et al. (2019) propose Sequence-to-Action (S2A) by using MAML to solve CQA problems. They label all the examples in training set with pseudo-gold annotations, then train an encoder-decoder model to retrieve relevant samples and a Seq2Seq based semantic parser to generate actions based on the annotations. Unlike S2A, we introduce a Meta-RL approach, which uses RL to train an NPI model without annotating questions in advance. 5 Conclusion CQA refers to answering complex natur"
2020.emnlp-main.469,P17-1003,0,0.282586,"20, 2020. 2020 Association for Computational Linguistics breadth-first search (BFS), to find a sequence of actions whose execution would yield the correct answer. This pseudo-gold annotation is then used to train the programmer using teacher forcing, aka behaviour cloning. However, BFS inevitably produces a single annotation and is ignorant to many other plausible annotations yielding the correct answer. To alleviate this issue, a second approach was proposed based on reinforcement learning (RL) to use the search policy prescribed by the programmer (Hua et al., 2020; Neelakantan et al., 2016; Liang et al., 2017). Compared to BFS which is a blind search algorithm, the RL-trained programmer can be regarded as an informed search algorithm for target programs. Therefore, the RL policy not only addresses the limitation of the 1-to-1 mapping between the questions and annotations, but also produces reasonable programs faster than BFS. The conventional approach to CQA is to train one model to fit the entire training set, and then use it for answering all complex questions at the test time. However, such a one-size-fits-all strategy is sub-optimal as the test questions may have diversity due to their inherent"
2020.emnlp-main.469,P16-1003,0,0.053957,"Missing"
2020.emnlp-main.469,D19-1248,0,0.079502,"nswer is then generated by counting the entities in the intersection of the two sets. More concretely, the question is transformed into the action sequence “Select (China, flow, river), Intersection (India, flow, river), Count”, which is executed on the KB to yield the answer. As such, the CQA task results in a massive search space beyond just entities in the KB and includes (lists of) Boolean values and integers. Multi-hop questions only require the join operator. In contrast, CQA requires various types of additional symbolic reasoning, e.g., logical, comparative, and quantitative reasoning (Shen et al., 2019; Ansari et al., 2019), where a more diverse array of complex queries is involved (Saha et al., 2019). The massive search space and complex queries make CQA considerably challenging and more complicated than multihop question answering. Due to the difficulty of collecting annotations, the existing CQA dataset (Saha et al., 2018) only contains the denotations for each question. The literature takes two approaches to deal with the missing annotations. The first approach aims to transform learning a CQA model into learning by demonstration, aka imitation learning, where a pseudo-gold action seque"
2020.emnlp-main.469,2020.emnlp-demos.6,0,0.0790198,"Missing"
2020.emnlp-main.469,P14-2105,0,0.0530736,"Missing"
2020.emnlp-main.469,P16-2033,0,0.0117551,"stion-answering (KBQA) interrogates a knowledge-base (KB) (Yin et al., 2016; Yu et al., 2017; Jin et al., 2019) by interpreting natural-language questions as logical forms (annotations), which can be directly executed on the KB to yield answers (denotations) (Pasupat and Liang, 2016). KBQA includes simple questions that retrieve answers from single-hop triples (“what is Donald Trump’s nationality”) (Berant et al., 2013; Yih et al., 2014), multi-hop questions that infer answers over triple chains of at least 2 hops under specific constraints (“who is the president of the European Union 2012”) (Yih et al., 2016; Liang ∗ Corresponding Author. et al., 2017), and complex questions that involve set operations (“how many rivers flow through India and China”) (Saha et al., 2019). In particular, complex question answering (CQA) (Saha et al., 2018) is a sophisticated KBQA task in which a sequence of discrete actions—e.g., set intersection and union, counting, comparison—needs to be executed, and is the subject of this paper. Consider the complex question “How many rivers flow through India and China?”. We first form a set of entities whose type is river and flow in China from the KB. We then form another se"
2020.emnlp-main.469,C16-1164,0,0.048734,"Missing"
2020.emnlp-main.469,P17-1053,0,0.0326765,"Missing"
2020.emnlp-main.497,W19-1909,0,0.0535808,"Missing"
2020.emnlp-main.497,D18-1316,0,0.0387267,"Missing"
2020.emnlp-main.497,N19-1054,0,0.0136014,"-discriminative representations (Shen et al., 2018). Inspired by the Generative Adversarial Network (GAN) (Goodfellow et al., 2014), the adversarial-based methods learn a representation that is discriminative for the target task and indiscriminative to the shift between the domains (Ganin and Lempitsky, 2015). Domain Adaptation with MLM. Performance of fine-tuned MLM can deteriorate substantially on the presence of domain mismatch. The most straightforward domain adaptation approach in MLM is to adapt general contextual embedding to a specific domain (Lee et al., 2020; Alsentzer et al., 2019; Chakrabarty et al., 2019), that is to further improve pretrained MLM by continuing to pretrain language models on related domain or similar tasks (Gururangan et al., 2020), or via intermediate task which is also referred to as STILTs (Phang et al., 2018). Recent works have proposed twostep adaptive domain adaptation framework which consists of domain tuning and task finetuning (Ma et al., 2019; Xu et al., 2019; Wang et al., 2019c; Logeswaran et al., 2019). They have demonstrated that domain tuning is necessary to adapt MLM with both domain knowledge and task knowledge before finetuning, especially when the labelled da"
2020.emnlp-main.497,W04-1213,0,0.0662496,"a sampling path is returned as l in Algorithm 2, which is then used for backpropagation. 5 Experiments We evaluate our proposed masking strategy in UDA for named entity span prediction tasks coming from three different domains. 5.1 Unsupervised Domain Adaptation Tasks Source and Target Domain Tasks. Our evaluation is focused on the problem of identifying named entity spans in domain-specific text without access to labeled data. The evaluation tasks comes from several named entity recognition (NER) dataset including WNUT2016 (Strauss et al., 2016), FIN (Salinas Alvarado et al., 2015), JNLPBA (Collier and Kim, 2004), BC2GM (Smith et al., 2008), BioNLP09 (Kim et al., 2009), and BioNLP11EPI (Kim et al., 2011). Table 1 reports data statistics. These datasets cover three domains social media (T WEETS), financial (F IN) and biomedical (B IO M ED). We utilize the CoNLL-2003 English NER dataset in newstext domain (N EWS) as the source task and others as the target. We perform domain-tuning and source task-tuning, followed by zero-shot transfer to the target tasks, as described in §2. Crucially, we do not use the labels of the training sets of the target tasks, and only use their sentences for domain adaptation."
2020.emnlp-main.497,N19-1423,0,0.24838,"those tokens which are harder to reconstruct by the underlying MLM. The adversarial objective leads to a challenging combinatorial optimisation problem over subsets of tokens, which we tackle efficiently through relaxation to a variational lower-bound and dynamic programming. On six unsupervised domain adaptation tasks involving named entity recognition, our method strongly outperforms the random masking strategy and achieves up to +1.64 F1 score improvements. 1 Introduction Contextualised word embedding models are becoming the foundation of state-of-the-art NLP systems (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Raffel et al., 2019; Brown et al., 2020; Clark et al., 2020). These models are pretrained on large amounts of raw text using self-supervision to reduce the labeled data requirement of target tasks of interest by providing useful feature representations (Wang et al., 2019a). Recent work has shown the importance of further training of pre-trained masked language models (MLMs) on the target domain text, as the benefits of their contextualised representations can deteriorate substantially in the presence of domain mismatch (Ma et al., 2019; Xu et al., 2019; W"
2020.emnlp-main.497,P18-2006,0,0.0463405,"Missing"
2020.emnlp-main.497,2020.acl-main.740,0,0.0281186,"Missing"
2020.emnlp-main.497,D19-1433,0,0.355266,"-supervision to reduce the labeled data requirement of target tasks of interest by providing useful feature representations (Wang et al., 2019a). Recent work has shown the importance of further training of pre-trained masked language models (MLMs) on the target domain text, as the benefits of their contextualised representations can deteriorate substantially in the presence of domain mismatch (Ma et al., 2019; Xu et al., 2019; Wang et al., 2019c; Gururangan et al., 2020). This is particularly crucial in unsupervised domain adaptation (UDA), where there is no labeled data in the target domain (Han and Eisenstein, 2019) and the knowledge from source domain labeled data is transferred to the target domain via a common representation space. However, current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of randomly masked-out tokens. Thus, it remains to investigate whether there exist more effective self-supervision strategies to bridge the knowledge gap of MLMs about the domains to yield higher-quality adapted models. A key principle of UDA is to learn a common embedding space of both domains which enables transferring a learned model on source task to"
2020.emnlp-main.497,N18-1170,0,0.0483704,"Missing"
2020.emnlp-main.497,W09-1401,0,0.00830702,"n used for backpropagation. 5 Experiments We evaluate our proposed masking strategy in UDA for named entity span prediction tasks coming from three different domains. 5.1 Unsupervised Domain Adaptation Tasks Source and Target Domain Tasks. Our evaluation is focused on the problem of identifying named entity spans in domain-specific text without access to labeled data. The evaluation tasks comes from several named entity recognition (NER) dataset including WNUT2016 (Strauss et al., 2016), FIN (Salinas Alvarado et al., 2015), JNLPBA (Collier and Kim, 2004), BC2GM (Smith et al., 2008), BioNLP09 (Kim et al., 2009), and BioNLP11EPI (Kim et al., 2011). Table 1 reports data statistics. These datasets cover three domains social media (T WEETS), financial (F IN) and biomedical (B IO M ED). We utilize the CoNLL-2003 English NER dataset in newstext domain (N EWS) as the source task and others as the target. We perform domain-tuning and source task-tuning, followed by zero-shot transfer to the target tasks, as described in §2. Crucially, we do not use the labels of the training sets of the target tasks, and only use their sentences for domain adaptation. Since the number of entity types are different in each t"
2020.emnlp-main.497,W11-1801,0,0.0414716,"Missing"
2020.emnlp-main.497,2021.ccl-1.108,0,0.103033,"Missing"
2020.emnlp-main.497,P19-1335,0,0.0186623,"The most straightforward domain adaptation approach in MLM is to adapt general contextual embedding to a specific domain (Lee et al., 2020; Alsentzer et al., 2019; Chakrabarty et al., 2019), that is to further improve pretrained MLM by continuing to pretrain language models on related domain or similar tasks (Gururangan et al., 2020), or via intermediate task which is also referred to as STILTs (Phang et al., 2018). Recent works have proposed twostep adaptive domain adaptation framework which consists of domain tuning and task finetuning (Ma et al., 2019; Xu et al., 2019; Wang et al., 2019c; Logeswaran et al., 2019). They have demonstrated that domain tuning is necessary to adapt MLM with both domain knowledge and task knowledge before finetuning, especially when the labelled data 6170 Task Model acc. non-OOV OOV rand pos ent adv 23.04 23.78 23.95 24.20 21.88 22.77 22.95 22.79 24.99 25.48 25.62 26.57 rand pos ent adv 27.66 28.51 28.09 29.36 25.01 27.23 27.67 27.56 30.88 29.36 31.21 33.90 rand pos ent adv 7.77 9.74 8.79 7.92 7.86 9.83 8.81 7.89 7.50 9.50 8.74 8.01 rand pos ent adv 11.38 13.09 13.19 14.53 11.35 12.88 12.89 14.44 11.48 13.89 14.28 14.84 rand pos ent adv 9.49 9.45 13.11 14.82 8.88 10.51 15.6"
2020.emnlp-main.497,D19-6109,0,0.105208,"s et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Raffel et al., 2019; Brown et al., 2020; Clark et al., 2020). These models are pretrained on large amounts of raw text using self-supervision to reduce the labeled data requirement of target tasks of interest by providing useful feature representations (Wang et al., 2019a). Recent work has shown the importance of further training of pre-trained masked language models (MLMs) on the target domain text, as the benefits of their contextualised representations can deteriorate substantially in the presence of domain mismatch (Ma et al., 2019; Xu et al., 2019; Wang et al., 2019c; Gururangan et al., 2020). This is particularly crucial in unsupervised domain adaptation (UDA), where there is no labeled data in the target domain (Han and Eisenstein, 2019) and the knowledge from source domain labeled data is transferred to the target domain via a common representation space. However, current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of randomly masked-out tokens. Thus, it remains to investigate whether there exist more effective self-supervision strategies to bridge the know"
2020.emnlp-main.497,P10-2041,0,0.0862055,"tokens. Thus, it remains to investigate whether there exist more effective self-supervision strategies to bridge the knowledge gap of MLMs about the domains to yield higher-quality adapted models. A key principle of UDA is to learn a common embedding space of both domains which enables transferring a learned model on source task to target task. It is typically done by further pretraining the MLM on a combination of both source and target data. Selecting relevant training examples has been shown to be effective in preventing the negative transfer and boosting the performance of adapted models (Moore and Lewis, 2010; Ruder and Plank, 2017). Therefore, we hypothesise that the computational effort of the further pretraining should concentrate more on learning words which are specific to the target domain or undergo semantic/syntactic shifts between the domains. In this paper, we show that the adapted model can benefit from careful masking strategy and propose an adversarial objective to select subsets for which the current underlying MLM is less confident. This objective raises a challenging combinatorial optimisation problem which we tackle by optimising its variational lower bound. We propose a training"
2020.emnlp-main.497,N18-1202,0,0.010053,"rsarially masking out those tokens which are harder to reconstruct by the underlying MLM. The adversarial objective leads to a challenging combinatorial optimisation problem over subsets of tokens, which we tackle efficiently through relaxation to a variational lower-bound and dynamic programming. On six unsupervised domain adaptation tasks involving named entity recognition, our method strongly outperforms the random masking strategy and achieves up to +1.64 F1 score improvements. 1 Introduction Contextualised word embedding models are becoming the foundation of state-of-the-art NLP systems (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Raffel et al., 2019; Brown et al., 2020; Clark et al., 2020). These models are pretrained on large amounts of raw text using self-supervision to reduce the labeled data requirement of target tasks of interest by providing useful feature representations (Wang et al., 2019a). Recent work has shown the importance of further training of pre-trained masked language models (MLMs) on the target domain text, as the benefits of their contextualised representations can deteriorate substantially in the presence of domain mismatch (Ma et al., 201"
2020.emnlp-main.497,D17-1038,0,0.0242994,"s to investigate whether there exist more effective self-supervision strategies to bridge the knowledge gap of MLMs about the domains to yield higher-quality adapted models. A key principle of UDA is to learn a common embedding space of both domains which enables transferring a learned model on source task to target task. It is typically done by further pretraining the MLM on a combination of both source and target data. Selecting relevant training examples has been shown to be effective in preventing the negative transfer and boosting the performance of adapted models (Moore and Lewis, 2010; Ruder and Plank, 2017). Therefore, we hypothesise that the computational effort of the further pretraining should concentrate more on learning words which are specific to the target domain or undergo semantic/syntactic shifts between the domains. In this paper, we show that the adapted model can benefit from careful masking strategy and propose an adversarial objective to select subsets for which the current underlying MLM is less confident. This objective raises a challenging combinatorial optimisation problem which we tackle by optimising its variational lower bound. We propose a training algorithm which alternat"
2020.emnlp-main.497,U15-1010,0,0.0171926,"obabilities for the decisions in a sampling path is returned as l in Algorithm 2, which is then used for backpropagation. 5 Experiments We evaluate our proposed masking strategy in UDA for named entity span prediction tasks coming from three different domains. 5.1 Unsupervised Domain Adaptation Tasks Source and Target Domain Tasks. Our evaluation is focused on the problem of identifying named entity spans in domain-specific text without access to labeled data. The evaluation tasks comes from several named entity recognition (NER) dataset including WNUT2016 (Strauss et al., 2016), FIN (Salinas Alvarado et al., 2015), JNLPBA (Collier and Kim, 2004), BC2GM (Smith et al., 2008), BioNLP09 (Kim et al., 2009), and BioNLP11EPI (Kim et al., 2011). Table 1 reports data statistics. These datasets cover three domains social media (T WEETS), financial (F IN) and biomedical (B IO M ED). We utilize the CoNLL-2003 English NER dataset in newstext domain (N EWS) as the source task and others as the target. We perform domain-tuning and source task-tuning, followed by zero-shot transfer to the target tasks, as described in §2. Crucially, we do not use the labels of the training sets of the target tasks, and only use their"
2020.emnlp-main.497,W16-3919,0,0.0556612,"Missing"
2020.emnlp-main.497,D19-1254,0,0.0739927,"ion tasks involving named entity recognition, our method strongly outperforms the random masking strategy and achieves up to +1.64 F1 score improvements. 1 Introduction Contextualised word embedding models are becoming the foundation of state-of-the-art NLP systems (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Raffel et al., 2019; Brown et al., 2020; Clark et al., 2020). These models are pretrained on large amounts of raw text using self-supervision to reduce the labeled data requirement of target tasks of interest by providing useful feature representations (Wang et al., 2019a). Recent work has shown the importance of further training of pre-trained masked language models (MLMs) on the target domain text, as the benefits of their contextualised representations can deteriorate substantially in the presence of domain mismatch (Ma et al., 2019; Xu et al., 2019; Wang et al., 2019c; Gururangan et al., 2020). This is particularly crucial in unsupervised domain adaptation (UDA), where there is no labeled data in the target domain (Han and Eisenstein, 2019) and the knowledge from source domain labeled data is transferred to the target domain via a common representation sp"
2020.emnlp-main.497,N19-1242,0,0.127439,"evlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Raffel et al., 2019; Brown et al., 2020; Clark et al., 2020). These models are pretrained on large amounts of raw text using self-supervision to reduce the labeled data requirement of target tasks of interest by providing useful feature representations (Wang et al., 2019a). Recent work has shown the importance of further training of pre-trained masked language models (MLMs) on the target domain text, as the benefits of their contextualised representations can deteriorate substantially in the presence of domain mismatch (Ma et al., 2019; Xu et al., 2019; Wang et al., 2019c; Gururangan et al., 2020). This is particularly crucial in unsupervised domain adaptation (UDA), where there is no labeled data in the target domain (Han and Eisenstein, 2019) and the knowledge from source domain labeled data is transferred to the target domain via a common representation space. However, current self-supervised adaptation methods are simplistic, as the training signal comes from a small percentage of randomly masked-out tokens. Thus, it remains to investigate whether there exist more effective self-supervision strategies to bridge the knowledge gap of MLMs"
2020.emnlp-main.497,W19-5808,0,0.0267562,"Missing"
2020.findings-emnlp.87,D18-1217,0,0.0243125,"ed sparse transformer and cross attention information fusion outperform previous systems adapted from the machine translation and graph generation literature. We further contribute our large graph modification datasets to the research community to encourage future research for this new problem. 1 Introduction Parsing text into structured semantics representation is one of the most long-standing and active research problems in NLP. Numerous parsing methods have been developed for many different semantic structure representations (Chen and Manning, 2014; Mrini et al., 2019; Zhou and Zhao, 2019; Clark et al., 2018; Wang et al., 2018). However, most of these previous works focus on parsing a single sentence, while a typical human-computer interaction session or conversation is not singleturn. A prominent example is image search. Users usually start with short phrases describing the main objects or topics they are looking for. Depending on the result, the users may then modify their query to add more constraints or give additional information. In this case, without the modification capability, a static representation is not suitable to track the changing intent of the user. We argue that the back-and-for"
2020.findings-emnlp.87,Q19-1019,0,0.355462,"e (i), the system copies the source graph to the target graph3 . In the “Text2Text” baseline (ii), we flatten the graph and reconstruct the natural sentence similarly to the modification query. In the “Modified GraphRNN” baseline (iii), we use the breadth-first-search (BFS) based node ordering to flatten the graph4 , and use RNNs as the encoders (You et al., 2018) and a decoder similar to our systems. In the final two baselines, “Graph Transformer” (iv) and “Deep Convolutional Graph Networks” (DCGCN) (v), we use the Graph Transformers (Cai and Lam, 2019) and Deep Convolutional Graph Networks (Guo et al., 2019) to encode the source graph (the decoder is identical to ours). boy boy in shirt shirt in &lt;attribute> black black &lt;null> &lt;attribute> Figure 6: Adjacency matrix style decoder. We use an attentional decoder using GRU units for generating edges. It operates similarly to the node-level decoder using Equation 11 and Equation 12. For more accurate typed edge generation, however, we incorporate the hidden states of the source and target nodes (from the node decoder) as inputs when updating the hidden state of the edge decoder: N E hEi,j = GRUE (zi,j−1 , hN i , hj , hi,j−1 ), (14) where hEi,j is the h"
2020.findings-emnlp.87,U19-1013,1,0.829644,"nodes from the source graph are not preserved. We believe that the proposed approach can reduce the noise in graph generation, and retain fine-grained details better than the baselines. 5 Related Work Semantic parsing is a sequence-to-graph transduction task, mapping natural language sentences to their meaning representation, e.g. see (Buys and Blunsom, 2017; Iyer et al., 2017; Dong and Lapata, 2018); this is different from our graph conditional semantic parsing. Recently, context-dependent semantic parsing has gained attraction (Iyyer et al., 2017; Srivastava et al., 2017; Suhr et al., 2018; He et al., 2019). Our work focuses on the update of scene graphs based on users’ queries, while previous works model the modifications of semantic representations in multi-turn dialogue systems. Due to their effectiveness, GCNs and graph transformer have been used as graph encoder for graphto-sequence transduction in semantic-based text generation (Bastings et al., 2017; Beck et al., 2018; Guo et al., 2019; Cai and Lam, 2019; Song et al., 2018; Wu et al., 2020). 6 Conclusion In this paper, we explore a novel problem of conditional graph modification, in which a system needs to modify a source graph according"
2020.findings-emnlp.87,P17-1089,0,0.0249495,"evidenced by the first example A. In addition, example B demonstrates when graph transformer observes a longer description, it lacks the capability of fusing the semantics between the source graph and the modification query; then certain nodes from the source graph are not preserved. We believe that the proposed approach can reduce the noise in graph generation, and retain fine-grained details better than the baselines. 5 Related Work Semantic parsing is a sequence-to-graph transduction task, mapping natural language sentences to their meaning representation, e.g. see (Buys and Blunsom, 2017; Iyer et al., 2017; Dong and Lapata, 2018); this is different from our graph conditional semantic parsing. Recently, context-dependent semantic parsing has gained attraction (Iyyer et al., 2017; Srivastava et al., 2017; Suhr et al., 2018; He et al., 2019). Our work focuses on the update of scene graphs based on users’ queries, while previous works model the modifications of semantic representations in multi-turn dialogue systems. Due to their effectiveness, GCNs and graph transformer have been used as graph encoder for graphto-sequence transduction in semantic-based text generation (Bastings et al., 2017; Beck"
2020.findings-emnlp.87,P17-1167,0,0.0327182,"Missing"
2020.findings-emnlp.87,W15-3014,0,0.0302254,"denote the nodes and edges of the graph zG . Given a training dataset of input-output pairs, denoted by D ≡ {(xGd , yd , zGd )}D d=1 , we train the model by maximizing the conditional loglikelihood `CLL = `Node + `Edge where, `Node = X log p(zN |x, y; θN ) (2) (x,y,z)∈D `Edge = X log p(zE |x, y, zN ; θE ). (3) (x,y,z)∈D During learning and decoding, we sort the nodes according to a topological order which exists for all the directed graphs in our user-generated and synthetic datasets. 3.2 Graph-based Encoder-Decoder Model Inspired by the machine translation literature (Bahdanau et al., 2014; Jean et al., 2015), we build our model based on the encoder-decoder framework. Since our task takes a source graph and a modification query as inputs, we need two encoders to model the graph and text information separately. Thus, there are four main components in our model: the query encoder, the graph encoder, the edge decoder and the node decoder. The information flow between the components is shown in Figure 4. In general, we encode the graph and text modification query into a joint representation, then we generate the target graph in two stages. Firstly, the target 975 nodes are generated via a node-level r"
2020.findings-emnlp.87,P18-1068,0,0.0199741,"rst example A. In addition, example B demonstrates when graph transformer observes a longer description, it lacks the capability of fusing the semantics between the source graph and the modification query; then certain nodes from the source graph are not preserved. We believe that the proposed approach can reduce the noise in graph generation, and retain fine-grained details better than the baselines. 5 Related Work Semantic parsing is a sequence-to-graph transduction task, mapping natural language sentences to their meaning representation, e.g. see (Buys and Blunsom, 2017; Iyer et al., 2017; Dong and Lapata, 2018); this is different from our graph conditional semantic parsing. Recently, context-dependent semantic parsing has gained attraction (Iyyer et al., 2017; Srivastava et al., 2017; Suhr et al., 2018; He et al., 2019). Our work focuses on the update of scene graphs based on users’ queries, while previous works model the modifications of semantic representations in multi-turn dialogue systems. Due to their effectiveness, GCNs and graph transformer have been used as graph encoder for graphto-sequence transduction in semantic-based text generation (Bastings et al., 2017; Beck et al., 2018; Guo et al."
2020.findings-emnlp.87,P81-1022,0,0.561842,"Missing"
2020.findings-emnlp.87,D18-1045,0,0.0220468,", 2017; Ren et al., 2018), we consider the scene graph modification problem as follows. Given an initial scene graph and a new query issued by the user, the goal is to generate a new scene graph taking into account the original graph and the new query. We formulate the problem as conditional graph modification, and create three datasets for this problem. We propose novel encoder-decoder architectures for conditional graph modification. More specifically, our graph encoder is built upon the self-attention architecture popular in state-of-theart machine translation models (Vaswani et al., 2017; Edunov et al., 2018), which is superior to, according to our study, Graph Convolutional Networks (GCN) (Kipf and Welling, 2016). Unique to our problem, however, is the fact that we have an open set of relation types in the graphs. Thus, we propose a novel graph-conditioned sparse transformer, in which the relation information is embed972 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 972–990 c November 16 - 20, 2020. 2020 Association for Computational Linguistics A young boy in a black shirt ded directly into the self-attention grid. For the decoder, we treat the graph modification t"
2020.findings-emnlp.87,D15-1166,0,0.0214868,"according to the connections of the sparsely connected transformer as well as all query tokens. The final representation m is taken from the output of transformer. Figure 5 shows the information flow in the cross-attention mechanism. 3.2.4 We use GRU cells (Cho et al., 2014) for our RNN decoders. The node-level decoder is a vanilla autoregressive model described as, N N hN t = GRU (zt−1 , ht−1 ) (10) N N cN t = ATTN (ht , m) (11) Figure 5: Cross-attention fusion. (12) N softmax(W[hN t , ct ] + b), (13) where z&lt;t denotes the nodes generated before time step t, ATTNN is a Luong-style attention (Luong et al., 2015), and m is the memory vectors from information fusion of the encoders (see §3.2.3). 3.2.5 Recall that the parameters of the graph and query encoders are shared to enable encoding of the two sources in the same semantic space. That is, we use the same transformer encoder for both sources. In cross-attention, we concatenate the x (from Equation 4) and y before rather than after the transformer encoder. As such, the encoder’s input is [x, y]. In the transformer, the representation of each query token gets updated by self-attending to the representations of all the query tokens and graph nodes in"
2020.lrec-1.378,D16-1250,0,0.0220935,"lingual word embeddings into a common space and thus should be able to decipher the ‘meaning’ or the ‘sense’ of two different words better, when they belong to different languages. Given the recent advancements in word representation models, cross-lingual word embedding based models should be employed for such a task. Please also note that we do not propose a new approach for the task of False friends’ detection and hence do not perform any experimentation with cross-lingual word embeddings. However, Merlo and Rodriguez (2019) show that cross-lingual word embeddings obtained using the VecMap (Artetxe et al., 2016) approach have shown promise and can be used to obtain a semantic comparison between two words from different languages. 6. aid the NLP tasks of Machine Translation, Cross-lingual Information Retrieval, and Computational Phylogenetics. We hope better approaches are developed for these tasks which can perform well on our challenge dataset. In the near future, we shall include partial cognates in our dataset creation approach and release another dataset on the same repository. Partial cognates mean different given different contexts and can confuse an NLP task. Hence, we believe it is also impor"
2020.lrec-1.378,N09-3008,0,0.691237,"ng based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Among cognate sets, common overlap set measures like set intersection, Jaccard (J¨arvelin et al., 2007) or XDice (Brew et al., 1996) could be used to measure similarities and validate the members of the set. 3. Dataset Creation We create three different datasets to help the NLP tasks of cognate and false friends’ detection. In this section, we describe the creation of these three datasets for twelve Indian languages, namely Sanskrit, Hindi, Assamese, Oriya, Kannada, Gujarati, Tamil, Telugu, Punjabi, Beng"
2020.lrec-1.378,W18-3903,0,0.0630431,"Missing"
2020.lrec-1.378,P14-2017,0,0.425837,", 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Among cognate sets, common overlap set measures like set intersection, Jaccard (J¨arvelin et al., 2007) or XDice (Brew et al., 1996) could be used to measure similarities and validate the members of the set. 3. Dataset Creation We create three different datasets to help the NLP tasks of cognate and false friends’ detection. In this section, we describe the creation of these three datasets for twelve Indian languages, namely Sanskrit, Hindi, Ass"
2020.lrec-1.378,P15-2071,0,0.353185,"(Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Among cognate sets, common overlap set measures like set intersection, Jaccard (J¨arvelin et al., 2007) or XDice (Brew et al., 1996) could be used to measure similarities and validate the members of the set. 3. Dataset Creation We create three different datasets to help the NLP tasks of cognate and false friends’ detection. In this section, we describe the creation of these three datasets for twelve Indian languages, namely Sanskrit, Hindi, Assamese, Oriya, Kannada, Gu"
2020.lrec-1.378,I11-1097,0,0.0276839,"ngst them is the Edit distance-based similarity measure (Melamed, 1999). Research in automatic cognate detection using various aspects involves computation of similarity by decomposing 3 The term linguistic area or Sprachbund (Emeneau, 1956) refers to a group of languages that have become similar in some way as a result of proximity and language contact, even if they belong to different families. The best-known example is the Indian (or South Asian) linguistic area. phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) emplo"
2020.lrec-1.378,E17-1113,0,0.143587,"Missing"
2020.lrec-1.378,2019.gwc-1.51,1,0.582606,"bda Kosha” and its annotation with linked Wordnet IDs. With the help of a lexicographer, we perform the digitization of this dictionary. Further, we annotate the cognate sets from the dicCognate False Friend Hindi (Hi) Marathi (Mr) Hindi Meaning Marathi Meaning ank shikshA ank shikshA Number Education Number Punishment Table 1: An example each of a cognate pair and a false friend pair from the closely related Indian languages Hindi (Hi) and Marathi (Mr) tionary with Wordnet synset IDs based on manual validation, where the lexicographer checks each Wordnet in the existing linked sense.Based on Kanojia et al. (2019b)’s approach, we use linked Indian Wordnets to generate true cognate data and create another cognate dataset. Additionally, we use the same Wordnet data to produce a list of False Friends and release2 all the three datasets publicly. Our cognate sets can be utilized for lookup in phrase tables produced during Machine Translation to assess the quality of the translation system in question. They can be utilized as candidate translations for words, and our false friends’ list can be utilized by language learners to avoid pitfalls during the acquisition of a second language. False Friend and Cogn"
2020.lrec-1.378,C04-1137,0,0.122835,"True Cognates (Word X and Word P), False Friends (Word Y) and Partial Cognates (Word A and Word Z) explained for creating our Datasets (D2 and D3). information retrieval (Meng et al., 2001) in the Indian setting, thus encouraging us to investigate this problem for this linguistic area3 . Some other applications of cognate detection in NLP have been sentence alignment (Simard et al., 1993; Melamed, 1999), inducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Work Wu and Yarowsky (2018) release cognate sets for Romance language family and provide a methodology to complete the cognate chain for related languages. Our work releases similar data for Indian languages. Such a co"
2020.lrec-1.378,A00-2038,0,0.820934,"s are often used as baseline methods for cognate detection, and the most commonly used method amongst them is the Edit distance-based similarity measure (Melamed, 1999). Research in automatic cognate detection using various aspects involves computation of similarity by decomposing 3 The term linguistic area or Sprachbund (Emeneau, 1956) refers to a group of languages that have become similar in some way as a result of proximity and language contact, even if they belong to different families. The best-known example is the Indian (or South Asian) linguistic area. phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu"
2020.lrec-1.378,W12-0216,0,0.467071,"Research in automatic cognate detection using various aspects involves computation of similarity by decomposing 3 The term linguistic area or Sprachbund (Emeneau, 1956) refers to a group of languages that have become similar in some way as a result of proximity and language contact, even if they belong to different families. The best-known example is the Indian (or South Asian) linguistic area. phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment."
2020.lrec-1.378,N01-1020,0,0.505957,"l 1 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 3096 Github Link Figure 1: The difference between True Cognates (Word X and Word P), False Friends (Word Y) and Partial Cognates (Word A and Word Z) explained for creating our Datasets (D2 and D3). information retrieval (Meng et al., 2001) in the Indian setting, thus encouraging us to investigate this problem for this linguistic area3 . Some other applications of cognate detection in NLP have been sentence alignment (Simard et al., 1993; Melamed, 1999), inducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Work Wu and Yarowsky (2018) release cognate sets for Roman"
2020.lrec-1.378,J99-1003,0,0.897363,"ics (Rama et al., 2018) as well as cross-lingual 1 Cognates can also exist in the same language. Such word pairs/sets are commonly referred to as doublets. 2 3096 Github Link Figure 1: The difference between True Cognates (Word X and Word P), False Friends (Word Y) and Partial Cognates (Word A and Word Z) explained for creating our Datasets (D2 and D3). information retrieval (Meng et al., 2001) in the Indian setting, thus encouraging us to investigate this problem for this linguistic area3 . Some other applications of cognate detection in NLP have been sentence alignment (Simard et al., 1993; Melamed, 1999), inducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Wo"
2020.lrec-1.378,K19-1011,0,0.115603,"ngual embeddings may not be an appropriate feature. Cross-lingual word embeddings project monolingual word embeddings into a common space and thus should be able to decipher the ‘meaning’ or the ‘sense’ of two different words better, when they belong to different languages. Given the recent advancements in word representation models, cross-lingual word embedding based models should be employed for such a task. Please also note that we do not propose a new approach for the task of False friends’ detection and hence do not perform any experimentation with cross-lingual word embeddings. However, Merlo and Rodriguez (2019) show that cross-lingual word embeddings obtained using the VecMap (Artetxe et al., 2016) approach have shown promise and can be used to obtain a semantic comparison between two words from different languages. 6. aid the NLP tasks of Machine Translation, Cross-lingual Information Retrieval, and Computational Phylogenetics. We hope better approaches are developed for these tasks which can perform well on our challenge dataset. In the near future, we shall include partial cognates in our dataset creation approach and release another dataset on the same repository. Partial cognates mean different"
2020.lrec-1.378,W97-1102,0,0.888502,"from gold-standard translation candidates. Keeping the application of our dataset in mind, we ignore the inclusion of partial cognates from this dataset. 3.2. D2 - True Cognate Pairs via IndoWornet In their paper, Kanojia et al. (2019b) identify IndoWordnet (Bhattacharyya, 2017) as a potential resource for the task of cognate detection. They utilize deep neural network based approaches to validate their approach for cognate detection. We build this dataset using a simple orthographic similarity based approach from the IndoWordnet dataset. Our approach combines Normalized Edit Distance (NED) (Nerbonne and Heeringa, 1997) and Cosine Similarity (CoS) (Salton and Buckley, 1988) between words. We compare synset words from every language pair using NED and populate a list of cognate sets where NED score is 0.7 and above. Similarly, we populate another list of cognate sets from every language pair using a shingle (n-gram) based Cosine Similarity with the same threshold. Due to the different methods using which NED and CoS similarity techniques compute scores, both NED and CoS output a different number of word pairs. We choose a common intersection of cognate pairs from among both the lists, and populate a final ‘po"
2020.lrec-1.378,N18-2063,0,0.455793,"Missing"
2020.lrec-1.378,C16-1097,0,0.756328,"automatic cognate detection using various aspects involves computation of similarity by decomposing 3 The term linguistic area or Sprachbund (Emeneau, 1956) refers to a group of languages that have become similar in some way as a result of proximity and language contact, even if they belong to different families. The best-known example is the Indian (or South Asian) linguistic area. phonetically transcribed words (Kondrak, 2000), acoustic models (Mielke et al., 2012), clustering based on semantic equivalence (Hauer and Kondrak, 2011), and aligned segments of transcribed phonemes (List, 2012). Rama (2016) employs a Siamese convolutional neural network to learn the phonetic features jointly with language relatedness for cognate identification, which was achieved through phoneme encodings. J¨ager et al. (2017) use SVM for phonetic alignment and perform cognate detection for various language families. Various works on orthographic cognate detection usually take alignment of substrings within classifiers like SVM (Ciobanu and Dinu, 2014; Ciobanu and Dinu, 2015) or HMM (Bhargava and Kondrak, 2009). Ciobanu and Dinu (2014) employ dynamic programming based methods for sequence alignment. Among cognat"
2020.lrec-1.378,C02-1002,0,0.191073,"st in the same language. Such word pairs/sets are commonly referred to as doublets. 2 3096 Github Link Figure 1: The difference between True Cognates (Word X and Word P), False Friends (Word Y) and Partial Cognates (Word A and Word Z) explained for creating our Datasets (D2 and D3). information retrieval (Meng et al., 2001) in the Indian setting, thus encouraging us to investigate this problem for this linguistic area3 . Some other applications of cognate detection in NLP have been sentence alignment (Simard et al., 1993; Melamed, 1999), inducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Work Wu and Yarowsky (2018) release cognate sets for Romance language fa"
2020.lrec-1.378,L18-1538,0,0.0193274,"nducing translation lexicons (Mann and Yarowsky, 2001; Tufis, 2002), improving statistical machine translation models (Al-Onaizan et al., 1999), and identification of confusable drug names (Kondrak and Dorr, 2004). All these applications depend on an effective method of identifying cognates by computing a numerical score that reflects the likelihood that the two words are cognates. Our work provides cognate sets for Indian languages, which can help the automated cognate detection methodologies and can also be used as possible translation candidates for applications such as MT. 2. Related Work Wu and Yarowsky (2018) release cognate sets for Romance language family and provide a methodology to complete the cognate chain for related languages. Our work releases similar data for Indian languages. Such a cognate set data has not been released previously for Indian languages, to the best of our knowledge. Additionally, we release lists of false friends’ for language pairs. These cognates can be used to challenge the previously established cognate detection approaches further. Kanojia et al. (2019a) perform cognate detection for some Indian languages, but a prominent part of their work includes manual verifica"
2020.wmt-1.3,P19-1285,0,0.0272739,"Missing"
2020.wmt-1.3,2020.emnlp-main.5,0,0.0391197,"were encouraged to submit both directions (i.e. modelling both speakers was desired), in this first round of the task, we emphasized on the agent side (English→German) and performed human evaluation in that direction exclusively. This decision is not entrenched and, thus, for future tasks we will aim at evaluating both translation directions. We decided to pursue this direction because the customer side (German→English) suffers from “translationese”: English was the original source, and it was recently shown that translationese has a significant impact in evaluation both in automatic metrics (Freitag et al., 2020) and human evaluation (L¨aubli et al., 2020). 3.1 Baseline 4 Participants Six participants submitted their systems to the Chat Translation shared task. Although the German→English direction (i.e. customer side) was optional, all participants submitted their systems for both directions. In total, 14 runs were submitted (although only primary submissions were considered for human evaluation). Table 3 summarizes the participants and their affiliations. Data The main data source for this shared task is BConTrasT. As mentioned in §2, the translated conversations are sampled from the original Taskma"
2020.wmt-1.3,2020.wmt-1.56,0,0.0343382,"tures for this task: (i) standard transformer pre-trained on WMT17 News and finetuned on the WMT20 Chat data, and (ii) modified transformer by including additional encoder to process one previous utterance in tandem with the current utterance, also pre-trained on WMT17 News and fine-tuned on a mix of WMT20 Chat data and a subset of WMT19 News data. The primary system is based on the first architecture while the second architecture is used for the two contrastive submissions. The contrastive submissions differ in the manner and timing in which training data was processed. For more details see (Bao et al., 2020). Universities of Edinburgh and Uppsala The joint submissions of University of Edinburgh and Uppsala University are based on the transformer-big architecture (Vaswani et al., 2017) and rely on fine-tuning pre-existing systems from the WMT 2019 News Translation Task (experiment with both UEdin’s submission based on Marian (Junczys-Dowmunt et al., 2018) and Facebook’s submission based on Fairseq (Ott et al., 2019)). They are fine-tuned on pseudo-in-domain web crawled data and in-domain task data. The authors also experiment with (i) domain and speaker-level adaptation by automatically tagging th"
2020.wmt-1.3,P82-1020,0,0.775149,"Missing"
2020.wmt-1.3,W19-5321,0,0.0202003,"Missing"
2020.wmt-1.3,P18-4020,1,0.756444,"Missing"
2020.wmt-1.3,2020.wmt-1.57,0,0.0725618,"Missing"
2020.wmt-1.3,D19-1459,0,0.238444,"man and sent to the customer. Translating conversational text, in particular customer support chats, is an important and challenging application task for machine translation technology. This type of content has so far not been extensively explored in prior MT research, largely due to the lack of publicly available data sets. Prior related work has mostly focused on movie subtitles and European Parliament speeches. To alleviate this problem, we created a corpus for this shared task, BConTrasT(§2), which is translated from English into German and is based on the monolingual Taskmaster-1 corpus (Byrne et al., 2019). We report the results of the first edition of the WMT shared task on Chat Translation. The task consisted of translating bilingual conversational text, in particular customer support chats for the English-German language pair (English agent, German customer). This task varies from the other translation shared tasks, i.e. news and biomedical, mainly due to the fact that the conversations are bilingual, less planned, more informal, and often ungrammatical. Furthermore, such conversations are usually characterized by shorter and simpler sentences and contain more pronouns. We received 14 submis"
2020.wmt-1.3,2005.mtsummit-papers.11,0,0.0593983,"er-1 corpus. Since pronouns are one of the main challenges in translating conversational data, we selected the conversations that contain at least one English anaphoric pronoun it. For this we used NEURALCOREF 2 and selected around 18k sentence pairs and then divided them into train, development, and test sets (see Table 2). Bilingual Conversational Data One of the main challenges of bilingual conversation translation is the lack of publicly available data sets targeted for the task. The most commonly used datasets are movie subtitles (Lison and Tiedemann, 2016), European Parliament speeches (Koehn, 2005), and conversations extracted from the public forums such as Ubuntu Dialogue corpus (Lowe et al., 2015). These corpora, however, usually involve more than two speakers, contain a significant amount of noise (e.g. speakers information missing in the case of movie subtitles), and usually cover very broad domains. For the Chat Translation task, we aim to develop a common ground for MT researchers to train and test their solutions by providing common training, validation, and test sets, as well as a common shared task definition. Unfortunately, due to the General Data Protection Regulation (GDPR),"
2020.wmt-1.3,E06-1032,0,0.0342614,"of the Chat Translation shared task we follow the standard procedure of WMT shared tasks and evaluate both on automatic metrics and human evaluation with context. Even though automatic metrics provide a cheap mechanism to evaluate Machine Translation (MT) systems outputs, they do not tell the whole story for highperforming systems (Ma et al., 2019). For example, recent “sentence-level human parity” claims do not seem to hold when the context of the document is considered (L¨aubli et al., 2018), and metrics such as BLEU (Papineni et al., 2002) fail to correlate properly with human assessment (Callison-Burch et al., 2006). In this edition of the shared task, we aim for both automatic and manual evaluations. NaverLabs-Sys1 NaverLabs-Sys2 58.8 60.4 26.8 25.1 59.4 61.6 24.6 23.1 UEdinUppsala-Sys1 UEdinUppsala-Sys2 60.2 59.8 25.3 25.4 61.8 61.5 22.8 23.8 Tencent-Sys1 Tencent-Sys2 53.6 58.6 30.6 26.6 54.0 61.9 28.8 23.2 UniMaryland-Sys1 UniMaryland-Sys2 55.6 56.4 28.3 28.1 49.4 49.4 32.0 32.0 Table 4: Automatic evaluation scores for the agent (En→De) and customer (De→En). Specifically, we build HITs (following the Mechanical Turk’s term human intelligence task) for the Segment Rating + Document Context (SR+DC) conf"
2020.wmt-1.3,2020.wmt-1.58,0,0.0235277,"Junczys-Dowmunt et al., 2018) and Facebook’s submission based on Fairseq (Ott et al., 2019)). They are fine-tuned on pseudo-in-domain web crawled data and in-domain task data. The authors also experiment with (i) domain and speaker-level adaptation by automatically tagging the source and target sentences with domain and speaker tags respectively, and (ii) contextual NMT by exploiting the previous context, varying the type and number of previous utterances used. The final submission is an ensemble of four models trained with domain tags and using noisy-channel re-ranking. For more details see (Moghe et al., 2020). 4.1.3 Tencent 4.1.6 Jordan University of Science and Technology Mohammed et al. (2020) train separate models for the agent and customer sides after combining the training and development datasets for each side. They use bidirectional RNN (LSTM) with pretrained BERT (Devlin et al., 2018) embeddings for each of the translation directions. In addition, the authors report using different parameters for training, resulting in different models which then are used for ensemble decoding. For more details see (Mohammed et al., 2020). Tao Wang (individual participant) Individual participant Tao Wang u"
2020.wmt-1.3,D18-1512,0,0.0910736,"Missing"
2020.wmt-1.3,2020.wmt-1.59,0,0.0334625,"2019)). They are fine-tuned on pseudo-in-domain web crawled data and in-domain task data. The authors also experiment with (i) domain and speaker-level adaptation by automatically tagging the source and target sentences with domain and speaker tags respectively, and (ii) contextual NMT by exploiting the previous context, varying the type and number of previous utterances used. The final submission is an ensemble of four models trained with domain tags and using noisy-channel re-ranking. For more details see (Moghe et al., 2020). 4.1.3 Tencent 4.1.6 Jordan University of Science and Technology Mohammed et al. (2020) train separate models for the agent and customer sides after combining the training and development datasets for each side. They use bidirectional RNN (LSTM) with pretrained BERT (Devlin et al., 2018) embeddings for each of the translation directions. In addition, the authors report using different parameters for training, resulting in different models which then are used for ensemble decoding. For more details see (Mohammed et al., 2020). Tao Wang (individual participant) Individual participant Tao Wang uses a sentencelevel system trained on all the WMT20 En-De parallel data. The author uses"
2020.wmt-1.3,W19-5333,0,0.0119796,"er and an agent. Customer lines words Training Dev Test 6,216 862 967 41,492 5,805 6,464 training data provided by the News shared task organizers. Moreover, they were allowed to use existing pre-trained models, such as BERT (Devlin et al., 2018), Transformer-XL (Dai et al., 2019), Reformer (Kitaev et al., 2020), among others. Agent lines words 7,629 1,040 1,133 70,193 9,569 10,187 3.2 Table 2: Statistics of the English side of the training, dev, and test sets. To define our non-human baseline, we use Facebook’s last year submissions to the document-level translation task for both directions (Ng et al., 2019) as the terms of comparison. Even though these models are not domain adapted for the Chat Translation task, we find them to have a reasonable quality for this domain. However, it is worth mentioning that we solely report the results of these models with the automatic metrics and we do not perform any type of direct assessment on these models. pared to the cases like news, biomedical, etc. In the first edition of this shared task we focused on this environment and asked the participants to translate the customer’s utterances from German into English and the agent’s from English into German. Alt"
2020.wmt-1.3,L16-1147,0,0.0152834,"is scarce, we translated only a small set of the Taskmaster-1 corpus. Since pronouns are one of the main challenges in translating conversational data, we selected the conversations that contain at least one English anaphoric pronoun it. For this we used NEURALCOREF 2 and selected around 18k sentence pairs and then divided them into train, development, and test sets (see Table 2). Bilingual Conversational Data One of the main challenges of bilingual conversation translation is the lack of publicly available data sets targeted for the task. The most commonly used datasets are movie subtitles (Lison and Tiedemann, 2016), European Parliament speeches (Koehn, 2005), and conversations extracted from the public forums such as Ubuntu Dialogue corpus (Lowe et al., 2015). These corpora, however, usually involve more than two speakers, contain a significant amount of noise (e.g. speakers information missing in the case of movie subtitles), and usually cover very broad domains. For the Chat Translation task, we aim to develop a common ground for MT researchers to train and test their solutions by providing common training, validation, and test sets, as well as a common shared task definition. Unfortunately, due to th"
2020.wmt-1.3,N19-4009,0,0.0122697,"second architecture is used for the two contrastive submissions. The contrastive submissions differ in the manner and timing in which training data was processed. For more details see (Bao et al., 2020). Universities of Edinburgh and Uppsala The joint submissions of University of Edinburgh and Uppsala University are based on the transformer-big architecture (Vaswani et al., 2017) and rely on fine-tuning pre-existing systems from the WMT 2019 News Translation Task (experiment with both UEdin’s submission based on Marian (Junczys-Dowmunt et al., 2018) and Facebook’s submission based on Fairseq (Ott et al., 2019)). They are fine-tuned on pseudo-in-domain web crawled data and in-domain task data. The authors also experiment with (i) domain and speaker-level adaptation by automatically tagging the source and target sentences with domain and speaker tags respectively, and (ii) contextual NMT by exploiting the previous context, varying the type and number of previous utterances used. The final submission is an ensemble of four models trained with domain tags and using noisy-channel re-ranking. For more details see (Moghe et al., 2020). 4.1.3 Tencent 4.1.6 Jordan University of Science and Technology Mohamm"
2020.wmt-1.3,2020.eamt-1.24,1,0.709928,"at Translation M. Amin Farajian1∗ Ant´onio V. Lopes1∗ Andr´e F. T. Martins1,3 Sameen Maruf2 Gholamreza Haffari2 1 Unbabel, Rua Castilho 52, 1250-069, Lisbon, Portugal 2 Monash University, VIC, Australia 3 Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisbon, Portugal {amin, antonio.lopes, andre.martins}@unbabel.com {sameen.maruf, gholamreza.haffari}@monash.edu Abstract and Scherrer, 2017; Zhang et al., 2018; Maruf et al., 2019; Miculicich et al., 2018; Voita et al., 2019b; Tu et al., 2018; Maruf et al., 2018; Jean et al., 2017; Voita et al., 2018, 2019a; JunczysDowmunt, 2019; Lopes et al., 2020), focusing on extending both Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) with additional encoders or decoders to incorporate previous sentences context. However, often, the approaches are developed for single speaker and document-like tasks. By contrast, in this shared task, we focus on the online multispeaker and multi-lingual setting, where each participant in the conversation speaks in their native language. This task has been first considered by Maruf et al. (2018). In the first round of the Chat Translation shared task, we propos"
2020.wmt-1.3,P02-1040,0,0.114414,"d by conversational data as a content type, which has a broad application in industry-level services. In this content type, the text is usually not carefully well formatted, frequently contains typos, abbreviations, and inconsistent casing, usually with shorter sentences, often informal and ungrammatical. Since chat sessions are interactive, the task of translating conversations can be seen as a two-in-one task, modelling both dialogue and document-level translation at the same time. In order to evaluate the translation quality of the participating systems we use both automatic metrics (BLEU (Papineni et al., 2002) and TER (Snover et al., 2006)), and human evaluation, consisting of Direct Assessment (DA). For DA, we define the evaluation process similarly to last year’s WMT News Translation task (Barrault et al., 2019) with document-level context and following the set of recommendations of L¨aubli et al. (2020). However, differently than the News task, here we rely on professional translators instead of a crowd. This is mainly based on the observations of L¨aubli et al. (2020), which provides evidence of the professional translators having better judgment and ability to detect fine-grained phenomena. Si"
2020.wmt-1.3,W15-4640,0,0.0167846,"selected the conversations that contain at least one English anaphoric pronoun it. For this we used NEURALCOREF 2 and selected around 18k sentence pairs and then divided them into train, development, and test sets (see Table 2). Bilingual Conversational Data One of the main challenges of bilingual conversation translation is the lack of publicly available data sets targeted for the task. The most commonly used datasets are movie subtitles (Lison and Tiedemann, 2016), European Parliament speeches (Koehn, 2005), and conversations extracted from the public forums such as Ubuntu Dialogue corpus (Lowe et al., 2015). These corpora, however, usually involve more than two speakers, contain a significant amount of noise (e.g. speakers information missing in the case of movie subtitles), and usually cover very broad domains. For the Chat Translation task, we aim to develop a common ground for MT researchers to train and test their solutions by providing common training, validation, and test sets, as well as a common shared task definition. Unfortunately, due to the General Data Protection Regulation (GDPR), 3 Task Description A critical challenge faced by international companies today is delivering customer"
2020.wmt-1.3,W19-5302,0,0.0121262,"their systems. 5 System 43.4 38.0 Primary NaverLabs UEdinUppsala IndTaoWang Tencent UniMaryland UJordan 60.1 60.2 59.7 58.6 56.7 46.4 25.7 25.4 26.0 26.7 28.2 38.2 49.7 32.0 61.0 62.4 61.3 62.3 49.4 42.5 23.3 22.8 23.5 23.0 32.0 40.2 Contrastive For the first round of the Chat Translation shared task we follow the standard procedure of WMT shared tasks and evaluate both on automatic metrics and human evaluation with context. Even though automatic metrics provide a cheap mechanism to evaluate Machine Translation (MT) systems outputs, they do not tell the whole story for highperforming systems (Ma et al., 2019). For example, recent “sentence-level human parity” claims do not seem to hold when the context of the document is considered (L¨aubli et al., 2018), and metrics such as BLEU (Papineni et al., 2002) fail to correlate properly with human assessment (Callison-Burch et al., 2006). In this edition of the shared task, we aim for both automatic and manual evaluations. NaverLabs-Sys1 NaverLabs-Sys2 58.8 60.4 26.8 25.1 59.4 61.6 24.6 23.1 UEdinUppsala-Sys1 UEdinUppsala-Sys2 60.2 59.8 25.3 25.4 61.8 61.5 22.8 23.8 Tencent-Sys1 Tencent-Sys2 53.6 58.6 30.6 26.6 54.0 61.9 28.8 23.2 UniMaryland-Sys1 UniMar"
2020.wmt-1.3,W18-6319,0,0.0210084,"delines, we use trusted professional translators from the Unbabel community to evaluate the adequacy of the translation on a scale of 0 to 100. The guidelines to the translators were as simple as possible to avoid any type of bias, asking them to rate each sentence taking the context into account and penalizing when there is a context error, as they would for a noncontextual error. For the first edition of this shared task, we perAutomatic Evaluation For the automatic evaluation, we use both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) metrics. For the former, we use SacreBLEU3 (Post, 2018), while for TER we use v0.7.254 and report case-sensitive scores. The automatic metrics are used to measure the quality of the translations of both sides, i.e. customer and agent. 5.2 BLEU↑ TER↓ BLEU↑ TER↓ FAIR WMT’19 Evaluation Procedures 5.1 Customer Human Evaluation For the human evaluation we follow a similar procedure to last year’s WMT News shared task (Barrault et al., 2019) but take into account the set of recommendations defined by L¨aubli et al. (2020). 3 BLEU+case.mixed+lang.ende+numrefs.1+smooth.exp+tok.13a+version.1.4.13, BLEU+case.mixed+lang.deen+numrefs.1+smooth.exp+tok.13a+vers"
2020.wmt-1.3,2006.amta-papers.25,0,0.489412,"ntent type, which has a broad application in industry-level services. In this content type, the text is usually not carefully well formatted, frequently contains typos, abbreviations, and inconsistent casing, usually with shorter sentences, often informal and ungrammatical. Since chat sessions are interactive, the task of translating conversations can be seen as a two-in-one task, modelling both dialogue and document-level translation at the same time. In order to evaluate the translation quality of the participating systems we use both automatic metrics (BLEU (Papineni et al., 2002) and TER (Snover et al., 2006)), and human evaluation, consisting of Direct Assessment (DA). For DA, we define the evaluation process similarly to last year’s WMT News Translation task (Barrault et al., 2019) with document-level context and following the set of recommendations of L¨aubli et al. (2020). However, differently than the News task, here we rely on professional translators instead of a crowd. This is mainly based on the observations of L¨aubli et al. (2020), which provides evidence of the professional translators having better judgment and ability to detect fine-grained phenomena. Six teams participated in this f"
2020.wmt-1.3,N19-1313,1,0.869815,"Missing"
2020.wmt-1.3,W17-4811,0,0.0264946,"Missing"
2020.wmt-1.3,W18-6312,0,0.0139961,"s (BLEU and TER) for evaluating the translations of both agent and customer messages and human document-level direct assessments to evaluate the agent translations. 1 Introduction Despite the significant progress in Neural Machine Translation (NMT) in the last years (Vaswani et al., 2017; Hassan et al., 2018), most systems still operate at sentence-level, disregarding the context of previous sentences. It has been pointed out that ignoring the context may degrade the quality of translations, leading to incorrect choice of pronouns, lexical inconsistency, and incoherence (L¨aubli et al., 2018; Toral et al., 2018). This is particularly relevant in the context of bilingual chat translation, which normally consists of short messages, referencing each other, and where the correct lexical choice to translate a speaker might have been uttered in a previous turn by the other speaker. Numerous systems have been proposed recently to address document-level translation (Tiedemann ∗ These authors contributed equally. 65 Proceedings of the 5th Conference on Machine Translation (WMT), pages 65–75 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics The main motivation of this shared task i"
2020.wmt-1.3,W18-6311,1,0.544991,"ean et al., 2017; Voita et al., 2018, 2019a; JunczysDowmunt, 2019; Lopes et al., 2020), focusing on extending both Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) with additional encoders or decoders to incorporate previous sentences context. However, often, the approaches are developed for single speaker and document-like tasks. By contrast, in this shared task, we focus on the online multispeaker and multi-lingual setting, where each participant in the conversation speaks in their native language. This task has been first considered by Maruf et al. (2018). In the first round of the Chat Translation shared task, we propose translating dialogues with two speakers, where the first speaker is speaking in the German→English direction and the second is speaking in the English→German. Moreover, we tailor this task for a specific use case: translating conversational text of the customer support chats. In this setting the utterances of the German speaking customer are translated using a machine translation system into English. Then, the replies of the English speaking agent are translated into German and sent to the customer. Translating conversational"
2020.wmt-1.3,D18-1325,0,0.0292722,"Missing"
2020.wmt-1.3,Q18-1029,0,0.0312375,"Missing"
2020.wmt-1.3,D19-1081,0,0.0244263,"Missing"
2020.wmt-1.3,P19-1116,0,0.0720709,"Missing"
2020.wmt-1.3,P18-1117,0,0.0615757,"Missing"
2020.wmt-1.3,2020.wmt-1.60,0,0.0353678,".1.2 4.1.4 Tencent systems are based on self-attention networks including document-level multi-encoder and sentence-level Transformer. In order to get more in-domain data the authors use a multi-feature data selection method (e.g. FDA, n-gram LM, Transformer LM and BERT) to select data from news corpus. Furthermore, the systems have different fine-tuning strategies, ranging from sentence-level to document-level. Finally, these systems use large scale pre-trained language models including monolingual BERT (Devlin et al., 2018) and bilingual XLM (Lample and Conneau, 2019). For more details see (Wang et al., 2020). 4.1.5 University of Maryland The University of Maryland systems are both sentence and document-level systems, with two distinct architectures for this task: (i) standard transformer pre-trained on WMT17 News and finetuned on the WMT20 Chat data, and (ii) modified transformer by including additional encoder to process one previous utterance in tandem with the current utterance, also pre-trained on WMT17 News and fine-tuned on a mix of WMT20 Chat data and a subset of WMT19 News data. The primary system is based on the first architecture while the second architecture is used for the two contras"
2020.wmt-1.3,D18-1049,0,0.0402907,"Missing"
2021.acl-long.415,N19-1357,0,0.0207755,"and are mostly applied to models with differentiable functions. Further, they may be sensitive 1 This approach does not aim to improve the transparency (Lipton, 2018) of the black-box model. to randomized model initializations or permuted data labels (Adebayo et al., 2018), which is undesirable. These methods can be computationally heavy in the case of complex black-box models (Wu and Ong, 2021), e.g., BERT (Devlin et al., 2018). Attention-based methods (Wiegreffe and Pinter, 2019) can only be applied to Transformer-based models (Vaswani et al., 2017), and their effectiveness is questionable (Jain and Wallace, 2019; Serrano and Smith, 2019). Perturbation-based methods approximate feature importance by observing changes in a model’s outcome after a feature is changed. They either consider changes in performance as an indicator of feature importance directly (Martens and Provost, 2014; Zeiler and Fergus, 2014; Schwab and Karlen, 2019), or they employ a higher-order approximation of the decision boundary (Ribeiro et al., 2016; Lundberg and Lee, 2017). Perturbation-based methods are typically computationally inefficient for explaining high-dimensional data, and they suffer from high variance due to perturba"
2021.acl-long.415,2020.emnlp-main.747,0,0.0277781,"statistical significance. Detailed precision and recall values of positive reviews appear in Appendix G. Faithfulness. We select the top-K important words generated by an explanation method and compute the precision, recall and F1 against the human-annotated rationales. It is worth noting that our L2E explainer is not supervised by human rationales directly. Instead, we use the same experimental setup as in Section 4.5 to ensure the L2E explainer is learning from the baseline algorithms rather than the human rationales. Table 4 displays the average values over all test instances. As noted by Carton et al. (2020), the rationales in the original dataset are not exhaustively identified by human annotators. For a particular event, we expect to observe a lower precision than recall, since the black-box model might still be able to utilize the words not being annotated in addition to the words annotated by a human. The results in Table 4 align with this hypothesis. For instance, besides LRP for the positive reviews and Kernel SHAP for both reviews, all baselines and the corresponding L2E have higher recall than precision. Furthermore, L2E outperforms the corresponding baseline A significantly in most cases"
2021.acl-long.415,2020.acl-main.409,0,0.0207685,"high-dimensional data, and they suffer from high variance due to perturbation randomness (Slack et al., 2020; Chen et al., 2019). Model-based Approaches. These approaches train the explainer with an objective function to improve efficiency at test time. The closest work to ours is by Schwab and Karlen (2019), who train an explainer using a causality-based explanation algorithm. However, these approaches do not learn from arbitrary algorithms or discretize feature weights — the high variation of continuous weights may impair the ability to capture the commonalities in an explanation algorithm. Jain et al. (2020) discretize the weights produced by an existing method, but they use these weights to build a faithful classifier for an underlying blackbox model, rather than using them to explain the model directly. Other works train a classifier and an explainer jointly in order to incorporate explainability directly into the classifier (Lei et al., 2016; Camburu et al., 2018). Unlike these approaches, we do not change the classifier or require an expensive process to collect human rationales, as done in (Camburu et al., 2018). Lastly, a few works use information-theoretic objectives to train an explainer"
2021.acl-short.100,P19-1470,0,0.0141547,"thods (Yang et al., 2014; Dettmers et al., 2018). In these approaches, entities and relations are embedded in a complex space, and using a scoring function plausibility of a triple is estimated (Bordes et al., 2013; Trouillon et al., 2016; Sun et al., 2018). In addition to node embedding, graph embedding methods have also been used to capture the structural complexity of knowledge bases (Schlichtkrull et al., 2018; Shang et al., 2019). Language generative models also have been applied on knowledge bases in order to use the rich information of pre-trained models to address CKG completion task (Bosselut et al., 2019; Moghimifar et al., 2020). Malaviya et al. (2020) proposed a method based on using language models and graph networks to solve the problem of the sparsity of CKGs, by taking structural and contextual characteristics of CKGs into account. However, the aforementioned models are highly dependant on training on a set of static entities, and fail to perform when new triples are presented. 3 Our Approach A CKG G = (N, E), where N is the set of nodes and E is the set of edges in G, consists of triples in form of r(h, t), where h, t ∈ N are referred to as the head and the tail of the triple, and r ∈"
2021.acl-short.100,N19-1423,0,0.0931381,"and ConceptNet-100k. Unseen Nodes is the ratio of the nodes in test set that are not in train set to all of the nodes in test set. Unseen edges is the ratio of edges where either the head or tail nodes are not in train set to the number of all edges in test set. Query Given a rule with a rightmost atom rk−1 (tk−1 , X) in the rule body, we send the representation of tk−1 as query to the given CKG to retrieve unification candidates. A node in a CKG is a word sequence. To support comparison of nodes w.r.t. their semantic similarities, we encode queries and nodes in a CKG with a pre-trained BERT (Devlin et al., 2019) into embeddings. To this end, a node is converted into [CLS]+node+[SEP ], and fed into the model, and we use the representation of [CLS] token from the last layer of BERT as representation of node node. We apply FAISS1 (Johnson et al., 2019) to index embeddings of an CKG, because it supports fast retrieval of k nearest neighbours of a dense vector. For each node v in the topk list, we collect a set of triples C(v), which are all triples having v as the head in the CKG. As a result, we have k such sets and form a candidate set C by taking the union of them. Weak Unification From a candidate se"
2021.acl-short.100,P16-1137,0,0.0290907,"4), ComplEx (Trouillon et al., 2016), ConvE (Dettmers et al., 2018), RotatE (Sun et al., 2018), and Malaviya (Malaviya et al., 2020). The first four models are high performance models in conventional KB completion, whereas the latter is proposed for CKG completion. 4.1 Datasets ATOMIC 3 is a CKG consisting of commonsense facts in form of triples, based on if-then relations (Sap et al., 2019). This dataset consists of more 877K facts, and more than 300K entities. ConceptNet-100K 4 is a subset of ConceptNet 5 (Speer et al., 2017), containing Open Mind Common Sense (OMCS) entries, introduced by (Li et al., 2016). This dataset contains general commonsense facts in form of triples. 2 Code available at https://github.com/ farhadmfar/commonsense_reasoner 3 https://homes.cs.washington.edu/ msap/atomic/ 4 https://ttic.uchicago.edu/ kgimpel/commonsense.html In order to evaluate the performance of the models in dynamic CKG completion, we choose a subset of the test set of ATOMIC and ConceptNet100K, where for any (h, r, t) either h or t is not seen by the model in the train set. Statistics on ATOMIC and ConceptNet-100k are provided in table 1. To train our model, each triple in form r(h, t) in train set was a"
2021.acl-short.100,2020.coling-main.467,1,0.770611,"4; Dettmers et al., 2018). In these approaches, entities and relations are embedded in a complex space, and using a scoring function plausibility of a triple is estimated (Bordes et al., 2013; Trouillon et al., 2016; Sun et al., 2018). In addition to node embedding, graph embedding methods have also been used to capture the structural complexity of knowledge bases (Schlichtkrull et al., 2018; Shang et al., 2019). Language generative models also have been applied on knowledge bases in order to use the rich information of pre-trained models to address CKG completion task (Bosselut et al., 2019; Moghimifar et al., 2020). Malaviya et al. (2020) proposed a method based on using language models and graph networks to solve the problem of the sparsity of CKGs, by taking structural and contextual characteristics of CKGs into account. However, the aforementioned models are highly dependant on training on a set of static entities, and fail to perform when new triples are presented. 3 Our Approach A CKG G = (N, E), where N is the set of nodes and E is the set of edges in G, consists of triples in form of r(h, t), where h, t ∈ N are referred to as the head and the tail of the triple, and r ∈ E denotes their relation."
2021.eacl-main.109,P18-1071,0,0.0433823,"Missing"
2021.eacl-main.109,J19-1002,0,0.0938722,"strates the effectiveness of each individual proposed technique. The results are statistically significant with p≤0.05 according to the Wilcoxon signed-rank test (Wilcoxon, 1992). 2 Related Work Semantic parsing There is ample of work on machine learning models for semantic parsing. The recent surveys (Kamath and Das, 2018; Zhu et al., 2019) cover a wide range of work in this area. The semantic formalism of meaning representations range from lambda calculas (Montague, 1973), SQL, to abstract meaning representation (Banarescu et al., 2013). At the core of most recent models (Chen et al., 2018; Cheng et al., 2019; Lin et al., 2019; Zhang et al., 2019b; Yin and Neubig, 2018) is S EQ 2S EQ with attention (Bahdanau et al., 2014) by formulating the task as a machine translation problem. C OARSE 2F INE (Dong and Lapata, 2018) reports the highest accuracy on G EO Q UERY (Zelle and Mooney, 1996) and ATIS (Price, 1990) in a supervised setting. IRN ET (Guo et al., 2019) and RATSQL (Wang et al., 2019) are two best performing models on the Text-to-SQL benchmark, S PIDER (Yu et al., 2018). They are also designed to be able to generalize to unseen database schemas. However, supervised models perform well only when"
2021.eacl-main.109,P15-1033,0,0.0141305,"s. Thus we apply a template normalization procedure in a similar manner as (Iyer et al., 2019) to preprocess all LF templates. It collapses idioms into single units such that all LF templates are converted into a compact form. The neural transition system consists of an encoder and a decoder for estimating action probabilities. P (a|x) = |a| Y P (at |a&lt;t , x) (1) t=1 Encoder We apply a bidirectional Long Shortterm Memory (LSTM) network (Gers et al., 1999) to map a sequence of n words into a sequence of contextual word representations {e}ni=1 . Template Decoder The decoder applies a stackLSTM (Dyer et al., 2015) to generate action sequences. A stack-LSTM is an unidirectional LSTM augmented with a pointer. The pointer points to a particular hidden state of the LSTM, which represents a particular state of the stack. It moves to a different hidden state to indicate a different state of the stack. At time t, the stack-LSTM produces a hidden state hdt by hdt = LSTM(µt , hdt−1 ), where µt is a concatenation of the embedding of the action cat−1 estimated at time t − 1 and the representation hyt−1 of the partial tree generated by history actions at time t − 1. As a common practice, hdt is concatenated with a"
2021.eacl-main.109,W13-2322,0,0.0434961,"s the competitive baselines with a significant margin. The ablation study demonstrates the effectiveness of each individual proposed technique. The results are statistically significant with p≤0.05 according to the Wilcoxon signed-rank test (Wilcoxon, 1992). 2 Related Work Semantic parsing There is ample of work on machine learning models for semantic parsing. The recent surveys (Kamath and Das, 2018; Zhu et al., 2019) cover a wide range of work in this area. The semantic formalism of meaning representations range from lambda calculas (Montague, 1973), SQL, to abstract meaning representation (Banarescu et al., 2013). At the core of most recent models (Chen et al., 2018; Cheng et al., 2019; Lin et al., 2019; Zhang et al., 2019b; Yin and Neubig, 2018) is S EQ 2S EQ with attention (Bahdanau et al., 2014) by formulating the task as a machine translation problem. C OARSE 2F INE (Dong and Lapata, 2018) reports the highest accuracy on G EO Q UERY (Zelle and Mooney, 1996) and ATIS (Price, 1990) in a supervised setting. IRN ET (Guo et al., 2019) and RATSQL (Wang et al., 2019) are two best performing models on the Text-to-SQL benchmark, S PIDER (Yu et al., 2018). They are also designed to be able to generalize to"
2021.eacl-main.109,P18-1033,0,0.0498769,"Missing"
2021.eacl-main.109,D18-1216,0,0.0162668,"nario in pre-training. • Attention regularization. In this work, new predicates appear approximately once or twice during training. Thus, it is insufficient to learn reliable attention scores in the Seq2Seq architecture for those predicates. In the spirit of supervised attention (Liu et al., 2016), we propose to regularize them with alignment scores estimated by using co-occurrence statistics and string similarity between words and predicates. The prior work on supervised attention is not applicable, because it requires either large parallel data (Liu et al., 2016), significant manual effort (Bao et al., 2018; Rabinovich et al., 2017), or it is designed only for applications other than semantic parsing (Liu et al., 2017; Kamigaito et al., 2017). • Pre-training smoothing. The vocabulary of predicates in fine-tuning is higher than that in pre-training, which leads to a distribution discrepancy between the two training stages. Inspired by Laplace smoothing (Manning et al., 2008), we achieve significant performance gain by applying a smoothing technique during pre-training to alleviate the discrepancy. Our extensive experiments on three benchmark corpora show that ProtoParser outperforms the competiti"
2021.eacl-main.109,S13-1045,0,0.0342334,"Q UERY (Zelle and Mooney, 1996) and ATIS (Price, 1990) in a supervised setting. IRN ET (Guo et al., 2019) and RATSQL (Wang et al., 2019) are two best performing models on the Text-to-SQL benchmark, S PIDER (Yu et al., 2018). They are also designed to be able to generalize to unseen database schemas. However, supervised models perform well only when there is sufficient training data. Data Sparsity Most semantic parsing datasets are small in size. To address this issue, one line of research is to augment existing datasets with automatically generated data (Su and Yan, 2017; Jia and Liang, 2016; Cai and Yates, 2013). Another line of research is to exploit available resources, such as knowledge bases (Krishnamurthy et al., 2017; Herzig and Berant, 2018; Chang et al., 2019; Lee, 2019; Zhang et al., 2019a; Guo et al., 2019; Wang et al., 2019), semantic features in different domains (Dadashkarimi et al., 2018; Li et al., 2020), or unlabeled data (Yin et al., 2018; Koˇcisk`y et al., 2016; Sun et al., 2019). Those works are orthogonal to our setting because our approach aims to efficiently exploit a handful of labeled data of new predicates, which are not limited to the ones in knowledge bases. Our setting als"
2021.eacl-main.109,P19-1444,0,0.233851,"range of work in this area. The semantic formalism of meaning representations range from lambda calculas (Montague, 1973), SQL, to abstract meaning representation (Banarescu et al., 2013). At the core of most recent models (Chen et al., 2018; Cheng et al., 2019; Lin et al., 2019; Zhang et al., 2019b; Yin and Neubig, 2018) is S EQ 2S EQ with attention (Bahdanau et al., 2014) by formulating the task as a machine translation problem. C OARSE 2F INE (Dong and Lapata, 2018) reports the highest accuracy on G EO Q UERY (Zelle and Mooney, 1996) and ATIS (Price, 1990) in a supervised setting. IRN ET (Guo et al., 2019) and RATSQL (Wang et al., 2019) are two best performing models on the Text-to-SQL benchmark, S PIDER (Yu et al., 2018). They are also designed to be able to generalize to unseen database schemas. However, supervised models perform well only when there is sufficient training data. Data Sparsity Most semantic parsing datasets are small in size. To address this issue, one line of research is to augment existing datasets with automatically generated data (Su and Yan, 2017; Jia and Liang, 2016; Cai and Yates, 2013). Another line of research is to exploit available resources, such as knowledge bases"
2021.eacl-main.109,D18-1190,0,0.0173847,") are two best performing models on the Text-to-SQL benchmark, S PIDER (Yu et al., 2018). They are also designed to be able to generalize to unseen database schemas. However, supervised models perform well only when there is sufficient training data. Data Sparsity Most semantic parsing datasets are small in size. To address this issue, one line of research is to augment existing datasets with automatically generated data (Su and Yan, 2017; Jia and Liang, 2016; Cai and Yates, 2013). Another line of research is to exploit available resources, such as knowledge bases (Krishnamurthy et al., 2017; Herzig and Berant, 2018; Chang et al., 2019; Lee, 2019; Zhang et al., 2019a; Guo et al., 2019; Wang et al., 2019), semantic features in different domains (Dadashkarimi et al., 2018; Li et al., 2020), or unlabeled data (Yin et al., 2018; Koˇcisk`y et al., 2016; Sun et al., 2019). Those works are orthogonal to our setting because our approach aims to efficiently exploit a handful of labeled data of new predicates, which are not limited to the ones in knowledge bases. Our setting also does not require involvement of humans in the loop such as active learning (Duong et al., 2018; Ni et al., 2019) and crowd-sourcing (Wan"
2021.eacl-main.109,D19-1394,0,0.0112045,"2019; Lee, 2019; Zhang et al., 2019a; Guo et al., 2019; Wang et al., 2019), semantic features in different domains (Dadashkarimi et al., 2018; Li et al., 2020), or unlabeled data (Yin et al., 2018; Koˇcisk`y et al., 2016; Sun et al., 2019). Those works are orthogonal to our setting because our approach aims to efficiently exploit a handful of labeled data of new predicates, which are not limited to the ones in knowledge bases. Our setting also does not require involvement of humans in the loop such as active learning (Duong et al., 2018; Ni et al., 2019) and crowd-sourcing (Wang et al., 2015; Herzig and Berant, 2019). We assume availability of resources different than the prior work and focus on the problems caused by new predicates. We develop an approach to generalize to unseen LF templates consisting of both known and new predicates. Few-Shot Learning Few-shot learning is a type of machine learning problems that provides a handful of labeled training examples for a specific task. The survey (Zhu et al., 2019) gives a comprehensive overview of the data, models, and algorithms 1282 t t1 t2 t3 t4 t5 t6 t7 Actions GEN [(ground transport va )] GEN [(to city va ve )] GEN [(from airport va ve )] GEN [(= (grou"
2021.eacl-main.109,C18-1041,0,0.0611641,"Missing"
2021.eacl-main.109,N18-2115,0,0.117991,"83 19.61 16.08 19.06 28.22 30.84 21.31 17.76 32.5 33.01 14.36 33.59 35.63 28.93 28.55 35.05 44.08 40.97 20.88 22.52 48.45 18.76 7.91 10.17 21.08 9.25 17.73 20.11 15.73 18.05 17.18 12.28 22.48 3.32e-04 6.65e-06 5.30e-05 1.48e-04 2.35e-06 1.13e-05 2.86e-05 2.76e-03 2.47e-04 1.13e-06 1.73e-06 Table 2: Evaluation of learning results on three datasets. (Left) The one-shot results. (Right) The two-shot results. Baselines. We compared our methods with five competitive baselines, S EQ 2S EQ with attention (Luong et al., 2015), C OARSE 2F INE (Dong and Lapata, 2018), IRN ET (Guo et al., 2019), PTMAML (Huang et al., 2018) and DA (Li et al., 2020). C OARSE 2F INE is the best performing supervised model on the standard split of G EO Q UERY and ATIS datasets. PT-MAML is a few-shot learning semantic parser that adopts Model-Agnostic Meta-Learning (Finn et al., 2017). We adapt PTMAML in our scenario by considering a group of instances that share the same template as a pseudotask. DA is the most recently proposed neural semantic parser applying domain adaptation techniques. IRN ET is the strongest semantic parser that can generalize to unseen database schemas. In our case, we consider a list of predicates in support"
2021.eacl-main.109,D19-1545,0,0.0123551,"e stack. Table 1 shows such an action sequence for generating the above LF template. Each action produces known or new predicates. 3.2 Base Parser ProtoParser generates an LF in two steps: i) template generation, ii) slot filling. The base architecture largely resembles (Cheng et al., 2019). 1283 Template Generation Given an utterance, the task is to generate a sequence of actions a = a1 , ..., ak to build an abstract tree τy . We found out LFs often contain idioms, which are frequent subtrees shared across LF templates. Thus we apply a template normalization procedure in a similar manner as (Iyer et al., 2019) to preprocess all LF templates. It collapses idioms into single units such that all LF templates are converted into a compact form. The neural transition system consists of an encoder and a decoder for estimating action probabilities. P (a|x) = |a| Y P (at |a&lt;t , x) (1) t=1 Encoder We apply a bidirectional Long Shortterm Memory (LSTM) network (Gers et al., 1999) to map a sequence of n words into a sequence of contextual word representations {e}ni=1 . Template Decoder The decoder applies a stackLSTM (Dyer et al., 2015) to generate action sequences. A stack-LSTM is an unidirectional LSTM augmen"
2021.eacl-main.109,P16-1002,0,0.0298079,"est accuracy on G EO Q UERY (Zelle and Mooney, 1996) and ATIS (Price, 1990) in a supervised setting. IRN ET (Guo et al., 2019) and RATSQL (Wang et al., 2019) are two best performing models on the Text-to-SQL benchmark, S PIDER (Yu et al., 2018). They are also designed to be able to generalize to unseen database schemas. However, supervised models perform well only when there is sufficient training data. Data Sparsity Most semantic parsing datasets are small in size. To address this issue, one line of research is to augment existing datasets with automatically generated data (Su and Yan, 2017; Jia and Liang, 2016; Cai and Yates, 2013). Another line of research is to exploit available resources, such as knowledge bases (Krishnamurthy et al., 2017; Herzig and Berant, 2018; Chang et al., 2019; Lee, 2019; Zhang et al., 2019a; Guo et al., 2019; Wang et al., 2019), semantic features in different domains (Dadashkarimi et al., 2018; Li et al., 2020), or unlabeled data (Yin et al., 2018; Koˇcisk`y et al., 2016; Sun et al., 2019). Those works are orthogonal to our setting because our approach aims to efficiently exploit a handful of labeled data of new predicates, which are not limited to the ones in knowledge"
2021.eacl-main.109,I17-2002,0,0.0531057,"Missing"
2021.eacl-main.109,D16-1116,0,0.0643745,"Missing"
2021.eacl-main.109,D17-1160,0,0.0255054,"nd RATSQL (Wang et al., 2019) are two best performing models on the Text-to-SQL benchmark, S PIDER (Yu et al., 2018). They are also designed to be able to generalize to unseen database schemas. However, supervised models perform well only when there is sufficient training data. Data Sparsity Most semantic parsing datasets are small in size. To address this issue, one line of research is to augment existing datasets with automatically generated data (Su and Yan, 2017; Jia and Liang, 2016; Cai and Yates, 2013). Another line of research is to exploit available resources, such as knowledge bases (Krishnamurthy et al., 2017; Herzig and Berant, 2018; Chang et al., 2019; Lee, 2019; Zhang et al., 2019a; Guo et al., 2019; Wang et al., 2019), semantic features in different domains (Dadashkarimi et al., 2018; Li et al., 2020), or unlabeled data (Yin et al., 2018; Koˇcisk`y et al., 2016; Sun et al., 2019). Those works are orthogonal to our setting because our approach aims to efficiently exploit a handful of labeled data of new predicates, which are not limited to the ones in knowledge bases. Our setting also does not require involvement of humans in the loop such as active learning (Duong et al., 2018; Ni et al., 2019"
2021.eacl-main.109,D19-1624,0,0.0166178,"to-SQL benchmark, S PIDER (Yu et al., 2018). They are also designed to be able to generalize to unseen database schemas. However, supervised models perform well only when there is sufficient training data. Data Sparsity Most semantic parsing datasets are small in size. To address this issue, one line of research is to augment existing datasets with automatically generated data (Su and Yan, 2017; Jia and Liang, 2016; Cai and Yates, 2013). Another line of research is to exploit available resources, such as knowledge bases (Krishnamurthy et al., 2017; Herzig and Berant, 2018; Chang et al., 2019; Lee, 2019; Zhang et al., 2019a; Guo et al., 2019; Wang et al., 2019), semantic features in different domains (Dadashkarimi et al., 2018; Li et al., 2020), or unlabeled data (Yin et al., 2018; Koˇcisk`y et al., 2016; Sun et al., 2019). Those works are orthogonal to our setting because our approach aims to efficiently exploit a handful of labeled data of new predicates, which are not limited to the ones in knowledge bases. Our setting also does not require involvement of humans in the loop such as active learning (Duong et al., 2018; Ni et al., 2019) and crowd-sourcing (Wang et al., 2015; Herzig and Bera"
2021.eacl-main.109,J82-2003,0,0.700581,"Missing"
2021.eacl-main.109,C16-1291,0,0.0265894,"resented with supervisely learned embeddings, while new predicates are better initialized by a metric-based few-shot learning algorithm (Snell et al., 2017). In order to let the two types of embeddings work together in a single model, we devised a training procedure called predicate-dropout to simulate the testing scenario in pre-training. • Attention regularization. In this work, new predicates appear approximately once or twice during training. Thus, it is insufficient to learn reliable attention scores in the Seq2Seq architecture for those predicates. In the spirit of supervised attention (Liu et al., 2016), we propose to regularize them with alignment scores estimated by using co-occurrence statistics and string similarity between words and predicates. The prior work on supervised attention is not applicable, because it requires either large parallel data (Liu et al., 2016), significant manual effort (Bao et al., 2018; Rabinovich et al., 2017), or it is designed only for applications other than semantic parsing (Liu et al., 2017; Kamigaito et al., 2017). • Pre-training smoothing. The vocabulary of predicates in fine-tuning is higher than that in pre-training, which leads to a distribution discr"
2021.eacl-main.109,P17-1164,0,0.0489678,"Missing"
2021.eacl-main.109,D15-1166,0,0.162035,"of the stack. At time t, the stack-LSTM produces a hidden state hdt by hdt = LSTM(µt , hdt−1 ), where µt is a concatenation of the embedding of the action cat−1 estimated at time t − 1 and the representation hyt−1 of the partial tree generated by history actions at time t − 1. As a common practice, hdt is concatenated with an attended representation hat over hidden  dencoder  ht states to yield ht , with ht = W a , where W is ht a weight matrix and hat is created by soft attention, hat = n X P (ei |hdt )ei (2) i=1 We apply dot product to compute the normalized attention scores P (ei |hdt ) (Luong et al., 2015). The supervised attention (Rabinovich et al., 2017; Yin and Neubig, 2018) is also applied to facilitate the learning of attention weights. Given ht , the probability of an action is estimated by: exp(c|at ht ) | a0 ∈At exp(ca0 ht ) P (at |ht ) = P (3) where ca denotes the embedding of action a, and At denotes the set of applicable actions at time t. The initialization of those embeddings will be explained in the following section. Slot Filling A tree node in a semantic tree may contain more than one slot variables due to template normalization. Since there are two types of slot variables, giv"
2021.eacl-main.109,D14-1162,0,0.0854933,"uation set. Training Details. We pre-train our parser on the training sets for {80, 100} epochs with the Adam optimizer (Kingma and Ba, 2014). The batch size is fixed to 64. The initial learning rate is 0.0025, and the weights are decayed after 20 epochs with decay rate 0.985. The predicate dropout rate is 0.5. The smoothing term is set to {3, 6}. The number of meta-support examples is 30 and the number of meta-test examples per support example is 15. The coefficient of attention regularization is set to 0.01 on J OBS and 1 on the other datasets. We employ the 200-dimensional GLOVE embedding (Pennington et al., 2014) to initialize the word embeddings for utterances. The hidden state size of all LSTM models (Hochreiter and Schmidhuber, 1997) is 256. During fine-tuning, the batch size is 2, the learning rates and the epochs are selected from {0.001, 0.0005} and {20, 30, 40, 60, 120}, respectively. 1286 S EQ 2S EQ (pt) S EQ 2S EQ (cb) S EQ 2S EQ (os) C OARSE 2F INE (pt) C OARSE 2F INE (cb) C OARSE 2F INE (os) IRN ET (pt) IRN ET (cb) IRN ET (os) DA PT-MAML Ours J OBS G EO Q UERY ATIS J OBS G EO Q UERY ATIS p-values 11.27 11.70 14.18 10.91 9.28 6.73 16.00 19.67 14.91 18.91 11.64 27.09 20.00 7.64 11.38 24.07 14"
2021.eacl-main.109,H90-1020,0,0.124589,"and Das, 2018; Zhu et al., 2019) cover a wide range of work in this area. The semantic formalism of meaning representations range from lambda calculas (Montague, 1973), SQL, to abstract meaning representation (Banarescu et al., 2013). At the core of most recent models (Chen et al., 2018; Cheng et al., 2019; Lin et al., 2019; Zhang et al., 2019b; Yin and Neubig, 2018) is S EQ 2S EQ with attention (Bahdanau et al., 2014) by formulating the task as a machine translation problem. C OARSE 2F INE (Dong and Lapata, 2018) reports the highest accuracy on G EO Q UERY (Zelle and Mooney, 1996) and ATIS (Price, 1990) in a supervised setting. IRN ET (Guo et al., 2019) and RATSQL (Wang et al., 2019) are two best performing models on the Text-to-SQL benchmark, S PIDER (Yu et al., 2018). They are also designed to be able to generalize to unseen database schemas. However, supervised models perform well only when there is sufficient training data. Data Sparsity Most semantic parsing datasets are small in size. To address this issue, one line of research is to augment existing datasets with automatically generated data (Su and Yan, 2017; Jia and Liang, 2016; Cai and Yates, 2013). Another line of research is to e"
2021.eacl-main.109,P17-1105,0,0.183146,"ing. • Attention regularization. In this work, new predicates appear approximately once or twice during training. Thus, it is insufficient to learn reliable attention scores in the Seq2Seq architecture for those predicates. In the spirit of supervised attention (Liu et al., 2016), we propose to regularize them with alignment scores estimated by using co-occurrence statistics and string similarity between words and predicates. The prior work on supervised attention is not applicable, because it requires either large parallel data (Liu et al., 2016), significant manual effort (Bao et al., 2018; Rabinovich et al., 2017), or it is designed only for applications other than semantic parsing (Liu et al., 2017; Kamigaito et al., 2017). • Pre-training smoothing. The vocabulary of predicates in fine-tuning is higher than that in pre-training, which leads to a distribution discrepancy between the two training stages. Inspired by Laplace smoothing (Manning et al., 2008), we achieve significant performance gain by applying a smoothing technique during pre-training to alleviate the discrepancy. Our extensive experiments on three benchmark corpora show that ProtoParser outperforms the competitive baselines with a signif"
2021.eacl-main.109,D17-1127,0,0.0179527,") reports the highest accuracy on G EO Q UERY (Zelle and Mooney, 1996) and ATIS (Price, 1990) in a supervised setting. IRN ET (Guo et al., 2019) and RATSQL (Wang et al., 2019) are two best performing models on the Text-to-SQL benchmark, S PIDER (Yu et al., 2018). They are also designed to be able to generalize to unseen database schemas. However, supervised models perform well only when there is sufficient training data. Data Sparsity Most semantic parsing datasets are small in size. To address this issue, one line of research is to augment existing datasets with automatically generated data (Su and Yan, 2017; Jia and Liang, 2016; Cai and Yates, 2013). Another line of research is to exploit available resources, such as knowledge bases (Krishnamurthy et al., 2017; Herzig and Berant, 2018; Chang et al., 2019; Lee, 2019; Zhang et al., 2019a; Guo et al., 2019; Wang et al., 2019), semantic features in different domains (Dadashkarimi et al., 2018; Li et al., 2020), or unlabeled data (Yin et al., 2018; Koˇcisk`y et al., 2016; Sun et al., 2019). Those works are orthogonal to our setting because our approach aims to efficiently exploit a handful of labeled data of new predicates, which are not limited to t"
2021.eacl-main.109,P18-1070,0,0.0165345,"re is sufficient training data. Data Sparsity Most semantic parsing datasets are small in size. To address this issue, one line of research is to augment existing datasets with automatically generated data (Su and Yan, 2017; Jia and Liang, 2016; Cai and Yates, 2013). Another line of research is to exploit available resources, such as knowledge bases (Krishnamurthy et al., 2017; Herzig and Berant, 2018; Chang et al., 2019; Lee, 2019; Zhang et al., 2019a; Guo et al., 2019; Wang et al., 2019), semantic features in different domains (Dadashkarimi et al., 2018; Li et al., 2020), or unlabeled data (Yin et al., 2018; Koˇcisk`y et al., 2016; Sun et al., 2019). Those works are orthogonal to our setting because our approach aims to efficiently exploit a handful of labeled data of new predicates, which are not limited to the ones in knowledge bases. Our setting also does not require involvement of humans in the loop such as active learning (Duong et al., 2018; Ni et al., 2019) and crowd-sourcing (Wang et al., 2015; Herzig and Berant, 2019). We assume availability of resources different than the prior work and focus on the problems caused by new predicates. We develop an approach to generalize to unseen LF te"
2021.eacl-main.109,D18-1425,0,0.0454545,"73), SQL, to abstract meaning representation (Banarescu et al., 2013). At the core of most recent models (Chen et al., 2018; Cheng et al., 2019; Lin et al., 2019; Zhang et al., 2019b; Yin and Neubig, 2018) is S EQ 2S EQ with attention (Bahdanau et al., 2014) by formulating the task as a machine translation problem. C OARSE 2F INE (Dong and Lapata, 2018) reports the highest accuracy on G EO Q UERY (Zelle and Mooney, 1996) and ATIS (Price, 1990) in a supervised setting. IRN ET (Guo et al., 2019) and RATSQL (Wang et al., 2019) are two best performing models on the Text-to-SQL benchmark, S PIDER (Yu et al., 2018). They are also designed to be able to generalize to unseen database schemas. However, supervised models perform well only when there is sufficient training data. Data Sparsity Most semantic parsing datasets are small in size. To address this issue, one line of research is to augment existing datasets with automatically generated data (Su and Yan, 2017; Jia and Liang, 2016; Cai and Yates, 2013). Another line of research is to exploit available resources, such as knowledge bases (Krishnamurthy et al., 2017; Herzig and Berant, 2018; Chang et al., 2019; Lee, 2019; Zhang et al., 2019a; Guo et al.,"
2021.eacl-main.109,D19-1537,0,0.0304484,"Missing"
2021.eacl-main.109,D19-1392,0,0.0514867,"Missing"
2021.eacl-main.109,P15-1129,0,0.0868032,"Missing"
2021.eacl-main.109,D18-2002,0,0.0910397,"ique. The results are statistically significant with p≤0.05 according to the Wilcoxon signed-rank test (Wilcoxon, 1992). 2 Related Work Semantic parsing There is ample of work on machine learning models for semantic parsing. The recent surveys (Kamath and Das, 2018; Zhu et al., 2019) cover a wide range of work in this area. The semantic formalism of meaning representations range from lambda calculas (Montague, 1973), SQL, to abstract meaning representation (Banarescu et al., 2013). At the core of most recent models (Chen et al., 2018; Cheng et al., 2019; Lin et al., 2019; Zhang et al., 2019b; Yin and Neubig, 2018) is S EQ 2S EQ with attention (Bahdanau et al., 2014) by formulating the task as a machine translation problem. C OARSE 2F INE (Dong and Lapata, 2018) reports the highest accuracy on G EO Q UERY (Zelle and Mooney, 1996) and ATIS (Price, 1990) in a supervised setting. IRN ET (Guo et al., 2019) and RATSQL (Wang et al., 2019) are two best performing models on the Text-to-SQL benchmark, S PIDER (Yu et al., 2018). They are also designed to be able to generalize to unseen database schemas. However, supervised models perform well only when there is sufficient training data. Data Sparsity Most semanti"
2021.eacl-main.233,D18-1337,0,0.442044,"research on S I MT relies on a strategy to decide when to read a word from the input or write a word to the output (Satija and Pineau, 2016; Gu et al., 2017). This is based on a sequential decision making formulation of S I MT, where the decision making about the next R EAD/W RITE action is made by an agent, interacting with the neural machine translation (NMT) environment. Current approaches are sub-optimal as they either fix the agent’s policy to focus learning the NMT model (Ma et al., 2019; Dalvi et al., 2018) or learn adaptive agent policies while the NMT model is fixed (Gu et al., 2017; Alinejad et al., 2018). We argue that the interpreter should also learn to generate correct translation from incomplete input information. This is challenging as we need to optimize We present an IL approach to efficiently learn effective coupled programmer-interpreter policies in S I MT, based on the following contributions. First, we present a simple, fast, and effective algorithmic oracle to produce oracle actions from the training bilingual sentence-pairs based on statistical word alignments (Brown et al., 1993). Next, we design a framework that uses scheduled sampling on both programmer and interpreter. This i"
2021.eacl-main.233,P19-1126,0,0.62478,"alignments between tokens. Unless otherwise specified, we use the default settings of the mentioned toolkit. Evaluation. We evaluate the S I MT systems based on its translation quality and delay. Translation quality can be measured by case sensitive BLEU (Papineni et al., 2002).3 We adopt three delay measurements by previous studies. First, average proportion (AP) (Gu et al., 2017) is a fraction of read source words per emited target words. Second, average lagging (AL) (Ma et al., 2019) is an average number of lagged source words until all inputs are read. Finally the differentiable-AL (DAL) (Arivazhagan et al., 2019) is a refinement of AL which also accumulates the cost of writing output tokens after inputs are fully read. Baseline. We compare against the wait-k baseline (Ma et al., 2019) where the programmer’s policy begins with k numbers of R EAD, and is followed by switching W RITE and R EAD, until the source sentence is exhausted or end of sentence (EOS) symbol is written. If the source sentence is exhausted, the programmer will only emit W RITE actions. This baseline was shown to be superior compared to the reinforcement learning approach (Zheng et al., 2019a), and k can be tuned for the desired dela"
2021.eacl-main.233,2020.iwslt-1.27,0,0.364956,"proach, showing the scalability of our approach. 5 Related Work Satija and Pineau (2016); Gu et al. (2017) and Alinejad et al. (2018) formulate simultane5 Because of the limitation of our computational resources, we are unable to use multiple GPUs for larger batch size. Using smaller batch size is known to reduce the overall performance of the transformer. 2716 ous NMT as sequential decision making problem where an agent interacts with the environment (i.e. the underlying NMT model) through R EAD/W RITE actions. They pre-train the NMT system, while the agent’s policy is trained using deep-RL. Arivazhagan et al. (2020) highlights the poor performance of finetuned offline translation model when translating prefixes of input, which is the case of S I MT. Their approach uses retranslation strategy where every R EAD is performed, a new translation is generated from scratch, allowing revising translation on the fly and mitigating error propagation on the decoder that was attributed to the insufficient evidence when generating past output words. Their approach uses a stability metric which takes number of suffixes revisions made to produce latest translation. This approach involves wait-k inference, which limits"
2021.eacl-main.233,J93-2003,0,0.19235,"lvi et al., 2018) or learn adaptive agent policies while the NMT model is fixed (Gu et al., 2017; Alinejad et al., 2018). We argue that the interpreter should also learn to generate correct translation from incomplete input information. This is challenging as we need to optimize We present an IL approach to efficiently learn effective coupled programmer-interpreter policies in S I MT, based on the following contributions. First, we present a simple, fast, and effective algorithmic oracle to produce oracle actions from the training bilingual sentence-pairs based on statistical word alignments (Brown et al., 1993). Next, we design a framework that uses scheduled sampling on both programmer and interpreter. This is different from the typical IL scenarios, where there is only one policy to learn. As the two policies collaborate, their learning needs to be robust not only to their own incorrect predictions, but also to incorrect predictions of the other policy to mitigate this coupled exposure bias. Experiments on six language pairs (translating to English from Arabic, Czech, German, Romanian, Hungarian, and Bulgarian) show the policies trained using our approach compares favorably with strong policies fr"
2021.eacl-main.233,2012.eamt-1.60,0,0.0168744,"quality corpus which is designed for spoken dialogue and carefully edited dataset. Additionally, we perform a single large scale experiment using crawled corpus such as WMT to show that our method also scales to a large dataset. We evaluate our proposed method on 6 language pairs, in all cases translating into English, with the source languages chosen to cover a wide range 2 Our oracle algorithm’s code is released in https://github.com/Monash-NLP-ML-Group/ arthur-eacl2021. of language families and syntax. We use German (DE), Czech (CS) and Arabic (AR) from the IWSLT 2016 translation dataset (Cettolo et al., 2012). We use the provided training and development sets as-is, and concatenate all provided test sets to create our test set. We also evaluate Hungarian (HR), Bulgarian (BG), and Romanian (RO) from the SETIMES corpus (Tyers and Alperen, 2010). As this corpus is not partitioned, we use the majority of the data for training, holding out 2000 random sentence pairs for development and another 2000 sentence pairs for testing. Together these languages are representative of Germanic, West Slavic, Arabic, Uralic, East Slavic, and Italic language families, respectively. We use sentencepiece (Kudo and Richa"
2021.eacl-main.233,N18-2079,0,0.487912,"needs to trade off delaying translation output and the quality of the generated translation. Recent research on S I MT relies on a strategy to decide when to read a word from the input or write a word to the output (Satija and Pineau, 2016; Gu et al., 2017). This is based on a sequential decision making formulation of S I MT, where the decision making about the next R EAD/W RITE action is made by an agent, interacting with the neural machine translation (NMT) environment. Current approaches are sub-optimal as they either fix the agent’s policy to focus learning the NMT model (Ma et al., 2019; Dalvi et al., 2018) or learn adaptive agent policies while the NMT model is fixed (Gu et al., 2017; Alinejad et al., 2018). We argue that the interpreter should also learn to generate correct translation from incomplete input information. This is challenging as we need to optimize We present an IL approach to efficiently learn effective coupled programmer-interpreter policies in S I MT, based on the following contributions. First, we present a simple, fast, and effective algorithmic oracle to produce oracle actions from the training bilingual sentence-pairs based on statistical word alignments (Brown et al., 199"
2021.eacl-main.233,N13-1073,0,0.10195,"d Romanian (RO) from the SETIMES corpus (Tyers and Alperen, 2010). As this corpus is not partitioned, we use the majority of the data for training, holding out 2000 random sentence pairs for development and another 2000 sentence pairs for testing. Together these languages are representative of Germanic, West Slavic, Arabic, Uralic, East Slavic, and Italic language families, respectively. We use sentencepiece (Kudo and Richardson, 2018) to build and tokenize our training data with 16k vocabulary size. Then we generate our oracle program actions based on the segmented tokens. We use fast_align (Dyer et al., 2013) to generate symmetrized alignments between tokens. Unless otherwise specified, we use the default settings of the mentioned toolkit. Evaluation. We evaluate the S I MT systems based on its translation quality and delay. Translation quality can be measured by case sensitive BLEU (Papineni et al., 2002).3 We adopt three delay measurements by previous studies. First, average proportion (AP) (Gu et al., 2017) is a fraction of read source words per emited target words. Second, average lagging (AL) (Ma et al., 2019) is an average number of lagged source words until all inputs are read. Finally the"
2021.eacl-main.233,D14-1140,0,0.176588,"Missing"
2021.eacl-main.233,E17-1099,0,0.20855,"te sequence of R EAD/W RITE actions with low translation latency and high translation quality is under-explored. Introduction Simultaneous machine translation (S I MT) is a setting where the translator needs to incrementally generate the translation while the source utterance is being received. This is a challenging translation scenario as the S I MT model needs to trade off delaying translation output and the quality of the generated translation. Recent research on S I MT relies on a strategy to decide when to read a word from the input or write a word to the output (Satija and Pineau, 2016; Gu et al., 2017). This is based on a sequential decision making formulation of S I MT, where the decision making about the next R EAD/W RITE action is made by an agent, interacting with the neural machine translation (NMT) environment. Current approaches are sub-optimal as they either fix the agent’s policy to focus learning the NMT model (Ma et al., 2019; Dalvi et al., 2018) or learn adaptive agent policies while the NMT model is fixed (Gu et al., 2017; Alinejad et al., 2018). We argue that the interpreter should also learn to generate correct translation from incomplete input information. This is challengin"
2021.eacl-main.233,N03-1017,0,0.285791,"Missing"
2021.eacl-main.233,D18-2012,0,0.0178505,"et al., 2012). We use the provided training and development sets as-is, and concatenate all provided test sets to create our test set. We also evaluate Hungarian (HR), Bulgarian (BG), and Romanian (RO) from the SETIMES corpus (Tyers and Alperen, 2010). As this corpus is not partitioned, we use the majority of the data for training, holding out 2000 random sentence pairs for development and another 2000 sentence pairs for testing. Together these languages are representative of Germanic, West Slavic, Arabic, Uralic, East Slavic, and Italic language families, respectively. We use sentencepiece (Kudo and Richardson, 2018) to build and tokenize our training data with 16k vocabulary size. Then we generate our oracle program actions based on the segmented tokens. We use fast_align (Dyer et al., 2013) to generate symmetrized alignments between tokens. Unless otherwise specified, we use the default settings of the mentioned toolkit. Evaluation. We evaluate the S I MT systems based on its translation quality and delay. Translation quality can be measured by case sensitive BLEU (Papineni et al., 2002).3 We adopt three delay measurements by previous studies. First, average proportion (AP) (Gu et al., 2017) is a fracti"
2021.eacl-main.233,D15-1166,0,0.0411061,"Missing"
2021.eacl-main.233,W18-6322,0,0.0338959,"Missing"
2021.eacl-main.233,P02-1040,0,0.109673,"anic, West Slavic, Arabic, Uralic, East Slavic, and Italic language families, respectively. We use sentencepiece (Kudo and Richardson, 2018) to build and tokenize our training data with 16k vocabulary size. Then we generate our oracle program actions based on the segmented tokens. We use fast_align (Dyer et al., 2013) to generate symmetrized alignments between tokens. Unless otherwise specified, we use the default settings of the mentioned toolkit. Evaluation. We evaluate the S I MT systems based on its translation quality and delay. Translation quality can be measured by case sensitive BLEU (Papineni et al., 2002).3 We adopt three delay measurements by previous studies. First, average proportion (AP) (Gu et al., 2017) is a fraction of read source words per emited target words. Second, average lagging (AL) (Ma et al., 2019) is an average number of lagged source words until all inputs are read. Finally the differentiable-AL (DAL) (Arivazhagan et al., 2019) is a refinement of AL which also accumulates the cost of writing output tokens after inputs are fully read. Baseline. We compare against the wait-k baseline (Ma et al., 2019) where the programmer’s policy begins with k numbers of R EAD, and is followed"
2021.eacl-main.233,W18-6319,0,0.0262644,"Missing"
2021.eacl-main.233,D19-1137,0,0.813646,"ITE actions for training bilingual sentence-pairs using the notion of word alignments. This oracle actions are designed to capture enough information from the partial input before writing the output. Next, we perform a coupled scheduled sampling to effectively mitigate the exposure bias when learning both policies jointly with imitation learning. Experiments on six language-pairs show our method outperforms strong baselines in terms of translation quality while keeping the translation delay low. 1 Previous research has considered the use of imitation learning (IL) to train the agent’s policy (Zheng et al., 2019a,b), which is generally superior to reinforcement Learning (RL) in terms of the stability and sample complexity. However, the bottleneck of IL in S I MT is the unavailability of the oracle sequence of actions. Designing algorithmic oracles to compute sequence of R EAD/W RITE actions with low translation latency and high translation quality is under-explored. Introduction Simultaneous machine translation (S I MT) is a setting where the translator needs to incrementally generate the translation while the source utterance is being received. This is a challenging translation scenario as the S I M"
2021.eacl-main.233,P19-1582,0,0.388709,"ITE actions for training bilingual sentence-pairs using the notion of word alignments. This oracle actions are designed to capture enough information from the partial input before writing the output. Next, we perform a coupled scheduled sampling to effectively mitigate the exposure bias when learning both policies jointly with imitation learning. Experiments on six language-pairs show our method outperforms strong baselines in terms of translation quality while keeping the translation delay low. 1 Previous research has considered the use of imitation learning (IL) to train the agent’s policy (Zheng et al., 2019a,b), which is generally superior to reinforcement Learning (RL) in terms of the stability and sample complexity. However, the bottleneck of IL in S I MT is the unavailability of the oracle sequence of actions. Designing algorithmic oracles to compute sequence of R EAD/W RITE actions with low translation latency and high translation quality is under-explored. Introduction Simultaneous machine translation (S I MT) is a setting where the translator needs to incrementally generate the translation while the source utterance is being received. This is a challenging translation scenario as the S I M"
2021.emnlp-main.233,2020.emnlp-main.255,0,0.011346,"ring LLE to three baselines on text classification tasks show that LLE can enhance the stability of the explanations for all seen tasks and maintain the same level of faithfulness to the black-box model as the teacher, while being up to 102 times faster at test time. Our ablation study shows that the ER mechanism in our LLE approach enhances the learning capabilities of the student explainer. Our code is available at https://github.com/situsnow/LLE. 1 Introduction 2021); perturbation-based methods, which observe changes in model performance after feature perturbation (Schwab and Karlen, 2019; Kim et al., 2020), or approximate the local decision boundary through perturbed samples (Ribeiro et al., 2016; Lundberg and Lee, 2017); and model-based methods, which train an explainer model by optimizing an explanation-meritorious objective,1 such as robustness/stability (Lakkaraju et al., 2020; AlvarezMelis and Jaakkola, 2018) that requires similar examples to have similar explanations. All these methods aim to explain static black-box models, whereas explaining dynamic ones, as in the lifelong learning (LL) (Silver et al., 2013) setting, is under-explored. We propose a Lifelong Explanation (LLE) approach t"
2021.emnlp-main.268,2020.acl-main.692,0,0.286428,"t language monotext in the vised domain adaptation (GUDA) for NMT as new domain. The challenge, however, is how to the problem of adapting an NMT model trained train a classifier for data selection for the languageon an old domain Pold (X, Y ) to a new domain side with missing data. We address this problem 1 in Section 3, then mention how to adapt the NMT Source code is available at https://github.com/ trangvu/guda. model to the new domain in Section 4. 3336 Figure 1: Our proposed cross-lingual data selection method for GUDA with source monotext Xnew . 3 Cross-lingual In-domain Data Selection Aharoni and Goldberg (2020) have shown that the emergent domain clusters via BERT (Devlin et al., 2019) can be used to select in-domain bitext for NMT. Inspired from that observation, we leverage the sentence representations produced by the multilingual BERT (mBERT) for cross-lingual monotext selection. We first align the source and target language representation space while preserving the domain clustering characteristics in each space. Using the available monotext in one language, we train a binary classifier to detect old and new domains on the aligned semantic spaces. This classifier is then transferred to pick in-d"
2021.emnlp-main.268,Q16-1008,0,0.0473635,"Missing"
2021.emnlp-main.268,N19-1423,0,0.0308071,"e challenge, however, is how to the problem of adapting an NMT model trained train a classifier for data selection for the languageon an old domain Pold (X, Y ) to a new domain side with missing data. We address this problem 1 in Section 3, then mention how to adapt the NMT Source code is available at https://github.com/ trangvu/guda. model to the new domain in Section 4. 3336 Figure 1: Our proposed cross-lingual data selection method for GUDA with source monotext Xnew . 3 Cross-lingual In-domain Data Selection Aharoni and Goldberg (2020) have shown that the emergent domain clusters via BERT (Devlin et al., 2019) can be used to select in-domain bitext for NMT. Inspired from that observation, we leverage the sentence representations produced by the multilingual BERT (mBERT) for cross-lingual monotext selection. We first align the source and target language representation space while preserving the domain clustering characteristics in each space. Using the available monotext in one language, we train a binary classifier to detect old and new domains on the aligned semantic spaces. This classifier is then transferred to pick in-domain sentences in the other language (fig. 1). Representation Alignment. We"
2021.emnlp-main.268,D17-1158,0,0.0183069,"del from the old to new domain by learning domain-invariant representations of both encoder and decoder via domain discrimination loss. Unsupervised Domain Adaptation of NMT. There are two main approaches in UDA for NMT, including model-centric and data-centric methods (Chu and Wang, 2018). In the model-centric approach, the model architecture is modified and jointly trained on MT tasks, and other auxiliary tasks such as language modelling (Gulcehre et al., 2015). On the other hand, the data-centric methods focus on constructing in-domain parallel corpus by data-selection from general corpus (Domhan and Hieber, 2017), and back-translation (Jin et al., 2020; Mahdieh et al., 2020). Most prior works in UDA of NMT often assume the availability of indomain data in the target language. While there are few studies on the UDA problem with in-domain source-language data in statistical MT (Mansour and Ney, 2014; Cuong et al., 2016), this problem remains unexplored in NMT. 8 Conclusion We have proposed a cross-lingual data selection method to the GUDA problem for NMT where only monolingual data from one language side is available in the new domain. We first learn an adaptive layer to align the BERT representation of"
2021.emnlp-main.268,2020.emnlp-main.475,0,0.0351701,"Missing"
2021.emnlp-main.268,D19-1147,0,0.017421,"ailability of either non-parallel texts of both lan- the missing language side, the original adaptation guages or only the target-language monolingual problem is transformed to the usual setting of UDA text in the new domain to adapt the NMT model. problem, and can be approached by the existing The adaptation is achieved by modifying the model UDA methods. In this paper, we extend the disarchitecture and joint training with other auxiliary criminative domain mixing method for supervised tasks (Gulcehre et al., 2015; Domhan and Hieber, domain adaptation (Britz et al., 2017) which jointly 2017; Dou et al., 2019), or constructing a parallel learns domain discrimination and translation to the corpus for the new domain from a general-domain unsupervised setting. More specifically, the NMT parallel text using data-selection methods (Silva model jointly learns to translate with the translation et al., 2018; Hu et al., 2019). However, very little loss on pseudo bitext, and captures the characteris3335 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3335–3346 c November 7–11, 2021. 2021 Association for Computational Linguistics tics of the new domain by the doma"
2021.emnlp-main.268,P13-2119,0,0.0109003,"oss five diverse (Chen et al., 2020), such that the representations domains in three language pairs, as well as a of source and target language are aligned. The real-world scenario of translation for COVID19. The results show that our proposed method aligned representations enable the transferability outperforms other selection baselines up to of a domain classifier trained on one language side +1.5 BLEU score. to the other language for in-domain data detection. Previous works have explored filtering data of the 1 Introduction same language for MT (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; JunczysUnsupervised domain adaptation (UDA) aims to generalise MT models trained on domains with typ- Dowmunt, 2018); however, utilising data in one ically large-scale bilingual parallel text to new do- language to detect in-domain data in the other lanmains without parallel data (Chu and Wang, 2018). guage is under-explored. Most prior works in UDA of NMT assume the With selected sentences in the new domain of availability of either non-parallel texts of both lan- the missing language side, the original adaptation guages or only the target-language monolingual problem is transformed to the"
2021.emnlp-main.268,W17-4713,0,0.0358141,"Missing"
2021.emnlp-main.268,D19-1433,0,0.0247952,"ot only able to preserve the domain clusters in Unsupervised Domain Adaptation. Previous English sentences but also learn domain clustered works in UDA has been focused on aligning dorepresentations of Arabic sentences in which the main distribution by minimising the discrepancy 3342 between representations of source and target domains (Shen et al., 2018; Wang et al., 2018); learning domain-invariant representation via adversarial learning (Ganin and Lempitsky, 2015; Shah et al., 2018; Moghimifar et al., 2020); bridging the domain gap by adaptive pretraining of contextualised word embeddings (Han and Eisenstein, 2019; Vu et al., 2020). In this paper, we adapt the NMT model from the old to new domain by learning domain-invariant representations of both encoder and decoder via domain discrimination loss. Unsupervised Domain Adaptation of NMT. There are two main approaches in UDA for NMT, including model-centric and data-centric methods (Chu and Wang, 2018). In the model-centric approach, the model architecture is modified and jointly trained on MT tasks, and other auxiliary tasks such as language modelling (Gulcehre et al., 2015). On the other hand, the data-centric methods focus on constructing in-domain p"
2021.emnlp-main.268,W18-6478,0,0.0513641,"Missing"
2021.emnlp-main.268,P07-2045,0,0.00908588,"Missing"
2021.emnlp-main.268,W17-3204,0,0.0262612,"get sentences in the old domain’s Dold bitext, and λ3 controls the effect of the old domain. 5 Experiments We evaluate our proposed approach for GUDA on the three language pairs covering five domains, and a real-world translation task, namely, TICO-19. 5.1 Setup Datasets. Table 1 shows data statistics. The general domain datasets come from WMT2014 for English-French, WMT2020 for English-German, news parallel corpus from OPUS for ArabicEnglish3 . We appraise our proposed methods on following specific domains: TED talk, Law, Medical, IT, Koran from OPUS (Tiedemann, 2012) following the recipe in Koehn and Knowles (2017). We sample 10M English sentences from Newcrawl 2007-2019 as the generic monolingual corpus. Data pre-processing is described in Appendix A. Source Monotext Loss. To take into account the clean text in source language of the new domain, we apply the discriminative domain mixing method (Britz et al., 2017) to force the encoder towards capturing new domain’s characteristics. For this purpose, we build a classifier cψ e (zz e ), a feedforward network parametrised by ψ e , whose output is x)) is the new domain’s probability. z e = h (encθ (x 3 the representation of the sentence x , computed by Glo"
2021.emnlp-main.268,P18-1007,0,0.0614737,"Missing"
2021.emnlp-main.268,W14-3359,0,0.014118,"n the model-centric approach, the model architecture is modified and jointly trained on MT tasks, and other auxiliary tasks such as language modelling (Gulcehre et al., 2015). On the other hand, the data-centric methods focus on constructing in-domain parallel corpus by data-selection from general corpus (Domhan and Hieber, 2017), and back-translation (Jin et al., 2020; Mahdieh et al., 2020). Most prior works in UDA of NMT often assume the availability of indomain data in the target language. While there are few studies on the UDA problem with in-domain source-language data in statistical MT (Mansour and Ney, 2014; Cuong et al., 2016), this problem remains unexplored in NMT. 8 Conclusion We have proposed a cross-lingual data selection method to the GUDA problem for NMT where only monolingual data from one language side is available in the new domain. We first learn an adaptive layer to align the BERT representation of the source and target languages. We then utilise a domain classifier trained on one language to select in-domain data for another. Experiments on translation tasks of several language pairs and domains show the effectiveness of our method over other baselines. Acknowledgments This materia"
2021.emnlp-main.268,2020.alta-1.1,1,0.733029,"high overlap rate. As can be seen, our 7 Related Works contrastive-based representation alignment method is not only able to preserve the domain clusters in Unsupervised Domain Adaptation. Previous English sentences but also learn domain clustered works in UDA has been focused on aligning dorepresentations of Arabic sentences in which the main distribution by minimising the discrepancy 3342 between representations of source and target domains (Shen et al., 2018; Wang et al., 2018); learning domain-invariant representation via adversarial learning (Ganin and Lempitsky, 2015; Shah et al., 2018; Moghimifar et al., 2020); bridging the domain gap by adaptive pretraining of contextualised word embeddings (Han and Eisenstein, 2019; Vu et al., 2020). In this paper, we adapt the NMT model from the old to new domain by learning domain-invariant representations of both encoder and decoder via domain discrimination loss. Unsupervised Domain Adaptation of NMT. There are two main approaches in UDA for NMT, including model-centric and data-centric methods (Chu and Wang, 2018). In the model-centric approach, the model architecture is modified and jointly trained on MT tasks, and other auxiliary tasks such as language mod"
2021.emnlp-main.268,P10-2041,0,0.359627,"ontrastive learning lection method on NMT across five diverse (Chen et al., 2020), such that the representations domains in three language pairs, as well as a of source and target language are aligned. The real-world scenario of translation for COVID19. The results show that our proposed method aligned representations enable the transferability outperforms other selection baselines up to of a domain classifier trained on one language side +1.5 BLEU score. to the other language for in-domain data detection. Previous works have explored filtering data of the 1 Introduction same language for MT (Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; JunczysUnsupervised domain adaptation (UDA) aims to generalise MT models trained on domains with typ- Dowmunt, 2018); however, utilising data in one ically large-scale bilingual parallel text to new do- language to detect in-domain data in the other lanmains without parallel data (Chu and Wang, 2018). guage is under-explored. Most prior works in UDA of NMT assume the With selected sentences in the new domain of availability of either non-parallel texts of both lan- the missing language side, the original adaptation guages or only the target-language mo"
2021.emnlp-main.268,N19-4009,0,0.0302512,"Missing"
2021.emnlp-main.268,P19-1286,0,0.0177854,"achieved by modifying the model UDA methods. In this paper, we extend the disarchitecture and joint training with other auxiliary criminative domain mixing method for supervised tasks (Gulcehre et al., 2015; Domhan and Hieber, domain adaptation (Britz et al., 2017) which jointly 2017; Dou et al., 2019), or constructing a parallel learns domain discrimination and translation to the corpus for the new domain from a general-domain unsupervised setting. More specifically, the NMT parallel text using data-selection methods (Silva model jointly learns to translate with the translation et al., 2018; Hu et al., 2019). However, very little loss on pseudo bitext, and captures the characteris3335 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3335–3346 c November 7–11, 2021. 2021 Association for Computational Linguistics tics of the new domain by the domain discrimination loss on data from the old and new domains. Our contributions can be summarised as follows: • We introduce a generalised UDA (GUDA) problem for NMT which unifies both the usual setting of having only target language monotext and the under-explored setting with only source language monotext in th"
2021.emnlp-main.268,P16-1009,0,0.0361253,"ngual text Xnew = {x the source language in the new domain and a generic monolingual text Dt of the target language. • An initial monolingual text Ynew = {yy k } of the target language in the new domain and a generic monolingual text Ds of the source language. Crucially, in both cases we do not require any parallel text in the new domain, hence the term unsupervised domain adaptation. The goal is to adapt an NMT model, parametrised by θ , trained on the old domain bitext Dold to the new domain. In the setting involving Ynew , it can be used to create pseudo-parallel data via back-translation (Sennrich et al., 2016), or to adapt the decoder via multi-task learning (Gulcehre et al., 2015; Domhan 2 Generalised Unsupervised Domain and Hieber, 2017). This setting is the usual formuAdaptation lation in UDA for NMT (Chu and Wang, 2018). In Domain adaptation is an important problem in contrast, the setting involving the source monotext NMT as it is very expensive to obtain training data Xnew is not well explored in the literature. that are both large and relevant to all possible doOur approach for addressing GUDA is to create mains. Supervised adaptation problem requires the in-domain monotext for the language"
2021.emnlp-main.268,D18-1131,0,0.0222994,"ns. ters exhibit a high overlap rate. As can be seen, our 7 Related Works contrastive-based representation alignment method is not only able to preserve the domain clusters in Unsupervised Domain Adaptation. Previous English sentences but also learn domain clustered works in UDA has been focused on aligning dorepresentations of Arabic sentences in which the main distribution by minimising the discrepancy 3342 between representations of source and target domains (Shen et al., 2018; Wang et al., 2018); learning domain-invariant representation via adversarial learning (Ganin and Lempitsky, 2015; Shah et al., 2018; Moghimifar et al., 2020); bridging the domain gap by adaptive pretraining of contextualised word embeddings (Han and Eisenstein, 2019; Vu et al., 2020). In this paper, we adapt the NMT model from the old to new domain by learning domain-invariant representations of both encoder and decoder via domain discrimination loss. Unsupervised Domain Adaptation of NMT. There are two main approaches in UDA for NMT, including model-centric and data-centric methods (Chu and Wang, 2018). In the model-centric approach, the model architecture is modified and jointly trained on MT tasks, and other auxiliary"
2021.emnlp-main.268,W18-6323,0,0.029138,"Missing"
2021.emnlp-main.268,tiedemann-2012-parallel,0,0.0176553,"ised by ψ d for the decoder, t is the target sentences in the old domain’s Dold bitext, and λ3 controls the effect of the old domain. 5 Experiments We evaluate our proposed approach for GUDA on the three language pairs covering five domains, and a real-world translation task, namely, TICO-19. 5.1 Setup Datasets. Table 1 shows data statistics. The general domain datasets come from WMT2014 for English-French, WMT2020 for English-German, news parallel corpus from OPUS for ArabicEnglish3 . We appraise our proposed methods on following specific domains: TED talk, Law, Medical, IT, Koran from OPUS (Tiedemann, 2012) following the recipe in Koehn and Knowles (2017). We sample 10M English sentences from Newcrawl 2007-2019 as the generic monolingual corpus. Data pre-processing is described in Appendix A. Source Monotext Loss. To take into account the clean text in source language of the new domain, we apply the discriminative domain mixing method (Britz et al., 2017) to force the encoder towards capturing new domain’s characteristics. For this purpose, we build a classifier cψ e (zz e ), a feedforward network parametrised by ψ e , whose output is x)) is the new domain’s probability. z e = h (encθ (x 3 the r"
2021.emnlp-main.268,2020.emnlp-main.497,1,0.763261,"he domain clusters in Unsupervised Domain Adaptation. Previous English sentences but also learn domain clustered works in UDA has been focused on aligning dorepresentations of Arabic sentences in which the main distribution by minimising the discrepancy 3342 between representations of source and target domains (Shen et al., 2018; Wang et al., 2018); learning domain-invariant representation via adversarial learning (Ganin and Lempitsky, 2015; Shah et al., 2018; Moghimifar et al., 2020); bridging the domain gap by adaptive pretraining of contextualised word embeddings (Han and Eisenstein, 2019; Vu et al., 2020). In this paper, we adapt the NMT model from the old to new domain by learning domain-invariant representations of both encoder and decoder via domain discrimination loss. Unsupervised Domain Adaptation of NMT. There are two main approaches in UDA for NMT, including model-centric and data-centric methods (Chu and Wang, 2018). In the model-centric approach, the model architecture is modified and jointly trained on MT tasks, and other auxiliary tasks such as language modelling (Gulcehre et al., 2015). On the other hand, the data-centric methods focus on constructing in-domain parallel corpus by"
2021.emnlp-main.268,P17-2089,0,0.0279657,"Missing"
2021.emnlp-main.268,N18-1001,0,0.0138691,"e not well-clustered according to their domains where their domain clus- than other selection methods in all domains. ters exhibit a high overlap rate. As can be seen, our 7 Related Works contrastive-based representation alignment method is not only able to preserve the domain clusters in Unsupervised Domain Adaptation. Previous English sentences but also learn domain clustered works in UDA has been focused on aligning dorepresentations of Arabic sentences in which the main distribution by minimising the discrepancy 3342 between representations of source and target domains (Shen et al., 2018; Wang et al., 2018); learning domain-invariant representation via adversarial learning (Ganin and Lempitsky, 2015; Shah et al., 2018; Moghimifar et al., 2020); bridging the domain gap by adaptive pretraining of contextualised word embeddings (Han and Eisenstein, 2019; Vu et al., 2020). In this paper, we adapt the NMT model from the old to new domain by learning domain-invariant representations of both encoder and decoder via domain discrimination loss. Unsupervised Domain Adaptation of NMT. There are two main approaches in UDA for NMT, including model-centric and data-centric methods (Chu and Wang, 2018). In the"
2021.emnlp-main.310,U19-1011,0,0.0221043,"ing the quel to fill memories with the examples most likely second task. The training on the initial task ap- mitigating catastrophic forgetting. We also present pears to be crucial on mitigating catastrophic for- the two-stage training algorithm FSCL to facilitate getting. The BERT-based parser with/without fine- knowledge transfer between tasks. tuning obtains no improvement over the one using Sampling Method. DLFS improves Episodic GLOVE. The forgetting with BERT is even more Memory Replay (EMR) (Wang et al., 2019; serious compared with using GLOVE. The same phenomenon is also observed in (Arora et al., 2019) Chaudhry et al., 2019) by proposing a designated sampling method for continual semantic parsthat the models with pre-trained language models obtain inferior performance than LSTM or CNN, ing. EMR utilizes a memory module Mk = (k) (k) (k) (k) when fine-tuning incrementally on each task. They {(x1 , y1 ), ..., (xM , yM )} to store a few exam3819 ples sampled from the training set of task T (k) , (k) (k) (k) where (xm , ym ) ∈ Dtrain and M is the size of the memory. The training loss of EMR takes the following form: k−1 X LEMR = LD(k) + LMi (3) train where LD(k) train i=1 and LMi denotes the los"
2021.emnlp-main.310,W13-2322,0,0.0166745,"assification problem. and Das, 2018; Zhu et al., 2019; Li et al., 2020) Therefore, we propose T OTAL R ECALL (TR), a cover an ample of work in semantic parsing. Most continual learning method that is especially de- current work employ a sequence-to-sequence arsigned to address the semantic parsing specific chitecture (Sutskever et al., 2014) to map an utproblems from two perspectives. First, we cus- terance into a structured meaning representations, tomize the sampling algorithm for memory re- such as LFs, SQL, and abstract meaning repreplay, which stores a small sample of examples sentation (Banarescu et al., 2013). The output sefrom each previous task when continually learning quences are either linearized LFs (Dong and Lanew tasks. The corresponding sampling algorithm, pata, 2016, 2018; Cao et al., 2019) or sequences called Diversified Logical Form Selection (DLFS), of parse actions (Chen et al., 2018; Cheng et al., diversifies LF templates and maximizes the entropy 2019; Lin et al., 2019; Zhang et al., 2019; Yin and of the parse action distribution in a memory. Sec- Neubig, 2018; Chen et al., 2018; Guo et al., 2019; ond, motivated by findings from cognitive neuro- Wang et al., 2020a; Li et al., 2021)"
2021.emnlp-main.310,P13-2131,0,0.0313957,"Dtrain into M clusters, followed by selecting to another. Inspired by findings from cognitive representative instances from each cluster to max- neuroscience, the learning should be divided into imize entropy of actions in a memory. To char- slow learning of stationary aspects between tasks acterize differences in structures, we first com- and fast learning of task-specific aspects (Goyal pute similarities between LFs by sim(yi , yj ) = and Bengio, 2020). This is an inductive bias that (Smatch(yi , yj ) + Smatch(yj , yi ))/2, where can be leveraged to obtain cross-task generalization Smatch (Cai and Knight, 2013) is a asymmetrical in the space of all functions. similarity score between two LFs yielded by calWe implement this inductive bias by introducculating the overlapping percentage of their triples. ing a two-stages training algorithm. In the base Then we run a flat clustering algorithm using the model, action embeddings ca (Eq. (7)) are taskdistance function 1 − sim(yi , yj ) and the number specific, while the remaining parts of the model, of clusters is the same as the size of a memory. which builds representations of utterances and acWe choose K-medoids (Park and Jun, 2009) in this tion histori"
2021.emnlp-main.310,P19-1007,0,0.026185,"specially de- current work employ a sequence-to-sequence arsigned to address the semantic parsing specific chitecture (Sutskever et al., 2014) to map an utproblems from two perspectives. First, we cus- terance into a structured meaning representations, tomize the sampling algorithm for memory re- such as LFs, SQL, and abstract meaning repreplay, which stores a small sample of examples sentation (Banarescu et al., 2013). The output sefrom each previous task when continually learning quences are either linearized LFs (Dong and Lanew tasks. The corresponding sampling algorithm, pata, 2016, 2018; Cao et al., 2019) or sequences called Diversified Logical Form Selection (DLFS), of parse actions (Chen et al., 2018; Cheng et al., diversifies LF templates and maximizes the entropy 2019; Lin et al., 2019; Zhang et al., 2019; Yin and of the parse action distribution in a memory. Sec- Neubig, 2018; Chen et al., 2018; Guo et al., 2019; ond, motivated by findings from cognitive neuro- Wang et al., 2020a; Li et al., 2021). There are science (Goyal and Bengio, 2020), we facilitate also work (Guo et al., 2019; Wang et al., 2020a; knowledge transfer between tasks by proposing Li et al., 2021) exploring semantic pars"
2021.emnlp-main.310,2020.acl-main.677,0,0.0230266,"Missing"
2021.emnlp-main.310,N19-1086,0,0.114755,"17; Zenke et al., 2017; Ritter et al., 2018; Li and Hoiem, 2017; Zhao et al., 2020; Schwarz et al., 2018) which either applies knowledge distillation (Hinton et al., 2015) to penalize the loss updates or regularizes parameters which are crucial to the old tasks; ii) dynamic architecture methods (Mallya and Lazebnik, 2018; Serra et al., 2018; Maltoni and Lomonaco, 2019; Houlsby et al., 2019; Wang et al., 2020b; Pfeiffer et al., 2021; Rusu et al., 2016) which dynamically alter the structures of models to reduce the catastrophic forgetting; iii) memory-based methods (Lopez-Paz and Ranzato, 2017; Wang et al., 2019; Han et al., 2020; Aljundi et al., 2019; Chrysakis and Moens, 2020; Kim et al., 2020) which stores historical instances and continually learn them along with instances in new tasks. There are also hybrid methods (Mi et al., 2020; Liu et al., 2020; Rebuffi et al., 2017) which integrate more than one type of such methods. In natural language processing (NLP), continual learning is applied to tasks such as relation extraction (Wang et al., 2019; Han et al., 2020), natural language generation (Mi et al., 2020), language modelling (Sun et al., 2019) and the pretrained language models adapting to m"
2021.emnlp-main.310,D19-1392,0,0.0386961,"Missing"
2021.emnlp-main.537,N16-1111,0,0.114901,"eement. Table 1: Translation and interpretation differ in style while conveying the same source information. Introduction Simultaneous interpretation (SI) is a task of translating natural language in real time. SiMT systems are expected to generate interpreted text as if the text was produced by human interpreters while maintaining acceptable delay (Ma et al., 2019; Arthur et al., 2021). However, most current SiMT systems are trained and evaluated on offline translations differing from real-life SI scenarios where translations are flexibly paraphrased, without compromising the source message (He et al., 2016; Paulik and Waibel, 2009). For instance, in Table 1 the interpretation sentence drops &quot;at this point&quot; and condenses &quot;seriousness of this line of argument&quot; to &quot;agreement&quot;; it delivers the source message as reliably as the offline translation. Prior work attempted to build interpretation corpora in a small scale (Tohyama and Inagaki, 2004; Shimizu et al., 2014; Bernardini et al., 2016), or constructed speech interpretation training corpora for MT tasks (Paulik and Waibel, 2010). But, very little attempt has been made on empirically quantifying the evaluation gap. An exception is Shimizu et al."
2021.emnlp-main.537,shimizu-etal-2014-collection,0,0.015658,"t al., 2019; Arthur et al., 2021). However, most current SiMT systems are trained and evaluated on offline translations differing from real-life SI scenarios where translations are flexibly paraphrased, without compromising the source message (He et al., 2016; Paulik and Waibel, 2009). For instance, in Table 1 the interpretation sentence drops &quot;at this point&quot; and condenses &quot;seriousness of this line of argument&quot; to &quot;agreement&quot;; it delivers the source message as reliably as the offline translation. Prior work attempted to build interpretation corpora in a small scale (Tohyama and Inagaki, 2004; Shimizu et al., 2014; Bernardini et al., 2016), or constructed speech interpretation training corpora for MT tasks (Paulik and Waibel, 2010). But, very little attempt has been made on empirically quantifying the evaluation gap. An exception is Shimizu et al. (2013) which incorporated interpretation data in the training stage of a statistical MT system, but the lack of training data and the scale of evaluation set resulted in a marginal BLEU score difference.2 We compile a genuine interpretation test set of 1k utterances from the European Parliament (EP) Plenary focusing on German→English. We examine the real perf"
2021.emnlp-main.537,2005.mtsummit-papers.11,0,0.0446451,"ults by underlining the performance gap between evaluation on translated and interpreted texts (§4.3), and showing the effectiveness of our T2I style transfer both quantitatively and qualitatively (§4.4). We followed the instructions in Arthur et al. (2021) to preprocess data, and their hyperparameters for training all wait-k models. For style transfer models, we used the standard setup for both PBMT and HPBMT.8 4.1 Datasets We conducted evaluation investigation on four languages pairs, including German (DE), French (FR), Polish (PL), Italian (IT) → English (EN) , and used Europarl v7 corpus (Koehn, 2005) for training a SiMT model for each pair (see Table 2 for data statistics). For DE-EN, our annotated test set has 1,051 triples, for InterpretationASR and Interpretation. For the rest, we used EPTIC (Bernardini et al., 2016), a small-scale parallel corpus with data collected from the EP Plenary; it has source languages of FR, PL and IT, with 675, 463 and 480 instances, respectively. In the experiments of bridging the evaluation gap, Raw has 120,114 and 1,000 utterances for training and dev sets, while Clean has 4,240 triples, all used for training style transfer models. To train PBMT, we augme"
2021.emnlp-main.537,P07-2045,0,0.00921662,"53 13.87 14.26 10.33 13.21 13.56 13.60 Table 3: Evaluation on human annotated Translation Test, Interpretation TestASR and Interpretation Test. ∗ : performance gap. Underlined: lowest delay across systems. Bold: Best BLEU on Interpretation Test. percentage of read source tokens for every generated target token, while AL measures the number of lagged source tokens until all source tokens are read. Style Transfer Models In supervised settings, we used PBMT and HPBMT; in unsupervised settings we only used HPBMT, as PBMT requires additional paired data to find the best weights. We deployed Moses (Koehn et al., 2007) for above systems. We also experimented with a Seq2Seq (unsupervised) model (Ott et al., 2019) to compare. 4.3 Performance Gap We train separate wait-k models for the four language pairs and report the evaluation results on their corresponding Translation Test and Interpretation Test10 in Table 2. The observed significant gap of up-to 13.83 BLEU score (24.47 vs 10.64 for IT) highlights the daunting task SiMT models face in real-life SI. Interestingly, the gap for DE-EN is the lowest, and this is likely to be due to the fact that both are Germanic languages. We explored the feasibility of narr"
2021.emnlp-main.537,N03-1017,0,0.0686104,"s, where only Raw is used. Supervised training Given that our Clean set consists of roughly 4.2k triples, we opt for statistical MT systems which inherently require far less data for sequence-to-sequence mapping tasks compared to their neural counterparts. Furthermore, conducting style transfer in the same language involves word replacement and ordering, which conforms with the behaviors of SMT systems that chunk an input sequence into segments, translate, and reorder the translated chunks (Lopez, 2008). More specifically, we employ two classic statistical MT methods: phrase-based SMT (PBMT) (Koehn et al., 2003) and Hierarchical phrase-based MT (HPBMT) (Chiang, 2005).6 A similar framework was tried by Xu et al. (2012) for text simplification. We will describe the T2I pipeline process for unsupervised settings, as both settings have a similar process with different data configurations. The main difference is that we use Clean instead of Raw, which will be detailed in §4.1. Unsupervised training Figure 1 shows the three stages of our T2I approach in unsupervised settings: the first stage is to convert interpretations in Raw to translation-style data by applying roundtrip translation on interpretations,"
2021.emnlp-main.537,C12-1177,0,0.0342376,"t for statistical MT systems which inherently require far less data for sequence-to-sequence mapping tasks compared to their neural counterparts. Furthermore, conducting style transfer in the same language involves word replacement and ordering, which conforms with the behaviors of SMT systems that chunk an input sequence into segments, translate, and reorder the translated chunks (Lopez, 2008). More specifically, we employ two classic statistical MT methods: phrase-based SMT (PBMT) (Koehn et al., 2003) and Hierarchical phrase-based MT (HPBMT) (Chiang, 2005).6 A similar framework was tried by Xu et al. (2012) for text simplification. We will describe the T2I pipeline process for unsupervised settings, as both settings have a similar process with different data configurations. The main difference is that we use Clean instead of Raw, which will be detailed in §4.1. Unsupervised training Figure 1 shows the three stages of our T2I approach in unsupervised settings: the first stage is to convert interpretations in Raw to translation-style data by applying roundtrip translation on interpretations, with pretrained NMT models (Ng et al., 2019). It is expected that the outputs after this round-tripping, de"
2021.emnlp-main.537,W19-5333,0,0.0119705,"d MT (HPBMT) (Chiang, 2005).6 A similar framework was tried by Xu et al. (2012) for text simplification. We will describe the T2I pipeline process for unsupervised settings, as both settings have a similar process with different data configurations. The main difference is that we use Clean instead of Raw, which will be detailed in §4.1. Unsupervised training Figure 1 shows the three stages of our T2I approach in unsupervised settings: the first stage is to convert interpretations in Raw to translation-style data by applying roundtrip translation on interpretations, with pretrained NMT models (Ng et al., 2019). It is expected that the outputs after this round-tripping, denoted as Translation-FB, sit close to the translation domain, thus achieving the effects of interpretationto-translation. The second stage is to train a style transfer model to learn the mapping between the data points in Translation-FB and their corresponding interpretations in Raw. Lastly, we apply the trained style transfer model on offline Europarl translations and produce interpretation-like sequences which we call Pseudo-I.7 6 PBMT creates a phrase table, a reordering model and a language model, followed by tuning their weigh"
2021.emnlp-main.537,N19-4009,0,0.0145696,"nterpretation TestASR and Interpretation Test. ∗ : performance gap. Underlined: lowest delay across systems. Bold: Best BLEU on Interpretation Test. percentage of read source tokens for every generated target token, while AL measures the number of lagged source tokens until all source tokens are read. Style Transfer Models In supervised settings, we used PBMT and HPBMT; in unsupervised settings we only used HPBMT, as PBMT requires additional paired data to find the best weights. We deployed Moses (Koehn et al., 2007) for above systems. We also experimented with a Seq2Seq (unsupervised) model (Ott et al., 2019) to compare. 4.3 Performance Gap We train separate wait-k models for the four language pairs and report the evaluation results on their corresponding Translation Test and Interpretation Test10 in Table 2. The observed significant gap of up-to 13.83 BLEU score (24.47 vs 10.64 for IT) highlights the daunting task SiMT models face in real-life SI. Interestingly, the gap for DE-EN is the lowest, and this is likely to be due to the fact that both are Germanic languages. We explored the feasibility of narrowing the performance gap using our T2I method on DE-EN. Being a head-final language, German is"
2021.emnlp-main.537,2020.acl-demos.14,0,0.0136715,"ranslations had a different number of sentences. Next a manual process was applied, including removals of dialogues with non-essential contents and truncation of interpretations whose first and last sentences did not match the corresponding offline translations (mostly due to imperfect audio segmentation). 987 dialogues5 were thus retained, each of which having 14.5 sentences on average. We aligned translations with transcriptions (interpretations). For each dialogue, as the transcriptions may not be well segmented in the ASR process, we identified sentences in the transcriptions with stanza (Qi et al., 2020), before segmenting them using dynamic programming. Manual inspection revealed that there were a portion of mismatched pairs, which was due to occasional interpreting failure resulting from interpreters’ accumulated cognitive load (Mizuno, 2017; Sudoh et al., 2020). We further removed pairs the lengths of whose source and target were far off, and call it Clean, containing triples <source, translation, interpretation>. Figure 1: T2I style transfer in unsupervised settings. Translation and Interpretation Test Sets. To ensure the quality of interpretation data for evaluation, we hired a bilingual"
2021.emnlp-main.537,2013.iwslt-papers.3,0,0.0491829,"He et al., 2016; Paulik and Waibel, 2009). For instance, in Table 1 the interpretation sentence drops &quot;at this point&quot; and condenses &quot;seriousness of this line of argument&quot; to &quot;agreement&quot;; it delivers the source message as reliably as the offline translation. Prior work attempted to build interpretation corpora in a small scale (Tohyama and Inagaki, 2004; Shimizu et al., 2014; Bernardini et al., 2016), or constructed speech interpretation training corpora for MT tasks (Paulik and Waibel, 2010). But, very little attempt has been made on empirically quantifying the evaluation gap. An exception is Shimizu et al. (2013) which incorporated interpretation data in the training stage of a statistical MT system, but the lack of training data and the scale of evaluation set resulted in a marginal BLEU score difference.2 We compile a genuine interpretation test set of 1k utterances from the European Parliament (EP) Plenary focusing on German→English. We examine the real performance gap of wait-k (Ma et al., 2019), a state-of-the-art SiMT system, on our test set along with 3 smaller scale (Bernardini et al., 2016) translation and interpretation language-pairs and observe a drop of up-to 13.83 BLEU score. In the abse"
2021.emnlp-main.580,2021.findings-acl.420,1,0.812354,"Missing"
2021.emnlp-main.580,2020.acl-main.747,0,0.248181,"both heterogeneous (different corpora reveal difso that the learning process is balanced and ferent linguistic properties) and imbalance (the aclow-resource cases can benefit from the highcessibility of training data varies across corpora). resource ones. However, automatic balancing The standard practice to address this issue is to admethods usually depend on the intra- and interjust the training data distribution heuristically by dataset characteristics, which is usually agnosup-sampling the training data from LRLs/LRDs tic or requires human priors. In this work, (Arivazhagan et al., 2019; Conneau et al., 2020). we propose an approach, M ULTI UAT, that dynamically adjusts the training data usage based Arivazhagan et al. (2019) rescale the training on the model’s uncertainty on a small set of data distribution with a heuristic temperature term trusted clean data for multi-corpus machine and demonstrate that the ideal temperature can subtranslation. We experiment with two classes of stantially improve the overall performance. Howuncertainty measures on multilingual (16 lanever, the optimal value for such heuristics is both guages with 4 settings) and multi-domain sethard to find and varies from one ex"
2021.emnlp-main.580,P07-1033,0,0.341337,"Missing"
2021.emnlp-main.580,2020.findings-emnlp.377,0,0.195595,"in different languages, raising the tween a small set of trusted clean data and training problem of learning a NLP system from the hetero- data. They instantiate this framework on multilingeneous corpora, such as multilingual models (Wu gual NMT, known as M ULTI DDS, to dynamically and Dredze, 2019; Arivazhagan et al., 2019; Aha- weigh the importance of language pairs. Both the roni et al., 2019; Freitag and Firat, 2020; Arthur hypothesis and the proposed approach rely on the et al., 2021) and multi-domain models (Daumé III, assumption that knowledge learned from one cor2007; Li et al., 2019; Deng et al., 2020; Jiang et al., pus can always be beneficial to the other corpora. 2020). A strong demand is to deploy a unified However, their assumption does not always hold. model for all the languages and domains, because If the knowledge learned from one corpus is not ∗ Work done during the internship at Huawei Noah’s Ark able to be transferred easily or is useless to the other Lab. corpora, this approach fails. Unlike cosine similar1 Code available at https://github.com/ ity, model uncertainty is free from the aforemenhuawei-noah/noah-research/tree/master/ noahnmt/multiuat tioned assumption on cross-cor"
2021.emnlp-main.580,P15-1166,0,0.0208238,"ty over KDE of M ULTI DDS-S and M ULTI UAT with different temperature priors. TI UAT with different prior sampling distributions in Figure 4. The learned sampling distribution by M ULTI UAT always converges to uniform distribution, regardless of the change of prior sampling distribution. However, the change of priors significantly affects the learned sampling distribution of M ULTI DDS-S. 7 Related Work Multi-corpus NLP Multilingual training has been particularly prominent in recent advances driven by the demand of training a unified model 6.4 Effects of Sampling Priors for all the languages (Dong et al., 2015; Plank et al., Both M ULTI DDS-S and M ULTI UAT initialize the 2016; Johnson et al., 2017; Arivazhagan et al., sampling probability distribution to proportional 2019). Freitag and Firat (2020) extend current distribution (line 1 in Algorithm 1). We investi- English-centric training to a many-to-many setup gate how the prior sampling distribution affects without sacrificing the performance on Englishthe performance and present the results in Table 3. centric language pairs. Wang et al. (2021) imWe can observe that the prior sampling distribu- prove the multilingual training by adjusting gratio"
2021.emnlp-main.580,P18-1069,0,0.0189446,"on et al., 2007; von Stackelberg et al., 2011). 3 Algorithm 1: Training with M ULTI UAT Methodology In this work, we leverage the idea of DDS under the multi-corpus scenarios. We utilize a differentiable domain/language scorer ψ to weigh the training corpora. To learn ψ , we exploit the model uncertainty to measure the model’s ability over the target corpus. Below, we elaborate on the details of our method. 1 2 3 4 5 6 3.1 Model Uncertainty 7 Model uncertainty can be a measure that indicates whether the model parameters θ are able to describe the data distribution well (Kendall and Gal, 2017; Dong et al., 2018; Xiao and Wang, 2019). Bayesian neural networks can be used for quantifying the model uncertainty (Buntine and Weigend, 1991), which models the θ as a probabilistic distribution with constant input and output. From the Bayesian point of view, θ is interpreted as a random variable with the prior p(θθ ). Given a dataset D, the posterior p(θθ |D) can be obtained via Bayes’ rule. However, the exact Bayesian inference is intractable for neural networks, so that it is common to place the approximation q(θθ ) to the true posterior p(θθ |D). Several variational inference methods have been proposed (G"
2021.emnlp-main.580,D17-1063,0,0.0202276,"dient descent between two updates 3 We parameterize the scorer ψ following Wang et al. (2020b). 7293 of ψ , like common gradient-based optimization, and hence the objective is formulated as follows: • Variance of Translation Probability (VARTP): The variance of the distribution of maximal position-wise translation probability, ψ )) ψ = argmin L(Ddev ; θ (ψ ψ ψ) = θ (ψ n argmin En∼pψ (n) [L(Dtrn ; θ )] . θ A considerable problem here is Equation 6 is not directly differentiable w.r.t. the scorer ψ . To tackle this problem, reinforcement learning (RL) with suitable reward functions is required (Fang et al., 2017; Wang et al., 2020a): ψ ←ψ− N X R(n) · ∇ψ log pψ (n) . (9) n=1 Details for the reward functions R(n) are depicted at Section 3.3 and the update of ψ follows the REINFORCE algorithm (Williams, 1992). 3.3 xn , y n,&lt;t ; θ k )] . RVARTP (n; θ k ) = Var[p(ˆ yn,t |x (8) Uncertainty Measures We explore the utility of two groups of model uncertainty measures: probability-based and entropybased measures at the sentence level (Wang et al., 2019; Fomicheva et al., 2020; Malinin and Gales, 2021). Probability-Based Measures We explore four probability-based uncertainty measures following the definition of"
2021.emnlp-main.580,2020.wmt-1.66,0,0.175795,"S) that automati1 Introduction cally adjusts the importance of data points, whose Text corpora are commonly collected from several reward is the cosine similarity of the gradients bedifferent sources in different languages, raising the tween a small set of trusted clean data and training problem of learning a NLP system from the hetero- data. They instantiate this framework on multilingeneous corpora, such as multilingual models (Wu gual NMT, known as M ULTI DDS, to dynamically and Dredze, 2019; Arivazhagan et al., 2019; Aha- weigh the importance of language pairs. Both the roni et al., 2019; Freitag and Firat, 2020; Arthur hypothesis and the proposed approach rely on the et al., 2021) and multi-domain models (Daumé III, assumption that knowledge learned from one cor2007; Li et al., 2019; Deng et al., 2020; Jiang et al., pus can always be beneficial to the other corpora. 2020). A strong demand is to deploy a unified However, their assumption does not always hold. model for all the languages and domains, because If the knowledge learned from one corpus is not ∗ Work done during the internship at Huawei Noah’s Ark able to be transferred easily or is useless to the other Lab. corpora, this approach fails. U"
2021.emnlp-main.580,2020.aacl-main.73,0,0.0293311,"multilingual training by adjusting gration can affect the overall performance. For both dient directions based on gradient similarity. ExM ULTI DDS-S and M ULTI UAT, the overall results isting works on multi-domain training commonly on both in-domain and out-of-domain evaluation attempt to leverage architectural domain-specific are negatively correlated with the prior τ . components or auxiliary loss (Sajjad et al., 2017; We also visualize the change of sampling prob- Tars and Fishel, 2018; Zeng et al., 2018; Li et al., ability of KDE given by M ULTI DDS-S and M UL - 2018; Deng et al., 2020; Jiang et al., 2020). These 7298 approaches commonly do not explore much on the training proportion across domains and are limited to in-domain prediction and less generalizable to unseen domains. Zaremoodi and Haffari (2019) dynamically balance the importance of tasks in multitask NMT to improve the low-resource NMT performance. Vu et al. (2021) leverage a pre-trained language model to select useful monolingual data from either source language or target language to perform unsupervised domain adaptation for NMT models. Our work is directly related to Wang et al. (2020a) and Wang et al. (2020b) that leverage cosi"
2021.emnlp-main.580,W04-3250,0,0.6712,"nvolved in this work, including the baseline approaches and M ULTI UAT, are model-agnostic. To validate this idea, we experiment two variants of transformer (Vaswani et al., 2017). For multilingual NMT, the model architecture is a transformer with 4 attention heads and 6 layers.10 And for multi-domain NMT models, we use the standard transformer-base with 8 attention heads and 6 layers.11 All the models in this work are implemented by fairseq (Ott et al., 2019). 4.5 Evaluation We report detokenized BLEU (Papineni et al., 2002) using SacreBLEU (Post, 2018) with statistical significance given by Koehn (2004).12 µBLEU is the macro average of BLEU scores within the same setting, with the assumption that all the language pairs/domains are equally important. 8 https://opus.nlpl.eu/QED.php https://opus.nlpl.eu/TED2013.php 10 Signature: multilingual_transformer_iwslt_de_en 11 Signature: transformer 12 Signature: BLEU+case.mixed+numrefs.1 +smooth.exp+tok.13a+version.1.4.14 7295 9 Multilingual Related Multi-Domain Diverse ID OOD 16.79 17.94 16.86 18.24 35.69 36.92 36.85 36.42 25.15 23.46 22.22 22.74 19.57† 19.59† 19.62† 19.67† 19.68† 19.76† 37.50† 37.29 35.49 37.25 37.27 37.44† 23.77 23.89 23.75 23.36 23"
2021.emnlp-main.580,P07-2045,0,0.00742773,"les. 4 Refer to Wang et al. (2020b) for dataset statistics. https://opus.nlpl.eu/Tanzil.php 6 https://opus.nlpl.eu/EMEA.php 7 https://opus.nlpl.eu/KDE4.php 5 ID WMT Tanzil EMEA KDE OOD QED TED Train Valid Test 3, 950K 449K 277K 135K 11K 3K 3K 3K 3K 3K 3K 3K - - 3K 3K Table 1: Dataset statistics of multi-domain corpora. Out-Of-Domain (OOD) (i) QED,8 a collection of subtitles for educational videos and lectures (Abdelali et al., 2014); (ii) TED,9 a parallel corpus of TED talk subtitles. These two domains are only used for out-of-domain evaluation. All these corpora are first tokenized by Moses (Koehn et al., 2007) and processed into sub-word units by BPE (Sennrich et al., 2016) with 32K merge operations. Sentence pairs that are duplicated and violates source-target ratio of 1.5 are removed. The validation sets and test sets are randomly sampled, except for WMT. The dataset statistics are listed in Table 1. 4.4 Model Architecture We believe all the approaches involved in this work, including the baseline approaches and M ULTI UAT, are model-agnostic. To validate this idea, we experiment two variants of transformer (Vaswani et al., 2017). For multilingual NMT, the model architecture is a transformer with"
2021.emnlp-main.580,P16-2067,0,0.0541322,"Missing"
2021.emnlp-main.580,P19-1583,0,0.0181778,"ta usage based Arivazhagan et al. (2019) rescale the training on the model’s uncertainty on a small set of data distribution with a heuristic temperature term trusted clean data for multi-corpus machine and demonstrate that the ideal temperature can subtranslation. We experiment with two classes of stantially improve the overall performance. Howuncertainty measures on multilingual (16 lanever, the optimal value for such heuristics is both guages with 4 settings) and multi-domain sethard to find and varies from one experimental settings (4 for in-domain and 2 for out-of-domain ting to another (Wang and Neubig, 2019; Wang on English-German translation) and demonstrate our approach M ULTI UAT substantially et al., 2020a,b). Wang et al. (2020a) and Wang et al. outperforms its baselines, including both static (2020b) hypothesize that the training data instances and dynamic strategies. We analyze the crossthat are similar to the validation set can be more domain transfer and show the deficiency of beneficial to the evaluation performance and prostatic and similarity based methods.1 pose a general reinforcement-learning framework Differentiable Data Selection (DDS) that automati1 Introduction cally adjusts th"
2021.emnlp-main.580,W18-6319,0,0.0116562,"Model Architecture We believe all the approaches involved in this work, including the baseline approaches and M ULTI UAT, are model-agnostic. To validate this idea, we experiment two variants of transformer (Vaswani et al., 2017). For multilingual NMT, the model architecture is a transformer with 4 attention heads and 6 layers.10 And for multi-domain NMT models, we use the standard transformer-base with 8 attention heads and 6 layers.11 All the models in this work are implemented by fairseq (Ott et al., 2019). 4.5 Evaluation We report detokenized BLEU (Papineni et al., 2002) using SacreBLEU (Post, 2018) with statistical significance given by Koehn (2004).12 µBLEU is the macro average of BLEU scores within the same setting, with the assumption that all the language pairs/domains are equally important. 8 https://opus.nlpl.eu/QED.php https://opus.nlpl.eu/TED2013.php 10 Signature: multilingual_transformer_iwslt_de_en 11 Signature: transformer 12 Signature: BLEU+case.mixed+numrefs.1 +smooth.exp+tok.13a+version.1.4.14 7295 9 Multilingual Related Multi-Domain Diverse ID OOD 16.79 17.94 16.86 18.24 35.69 36.92 36.85 36.42 25.15 23.46 22.22 22.74 19.57† 19.59† 19.62† 19.67† 19.68† 19.76† 37.50† 37.29"
2021.emnlp-main.580,E17-2045,0,0.0289594,"sacrificing the performance on Englishthe performance and present the results in Table 3. centric language pairs. Wang et al. (2021) imWe can observe that the prior sampling distribu- prove the multilingual training by adjusting gration can affect the overall performance. For both dient directions based on gradient similarity. ExM ULTI DDS-S and M ULTI UAT, the overall results isting works on multi-domain training commonly on both in-domain and out-of-domain evaluation attempt to leverage architectural domain-specific are negatively correlated with the prior τ . components or auxiliary loss (Sajjad et al., 2017; We also visualize the change of sampling prob- Tars and Fishel, 2018; Zeng et al., 2018; Li et al., ability of KDE given by M ULTI DDS-S and M UL - 2018; Deng et al., 2020; Jiang et al., 2020). These 7298 approaches commonly do not explore much on the training proportion across domains and are limited to in-domain prediction and less generalizable to unseen domains. Zaremoodi and Haffari (2019) dynamically balance the importance of tasks in multitask NMT to improve the low-resource NMT performance. Vu et al. (2021) leverage a pre-trained language model to select useful monolingual data from"
2021.emnlp-main.580,P16-1162,0,0.0160701,"tps://opus.nlpl.eu/Tanzil.php 6 https://opus.nlpl.eu/EMEA.php 7 https://opus.nlpl.eu/KDE4.php 5 ID WMT Tanzil EMEA KDE OOD QED TED Train Valid Test 3, 950K 449K 277K 135K 11K 3K 3K 3K 3K 3K 3K 3K - - 3K 3K Table 1: Dataset statistics of multi-domain corpora. Out-Of-Domain (OOD) (i) QED,8 a collection of subtitles for educational videos and lectures (Abdelali et al., 2014); (ii) TED,9 a parallel corpus of TED talk subtitles. These two domains are only used for out-of-domain evaluation. All these corpora are first tokenized by Moses (Koehn et al., 2007) and processed into sub-word units by BPE (Sennrich et al., 2016) with 32K merge operations. Sentence pairs that are duplicated and violates source-target ratio of 1.5 are removed. The validation sets and test sets are randomly sampled, except for WMT. The dataset statistics are listed in Table 1. 4.4 Model Architecture We believe all the approaches involved in this work, including the baseline approaches and M ULTI UAT, are model-agnostic. To validate this idea, we experiment two variants of transformer (Vaswani et al., 2017). For multilingual NMT, the model architecture is a transformer with 4 attention heads and 6 layers.10 And for multi-domain NMT model"
2021.emnlp-main.580,2020.acl-main.754,0,0.136071,"heuristic temperature term trusted clean data for multi-corpus machine and demonstrate that the ideal temperature can subtranslation. We experiment with two classes of stantially improve the overall performance. Howuncertainty measures on multilingual (16 lanever, the optimal value for such heuristics is both guages with 4 settings) and multi-domain sethard to find and varies from one experimental settings (4 for in-domain and 2 for out-of-domain ting to another (Wang and Neubig, 2019; Wang on English-German translation) and demonstrate our approach M ULTI UAT substantially et al., 2020a,b). Wang et al. (2020a) and Wang et al. outperforms its baselines, including both static (2020b) hypothesize that the training data instances and dynamic strategies. We analyze the crossthat are similar to the validation set can be more domain transfer and show the deficiency of beneficial to the evaluation performance and prostatic and similarity based methods.1 pose a general reinforcement-learning framework Differentiable Data Selection (DDS) that automati1 Introduction cally adjusts the importance of data points, whose Text corpora are commonly collected from several reward is the cosine similarity of the grad"
2021.findings-acl.214,P16-1201,0,0.0240347,"s, i.e., fewshot learning (Finn et al., 2017; Snell et al., 2017; Zhang et al., 2018a). Yet few-shot event detection (FSED) has been less studied until recently (Lai et al., 2020a; Deng et al., 2020). Although these methods achieve encouraging progress on typical N -way M -shot setting (Figure 1), the performance remains unsatisfactory as the diversity of examples in the support set is usually limited. Intuitively, introducing high-quality semantic knowledge, such as FrameNet (Baker et al., 1998), is a potential solution to the insufficient diversity issue (Qu et al., 2020; Tong et al., 2020; Liu et al., 2016, 2020). However, as shown in Figure 2, such knowledge-enhanced methods also suffer from two major issues: (1) the incomplete coverage by the knowledge base and (2) the uncertainty caused by the inexact alignment between predefined knowledge and diverse applications. To tackle the above issues, in this paper, we propose an Adaptive Knowledge-Enhanced Bayesian Meta-Learning (AKE-BML) framework. More specifically, (1) we align the event types between the support set and FrameNet via heuristic rules.1 (2) We propose encoders for encoding the sam1 For event types that cannot be accurately aligned"
2021.findings-acl.214,N19-1080,0,0.016364,"es. Introduction Event detection is an important task in information extraction, aiming at detecting event triggers from text and then classifying them into event types (Chen et al., 2015). For example, in “The police arrested Harry on charges of manslaughter”, the trigger word is arrested, indicating an “Arrest” event. Event detection has been widely applied in Twitter analysis (Zhou et al., 2017), legal case extraction (de Araujo et al., 2017), and financial event extraction (Zheng et al., 2019), to name a few. Typical approaches to event detection (Chen et al., 2015; McClosky et al., 2011; Liu et al., 2019) generally rely on large-scale annotated datasets for training. Yet in real-world applications, adequate labeled data is usually unavailable. Hence, methods that generalize effectively with small quantities * Corresponding author. of labeled samples and adapt quickly to new event types are highly desirable for event detection. Various approaches have been proposed to enable learning from only a few samples, i.e., fewshot learning (Finn et al., 2017; Snell et al., 2017; Zhang et al., 2018a). Yet few-shot event detection (FSED) has been less studied until recently (Lai et al., 2020a; Deng et al."
2021.findings-acl.214,2021.acl-long.373,0,0.0200639,"or prototype representations to classify query instances into event types. We conduct comprehensive experiments on the aggregated benchmark dataset of few-shot event detection (Deng et al., 2020). The experimental results show that our method consistently and substantially outperforms state-of-the-art methods. In all six N -way-M -shot settings, our model achieves a large F1 superiority of at least 15 absolute points. 2 Related Work Event Detection. Recent event detection methods based on neural networks have achieved good performance (Chen et al., 2015; Sha et al., 2016; Nguyen et al., 2016; Lou et al., 2021). These methods use neural networks to construct the context features of candidate trigger words to classify events. Pre-trained language models such as BERT (Devlin et al., 2019) have also become an indispensable component of event detection models (Yang et al., 2019; Wadden et al., 2019; Shen et al., 2020). However, neural models rely on largescale labeled event datasets and fail to predict the labels of new event types. A recent study utilized the basic metric-based few-shot learning method for event detection (Lai et al., 2020b). Deng et al. (2020) tackles few-shot learning for event class"
2021.findings-acl.214,P11-1163,0,0.0152708,"nt types of query samples. Introduction Event detection is an important task in information extraction, aiming at detecting event triggers from text and then classifying them into event types (Chen et al., 2015). For example, in “The police arrested Harry on charges of manslaughter”, the trigger word is arrested, indicating an “Arrest” event. Event detection has been widely applied in Twitter analysis (Zhou et al., 2017), legal case extraction (de Araujo et al., 2017), and financial event extraction (Zheng et al., 2019), to name a few. Typical approaches to event detection (Chen et al., 2015; McClosky et al., 2011; Liu et al., 2019) generally rely on large-scale annotated datasets for training. Yet in real-world applications, adequate labeled data is usually unavailable. Hence, methods that generalize effectively with small quantities * Corresponding author. of labeled samples and adapt quickly to new event types are highly desirable for event detection. Various approaches have been proposed to enable learning from only a few samples, i.e., fewshot learning (Finn et al., 2017; Snell et al., 2017; Zhang et al., 2018a). Yet few-shot event detection (FSED) has been less studied until recently (Lai et al.,"
2021.findings-acl.420,2004.iwslt-evaluation.1,0,0.182696,"Missing"
2021.findings-acl.420,2020.coling-tutorials.3,0,0.0205591,"agent using reinforcement learning with assigned rewards to balance the trade-off between translation quality and delay (Gu et al., 2016; Satija and Pineau, 2016; Alinejad et al., 2018). However, it has stability and robustness issues due to the sparse reward signals, so imitation learning using oracle actions has been independently attempted (Zheng et al., 2019; Arthur et al., 2020; Dalvi et al., 2018). Multilingual Machine Translation In NMT, multilingual training is a popular MTL approach as it is very simple, but effective (Johnson et al., 2017; Sachan and Neubig, 2018; Dong et al., 2015; Dabre et al., 2020). Instead of choosing entirely different NLP tasks and increase complexity of implementation (Niehues and Cho, 2017; Zaremoodi and Haffari, 2018), multilingual setting only involves concatenating multiple bilingual language pairs for training (Johnson et al., 2017). The language pairs are the task space in MTL, which determines the performance of the model, and so, the selection of language pairs influences the overall performance of translation (Tan et al., 2019). Parameter sharing in multilingual setting has also been extensively studied. Dong et al. (2015) initially had language-specific de"
2021.findings-acl.420,W18-6450,0,0.0117699,"rom TED. We choose the Germanic language group, German (DE) and Dutch (NL), and the Romance language group, Italian (IT), French (FR), and Romanian (RO). The languages within the same group generally have high syntactic similarity and the same word order. Unless otherwise specified, we use the same settings and preprocessing as described in Arthur et al. (2020).2 2 In our preliminary experiment, the pre-processing under concatenation of multilingual corpora with larger vocabulary 4759 S I MT Systems We compared two S I MT baselines, C OUPLED P OLICY (Arthur et al., 2020) and the WAIT-K model (Ma et al., 2018). For a fair comparison, we choose a value of k, which achieves comparable translation quality to the C OU PLED P OLICY system. Following some initial experiments, we choose k = 2. Parameter Sharing Since our model deals with the many-to-one translation task with an agent, we decided to separate i) encoder, ii) agent, and iii) encoder + agent. This idea came from the performance improvements that a number of studies demonstrated by separating the decoder in offline one-to-many MT (Dong et al., 2015; Sachan and Neubig, 2018). In S I MT, two modules, encoder and agent, are tied to the source, an"
2021.findings-acl.420,W17-4708,0,0.0200183,"delay (Gu et al., 2016; Satija and Pineau, 2016; Alinejad et al., 2018). However, it has stability and robustness issues due to the sparse reward signals, so imitation learning using oracle actions has been independently attempted (Zheng et al., 2019; Arthur et al., 2020; Dalvi et al., 2018). Multilingual Machine Translation In NMT, multilingual training is a popular MTL approach as it is very simple, but effective (Johnson et al., 2017; Sachan and Neubig, 2018; Dong et al., 2015; Dabre et al., 2020). Instead of choosing entirely different NLP tasks and increase complexity of implementation (Niehues and Cho, 2017; Zaremoodi and Haffari, 2018), multilingual setting only involves concatenating multiple bilingual language pairs for training (Johnson et al., 2017). The language pairs are the task space in MTL, which determines the performance of the model, and so, the selection of language pairs influences the overall performance of translation (Tan et al., 2019). Parameter sharing in multilingual setting has also been extensively studied. Dong et al. (2015) initially had language-specific decoder under oneto-many translation. This was further extended to sharing decoder parameters partially (Sachan and N"
2021.findings-acl.420,P02-1040,0,0.11678,"anslation task with an agent, we decided to separate i) encoder, ii) agent, and iii) encoder + agent. This idea came from the performance improvements that a number of studies demonstrated by separating the decoder in offline one-to-many MT (Dong et al., 2015; Sachan and Neubig, 2018). In S I MT, two modules, encoder and agent, are tied to the source, and therefore, reasonable to have them as language-specific parameters. Evaluation. Following Arthur et al. (2020), we evaluate the systems based on their translation quality and delay. Translation quality can be measured by case sensitive BLEU (Papineni et al., 2002). 3 We adopt two delay measurements by previous studies: (1) average proportion (AP) (Cho and Esipova, 2016) is a fraction of reading source words per emitted target words, and (2) average lagging (AL) (Ma et al., 2019) is an average number of lagged source words until all inputs are read. 3.1 Results In this section, we will describe the results of parameter sharing in S I MT. Following that, we present the comparison of multilingualism under different language groups. Parameter Sharing Strategies. Table 1 presents the results of various parameter sharing strategies for FR/IT/RO in the Romanc"
2021.findings-acl.420,W18-6319,0,0.0396395,"Missing"
2021.findings-emnlp.114,2020.coling-main.302,1,0.575124,"e distillation is summarized in Alg. 1 in (section A.1 - Appendix), which is similar to the one used in (Tan et al., 2019b). n=n+1; lowing KD objective for each language pair, sim Ladaptive (Dl , ✓s , {✓c }C , ↵) 1 KD ↵c |y| X X := C sim X X c=1 x,y2D l Q(v|y&lt;t , x, ✓c ) log P (v|y&lt;t , x, ✓s ) t=1 v2V (3) where ↵ dynamically weigh the contribution of the teacher-assistants/clusters. ↵ is computed via an attention mechanism based on the rewards (negative perplexity) attained by the teachers on the data, where these values are passed through a softmax transformation to turn into a distribution (Saleh et al., 2020). This adaptive distillation of knowledge allows the student model to get the best of teacher-assistants (which are representative of different linguistic features) based on their effectiveness to improve the knowledge gap of the student. The total loss function then becomes a weighted combination of losses coming from the ensemble of teachers and the data, sim Ladaptive (Dl , ✓s , {✓c }C , ↵) := 1 ALL 1316 1 LN LL (D l , ✓s ) + (4) adapt. l c Csim , ↵) 2 LKD (D , ✓s , {✓ }1 cluster 1 Japanese Korean Mongolian Burmese cluster 2 cluster 3 Chinese Malay Indonesian Thai Vietnamese Marathi Tamli B"
2021.findings-emnlp.114,P16-1162,0,0.0626839,"Missing"
2021.findings-emnlp.114,K16-1002,0,0.0640724,"Missing"
2021.findings-emnlp.114,N19-4009,0,0.0203711,"Missing"
2021.findings-emnlp.114,P02-1040,0,0.110742,"ia ms, id Japonic my, zh Au str on es he, ar Si no -T ib eta n sia et, fi, hu Af ro a kk, az, tr al ic Ur tic m an ic nb, da sv, de, nl Tu rk ic gl, pt, ro fr, es, it IE /G er an ia n bn, ur, ku hi, fa, mr IE /In do -Ir IE /It al ic -S lav ic lto IE /B a be, bs, sl, mk, lt, sk cs, uk, hr, sr, bg, pl, ru mn ta eu 4.1 Experimental Results The translation results of (53 languages ! English) for all approaches are summarised in Table 6. The language pairs are sorted based on the size of training data in an ascending order. The translation quality is evaluated and reported based on the BLEU score (Papineni et al., 2002). 4.1.1 Studies of cluster-based MNMT models In this section, we discuss the cluster-based MNMTs’ results through the following observations. Extremely low resource Low resource Enough resource Training Configuration: All models were trained with Transformer architecture (Vaswani et al., 2017). The individual baseline models were trained with the model hidden size of 256, feed-forward hidden size of 1024, and 2 layers. All multilingual models were trained with the model hidden size of 512, feed-forward hidden size of 1024, and 6 layers. We used selective multilingual knowledge distillation app"
2021.findings-emnlp.114,N18-2084,0,0.0416365,"Missing"
2021.findings-emnlp.114,2020.acl-main.754,0,0.034584,"Missing"
2021.findings-emnlp.231,2020.emnlp-main.469,1,0.695429,"y the nearest number as the related one to a given entity (“August 1996 to December 1997” for entity “PUK and KDP later co-operated”), resulting in wrong predictions. Complex Question Answering (CQA) is a challenging task, requiring a model to perform compositional and numerical reasoning. Originally proposed for the visual question answering (VQA) task, Neural Module Networks (NMNs) (Andreas et al., 2016) have recently been adopted to tackle the CQA problem over text (Gupta et al., 2020). The NMNs is an end-to-end differentiable model in the programmer-interpreter paradigm (Guo et al., 2020; Hua et al., 2020a,b). Briefly, the programmer learns to map each question into a program, i.e. a sequence of neural modules, and the interpreter then “executes” the program, operationalized by modules, on the paragraph to yield the answer for different types of complex questions. NMNs achieves the best performance on a subset of the challenging DROP dataset (Dua et al., 2019) and is interpertable by nature. However, NMNs’ performance advantage is not consistent, as it underperforms in some types of questions that require numerical reasoning. For instance, for date-compare questions, MTMSN (Hu 2713 1 All F1 an"
2021.findings-emnlp.231,D19-1251,0,0.060584,"n on the question, the interpreter can exploit the information contained in the question. Secondly, we propose an intuitive constraint to better relate numbers and their corresponding entities in the paragraph. Finally, we strengthen the auxiliary loss to increase attention values of entities in closer vicinity within a sentence. Experimental results show that our modifications significantly improve NMNs’ numerical reasoning performance by up to 3.0 absolute F1 points. With minor modification, these mechanisms are simple enough to be applied to other modular approaches. 2 Related Work NumNet (Ran et al., 2019) leveraged Graph Neural Network (GNN) to design a number-aware deep learning model. Also leveraging GNN, Chen et al. (2020a) distinguished number types more precisely by adding the connection with entities and obtained better performance. Chen et al. (2020b) searched possible programs exhaustively based on answer numbers and employed these programs as weak supervision to train the whole model. Using dependency parsing of questions, Saha et al. (2021) focused on the numerical part and obtained excellent results on different kinds of numerical reasoning questions. Neural Module Networks (NMNs) ("
2021.findings-emnlp.231,2020.emnlp-main.549,0,0.0625773,"Missing"
2021.findings-emnlp.231,N19-1246,0,0.0585475,"Missing"
2021.findings-emnlp.231,2020.coling-main.434,1,0.735245,"r tends to identify the nearest number as the related one to a given entity (“August 1996 to December 1997” for entity “PUK and KDP later co-operated”), resulting in wrong predictions. Complex Question Answering (CQA) is a challenging task, requiring a model to perform compositional and numerical reasoning. Originally proposed for the visual question answering (VQA) task, Neural Module Networks (NMNs) (Andreas et al., 2016) have recently been adopted to tackle the CQA problem over text (Gupta et al., 2020). The NMNs is an end-to-end differentiable model in the programmer-interpreter paradigm (Guo et al., 2020; Hua et al., 2020a,b). Briefly, the programmer learns to map each question into a program, i.e. a sequence of neural modules, and the interpreter then “executes” the program, operationalized by modules, on the paragraph to yield the answer for different types of complex questions. NMNs achieves the best performance on a subset of the challenging DROP dataset (Dua et al., 2019) and is interpertable by nature. However, NMNs’ performance advantage is not consistent, as it underperforms in some types of questions that require numerical reasoning. For instance, for date-compare questions, MTMSN (H"
2021.inlg-1.12,2020.inlg-1.23,0,0.0452387,"Missing"
2021.inlg-1.12,2020.nl4xai-1.12,0,0.0346515,"se explanations address actual user expectations. 2 Related Work In 1990-2000, explanations derived from knowledge bases were enhanced by addressing aspects of users’ reasoning. Specifically, Zukerman and McConachy (1993) and Horacek (1997) considered potential inferences from explanations, omitting easily inferable information and addressing erroneous inferences; Korb et al. (1997) took into account reasoning fallacies when explaining the reasoning of Bayesian Networks; and Stone (2000) generated instructions from which users could draw appropriate inferences about actions to take. Recently, Krause and Vossen (2020) identified additional triggers that should be addressed in explanations. Current research on explanation generation focuses on explaining the predictions made by ML models – a sub-field called Explainable AI (XAI). In particular, neural networks have received a lot of attention owing to their superior performance on one hand, and their opaqueness on the other hand. A common first step in explaining the predictions 1 The participants in our study were told that they have an AI, but they were not informed about the specifics of the ML model. Other explanatory objectives include enhancing trust"
2021.inlg-1.12,W19-8402,1,0.924562,"be addressed in explanations. Current research on explanation generation focuses on explaining the predictions made by ML models – a sub-field called Explainable AI (XAI). In particular, neural networks have received a lot of attention owing to their superior performance on one hand, and their opaqueness on the other hand. A common first step in explaining the predictions 1 The participants in our study were told that they have an AI, but they were not informed about the specifics of the ML model. Other explanatory objectives include enhancing trust in the system, and helping debug a system (Reiter, 2019). of neural networks is to build a local surrogate explainer model that uses a transparent model to approximate the neighbourhood of an instance of interest. Linear regression (Ribeiro et al., 2016; ˇ Strumbelj and Kononenko, 2014; Lundberg and Lee, 2017), decision rules (Ribeiro et al., 2018) and DTs (van der Waa et al., 2018; Guidotti et al., 2019; Sokol and Flach, 2020a) have been employed for this purpose. A DT’s prediction is generally explained by tracing the path from the root to a predicted outcome (Guidotti et al., 2019; Stepin et al., 2020). Recently, researchers have generated class"
2021.inlg-1.12,N16-3020,0,0.0431343,"ural networks have received a lot of attention owing to their superior performance on one hand, and their opaqueness on the other hand. A common first step in explaining the predictions 1 The participants in our study were told that they have an AI, but they were not informed about the specifics of the ML model. Other explanatory objectives include enhancing trust in the system, and helping debug a system (Reiter, 2019). of neural networks is to build a local surrogate explainer model that uses a transparent model to approximate the neighbourhood of an instance of interest. Linear regression (Ribeiro et al., 2016; ˇ Strumbelj and Kononenko, 2014; Lundberg and Lee, 2017), decision rules (Ribeiro et al., 2018) and DTs (van der Waa et al., 2018; Guidotti et al., 2019; Sokol and Flach, 2020a) have been employed for this purpose. A DT’s prediction is generally explained by tracing the path from the root to a predicted outcome (Guidotti et al., 2019; Stepin et al., 2020). Recently, researchers have generated class-contrastive counterfactual explanations to enhance the explanations of DT predictions. Stepin et al. (2020) generated explanations that have a factual and a counterfactual component; the former is"
ahmadnia-etal-2017-persian,J03-3002,0,\N,Missing
ahmadnia-etal-2017-persian,P07-1018,0,\N,Missing
ahmadnia-etal-2017-persian,P07-1108,0,\N,Missing
ahmadnia-etal-2017-persian,P07-2045,0,\N,Missing
ahmadnia-etal-2017-persian,2005.mtsummit-papers.11,0,\N,Missing
ahmadnia-etal-2017-persian,N15-1125,0,\N,Missing
ahmadnia-etal-2017-persian,tiedemann-2012-parallel,0,\N,Missing
ahmadnia-etal-2017-persian,N13-1073,0,\N,Missing
C08-1039,P07-1036,0,0.0437843,"Missing"
C08-1039,N04-1042,0,0.0400746,"ror on Citation Test (300L5000U) EMfreez Error on Citation Test (300L5000U) λ2 0.18 λ 0.21 Viterbi Decoding SMS Decoding Viterbi Decoding SMS Decoding 0.2 0.17 Error (per position) Error (per position) 0.19 0.16 0.15 0.14 0.18 0.17 0.16 0.15 0.14 0.13 λMLE 0.12 0 0.1 0.2 0.3 0.4 0.5 λ 0.6 0.7 0.8 0.9 λMLE 0.13 0.12 1 0 (a) 0.1 0.2 0.3 0.4 0.5 λ 0.6 0.7 0.8 0.9 1 (b) Figure 2: EMλ error rates while increasing the allocation from 0 to 1 by the step size 0.025. to segment the document into fields, and to label each field. In our experiments we use the bibliographic citation dataset described in (Peng and McCallum, 2004) (see Fig. 1 for an example of the input and expected label output for this task). This dataset has 500 annotated citations with 13 fields; 5000 unannotated citations were added to it later by (Grenager et al., 2005). The annotated data is split into a 300-document training set, a 100-document development (dev) set, and a 100document test set7 . We use a first order HMM with the size of hidden states equal to the number of fields (equal to 13). We freeze the transition probabilities to what has been observed in the labeled data and only learn the emission probabilities. The transition probabil"
C08-1039,J94-2001,0,\N,Missing
C08-1039,P05-1046,0,\N,Missing
C16-1302,J90-2002,0,0.908378,"Missing"
C16-1302,J93-2003,0,0.148786,"Missing"
C16-1302,N15-1144,0,0.0255671,"robabilities to a target word t using a conditional Gaussian distribution. We explain the required modifications needed in IBM alignment model 1 for using word embedding and briefly review the EM algorithm in this model. We then present the method to use the similar words for updating the distributions of each word and improving the alignment results. The embedding of word w ∈ W will be written as vw ∈ Rd . The proposed model will be called the Vector Model. 3.1 Alignment Model The IBM alignment models use a translation model in the form of conditional probability p(t|s). In a similar way to (Lin et al., 2015), given a source word s ∈ S, instead of the probability of the target word t ∈ T , the probability of a vector representation vt ∈ Rd of the word can be calculated. Correspondingly, each source word s ∈ S is represented by mean µs and covariance matrix Σs : p(t|s) = p(vt |µs , Σs ) = exp −1 − 2 (vtp µs )T Σ−1 s (vt − µs ) (2π)d |Σs |  (1) In this model, vector representations are only used for target words and source words are replaced with distributions in the target language vector space. 3.2 EM Algorithm The EM algorithm should have some modifications to update the Gaussian distributions."
C16-1302,P04-1066,0,0.0511394,"language given the sentence in another language. Most SMT systems use an implementation of the IBM alignment models (Brown et al., 1993). These models use expectation maximization (EM) algorithm in training and only require sentence-aligned bilingual texts. Inducing word alignment from bilingual texts requires large amount of sentence-aligned parallel data which is not available for most of the language pairs. It also causes more problems for rare words which are the key parts of the sentences, but at the same time, are less frequent and therefore have a more biased probability distribution (Moore, 2004). This paper deals with the alignment task for the rare words by proposing a new alignment model based on the low-dimensional representations of the words. We explore the effect of replacing words with their vector space embeddings in IBM Alignment Model 1. In this model, instead of using a conditional multinomial distribution (to generate a target word ti ∈ T given a source word si ∈ S), we use a conditional Gaussian distribution and generate a d-dimensional word embedding Vti ∈ Rd given si . We then propose a method to improve the alignments for rare words by using their similar words and up"
C16-1302,P00-1056,0,0.67084,"and sim(x, y) is the cosine similarity of vx and vy . In order to use the neighbors for updating the distributions of source words, in the maximization step of the EM algorithm, calculation of the means and covariances will have additional steps: X ∀s : µs = λµs + (1 − λ) ws (s0 )µs0 (8) s0 ∈Nk (s) ∀s : Σs = λΣs + (1 − λ) X s0 ∈N ws (s0 )Σs0 (9) k (s) where λ is the linear interpolation parameter and it should be estimated. 4 Experiments 4.1 Data The language pair used for all the experiments is English-French. The test set is a word-aligned bilingual corpus that contains 447 sentence pairs (Och and Ney, 2000b). Using larger data sets can result in better learning for the model, and in order to create corpora with different sizes, we appended more parallel sentences from the training set to the test data (Note that the proposed model is unsupervised and it does not use the gold alignment in the training). We created corpora with different sizes of sentence pairs and the experiments use these corpora. For creating the word embeddings, we used the tool word2vec1 (Mikolov et al., 2013). For the input, we used the English sentences and the French sentences separately and created two sets of vectors. T"
C16-1302,D14-1162,0,0.0892156,"y using their similar words and updating their distributions. The advantages of this model are: i) It uses monolingual word embedding which has more available training data; ii) By using the extracted knowledge from monolingual data, it has better results in lowresource word alignment tasks; iii) The Gaussian model can be applied to all generative alignment models by replacing the conditional distribution, and thus getting even better results. 2 2.1 Related Works Word Embeddings Recent works on word embedding show improvements in capturing semantic features of the words (Mikolov et al., 2013; Pennington et al., 2014). Since the word alignment task requires a form of statistics or comparison between words from the source and the target languages, a good translation model This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3209 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3209–3215, Osaka, Japan, December 11-17 2016. can result in better alignments. Bilingual word embedding models like (Zou et al., 2013; Søgaard et al., 2015) train vectors for"
C16-1302,P15-1165,0,0.0518196,"Missing"
C16-1302,P10-2005,0,0.0210503,"likely generate p(t|srare ) &gt; p(t|s), and therefore most of the target words will align to srare . The reason for this problem is the rare word srare has co-occurrence with only a few target words and it increases the conditional probabilities for those target words. If a source word s which is similar to srare exists and has co-occurrence with more target words, those target words could be used to improve the distribution of srare . There were proposed methods to overcome the problem of low-resource languages based on using different word alignment methods and combining the results like in (Xiang et al., 2010). We try to provide a combination of alignments to get better performance for the rare words. 3 Vector Model In this section, we develop a conditional model p(t|s) that, given a source word s, assigns probabilities to a target word t using a conditional Gaussian distribution. We explain the required modifications needed in IBM alignment model 1 for using word embedding and briefly review the EM algorithm in this model. We then present the method to use the similar words for updating the distributions of each word and improving the alignment results. The embedding of word w ∈ W will be written"
C16-1302,D13-1141,0,0.0307813,"the words (Mikolov et al., 2013; Pennington et al., 2014). Since the word alignment task requires a form of statistics or comparison between words from the source and the target languages, a good translation model This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 3209 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3209–3215, Osaka, Japan, December 11-17 2016. can result in better alignments. Bilingual word embedding models like (Zou et al., 2013; Søgaard et al., 2015) train vectors for words in both languages in the same vector space, Hence a translation model can be made by these embeddings and it can be really useful for the task of word alignment. Despite the advantages of bilingual embedding models, to achieve a good performance by these models and building more informative vector representations for words, a large amount of data is required, which is not available in low-resource language pairs. On the other hand, providing a good monolingual corpus for most of the languages needs less effort and building a model that mostly use"
C18-1120,D17-1209,0,0.0573428,"Missing"
C18-1120,P17-1177,0,0.0203115,"ion. Then, a decoder generates the translation from the encoded vector, which can dynamically change using the attention mechanism. One of the main premises about natural language is that words of a sentence are inter-related according to a (latent) hierarchical structure, i.e. a syntactic tree. Therefore, it is expected that modeling the syntactic structure should improve the performance of NMT, especially in low-resource or linguistically divergent scenarios, such as English-Farsi. In this direction, (Li et al., 2017) uses a sequence-to-sequence model, making use of linearised parse trees. (Chen et al., 2017b) has proposed a model which uses syntax to constrain the dynamic encoding of the source sentence via structurally constrained attention. (Bastings et al., 2017; Shuangzhi Wu, 2017) have incorporated syntactic information provided by the dependency tree of the source sentence. (Marcheggiani et al., 2018) has proposed a model to inject semantic bias into the encoder of NMT model. Recently, (Eriguchi et al., 2016; Chen et al., 2017a) have proposed methods to incorporate the hierarchical syntactic constituency information of the source sentence. In addition to the embedding of words, computed us"
C18-1120,J82-2005,0,0.674785,"Missing"
C18-1120,N16-1102,1,0.840265,"), where the forests are binarised, i.e. hyperedges with more than two tail nodes are converted to multiple hyperedges with two tail nodes. This is to ensure a fair comparison between our model and the T REE 2S EQ model (Eriguchi et al., 2016) where they use binary HPSG parse trees. Furthermore, we prune the forests by removing low probability hyperedges, which significantly reduces the size of the forests. In all experiments, we use the development sets for setting the hyper-parameters, and the test sets for evaluation. Implementation Details. We use Mantis implementation of attentional NMT (Cohn et al., 2016) to develop our code for F OREST 2S EQ and T REE 2S EQ with DyNet (Neubig et al., 2017). All neural models are trained end-to-end using Stochastic Gradient Descent, where the mini-batch size is set to 128. The maximum training epochs is set to 20, and we use early stopping on the development set as a stopping condition. We generate the translations using greedy decoding. The BLEU score is computed using ’multi-bleu.perl’ script in Moses. 5.2 Results The perplexity and BLEU scores of different models for all translation tasks are presented in Table 2. In all translation tasks, F OREST 2S EQ out"
C18-1120,P16-1078,0,0.334376,"in low-resource or linguistically divergent scenarios, such as English-Farsi. In this direction, (Li et al., 2017) uses a sequence-to-sequence model, making use of linearised parse trees. (Chen et al., 2017b) has proposed a model which uses syntax to constrain the dynamic encoding of the source sentence via structurally constrained attention. (Bastings et al., 2017; Shuangzhi Wu, 2017) have incorporated syntactic information provided by the dependency tree of the source sentence. (Marcheggiani et al., 2018) has proposed a model to inject semantic bias into the encoder of NMT model. Recently, (Eriguchi et al., 2016; Chen et al., 2017a) have proposed methods to incorporate the hierarchical syntactic constituency information of the source sentence. In addition to the embedding of words, computed using the vanilla sequential encoder, they compute the embeddings of phrases recursively, directed by the top-1 parse tree of the source sentence generated by a parser. Though the results are promising, the top-1 trees are prone to parser error, and furthermore cannot capture semantic ambiguities of the source sentence. In this paper, we address the aforementioned issues by using exponentially many trees encoded i"
C18-1120,P08-1067,0,0.302931,"+ |y|(N + x)) The difference among the three methods is N . For the S EQ 2S EQ model N is 0. For the T REE 2S EQ model the number of nodes in the tree is a constant function of the input size: N = |x |− 1. Since 1425 Sentence Length <10 10-19 20-29 >30 all Avg. tree nodes 7.94 12.3 21.18 31 10.33 Avg. forest nodes 9.77 18.99 41.79 78.72 14.84 Avg. # of trees in forests 6.13E+4 2.62E+16 2.76E+22 2.21E+15 1.41E+20 Table 1: The average number of nodes in trees and forests along with average number of trees in forests for En→Fa bucketed dataset. we used pruned forests obtained from the parser in (Huang, 2008), the number of nodes in the forest is variable. Table 1 shows the average value of N for trees/forests for different source lengths for one of the datasets we used in experiments. As seen, while forests contain exponentially many trees, on average, the number of nodes in parse forests is less than twice the number of nodes in the corresponding top-1 parse trees. It shows that our method considers exponentially many trees instead of top-1 tree using only a small linear overhead. 5 Experiments 5.1 The Setup Datasets. We make use of three different language pairs: English (En) to Farsi (Fa), Chi"
C18-1120,P07-2045,0,0.0107986,"guages are linguistically divergent and close, respectively. For En→Fa, we use the TEP corpus (Tiedemann, 2009) which is extracted from movie subtitles. It has about 341K sentence pairs, where we split into 337K for training, 2K for development, and 2K for test. For En→Ch, we use BTEC where ‘devset1 2’ and ‘devset 3’ are used as the development and test sets, and training consists of 44,016 sentence pairs. For En→De, we use the first 100K sentences of Europarl1 for training, ‘newstest2013’ for development, and ‘newstest2014’ for test. We lowercase and tokenise the corpora using Moses scripts (Koehn et al., 2007). Sentences longer than 50 words are removed, and words with frequency less than 5 are replaced with <U NK>. Compact forests and trees for source English sentences are obtained from the parser in (Huang, 2008), where the forests are binarised, i.e. hyperedges with more than two tail nodes are converted to multiple hyperedges with two tail nodes. This is to ensure a fair comparison between our model and the T REE 2S EQ model (Eriguchi et al., 2016) where they use binary HPSG parse trees. Furthermore, we prune the forests by removing low probability hyperedges, which significantly reduces the si"
C18-1120,P17-1064,0,0.0207054,"twork) reads the source sentences sequentially to produce a fixed-length vector representation. Then, a decoder generates the translation from the encoded vector, which can dynamically change using the attention mechanism. One of the main premises about natural language is that words of a sentence are inter-related according to a (latent) hierarchical structure, i.e. a syntactic tree. Therefore, it is expected that modeling the syntactic structure should improve the performance of NMT, especially in low-resource or linguistically divergent scenarios, such as English-Farsi. In this direction, (Li et al., 2017) uses a sequence-to-sequence model, making use of linearised parse trees. (Chen et al., 2017b) has proposed a model which uses syntax to constrain the dynamic encoding of the source sentence via structurally constrained attention. (Bastings et al., 2017; Shuangzhi Wu, 2017) have incorporated syntactic information provided by the dependency tree of the source sentence. (Marcheggiani et al., 2018) has proposed a model to inject semantic bias into the encoder of NMT model. Recently, (Eriguchi et al., 2016; Chen et al., 2017a) have proposed methods to incorporate the hierarchical syntactic constit"
C18-1120,D15-1166,0,0.132927,"Missing"
C18-1120,N18-2078,0,0.0222713,"Therefore, it is expected that modeling the syntactic structure should improve the performance of NMT, especially in low-resource or linguistically divergent scenarios, such as English-Farsi. In this direction, (Li et al., 2017) uses a sequence-to-sequence model, making use of linearised parse trees. (Chen et al., 2017b) has proposed a model which uses syntax to constrain the dynamic encoding of the source sentence via structurally constrained attention. (Bastings et al., 2017; Shuangzhi Wu, 2017) have incorporated syntactic information provided by the dependency tree of the source sentence. (Marcheggiani et al., 2018) has proposed a model to inject semantic bias into the encoder of NMT model. Recently, (Eriguchi et al., 2016; Chen et al., 2017a) have proposed methods to incorporate the hierarchical syntactic constituency information of the source sentence. In addition to the embedding of words, computed using the vanilla sequential encoder, they compute the embeddings of phrases recursively, directed by the top-1 parse tree of the source sentence generated by a parser. Though the results are promising, the top-1 trees are prone to parser error, and furthermore cannot capture semantic ambiguities of the sou"
C18-1120,P15-1150,0,0.453579,"mputes the embeddings of phrases in addition to the words, then attend on both words and phrases in the decoder. An example is depicted in Figure 1(b). Tree Encoder It consists of sequential and recursive parts. The sequential part is the vanilla sequence encoder discussed in Section 2.1, which computes the embeddings of words. Then, the embeddings of phrases are computed using the embeddings of their constituent words in a recursive bottom-up fashion: (phr) hk = TreeLSTM(hlk , hrk ). where hlk and hrk are hidden states of left and right children respectively. This method uses TreeLSTM units (Tai et al., 2015) to calculate the embedding of a parent node using its two two children units as follow: (i) i = σ(Ul hl + Ur(i) hr + b(i) ) (fl ) l f l = σ(Ul h + Ur(fl ) hr + b(fl ) ) (fr ) r f r = σ(Ul h + Ur(fr ) hr + b(fr ) ) (o) o = σ(Ul hl + Ur(o) hr + b(o) ) (˜ c) c˜ = tanh(Ul hl + Ur(˜c) hr + b(˜c) ) c(phr) = i c˜ + f l cl + f r cr h(phr) = o tanh(c(phr) ) where i, f l , f r , oj ,˜ cj are the input gate, left and right forget gates, output gate, and a state for updating r memory cell; c and cl are memory cells of the right and left units. Sequential Decoder Eriguchi et al. set the initial state of t"
D14-1208,P11-1055,0,0.582331,"set of relations or none of them. Consequently, a number of models have been proposed in literature to provide better heuristics for the mapping between the entity pair in the database and its mentions in the sentences of the corpus. Riedel et al. (2010) tightens the assumption of distant supervision in the following manner: “Given a pair of entities and their mentions in sentences from a corpus, at least one of the mentions express the relation given in the database”. In other words, it models the problem as that of multi-instance (mentions) single-label (relation) learning. Following this, Hoffmann et al. (2011) and Surdeanu et al. (2012) propose models that consider the mapping as that of multi-instance multi-label learning. The instances are the mentions of the entity pair in the sentences of the corpus and the entity pair can participate in more than one relation. Although, these models work very well in practice, they have a number of shortcomings. One of them is the possibility that during the alignment, a fact in the database might not have an instantiation in the corpus. For instance, if our corpus only contains documents from the years 2000 to 2005, the fact presidentOf(Barack Obama, United S"
D14-1208,P09-1113,0,0.702117,"inference procedures in training as integer linear programming (ILP) problems and implement the relaxation to the “at least one ” heuristic via a soft constraint in this formulation. Empirically, we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches. 1 Introduction Although supervised approaches to relation extraction (GuoDong et al., 2005; Surdeanu and Ciaramita, 2007) achieve very high accuracies, they do not scale as they are data intensive and the cost of creating annotated data is quite high. To alleviate this problem, Mintz et al. (2009) proposed relation extraction in the paradigm of distant supervision. In this approach, given a database of facts (e.g. Freebase1 ) and an unannotated document collection, the goal is to heuristically align the facts in the database to the sentences in the corpus which contain the entities mentioned in the fact. This is done to create weakly labeled training data to train a classifier for relation extraction. The underlying assumption is that all mentions of 1 Ganesh Ramakrishnan 3 Dept. of CSE, IIT Bombay ganesh@cse.iitb.ac.in an entity pair2 (i.e. sentences containing the entity pair) in the"
D14-1208,W04-2401,0,0.259456,"Missing"
D14-1208,D12-1042,0,0.582717,"f them. Consequently, a number of models have been proposed in literature to provide better heuristics for the mapping between the entity pair in the database and its mentions in the sentences of the corpus. Riedel et al. (2010) tightens the assumption of distant supervision in the following manner: “Given a pair of entities and their mentions in sentences from a corpus, at least one of the mentions express the relation given in the database”. In other words, it models the problem as that of multi-instance (mentions) single-label (relation) learning. Following this, Hoffmann et al. (2011) and Surdeanu et al. (2012) propose models that consider the mapping as that of multi-instance multi-label learning. The instances are the mentions of the entity pair in the sentences of the corpus and the entity pair can participate in more than one relation. Although, these models work very well in practice, they have a number of shortcomings. One of them is the possibility that during the alignment, a fact in the database might not have an instantiation in the corpus. For instance, if our corpus only contains documents from the years 2000 to 2005, the fact presidentOf(Barack Obama, United States) will not be present"
D14-1208,P05-1053,0,0.116053,"n the corpus. In this paper, we discuss and critically analyse a popular alignment strategy called the “at least one” heuristic. We provide a simple, yet effective relaxation to this strategy. We formulate the inference procedures in training as integer linear programming (ILP) problems and implement the relaxation to the “at least one ” heuristic via a soft constraint in this formulation. Empirically, we demonstrate that this simple strategy leads to a better performance under certain settings over the existing approaches. 1 Introduction Although supervised approaches to relation extraction (GuoDong et al., 2005; Surdeanu and Ciaramita, 2007) achieve very high accuracies, they do not scale as they are data intensive and the cost of creating annotated data is quite high. To alleviate this problem, Mintz et al. (2009) proposed relation extraction in the paradigm of distant supervision. In this approach, given a database of facts (e.g. Freebase1 ) and an unannotated document collection, the goal is to heuristically align the facts in the database to the sentences in the corpus which contain the entities mentioned in the fact. This is done to create weakly labeled training data to train a classifier for"
D15-1288,D07-1090,0,0.128612,"a collection. 1 Introduction Language models (LMs) are critical components in many modern NLP systems, including machine translation (Koehn, 2010) and automatic speech recognition (Rabiner and Juang, 1993). The most widely used LMs are mgram models (Chen and Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; In contrast, we1 make use of recent advances in compressed suffix trees (C STs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ the size of the corpus). We present methods for extracti"
D15-1288,P96-1041,0,0.495082,"ich provides near optimal compression while supporting efficient search. We present algorithms for on-the-fly computation of probabilities under a Kneser-Ney language model. Our technique is exact and although slower than leading LM toolkits, it shows promising scaling properties, which we demonstrate through ∞-order modeling over the full Wikipedia collection. 1 Introduction Language models (LMs) are critical components in many modern NLP systems, including machine translation (Koehn, 2010) and automatic speech recognition (Rabiner and Juang, 1993). The most widely used LMs are mgram models (Chen and Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepp"
D15-1288,W09-1505,0,0.222632,"on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; In contrast, we1 make use of recent advances in compressed suffix trees (C STs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ the size of the corpus). We present methods for extracting frequency and unique context count statistics for mgram queries from C STs, and two algorithms for computing Kneser-Ney LM probabilities on the fly using these statistics. The first method uses two C STs (over the corpus and the reversed corpus), which allow for efficient computation of the number of unique contexts to the left and right of an mgram, but is inefficient"
D15-1288,D10-1026,0,0.238269,"d Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; In contrast, we1 make use of recent advances in compressed suffix trees (C STs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ the size of the corpus). We present methods for extracting frequency and unique context count statistics for mgram queries from C STs, and two algorithms for computing Kneser-Ney LM probabilities on the fly using these statistics. The first method uses two C STs (over the corpus and the reversed corpus), which allow"
D15-1288,W11-2123,0,0.180217,"Missing"
D15-1288,kennington-etal-2012-suffix,0,0.0888967,"Missing"
D15-1288,P07-1065,0,0.046429,"are mgram models (Chen and Goodman, 1996), based on explicit storage of mgrams and their counts, which have proved highly accurate when trained on large datasets. To be useful, LMs need to be not only accurate but also fast and compact. Depending on the order and the training corpus size, a typical mgram LM may contain as many as several hundred billions of mgrams (Brants et al., 2007), raising challenges of efficient storage and retrieval. As always, there is a trade-off between accuracy, space, and time, with recent papers considering small but approximate lossy LMs (Chazelle et al., 2004; Talbot and Osborne, 2007; Guthrie and Hepple, 2010), or loss-less LMs backed by tries (Stolcke et al., 2011), or related compressed structures (Germann et al., 2009; In contrast, we1 make use of recent advances in compressed suffix trees (C STs) (Sadakane, 2007) to build compact indices with much more modest memory requirements (≈ the size of the corpus). We present methods for extracting frequency and unique context count statistics for mgram queries from C STs, and two algorithms for computing Kneser-Ney LM probabilities on the fly using these statistics. The first method uses two C STs (over the corpus and the rev"
D15-1288,P09-2086,0,0.24557,"Missing"
D15-1288,P11-1027,0,0.132637,"Missing"
D16-1094,W11-2123,0,0.285088,"one of language modeling, decompose the probability of an utterance into conditional probabilities of words given a fixed-length context. Due to sparsity of the events in natural language, smoothing techniques are critical for generalisation beyond the training text when estimating the parameters of m-gram LMs. This is particularly important when the training text is 1 For the implementation see: https://github.com/ eehsan/cstlm Previous research in MKN language modeling, and more generally m-gram models, has mainly dedicated efforts to make them faster and more compact (Stolcke et al., 2011; Heafield, 2011; Shareghi et al., 2015) using advanced data structures such as succinct suffix trees. An exception is Hierarchical Pitman-Yor Process LMs (Teh, 2006a; Teh, 2006b) providing a rich Bayesian smoothing scheme, for which Kneser-Ney smoothing corresponds to an approximate inference method. Inspired by this work, we directly enrich MKN smoothing realising some of the reductions while remaining more efficient in learning and inference. We provide estimators for our additional discount parameters by extending the discount bounds in MKN. We empirically analyze our enriched MKN LMs on several European"
D16-1094,2005.mtsummit-papers.11,0,0.105888,"s,  if i = 0 0, ni+1 [m] n1 [m] m D (i) = i − (i + 1) ni [m] n1 [m]+2n2 [m] , if i < 10  n11 [m] n1 [m] 10 − 11 n10 [m] . n1 [m]+2n2 [m] , if i ≥ 10 It can be shown that the above estimators for our discount parameters are derived by maximizing a lower bound on the leave-one-out likelihood of the training set, following (Ney et al., 1994; Chen and Goodman, 1999) (see Appendix B for the proof sketch). 3 Experiments We compare the effect of using different numbers of discount parameters on perplexity using the Finnish (FI), Spanish (ES), German (DE), English (EN) portions of the Europarl v7 (Koehn, 2005) corpus. For each language we excluded the first 10K sentences and used it as the in-domain test set (denoted as EU), skipped the second 10K sentences, and used the rest as the training set. The data was tokenized, sentence split, and the XML markup discarded. We tested the effect of domain mismatch, under two settings for out-of-domain test sets: i) mild using the Spanish section of news-test 2013, the German, English sections of news-test 2014, and the Finnish section 4 We have selected the value of 10 arbitrarily; however our approach can be used with larger number of discount parameters, w"
D16-1094,D15-1288,1,0.833577,"modeling, decompose the probability of an utterance into conditional probabilities of words given a fixed-length context. Due to sparsity of the events in natural language, smoothing techniques are critical for generalisation beyond the training text when estimating the parameters of m-gram LMs. This is particularly important when the training text is 1 For the implementation see: https://github.com/ eehsan/cstlm Previous research in MKN language modeling, and more generally m-gram models, has mainly dedicated efforts to make them faster and more compact (Stolcke et al., 2011; Heafield, 2011; Shareghi et al., 2015) using advanced data structures such as succinct suffix trees. An exception is Hierarchical Pitman-Yor Process LMs (Teh, 2006a; Teh, 2006b) providing a rich Bayesian smoothing scheme, for which Kneser-Ney smoothing corresponds to an approximate inference method. Inspired by this work, we directly enrich MKN smoothing realising some of the reductions while remaining more efficient in learning and inference. We provide estimators for our additional discount parameters by extending the discount bounds in MKN. We empirically analyze our enriched MKN LMs on several European languages in in- and out"
D16-1094,P06-1124,0,0.0559302,"of the events in natural language, smoothing techniques are critical for generalisation beyond the training text when estimating the parameters of m-gram LMs. This is particularly important when the training text is 1 For the implementation see: https://github.com/ eehsan/cstlm Previous research in MKN language modeling, and more generally m-gram models, has mainly dedicated efforts to make them faster and more compact (Stolcke et al., 2011; Heafield, 2011; Shareghi et al., 2015) using advanced data structures such as succinct suffix trees. An exception is Hierarchical Pitman-Yor Process LMs (Teh, 2006a; Teh, 2006b) providing a rich Bayesian smoothing scheme, for which Kneser-Ney smoothing corresponds to an approximate inference method. Inspired by this work, we directly enrich MKN smoothing realising some of the reductions while remaining more efficient in learning and inference. We provide estimators for our additional discount parameters by extending the discount bounds in MKN. We empirically analyze our enriched MKN LMs on several European languages in in- and outof-domain settings. The results show that our discounting mechanism significantly improves the perplexity compared to MKN and"
D17-1014,P17-2058,0,0.0315482,"Missing"
D17-1014,J96-1002,0,0.288209,"wing objective function subject to the simplex constraints: Q(yˆ1 , . . . , yˆ` ) − γ ` X X yˆi (w) log i=1 w∈VT Exponentiated Gradient (EG) Exponentiated gradient (Kivinen and Warmuth, 1997) is an elegant algorithm for solving optimisation problems involving simplex constraints. Re2 X = Q(yˆ1 , . . . , yˆ` ) − γ ` X Entropy(yˆi ) 1 yˆi (w) (7) i=1 In other words, the algorithm looks for the maximum entropy solution which also maximizes the Ties are broken arbitrarily. 148 log likelihood under the model. There are intriguing parallels with the maximum entropy formulation of log-linear models (Berger et al., 1996). In our setting, the entropy term acts as a prior which discourages overly-confident estimates in the absence of sufficient evidence. 3.2 reflects the natural temporal order of spoken language. However, the right-to-left model is likely to provide a complementary signal in translation, as it will be bringing different biases and making largely independent prediction errors to those of the left-to-right model. For this reason, we propose to use both models, and seek to find translations that have high probability according both models (this mirrors work on bidirectional decoding in classical s"
D17-1014,2006.amta-papers.11,0,0.0406815,"resulting SGD algorithm is summarized in Algorithm 2. 4 (8) Bilingual Ensemble. Another source of complementary information is in terms of the translation direction, that is forward translation from the source to the target language, and reverse translation in the target to source direction. Decoding must find a translation which scores well under both the forward and reverse translation models. This is inspired by the direct and reverse feature functions commonly used in classical discriminative SMT (Och and Ney, 2002) which have been shown to offer some complementary benefits (although see Lopez and Resnik (2006)). More specifically, we decode for the best translation in the intersection of the source-to-target and targetto-source models by minimizing the following objective function: Decoding in Extended NMT Our decoding framework allows us to effectively and flexibly add additional global factors over the output symbols during inference. This enables decoding for richer global models, for which there is no effective means of greedy decoding or beam search. We outline several such models, and their corresponding relaxed objective functions for optimisation-based decoding. Bidirectional Ensemble. Stan"
D17-1014,W16-2323,0,0.0203662,"the Mantidae toolkit4 (Cohn et al., 2016), and using the dynet deep learning library5 (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For the vocabulary, we use word frequency cut-off of 5, and words rarer than this were mapped to a sentinel. For the large-scale WMT dataset, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) to better handle unknown words.6 For training our neural models, we use early stopping based on development perplexity, which usually occurs after 5-8 epochs. Evaluation Metrics. We evaluated in terms of search error, measured using the model score of the inferred solution (either continuous or discrete), as well as measuring the end translation quality with case-insensitive BLEU (Papineni et al., 2002). The continuous cost measures 1 ˆ |x) under the model Θ; the dis− |y| ˆ log PΘ (y crete model score has the same formulation, albeit using the discrete rounded solution y (see §3). Note the co"
D17-1014,D16-1096,0,0.107284,"Missing"
D17-1014,2014.iwslt-evaluation.1,0,0.105828,"Missing"
D17-1014,N16-1102,1,0.737862,"ation problem is, in general, non-convex, finding a plausible initialisation is likely to be important for avoiding local optima. Furthermore, a proper step size is a key in the success of the EG-based and SGD-based optimisation algorithms, and there is no obvious method how to best choose its value. We may also adaptively change the step size using (scheduled) annealing or via the line search. We return to this considerations in the experimental evaluation. 5 5.1 . η is the step size NMT Models. We implemented our continuous-optimisation based decoding method on top of the Mantidae toolkit4 (Cohn et al., 2016), and using the dynet deep learning library5 (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For the vocabulary, we use word frequency cut-off of 5, and words rarer than this were mapped to a sentinel. For the large-scale WMT dataset, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) to better handle un"
D17-1014,P02-1038,0,0.067888,"re-ranking evaluation. X ∂Q(.) ∂ yˆi (w0 ) ∂Q = ∂ rˆi (w) ∂ yˆi (w0 ) ∂ rˆi (w) 0 w ∈VT The resulting SGD algorithm is summarized in Algorithm 2. 4 (8) Bilingual Ensemble. Another source of complementary information is in terms of the translation direction, that is forward translation from the source to the target language, and reverse translation in the target to source direction. Decoding must find a translation which scores well under both the forward and reverse translation models. This is inspired by the direct and reverse feature functions commonly used in classical discriminative SMT (Och and Ney, 2002) which have been shown to offer some complementary benefits (although see Lopez and Resnik (2006)). More specifically, we decode for the best translation in the intersection of the source-to-target and targetto-source models by minimizing the following objective function: Decoding in Extended NMT Our decoding framework allows us to effectively and flexibly add additional global factors over the output symbols during inference. This enables decoding for richer global models, for which there is no effective means of greedy decoding or beam search. We outline several such models, and their corres"
D17-1014,P02-1040,0,0.11969,"Missing"
D17-1014,P16-1162,0,0.034548,"the Mantidae toolkit4 (Cohn et al., 2016), and using the dynet deep learning library5 (Neubig et al., 2017). All neural network models were configured with 512 input embedding and hidden layer dimensions, and 256 alignment dimension, with 1 and 2 hidden layers in the source and target, respectively. We used a LSTM recurrent structure (Hochreiter and Schmidhuber, 1997) for both source and target RNN sequences. For the vocabulary, we use word frequency cut-off of 5, and words rarer than this were mapped to a sentinel. For the large-scale WMT dataset, we applied byte-pair encoding (BPE) method (Sennrich et al., 2016) to better handle unknown words.6 For training our neural models, we use early stopping based on development perplexity, which usually occurs after 5-8 epochs. Evaluation Metrics. We evaluated in terms of search error, measured using the model score of the inferred solution (either continuous or discrete), as well as measuring the end translation quality with case-insensitive BLEU (Papineni et al., 2002). The continuous cost measures 1 ˆ |x) under the model Θ; the dis− |y| ˆ log PΘ (y crete model score has the same formulation, albeit using the discrete rounded solution y (see §3). Note the co"
D17-1014,C02-1050,0,0.689456,"recurrent architecture, and accordingly are non-Markov. This prevents exact dynamic programming solutions, and moreover, limits the potential to incorporate additional global features or constraints. Global factors can be highly useful in producing better and more diverse translations. Secondly, the sequential decoding of symbols in the target sequence, the inter-dependencies among the target symbols are not fully exploited. For example, when decoding the words of the target sentence in a left-to-right manner, the right context is not exploited leading potentially to inferior performance (see Watanabe and Sumita (2002a) who apply this idea in traditional statistical MT). A natural way to capture this is to intersect leftto-right and right-to-left models, however the resulting model has no natural generation order, and thus standard decoding methods are unsuitable. We introduce a novel decoding framework (§ 3) that relaxes this discrete optimisation problem into a continuous optimisation problem. This is akin to linear programming relaxation approach for approximate inference in graphical models with discrete random variables, where the exact inference is NP-hard (Sontag, 2010; Belanger and McCallum, 2016)."
D17-1014,D16-1137,0,0.0205172,"Missing"
D17-1014,P15-1002,0,\N,Missing
D17-1014,2002.tmi-tutorials.2,0,\N,Missing
D17-1229,N16-1037,1,0.873035,"and Ingrid Zukerman and Gholamreza Haffari Faculty of Information Technology Monash University, Australia hung.tran,ingrid.zukerman,gholamreza.haffari@monash.edu Abstract et al., 2000; Kalchbrenner and Blunsom, 2013; Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017). Instance-based methods treat each utterance as an independent data point, which allows the application of general machine learning models, such as Support Vector Machines. Sequencelabeling methods include methods based on Hidden Markov Models (HMMs) (Stolcke et al., 2000) and neural networks (Kalchbrenner and Blunsom, 2013; Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017). This paper introduces a novel training/decoding strategy for sequence labeling. Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classification demonstrate the effectiveness of this approach. Even though our underlying neural network model is relatively si"
D17-1229,W13-3214,0,0.624006,"t Classification Quan Hung Tran and Ingrid Zukerman and Gholamreza Haffari Faculty of Information Technology Monash University, Australia hung.tran,ingrid.zukerman,gholamreza.haffari@monash.edu Abstract et al., 2000; Kalchbrenner and Blunsom, 2013; Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017). Instance-based methods treat each utterance as an independent data point, which allows the application of general machine learning models, such as Support Vector Machines. Sequencelabeling methods include methods based on Hidden Markov Models (HMMs) (Stolcke et al., 2000) and neural networks (Kalchbrenner and Blunsom, 2013; Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017). This paper introduces a novel training/decoding strategy for sequence labeling. Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classification demonstrate the effectiveness of this approach. Even though our underlying neural network model"
D17-1229,J00-3003,0,0.948704,"Missing"
D17-1229,E17-1041,1,0.89042,"ffari Faculty of Information Technology Monash University, Australia hung.tran,ingrid.zukerman,gholamreza.haffari@monash.edu Abstract et al., 2000; Kalchbrenner and Blunsom, 2013; Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017). Instance-based methods treat each utterance as an independent data point, which allows the application of general machine learning models, such as Support Vector Machines. Sequencelabeling methods include methods based on Hidden Markov Models (HMMs) (Stolcke et al., 2000) and neural networks (Kalchbrenner and Blunsom, 2013; Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017). This paper introduces a novel training/decoding strategy for sequence labeling. Instead of greedily choosing a label at each time step, and using it for the next prediction, we retain the probability distribution over the current label, and pass this distribution to the next prediction. This approach allows us to avoid the effect of label bias and error propagation in sequence learning/decoding. Our experiments on dialogue act classification demonstrate the effectiveness of this approach. Even though our underlying neural network model is relatively simple, it outperforms more complex neural"
D18-1341,W17-4772,0,0.293592,"iting is considered as the modification process of a machine translated text with a minimum labor effort rather than re-translation from scratch. Previous studies in neural APE have primarily concentrated on formalizing APE as a monolingual MT problem in the target language, with or without conditioning on the source sentence (Pal et al., 2016; Chatterjee et al., 2017). MT approach has suffered from over-correction where APE system performs unnecessary correction leading to paraphasing and the degradation of the output quality (Bojar et al., 2016, 2017). Recent works (Libovick´y et al., 2016; Berard et al., 2017) have attempted to learn the predict of sequence of post-editing operations, e.g. insertion and deletion, to induce APE programs to turn the machine translated text into the desired outGholamreza Haffari Faculty of Information Technology Monash University, Austrlia first.last@monash.edu put. Previous program induction approaches suffer from over-cautiousness, where the APE system tends to keep the machine translated text without any modification (Bojar et al., 2017). In this paper, we propose a programmerinterpreter approach to the APE task to address the over-cautiousness problem. Our archite"
D18-1341,W17-4773,0,0.0627667,"machine translation (MT) output. APE systems can also be used to adapt general-purpose MT output to specific domains without re-training MT models, or to incorporate information which is not available or expensive to compute at MT decoding stage. Postediting is considered as the modification process of a machine translated text with a minimum labor effort rather than re-translation from scratch. Previous studies in neural APE have primarily concentrated on formalizing APE as a monolingual MT problem in the target language, with or without conditioning on the source sentence (Pal et al., 2016; Chatterjee et al., 2017). MT approach has suffered from over-correction where APE system performs unnecessary correction leading to paraphasing and the degradation of the output quality (Bojar et al., 2016, 2017). Recent works (Libovick´y et al., 2016; Berard et al., 2017) have attempted to learn the predict of sequence of post-editing operations, e.g. insertion and deletion, to induce APE programs to turn the machine translated text into the desired outGholamreza Haffari Faculty of Information Technology Monash University, Austrlia first.last@monash.edu put. Previous program induction approaches suffer from over-cau"
D18-1341,W16-2378,0,0.0303057,"ng The architecture consists of three components (i) A S EQ 2S EQ model to translate the source sentence to the target in the forced-decoding mode (MT), (ii) A S EQ 2S EQ model to incrementally generate the sequence of edit operations (Action Generator), and (iii) An RNN to summarize the post edited sequence of words produced from the execution of actions generated so far (Interpreter). 3049 dataset contains 12K and 11K post-editing triplets (English, translated German, post-edited German) respectively in IT domain. We concatenated them to an 23K triplets. A synthetic corpus of 500K triplets (Junczys-Dowmunt and Grundkiewicz, 2016) is also available as additional training data. We performed our experiment in two different settings with and without synthetic data for comparison with Berard et al. (2017). The RNN in the interpreter component can be thought of as a language model. This paves the way to pre-train it using monolingual text. We collect in-domain IT text from OPUS3 from the following sections: GNOME, KDE, KDEdoc, OpenOffice, OpenOffice3, PHP and Ubuntu. After tokenizing, filtering out sentences containing special characters, and removing duplications, we obtain around 170K sentences. Interpreter ???? ??1 ?????"
D18-1341,I17-1013,0,0.0369452,"rogram when executing on MT would return the PE reference. The color denotes the alignment between MT, reference PE and the APE program. The number subscript shows the edit position in original MT sentence. was able to reduce the preposition related errors. Blindly performing edition over MT output, the monolingual APE has difficulty to correct missing word or information in the source sentence. Neural multi-source MT architectures are applied to better capture the connection between the source sentence/machine translated text and the PE output (Libovick´y et al., 2016; Varis and Bojar, 2017; Junczys-Dowmunt and Grundkiewicz, 2017). Chatterjee et al. (2017) ensemble several different models including monolingual MT (TGT→PE), bilingual MT (SRC→PE), and multi-source (SRC,TGT→PE). Libovick´y et al. (2016); Berard et al. (2017) have proposed learning to predict the sequence of edit operations, aka the program, to produce the post-editing sentence (c.f. §3). Our work is motivated by Ling et al. (2017) on learning to indirectly solve an algebraic word problem by inducing a program which generates the answer together with an explanation. It further builds up on recent work on neural programmerinterpreter (Reed and De Freitas,"
D18-1341,W16-2361,0,0.0943779,"Missing"
D18-1341,P17-1015,0,0.0612537,"ence. Neural multi-source MT architectures are applied to better capture the connection between the source sentence/machine translated text and the PE output (Libovick´y et al., 2016; Varis and Bojar, 2017; Junczys-Dowmunt and Grundkiewicz, 2017). Chatterjee et al. (2017) ensemble several different models including monolingual MT (TGT→PE), bilingual MT (SRC→PE), and multi-source (SRC,TGT→PE). Libovick´y et al. (2016); Berard et al. (2017) have proposed learning to predict the sequence of edit operations, aka the program, to produce the post-editing sentence (c.f. §3). Our work is motivated by Ling et al. (2017) on learning to indirectly solve an algebraic word problem by inducing a program which generates the answer together with an explanation. It further builds up on recent work on neural programmerinterpreter (Reed and De Freitas, 2016), where a neural network programmer learns to program an interpreter. The architecture is then trained using expert action trajectories as programs. 3 The NPI-APE Approach Given a source sentence s and a machine translated sentence m , the goal is to find a postm, s ) where edited sentence t = arg maxt 0 Pape (tt0 |m Pape (.) is our probabilistic APE model. In our"
D18-1341,P16-2046,0,0.0942443,"Missing"
D18-1341,W17-4777,0,0.217502,"e 1: An example of PE program when executing on MT would return the PE reference. The color denotes the alignment between MT, reference PE and the APE program. The number subscript shows the edit position in original MT sentence. was able to reduce the preposition related errors. Blindly performing edition over MT output, the monolingual APE has difficulty to correct missing word or information in the source sentence. Neural multi-source MT architectures are applied to better capture the connection between the source sentence/machine translated text and the PE output (Libovick´y et al., 2016; Varis and Bojar, 2017; Junczys-Dowmunt and Grundkiewicz, 2017). Chatterjee et al. (2017) ensemble several different models including monolingual MT (TGT→PE), bilingual MT (SRC→PE), and multi-source (SRC,TGT→PE). Libovick´y et al. (2016); Berard et al. (2017) have proposed learning to predict the sequence of edit operations, aka the program, to produce the post-editing sentence (c.f. §3). Our work is motivated by Ling et al. (2017) on learning to indirectly solve an algebraic word problem by inducing a program which generates the answer together with an explanation. It further builds up on recent work on neural pro"
D19-5304,E17-3017,0,0.0348488,"pineni et al., 2001) results using the median performance according to the dev set and an ensemble of the 5 models. For the word-based models, we remove any tokens with frequency lower than 2 (as in Sperber et al. (2017)), while for subword models we do not perform any threshold pruning. We report all results on the Fisher “dev2” set.4 Models and Evaluation All our models are trained on the Fisher training set. For the 1-best baseline we use a standard seq2seq architecture and for the GGNN models, we use the same setup as Beck et al. (2018). Our implementation is based on the Sockeye toolkit (Hieber et al., 2017) and we use default values for most hyperparameters, except for batch size (16) and GGNN layers (8).3 For regularisation, we apply 0.5 dropout on the input embeddings and perform early stopping on the corresponding Fisher dev set. 3.1 Out-of-the-box ASR scenario In this scenario we assume only lattices and 1-best outputs are available, simulating a setting where we do not have access to the transcriptions. Table 1 shows that results are consistent with previous work: lattices provide significant improvements over simply using the 1-best output. More importantly though, the results also highlig"
D19-5304,U17-1006,1,0.890487,"Missing"
D19-5304,N18-1008,0,0.0959931,"Missing"
D19-5304,D15-1218,0,0.021353,"s to the original transcriptions used to train the ASR system, which can limit applicability in real scenarios. In this work we propose an approach for speech translation through lattice transformations and neural models based on graph networks. Experimental results show that our approach reaches competitive performance without relying on transcriptions, while also being orders of magnitude faster than previous work. 1 Introduction Translation from speech utterances is a challenging problem that has been studied both under statistical, symbolic approaches (Ney, 1999; Casacuberta et al., 2004; Kumar et al., 2015) and more recently using neural models (Sperber et al., 2017). Most previous work rely on pipeline approaches, using the output of a speech recognition system (ASR) as an input to a machine translation (MT) one. These inputs can be simply the 1-best sentence returned by the ASR system or a more structured representation such as a lattice. Some recent work on end-to-end systems bypass the need for intermediate representations, with impressive results (Weiss et al., 2017). However, such a scenario has drawbacks. From a practical perspective, it requires access to the original speech utterances a"
D19-5304,D17-1209,0,0.0431198,"Missing"
D19-5304,P18-1026,1,0.899753,"tical perspective, intermediate representations such as lattices can be enriched through external, textual resources such as monolingual corpora or dictionaries. Sperber et al. (2017) proposes a lattice-tosequence model which, in theory, can address both problems above. However, their model suffers 2 Approach Many graph network options exist in the literature (Bruna et al., 2014; Duvenaud et al., 2015; Kipf and Welling, 2017; Gilmer et al., 2017): in this work we opt for a Gated Graph Neural Network (Li et al., 2016, GGNN), which was recently incorporated in an encoder-decoder architecture by Beck et al. (2018). Assume a directed graph G = {V, E, LV , LE }, where V is a set of nodes (v, `v ), E is a set of edges (vi , vj , `e ) and LV and LE are respectively vocabularies for nodes and edges, from which node and edge labels (`v and `e ) are defined. Given an input graph with node embeddings X, a GGNN is defined as h0v = xv ! rtv =σ crv X W`re h(t−1) u + br`e u∈Nv ! ztv = σ czv X W`ze h(t−1) + bz`e u u∈Nv 26 Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13), pages 26–31 c Hong Kong, November 4, 2019. 2019 Association for Computational Linguis"
D19-5304,D15-1166,0,0.115404,"Missing"
D19-5304,D17-1159,0,0.031261,"valence classes, running in O(n log n) time, where n is the number of nodes. 1 con con qui´ en &lt;/s> hab@@ lo la &lt;s> con qui´ en &lt;/s> hab@@ lo Figure 1: Proposed lattice transformations. From top to bottom: 1) Original lattice with scores removed; 2) Line graph transformation; 3) Subword segmentation; 4) Lattice minimisation; 5) Addition of reverse and self-loop edges. The final step adds reverse and self-loop edges to the lattice, where these new edges have specific parameters in the encoder. This eases propagation of information and is standard practice when using graph networks as encoders (Marcheggiani and Titov, 2017; Bastings et al., 2017; Beck et al., 2018). We show an example of all the transformation steps on Figure 1. In Figure 2 we show the architecture of our system, using the final lattice from Figure 1 as an example. Nodes are represented as embeddings that are updated according to the lattice structure, resulting in a set of hidden states as the output. Other components follow a standard seq2seq model, using a bilinear attention module (Luong et al., 2015) and a 2-layer LSTM (Hochreiter and This procedure is also done in Sperber et al. (2017). 27 &lt;s> with con who quién ... hab@@ Bilinear Attenti"
D19-5304,2001.mtsummit-papers.68,0,0.0663366,"ded with the datasets. Following previous work (Post et al., 2013; Sperber et al., 2017), we lowercase and remove punctuation from the English translations. To build the BPE models, we extract the vocabulary from the Spanish training lattices, using 8K split operations. 1-best L L+S L+S+M 32.4 36.1 34.4 38.3 34.5 38.7 34.3 39.1 Table 1: Out-of-the-box scenario results, in BLEU scores. “L” corresponds to word lattice inputs, “L+S” and “L+S+M” correspond to lattices after subword segmentation and after minimisation, respectively. Each model is trained using 5 different seeds and we report BLEU (Papineni et al., 2001) results using the median performance according to the dev set and an ensemble of the 5 models. For the word-based models, we remove any tokens with frequency lower than 2 (as in Sperber et al. (2017)), while for subword models we do not perform any threshold pruning. We report all results on the Fisher “dev2” set.4 Models and Evaluation All our models are trained on the Fisher training set. For the 1-best baseline we use a standard seq2seq architecture and for the GGNN models, we use the same setup as Beck et al. (2018). Our implementation is based on the Sockeye toolkit (Hieber et al., 2017)"
D19-5304,2013.iwslt-papers.14,0,0.638115,"TM (Hochreiter and This procedure is also done in Sperber et al. (2017). 27 &lt;s> with con who quién ... hab@@ Bilinear Attention am la I lo speaking &lt;/s> Embeddings GGNN Encoder Attention RNN Decoder Figure 2: Model architecture, using the final Spanish lattice from Figure 1 and its corresponding English translation as an example. Schmidhuber, 1997) as the decoder. 3 Median Ensemble Experiments Data We perform experiments using the Fisher/Callhome Speech Translation corpus, composed of Spanish telephone conversations with their corresponding English translations. We use the original release by Post et al. (2013), containing both 1-best and pruned lattice outputs from an ASR system for each Spanish utterance.2 The Fisher corpus contain 150K instances and we use the original splits provided with the datasets. Following previous work (Post et al., 2013; Sperber et al., 2017), we lowercase and remove punctuation from the English translations. To build the BPE models, we extract the vocabulary from the Spanish training lattices, using 8K split operations. 1-best L L+S L+S+M 32.4 36.1 34.4 38.3 34.5 38.7 34.3 39.1 Table 1: Out-of-the-box scenario results, in BLEU scores. “L” corresponds to word lattice inp"
D19-5304,N03-1009,0,0.0218424,"Missing"
D19-5304,P16-1162,0,0.0488546,"l Beck† Trevor Cohn† Gholamreza Haffari‡ † School of Computing and Information Systems University of Melbourne, Australia {d.beck,t.cohn}@unimelb.edu.au ‡ Faculty of Information Technology Monash University, Australia gholamreza.haffari@monash.edu Abstract from training speed performance due to the lack of efficient batching procedures and they rely on transcriptions for pretraining. In this work, we address these two problems by applying lattice transformations and graph networks as encoders. More specifically, we enrich the lattices by applying subword segmentation using byte-pair encoding (Sennrich et al., 2016, BPE) and perform a minimisation step to remove redundant nodes arising from this procedure. Together with the standard batching strategies provided by graph networks, we are able to decrease training time by two orders of magnitude, enabling us to match their translation performance under the same training speed constraints without relying on gold transcriptions. Speech translation systems usually follow a pipeline approach, using word lattices as an intermediate representation. However, previous work assume access to the original transcriptions used to train the ASR system, which can limit"
D19-5304,D17-1145,0,0.426551,"em, which can limit applicability in real scenarios. In this work we propose an approach for speech translation through lattice transformations and neural models based on graph networks. Experimental results show that our approach reaches competitive performance without relying on transcriptions, while also being orders of magnitude faster than previous work. 1 Introduction Translation from speech utterances is a challenging problem that has been studied both under statistical, symbolic approaches (Ney, 1999; Casacuberta et al., 2004; Kumar et al., 2015) and more recently using neural models (Sperber et al., 2017). Most previous work rely on pipeline approaches, using the output of a speech recognition system (ASR) as an input to a machine translation (MT) one. These inputs can be simply the 1-best sentence returned by the ASR system or a more structured representation such as a lattice. Some recent work on end-to-end systems bypass the need for intermediate representations, with impressive results (Weiss et al., 2017). However, such a scenario has drawbacks. From a practical perspective, it requires access to the original speech utterances and transcriptions, which can be unrealistic if a user needs t"
D19-5618,P05-1045,0,0.00612294,"ty of the translations. More specifically, we want to compare the number of words in the gold translations which are missed in the generated translations produced by the following systems: (i) MT only; (ii) MTL-Biased; (iii) MTLUniform + AIW. To find out what kind of knowledge is missed in the process of generating the translations, we categorised words by their Partof-Speech tags and named-entities types. We have done this analysis on En→Es language pair as there are accurate annotators for the Spanish language. We use Stanford POS tagger (Toutanova et al., 2003) and named-entity recogniser (Finkel et al., 2005) to annotate Spanish gold translations. Then, we categorised the missed words in the generated translations concerning these tags, and count the number of missed words in each category. Figure 5 depicts the result. As seen in Analysis on how/when auxiliary tasks have been used? This analysis aims to shed light on how AIWs control the contribution of each task through the training. As seen, our method has the best result when it is combined with the Uniform MTL schedule. In this schedule, at each update iteration, we have one mini-batch from each of the tasks, and AIWs are determined for all of"
D19-5618,P18-1064,0,0.262727,"data to achieve reasonable translation quality (Koehn and Knowles, 2017). Recent work has investigated multitask learning (MTL) for injecting inductive biases from auxiliary syntactic and/or se177 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 177–186 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d training to improve the main task the most. achieve competitive performance with existing single-task models of each task, and not necessarily much better performance (Chen et al., 2018; Guo et al., 2018b). On the other hand, biased-MTL focuses on the main task to achieve higher improvements on it. (Zaremoodi and Haffari, 2018) has proposed a fixed training schedule to balance out the importance of the main NMT task vs auxiliary task to improve NMT the most. (Kiperwasser and Ballesteros, 2018) has shown the effectiveness of a changing training schedule through the MTL process. However, their approach is based on hand-engineered heuristics, and should be (re)designed and fine-tuned for every change in tasks or even training data. In this paper, for the first time to the best of our knowledge,"
D19-5618,N19-1355,0,0.0249199,"thod automatically finds a schedule which puts more importance to the auxiliary syntactic tasks at the beginning while gradually it alters the importance toward the auxiliary semantic task. As this method does not rely on hand-engineered heuristics, as a future work, we want to apply it for effective learning of multitask architectures beyond NMT. Training schedules can be fixed/dynamic throughout the training and be handengineered/adaptive. (Zaremoodi and Haffari, 2018) has made use of a fixed hand-engineered schedule for improving low-resource NMT with auxiliary linguistic tasks. Recently, (Guo et al., 2019) has proposed an adaptive way to compute the importance weights of tasks. Instead of manual tuning of importance weights via a large grid search, they model the performance of each set of weights as a sample from a Gaussian Process (GP), and search for optimal values. In fact, their method is not completely adaptive as a strong prior needs to be set for the main task. This method can be seen as a guided yet computationally exhaustive trial-and-error where in each trial, MTL models need to be re-trained (from scratch) with the sampled weights. Moreover, the weight of tasks are fixed throughout"
D19-5618,P17-2054,0,0.0172996,"tasks (Ruder et al., 2017; Zaremoodi et al., 2018); (2) Training schedule: works in this area, including ours, focus on setting the importance of tasks. Related Work Training schedule is the beating heart of MTL, and has a critical role in the performance of the resulted model. Since there are more than one task involved in MTL, the performance is measured differently in different MTL flavours: (1) Multitask learning (Caruana, 1997) has been used for various NLP problems, e.g. machine translation (Dong et al., 2015), dependency parsing (Peng et al., 2017), key-phrase boundary classification (Augenstein and Søgaard, 2017), video 183 1200 14000 1100 12000 1000 10000 900 8000 800 6000 700 600 4000 500 2000 400 MTL-Biased MT (a) Named-Entities s al es Da t Ve rb s ns No un s MTL-Biased Nu m er LUG Ad ve rb s Pr ep os iti on s PERS MTL-Uniform + AIW Pr on ou ORG MT ju nc ive s OTROS Co n Ad je ct 200 tio ns De te rm in er s Pu nc tu at io n 0 300 MTL-Uniform + AIW (b) Part-of-Speech tags Figure 5: The number of words in the gold English→Spanish translation which are missed in the generated translations (lower is better). Missed words are categorised by their tags (Part-of-Speech and named-entity types). for MTL in"
D19-5618,P17-4012,0,0.0376026,"he MTL architecture proposed in (Zaremoodi and Haffari, 2018). We use three stacked LSTM layers in encoders and decoders. For En→Vi and En→Tr, one/two layer(s) are shared among encoders/decoders while for En→Es, two/one layer(s) are shared among encoders/decoders. The LSTM dimensions, batch size and dropout are set to 512, 32 and 0.3, respectively. We use Adam optimiser (Kingma and Ba, 2014) with the learning rate of 0.001. We train models for 25 epochs and save the best model based on the perplexity on the validation (Val) set. We have implemented the methods using PyTorch on top of OpenNMT (Klein et al., 2017). 2 • Constant: Pm (t) = α; When α is set to 0.5, it is similar to the Biased schedule we have seen before. • Exponential: Pm (t) = 1 − e−αt ; In this schedule the probability of selecting the main task increases exponentially throughout the training. 1 ; Similar to the 1 + e−αt previous schedule, the probability of selecting the main task increases, following a sigmoid function. • Sigmoid: Pm (t) = In each of these schedules, the rest of the probability is uniformly divided among the remaining 3 https://catalog.ldc.upenn.edu/LDC2017T10 181 Following their experiments, we set α to 0.5. tasks."
D19-5618,Q18-1017,0,0.312343,"ut the training of NMT: from syntax to semantic. 1 Adaptive-Semantic Adaptive-Syntactic Adaptive-NER Average Weight 0.2 Fixed-schedule 0.15 0.1 0.05 20 0 60 0 10 00 14 00 18 00 22 00 26 00 30 00 34 00 38 00 42 00 46 00 50 00 54 00 58 00 62 00 66 00 70 00 74 00 78 00 82 00 86 00 90 00 94 00 98 00 0 Training iteration Figure 1: The dynamic in the relative importance of named entity recognition, syntactic parsing, and semantic parsing as the auxiliary tasks for the main machine translation task (based on our experiments in §3). The plot shows our proposed adaptive scheduling vs fixed scheduling (Kiperwasser and Ballesteros, 2018) (scaled down for better illustration). mantic tasks into NMT to improve its generalisation (Zaremoodi and Haffari, 2018; Zaremoodi et al., 2018; Kiperwasser and Ballesteros, 2018). The majority of the MTL literature has focused on investigating how to share common knowledge among the tasks through tying their parameters and joint training using standard algorithms. However, a big challenge of MTL is how to get the best signal from the tasks by changing their importance in the training process aka training schedule; see Figure 1. Crucially, a proper training schedule would encourage positive t"
D19-5618,2015.iwslt-evaluation.1,0,0.0650701,"Missing"
D19-5618,2005.mtsummit-papers.11,0,0.086009,"fferent underlying linguistic structures. The structure of Vietnamese and Spanish is generally subject-verb-object (SVO) while Turkish follows subject-object-verb (SOV) structure. Although Spanish is not a low-resource language we have chosen it because of available accurate POS taggers and Named-Entity recognisers required for some of the analyses. For each pair, we use BPE (Sennrich et al., 2016) with 40K types on the union of the source and target vocabularies. We use the Moses toolkit (Koehn et al., 2007) to • English-Spanish: we have used the first 150K training pairs of Europarl corpus (Koehn, 2005). ”newstest2011”, ”newstest2012” and ”newstest2013” parts are used as validation, meta-validation and test set, respectively. 3.2 Auxiliary tasks Following (Zaremoodi and Haffari, 2018), we have chosen following auxiliary tasks to inject the syntactic and semantic knowledge to improve NMT: • Named-Entity Recognition (NER): we use CONLL shared task1 data. This dataset is 1 180 https://www.clips.uantwerpen.be/conll2003/ner MT only MTL with Fixed Schedule + Uniform + Biased (Constant)†‡ + Exponential‡ + Sigmoid‡ MTL with Adaptive Schedule + Biased + AIW + Uniform + AIW En→Vi BLEU Dev Test 22.83 2"
D19-5618,W14-3348,0,0.0213917,"nalysis on how/when auxiliary tasks have been used? This analysis aims to shed light on how AIWs control the contribution of each task through the training. As seen, our method has the best result when it is combined with the Uniform MTL schedule. In this schedule, at each update iteration, we have one mini-batch from each of the tasks, and AIWs are determined for all of the training pairs in these mini-batches. For this analysis, we divided the training into 200 update iteration chunks. In each chunk, we compute the average weights assigned to the training pairs of each task. 4 METEOR score (Denkowski and Lavie, 2014) is reported only for Spanish as it is the only target languages in our experiments which is supported by it. 182 1 0.08 0.9 0.07 0.8 0.06 0.7 0.05 0.6 0.5 0.04 0.4 0.03 0.3 0.02 0.2 0.01 0.1 9000 9400 9800 9800 8600 8200 7800 7400 7000 6600 6200 5800 5400 5000 4600 4200 Syntactic Parsing 9400 (a) Translation task (En→Es) vs. auxiliary tasks. 3800 3400 3000 2600 2200 1800 1400 600 1000 200 9800 9400 9000 8600 8200 7800 7400 6600 6200 5800 7000 Semantic Parsing Translation 9000 Auxiliary tasks 5400 5000 4600 4200 3800 3400 3000 2600 2200 1800 1400 600 1000 0 200 0 Named-Entity Recognition (b) A"
D19-5618,W17-3204,0,0.0451886,"to degradation of generalisation capabilities. Most of the works on training schedule focus on general MTL where the goal is to improve the performance of all tasks. They are based on addressing the imbalance in task difficulties and co-evolve easy and difficult tasks uniformly (performance-wise). These methods Introduction While Neural Machine Translation (NMT) is known for its ability to learn end-to-end without any need for many brittle design choices and hand-engineered features, it is notorious for its demand for large amounts of bilingual data to achieve reasonable translation quality (Koehn and Knowles, 2017). Recent work has investigated multitask learning (MTL) for injecting inductive biases from auxiliary syntactic and/or se177 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 177–186 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d training to improve the main task the most. achieve competitive performance with existing single-task models of each task, and not necessarily much better performance (Chen et al., 2018; Guo et al., 2018b). On the other hand, biased-MTL focuses on the main ta"
D19-5618,P15-1166,0,0.0379301,"arch: (1) Architecture design: works in this area try to learn effective parameter sharing among tasks (Ruder et al., 2017; Zaremoodi et al., 2018); (2) Training schedule: works in this area, including ours, focus on setting the importance of tasks. Related Work Training schedule is the beating heart of MTL, and has a critical role in the performance of the resulted model. Since there are more than one task involved in MTL, the performance is measured differently in different MTL flavours: (1) Multitask learning (Caruana, 1997) has been used for various NLP problems, e.g. machine translation (Dong et al., 2015), dependency parsing (Peng et al., 2017), key-phrase boundary classification (Augenstein and Søgaard, 2017), video 183 1200 14000 1100 12000 1000 10000 900 8000 800 6000 700 600 4000 500 2000 400 MTL-Biased MT (a) Named-Entities s al es Da t Ve rb s ns No un s MTL-Biased Nu m er LUG Ad ve rb s Pr ep os iti on s PERS MTL-Uniform + AIW Pr on ou ORG MT ju nc ive s OTROS Co n Ad je ct 200 tio ns De te rm in er s Pu nc tu at io n 0 300 MTL-Uniform + AIW (b) Part-of-Speech tags Figure 5: The number of words in the gold English→Spanish translation which are missed in the generated translations (lower"
D19-5618,P17-1014,0,0.0234119,"e articles from the Reuters Corpus. • Syntactic Parsing: we use Penn TreeBank parsing with the standard split (Marcheggiani and Titov, 2017). This task is casted to S EQ 2S EQ transduction by linearising constituency trees (Vinyals et al., 2015) • Uniform: Selects a random mini-batch from all of the tasks; • Biased (Zaremoodi and Haffari, 2018): Selects a random mini-batch from the translation task (bias towards the main task) and another one for a randomly selected task. • Semantic Parsing: we use Abstract Meaning Representation (AMR) corpus Release 2.02 linearised by the method proposed in (Konstas et al., 2017). This corpus is gathered from from newswire, weblogs, web discussion forums and broadcast conversations. 3.3 We also use schedules proposed in (Kiperwasser and Ballesteros, 2018). They consider a slope parameter3 α and the fraction of training epochs done so far t = sents/||corpus||. The schedules determine the probability of selecting each of the tasks as the source of the next training pair. In each of these schedules the probability of selecting the main task is: MTL architecture and training schedule Since partial-sharing has been shown to be more effective than full sharing (Liu et al.,"
D19-5618,P18-2104,1,0.908926,"0 10 00 14 00 18 00 22 00 26 00 30 00 34 00 38 00 42 00 46 00 50 00 54 00 58 00 62 00 66 00 70 00 74 00 78 00 82 00 86 00 90 00 94 00 98 00 0 Training iteration Figure 1: The dynamic in the relative importance of named entity recognition, syntactic parsing, and semantic parsing as the auxiliary tasks for the main machine translation task (based on our experiments in §3). The plot shows our proposed adaptive scheduling vs fixed scheduling (Kiperwasser and Ballesteros, 2018) (scaled down for better illustration). mantic tasks into NMT to improve its generalisation (Zaremoodi and Haffari, 2018; Zaremoodi et al., 2018; Kiperwasser and Ballesteros, 2018). The majority of the MTL literature has focused on investigating how to share common knowledge among the tasks through tying their parameters and joint training using standard algorithms. However, a big challenge of MTL is how to get the best signal from the tasks by changing their importance in the training process aka training schedule; see Figure 1. Crucially, a proper training schedule would encourage positive transfer and prevent negative transfer, as the inductive biases of the auxiliary tasks may interfere with those of the main task leading to degra"
D19-5618,P17-1001,0,0.162277,"t al., 2017). This corpus is gathered from from newswire, weblogs, web discussion forums and broadcast conversations. 3.3 We also use schedules proposed in (Kiperwasser and Ballesteros, 2018). They consider a slope parameter3 α and the fraction of training epochs done so far t = sents/||corpus||. The schedules determine the probability of selecting each of the tasks as the source of the next training pair. In each of these schedules the probability of selecting the main task is: MTL architecture and training schedule Since partial-sharing has been shown to be more effective than full sharing (Liu et al., 2017; Guo et al., 2018a; Zaremoodi and Haffari, 2018), we use the MTL architecture proposed in (Zaremoodi and Haffari, 2018). We use three stacked LSTM layers in encoders and decoders. For En→Vi and En→Tr, one/two layer(s) are shared among encoders/decoders while for En→Es, two/one layer(s) are shared among encoders/decoders. The LSTM dimensions, batch size and dropout are set to 512, 32 and 0.3, respectively. We use Adam optimiser (Kingma and Ba, 2014) with the learning rate of 0.001. We train models for 25 epochs and save the best model based on the perplexity on the validation (Val) set. We hav"
D19-5618,N18-1123,1,0.880857,"Missing"
D19-5618,2015.iwslt-evaluation.11,0,0.114694,"Missing"
D19-5618,D17-1159,0,0.0376524,"Missing"
D19-5618,W17-4708,0,0.0215392,"200 3800 3400 3000 2600 2200 1800 1400 1000 0 600 0.01 0 200 0.1 600 0.02 0.2 Named-Entity Recognition (d) Auxiliary tasks vs. each other. Figure 4: Weights assigned to the training pairs of different tasks (averaged over 200 update iteration chunks). Y-axis shows the average weight and X-axis shows the number of update iteration. In the top figures, the main translation task is English→Spanish while in the bottom ones it is English→Turkish. captioning (Pasunuru and Bansal, 2017), Chinese word segmentation, and text classification problem (Liu et al., 2017). For the case of low-resource NMT, (Niehues and Cho, 2017) has explored the use of part-of-speech and named-entity recognition in improving NMT. (Kiperwasser and Ballesteros, 2018) has investigated part-of-speech tagging and dependency parsing tasks, and (Zaremoodi et al., 2018; Zaremoodi and Haffari, 2018) have tried syntactic parsing, semantic parsing, and named-entity recognition tasks. Figure 5a, the knowledge learned from auxiliary tasks helps the MTL model to miss less number of named-entities during translation. Moreover, AIWs help the MTL model further by making better use of the knowledge conveyed in the auxiliary tasks. We can see the same"
D19-5618,P17-1117,0,0.0234435,"0 5800 5400 5000 4600 4200 3800 3400 3000 2600 2200 1800 1400 1000 200 9800 9400 9000 8600 8200 7800 7400 7000 6600 6200 5800 5400 5000 4600 4200 3800 3400 3000 2600 2200 1800 1400 1000 0 600 0.01 0 200 0.1 600 0.02 0.2 Named-Entity Recognition (d) Auxiliary tasks vs. each other. Figure 4: Weights assigned to the training pairs of different tasks (averaged over 200 update iteration chunks). Y-axis shows the average weight and X-axis shows the number of update iteration. In the top figures, the main translation task is English→Spanish while in the bottom ones it is English→Turkish. captioning (Pasunuru and Bansal, 2017), Chinese word segmentation, and text classification problem (Liu et al., 2017). For the case of low-resource NMT, (Niehues and Cho, 2017) has explored the use of part-of-speech and named-entity recognition in improving NMT. (Kiperwasser and Ballesteros, 2018) has investigated part-of-speech tagging and dependency parsing tasks, and (Zaremoodi et al., 2018; Zaremoodi and Haffari, 2018) have tried syntactic parsing, semantic parsing, and named-entity recognition tasks. Figure 5a, the knowledge learned from auxiliary tasks helps the MTL model to miss less number of named-entities during translat"
D19-5618,P17-1186,0,0.0398771,"Missing"
D19-5618,P16-1162,0,0.0739334,"ts Bilingual Corpora We use three language-pairs, translating from English to Vietnamese (Vi), Turkish (Tr) and Spanish (Es). We have chosen them to analyse the effect of adaptive mini-batch weighting on languages with different underlying linguistic structures. The structure of Vietnamese and Spanish is generally subject-verb-object (SVO) while Turkish follows subject-object-verb (SOV) structure. Although Spanish is not a low-resource language we have chosen it because of available accurate POS taggers and Named-Entity recognisers required for some of the analyses. For each pair, we use BPE (Sennrich et al., 2016) with 40K types on the union of the source and target vocabularies. We use the Moses toolkit (Koehn et al., 2007) to • English-Spanish: we have used the first 150K training pairs of Europarl corpus (Koehn, 2005). ”newstest2011”, ”newstest2012” and ”newstest2013” parts are used as validation, meta-validation and test set, respectively. 3.2 Auxiliary tasks Following (Zaremoodi and Haffari, 2018), we have chosen following auxiliary tasks to inject the syntactic and semantic knowledge to improve NMT: • Named-Entity Recognition (NER): we use CONLL shared task1 data. This dataset is 1 180 https://ww"
D19-5618,N03-1033,0,0.0178426,"ns and see how the proposed method improved the quality of the translations. More specifically, we want to compare the number of words in the gold translations which are missed in the generated translations produced by the following systems: (i) MT only; (ii) MTL-Biased; (iii) MTLUniform + AIW. To find out what kind of knowledge is missed in the process of generating the translations, we categorised words by their Partof-Speech tags and named-entities types. We have done this analysis on En→Es language pair as there are accurate annotators for the Spanish language. We use Stanford POS tagger (Toutanova et al., 2003) and named-entity recogniser (Finkel et al., 2005) to annotate Spanish gold translations. Then, we categorised the missed words in the generated translations concerning these tags, and count the number of missed words in each category. Figure 5 depicts the result. As seen in Analysis on how/when auxiliary tasks have been used? This analysis aims to shed light on how AIWs control the contribution of each task through the training. As seen, our method has the best result when it is combined with the Uniform MTL schedule. In this schedule, at each update iteration, we have one mini-batch from eac"
D19-5618,N03-1000,0,0.167448,"Missing"
D19-5628,P16-1162,0,0.0461506,"a soft or sparse attention over the words in the sentences. For more details of how the documentlevel context representations are computed, we refer the reader to the original paper (Maruf et al., 2019). Table 1: Sentence-parallel training corpora statistics. From all corpora, we also remove sentences with length greater than 75 tokens after tokenisation.4 ,5 Table 1 summarises the number of sentences of each corpus in the pre-processed sentence-parallel dataset. We further apply truecasing using Moses (Koehn et al., 2007) followed by joint byte-pair encoding (BPE) with 50K merge operations (Sennrich et al., 2016). 2.2 Model and Training We use the DyNet toolkit (Neubig et al., 2017) for all of our experiments; the implementation of the sentence-level system is in DyNet, namely Transformer-DyNet.6 Our experiments are run on a single V100 GPU, so we use a rather small minibatch size of 900 tokens. Furthermore, we have filtered sentences with length greater than 85 tokens to fit the computation graph in GPU memory. The hyper-parameter settings are the same as in the Transformer-base model except the number of layers which is set to 4. We also employ all four types of dropouts as in the original Transform"
D19-5628,P11-2031,0,0.0319005,"each sentence is updated using the document-context NMT model while fixing the translations of the other sentences. This is what we refer to as two-pass iterative decoding • Ensemble-Avg. Apart from combining the probability distributions at the softmax level, we also average the context representations from each run, i.e., we use the same initial context representations for the different runs of a document-level model (Figure 2). For evaluation, BLEU (Papineni et al., 2002) is reported on the detruecased translations (with original tokenisation) and is calculated using the MultEval toolkit (Clark et al., 2011). 9 input dropout - dropout applied to the sum of token embeddings and position encodings, residual dropout - dropout applied to the output of each sublayer before adding to the sublayer input, relu dropout - dropout applied to the inner layer output after ReLU activation in each feed-forward sublayer, and attentiondropout - dropout applied to attention weight in each attention sublayer 4.2 English→German It can be seen from Table 2 that for all runs, the document-level models outperform the sentencelevel baseline trained with 4 times the data. The 258 Integration into Encoder Integration into"
D19-5628,Q18-1029,0,0.0235705,"isual material”, etc. 1 http://www.statmt.org/wmt19/ translation-task.html 256 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 256–261 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d Corpus Europarl v9 Common Crawl News Commentary v14 Rapid Rotowire #Sentence-Pairs 1.79M 2.37M 0.33M 1.46M 3247 the document-level context representation is combined with the deep representation of either the source or target word (output from the last layer of the Transformer) using a gating mechanism (Tu et al., 2018). The document-level context representation is itself computed in two ways: (i) a single-level flat attention over all sentences in the documentcontext, or (ii) a hierarchical attention which has the ability to identify the key sentences in the document-context and then attend to the key words within those sentences. For the former, we use a soft attention over the sentences, while for the latter we use a sparse attention over the sentences and a soft or sparse attention over the words in the sentences. For more details of how the documentlevel context representations are computed, we refer th"
D19-5628,P18-1118,1,0.835453,"of the sentences from the shared task corpus. 5 Tokenisation script was provided by WNGT organisers. 6 https://github.com/duyvuleo/ Transformer-DyNet 7 Europarl v9 also had document boundaries but these resulted in very long documents and thus we decided against using it for the document-level training. 8 The training corpus used for training the document-level models is a subset of the training corpus used for training the baseline sentence-level model. 257 Figure 1: Ensemble decoding. Figure 2: Ensemble-Avg decoding. uments comprising 1.21M sentences for training our document-level models. (Maruf and Haffari, 2018). It should also be mentioned that since decoding is a computationally expensive process, we perform greedy decoding in both passes. 3.3 Training We use a stage-wise method to train the variants of the document-context NMT model. We pre-train the sentence-level model described in the previous section and then use it to compute the monolingual and the bilingual context representations. These are then used to compute the documentlevel context representation in our models. The pre-trained sentence-level model is also used to initialise our document-level model and is further fine-tuned alongwith"
D19-5628,N19-1313,1,0.919878,"rkshop on Neural Generation and Translation (WNGT 2019). Despite the boom of work on document-level machine translation in the past two years, we have witnessed a lack of the application of the proposed approaches to MT shared tasks. Thus, our main focus in this work is on employing an established document-level neural machine translation model for this task. We first explore a strong sentence-level baseline, trained on large-scale parallel data made available by WMT 2019 for their news translation task.1 We use this system as the initialisation of the document-level models, first proposed by Maruf et al. (2019), making use of the complete document (both past and future sentences) as the conditioning context when translating a sentence. Given the task of translating Rotowire basketball articles, we leverage the document-delimited data 2.1 Data Preparation To train our sentence-level model, we want to use the maximum allowable high-quality data from the English-German news task in WMT 2019. This would produce a fair baseline for comparing with our document-level models. Upon considering the task of translating basketball-related articles, we have decided to utilise parallel data from Europarl v9, Comm"
D19-5628,P02-1040,0,0.106438,"by generating initial translations from the sentence-level NMT model. This is followed by a second pass of decoding, where the translation for each sentence is updated using the document-context NMT model while fixing the translations of the other sentences. This is what we refer to as two-pass iterative decoding • Ensemble-Avg. Apart from combining the probability distributions at the softmax level, we also average the context representations from each run, i.e., we use the same initial context representations for the different runs of a document-level model (Figure 2). For evaluation, BLEU (Papineni et al., 2002) is reported on the detruecased translations (with original tokenisation) and is calculated using the MultEval toolkit (Clark et al., 2011). 9 input dropout - dropout applied to the sum of token embeddings and position encodings, residual dropout - dropout applied to the output of each sublayer before adding to the sublayer input, relu dropout - dropout applied to the inner layer output after ReLU activation in each feed-forward sublayer, and attentiondropout - dropout applied to attention weight in each attention sublayer 4.2 English→German It can be seen from Table 2 that for all runs, the d"
E17-1039,P05-1045,0,0.292168,"significantly fewer queries compared to the standard benchmarking technique to identify the best system according to Fmeasure. 1 Introduction F-measureAs new NLP systems are (continually) introduced for a task of interest, such as named entity recognition (NER), it is crucial for practioneers and researchers to select the best system. These systems may be designed based on different models and/or learning algorithms. For instance, due to recent advancement in NER research, several NER systems have been proposed and then supported in APIs such as OpenNLP (Ingersoll et al., 2013), Stanford NER (Finkel et al., 2005), ANNIE (Cunningham et al., 2002) and Meaning Cloud (MeaningCloud-LLC, 1998) to name a few. 408 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 408–416, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics we present Thompson Sampling (TS) and one of its variants, called Pure Exploration TS (PETS), designed specifically for the best arm selection. wait for a final answer at the end of an experiment. As such, we need to model the uncertainty regarding the estimated F-measure o"
E17-1039,ide-etal-2008-masc,0,0.0184125,"d data DF , i.e. the data annotated at both the sentence and report level. 4.4 Named Entity Recognition • Mpartial : where the model is trained on both DF and the partially annotated data DP in which the sentence level annotation is missing but the reports are labeled. In our second set of experiment, we attempt to see how our frameworks and F1 models perform using realistic data. • Munlab : where the model is trained on DF and DU in which the annotation is missing at both sentence and report level. MASC Corpus. For benchmarking the NER systems, we use the Manually Annotated SubCorpus (MASC) (Ide et al., 2008) that includes 19 different domains. The corpus consists of approximately 500K words of contemporary American English written and spoken data drawn from the • Mall : where the model is trained on all of the available data described above. 414 Open American National Corpus (OANC). This corpus includes a wide variety of linguistic annotations with a balanced selection of texts from a broad range of genres/domains. The diversity of the corpus will enable us to assess the robustness of tools across different domains. The number of documents in the MASC corpus is about 392. Baseline2 BaselineK−1 Hi"
E17-1039,P07-1055,0,0.123889,"Missing"
E17-1041,W13-3214,0,0.475914,"Missing"
E17-1041,D15-1166,0,0.0636385,"Missing"
E17-1041,J93-2003,0,0.0656162,"This model is combined with an attention mechanism that focuses on salient tokens in utterances. Our experimental results show that our model outperforms strong baselines on two popular datasets, Switchboard and MapTask; and our detailed empirical analysis highlights the impact of each aspect of our model. 1 Introduction The sequence-labeling task involves learning a model that maps an input sequence to an output sequence. Many NLP problems can be treated as sequence-labeling tasks, e.g., part-of-speech (PoS) tagging (Toutanova et al., 2003; Toutanova and Manning, 2000), machine translation (Brown et al., 1993) and automatic speech recognition (Gales and Young, 2008). Recurrent Neural Nets (RNNs) have been the workhorse model for many NLP sequence-labeling tasks, e.g., machine translation (Sutskever et al., 2014) and speech recognition (Amodei et al., 2015), due to their ability to capture long-range dependencies inherent in natural language. In this paper, we propose a hierarchical RNN for labeling a sequence of utterances (i.e., contributions) in a dialogue with their Dialogue Acts 428 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volu"
E17-1041,J00-3003,0,0.931309,"Missing"
E17-1041,W00-1308,0,0.05118,"Missing"
E17-1041,N03-1033,0,0.0463466,"ture long-range dependencies at the dialogue level and the utterance level. This model is combined with an attention mechanism that focuses on salient tokens in utterances. Our experimental results show that our model outperforms strong baselines on two popular datasets, Switchboard and MapTask; and our detailed empirical analysis highlights the impact of each aspect of our model. 1 Introduction The sequence-labeling task involves learning a model that maps an input sequence to an output sequence. Many NLP problems can be treated as sequence-labeling tasks, e.g., part-of-speech (PoS) tagging (Toutanova et al., 2003; Toutanova and Manning, 2000), machine translation (Brown et al., 1993) and automatic speech recognition (Gales and Young, 2008). Recurrent Neural Nets (RNNs) have been the workhorse model for many NLP sequence-labeling tasks, e.g., machine translation (Sutskever et al., 2014) and speech recognition (Amodei et al., 2015), due to their ability to capture long-range dependencies inherent in natural language. In this paper, we propose a hierarchical RNN for labeling a sequence of utterances (i.e., contributions) in a dialogue with their Dialogue Acts 428 Proceedings of the 15th Conference of the"
E17-1041,C10-2150,0,0.0405562,"Missing"
I13-1050,J04-4002,0,0.0803904,"Canada anoop@cs.sfu.ca X → hβp Xk βs , γp Xk γs i (1) Here βp (βs ) refers to any prefix (suffix) of β that precedes (follows) fij . Note that the non-terminals are co-indexed with a unique index so that they are rewritten simultaneously. 数 月 months Introduction , 联合国 , 难民 the 专员 公署 unhcr Figure 1: Chinese-English phrase-pair with alignments Hierarchical phrase-based translation (Hiero) as described in (Chiang, 2005; Chiang, 2007) uses a synchronous context-free grammar (SCFG) derived from heuristically extracted phrase pairs obtained by symmetrizing bidirectional many-tomany word alignments (Och and Ney, 2004). The phrase-pairs are constrained by the source-target alignments such that all the alignment links from the source (target) words are connected to the target (source) words within the phrase. Given a word-aligned sentence pair hf1J , eI1 , Ai, where A indicate the alignments, the source-target se0 quence pair hfij , eji0 i can be a phrase-pair iff the following alignment constraint is satisfied. As a concrete example, consider the word aligned Chinese-English phrase pair shown in Figure 1. Notice that the phrase 联合国 (united nations) is incorrectly aligned to English determiner the, even thou"
I13-1050,W99-0604,0,0.143945,"tistics and then communicates the statistics to a central aggregator reduce node. Distributed inference for Expectation Maximization algorithm was studied in (Wolfe et al., 2008). They used three different topologies in 4 Experiments Training Corpus URochester data ISI Ar-En corpus HK + GALE ph-1 Train/ Tune/ Test 59218/ 1118/ 1118 1.1 M/ 1982/ 987 2.3 M/ 1928/ 919 Table 1: Corpus Statistics in # of sentences We follow the standard MT practice and use GIZA++ (Och and Ney, 2003) for word aligning the parallel corpus. We then use the heuristic step that symmetrizes the bidirectional alignments (Och et al., 1999) to extract the initial phrase-pairs up to a certain length, consistent with the word alignments. Finally we employ our proposed Variational-Bayes training to learn rules for 5 We simulate the Map-Reduce style of computation using a regular high-performance cluster using a mounted filesystem rather than a Hadoop cluster with a distributed filesystem. In our experiments, we set the number of iterations to 10. 441 Model Baseline Variational-Bayes Ko-En 7.18 7.68 Ar-En 37.82 37.76 Cn-En 28.58 28.40 a simple threshold pruning strategy on the grammar learned from our proposed model. Table 3 shows t"
I13-1050,P09-1088,0,0.0771872,"BLEU BLEU Model size (in Millions) Terminal 23.3 27.9 38.4 27.2 27 26.8 VB-‐Pr (1.0) Figure 6: Cn-En: Model sizes and B LEU for different grammars. The pruned models are identified by the suffix ’Pr’, whose mincount is shown in the brackets. The y-axis on the left marks the model sizes and that on the right denotes B LEU. The numbers in the stacked bars denote the # of rules (in millions) for the corresponding rule type. 444 Most of the research on learning Hiero SCFG rules has been focussed on inducing phrasal alignments between source and target using Bayesian models (Blunsom et al., 2008; Blunsom et al., 2009; Levenberg et al., 2012; Cohn and Haffari, 2013). Broadly speaking, these generative approaches learn a posterior over parallel tree structures on the sentence pairs. While these methods extract hierarchical rules, they do not conform to Hierostyle rules. Consequently the hierarchical rules are used only for learning an alignment model and cannot be used directly in the Hiero decoder. Instead, these approaches employ the standard Hiero heuristics to extract rules to be used by the decoder from the alignments predicted by their model. In this sense, these are similar to Bayesian models for lea"
I13-1050,N10-1050,0,0.0475371,"Missing"
I13-1050,P05-1033,0,0.0912779,"bic and Chinese into English demonstrate that they are able to exceed or retain the performance of baseline hierarchical phrase-based models. 1 Anoop Sarkar Simon Fraser University Burnaby BC. Canada anoop@cs.sfu.ca X → hβp Xk βs , γp Xk γs i (1) Here βp (βs ) refers to any prefix (suffix) of β that precedes (follows) fij . Note that the non-terminals are co-indexed with a unique index so that they are rewritten simultaneously. 数 月 months Introduction , 联合国 , 难民 the 专员 公署 unhcr Figure 1: Chinese-English phrase-pair with alignments Hierarchical phrase-based translation (Hiero) as described in (Chiang, 2005; Chiang, 2007) uses a synchronous context-free grammar (SCFG) derived from heuristically extracted phrase pairs obtained by symmetrizing bidirectional many-tomany word alignments (Och and Ney, 2004). The phrase-pairs are constrained by the source-target alignments such that all the alignment links from the source (target) words are connected to the target (source) words within the phrase. Given a word-aligned sentence pair hf1J , eI1 , Ai, where A indicate the alignments, the source-target se0 quence pair hfij , eji0 i can be a phrase-pair iff the following alignment constraint is satisfied."
I13-1050,W11-2167,1,0.850112,"Rochester KoreanEnglish dataset consisting of almost 60K sentence pairs for the small data setting. For moderate and large datasets we use Arabic-English (ISI parallel corpus) and Chinese-English (Hong Kong parallel text and GALE phase-1) corpora. We use the MTC dataset having 4 references for tuning and testing for our Chinese-English experiments. The statistics of the corpora used in our experiments are summarized in Table 1. We run inference for a fixed number of iterations4 and use the grammar along with their posterior counts from the last iteration for the translation table. Following (Sankaran et al., 2011), we use the shift-reduce style algorithm to efficiently encode the word aligned phrase-pair as a normalized decomposition tree (Zhang et al., 2008). The possible derivations (that are consistent with the word alignments) could then be enumerated by simply traversing every node in the decomposition tree and replacing its span by a non-terminal X. 3.1 Lang. Ko-En Ar-En Cn-En Distributing Inference While the above training procedure works well for smaller datasets, it does not scale well for the realistic MT datasets (which have millions of sentence pairs) due to greater memory and time requirem"
I13-1050,J07-2003,0,0.869994,"y co-indexed non-terminals and rewriting the replaced source-target word sequences as separate rules. Consider a rule X → hβ, γi, where β and γ are sequences of terminals and non-terminals. Now, given another rule 0 0 X → hfij , eji0 i, such that fij and eji0 are contained fully within β and γ as sub-phrases, the larger rule could be rewritten to create a new rule. We present a Variational-Bayes model for learning rules for the Hierarchical phrasebased model directly from the phrasal alignments. Our model is an alternative to heuristic rule extraction in hierarchical phrase-based translation (Chiang, 2007), which uniformly distributes the probability mass to the extracted rules locally. In contrast, in our approach the probability assigned to a rule is globally determined by its contribution towards all phrase pairs and results in a sparser rule set. We also propose a distributed framework for efficiently running inference for realistic MT corpora. Our experiments translating Korean, Arabic and Chinese into English demonstrate that they are able to exceed or retain the performance of baseline hierarchical phrase-based models. 1 Anoop Sarkar Simon Fraser University Burnaby BC. Canada anoop@cs.sf"
I13-1050,2012.amta-papers.16,1,0.721088,"Missing"
I13-1050,P13-1077,1,0.834378,".3 27.9 38.4 27.2 27 26.8 VB-‐Pr (1.0) Figure 6: Cn-En: Model sizes and B LEU for different grammars. The pruned models are identified by the suffix ’Pr’, whose mincount is shown in the brackets. The y-axis on the left marks the model sizes and that on the right denotes B LEU. The numbers in the stacked bars denote the # of rules (in millions) for the corresponding rule type. 444 Most of the research on learning Hiero SCFG rules has been focussed on inducing phrasal alignments between source and target using Bayesian models (Blunsom et al., 2008; Blunsom et al., 2009; Levenberg et al., 2012; Cohn and Haffari, 2013). Broadly speaking, these generative approaches learn a posterior over parallel tree structures on the sentence pairs. While these methods extract hierarchical rules, they do not conform to Hierostyle rules. Consequently the hierarchical rules are used only for learning an alignment model and cannot be used directly in the Hiero decoder. Instead, these approaches employ the standard Hiero heuristics to extract rules to be used by the decoder from the alignments predicted by their model. In this sense, these are similar to Bayesian models for learning alignments using stochastic Inversion trans"
I13-1050,P06-1121,0,0.191541,"Missing"
I13-1050,P09-1037,0,0.0302037,"Missing"
I13-1050,E09-1044,0,0.0434264,"Missing"
I13-1050,J97-3002,0,0.266872,"Missing"
I13-1050,D07-1103,0,0.0891106,"Missing"
I13-1050,P09-2060,0,0.0460073,"Missing"
I13-1050,D12-1021,0,0.1033,"in Millions) Terminal 23.3 27.9 38.4 27.2 27 26.8 VB-‐Pr (1.0) Figure 6: Cn-En: Model sizes and B LEU for different grammars. The pruned models are identified by the suffix ’Pr’, whose mincount is shown in the brackets. The y-axis on the left marks the model sizes and that on the right denotes B LEU. The numbers in the stacked bars denote the # of rules (in millions) for the corresponding rule type. 444 Most of the research on learning Hiero SCFG rules has been focussed on inducing phrasal alignments between source and target using Bayesian models (Blunsom et al., 2008; Blunsom et al., 2009; Levenberg et al., 2012; Cohn and Haffari, 2013). Broadly speaking, these generative approaches learn a posterior over parallel tree structures on the sentence pairs. While these methods extract hierarchical rules, they do not conform to Hierostyle rules. Consequently the hierarchical rules are used only for learning an alignment model and cannot be used directly in the Hiero decoder. Instead, these approaches employ the standard Hiero heuristics to extract rules to be used by the decoder from the alignments predicted by their model. In this sense, these are similar to Bayesian models for learning alignments using s"
I13-1050,C08-1136,0,0.0202451,"ish (ISI parallel corpus) and Chinese-English (Hong Kong parallel text and GALE phase-1) corpora. We use the MTC dataset having 4 references for tuning and testing for our Chinese-English experiments. The statistics of the corpora used in our experiments are summarized in Table 1. We run inference for a fixed number of iterations4 and use the grammar along with their posterior counts from the last iteration for the translation table. Following (Sankaran et al., 2011), we use the shift-reduce style algorithm to efficiently encode the word aligned phrase-pair as a normalized decomposition tree (Zhang et al., 2008). The possible derivations (that are consistent with the word alignments) could then be enumerated by simply traversing every node in the decomposition tree and replacing its span by a non-terminal X. 3.1 Lang. Ko-En Ar-En Cn-En Distributing Inference While the above training procedure works well for smaller datasets, it does not scale well for the realistic MT datasets (which have millions of sentence pairs) due to greater memory and time requirements. To address this shortcoming, we distribute the training using a Map-Reduce style framework, where each node works on the local dataset in comp"
I13-1050,C08-1144,0,0.0496567,"Missing"
I13-1050,J03-1002,0,0.0106887,"we distribute the training using a Map-Reduce style framework, where each node works on the local dataset in computing the required statistics and then communicates the statistics to a central aggregator reduce node. Distributed inference for Expectation Maximization algorithm was studied in (Wolfe et al., 2008). They used three different topologies in 4 Experiments Training Corpus URochester data ISI Ar-En corpus HK + GALE ph-1 Train/ Tune/ Test 59218/ 1118/ 1118 1.1 M/ 1982/ 987 2.3 M/ 1928/ 919 Table 1: Corpus Statistics in # of sentences We follow the standard MT practice and use GIZA++ (Och and Ney, 2003) for word aligning the parallel corpus. We then use the heuristic step that symmetrizes the bidirectional alignments (Och et al., 1999) to extract the initial phrase-pairs up to a certain length, consistent with the word alignments. Finally we employ our proposed Variational-Bayes training to learn rules for 5 We simulate the Map-Reduce style of computation using a regular high-performance cluster using a mounted filesystem rather than a Hadoop cluster with a distributed filesystem. In our experiments, we set the number of iterations to 10. 441 Model Baseline Variational-Bayes Ko-En 7.18 7.68"
K18-1033,P09-1021,1,0.942512,"ry their translation from an oracle up to the annotation budget. The queried sentences need to be selected carefully to get the value for the budget, i.e. get the highest improvements in the translation quality of the retrained model. The AL approach is orthogonal to the aforementioned approaches to bilingually lowresource NMT, and can be potentially combined with them. We present a framework to learn the sentence selection policy most suitable and effective for the NMT task at hand. This is in contrast to the majority of work in AL-MT where hard-coded heuristics are used for query selection (Haffari and Sarkar, 2009; Bloodgood and Callison-Burch, 2010). More concretely, we learn the query policy based on a high-resource language-pair sharing similar characteristics with the low-resource language-pair of interest. After trained, the policy is applied to the language-pair of interest capitalising on the learned signals for effective query selection. We make use of imitation learning (IL) to train the query policy. Previous work has shown that the IL approach leads to more effective policy learning (Liu et al., 2018), compared to reinforcement learning (RL) (Fang et al., 2017) . Our proposed method effectiv"
K18-1033,W18-2703,1,0.834814,"l and extremely small data conditions. Expoliting monolingual data for nmt Monolingual data play a key role in neural machine translation systems, previous work have considered training a seperate language model on the target side (Jean et al., 2014; Gulcehre et al., 2015; Domhan and Hieber, 2017). Rather than using explicit language model, Cheng et al. (2016) introduced an auto-encoder-based approach, in which the source-to-target and target-to-source translation models act as encoder and decoder respectively. Moreover, back translation approaches (Sennrich et al., 2015a; Zhang et al., 2018; Hoang et al., 2018) show efficient use of monolingual data to improve neural machine translation. Dual learning (He et al., 2016) extends back translation by using a deep RL approach. More recently, unsupervised approaches (Lample et al., 2017b; Artetxe et al., 2017) and phrase-based NMT (Lample et al., 2018) learn how to translate when having access to only a large amount of monolingual corpora, these models also extend the use of back translation and cross-lingual word embeddings are provided as the latent semantic space for sentences from monolingual corpora in different languages. Acknowledgments We would li"
K18-1033,P10-1088,0,0.0290999,"an oracle up to the annotation budget. The queried sentences need to be selected carefully to get the value for the budget, i.e. get the highest improvements in the translation quality of the retrained model. The AL approach is orthogonal to the aforementioned approaches to bilingually lowresource NMT, and can be potentially combined with them. We present a framework to learn the sentence selection policy most suitable and effective for the NMT task at hand. This is in contrast to the majority of work in AL-MT where hard-coded heuristics are used for query selection (Haffari and Sarkar, 2009; Bloodgood and Callison-Burch, 2010). More concretely, we learn the query policy based on a high-resource language-pair sharing similar characteristics with the low-resource language-pair of interest. After trained, the policy is applied to the language-pair of interest capitalising on the learned signals for effective query selection. We make use of imitation learning (IL) to train the query policy. Previous work has shown that the IL approach leads to more effective policy learning (Liu et al., 2018), compared to reinforcement learning (RL) (Fang et al., 2017) . Our proposed method effectively trains AL policies for batch quer"
K18-1033,P16-1185,0,0.0254712,"where the policies are transferred across languages using multilingual word embeddings. Our experiments confirms that our method is more effective than strong heuristic-based methods in various conditions, including cold-start and warm-start as well as small and extremely small data conditions. Expoliting monolingual data for nmt Monolingual data play a key role in neural machine translation systems, previous work have considered training a seperate language model on the target side (Jean et al., 2014; Gulcehre et al., 2015; Domhan and Hieber, 2017). Rather than using explicit language model, Cheng et al. (2016) introduced an auto-encoder-based approach, in which the source-to-target and target-to-source translation models act as encoder and decoder respectively. Moreover, back translation approaches (Sennrich et al., 2015a; Zhang et al., 2018; Hoang et al., 2018) show efficient use of monolingual data to improve neural machine translation. Dual learning (He et al., 2016) extends back translation by using a deep RL approach. More recently, unsupervised approaches (Lample et al., 2017b; Artetxe et al., 2017) and phrase-based NMT (Lample et al., 2018) learn how to translate when having access to only a"
K18-1033,W17-3204,0,0.0380925,"pitalizes on the shared characteristics between the language pairs to make an effective use of the AL budget. Our experiments on three language-pairs confirms that our method is more effective than strong heuristic-based methods in various conditions, including cold-start and warm-start as well as small and extremely small data conditions. 1 Introduction Parallel training bitext plays a key role in the quality neural machine translation (NMT). Learning high-quality NMT models in bilingually lowresource scenarios is one of the key challenges, as NMT’s quality degrades severely in such setting (Koehn and Knowles, 2017). Recently, the importance of learning NMT models in scarce parallel bitext scenarios has gained attention. Unsupervised approaches try to learn NMT models without the need for parallel bitext (Artetxe et al., 2017; Lample et al., 2017a). Dual learning/backtranslation tries to start off from a small amount of bilingual text, and leverage monolingual text in the source and target language (Sennrich et al., 2015a; He et al., 2016). Zero/few shot approach attempts to transfer NMT learned from rich bilingual settings to low-resource settings (Johnson et al., 2016; Gu et al., 2018). 334 Proceedings"
K18-1033,D17-1158,0,0.0187888,"licy. We have provided experimental results on three language pairs, where the policies are transferred across languages using multilingual word embeddings. Our experiments confirms that our method is more effective than strong heuristic-based methods in various conditions, including cold-start and warm-start as well as small and extremely small data conditions. Expoliting monolingual data for nmt Monolingual data play a key role in neural machine translation systems, previous work have considered training a seperate language model on the target side (Jean et al., 2014; Gulcehre et al., 2015; Domhan and Hieber, 2017). Rather than using explicit language model, Cheng et al. (2016) introduced an auto-encoder-based approach, in which the source-to-target and target-to-source translation models act as encoder and decoder respectively. Moreover, back translation approaches (Sennrich et al., 2015a; Zhang et al., 2018; Hoang et al., 2018) show efficient use of monolingual data to improve neural machine translation. Dual learning (He et al., 2016) extends back translation by using a deep RL approach. More recently, unsupervised approaches (Lample et al., 2017b; Artetxe et al., 2017) and phrase-based NMT (Lample e"
K18-1033,D17-1063,0,0.40844,"sed for query selection (Haffari and Sarkar, 2009; Bloodgood and Callison-Burch, 2010). More concretely, we learn the query policy based on a high-resource language-pair sharing similar characteristics with the low-resource language-pair of interest. After trained, the policy is applied to the language-pair of interest capitalising on the learned signals for effective query selection. We make use of imitation learning (IL) to train the query policy. Previous work has shown that the IL approach leads to more effective policy learning (Liu et al., 2018), compared to reinforcement learning (RL) (Fang et al., 2017) . Our proposed method effectively trains AL policies for batch queries needed for NMT, as opposed to the previous work on single query selection. We conduct experiments on three language pairs Finnish-English, German-English, and Czech-English. Simulating low resource scenarios, we consider various settings, including cold-start and warm-start as well as small and extremely small data conditions. The experiments Traditional active learning (AL) methods for machine translation (MT) rely on heuristics. However, these heuristics are limited when the characteristics of the MT problem change due t"
K18-1033,N18-1032,0,0.0316148,"setting (Koehn and Knowles, 2017). Recently, the importance of learning NMT models in scarce parallel bitext scenarios has gained attention. Unsupervised approaches try to learn NMT models without the need for parallel bitext (Artetxe et al., 2017; Lample et al., 2017a). Dual learning/backtranslation tries to start off from a small amount of bilingual text, and leverage monolingual text in the source and target language (Sennrich et al., 2015a; He et al., 2016). Zero/few shot approach attempts to transfer NMT learned from rich bilingual settings to low-resource settings (Johnson et al., 2016; Gu et al., 2018). 334 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 334–344 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics Furthermore, it is presumably more efficient and practical to query the translation of an untranslated batch from a human translator, rather than one sentence in each AL round. show the effectiveness and superiority of our policy query compared to strong baselines. 2 Learning to Actively Learn MT Active learning is an iterative process: Firstly, a model is built using some initially ava"
K18-1033,J82-2005,0,0.487798,"Missing"
K18-1033,P18-1174,1,0.832788,"majority of work in AL-MT where hard-coded heuristics are used for query selection (Haffari and Sarkar, 2009; Bloodgood and Callison-Burch, 2010). More concretely, we learn the query policy based on a high-resource language-pair sharing similar characteristics with the low-resource language-pair of interest. After trained, the policy is applied to the language-pair of interest capitalising on the learned signals for effective query selection. We make use of imitation learning (IL) to train the query policy. Previous work has shown that the IL approach leads to more effective policy learning (Liu et al., 2018), compared to reinforcement learning (RL) (Fang et al., 2017) . Our proposed method effectively trains AL policies for batch queries needed for NMT, as opposed to the previous work on single query selection. We conduct experiments on three language pairs Finnish-English, German-English, and Czech-English. Simulating low resource scenarios, we consider various settings, including cold-start and warm-start as well as small and extremely small data conditions. The experiments Traditional active learning (AL) methods for machine translation (MT) rely on heuristics. However, these heuristics are li"
K18-1033,N09-1047,1,0.829785,"am of data. Woodward and Finn (2017) extended one shot learning to active learning and combined reinforcement learning with a deep recurrent model to make labeling decisions. As far as we know, we are the first one to develop the Meta-AL method to make use of monolingual data for neural machine translation, the method we proposed in this paper can be applied at mini-batch level and conducted in cross lingual settings. Table 4: The table gives an estimation of the resorted heuristics. 6 Related Work For statistical MT (SMT), active learning is well explored, e.g. see Haffari and Sarkar (2009); Haffari et al. (2009), where several heuristics for query sentence selection have been proposed, including the entropy over the potential translations (uncertainty sampling), query by committee, and a similarity-based sentence selection method. However, active learning is largely under-explored for NMT. The goal of this paper is to provide an approach to learn an active learning strategy for NMT based on a Hierarchical Markov Decision Process (HMDP) formulation of the pool-based AL (Bachman et al., 2017; Liu et al., 2018). 7 Conclusion We have introduced an effective approach for learning active learning policies"
K18-1033,D15-1166,0,0.105294,"imise the parameters to maximise the loglikelihood objective (Liu et al., 2018). More specifically, let (ccglobal , B , b ) be a training tuple in the replay memory. We define the probability of the correct action/batch as NMT Model Our baseline model consists of a 2-layer bi-directional LSTM encoder with an embeddings size of 512 and a hidden size of 512. The 1-layer LSTM decoder with 512 hidden units uses The preference score for a batch is the sum of its an attention network with 128 hidden units. We sentences’ preference scores, use a multiplicative-style attention attention architecture (Luong et al., 2015). The model is opti|bb| X b c c c x y score(b , global ) := π(c global , local&lt;t , rep(x t , t )) mized using Adam (Kingma and Ba, 2014) with a learning rate of 0.0001, where the dropout rate t=1 is set to 0.3. We set the mini-batch size to 200 where c local&lt;t denotes the local context up to the and the maximum sentence length to 50. We train sentence t in the batch. the base NMT models for 5 epochs on the initially To form the log-likelihood, we use recent tuples available bitext, as the perplexity on the dev set do and randomly sample several older ones from the not improve beyond more train"
K18-1033,D17-1153,0,0.052378,"Missing"
K18-1033,P16-1009,0,0.100657,"Missing"
K18-1033,D08-1112,0,0.326859,"ained word embeddings from Ammar et al. (2016) or • Random We randomly select monolingual build it based on the available bitext and monotext sentences up to the AL budget. in the source and target language (c.f. §5.2). To retrain our NMT model, we make parameter up• Length-based We use shortest/longest dates based on the mini-batches from the AL batch monolingual sentences up to the AL budget. as well as sampled mini-batches from the previous • Total Token Entropy (TTE) We sort iterations. monolingual sentences based on their TTE 5 Experiment which has been shown to be a strong AL heuristic (Settles and Craven, 2008) for Datasets Our experiments use the following 1 language pairs in the news domain based on http://www.statmt.org/moses score(bb, cglobal ) B , c global ) := P P r(bb|B . b0 , c global ) B score(b b 0 ∈B 338 System Base NMT (100K) Random Shortest Longest TTE Token Policy Random Shortest Longest TTE Token Policy FULL bitext (500K) EN→DE EN→FI EN→DE EN→CS 13.2 10.3 13.2 8.1 AL with 135K token budget 13.9 11.2 13.9 8.3 14.5 11.5 14.5 8.6 14.1 11.3 14.1 8.2 14.2 11.3 14.2 8.5 15.5 12.8 14.8 8.5 AL with 677K token budget 15.9 13.5 15.9 9.2 15.8 13.7 15.8 8.9 15.6 13.5 15.6 8.5 15.6 13.7 15.6 8.6 1"
K18-1056,C10-1075,0,0.0353479,"ompared with SEQ2SEQ, which mistranslates the second clause, our S2SMIX is not only capable of generating a group of correct translation, but also emitting synonyms for different mixture components. We provide more examples in the supplementary material. 5 Related Work Obviously, different domains aim at different readers, thus they exhibit distinctive genres compared to other domains. A well-tuned MT system cannot directly apply to new domains; otherwise, translation quality will degrade. Based on this factor, out-domain adaptation has been widely studied for MT, ranging from data selection (Li et al., 2010; Wang et al., 2017), tuning (Luong and Manning, 2015; Farajian et al., 2017) to domain tags (Chu et al., 2017). Similarly, in-domain adaptation is also a compelling direction. Normally, to train an universal MT system, the training data consist of gigantic corpora covering numerous and various domains.This training data is naturally so diverse that Mima et al. (1997) incorporated extralinguistic information to enhance translation quality. Michel and Neubig (2018) argue even without explicit signals (gender, politeness etc.), they can handle domain-specific information via annotation of speake"
K18-1056,2015.iwslt-evaluation.11,0,0.0618356,"-es). We use IWSLT14 dataset2 for en-es, IWSLT15 d `CLL (θ) = dθ K X X d P (z |x, y∗ ) log Pθ (y∗ |x, z) , dθ ∗ (x,y )∈D z=1 2 (15) 586 https://sites.google.com/site/iwsltevaluation2014/home Data Train Dev Test en-fr 208,719 5,685 2,762 en-de 189,600 6,775 2,762 en-vi 133,317 1,553 1,268 en-es 173,601 5,401 2,504 Table 1: Statistics of all language pairs for IWSLT data after preprocessing dataset for en-vi, and IWSLT16 dataset3 for en-fr and en-de. We pre-process the corpora by Moses tokenizer4 , and preserve the true case of the text. For en-vi, we use the pre-processed corpus distributed by Luong and Manning (2015)5 . For training and dev sets, we discard all of the sentence pairs where the length of either side exceeds 50 tokens. The number of sentence pairs of different language pairs after preprocessing are shown in Table 1. We apply byte pair encoding (BPE) (Sennrich et al., 2016) to handle rare words on en-fr, en-de and en-es, and share the BPE vocabularies between the encoder and decoder for each language pair. Figure 2: BLEU scores of the different variants of S2SMIX model and SEQ2SEQ model. not shared among the translations, i.e., M ∩m=1 ngrams(ˆ ym ) div_ngram ≡ 1 − M ∪m=1 ngrams(ˆ ym ) where n"
K18-1056,P18-2050,0,0.012323,", translation quality will degrade. Based on this factor, out-domain adaptation has been widely studied for MT, ranging from data selection (Li et al., 2010; Wang et al., 2017), tuning (Luong and Manning, 2015; Farajian et al., 2017) to domain tags (Chu et al., 2017). Similarly, in-domain adaptation is also a compelling direction. Normally, to train an universal MT system, the training data consist of gigantic corpora covering numerous and various domains.This training data is naturally so diverse that Mima et al. (1997) incorporated extralinguistic information to enhance translation quality. Michel and Neubig (2018) argue even without explicit signals (gender, politeness etc.), they can handle domain-specific information via annotation of speakers, and easily gain quality improvement from a larger number of domains. Our approach is considerably different from the previous work. We remove any extra annotation, and treat domain-related information as latent variables, which are learned from corpus. 6 Conclusions and Future Work In this paper, we propose a sequence to sequence mixture (S2SMIX) model to improve translation diversity within neural machine translation via incorporating a set of discrete latent"
K18-1056,P17-2061,0,0.0277881,"Missing"
K18-1056,N18-1033,0,0.0788216,"Missing"
K18-1056,P18-1115,0,0.0974162,"ever et al., 2014; Bahdanau et al., 2015). They have revolutionized MT by providing a unified end-to-end framework, as opposed to the traditional approaches requiring several submodels and long pipelines. The neural approach is superior or on-par with statistical MT in terms of translation quality on various MT tasks and domains e.g. (Wu et al., 2016; Hassan et al., 2018). A well recognized issue with SEQ2SEQ models is the lack of diversity in the generated translations. This issue is mostly attributed to the decoding algorithm (Li et al., 2016), and recently to the model (Zhang et al., 2016; Schulz et al., 2018a). The former direction has attempted to design diversity encouraging decoding algorithm, 1 For a given source sentence, usually there exist several valid translations. 583 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 583–592 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics 2 where M is the embedding table, and W and W 0 are learnable parameters. The context vector ct is computed based on the input and attention, Attentional Sequence to Sequence An attentional sequence to sequence (SEQ2SEQ)"
K18-1056,W17-4713,0,0.0206262,"X is not only capable of generating a group of correct translation, but also emitting synonyms for different mixture components. We provide more examples in the supplementary material. 5 Related Work Obviously, different domains aim at different readers, thus they exhibit distinctive genres compared to other domains. A well-tuned MT system cannot directly apply to new domains; otherwise, translation quality will degrade. Based on this factor, out-domain adaptation has been widely studied for MT, ranging from data selection (Li et al., 2010; Wang et al., 2017), tuning (Luong and Manning, 2015; Farajian et al., 2017) to domain tags (Chu et al., 2017). Similarly, in-domain adaptation is also a compelling direction. Normally, to train an universal MT system, the training data consist of gigantic corpora covering numerous and various domains.This training data is naturally so diverse that Mima et al. (1997) incorporated extralinguistic information to enhance translation quality. Michel and Neubig (2018) argue even without explicit signals (gender, politeness etc.), they can handle domain-specific information via annotation of speakers, and easily gain quality improvement from a larger number of domains. Our"
K18-1056,D17-1155,0,0.0127623,"SEQ, which mistranslates the second clause, our S2SMIX is not only capable of generating a group of correct translation, but also emitting synonyms for different mixture components. We provide more examples in the supplementary material. 5 Related Work Obviously, different domains aim at different readers, thus they exhibit distinctive genres compared to other domains. A well-tuned MT system cannot directly apply to new domains; otherwise, translation quality will degrade. Based on this factor, out-domain adaptation has been widely studied for MT, ranging from data selection (Li et al., 2010; Wang et al., 2017), tuning (Luong and Manning, 2015; Farajian et al., 2017) to domain tags (Chu et al., 2017). Similarly, in-domain adaptation is also a compelling direction. Normally, to train an universal MT system, the training data consist of gigantic corpora covering numerous and various domains.This training data is naturally so diverse that Mima et al. (1997) incorporated extralinguistic information to enhance translation quality. Michel and Neubig (2018) argue even without explicit signals (gender, politeness etc.), they can handle domain-specific information via annotation of speakers, and easily gain"
K18-1056,D16-1050,0,0.0401587,"nslation (MT) (Sutskever et al., 2014; Bahdanau et al., 2015). They have revolutionized MT by providing a unified end-to-end framework, as opposed to the traditional approaches requiring several submodels and long pipelines. The neural approach is superior or on-par with statistical MT in terms of translation quality on various MT tasks and domains e.g. (Wu et al., 2016; Hassan et al., 2018). A well recognized issue with SEQ2SEQ models is the lack of diversity in the generated translations. This issue is mostly attributed to the decoding algorithm (Li et al., 2016), and recently to the model (Zhang et al., 2016; Schulz et al., 2018a). The former direction has attempted to design diversity encouraging decoding algorithm, 1 For a given source sentence, usually there exist several valid translations. 583 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 583–592 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics 2 where M is the embedding table, and W and W 0 are learnable parameters. The context vector ct is computed based on the input and attention, Attentional Sequence to Sequence An attentional sequence t"
K18-1056,P16-5005,0,\N,Missing
N09-1020,N04-1043,0,0.0180066,"t L be the set of tree leaves and τ (a) be the distance from node or edge a to the leaves: τ (a) := min #{edges between a and l} l∈L (10) 177 a b Figure 2: τ (root) = 2, while τ (v) = 1 for shaded vertices v. Contracting a and b results in both child of b being direct children of a while b is removed. In the experiments we considered either contracting edges3 close to the leaves τ (a) = 1 (thus removing many of the long branches described above), or edges further up the tree τ (a) ≥ 2 (preserving the informative subtrees closer to the leaves while removing many internal nodes). See Figure 2. (Miller et al., 2004) cut the BCluster tree at a certain depth k to simplify the tree, meaning every leaf descending from a particular internal node at level k is made an immediate child of that node. They use the tree to get extra features for a discriminative model to tackle the problem of sparsity—the features obtained from the new tree do not suffer from sparsity since each node has several words as its leaves. This technique did not work well for our application so we will not report results using it in our experiments. 5 Experiments In this section we present experimental results on two IR datasets: Cranfiel"
N09-1020,P93-1024,0,0.340997,"vocabulary words as its leaves. Using a heap data structure, this basic agglomerative clustering algorithm requires O(n2 log(n) + sn2 ) computations where n is the size of the vocabulary and s is the amount of computation needed to compute the similarity between two clusters. Typically the vocabulary size n is large; to speed up the algorithm, we use a greedy version described in Algorithm 1 which restricts the number of cluster candidates to at most m ≪ n. This greedy version is faster with complexity O(nm(log m + s)). In the experiments we used m = 500. Distributional clustering (Dcluster) (Pereira et al., 1993) measures similarity among words in terms of the similarity among their local contexts. Each word is represented by the frequencies of various words in a window around each occurrence of the word. The similarity between two words is computed to be a symmetrized KL divergence between the distributions over neighboring words associated with the two words. For a cluster of words the neighboring words are the union of those associated with each word in the cluster. Dcluster has been used extensively in text classification (Baker and McCallum, 1998). Probabilistic hierarchical clustering (Pcluster)"
N09-1020,P07-1059,0,0.0325255,"f a term is inversely related to the number of documents that it appears in, i.e. the popularity of the term. This is because popular terms, e.g. common and stop words, are often uninformative, while rare terms are often very informative. Another important effect is that related or co-occurring terms are often useful in determining the relevance of documents. Because most relevance scores do not capture this effect, IR systems resort to techniques like query expansion which includes synonyms and other morphological forms of the original query terms in order to improve retrieval results; e.g. (Riezler et al., 2007; Metzler and Croft, 2007). In this paper we explore a probabilistic model for IR that simultaneously handles both effects in a principled manner. It builds upon the work of (Cowans, 2004) who proposed a hierarchical Dirichlet document model. In this model, each document is modeled using a multinomial distribution (making the bag-of-words assumption) whose parameters are given Dirichlet priors. The common mean of the Dirichlet priors is itself assumed random and given a Dirichlet hyperprior. (Cowans, 2004) showed that the shared mean parameter induces sharing of information across documents in"
N09-1020,J92-4003,0,\N,Missing
N09-1047,2005.iwslt-1.7,0,0.244638,"Missing"
N09-1047,W07-0737,0,0.119679,"Missing"
N09-1047,P03-1021,0,0.00557207,"test 2K in-dom L 11K in-dom U 20K See Sec. 4.2 Bangla in-dom dev 450 in-dom test 1K Hansards Fr out-dom L 5K Table 1: Specification of different data sets we will use in experiments. The target language is English in the bilingual sets, and the source languages are either French (Fr), German (Ge), Spanish (Sp), or Bangla. model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, and (d) a word penalty. These different models are combined log-linearly. Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on a development corpus which we will call dev1 in this paper. The weight vectors in n-gram and similarity methods are set to (.15, .2, .3, .35) to emphasize longer n-grams. We set α = β = .35 for HAS, and use the 100-best list of translations when identifying candidate phrases while setting the maximum phrase length to 10. We set ǫ = .5 to smooth probabilities when computing scores based on translation units. 4.1 Simulated Low Density Language Pairs We use three language pairs (French-English, German-English, Spanish-English) to compare all of the proposed sentence selection st"
N09-1047,P08-1098,0,0.0227829,"e to this method (or its computationally demanding generalization in which instead of a single sentence, several sets of sentences of size k are selected and ranked) we use a hill climbing search on the surface of dev2’s BLEU score. For a fixed value of the weight vector, dev1 sentences are ranked and then the top-k output is selected and the amount of improvement the retrained SMT system gives on dev2’s BLEU score is measured. Starting from a random initial value for αk ’s, we improve one dimension at a time and traverse the discrete grid 2 To see how different rankings can be combined, see (Reichart et al., 2008) which proposes this for multi-task AL. 3 Here the retrained SMT model is the one learned by adding a particular sentence from dev1 into L. 418 placed on the values of the weight vector. Starting with a coarse grid, we make it finer when we get stuck in local optima during hill climbing. 3.5 Hierarchical Adaptive Sampling (HAS) (Dasgupta and Hsu, 2008) propose a technique for sample selection that, under certain settings, is guaranteed to be no worse than random sampling. Their method exploits the cluster structure (if there is any) in the unlabeled data. Ideally, querying the label of only on"
N09-1047,W08-0305,0,0.117908,"Missing"
N09-1047,J07-1003,0,0.015363,"Missing"
N09-1047,P07-1004,1,0.944538,"orithm 1 AL-SMT 1: Given bilingual corpus L, and monolingual corpus U . 2: MF →E = train(L, ∅) 3: for t = 1, 2, ... do 4: U + = translate(U, MF →E ) 5: Select k sentence pairs from U + , and ask a human for their true translations. 6: Remove the k sentences from U , and add the k sentence pairs (translated by human) to L 7: MF →E = train(L, U + ) 8: Monitor the performance on the test set T 9: end for Phrase tables from U + will get a 0 score in minimum error rate training if they are not useful, so our method is more general. Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U + , or (2) combining the two sets of data and training from the bitext L ∪ U + . The setup in Algorithm 1 helps us to investigate how to maximally take advantage of human effort (for sentence translation) when learning an SMT model from the available data, that includes bilingual and monolingual text. 3 Sentence Selection Strategies Our sentence selection strategies can be divided into two categories: (1) those which are independent of the target language and just look into the source language, and (2) those which"
N09-1047,W07-0724,0,\N,Missing
N15-1090,P11-1055,0,0.570134,"t hand labeled, the facts in the database act as “weak” or “distant” labels, hence the learning scenario is termed as distantly supervised. Prior work casts this problem as a multi-instance multi-label learning problem (Hoffmann et al., 2011; Surdeanu et al., 2012). It is multi-instance since for a given entity-pair, only the label of the bag of sentences containing both entities (aka mentions) is given. It is multi-label since a bag of mentions can have multiple labels. The inter-dependencies between relation labels and (hidden) mention labels are modeled by a Markov Random Field (Figure 1) (Hoffmann et al., 2011). The learning algorithms used in the literature for this problem optimize the (conditional) likelihood, but the evaluation measure is commonly the F -score. Formally, the training data is D := {(xi , yi )}N i=1 where xi ∈ X is the entity-pair, yi ∈ Y denotes the relation labels, and hi ∈ H denotes the hidden mention labels. The possible relation labels for the entity pair are observed from a given knowledgebase. If there are L candidate relation labels in the knowledge-base, then yi ∈ {0, 1}L , (e.g. yi,` is 1 if the relation ` is licensed by the knowledge-base for the entity-pair) and hi ∈ {"
N15-1090,P09-1113,0,0.326401,"Missing"
N15-1090,Q13-1030,0,0.177481,"e analysis on the benefits of using this search strategy vis-a-vis the exhaustive search in the Experiments section. 4 Experiments Dataset: We use the challenging benchmark dataset created by Riedel et al. (2010) for distant supervision of relation extraction models. It is created by aligning relations from Freebase4 with the sentences in New York Times corpus (Sandhaus, 2008). The labels for the datapoints come from the Freebase 3 For a given (f p, f n), we set y0 by picking the sorted unary terms that maximize the score according to y. 4 www.freebase.com database but Freebase is incomplete (Ritter et al., 2013). So a data point is labeled nil when either no relation exists or the relation is absent in Freebase. To avoid this ambiguity we train and evaluate the baseline and our algorithms on a subset of this dataset which consists of only non-nil relation labeled datapoints (termed as positive dataset). For the sake of completeness, we do report the accuracies of the various approaches on the entire evaluation dataset. Systems and Baseline: Hoffmann et al. (2011) describe a state-of-the-art approach for this task. They use a perceptron-style parameter update scheme adapted to handle latent variables;"
N15-1090,D12-1042,0,0.0797486,"Obama, United States 2 2.1 Preliminaries Distant Supervision for Relation Extraction Our framework is motivated by distant supervision for learning relation extraction models (Mintz et al., 2009). The goal is to learn relation extraction models by aligning facts in a database to sentences in a large unlabeled corpus. Since the individual sentences are not hand labeled, the facts in the database act as “weak” or “distant” labels, hence the learning scenario is termed as distantly supervised. Prior work casts this problem as a multi-instance multi-label learning problem (Hoffmann et al., 2011; Surdeanu et al., 2012). It is multi-instance since for a given entity-pair, only the label of the bag of sentences containing both entities (aka mentions) is given. It is multi-label since a bag of mentions can have multiple labels. The inter-dependencies between relation labels and (hidden) mention labels are modeled by a Markov Random Field (Figure 1) (Hoffmann et al., 2011). The learning algorithms used in the literature for this problem optimize the (conditional) likelihood, but the evaluation measure is commonly the F -score. Formally, the training data is D := {(xi , yi )}N i=1 where xi ∈ X is the entity-pair"
N15-1090,P13-2117,0,0.435481,"Missing"
N15-1090,D14-1166,0,\N,Missing
N16-1037,D15-1262,0,0.161131,"formation as a latent variable, we attain better perplexity on held-out data. Discourse and dialog modeling Early work on discourse relation classification utilizes rich, handcrafted feature sets (Joty et al., 2012; Lin et al., 2009; Sagae, 2009). Recent representation learning approaches attempt to learn good representations jointly with discourse relation classifiers and discourse parsers (Ji and Eisenstein, 2014; Li et al., 2014). Of particular relevance are applications of neural architectures to PDTB implicit discourse relation classification (Ji and Eisenstein, 2015; Zhang et al., 2015; Braud and Denis, 2015). All of these approaches are essentially classifiers, and take supervision only from the 16,000 annotated discourse relations in the PDTB training set. In contrast, our approach is a probabilistic model over the entire text. Probabilistic models are frequently used in diaLatent variable neural networks Introducing latent variables to a neural network model increases its representational capacity, which is the main goal of prior efforts in this space (Kingma and Welling, 2014; Chung et al., 2015). From this perspective, our model with discourse relations as latent variables shares the same mer"
N16-1037,P15-1033,0,0.0205422,"network “tsunami” (Manning, 2016). A key advantage of these neural architectures is that they employ discriminatively-trained distributed representations, which can capture the meaning of linguistic phenomena ranging from individual words (Turian et al., 2010) to longer-range linguistic contexts at the sentence level (Socher et al., 2013) and beyond (Le and Mikolov, 2014). Because they are discriminatively trained, these methJacob Eisenstein Georgia Institute of Technology Atlanta, GA 30308, USA jacobe@gatech.edu ods can learn representations that yield very accurate predictive models (e.g., Dyer et al, 2015). However, in comparison with the probabilistic graphical models that were previously the dominant machine learning approach for NLP, neural architectures lack flexibility. By treating linguistic annotations as random variables, probabilistic graphical models can marginalize over annotations that are unavailable at test or training time, elegantly modeling multiple linguistic phenomena in a joint framework (Finkel et al., 2006). But because these graphical models represent uncertainty for every element in the model, adding too many layers of latent variables makes them difficult to train. In t"
N16-1037,W06-1673,0,0.0565286,"thJacob Eisenstein Georgia Institute of Technology Atlanta, GA 30308, USA jacobe@gatech.edu ods can learn representations that yield very accurate predictive models (e.g., Dyer et al, 2015). However, in comparison with the probabilistic graphical models that were previously the dominant machine learning approach for NLP, neural architectures lack flexibility. By treating linguistic annotations as random variables, probabilistic graphical models can marginalize over annotations that are unavailable at test or training time, elegantly modeling multiple linguistic phenomena in a joint framework (Finkel et al., 2006). But because these graphical models represent uncertainty for every element in the model, adding too many layers of latent variables makes them difficult to train. In this paper, we present a hybrid architecture that combines a recurrent neural network language model with a latent variable model over shallow discourse structure. In this way, the model learns a discriminatively-trained distributed representation of the local contextual features that drive word choice at the intra-sentence level, using techniques that are now state-of-the-art in language modeling (Mikolov et al., 2010). However"
N16-1037,P14-1002,1,0.828815,"age model that generates the current sentence. The most relevant work is the Document Context Language Model (Ji et al., 2015, DCLM); we describe the connection to this model in § 2. By adding discourse information as a latent variable, we attain better perplexity on held-out data. Discourse and dialog modeling Early work on discourse relation classification utilizes rich, handcrafted feature sets (Joty et al., 2012; Lin et al., 2009; Sagae, 2009). Recent representation learning approaches attempt to learn good representations jointly with discourse relation classifiers and discourse parsers (Ji and Eisenstein, 2014; Li et al., 2014). Of particular relevance are applications of neural architectures to PDTB implicit discourse relation classification (Ji and Eisenstein, 2015; Zhang et al., 2015; Braud and Denis, 2015). All of these approaches are essentially classifiers, and take supervision only from the 16,000 annotated discourse relations in the PDTB training set. In contrast, our approach is a probabilistic model over the entire text. Probabilistic models are frequently used in diaLatent variable neural networks Introducing latent variables to a neural network model increases its representational capac"
N16-1037,D12-1083,0,0.0230579,"amework to model the context and current sentence. Wang and Cho (2015) and Lin et al. (2015) construct bag-of-words representations of previous sentences, which are then used to inform the RNN language model that generates the current sentence. The most relevant work is the Document Context Language Model (Ji et al., 2015, DCLM); we describe the connection to this model in § 2. By adding discourse information as a latent variable, we attain better perplexity on held-out data. Discourse and dialog modeling Early work on discourse relation classification utilizes rich, handcrafted feature sets (Joty et al., 2012; Lin et al., 2009; Sagae, 2009). Recent representation learning approaches attempt to learn good representations jointly with discourse relation classifiers and discourse parsers (Ji and Eisenstein, 2014; Li et al., 2014). Of particular relevance are applications of neural architectures to PDTB implicit discourse relation classification (Ji and Eisenstein, 2015; Zhang et al., 2015; Braud and Denis, 2015). All of these approaches are essentially classifiers, and take supervision only from the 16,000 annotated discourse relations in the PDTB training set. In contrast, our approach is a probabil"
N16-1037,W13-3214,0,0.614919,"he relationships between pairs of adjacent sentences — as a latent variable. As a result, the model can act as both a discourse relation classifier and a language model. Specifically: • If trained to maximize the conditional likelihood of the discourse relations, it outperforms state-of-the-art methods for both implicit discourse relation classification in the Penn Discourse Treebank (Rutherford and Xue, 2015) and dialog act classification in Switch332 Proceedings of NAACL-HLT 2016, pages 332–342, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics board (Kalchbrenner and Blunsom, 2013). The model learns from both the discourse annotations as well as the language modeling objective, unlike previous recursive neural architectures that learn only from annotated discourse relations (Ji and Eisenstein, 2015). • If the model is trained to maximize the joint likelihood of the discourse relations and the text, it is possible to marginalize over discourse relations at test time, outperforming language models that do not account for discourse structure. In contrast to recent work on continuous latent variables in recurrent neural networks (Chung et al., 2015), which require complex v"
N16-1037,D14-1220,0,0.0211294,"the current sentence. The most relevant work is the Document Context Language Model (Ji et al., 2015, DCLM); we describe the connection to this model in § 2. By adding discourse information as a latent variable, we attain better perplexity on held-out data. Discourse and dialog modeling Early work on discourse relation classification utilizes rich, handcrafted feature sets (Joty et al., 2012; Lin et al., 2009; Sagae, 2009). Recent representation learning approaches attempt to learn good representations jointly with discourse relation classifiers and discourse parsers (Ji and Eisenstein, 2014; Li et al., 2014). Of particular relevance are applications of neural architectures to PDTB implicit discourse relation classification (Ji and Eisenstein, 2015; Zhang et al., 2015; Braud and Denis, 2015). All of these approaches are essentially classifiers, and take supervision only from the 16,000 annotated discourse relations in the PDTB training set. In contrast, our approach is a probabilistic model over the entire text. Probabilistic models are frequently used in diaLatent variable neural networks Introducing latent variables to a neural network model increases its representational capacity, which is the"
N16-1037,D09-1036,0,0.853274,"paragraph, and need not be adjacent to either Arg2 nor the discourse marker. However, automatically classifying these relations is considered to be relatively easy, due to the constraints from the discourse marker itself (Pitler et al., 2008). In addition, explicit relations are difficult to incorporate into language models which must generate each word exactly once. On the contrary, implicit discourse relations are annotated only between adjacent sentences, based on a semantic understanding of the discourse arguments. Automatically classifying these discourse relations is a challenging task (Lin et al., 2009; Pitler et al., 2009; Rutherford and Xue, 2015; Ji and Eisenstein, 2015). We therefore focus on implicit discourse relations, leaving to the future work the question of how to apply our modeling framework to explicit discourse relations. During training, we collapse all relation types other than implicit (explicit, E NT R EL, and N O R EL) into a single dummy relation type, which holds between all adjacent sentence pairs that do not share an implicit relation. As in the prior work on first-level discourse relation identification (e.g., Park and Cardie, 2012), we use sections 2-20 of the PDTB"
N16-1037,D15-1106,0,0.0802735,"classification, which characterizes the structure of interpersonal communication in the Switchboard corpus (Stolcke et al., 2000), and is a key component of contemporary dialog systems (Williams and Young, 2007). Our model outperforms state-of-the-art alternatives for implicit discourse relation classification in the Penn Discourse Treebank, and for dialog act classification in the Switchboard corpus. 2 Background Our model scaffolds on recurrent neural network (RNN) language models (Mikolov et al., 2010), and recent variants that exploit multiple levels of linguistic detail (Ji et al., 2015; Lin et al., 2015). 333 RNN Language Models Let us denote token n in a sentence t by yt,n ∈ {1 . . . V }, and write yt = {yt,n }n∈{1...Nt } to indicate the sequence of words in sentence t. In an RNN language model, the probability of the sentence is decomposed as, p(yt ) = Nt Y p(yt,n |yt,&lt;n ), (1) n where the probability of each word yt,n is conditioned on the entire preceding sequence of words yt,&lt;n through the summary vector ht,n−1 . This vector is computed recurrently from ht,n−2 and from the embedding of the current word, Xyt,n−1 , where X ∈ RK×V and K is the dimensionality of the word embeddings. The lang"
N16-1037,W14-1505,0,0.00993717,"ging has been widely studied in both NLP and speech communities. We follow the setup used by Stolcke et al. (2000) to conduct experiments, and adopt the following systems for comparison: Stolcke et al. (2000) employ a hidden Markov model, with each HMM state corresponding to a dialogue act. Kalchbrenner and Blunsom (2013) employ a complex neural architecture, with a convolutional network at each utterance and a recurrent network over the length of the dialog. To our knowledge, this model attains state-of-the-art accuracy on this task, outperforming other prior work such as (Webb et al., 2005; Milajevs and Purver, 2014). Results As shown in Table 2, the conditionallytrained discourse relation language model (D R LM) outperforms all competitive systems on this task. A binomial test shows the result in line 6 is significantly better than the previous state-of-the-art (line 4). All comparisons are against published results, and Macro-F1 scores are not available. Accuracy 338 is more reliable on this evaluation, since no single class dominates, unlike the PDTB task. 5.3 Discourse-aware language modeling As a joint model for discourse and language modeling, D R LM can also function as a language model, assigning"
N16-1037,C08-2022,0,0.0306807,"el discourse annotation on written texts. In the PDTB, each discourse relation is annotated between two argument spans, Arg1 and Arg2. There are two types of relations: explicit and implicit. Explicit relations are signalled by discourse markers (e.g., “however”, “moreover”), and the span of Arg1 is almost totally unconstrained: it can range from a single clause to an entire paragraph, and need not be adjacent to either Arg2 nor the discourse marker. However, automatically classifying these relations is considered to be relatively easy, due to the constraints from the discourse marker itself (Pitler et al., 2008). In addition, explicit relations are difficult to incorporate into language models which must generate each word exactly once. On the contrary, implicit discourse relations are annotated only between adjacent sentences, based on a semantic understanding of the discourse arguments. Automatically classifying these discourse relations is a challenging task (Lin et al., 2009; Pitler et al., 2009; Rutherford and Xue, 2015; Ji and Eisenstein, 2015). We therefore focus on implicit discourse relations, leaving to the future work the question of how to apply our modeling framework to explicit discours"
N16-1037,P09-1077,0,0.114386,"d not be adjacent to either Arg2 nor the discourse marker. However, automatically classifying these relations is considered to be relatively easy, due to the constraints from the discourse marker itself (Pitler et al., 2008). In addition, explicit relations are difficult to incorporate into language models which must generate each word exactly once. On the contrary, implicit discourse relations are annotated only between adjacent sentences, based on a semantic understanding of the discourse arguments. Automatically classifying these discourse relations is a challenging task (Lin et al., 2009; Pitler et al., 2009; Rutherford and Xue, 2015; Ji and Eisenstein, 2015). We therefore focus on implicit discourse relations, leaving to the future work the question of how to apply our modeling framework to explicit discourse relations. During training, we collapse all relation types other than implicit (explicit, E NT R EL, and N O R EL) into a single dummy relation type, which holds between all adjacent sentence pairs that do not share an implicit relation. As in the prior work on first-level discourse relation identification (e.g., Park and Cardie, 2012), we use sections 2-20 of the PDTB as the training set,"
N16-1037,prasad-etal-2008-penn,0,0.355405,"mation ct impacts the hidden state ht+1 , rather than going directly to the outputs yt+1 . They obtain slightly better perplexity with this approach, which has fewer trainable parameters. However, this model would couple zt with all subsequent sentences y>t , making prediction and marginalization of discourse relations considerably more challenging. Sequential Monte Carlo algorithms offer a possible solution (de Freitas et al., ; Gu et al., 2015), which may be considered in future work. 4 Data and Implementation We evaluate our model on two benchmark datasets: (1) the Penn Discourse Treebank (Prasad et al., 2008, PDTB), which is annotated on a corpus of Wall Street Journal acticles; (2) the Switchboard di336 there are an equal number of instances with and without each relation type (Park and Cardie, ; Biran and McKeown, 2013; Rutherford and Xue, 2014). In this paper, we target the more challenging multiway classification problem, so this strategy is not applicable; in any case, since our method deals with entire documents, it is not possible to balance the training set in this way. The Switchboard Dialog Act Corpus (SWDA) is annotated on the Switchboard Corpus of humanhuman conversational telephone s"
N16-1037,E14-1068,0,0.0566142,"quent sentences y>t , making prediction and marginalization of discourse relations considerably more challenging. Sequential Monte Carlo algorithms offer a possible solution (de Freitas et al., ; Gu et al., 2015), which may be considered in future work. 4 Data and Implementation We evaluate our model on two benchmark datasets: (1) the Penn Discourse Treebank (Prasad et al., 2008, PDTB), which is annotated on a corpus of Wall Street Journal acticles; (2) the Switchboard di336 there are an equal number of instances with and without each relation type (Park and Cardie, ; Biran and McKeown, 2013; Rutherford and Xue, 2014). In this paper, we target the more challenging multiway classification problem, so this strategy is not applicable; in any case, since our method deals with entire documents, it is not possible to balance the training set in this way. The Switchboard Dialog Act Corpus (SWDA) is annotated on the Switchboard Corpus of humanhuman conversational telephone speech (Godfrey et al., 1992). The annotations label each utterance with one of 42 possible speech acts, such as AGREE, HEDGE, and WH - QUESTION . Because these speech acts form the structure of the dialogue, most of them pertain to both the pre"
N16-1037,N15-1081,0,0.54352,"t drive word choice at the intra-sentence level, using techniques that are now state-of-the-art in language modeling (Mikolov et al., 2010). However, the model treats shallow discourse structure — specifically, the relationships between pairs of adjacent sentences — as a latent variable. As a result, the model can act as both a discourse relation classifier and a language model. Specifically: • If trained to maximize the conditional likelihood of the discourse relations, it outperforms state-of-the-art methods for both implicit discourse relation classification in the Penn Discourse Treebank (Rutherford and Xue, 2015) and dialog act classification in Switch332 Proceedings of NAACL-HLT 2016, pages 332–342, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics board (Kalchbrenner and Blunsom, 2013). The model learns from both the discourse annotations as well as the language modeling objective, unlike previous recursive neural architectures that learn only from annotated discourse relations (Ji and Eisenstein, 2015). • If the model is trained to maximize the joint likelihood of the discourse relations and the text, it is possible to marginalize over discourse relations at"
N16-1037,W09-3813,0,0.0317933,"ent sentence. Wang and Cho (2015) and Lin et al. (2015) construct bag-of-words representations of previous sentences, which are then used to inform the RNN language model that generates the current sentence. The most relevant work is the Document Context Language Model (Ji et al., 2015, DCLM); we describe the connection to this model in § 2. By adding discourse information as a latent variable, we attain better perplexity on held-out data. Discourse and dialog modeling Early work on discourse relation classification utilizes rich, handcrafted feature sets (Joty et al., 2012; Lin et al., 2009; Sagae, 2009). Recent representation learning approaches attempt to learn good representations jointly with discourse relation classifiers and discourse parsers (Ji and Eisenstein, 2014; Li et al., 2014). Of particular relevance are applications of neural architectures to PDTB implicit discourse relation classification (Ji and Eisenstein, 2015; Zhang et al., 2015; Braud and Denis, 2015). All of these approaches are essentially classifiers, and take supervision only from the 16,000 annotated discourse relations in the PDTB training set. In contrast, our approach is a probabilistic model over the entire text"
N16-1037,D13-1170,0,0.0012234,"on in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline. 1 Introduction Natural language processing (NLP) has recently experienced a neural network “tsunami” (Manning, 2016). A key advantage of these neural architectures is that they employ discriminatively-trained distributed representations, which can capture the meaning of linguistic phenomena ranging from individual words (Turian et al., 2010) to longer-range linguistic contexts at the sentence level (Socher et al., 2013) and beyond (Le and Mikolov, 2014). Because they are discriminatively trained, these methJacob Eisenstein Georgia Institute of Technology Atlanta, GA 30308, USA jacobe@gatech.edu ods can learn representations that yield very accurate predictive models (e.g., Dyer et al, 2015). However, in comparison with the probabilistic graphical models that were previously the dominant machine learning approach for NLP, neural architectures lack flexibility. By treating linguistic annotations as random variables, probabilistic graphical models can marginalize over annotations that are unavailable at test or"
N16-1037,N15-1020,1,0.781846,"etwork language model. An alternative neural approach for dialogue act tagging is the combined convolutionalrecurrent architecture of Kalchbrenner and Blunsom (2013). Our modeling framework is simpler, relying on a latent variable parametrization of a purely recurrent architecture. This paper draws on previous work in both discourse modeling and language modeling. Language modeling There are an increasing number of attempts to incorporate document-level context information into language modeling. For example, Mikolov and Zweig (2012) introduce LDAstyle topics into RNN based language modeling. Sordoni et al. (2015) use a convolutional structure to summarize the context from previous two utterances as context vector for RNN based language modeling. Our models in this paper provide a unified framework to model the context and current sentence. Wang and Cho (2015) and Lin et al. (2015) construct bag-of-words representations of previous sentences, which are then used to inform the RNN language model that generates the current sentence. The most relevant work is the Document Context Language Model (Ji et al., 2015, DCLM); we describe the connection to this model in § 2. By adding discourse information as a l"
N16-1037,J00-3003,0,0.979587,"Missing"
N16-1037,P10-1040,0,0.0155191,"ation classification in the Penn Discourse Treebank, and dialog act classification in the Switchboard corpus. Furthermore, by marginalizing over latent discourse relations at test time, we obtain a discourse informed language model, which improves over a strong LSTM baseline. 1 Introduction Natural language processing (NLP) has recently experienced a neural network “tsunami” (Manning, 2016). A key advantage of these neural architectures is that they employ discriminatively-trained distributed representations, which can capture the meaning of linguistic phenomena ranging from individual words (Turian et al., 2010) to longer-range linguistic contexts at the sentence level (Socher et al., 2013) and beyond (Le and Mikolov, 2014). Because they are discriminatively trained, these methJacob Eisenstein Georgia Institute of Technology Atlanta, GA 30308, USA jacobe@gatech.edu ods can learn representations that yield very accurate predictive models (e.g., Dyer et al, 2015). However, in comparison with the probabilistic graphical models that were previously the dominant machine learning approach for NLP, neural architectures lack flexibility. By treating linguistic annotations as random variables, probabilistic g"
N16-1037,K15-2001,0,0.0454434,"olkits such as Theano, Torch, and CNN. We focus on a class of shallow discourse relations, which hold between pairs of adjacent sentences (or utterances). These relations describe how the adjacent sentences are related: for example, they may be in CONTRAST, or the latter sentence may offer an answer to a question posed by the previous sentence. Shallow relations do not capture the full range of discourse phenomena (Webber et al., 2012), but they account for two well-known problems: implicit discourse relation classification in the Penn Discourse Treebank, which was the 2015 CoNLL shared task (Xue et al., 2015); and dialog act classification, which characterizes the structure of interpersonal communication in the Switchboard corpus (Stolcke et al., 2000), and is a key component of contemporary dialog systems (Williams and Young, 2007). Our model outperforms state-of-the-art alternatives for implicit discourse relation classification in the Penn Discourse Treebank, and for dialog act classification in the Switchboard corpus. 2 Background Our model scaffolds on recurrent neural network (RNN) language models (Mikolov et al., 2010), and recent variants that exploit multiple levels of linguistic detail ("
N16-1037,D15-1266,0,0.0952637,"adding discourse information as a latent variable, we attain better perplexity on held-out data. Discourse and dialog modeling Early work on discourse relation classification utilizes rich, handcrafted feature sets (Joty et al., 2012; Lin et al., 2009; Sagae, 2009). Recent representation learning approaches attempt to learn good representations jointly with discourse relation classifiers and discourse parsers (Ji and Eisenstein, 2014; Li et al., 2014). Of particular relevance are applications of neural architectures to PDTB implicit discourse relation classification (Ji and Eisenstein, 2015; Zhang et al., 2015; Braud and Denis, 2015). All of these approaches are essentially classifiers, and take supervision only from the 16,000 annotated discourse relations in the PDTB training set. In contrast, our approach is a probabilistic model over the entire text. Probabilistic models are frequently used in diaLatent variable neural networks Introducing latent variables to a neural network model increases its representational capacity, which is the main goal of prior efforts in this space (Kingma and Welling, 2014; Chung et al., 2015). From this perspective, our model with discourse relations as latent varia"
N16-1037,J15-4006,0,\N,Missing
N16-1102,W11-1218,0,0.0140755,"s of parameters with vast potential for over-fitting. Table 1 shows the statistics of the training sets.6 For Chinese-English, the data comes from the BTEC corpus, where the number of training sentence pairs is 44,016. We used ‘devset1 2’ and ‘devset 3’ as the development and test sets, respectively, and in both cases used only the first reference for evaluation. For Romanian and Estonian, the data come from the Europarl corpus (Koehn, 2005), where we used 100K sentence pairs for training, and 3K for development and 2K for testing.7 The RussianEnglish data was taken from a web derived corpus (Antonova and Misyurev, 2011). The dataset is split into three parts using the same technique as for the Europarl sets. During the preprocessing stage we lower-cased and tokenized the data, and excluded sentences longer than 30 words. For the Europarl 5 As the alignment cells are normalised using the softmax and thus take values in [0,1], the trace term is bounded above by min(I, J) which occurs when the two alignment matrices are transposes of each other, representing perfect one-to-one alignments in both directions 880 We could share some parameters, e.g., the word embedding matrices, however we found this didn’t make m"
N16-1102,J93-2003,0,0.177161,"er-decoder (Sutskever et al., 2014), in which the source language is encoded into a distributed representation, followed by a decoding step which generates the target translation. We focus on the attentional model of translation (Bahdanau et al., 2015) which uses a dynamic representation of the source sentence while allowing the decoder to attend to different parts of the source as it generates the target sentence. The attentional model raises intriguing opportunities, given the correspondence between the notions of attention and alignment in traditional word-based machine translation models (Brown et al., 1993). In this paper we map modelling biases from word based translation models into the attentional model, such that known linguistic elements of translation can be better captured. We incorporate absolute positional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged ("
N16-1102,N13-1073,1,0.930705,"to different parts of the source as it generates the target sentence. The attentional model raises intriguing opportunities, given the correspondence between the notions of attention and alignment in traditional word-based machine translation models (Brown et al., 1993). In this paper we map modelling biases from word based translation models into the attentional model, such that known linguistic elements of translation can be better captured. We incorporate absolute positional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encouraged to agree (e.g. symmetrisation heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)). We provide an empirical analysis of incorporating the above"
N16-1102,P08-1112,0,0.0515496,"Missing"
N16-1102,W11-2123,0,0.0299588,"els and Baselines. We have implemented our neural translation model with linguistic features in C++ using the CNN library.9 We compared our proposed model against our implementations of the attentional model (Bahdanau et al., 2015) and encoder-decoder architecture (Sutskever et al., 2014). As the baseline, we used a state-of-the-art phrase-based statistical machine translation model built using Moses (Koehn et al., 2007) with the standard features: relative-frequency and lexical translation model probabilities in both directions; distortion model; language model and word count. We used KenLM (Heafield, 2011) to create 3-gram language models with Kneser-Ney smoothing on the target side of the bilingual training corpora. configuration Sutskever encdec Attentional +align +align+glofer +align+glofer-pre +align+sym +align+sym+glofer-pre perplexity data, we also removed sentences containing headings and other meeting formalities.8 0 2 4 6 8 epochs Figure 3: Perplexity with training epochs on ro-en translation, comparing several model variants. embedding, 512 hidden, and 256 alignment dimensions. For each model, we also report the number of its parameters. Models are trained end-to-end using stochastic"
N16-1102,D13-1176,0,0.721739,"ation model to include structural biases from word based alignment models, including positional bias, Markov conditioning, fertility and agreement over translation directions. We show improvements over a baseline attentional model and standard phrase-based model over several language pairs, evaluating on difficult languages in a low resource setting. 1 Introduction Recently, models of end-to-end machine translation based on neural network classification have been shown to produce excellent translations, rivalling or in some cases surpassing traditional statistical machine translation systems (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). This is despite the neural approaches using an overall simpler model, with fewer assumptions about the learning and prediction problem. Broadly, neural approaches are based around the notion of an encoder-decoder (Sutskever et al., 2014), in which the source language is encoded into a distributed representation, followed by a decoding step which generates the target translation. We focus on the attentional model of translation (Bahdanau et al., 2015) which uses a dynamic representation of the source sentence while allowing the decoder to attend"
N16-1102,N03-1017,0,0.101355,"ranslate as several words. Compared to the fertility model in IBM 3–5 (Brown et al., 1993), ours uses many fewer parameters through working over vector embeddings, and moreover, the BiRNN encoding of the source means that we learn context-dependent fertilities, which can be useful for dealing with fixed syntactic patterns or multi-word expressions. 3.4 Bilingual Symmetry So far we have considered a conditional model of the target given the source, modelling p(t|s). However it is well established for latent variable translation models that the alignments improve if p(s|t) is 3 Modern decoders (Koehn et al., 2003) often impose the restriction of each word being translated exactly once, however this is tempered by their use of phrases as translation units rather than words, which allow for higher fertility within phrases. 4 The normal distribution is deficient, as it has support for all scalar values, despite fi being bounded above and below (0 ≤ fi ≤ J). This could be corrected by using a truncated normal, or various other choices of distribution. lang-pair Zh-En Ru-En Et-En Ro-En # tokens (K) 422 454 1639 1809 1411 1857 1782 1806 # types (K) 3.44 3.12 145 65 90 25 39 24 Table 1: Statistics of the trai"
N16-1102,2005.iwslt-1.8,0,0.0597971,"truncated normal, or various other choices of distribution. lang-pair Zh-En Ru-En Et-En Ro-En # tokens (K) 422 454 1639 1809 1411 1857 1782 1806 # types (K) 3.44 3.12 145 65 90 25 39 24 Table 1: Statistics of the training sets, showing in each cell the count for the source language (left) and target language (right). Figure 2: Symmetric training with trace bonus, computed as matrix multiplication, − tr(αs←t αs→t > ). Dark shading indicates higher values. also modelled and the inferences of both directional models are combined – evidenced by the symmetrisation heuristics used in most decoders (Koehn et al., 2005), and also by explicit joint agreement training objectives (Liang et al., 2006; Ganchev et al., 2008). The rationale is that both models make somewhat independent errors, so an ensemble stands to gain from variance reduction. We propose a method for joint training of two directional models as pictured in Figure 2. Training twinned models involves optimising L = − log p(t|s) − log p(s|t) + γB where, as before, we consider only a single sentence pair, for simplicity of notation. This corresponds to a pseudo-likelihood objective, with the B linking the two models.5 The B component considers the a"
N16-1102,P07-2045,1,0.0319222,"rained the re-ranking models using MERT (Och, 2003) on development sets with 100-best translations. test 5.35 4.77 4.56 5.20 4.31 4.44 4.43 ● ● ● ●● ● ● ●●● ●●● ● ● ● ●● ●●●● ●●● ●● ● ●● ●● 5 Models and Baselines. We have implemented our neural translation model with linguistic features in C++ using the CNN library.9 We compared our proposed model against our implementations of the attentional model (Bahdanau et al., 2015) and encoder-decoder architecture (Sutskever et al., 2014). As the baseline, we used a state-of-the-art phrase-based statistical machine translation model built using Moses (Koehn et al., 2007) with the standard features: relative-frequency and lexical translation model probabilities in both directions; distortion model; language model and word count. We used KenLM (Heafield, 2011) to create 3-gram language models with Kneser-Ney smoothing on the target side of the bilingual training corpora. configuration Sutskever encdec Attentional +align +align+glofer +align+glofer-pre +align+sym +align+sym+glofer-pre perplexity data, we also removed sentences containing headings and other meeting formalities.8 0 2 4 6 8 epochs Figure 3: Perplexity with training epochs on ro-en translation, comp"
N16-1102,W04-3250,0,0.0357109,"15.0 15.0 15.5 15.5 30.1 31.2 Table 2: Perplexity results for attentional model variants evaluated on BTEC zh→en, and number of model parameters (in millions). vanilla +glofer +align +align +glofer pretrain +align +glofer 25 ● 20 ● 15 ● ● ● 10 Evaluation Measures. Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and translation results, as well as in an additional re-ranking setting, using BLEU (Papineni et al., 2002) measure. We applied bootstrap resampling (Koehn, 2004) to measure statistical significance, p &lt; 0.05, of our models compared to a baseline. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the underlying phrase-based model. We trained the re-ranking models using MERT (Och, 2003) on development sets with 100-best translations. test 5.35 4.77 4.56 5.20 4.31 4.44 4.43 ● ● ● ●● ● ● ●●● ●●● ● ● ● ●● ●●●● ●●● ●● ● ●● ●● 5 Models and Baselines. We have implemented our neural translation model with linguistic features i"
N16-1102,2005.mtsummit-papers.11,0,0.140774,". This serves to demonstrate the robustness and generalisation of our model on sparse data – something that has not yet been established for neural models with millions of parameters with vast potential for over-fitting. Table 1 shows the statistics of the training sets.6 For Chinese-English, the data comes from the BTEC corpus, where the number of training sentence pairs is 44,016. We used ‘devset1 2’ and ‘devset 3’ as the development and test sets, respectively, and in both cases used only the first reference for evaluation. For Romanian and Estonian, the data come from the Europarl corpus (Koehn, 2005), where we used 100K sentence pairs for training, and 3K for development and 2K for testing.7 The RussianEnglish data was taken from a web derived corpus (Antonova and Misyurev, 2011). The dataset is split into three parts using the same technique as for the Europarl sets. During the preprocessing stage we lower-cased and tokenized the data, and excluded sentences longer than 30 words. For the Europarl 5 As the alignment cells are normalised using the softmax and thus take values in [0,1], the trace term is bounded above by min(I, J) which occurs when the two alignment matrices are transposes"
N16-1102,N15-1063,0,0.0126112,"iance reduction. We propose a method for joint training of two directional models as pictured in Figure 2. Training twinned models involves optimising L = − log p(t|s) − log p(s|t) + γB where, as before, we consider only a single sentence pair, for simplicity of notation. This corresponds to a pseudo-likelihood objective, with the B linking the two models.5 The B component considers the alignment (attention) matrices, αs→t ∈ RJ×I and αt←s ∈ RI×J , and attempts to make these close to one another for both translation directions (see Fig. 2). To achieve this, we use a ‘trace bonus’, inspired by (Levinboim et al., 2015), formulated as B = − tr(αs←t > αs→t ) = XX j s←t s→t αi,j αj,i . i 4 Experiments Datasets. We conducted our experiments with four language pairs, translating between English ↔ Romanian, Estonian, Russian and Chinese. These languages were chosen to represent a range of translation difficulties, including languages with significant morphological complexity (Estonian, Russian). We focus on a (simulated) low resource setting, where only a limited amount of training data is available. This serves to demonstrate the robustness and generalisation of our model on sparse data – something that has not"
N16-1102,N06-1014,0,0.529688,"to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encouraged to agree (e.g. symmetrisation heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)). We provide an empirical analysis of incorporating the above structural biases into the attentional model, considering low resource translation scenario over four language-pairs. Our results demonstrate consistent improvements over vanilla encoder876 Proceedings of NAACL-HLT 2016, pages 876–885, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics RNN Attentional Decoder Beginnings are difficult START Aller Anfang ist schwer STOP Figure 1: Attentional model of translation (Bahdanau et al., 2015). The encoder is shown below the decod"
N16-1102,D15-1166,0,0.338515,"Missing"
N16-1102,W15-5003,0,0.0228087,"baseline. All other results are for the attentional model with a single-layer LSTM as encoder and two-layer LSTM as decoder, using 512 8 9 E.g., (The sitting was closed at 10.20pm). https://github.com/clab/cnn/ 881 #param (M) 8.7 15.0 15.0 15.5 15.5 30.1 31.2 Table 2: Perplexity results for attentional model variants evaluated on BTEC zh→en, and number of model parameters (in millions). vanilla +glofer +align +align +glofer pretrain +align +glofer 25 ● 20 ● 15 ● ● ● 10 Evaluation Measures. Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and translation results, as well as in an additional re-ranking setting, using BLEU (Papineni et al., 2002) measure. We applied bootstrap resampling (Koehn, 2004) to measure statistical significance, p &lt; 0.05, of our models compared to a baseline. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the underlying phrase-based model. We trained the re-ranking models using MERT (Och, 2003) on development"
N16-1102,J03-1002,0,0.129597,"sitional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encouraged to agree (e.g. symmetrisation heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)). We provide an empirical analysis of incorporating the above structural biases into the attentional model, considering low resource translation scenario over four language-pairs. Our results demonstrate consistent improvements over vanilla encoder876 Proceedings of NAACL-HLT 2016, pages 876–885, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics RNN Attentional Decoder Beginnings are difficult START Aller Anfang ist schwer STOP Figure 1: Attentional model of translation (Bahdanau et al., 2015"
N16-1102,P03-1021,0,0.0154081,"015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and translation results, as well as in an additional re-ranking setting, using BLEU (Papineni et al., 2002) measure. We applied bootstrap resampling (Koehn, 2004) to measure statistical significance, p &lt; 0.05, of our models compared to a baseline. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the underlying phrase-based model. We trained the re-ranking models using MERT (Och, 2003) on development sets with 100-best translations. test 5.35 4.77 4.56 5.20 4.31 4.44 4.43 ● ● ● ●● ● ● ●●● ●●● ● ● ● ●● ●●●● ●●● ●● ● ●● ●● 5 Models and Baselines. We have implemented our neural translation model with linguistic features in C++ using the CNN library.9 We compared our proposed model against our implementations of the attentional model (Bahdanau et al., 2015) and encoder-decoder architecture (Sutskever et al., 2014). As the baseline, we used a state-of-the-art phrase-based statistical machine translation model built using Moses (Koehn et al., 2007) with the standard features: rel"
N16-1102,P02-1040,0,0.0951203,"sed at 10.20pm). https://github.com/clab/cnn/ 881 #param (M) 8.7 15.0 15.0 15.5 15.5 30.1 31.2 Table 2: Perplexity results for attentional model variants evaluated on BTEC zh→en, and number of model parameters (in millions). vanilla +glofer +align +align +glofer pretrain +align +glofer 25 ● 20 ● 15 ● ● ● 10 Evaluation Measures. Following previous work (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015; Neubig et al., 2015), we evaluated all neural models using test set perplexities and translation results, as well as in an additional re-ranking setting, using BLEU (Papineni et al., 2002) measure. We applied bootstrap resampling (Koehn, 2004) to measure statistical significance, p &lt; 0.05, of our models compared to a baseline. For re-ranking, we generated 100-best translations using the baseline phrase-based model, to which we added log probability features from our neural models alongside all the features of the underlying phrase-based model. We trained the re-ranking models using MERT (Och, 2003) on development sets with 100-best translations. test 5.35 4.77 4.56 5.20 4.31 4.44 4.43 ● ● ● ●● ● ● ●●● ●●● ● ● ● ●● ●●●● ●●● ●● ● ●● ●● 5 Models and Baselines. We have implemented"
N16-1102,P16-1008,0,0.0479383,"ion mechanism to be more local, by constraining attention to a text span, whose words’ representations are averaged. Similar in spirit to our work, recent research has proposed different ways of leveraging the attention history to incorporate alignment structural biases. (Luong et al., 2015) made use of the attention vector of the previous position when generating the attention vector for the next position. Feng et al. (2016) added another recurrent structure for the attention mechanism to enhance its memorization capabilities and capture long-range dependencies between the attention vectors. Tu et al. (2016) proposed a coverage vector to keep track of the attention history, hence refining future attentions. Finally, Cheng et al. (2015) proposed a similar agreement-based joint training for bidirectional attention-based neural machine translation, and showed significant improvements in BLEU for the large data French↔English translation. 6 Conclusion We have shown that the attentional model of translation does not capture many well known properties of traditional word-based translation models, and proposed several ways of imposing these as structural biases on the model. We show improvements across"
N16-1102,C96-2141,0,0.94621,"ord based translation models into the attentional model, such that known linguistic elements of translation can be better captured. We incorporate absolute positional bias whereby word order tends to be similar between the source sentence and its translation (e.g., IBM Model 2 and (Dyer et al., 2013)), fertility whereby each instance of a source word type tends to be translated into a consistent number of target tokens (e.g., IBM Models 3, 4, 5), relative position bias whereby prior preferences for monotonic alignments/attention can be encouraged (e.g., IBM Model 4, 5 and HMM-based Alignment (Vogel et al., 1996)), and alignment consistency whereby the attention in both translation directions are encouraged to agree (e.g. symmetrisation heuristics (Och and Ney, 2003) or joint modelling (Liang et al., 2006; Ganchev et al., 2008)). We provide an empirical analysis of incorporating the above structural biases into the attentional model, considering low resource translation scenario over four language-pairs. Our results demonstrate consistent improvements over vanilla encoder876 Proceedings of NAACL-HLT 2016, pages 876–885, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Ling"
N16-1102,W14-4012,0,\N,Missing
N16-1149,W14-4012,0,0.0170018,"Missing"
N16-1149,W11-2123,0,0.023925,"for language modelling. Furthermore, the sizes of our working datasets are an order of magnitude larger than the standard Penn Treebank set which is often used for evaluating neural language models. Set-up and Baselines. We have used cnn6 to implement our models. We use the same configurations for all neural models: 512 input embedding and hidden layer dimensions, 2 hidden layers, and vocabulary sizes as given in Table 1. We used the same vocabulary for the auxiliary and modelled text. We trained a conventional 5−gram language model using modified Kneser-Ney smoothing, with the KenLM toolkit (Heafield, 2011). We used the 3 http://www.europarl.europa.eu/ We ignored the period from June 2011 onwards, as from this date the EU stopped creating manual human translations. 5 This dataset will be released upon publication. 6 https://github.com/clab/cnn/ 4 Method 5-gram LM RNNLM LSTM DGLSTM input+add+k input+mlp+k input+stack+k output+mlp+k output+mlp+t output+mlp+d output+mlp+k+t output+mlp+k+d output+mlp+t+d output+mlp+k+t+d test2010 test2011 test2012 79.9 65.8 54.1 53.1 52.9 53.3 53.7 51.7 52.3 52.0 51.4 51.2 52.6 51.1 77.4 63.9 52.2 52.1 52.1 51.5 51.9 50.6 53.5 49.8 51.1 49.7 51.5 50.6 89.9 73.0 58.4"
N16-1149,P14-1006,0,0.0242849,"Missing"
N16-1149,D13-1176,0,0.0607725,"Missing"
N18-1115,W17-5526,0,0.0464322,"Missing"
N18-1115,D15-1181,0,0.0838655,"Missing"
N18-1115,N16-1037,1,0.926505,"code the 1277 Figure 2: CARNN for dialog. responses as shown in Figure 2, which depicts our architecture of CARNN for dialog. ∀l ∈ [1..L] : el = P osition Encoder(ylc ) (9) We then put a distribution over the candidate responses conditioned on the summarized dialog history hhis (Equation 10). P(y) = sof tmax(hThis ey1 , ..., hThis eyL ) 4.2 (10) Contextual language model Typically, language models operate at the sentence level, i.e., the sentences are treated independently. Several researchers have explored inter-sentence and inter-document level contextual information for language modelling (Ji et al., 2016a,b; Tran et al., 2016; Lau et al., 2017). Following Ji et al. (2016a,b), we investigate two types of contextual information: (i) the previous sentence context; and (ii) a latent variable capturing the connection information between sentences, such as discourse relation in the Penn Discourse Tree Bank dataset or Dialog Acts in the Switchboard dataset. Previous sentence context. The previous sentence (time-step t − 1) contextual information is encoded by a simplified version of the nCARNN, where the global context is absent. The final hidden vector of this sequence is then fed into the current"
N18-1115,P17-1033,0,0.0291302,"g. responses as shown in Figure 2, which depicts our architecture of CARNN for dialog. ∀l ∈ [1..L] : el = P osition Encoder(ylc ) (9) We then put a distribution over the candidate responses conditioned on the summarized dialog history hhis (Equation 10). P(y) = sof tmax(hThis ey1 , ..., hThis eyL ) 4.2 (10) Contextual language model Typically, language models operate at the sentence level, i.e., the sentences are treated independently. Several researchers have explored inter-sentence and inter-document level contextual information for language modelling (Ji et al., 2016a,b; Tran et al., 2016; Lau et al., 2017). Following Ji et al. (2016a,b), we investigate two types of contextual information: (i) the previous sentence context; and (ii) a latent variable capturing the connection information between sentences, such as discourse relation in the Penn Discourse Tree Bank dataset or Dialog Acts in the Switchboard dataset. Previous sentence context. The previous sentence (time-step t − 1) contextual information is encoded by a simplified version of the nCARNN, where the global context is absent. The final hidden vector of this sequence is then fed into the current recurrent computation (time-step t) as th"
N18-1115,E17-1001,0,0.204613,"understanding a piece of text may require far more than just extracting the information from that piece itself. If the piece of text is a paragraph of a document, the reader may have to consider it together with other paragraphs in the document and the topic of the document. To understand an utterance in a conversation, the utterance has to be put into the context of the conversation, which includes the goals of the participants and the dialog history. Hence the notion of context is an intrinsic component of language understanding. Inspired by recent works in dialog systems (Seo et al., 2017; Liu and Perez, 2017), we formalize the contextual sequence mapping problem as a sequence mapping problem with a strong controlling contextual element that regulates the flow of information. The system has two sources of signals: (i) the main text input, for example, the history utterance sequence in dialog systems or the sequence of words in language modelling; and (ii) the context signal, e.g., the previous utterance in a dialog system, the discourse information in contextual language modelling or the question in question answering. Our contribution in this work is two-fold. First, we propose a new family of rec"
N18-1115,miltsakaki-etal-2004-penn,0,0.0719031,"ot necessary. In the same spirit, our CARNN unit minimizes the use of non-linearity in the model to facilitate the ease of gradient flow. We also seek to keep the number of parameters to a minimum to improve trainability. We experiment with our models on a broad range of problems: dialog systems, contextual language modelling and question answering. Our systems outperform previous methods on several public datasets, which include the Babi Task 6 (Bordes and Weston, 2017) and the Frame dataset (Asri et al., 2017) for dialog, the Switchboard (Jurafsky et al., 1997) and Penn Discourse Tree Bank (Miltsakaki et al., 2004) for contextual language modelling, and the TrecQA 1274 Proceedings of NAACL-HLT 2018, pages 1274–1283 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics dataset (Wang et al., 2007) for question answering. We propose a different architecture for each task, but all models share the basic building block, the CARNN. 2 transition dynamic of RNN by removing the tanh non-linearity from the ˜cm . The equations for RAN are as follows: ˜cm = Wcx em Background and Notation gim = σ(Wih hm−1 + Wix em + bi ) Notation. As our paper describes several architectures with"
N18-1115,D17-1122,0,0.193232,"Missing"
N18-1115,I17-1057,1,0.797527,"d as follows. gum = σ(Wcu c + Weu em + bu ) gfm = σ(Wcf c + Wef em + bf ) hm = gum (gfm em ) + (1 − gum ) hm−1 (6) sCARNN can still be decomposed into a weighted sum of the sequence of input elements, and retains the parallel computation capability of the iCARNN. hM = guM gfM em + (1 − guM ) hM −1 = M X i=1 4 (gui gfi M Y (1 − guj )) ei j=i+1 (7) Summarizing the dialog history. The CARNN models take the embeddings of the sequence of utterances and produce the final representation hhis . We further enhance the output of the CARNN by adding the residual connection to the input (He et al., 2016; Tran et al., 2017), and the attention mechanism (Bahdanau et al., 2015) over the history. CARNN-based models for NLP problems In this section, we explain the details of our CARNN-based architectures for end-to-end dialog, language modelling and question answering. In each of these applications, one of the main design concerns is the choice of contextual information. As we will demonstrate in this section, the controlling context c can be derived from various sources: a sequence of words (dialog and question answering), a class variable (language modelling). Virtually any sources of strong information that can b"
N18-1115,N16-1090,1,0.796675,"2: CARNN for dialog. responses as shown in Figure 2, which depicts our architecture of CARNN for dialog. ∀l ∈ [1..L] : el = P osition Encoder(ylc ) (9) We then put a distribution over the candidate responses conditioned on the summarized dialog history hhis (Equation 10). P(y) = sof tmax(hThis ey1 , ..., hThis eyL ) 4.2 (10) Contextual language model Typically, language models operate at the sentence level, i.e., the sentences are treated independently. Several researchers have explored inter-sentence and inter-document level contextual information for language modelling (Ji et al., 2016a,b; Tran et al., 2016; Lau et al., 2017). Following Ji et al. (2016a,b), we investigate two types of contextual information: (i) the previous sentence context; and (ii) a latent variable capturing the connection information between sentences, such as discourse relation in the Penn Discourse Tree Bank dataset or Dialog Acts in the Switchboard dataset. Previous sentence context. The previous sentence (time-step t − 1) contextual information is encoded by a simplified version of the nCARNN, where the global context is absent. The final hidden vector of this sequence is then fed into the current recurrent computation"
N18-1115,D07-1003,0,0.710665,"periment with our models on a broad range of problems: dialog systems, contextual language modelling and question answering. Our systems outperform previous methods on several public datasets, which include the Babi Task 6 (Bordes and Weston, 2017) and the Frame dataset (Asri et al., 2017) for dialog, the Switchboard (Jurafsky et al., 1997) and Penn Discourse Tree Bank (Miltsakaki et al., 2004) for contextual language modelling, and the TrecQA 1274 Proceedings of NAACL-HLT 2018, pages 1274–1283 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics dataset (Wang et al., 2007) for question answering. We propose a different architecture for each task, but all models share the basic building block, the CARNN. 2 transition dynamic of RNN by removing the tanh non-linearity from the ˜cm . The equations for RAN are as follows: ˜cm = Wcx em Background and Notation gim = σ(Wih hm−1 + Wix em + bi ) Notation. As our paper describes several architectures with vastly different setups and input types, we introduce the following notation to maintain consistency and improve readability. First, the mth input to the recurrent unit will be denoted em . In language modelling, em is t"
N18-1115,P17-1062,0,0.0307967,"he second set of experiments, we use our end-to-end systems as “dialog managers”. The only difference compared to the end-to-end dialog setting is that the systems produce templatized responses instead of complete responses. Our motivation for this dialog manager setting is that in our preliminary experiments with the Babi dataset, we found out that many of the classification errors are due to very closely related responses, all of which fit the corresponding context. We argue that if we treat the systems as dialog managers, then we can delexicalize and group similar responses. Thus following Williams et al. (2017), we construct a templatized set of responses. For example, all the 1 Among the Babi tasks, we focus mainly on task 6, which is based on real human-machine interactions. The other five Babi datasets comprise synthetically generated data. 1279 Figure 4: CARNN for Question Answering. responses similar to “india house is in the west part of town” will be grouped into “ name is in the loc part of town”. The set of responses is reduced to 75 templatized responses. We call this new dataset “Babi reduced”.2 The third set of experiments is conducted on the Frame dataset. The general theme in this data"
N18-1123,P17-2054,0,0.046209,"proposed architectures by stacking up tasks on top of each other according to their linguistic level, eg from lower level tasks (POS tagging) to higher level tasks (parsing). In this approach, each task uses predicted annotations and hidden states of the lower-level tasks for making a better prediction. This is contrast to the approach taken in this paper where models with shared parameters are trained jointly on multiple tasks. More broadly, deep multitask learning has been used for various NLP problems, including graphbased parsing (Chen and Ye, 2011) and keyphrase boundary classification (Augenstein and Søgaard, 2017) . (Chen et al., 2017) has applied multi-task learning for Chinese word segmentation, and (Liu et al., 2017) applied it for text classification problem. Both of these works have used adversarial training to make sure the shared layer extract only common knowledge. MTL has been used effectively to learn from multimodal data. (Luong et al., 2016) has proposed MTL architectures for neural S EQ 2S EQ transduction for tasks including MT, image caption generation, and parsing. They fully share the encoders (many-to-one), the decoders (oneto-many), or some of the encoders and decoders (many-to-many)."
N18-1123,P17-1080,0,0.0227403,"nd Hieber, 2017) proposed a two-layer stacked decoder, which the bottom layer is trained on language modelling on the target language text. The next word is jointly predicted by the bottom layer language model and the top layer attentional RNN decoder. They reported only moderate improvements over the baseline and fall short against using synthetic parallel data. (Dalvi et al., 2017) investigated the amount of learned morphology and how it can be injected using MTL. Our method is related to what they call joint data-learning, where they share all of the S EQ 2S EQ components among the tasks. (Belinkov et al., 2017a; Shi et al., 2016; Belinkov et al., 2017b) investigate syntax/semantics phenomena learned as a byproduct of S EQ 2S EQ NMT training. We, in turn, investigate the effect of injecting syntax/semantic on learning NMT using MTL. The closet work to ours is (Niehues and Cho, 2017), which has made use of part-of-speech tagging and named-entity recognition tasks to improve NMT. They have used the attentional encoder-decoder with a shallow architecture, and share different parts eg the encoder, decoder, and attention. They report the best performance with fully sharing the encoder. In contrast, our a"
N18-1123,I17-1001,0,0.0439104,"Missing"
N18-1123,P17-1110,0,0.173706,"rough a fully connected layer followed by a softmax to predict the probability distribution over the tasks: PΘd (task id|hd ) ∼ softmax(Wd hd + bd ) hd := disLSTMs(shrRepΘmtl (x, y)) where disLSTMs denotes the discriminator LSTMs, shrRepΘmtl (x, y) denotes the representations in the shared layer of deep encoders and decoders in the MTL architecture, and Θd includes the disLSTMs parameters as well as {Wd , bd }. 1 When multiple layers are shared, we concatenate their hidden states at each time step, which is then input to the task discriminator’s LSTMs. 1358 Adversarial Objective. Inspired by (Chen et al., 2017), we add two additional terms to the MTL training objective in eqn 4. The first term is Ladv1 (Θd ) defined as: M X X En → Fr En → Fa En → vi Train 98,846 98,158 133,290 Dev 5,357 3,000 1,553 Test 5,357 4,000 1,268 Table 1: The statistics of bilingual corpora. log PΘd (m |disLSTMs(shrRepΘmtl (x, y))). m=0 (x,y)∈Dm Maximising the above objective over Θd ensures proper training of the discriminator to predict the identity of the task. The second term ensures that the parameters of the shared layers are trained so that they confuse the discriminator by maximising the entropy of its predicted dist"
N18-1123,N16-1102,1,0.840552,"Learning from the semantic parsing task enables the NMT model to pay attention to a meaning abstraction of the source sentence, in order to convey it to the target translation. We have made use of the Abstract Meaning Representation (AMR) corpus Release 2.0 (LDC2017T10), which pairs English sentences AMR meaning graphs. We linearise the AMR graphs, in order to convert semantic parsing as a S EQ 2S EQ transduction problem (Konstas et al., 2017). 5.3 Models and Baselines We have implemented the proposed multi-task learning architecture in C++ using DyNet (Neubig et al., 2017), on top of Mantis (Cohn et al., 2016) which is an implementation of the attentional S EQ 2S EQ NMT model in (?). In our multitask architecture, we do partial sharing of parameters, where the parameters of the top 2 stacked layers are shared among the encoders of the tasks. Moreover, we share the parameters of the top layer stacked decoder among the tasks. Source and target embedding tables are shared among the tasks, while the attention component is task-specific. 4 We compare against the following baselines: • Baseline 1: The vanila S EQ 2S EQ model without any multi-tasking. • Baseline 2: The multi-tasking architecture proposed"
N18-1123,I17-1015,0,0.0372745,"ata in the source language in a multitask learning framework by sharing encoder in the attentional encoderdecoder model. Their auxiliary task is to reorder the source text to make it close to the target language word order. (Domhan and Hieber, 2017) proposed a two-layer stacked decoder, which the bottom layer is trained on language modelling on the target language text. The next word is jointly predicted by the bottom layer language model and the top layer attentional RNN decoder. They reported only moderate improvements over the baseline and fall short against using synthetic parallel data. (Dalvi et al., 2017) investigated the amount of learned morphology and how it can be injected using MTL. Our method is related to what they call joint data-learning, where they share all of the S EQ 2S EQ components among the tasks. (Belinkov et al., 2017a; Shi et al., 2016; Belinkov et al., 2017b) investigate syntax/semantics phenomena learned as a byproduct of S EQ 2S EQ NMT training. We, in turn, investigate the effect of injecting syntax/semantic on learning NMT using MTL. The closet work to ours is (Niehues and Cho, 2017), which has made use of part-of-speech tagging and named-entity recognition tasks to imp"
N18-1123,D17-1158,0,0.0731232,"proach for compensating this requirement in bilingually scarce scenarios. Ideally, semantic and syntactic knowledge learned from existing linguistic resources provides NMT with proper inductive biases, leading to increased generalisation and better translation quality. Multi-task learning (MTL) is an effective approach to inject knowledge into a task, which is learned from other related tasks. Various recent works have attempted to improve NMT with an MTL approach (Peng et al., 2017; Liu et al., 2017; Zhang and Zong, 2016); however, they either do not make use of curated linguistic resources (Domhan and Hieber, 2017; Zhang and Zong, 2016), or their MTL architectures are restrictive yielding mediocre improvements (Niehues and Cho, 2017). The current research leaves open how to best leverage curated linguistic resources in a suitable MTL framework to improve NMT. In this paper, we make use of curated monolingual linguistic resources in the source side to improve NMT in bilingually scarce scenarios. More specifically, we scaffold the machine translation task on auxiliary tasks including semantic parsing, syntactic parsing, and named-entity recognition. This is achieved by casting the auxiliary tasks as sequ"
N18-1123,D17-1206,0,0.0373795,"entity recognition tasks to improve NMT. They have used the attentional encoder-decoder with a shallow architecture, and share different parts eg the encoder, decoder, and attention. They report the best performance with fully sharing the encoder. In contrast, our architecture uses partial sharing on deep stacked encoder and decoder components, and the results show that it is critical for NMT improvement in MTL. Furthermore, we propose adversarial training to prevent contamination of shared knowledge with task specific details. Taking another approach to MTL, (Søgaard and Goldberg, 2016) and (Hashimoto et al., 2017) have proposed architectures by stacking up tasks on top of each other according to their linguistic level, eg from lower level tasks (POS tagging) to higher level tasks (parsing). In this approach, each task uses predicted annotations and hidden states of the lower-level tasks for making a better prediction. This is contrast to the approach taken in this paper where models with shared parameters are trained jointly on multiple tasks. More broadly, deep multitask learning has been used for various NLP problems, including graphbased parsing (Chen and Ye, 2011) and keyphrase boundary classificat"
N18-1123,W17-3204,0,0.035948,"ctic knowledge into the translation model, which would otherwise require a large amount of training bitext. We empirically evaluate and show the effectiveness of our multi-task learning approach on three translation tasks: English-to-French, English-to-Farsi, and English-to-Vietnamese. 1 Introduction Neural Machine Translation (NMT) with attentional encoder-decoder architectures (Luong et al., 2015; Bahdanau et al., 2015) has revolutionised machine translation, and achieved state-of-the-art for several language pairs. However, NMT is notorious for its need for large amounts of bilingual data (Koehn and Knowles, 2017) to achieve reasonable translation quality. Leveraging existing monolingual resources is a potential approach for compensating this requirement in bilingually scarce scenarios. Ideally, semantic and syntactic knowledge learned from existing linguistic resources provides NMT with proper inductive biases, leading to increased generalisation and better translation quality. Multi-task learning (MTL) is an effective approach to inject knowledge into a task, which is learned from other related tasks. Various recent works have attempted to improve NMT with an MTL approach (Peng et al., 2017; Liu et a"
N18-1123,P17-1014,0,0.045377,"constituency trees, in order to turn syntactic parsing as a S EQ 2S EQ transduction (Vinyals et al., 2015). Semantic Parsing. A good translation should preserve the meaning. Learning from the semantic parsing task enables the NMT model to pay attention to a meaning abstraction of the source sentence, in order to convey it to the target translation. We have made use of the Abstract Meaning Representation (AMR) corpus Release 2.0 (LDC2017T10), which pairs English sentences AMR meaning graphs. We linearise the AMR graphs, in order to convert semantic parsing as a S EQ 2S EQ transduction problem (Konstas et al., 2017). 5.3 Models and Baselines We have implemented the proposed multi-task learning architecture in C++ using DyNet (Neubig et al., 2017), on top of Mantis (Cohn et al., 2016) which is an implementation of the attentional S EQ 2S EQ NMT model in (?). In our multitask architecture, we do partial sharing of parameters, where the parameters of the top 2 stacked layers are shared among the encoders of the tasks. Moreover, we share the parameters of the top layer stacked decoder among the tasks. Source and target embedding tables are shared among the tasks, while the attention component is task-specifi"
N18-1123,P17-1001,0,0.100103,"Missing"
N18-1123,2015.iwslt-evaluation.11,0,0.251728,"Missing"
N18-1123,D15-1166,0,0.0656135,"ning approach. More specifically, we scaffold the machine translation task on auxiliary tasks including semantic parsing, syntactic parsing, and named-entity recognition. This effectively injects semantic and/or syntactic knowledge into the translation model, which would otherwise require a large amount of training bitext. We empirically evaluate and show the effectiveness of our multi-task learning approach on three translation tasks: English-to-French, English-to-Farsi, and English-to-Vietnamese. 1 Introduction Neural Machine Translation (NMT) with attentional encoder-decoder architectures (Luong et al., 2015; Bahdanau et al., 2015) has revolutionised machine translation, and achieved state-of-the-art for several language pairs. However, NMT is notorious for its need for large amounts of bilingual data (Koehn and Knowles, 2017) to achieve reasonable translation quality. Leveraging existing monolingual resources is a potential approach for compensating this requirement in bilingually scarce scenarios. Ideally, semantic and syntactic knowledge learned from existing linguistic resources provides NMT with proper inductive biases, leading to increased generalisation and better translation quality. Mult"
N18-1123,J93-2004,0,0.0608274,"d-entities. Through the NER task, 1359 2 www.sobhe.ir/hazm the model hopefully learns the skill to recognize named entities. Speculatively, it would then enables leaning translation patterns by masking out named entities. The NER data comes from the CONLL shared task.3 Syntactic Parsing. This task enables NMT to learn the phrase structure of the input sentence, which would then be useful in better re-orderings. This would be most useful for language pairs with high syntactic divergence. The parsing data comes from the Penn Tree Bank with the standard split for training, development, and test (Marcus et al., 1993). We linearise the constituency trees, in order to turn syntactic parsing as a S EQ 2S EQ transduction (Vinyals et al., 2015). Semantic Parsing. A good translation should preserve the meaning. Learning from the semantic parsing task enables the NMT model to pay attention to a meaning abstraction of the source sentence, in order to convey it to the target translation. We have made use of the Abstract Meaning Representation (AMR) corpus Release 2.0 (LDC2017T10), which pairs English sentences AMR meaning graphs. We linearise the AMR graphs, in order to convert semantic parsing as a S EQ 2S EQ tra"
N18-1123,P17-1117,0,0.0479867,"(Chen et al., 2017) has applied multi-task learning for Chinese word segmentation, and (Liu et al., 2017) applied it for text classification problem. Both of these works have used adversarial training to make sure the shared layer extract only common knowledge. MTL has been used effectively to learn from multimodal data. (Luong et al., 2016) has proposed MTL architectures for neural S EQ 2S EQ transduction for tasks including MT, image caption generation, and parsing. They fully share the encoders (many-to-one), the decoders (oneto-many), or some of the encoders and decoders (many-to-many). (Pasunuru and Bansal, 2017) have made use of an MTL approach to improve video captioning with auxiliary tasks including video prediction and logical language entailment based on a many-to-many architecture. 7 Conclusions and Future Work We have presented an approach to improve NMT in bilingually scarce scenarios, by leveraging curated linguistic resources in the source, including semantic parsing, syntactic parsing, and named entity recognition. This is achieved via an effective MTL architecture, based on deep stacked en1363 coders and decoders, to share common knowledge among the MT and auxiliary tasks. Our experimenta"
N18-1123,P17-1186,0,0.0858713,"Missing"
N18-1123,P16-1162,0,0.0632963,"from the translation task in IWSLT 2015, and we use the preprocessed version provided by (Luong and Manning, 2015). The sentence pairs in which at least one of their sentences had more than 300 units (after applying BPE) are removed. “tst2012” and “tst2013” parts are used for validation and test sets, respectively. Bilingual Corpora We use three language-pairs, translating from English to French, Farsi, and Vietnamese. We have chosen these languages to analyse the effect of multi-task learning on languages with different underlying linguistic structures. The sentences are segmented using BPE (Sennrich et al., 2016) on the union of source and target vocabularies for English-French and English-Vietnamese. For English-Farsi, BPE is performed using separate vocabularies due to the disjoint alphabets. We use a special <U NK> token to replace unknown BPE units in the test and development sets. Table 1 show some statistics about the bilingual corpora. Further details about the corpora and their pre-processing is as follows: • The English-French corpus is a random subset of EuroParlv7 as distributed to WMT2014. Sentence pairs in which either the source 5.2 Auxiliary Tasks We have chosen the following auxiliary"
N18-1123,D16-1159,0,0.0198997,"d a two-layer stacked decoder, which the bottom layer is trained on language modelling on the target language text. The next word is jointly predicted by the bottom layer language model and the top layer attentional RNN decoder. They reported only moderate improvements over the baseline and fall short against using synthetic parallel data. (Dalvi et al., 2017) investigated the amount of learned morphology and how it can be injected using MTL. Our method is related to what they call joint data-learning, where they share all of the S EQ 2S EQ components among the tasks. (Belinkov et al., 2017a; Shi et al., 2016; Belinkov et al., 2017b) investigate syntax/semantics phenomena learned as a byproduct of S EQ 2S EQ NMT training. We, in turn, investigate the effect of injecting syntax/semantic on learning NMT using MTL. The closet work to ours is (Niehues and Cho, 2017), which has made use of part-of-speech tagging and named-entity recognition tasks to improve NMT. They have used the attentional encoder-decoder with a shallow architecture, and share different parts eg the encoder, decoder, and attention. They report the best performance with fully sharing the encoder. In contrast, our architecture uses pa"
N18-1123,P16-2038,0,0.028418,"part-of-speech tagging and named-entity recognition tasks to improve NMT. They have used the attentional encoder-decoder with a shallow architecture, and share different parts eg the encoder, decoder, and attention. They report the best performance with fully sharing the encoder. In contrast, our architecture uses partial sharing on deep stacked encoder and decoder components, and the results show that it is critical for NMT improvement in MTL. Furthermore, we propose adversarial training to prevent contamination of shared knowledge with task specific details. Taking another approach to MTL, (Søgaard and Goldberg, 2016) and (Hashimoto et al., 2017) have proposed architectures by stacking up tasks on top of each other according to their linguistic level, eg from lower level tasks (POS tagging) to higher level tasks (parsing). In this approach, each task uses predicted annotations and hidden states of the lower-level tasks for making a better prediction. This is contrast to the approach taken in this paper where models with shared parameters are trained jointly on multiple tasks. More broadly, deep multitask learning has been used for various NLP problems, including graphbased parsing (Chen and Ye, 2011) and k"
N18-1123,tiedemann-2012-parallel,0,0.0212969,"ed distribution over the task identities. That is, we add the term Ladv2 (Θmtl ) to the training objective defined as: M X X m=0 (x,y)∈Dm or the target has length more than 80 (before applying BPE) have been removed. The BPE is performed with a 30k total vocabulary size. The “news-test2012” and “news-test2013” portions are used for validation and test sets, respectively. • The English-Farsi corpus is assembled from all the parallel news text in LDC2016E93 Farsi Representative Language Pack from the Linguistic Data Consortium, combined with English-Farsi parallel subtitles from the TED corpus (Tiedemann, 2012). Since the TED subtitles are user-contributed, this text contained considerable variation in the encoding of its Perso-Arabic characters. To address this issue, we have normalized the corpus using the Hazm toolkit2 . Sentence pairs in which one of the sentences has more than 80 (before applying BPE) are removed, and BPE is performed with a 30k vocabulary size. Random subsets of this corpus (3k and 4k sentences each) are held out as validation and test sets, respectively.   H PΘd (. |disLSTMs(shrRepΘmtl (x, y))) where H[.] is the entropy of a distribution. In summary, the adversarial trainin"
N18-1123,D16-1160,0,0.133324,"e reasonable translation quality. Leveraging existing monolingual resources is a potential approach for compensating this requirement in bilingually scarce scenarios. Ideally, semantic and syntactic knowledge learned from existing linguistic resources provides NMT with proper inductive biases, leading to increased generalisation and better translation quality. Multi-task learning (MTL) is an effective approach to inject knowledge into a task, which is learned from other related tasks. Various recent works have attempted to improve NMT with an MTL approach (Peng et al., 2017; Liu et al., 2017; Zhang and Zong, 2016); however, they either do not make use of curated linguistic resources (Domhan and Hieber, 2017; Zhang and Zong, 2016), or their MTL architectures are restrictive yielding mediocre improvements (Niehues and Cho, 2017). The current research leaves open how to best leverage curated linguistic resources in a suitable MTL framework to improve NMT. In this paper, we make use of curated monolingual linguistic resources in the source side to improve NMT in bilingually scarce scenarios. More specifically, we scaffold the machine translation task on auxiliary tasks including semantic parsing, syntactic"
N18-1123,W17-4708,0,0.377077,"from existing linguistic resources provides NMT with proper inductive biases, leading to increased generalisation and better translation quality. Multi-task learning (MTL) is an effective approach to inject knowledge into a task, which is learned from other related tasks. Various recent works have attempted to improve NMT with an MTL approach (Peng et al., 2017; Liu et al., 2017; Zhang and Zong, 2016); however, they either do not make use of curated linguistic resources (Domhan and Hieber, 2017; Zhang and Zong, 2016), or their MTL architectures are restrictive yielding mediocre improvements (Niehues and Cho, 2017). The current research leaves open how to best leverage curated linguistic resources in a suitable MTL framework to improve NMT. In this paper, we make use of curated monolingual linguistic resources in the source side to improve NMT in bilingually scarce scenarios. More specifically, we scaffold the machine translation task on auxiliary tasks including semantic parsing, syntactic parsing, and named-entity recognition. This is achieved by casting the auxiliary tasks as sequence-to-sequence (S EQ 2S EQ) transduction tasks, and tie the parameters of their encoders and/or decoders with those of t"
N18-1123,P02-1040,0,0.100983,"t has 200 dimensions. For training, we used Adam algorithm (Kingma and Ba, 2014) with the initial learning rate of 0.003 for all of the tasks. Learning rates are halved when the performance on the corresponding dev set decreased. In order to speed-up the training, we use mini-batching with the size of 32. Dropout rates for both encoder and decoder are set to 0.5, and models are trained for 50 epochs where the best models is selected based on the perplexity on the dev set. λ for the adversarial training is set to 0.5. Once trained, the NMT model translates using the greedy search. We use BLEU (Papineni et al., 2002) to measure translation quality. 6 5.4 Results Table 2 reports the BLEU scores and perplexities for the baseline and our proposed method on the three aforementioned translation tasks. It can be seen that the performance of multi-task learning models are better than Baseline 1 (only MT task). This confirms that adding auxiliary tasks helps to increase the performance of the machine translation task. As expected, the effect of different tasks are not similar across the language pairs, possibly due to the following reasons: (i) these translation tasks datasets come from different domains so they"
N19-1313,N18-1118,0,0.0945833,"(Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall into the former category, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedemann and Scherrer, 2017; Voita et al., 2018); one previous sentence both in source and target (Bawden et al., 2018); more than one previous source sentence (Wang et al., 2017; Zhang et al., 2018); or a few previous source and target sentences (Miculicich et al., 2018). Apart from fixing the context length, there are few works which use cache-based memories to store contextual information (Tu et al., 2018; Kuang et al., 2018) and use that to improve the MT system performance. A recent work (Maruf et al., 2018) reports promising results when using the complete history for translating online conversations. For the offline setting, however, there is only one work that effectively uses the full documentcontext"
N19-1313,2012.eamt-1.60,0,0.215051,"both encoder and decoder as it would have redundant information from the source (the context incorporated in the decoder is bilingual), in addition to increasing the complexity of the model. 3095 Domain TED News Europarl #Sentences 0.21M/9K/2.3K 0.24M/2K/3K 1.67M/3.6K/5.1K Document length 120.89/96.42/98.74 38.93/26.78/19.35 14.14/14.95/14.06 Table 1: Training/development/test corpora statistics: number of sentences (K stands for thousands and M for millions), and average document length (in sentences). in genre, style and level of formality: • TED This corpus is from the IWSLT 2017 MT track (Cettolo et al., 2012) and contains transcripts of TED talks aligned at sentence level. Each talk is considered to be a document. We combine tst2016-2017 into the test set and the rest are used for development. • News-Commentary We obtain the sentencealigned document-delimited News Commentary v11 corpus for training.6 The WMT’16 newstest2015 and news-test2016 are used for development and testing, respectively. Figure 3: Decoder-side context integration. representations in that sentence. The queries Qw , Qs are linear transformations of the output of the Lth encoder layer which are then matched with the correspondin"
N19-1313,W14-4012,0,0.130924,"Missing"
N19-1313,P11-2031,0,0.0690388,"memory.10 7 https://github.com/duyvuleo/Transformer-DyNet The code is available at https://github.com/ sameenmaruf/selective-attn 8 9 We found this configuration to be much more stable than using 6 layers with almost no difference in performance as reported by Xia et al. (2018). 10 The experiments can also be run on GPUs with 1012GBs of memory by reducing the batch size at the expense Evaluation Metrics For evaluation, we use BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) scores on tokenised text, and measure statistical significance with respect to the baselines, p < 0.05 (Clark et al., 2011). 4.2 Main Results We divide our experiments into two parts: offline and online document MT. Offline Document MT From the scores of the two context-agnostic baselines in Table 2, we can see that the Transformer beats the RNNSearch model in all cases by atleast +2.5 BLEU and +2.1 Meteor scores showing that our hyperparameter choice for the Transformer is indeed effective. For the Encoder Context integration, our Hierarchical Attention models perform the (near) best for News and Europarl datasets with +1.98 and +1 BLEU and +1.99 and +0.82 Meteor improvements with respect to the Transformer. For"
N19-1313,N16-1102,1,0.857967,"fline document MT, we have two context-agnostic baselines: (i) a modified version of RNNSearch (Bahdanau et al., 2015), which incorporates dropout on the output layer and improves the attention model by feeding the previously generated word, and (ii) the stateof-the-art Transformer architecture. For the online case, we again have the Transformer as a contextagnostic baseline and two context-aware baselines (Zhang et al., 2018; Miculicich et al., 2018). All models are implemented in C++ using DyNet (Neubig et al., 2017). For RNNSearch, we modify the sentence-based NMT implementation in mantis (Cohn et al., 2016). The encoder is a single layer bidirectional GRU (Cho et al., 2014) and 3096 6 www.casmacat.eu/corpus/news-commentary.html Integration into Encoder TED News Europarl Model BLEU Meteor BLEU Meteor BLEU Meteor RNNSearch 19.24 40.81 16.51 36.79 26.26 44.14 Transformer 23.28 44.17 22.78 42.19 28.72 46.22 +Attention, sentence 24.47 45.25 24.78 43.90 29.60 46.98 word 24.55 44.89 24.55 43.75 29.63 46.94 24.23 44.81 24.76 44.10 29.72 47.03 +H-Attention, sparse-soft sparse-sparse 24.27 45.07 24.66 44.18 29.64 47.04 Integration into Decoder TED News Europarl BLEU Meteor BLEU Meteor BLEU Meteor 19.24 40"
N19-1313,W07-0734,0,0.0894651,"use Iterative Decoding only when using the bilingual context. All experiments are run on a single Nvidia P100 GPU with 16GBs of memory.10 7 https://github.com/duyvuleo/Transformer-DyNet The code is available at https://github.com/ sameenmaruf/selective-attn 8 9 We found this configuration to be much more stable than using 6 layers with almost no difference in performance as reported by Xia et al. (2018). 10 The experiments can also be run on GPUs with 1012GBs of memory by reducing the batch size at the expense Evaluation Metrics For evaluation, we use BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) scores on tokenised text, and measure statistical significance with respect to the baselines, p < 0.05 (Clark et al., 2011). 4.2 Main Results We divide our experiments into two parts: offline and online document MT. Offline Document MT From the scores of the two context-agnostic baselines in Table 2, we can see that the Transformer beats the RNNSearch model in all cases by atleast +2.5 BLEU and +2.1 Meteor scores showing that our hyperparameter choice for the Transformer is indeed effective. For the Encoder Context integration, our Hierarchical Attention models perform the (near) best for New"
N19-1313,D11-1084,0,0.11888,"eater commitment and sincerity in eliminating the obstacles to the return of Croatia ’s Serbian population . sj−4 : by signing a border arbitration agreement with its neighbour Slovenia , the new Croatian Government has not only eliminated an obstacle to the negotiating process , but has also paved the way for the resolution of other issues . Table 7: Example of pronoun disambiguation. Context sentences are ordered in decreasing probability mass. Conventional Document-level MT These can further be classified into two main categories. The first, which use cache-based memories (Tiedemann, 2010; Gong et al., 2011) and the second, which focus on specific discourse phenomema like anaphora (Hardmeier and Federico, 2010), lexical cohesion (Xiong et al., 2013; Gong et al., 2015; Mascarell, 2017) and coreference (Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall into the former cate"
N19-1313,P18-2059,1,0.861235,"Missing"
N19-1313,W15-2504,0,0.0171576,"its neighbour Slovenia , the new Croatian Government has not only eliminated an obstacle to the negotiating process , but has also paved the way for the resolution of other issues . Table 7: Example of pronoun disambiguation. Context sentences are ordered in decreasing probability mass. Conventional Document-level MT These can further be classified into two main categories. The first, which use cache-based memories (Tiedemann, 2010; Gong et al., 2011) and the second, which focus on specific discourse phenomema like anaphora (Hardmeier and Federico, 2010), lexical cohesion (Xiong et al., 2013; Gong et al., 2015; Mascarell, 2017) and coreference (Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall into the former category, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedemann and Scherrer, 2017; Voita et al., 2018); one previous s"
N19-1313,2010.iwslt-papers.10,0,0.158973,"n population . sj−4 : by signing a border arbitration agreement with its neighbour Slovenia , the new Croatian Government has not only eliminated an obstacle to the negotiating process , but has also paved the way for the resolution of other issues . Table 7: Example of pronoun disambiguation. Context sentences are ordered in decreasing probability mass. Conventional Document-level MT These can further be classified into two main categories. The first, which use cache-based memories (Tiedemann, 2010; Gong et al., 2011) and the second, which focus on specific discourse phenomema like anaphora (Hardmeier and Federico, 2010), lexical cohesion (Xiong et al., 2013; Gong et al., 2015; Mascarell, 2017) and coreference (Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall into the former category, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedeman"
N19-1313,P18-1118,1,0.861108,"ng an internship at Unbabel. basis, thus ignoring the long-range dependencies which may be useful when it comes to translating discourse phenomena. More recently, context-aware NMT has been gaining significant traction from the MT community with majority of works coming out in the past two years. Most of these focus on using a few previous sentences as context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a structured manner using word-level and sentencelevel abstractions; yet, it uses a limited number of past source and target sentences as context and is not scalable to entire document. In this work, we propose a"
N19-1313,2005.mtsummit-papers.11,0,0.13058,"the values are composed of the hidden representations of the target words, both from the last decoder layer. Again the keys Kw and Ks are either for individual target words or target sentences, and same goes for Vw and Vs . The queries Qw , Qs for the Context Layer come from the Source Attention sub-layer in the Lth layer of the decoder (Figure 3). 4 4.1 Experiments Setup Datasets We conduct experiments for English→German on three different domains: TED talks, News-Commentary and Europarl. These datasets are chosen based on their variance • Europarl This dataset is extracted from Europarl v7 (Koehn, 2005). The source and target sentences are aligned using the links provided by Tiedemann (2012). Following Maruf and Haffari (2018), we use the SPEAKER tag as the document delimiter. Documents longer than 5 sentences are kept and the resulting corpus is randomly split into training, dev and test sets. The corpora statistics are provided in Table 1. All datasets are tokenised and truecased using the Moses toolkit (Koehn et al., 2007), and split into subword units using a joint BPE model with 30K merge operations (Sennrich et al., 2016). Models and Baselines For offline document MT, we have two conte"
N19-1313,P07-2045,0,0.015997,"n on three different domains: TED talks, News-Commentary and Europarl. These datasets are chosen based on their variance • Europarl This dataset is extracted from Europarl v7 (Koehn, 2005). The source and target sentences are aligned using the links provided by Tiedemann (2012). Following Maruf and Haffari (2018), we use the SPEAKER tag as the document delimiter. Documents longer than 5 sentences are kept and the resulting corpus is randomly split into training, dev and test sets. The corpora statistics are provided in Table 1. All datasets are tokenised and truecased using the Moses toolkit (Koehn et al., 2007), and split into subword units using a joint BPE model with 30K merge operations (Sennrich et al., 2016). Models and Baselines For offline document MT, we have two context-agnostic baselines: (i) a modified version of RNNSearch (Bahdanau et al., 2015), which incorporates dropout on the output layer and improves the attention model by feeding the previously generated word, and (ii) the stateof-the-art Transformer architecture. For the online case, we again have the Transformer as a contextagnostic baseline and two context-aware baselines (Zhang et al., 2018; Miculicich et al., 2018). All models"
N19-1313,C18-1050,0,0.109805,"only, and offline—use both past and future contexts. Most works fall into the former category, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedemann and Scherrer, 2017; Voita et al., 2018); one previous sentence both in source and target (Bawden et al., 2018); more than one previous source sentence (Wang et al., 2017; Zhang et al., 2018); or a few previous source and target sentences (Miculicich et al., 2018). Apart from fixing the context length, there are few works which use cache-based memories to store contextual information (Tu et al., 2018; Kuang et al., 2018) and use that to improve the MT system performance. A recent work (Maruf et al., 2018) reports promising results when using the complete history for translating online conversations. For the offline setting, however, there is only one work that effectively uses the full documentcontext on both source and target-side using memory networks (Maruf and Haffari, 2018). The debate in document-level NMT today is mostly about how much of the previous context to use and there has been no comparison between the online and offline setting except using only one previous and following sentence (Voita et al"
N19-1313,D18-1512,0,0.131459,"Missing"
N19-1313,W18-6311,1,0.852023,"ategory, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedemann and Scherrer, 2017; Voita et al., 2018); one previous sentence both in source and target (Bawden et al., 2018); more than one previous source sentence (Wang et al., 2017; Zhang et al., 2018); or a few previous source and target sentences (Miculicich et al., 2018). Apart from fixing the context length, there are few works which use cache-based memories to store contextual information (Tu et al., 2018; Kuang et al., 2018) and use that to improve the MT system performance. A recent work (Maruf et al., 2018) reports promising results when using the complete history for translating online conversations. For the offline setting, however, there is only one work that effectively uses the full documentcontext on both source and target-side using memory networks (Maruf and Haffari, 2018). The debate in document-level NMT today is mostly about how much of the previous context to use and there has been no comparison between the online and offline setting except using only one previous and following sentence (Voita et al., 2018). Sparse Attention Sparse attention and its constrained variants have been use"
N19-1313,W17-4813,0,0.0183569,"nia , the new Croatian Government has not only eliminated an obstacle to the negotiating process , but has also paved the way for the resolution of other issues . Table 7: Example of pronoun disambiguation. Context sentences are ordered in decreasing probability mass. Conventional Document-level MT These can further be classified into two main categories. The first, which use cache-based memories (Tiedemann, 2010; Gong et al., 2011) and the second, which focus on specific discourse phenomema like anaphora (Hardmeier and Federico, 2010), lexical cohesion (Xiong et al., 2013; Gong et al., 2015; Mascarell, 2017) and coreference (Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall into the former category, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedemann and Scherrer, 2017; Voita et al., 2018); one previous sentence both in so"
N19-1313,D18-1325,0,0.151064,"ever, all of these models share the same inherent problem: the translation is still performed on a sentence-by-sentence ∗ Work initiated during an internship at Unbabel. basis, thus ignoring the long-range dependencies which may be useful when it comes to translating discourse phenomena. More recently, context-aware NMT has been gaining significant traction from the MT community with majority of works coming out in the past two years. Most of these focus on using a few previous sentences as context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a structured manner using word-level and sentencelevel abstractions; yet"
N19-1313,W17-1505,0,0.0481647,"Missing"
N19-1313,W18-6307,0,0.137183,"Missing"
N19-1313,P02-1040,0,0.105119,"escribed in §2.2. For inference, we use Iterative Decoding only when using the bilingual context. All experiments are run on a single Nvidia P100 GPU with 16GBs of memory.10 7 https://github.com/duyvuleo/Transformer-DyNet The code is available at https://github.com/ sameenmaruf/selective-attn 8 9 We found this configuration to be much more stable than using 6 layers with almost no difference in performance as reported by Xia et al. (2018). 10 The experiments can also be run on GPUs with 1012GBs of memory by reducing the batch size at the expense Evaluation Metrics For evaluation, we use BLEU (Papineni et al., 2002) and Meteor (Lavie and Agarwal, 2007) scores on tokenised text, and measure statistical significance with respect to the baselines, p < 0.05 (Clark et al., 2011). 4.2 Main Results We divide our experiments into two parts: offline and online document MT. Offline Document MT From the scores of the two context-agnostic baselines in Table 2, we can see that the Transformer beats the RNNSearch model in all cases by atleast +2.5 BLEU and +2.1 Meteor scores showing that our hyperparameter choice for the Transformer is indeed effective. For the Encoder Context integration, our Hierarchical Attention m"
N19-1313,P16-1162,0,0.400895,"d on their variance • Europarl This dataset is extracted from Europarl v7 (Koehn, 2005). The source and target sentences are aligned using the links provided by Tiedemann (2012). Following Maruf and Haffari (2018), we use the SPEAKER tag as the document delimiter. Documents longer than 5 sentences are kept and the resulting corpus is randomly split into training, dev and test sets. The corpora statistics are provided in Table 1. All datasets are tokenised and truecased using the Moses toolkit (Koehn et al., 2007), and split into subword units using a joint BPE model with 30K merge operations (Sennrich et al., 2016). Models and Baselines For offline document MT, we have two context-agnostic baselines: (i) a modified version of RNNSearch (Bahdanau et al., 2015), which incorporates dropout on the output layer and improves the attention model by feeding the previously generated word, and (ii) the stateof-the-art Transformer architecture. For the online case, we again have the Transformer as a contextagnostic baseline and two context-aware baselines (Zhang et al., 2018; Miculicich et al., 2018). All models are implemented in C++ using DyNet (Neubig et al., 2017). For RNNSearch, we modify the sentence-based N"
N19-1313,W10-2602,0,0.0487561,"d finances and greater commitment and sincerity in eliminating the obstacles to the return of Croatia ’s Serbian population . sj−4 : by signing a border arbitration agreement with its neighbour Slovenia , the new Croatian Government has not only eliminated an obstacle to the negotiating process , but has also paved the way for the resolution of other issues . Table 7: Example of pronoun disambiguation. Context sentences are ordered in decreasing probability mass. Conventional Document-level MT These can further be classified into two main categories. The first, which use cache-based memories (Tiedemann, 2010; Gong et al., 2011) and the second, which focus on specific discourse phenomema like anaphora (Hardmeier and Federico, 2010), lexical cohesion (Xiong et al., 2013; Gong et al., 2015; Mascarell, 2017) and coreference (Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall"
N19-1313,tiedemann-2012-parallel,0,0.0352592,"he last decoder layer. Again the keys Kw and Ks are either for individual target words or target sentences, and same goes for Vw and Vs . The queries Qw , Qs for the Context Layer come from the Source Attention sub-layer in the Lth layer of the decoder (Figure 3). 4 4.1 Experiments Setup Datasets We conduct experiments for English→German on three different domains: TED talks, News-Commentary and Europarl. These datasets are chosen based on their variance • Europarl This dataset is extracted from Europarl v7 (Koehn, 2005). The source and target sentences are aligned using the links provided by Tiedemann (2012). Following Maruf and Haffari (2018), we use the SPEAKER tag as the document delimiter. Documents longer than 5 sentences are kept and the resulting corpus is randomly split into training, dev and test sets. The corpora statistics are provided in Table 1. All datasets are tokenised and truecased using the Moses toolkit (Koehn et al., 2007), and split into subword units using a joint BPE model with 30K merge operations (Sennrich et al., 2016). Models and Baselines For offline document MT, we have two context-agnostic baselines: (i) a modified version of RNNSearch (Bahdanau et al., 2015), which"
N19-1313,Q18-1029,0,0.218452,"oving the translation quality (Vaswani et al., 2017). However, all of these models share the same inherent problem: the translation is still performed on a sentence-by-sentence ∗ Work initiated during an internship at Unbabel. basis, thus ignoring the long-range dependencies which may be useful when it comes to translating discourse phenomena. More recently, context-aware NMT has been gaining significant traction from the MT community with majority of works coming out in the past two years. Most of these focus on using a few previous sentences as context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a stru"
N19-1313,P18-1117,0,0.2424,"tion quality (Vaswani et al., 2017). However, all of these models share the same inherent problem: the translation is still performed on a sentence-by-sentence ∗ Work initiated during an internship at Unbabel. basis, thus ignoring the long-range dependencies which may be useful when it comes to translating discourse phenomena. More recently, context-aware NMT has been gaining significant traction from the MT community with majority of works coming out in the past two years. Most of these focus on using a few previous sentences as context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a structured manner using"
N19-1313,D17-1301,0,0.160643,"e effective in improving the translation quality (Vaswani et al., 2017). However, all of these models share the same inherent problem: the translation is still performed on a sentence-by-sentence ∗ Work initiated during an internship at Unbabel. basis, thus ignoring the long-range dependencies which may be useful when it comes to translating discourse phenomena. More recently, context-aware NMT has been gaining significant traction from the MT community with majority of works coming out in the past two years. Most of these focus on using a few previous sentences as context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual info"
N19-1313,D13-1163,0,0.0373133,"tion agreement with its neighbour Slovenia , the new Croatian Government has not only eliminated an obstacle to the negotiating process , but has also paved the way for the resolution of other issues . Table 7: Example of pronoun disambiguation. Context sentences are ordered in decreasing probability mass. Conventional Document-level MT These can further be classified into two main categories. The first, which use cache-based memories (Tiedemann, 2010; Gong et al., 2011) and the second, which focus on specific discourse phenomema like anaphora (Hardmeier and Federico, 2010), lexical cohesion (Xiong et al., 2013; Gong et al., 2015; Mascarell, 2017) and coreference (Miculicich Werlen and Popescu-Belis, 2017) to name a few. Most of these approaches are, however, restrictive as they mostly involve using handcrafted features similar to the conventional MT approaches. Document-level Neural MT The works here can again be divided into two categories: online— use previous context only, and offline—use both past and future contexts. Most works fall into the former category, with those that use only a single 3099 previous sentence in the source (Jean et al., 2017; Tiedemann and Scherrer, 2017; Voita et al., 20"
N19-1313,N16-1174,0,0.0618912,"s context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a structured manner using word-level and sentencelevel abstractions; yet, it uses a limited number of past source and target sentences as context and is not scalable to entire document. In this work, we propose a selective attention approach to first selectively focus on relevant sentences in the global document-context and then attend to key words in those sentences, while ignoring the rest.1 Towards this goal, we use sparse attention, enabling an efficient and scalable use of the context. The intuition behind this is the way humans translate a sentence"
N19-1313,D18-1049,0,0.142625,"i et al., 2017). However, all of these models share the same inherent problem: the translation is still performed on a sentence-by-sentence ∗ Work initiated during an internship at Unbabel. basis, thus ignoring the long-range dependencies which may be useful when it comes to translating discourse phenomena. More recently, context-aware NMT has been gaining significant traction from the MT community with majority of works coming out in the past two years. Most of these focus on using a few previous sentences as context (Jean et al., 2017; Wang et al., 2017; Tu et al., 2018; Voita et al., 2018; Zhang et al., 2018; Miculicich et al., 2018) and neglect the rest of the document. Only one existing work has endeavoured to consider the full document context (Maruf and Haffari, 2018), thus proposing a more generalised approach to document-level NMT. However, the model is restrictive as the document-level attention computed is sentence-based and static (computed only once for the sentence being translated). A more recent work (Miculicich et al., 2018) proposes to use a hierarchical attention network (HAN) (Yang et al., 2016) to model the contextual information in a structured manner using word-level and sente"
N19-1313,W17-4811,0,0.15642,"Missing"
P07-1004,J93-2003,0,0.0190284,"eights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on a development corpus which we will call dev1 in this paper. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm. After the main decoding step, rescoring with additional models is performed. The system generates a 5,000-best list of alternative translations for each source sentence. These lists are rescored with the following models: (a) the different models used in the decoder which are described above, (b) two different features based on IBM Model 1 (Brown et al., 1993), (c) posterior probabilities for words, phrases, n-grams, and sentence length (Zens and Ney, 2006; Ueffing and Ney, 2007), all calculated over the N best list and using the sentence probabilities which the baseline system assigns to the translation hypotheses. The weights of these additional models and of the decoder models are again optimized to maximize BLEU score. This is performed on a second development corpus, dev2. 3 The Framework 3.1 The Algorithm Our transductive learning algorithm, Algorithm 1, is inspired by the Yarowsky algorithm (Yarowsky, 1995; Abney, 2004). The algorithm works"
P07-1004,P04-1023,0,0.0375155,"Missing"
P07-1004,2002.tmi-tutorials.2,0,0.0466643,"3.2 Table 5: Translation quality using an additional phrase table trained on monolingual Chinese news data. Selection step using threshold on confidence scores. NIST Chinese–English. word alignment. Experiments showed that putting a large weight on the model trained on labeled data performs best. Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. The word alignments are used to train a standard phrasebased SMT system, resulting in increased translation quality . In (Callison-Burch, 2002) co-training is applied to MT. This approach requires several source languages which are sentence-aligned with each other and all translate into the same target language. One language pair creates data for another language pair and can be naturally used in a (Blum and Mitchell, 1998)-style co-training algorithm. Experiments on the EuroParl corpus show a decrease in WER. However, the selection algorithm applied there is actually supervised because it takes the reference translation into account. Moreover, when the algorithm is run long enough, large amounts of co-trained data injected too much"
P07-1004,P06-1097,0,0.0197031,"Missing"
P07-1004,niessen-etal-2000-evaluation,0,0.120084,"Missing"
P07-1004,P03-1021,0,0.0835163,"ployed by the decoder are: (a) one or several phrase table(s), which model the translation direction p(s |t), (b) one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the experiments reported here, we used 4-gram models on the NIST data, and a trigram model on EuroParl, (c) a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, and (d) a word penalty. These different models are combined loglinearly. Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on a development corpus which we will call dev1 in this paper. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm. After the main decoding step, rescoring with additional models is performed. The system generates a 5,000-best list of alternative translations for each source sentence. These lists are rescored with the following models: (a) the different models used in the decoder which are described above, (b) two different features based on IBM Model 1 (Brown et al., 1993), (c) posterior probabilities for words, phrases, n-grams, and s"
P07-1004,P02-1040,0,0.121563,"Missing"
P07-1004,J07-1003,1,0.66008,"us which we will call dev1 in this paper. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm. After the main decoding step, rescoring with additional models is performed. The system generates a 5,000-best list of alternative translations for each source sentence. These lists are rescored with the following models: (a) the different models used in the decoder which are described above, (b) two different features based on IBM Model 1 (Brown et al., 1993), (c) posterior probabilities for words, phrases, n-grams, and sentence length (Zens and Ney, 2006; Ueffing and Ney, 2007), all calculated over the N best list and using the sentence probabilities which the baseline system assigns to the translation hypotheses. The weights of these additional models and of the decoder models are again optimized to maximize BLEU score. This is performed on a second development corpus, dev2. 3 The Framework 3.1 The Algorithm Our transductive learning algorithm, Algorithm 1, is inspired by the Yarowsky algorithm (Yarowsky, 1995; Abney, 2004). The algorithm works as follows: First, the translation model is estimated based on the sentence pairs in the bilingual training data L. Then,"
P07-1004,2006.iwslt-papers.3,1,0.922512,"r scores 11: Xi := Xi ∪ {(tn , s, π (i) (tn |s))N n=1 } 12: end for 13: Scoring step: Si := Score(Xi ) // Assign a score to sentence pairs (t, s) from X. 14: Selection step: Ti := Select(Xi , Si ) // Choose a subset of good sentence pairs (t, s) from X. 15: i := i + 1. 16: until i &gt; R of the model on labeled and unlabeled data which can be very expensive if L is very large (as on the Chinese–English data set). This additional phrase table is small and specific to the development or test set it is trained on. It overlaps with the original phrase tables, but also contains many new phrase pairs (Ueffing, 2006). Mixture Model: Another alternative for Estimate is to create a mixture model of the phrase table probabilities with new phrase table probabilities p(s |t) = λ · Lp (s |t) + (1 − λ) · Tp (s |t) (2) where Lp and Tp are phrase table probabilities estimated on L and T , respectively. In cases where new phrase pairs are learned from T , they get added into the merged phrase table. 3.3 The Scoring Function In Algorithm 1, the Score function assigns a score to each translation hypothesis t. We used the following scoring functions in our experiments: Length-normalized Score: Each translated sentence"
P07-1004,P95-1026,0,0.188406,"features based on IBM Model 1 (Brown et al., 1993), (c) posterior probabilities for words, phrases, n-grams, and sentence length (Zens and Ney, 2006; Ueffing and Ney, 2007), all calculated over the N best list and using the sentence probabilities which the baseline system assigns to the translation hypotheses. The weights of these additional models and of the decoder models are again optimized to maximize BLEU score. This is performed on a second development corpus, dev2. 3 The Framework 3.1 The Algorithm Our transductive learning algorithm, Algorithm 1, is inspired by the Yarowsky algorithm (Yarowsky, 1995; Abney, 2004). The algorithm works as follows: First, the translation model is estimated based on the sentence pairs in the bilingual training data L. Then, a set of source language sentences, U , is translated based on the current model. A subset of good translations and their sources, Ti , is selected in each 26 iteration and added to the training data. These selected sentence pairs are replaced in each iteration, and only the original bilingual training data, L, is kept fixed throughout the algorithm. The process of generating sentence pairs, selecting a subset of good sentence pairs, and"
P07-1004,W06-3110,0,0.021885,"n a development corpus which we will call dev1 in this paper. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm. After the main decoding step, rescoring with additional models is performed. The system generates a 5,000-best list of alternative translations for each source sentence. These lists are rescored with the following models: (a) the different models used in the decoder which are described above, (b) two different features based on IBM Model 1 (Brown et al., 1993), (c) posterior probabilities for words, phrases, n-grams, and sentence length (Zens and Ney, 2006; Ueffing and Ney, 2007), all calculated over the N best list and using the sentence probabilities which the baseline system assigns to the translation hypotheses. The weights of these additional models and of the decoder models are again optimized to maximize BLEU score. This is performed on a second development corpus, dev2. 3 The Framework 3.1 The Algorithm Our transductive learning algorithm, Algorithm 1, is inspired by the Yarowsky algorithm (Yarowsky, 1995; Abney, 2004). The algorithm works as follows: First, the translation model is estimated based on the sentence pairs in the bilingual"
P07-1004,C04-1046,1,\N,Missing
P07-1004,W07-0724,1,\N,Missing
P07-1004,J04-3004,0,\N,Missing
P09-1021,N07-1029,0,0.0521419,"ild multiple MT systems from multiple source languages to the new target language, each MT system can be seen as a different ‘view’ on the desired output translation. Thus, we can train our multiple MT systems using either self-training or co-training (Blum and Mitchell, 1998). In selftraining each MT system is re-trained using human labeled data plus its own noisy translation output on the unlabeled data. In co-training each MT system is re-trained using human labeled data plus noisy translation output from the other MT systems in the ensemble. We use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training between multiple MT systems. This paper makes the following contributions: • We provide a new framework for multilingual MT, in which we build multiple MT systems and add a new language to an existing multilingual parallel corpus. The multilingual setStatistical machine translation (SMT) models require bilingual corpora for training, and these corpora are often multilingual with parallel text in multiple languages simultaneously. We introduce an active learning task of adding a new language to an existing multilingual set of parall"
P09-1021,P07-1004,1,0.0888127,"f highly informative sentences from U, and add those sentences together with their human-provided translations to L. This process is continued iteratively until a certain level of translation quality is met (we use the BLEU score, WER and PER) (Papineni et al., 2002). In the baseline, against which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training (MERT) (Och, 2003). Since each entry in U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (F d , E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidate"
P09-1021,W03-0310,0,0.0653416,"see Fig. 5). Enhancing the estimated distributions to capture this power law behavior would improve the quality of the proposed sentence selection methods. (Reichart et al., 2008) introduces multi-task active learning where unlabeled data require annotations for multiple tasks, e.g. they consider namedentities and parse trees, and showed that multiple tasks helps selection compared to individual tasks. Our setting is different in that the target language is the same across multiple MT tasks, which we exploit to use consensus translations and cotraining to improve active learning performance. (Callison-Burch and Osborne, 2003b; CallisonBurch and Osborne, 2003a) provide a co-training approach to MT, where one language pair creates data for another language pair. In contrast, our co-training approach uses consensus translations and our setting for active learning is very different from their semi-supervised setting. A Ph.D. proposal by Chris Callison-Burch (Callison-burch, 2003) lays out the promise of AL for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours. While we use consensus translations (He et al., 20"
P09-1021,P07-1092,0,0.0556948,"∗ KL(Preg k Preg ) 4.37 4.17 4.38 ∗ KL(Preg k unif ) 5.37 5.21 5.80 ∗ KL(Poov k Poov ) 3.04 4.58 4.73 ∗ KL(Poov k unif ) 3.41 4.75 4.99 Table 2: For regular/OOV phrases, the KL-divergence between the true distribution (P ∗ ) and the estimated (P ) or uniform (unif ) distributions are shown, where: ∗ P (x) KL(P ∗ k P ) := x P ∗ (x) log PP (x) . Analysis The basis for our proposed methods has been the popularity of regular/OOV phrases in U and their data is very noisy and future work should omit this pair. 4 Choice of Germanic and Romance for our experimental setting is inspired by results in (Cohn and Lapata, 2007) 187 Regular Phrases in U between the true and uniform distributions, in all three language pairs. Since uniform distribution conveys no information, this is evidence that there is some information encoded in the estimated distribution about the true distribution. However we noticed that the true distributions of regular/OOV phrases exhibit Zipfian (power law) behavior5 which is not well captured by the estimated distributions (see Fig. 5). Enhancing the estimated distributions to capture this power law behavior would improve the quality of the proposed sentence selection methods. (Reichart et"
P09-1021,C02-1117,0,0.0346595,"Missing"
P09-1021,N09-1047,1,0.822574,"entences from U, and add those sentences together with their human-provided translations to L. This process is continued iteratively until a certain level of translation quality is met (we use the BLEU score, WER and PER) (Papineni et al., 2002). In the baseline, against which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training (MERT) (Och, 2003). Since each entry in U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (F d , E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidates (e1 , .., eD ) in a c"
P09-1021,D08-1011,0,0.0328607,"tions. When we build multiple MT systems from multiple source languages to the new target language, each MT system can be seen as a different ‘view’ on the desired output translation. Thus, we can train our multiple MT systems using either self-training or co-training (Blum and Mitchell, 1998). In selftraining each MT system is re-trained using human labeled data plus its own noisy translation output on the unlabeled data. In co-training each MT system is re-trained using human labeled data plus noisy translation output from the other MT systems in the ensemble. We use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training between multiple MT systems. This paper makes the following contributions: • We provide a new framework for multilingual MT, in which we build multiple MT systems and add a new language to an existing multilingual parallel corpus. The multilingual setStatistical machine translation (SMT) models require bilingual corpora for training, and these corpora are often multilingual with parallel text in multiple languages simultaneously. We introduce an active learning task of adding a new language to an existing multil"
P09-1021,2005.mtsummit-papers.11,0,0.0110887,"ials responsible for generating phrases in each of the labeled and unlabeled data sets. To generate a phrase, we first toss a coin and depending on the outcome we either generate the phrase from the multinomial associated with reguoov lar phrases θ reg U or potential phrases θ U : 5 Sentence Scoring The sentence score is a linear combination of two terms: one coming from regular phrases and the other from OOV phrases: Experiments X λ P (x|θ U ) log reg P (x|θ L ) |Xs | reg x∈Xs + 1−λ |Xsoov | X X x∈Xsoov h∈Hx y∈Yx Corpora. We pre-processed the EuroParl corpus (http://www.statmt.org/europarl) (Koehn, 2005) and built a multilingual parallel corpus with 653,513 sentences, excluding the Q4/2000 portion of the data (2000-10 to 2000-12) which is reserved as the test set. We subsampled 5,000 sentences as the labeled data L and 20,000 sentences as U for the pool of untranslated sentences (while hiding the English part). The test set consists of 2,000 multi-language sentences and comes from the multilingual parallel corpus built from Q4/2000 portion of the data. Consensus Finding. Let T be the union of the nbest lists of translations for a particular sentence. The consensus translation tc is where θ U"
P09-1021,E06-1005,0,0.120986,"ems from multiple source languages to the new target language, each MT system can be seen as a different ‘view’ on the desired output translation. Thus, we can train our multiple MT systems using either self-training or co-training (Blum and Mitchell, 1998). In selftraining each MT system is re-trained using human labeled data plus its own noisy translation output on the unlabeled data. In co-training each MT system is re-trained using human labeled data plus noisy translation output from the other MT systems in the ensemble. We use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training between multiple MT systems. This paper makes the following contributions: • We provide a new framework for multilingual MT, in which we build multiple MT systems and add a new language to an existing multilingual parallel corpus. The multilingual setStatistical machine translation (SMT) models require bilingual corpora for training, and these corpora are often multilingual with parallel text in multiple languages simultaneously. We introduce an active learning task of adding a new language to an existing multilingual set of parallel text and constructin"
P09-1021,J04-4002,0,0.00429109,"ntinued iteratively until a certain level of translation quality is met (we use the BLEU score, WER and PER) (Papineni et al., 2002). In the baseline, against which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training (MERT) (Och, 2003). Since each entry in U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (F d , E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidates (e1 , .., eD ) in a co-training setting (sharing information between multiple SMT models). A whole range of methods exist in the li"
P09-1021,P03-1021,0,0.0118839,"inst which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training (MERT) (Och, 2003). Since each entry in U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (F d , E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidates (e1 , .., eD ) in a co-training setting (sharing information between multiple SMT models). A whole range of methods exist in the literature for combining the output translations of multiple MT systems for a single language pair, operating either at the sentence, phrase, or wor"
P09-1021,P02-1040,0,0.105082,"tial MT systems {MF d →E }D d=1 on the multilingual corpus L, and use them to translate all monolingual sentences in U. We denote sentences in U together with their multiple translations by U+ (line 4 of Algorithm 1). Then we retrain the SMT systems on L ∪ U+ and use the resulting model to decode the test set. Afterwards, we select and remove a subset of highly informative sentences from U, and add those sentences together with their human-provided translations to L. This process is continued iteratively until a certain level of translation quality is met (we use the BLEU score, WER and PER) (Papineni et al., 2002). In the baseline, against which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-"
P09-1021,P08-1098,0,0.383296,"pean Parliament (EuroParl) and U.N. proceedings. In this paper, we consider how to use active learning (AL) in order to add a new language to such a multilingual parallel corpus and at the same time we construct an MT system from each language in the original corpus into this new target language. We introduce a novel combined measure of translation quality for multiple target language outputs (the same content from multiple source languages). The multilingual setting provides new opportunities for AL over and above a single language pair. This setting is similar to the multi-task AL scenario (Reichart et al., 2008). In our case, the multiple tasks are individual machine translation tasks for several language pairs. The nature of the translation processes vary from any of the source ∗ Thanks to James Peltier for systems support for our experiments. This research was partially supported by NSERC, Canada (RGPIN: 264905) and an IBM Faculty Award. 181 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 181–189, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP overload the term entry to denote a tuple in L or in U (it should be clear from the context). For a single"
P09-1021,W07-0724,0,\N,Missing
P11-2125,N09-2066,0,0.0108452,"Missing"
P11-2125,J92-4003,0,0.626577,"inative dependency parsing was provided in (Koo et al., 2008) which involved clustering the labeled and unlabeled data and then each word in the dependency treebank was assigned a cluster identifier. These identifiers were used to augment the feature representation of the edge-factored or secondorder features, and this extended feature set was used to discriminatively train a dependency parser. The use of clusters leads to the question of how to integrate various types of clusters (possibly from different clustering algorithms) in discriminative dependency parsing. Clusters obtained from the (Brown et al., 1992) clustering algorithm are typically viewed as “semantic”, e.g. one cluster might contain plan, letter, request, memo, . . . while another may contain people, customers, employees, students, . . .. Another clustering view that is more “syntactic” in nature comes from the use of statesplitting in PCFGs. For instance, we could extract a syntactic cluster loss, time, profit, earnings, performance, rating, . . .: all head words of noun phrases corresponding to cluster of direct objects of 710 Marzieh Razavi and Anoop Sarkar School of Computing Science Simon Fraser University Vancouver, Canada {mraz"
P11-2125,D07-1101,0,0.00496803,"pendency Parsing A dependency tree represents the syntactic structure of a sentence with a directed graph (Figure 1), where nodes correspond to the words, and arcs indicate head-modifier pairs (Mel’ˇcuk, 1987). Graph-based dependency parsing searches for the highest-scoring tree according to a part-factored scoring function. In the first-order parsing models, the parts are individual head-modifier arcs in the dependency tree (McDonald et al., 2005). In the higher-order models, the parts consist of arcs together with some context, e.g. the parent or the sister arcs (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). With a linear scoring function, the parse for a sentence s X is: PARSE(s) = arg max w · f (s, r) (1) t∈T (s) r∈t where T (s) is the space of dependency trees for s, and f (s, r) is the feature vector for the part r which is linearly combined using the model parameter w to give the part score. The above arg max search for non-projective dependency parsing is accomProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710–714, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics root For"
P11-2125,W02-1001,0,0.0100834,"m the split non-terminals from the Berkeley parser output. The first row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters. plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifiers as the source of an additional set of features. The reader is directed to (Koo et al., 2008) for the list of cluster-based feature templates. The clusters inject long distance syntactic o"
P11-2125,C96-1058,0,0.108579,"NP-18 1010 access NN-13 NP-24 0011 to TO-0 TO-0 0011 American JJ-31 JJ-31 0110 markets NNS-25 NP-9 1011 Figure 1: Dependency tree with cluster identifiers obtained from the split non-terminals from the Berkeley parser output. The first row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters. plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifiers as the source of a"
P11-2125,D07-1097,0,0.0881868,"s it is Syn-High, and for Pronouns it is Syn-Low that is the best. But the ensemble always does the best in every grammatical category. Fig. 2(b) shows the F-score of the different models for various dependency lengths, where the length of a dependency from word wi to word wj is equal to |i − j|. We see that different models are experts on different lengths (Syn-Low on 8, Syn-High on 9), while the ensemble model can always combine their expertise and do better at each length. 6 Comparison to Related Work Several ensemble models have been proposed for dependency parsing (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009; Surdeanu and Manning, 2010). Essentially, all of these approaches combine different dependency parsing systems, i.e. transitionbased and graph-based. Although graph-based models are globally trained and can use exact inference algorithms, their features are defined over a limited history of parsing decisions. Since transitionbased parsing models have the opposite characteristics, the idea is to combine these two types of models to exploit their complementary strengths. The base parsing models are either independently trained (Sagae an"
P11-2125,P10-1001,0,0.00584032,"A dependency tree represents the syntactic structure of a sentence with a directed graph (Figure 1), where nodes correspond to the words, and arcs indicate head-modifier pairs (Mel’ˇcuk, 1987). Graph-based dependency parsing searches for the highest-scoring tree according to a part-factored scoring function. In the first-order parsing models, the parts are individual head-modifier arcs in the dependency tree (McDonald et al., 2005). In the higher-order models, the parts consist of arcs together with some context, e.g. the parent or the sister arcs (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). With a linear scoring function, the parse for a sentence s X is: PARSE(s) = arg max w · f (s, r) (1) t∈T (s) r∈t where T (s) is the space of dependency trees for s, and f (s, r) is the feature vector for the part r which is linearly combined using the model parameter w to give the part score. The above arg max search for non-projective dependency parsing is accomProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710–714, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics root For IN-1 PP-2 0111 , ,-0 ,-"
P11-2125,P08-1068,0,0.701852,"rithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifiers as the source of an additional set of features. The reader is directed to (Koo et al., 2008) for the list of cluster-based feature templates. The clusters inject long distance syntactic or semantic information into the model (in contrast with the use of POS tags in the baseline) and help alleviate the sparse data problem for complex features that include n-grams. 3 The Ensemble Model A word can have different syntactic or semantic cluster representations, each of which may lead to a different parsing model. We use ensemble learning (Dietterich, 2002) in order"
P11-2125,D10-1125,0,0.00600379,"-1 S-14 0101 trend NN-23 NP-18 1010 access NN-13 NP-24 0011 to TO-0 TO-0 0011 American JJ-31 JJ-31 0110 markets NNS-25 NP-9 1011 Figure 1: Dependency tree with cluster identifiers obtained from the split non-terminals from the Berkeley parser output. The first row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters. plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifier"
P11-2125,D07-1013,0,0.011974,"understand the contribution of each model to the ensemble, we take a closer look at the parsing errors for each model and the ensemble. For each dependent to head depen1 code.google.com/p/berkeleyparser Sentences of the Penn Treebank were excluded from the text used for the clustering. 3 people.csail.mit.edu/maestro/papers/bllip-clusters.gz 4 Terry Koo was kind enough to share the source code for the (Koo et al., 2008) paper with us, and we plan to incorporate all the features in our future work. 2 713 dency, Fig. 2(a) shows the error rate for each dependent grouped by a coarse POS tag (c.f. (McDonald and Nivre, 2007)). For most POS categories, the Brown cluster model is the best individual model, but for Adjectives it is Syn-High, and for Pronouns it is Syn-Low that is the best. But the ensemble always does the best in every grammatical category. Fig. 2(b) shows the F-score of the different models for various dependency lengths, where the length of a dependency from word wi to word wj is equal to |i − j|. We see that different models are experts on different lengths (Syn-Low on 8, Syn-High on 9), while the ensemble model can always combine their expertise and do better at each length. 6 Comparison to Rela"
P11-2125,E06-1011,0,0.0170693,"ross all our test sets. 2 Dependency Parsing A dependency tree represents the syntactic structure of a sentence with a directed graph (Figure 1), where nodes correspond to the words, and arcs indicate head-modifier pairs (Mel’ˇcuk, 1987). Graph-based dependency parsing searches for the highest-scoring tree according to a part-factored scoring function. In the first-order parsing models, the parts are individual head-modifier arcs in the dependency tree (McDonald et al., 2005). In the higher-order models, the parts consist of arcs together with some context, e.g. the parent or the sister arcs (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). With a linear scoring function, the parse for a sentence s X is: PARSE(s) = arg max w · f (s, r) (1) t∈T (s) r∈t where T (s) is the space of dependency trees for s, and f (s, r) is the feature vector for the part r which is linearly combined using the model parameter w to give the part score. The above arg max search for non-projective dependency parsing is accomProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710–714, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Ling"
P11-2125,P05-1012,0,0.262097,"orithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifiers as the source of an additional set of features. The reader is directed to (Koo et al., 2008) for the list of cluster-based feature templates. The clusters inject long distance syntactic or semantic information into the model (in contrast with the use of POS tags in the baseline) and help alleviate the sparse data problem for complex features that include n-grams. 3 The Ensemble Model A word can have different syntactic or semantic cluster representations, each of which may lead to a different parsing model. We use e"
P11-2125,P08-1108,0,0.0872987,"nd for Pronouns it is Syn-Low that is the best. But the ensemble always does the best in every grammatical category. Fig. 2(b) shows the F-score of the different models for various dependency lengths, where the length of a dependency from word wi to word wj is equal to |i − j|. We see that different models are experts on different lengths (Syn-Low on 8, Syn-High on 9), while the ensemble model can always combine their expertise and do better at each length. 6 Comparison to Related Work Several ensemble models have been proposed for dependency parsing (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009; Surdeanu and Manning, 2010). Essentially, all of these approaches combine different dependency parsing systems, i.e. transitionbased and graph-based. Although graph-based models are globally trained and can use exact inference algorithms, their features are defined over a limited history of parsing decisions. Since transitionbased parsing models have the opposite characteristics, the idea is to combine these two types of models to exploit their complementary strengths. The base parsing models are either independently trained (Sagae and Lavie, 2006; Hall et al."
P11-2125,P06-1055,0,0.00347755,"eems more scalable though, since we can incrementally add a large number of clustering algorithms into the ensemble. 4 Syntactic and Semantic Clustering In our ensemble model we use three different clustering methods to obtain three types of word representations that can help alleviate sparse data in a dependency parser. Our first word representation is exactly the same as the one used in (Koo et al., 2008) where words are clustered using the Brown algorithm (Brown et al., 1992). Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al., 2006). Split non-terminals from the Berkeley parser output are converted into cluster identifiers in two different ways: 1) the split POS tags for each word are used as an alternate word representation. We call this representation Syn-Low, and 2) head percolation rules are used to label each non-terminal in the parse such that each non-terminal has a unique daughter labeled as head. Each word is assigned a cluster identifier which is defined as the parent split non-terminal of that word if it is not marked as head, else if the parent is marked as head we recursively check its parent until we reach"
P11-2125,N10-1003,0,0.0111963,"Missing"
P11-2125,W96-0213,0,0.273964,"ig. 1. If we group all the head-words in the training data that project up to split non-terminal NP-24 then we get a cluster: loss, time, profit, earnings, performance, rating, . . . which are head words of the noun phrases that appear as direct object of verbs like improve. 5 Experimental Results ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! opment set (Section 22), and test sets (Sections 0, 1, 23, and 24). All our experimental settings match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005; Koo et al., 2008). POS tags for the development and test data were assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus. To generate part of speech tags for the training data, we used 20way jackknifing, i.e. we tagged each fold with the tagger trained on the other 19 folds. We set model weights αk in Eqn (2) to one for all experiments. The experiments were done on the English Penn Treebank, using standard head-percolation rules (Yamada and Matsumoto, 2003) to convert the phrase structure into dependency trees. We split the Syntactic State-Splitting The sentence-specific Treebank into a training set (Sections 2-21), a devel- word clusters are derived f"
P11-2125,N06-2033,0,0.14961,"odel, but for Adjectives it is Syn-High, and for Pronouns it is Syn-Low that is the best. But the ensemble always does the best in every grammatical category. Fig. 2(b) shows the F-score of the different models for various dependency lengths, where the length of a dependency from word wi to word wj is equal to |i − j|. We see that different models are experts on different lengths (Syn-Low on 8, Syn-High on 9), while the ensemble model can always combine their expertise and do better at each length. 6 Comparison to Related Work Several ensemble models have been proposed for dependency parsing (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009; Surdeanu and Manning, 2010). Essentially, all of these approaches combine different dependency parsing systems, i.e. transitionbased and graph-based. Although graph-based models are globally trained and can use exact inference algorithms, their features are defined over a limited history of parsing decisions. Since transitionbased parsing models have the opposite characteristics, the idea is to combine these two types of models to exploit their complementary strengths. The base parsing models are either independentl"
P11-2125,D08-1016,0,0.0113783,"DT-15 1101 improves VBZ-1 S-14 0101 trend NN-23 NP-18 1010 access NN-13 NP-24 0011 to TO-0 TO-0 0011 American JJ-31 JJ-31 0110 markets NNS-25 NP-9 1011 Figure 1: Dependency tree with cluster identifiers obtained from the split non-terminals from the Berkeley parser output. The first row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters. plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word"
P11-2125,N10-1091,0,0.148352,"semble always does the best in every grammatical category. Fig. 2(b) shows the F-score of the different models for various dependency lengths, where the length of a dependency from word wi to word wj is equal to |i − j|. We see that different models are experts on different lengths (Syn-Low on 8, Syn-High on 9), while the ensemble model can always combine their expertise and do better at each length. 6 Comparison to Related Work Several ensemble models have been proposed for dependency parsing (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009; Surdeanu and Manning, 2010). Essentially, all of these approaches combine different dependency parsing systems, i.e. transitionbased and graph-based. Although graph-based models are globally trained and can use exact inference algorithms, their features are defined over a limited history of parsing decisions. Since transitionbased parsing models have the opposite characteristics, the idea is to combine these two types of models to exploit their complementary strengths. The base parsing models are either independently trained (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell’Orletta, 2009; Surdeanu and Manning,"
P11-2125,W03-3023,0,0.0305711,")))length. ! ! ! ! 0.80 ! For the Berkeley parser output shown above, the resulting word representations and dependency tree is shown in Fig. 1. If we group all the head-words in the training data that project up to split non-terminal NP-24 then we get a cluster: loss, time, profit, earnings, performance, rating, . . . which are head words of the noun phrases that appear as direct object of verbs like improve. 5 Experimental Results ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! opment set (Section 22), and test sets (Sections 0, 1, 23, and 24). All our experimental settings match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005; Koo et al., 2008). POS tags for the development and test data were assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus. To generate part of speech tags for the training data, we used 20way jackknifing, i.e. we tagged each fold with the tagger trained on the other 19 folds. We set model weights αk in Eqn (2) to one for all experiments. The experiments were done on the English Penn Treebank, using standard head-percolation rules (Yamada and Matsumoto, 2003) to convert the phrase structure into dependency trees. We split the"
P13-1041,P11-2075,0,0.0969324,"ction 3 explains different word cluster based features employed to reduce data sparsity for monolingual SA. In section 4, alternative CLSA approaches based on word clustering are elucidated. Experimental details are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. Popular approaches for Cross-Lingual Sentiment Analysis (CLSA) (Wan, 2009; Duh et al., 2011) depend on Machine Translation (MT) for converting the labeled data from one language to the other (Hiroshi et al., 2004; Banea et al., 2008; Wan, 2009). However, many languages which are truly resource scarce, do not have an MT system or existing MT systems are not ripe to be used for CLSA (Balamurali et al., 2013). To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are used for this study. I"
P13-1041,D11-1100,1,0.878331,"d in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. In WordNet, paradigms are manually generated based on the principles of lexical and semantic relationship among words (Fellbaum, 1998). WordNets are primarily used to address the problem of word sense disambiguation. However, at present there are many NLP applications which use WordNet. One such application is Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recogni"
P13-1041,C12-2008,1,0.61224,"Missing"
P13-1041,D08-1014,0,0.141217,"approaches based on word clustering are elucidated. Experimental details are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. Popular approaches for Cross-Lingual Sentiment Analysis (CLSA) (Wan, 2009; Duh et al., 2011) depend on Machine Translation (MT) for converting the labeled data from one language to the other (Hiroshi et al., 2004; Banea et al., 2008; Wan, 2009). However, many languages which are truly resource scarce, do not have an MT system or existing MT systems are not ripe to be used for CLSA (Balamurali et al., 2013). To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are used for this study. Instead, language gap for performing CLSA is bridged using linked cluster or cross-lingual clusters (explained in section 4) with the help of"
P13-1041,P07-1056,0,0.376814,"Missing"
P13-1041,R09-1010,0,0.119334,"11), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cross-lingual sentiment analysis are used. Most often these methods depend on an intermediary machine translation system (Wan, 2009; Brooke et al., 2009) or a bilingual dictionary (Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge the language gap. Given the subtle and different ways the sentiment can be expressed which itself manifested as a result of cultural diversity amongst different languages, an MT system has to be of a superior quality to capture them. 3.2 Approach 2: Syntagmatic Property based Clustering For this particular study, a co-occurrence based algorithm is used to create word clusters. As the algorithm is based on co-occurrence, one can extract the classes that have the flavour of syntagmatic grouping, depending on the natu"
P13-1041,J92-4003,0,0.0993672,"s Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster b"
P13-1041,P11-2125,1,0.874869,"erties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser"
P13-1041,C04-1071,0,0.551503,"n 4, alternative CLSA approaches based on word clustering are elucidated. Experimental details are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. Popular approaches for Cross-Lingual Sentiment Analysis (CLSA) (Wan, 2009; Duh et al., 2011) depend on Machine Translation (MT) for converting the labeled data from one language to the other (Hiroshi et al., 2004; Banea et al., 2008; Wan, 2009). However, many languages which are truly resource scarce, do not have an MT system or existing MT systems are not ripe to be used for CLSA (Balamurali et al., 2013). To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are used for this study. Instead, language gap for performing CLSA is bridged using linked cluster or cross-lingual clusters (explained in section"
P13-1041,I08-1039,0,0.0607824,"f classifiers based on different sense-based and word-based features were compared. The results suggested that WordNet synset based features performed better than word-based features. In this study, synset identifiers are extracted from manually/automatically sense annotated corpora and used as features for creating sentiment classifiers. The classifier thus build is used as a baseline. Apart from this, another baseline employing word based features are used for a comprehensive comparison. syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cro"
P13-1041,J03-1002,0,0.0026508,"r the word clusterings process. For Brown clustering, an implementation by Liang (2005) was used. Cross-lingual clustering for CLSA 2 http://sanskrit.jnu.ac.in/ilci/index. jsp 416 CLSA: The same datasets used in SA are also used for CLSA. Three approaches (as described in section 4) were tested for English-Hindi and English-Marathi language pairs. To create alignments, English-Hindi and English-Marathi parallel corpora from ILCI were used. EnglishHindi parallel corpus contains 45992 sentences and English-Marathi parallel corpus contains 47881 sentences. To create alignments, GIZA++5 was used (Och and Ney, 2003). As a preprocessing step, all stop words were removed. Stemming was performed on English and Hindi whereas for Marathi data, Morphological Analyzer was used to reduce the words to their respective lemmas. All experiments were performed using C-SVM 3 http://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/ 4 http://www.cfilt.iitb.ac. in/resources/senti/MPLC_tour_ downloaderInfo.php 5 http://www-i6.informatik.rwth-aachen. de/Colleagues/och/software/GIZA++.html Features Words WordNet Sense (Paradigmatic) Clusters (Syntagmatic) En-TD 87.02 89.13 97.45 En-PD 77.60 74.50 87.80 Hi 77.36 85.80 83.50 z Mar"
P13-1041,P11-1033,0,0.0613431,"ntiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cross-lingual sentiment analysis are used. Most often these methods depend on an intermediary machine translation system (Wan, 2009; Brooke et al., 2009) or a bilingual dictionary (Ghorbel and Jacot, 2011; Lu et al., 2011) to bridge the language gap. Given the subtle and different ways the sentiment can be expressed which itself manifested as a result of cultural diversity amongst different languages, an MT system has to be of a superior quality to capture them. 3.2 Approach 2: Syntagmatic Property based Clustering For this particular study, a co-occurrence based algorithm is used to create word clusters. As the algorithm is based on co-occurrence, one can extract the classes that have the flavour of syntagmatic grouping, depending on the nature of underlying statistics. Agglomerative clustering algorithm by Br"
P13-1041,W02-1011,0,0.0162818,"cience and Engineering, IIT Bombay Australia {kashyap,balamurali,pb}@cse.iitb.ac.in reza@monash.edu Abstract Paradigmatic analysis of text is the analysis of concepts embedded in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. In WordNet, paradigms are manually generated based on the principles of lexical and semantic relationship among words (Fellbaum, 1998). WordNets are primarily used to address the problem of word sense disambiguation. However, at present there are many NLP applications which use WordNet. One such application is Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additiona"
P13-1041,N04-1043,0,0.034703,"c processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser effort through the means of syntagma based word clustering. In this paper,"
P13-1041,P07-1017,0,0.0109515,"entiment analysis, both monolingual and cross-lingual, is addressed through the means of clustering. Experiments show that cluster based data sparsity reduction leads to performance better than sense based classification for sentiment analysis at document level. Similar idea is applied to Cross Lingual Sentiment Analysis (CLSA), and it is shown that reduction in data sparsity (after translation or bilingual-mapping) produces accuracy higher than Machine Translation based CLSA and sense based CLSA. 1 Introduction Data sparsity is the bane of Natural Language Processing (NLP) (Xue et al., 2005; Minkov et al., 2007). Language units encountered in the test data but absent in the training data severely degrade the performance of an NLP task. NLP applications innovatively handle data sparsity through various means. A special, but very common kind of data sparsity viz., word sparsity, can be addressed in one of the two obvious ways: 1) sparsity reduction through paradigmatically related words or 2) sparsity reduction through syntagmatically related words. 412 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 412–422, c Sofia, Bulgaria, August 4-9 2013. 2013 Associ"
P13-1041,W04-3253,0,0.543596,"performing CLSA is bridged using linked cluster or cross-lingual clusters (explained in section 4) with the help of unlabelled monolingual corpora. The contributions of this paper are two fold: 2 Related Work The problem of SA at document level is defined as the classification of document into different polarity classes (positive and negative) (Turney, 2002). Both supervised (Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised approaches (Mei et al., 2007; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1 Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two widely spoken Indian languages with a speaker population of 450 million and 72 million respectively. 413 by using automatic/manual sense disambiguation techniques. Thereafter, accuracies of classifiers based on different sense-based and word-based features were c"
P13-1041,N10-1120,0,0.0630474,"utomatic/manual sense disambiguation techniques. Thereafter, accuracies of classifiers based on different sense-based and word-based features were compared. The results suggested that WordNet synset based features performed better than word-based features. In this study, synset identifiers are extracted from manually/automatically sense annotated corpora and used as features for creating sentiment classifiers. The classifier thus build is used as a baseline. Apart from this, another baseline employing word based features are used for a comprehensive comparison. syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations wh"
P13-1041,P06-2079,0,0.375498,"Missing"
P13-1041,R09-1067,0,0.0105564,"of text is the analysis of concepts embedded in the text (Cruse, 1986; Chandler, 2012). WordNet is a byproduct of such an analysis. In WordNet, paradigms are manually generated based on the principles of lexical and semantic relationship among words (Fellbaum, 1998). WordNets are primarily used to address the problem of word sense disambiguation. However, at present there are many NLP applications which use WordNet. One such application is Sentiment Analysis (SA) (Pang and Lee, 2002). Recent research has shown that word sense based semantic features can improve the performance of SA systems (Rentoumi et al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit"
P13-1041,R11-1020,0,0.0288582,"features performed better than word-based features. In this study, synset identifiers are extracted from manually/automatically sense annotated corpora and used as features for creating sentiment classifiers. The classifier thus build is used as a baseline. Apart from this, another baseline employing word based features are used for a comprehensive comparison. syntax (Matsumoto et al., 2005; Nakagawa et al., 2010), semantic (Balamurali et al., 2011) and negation (Ikeda et al., 2008) have also been explored for this task. There has been research related to clustering and sentiment analysis. In Rooney et al. (2011), documents are clustered based on the context of each document and sentiment labels are attached at the cluster level. Zhai et al. (2011) attempts to cluster features of a product to perform sentiment analysis on product reviews. In this work, word clusters (syntagmatic and paradigmatic) encoding a mixture of syntactic and semantic information are used for feature engineering. In situations where labeled data is not present in a language, approaches based on cross-lingual sentiment analysis are used. Most often these methods depend on an intermediary machine translation system (Wan, 2009; Bro"
P13-1041,W12-0704,0,0.018366,"010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible rea"
P13-1041,N12-1052,0,0.011642,"Missing"
P13-1041,P10-1040,0,0.0101778,"Missing"
P13-1041,P02-1053,0,0.00870777,"llel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are used for this study. Instead, language gap for performing CLSA is bridged using linked cluster or cross-lingual clusters (explained in section 4) with the help of unlabelled monolingual corpora. The contributions of this paper are two fold: 2 Related Work The problem of SA at document level is defined as the classification of document into different polarity classes (positive and negative) (Turney, 2002). Both supervised (Benamara et al., 2007; Martineau and Finin, 2009) and unsupervised approaches (Mei et al., 2007; Lin and He, 2009) exist for this task. Supervised approaches are popular because of their superior classification accuracy (Mullen and Collier, 2004; Pang and Lee, 2008). Feature engineering plays an important role in these systems. Apart from the commonly used bag-of-words features based on unigrams/bigrams/ngrams (Dave et al., 2003; Ng et al., 2006; Martineau and Finin, 2009), 1 Hindi and Marathi belong to the Indo-Aryan subgroup of the Indo-European language family and are two"
P13-1041,P08-1086,0,0.158288,"al., 2009; Tamara et al., 2010; Balamurali et al., 2011) compared to word based features. Syntagmatic analysis of text concentrates on the surface properties of the text. Compared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification."
P13-1041,P09-1027,0,0.414442,"ed work. Section 3 explains different word cluster based features employed to reduce data sparsity for monolingual SA. In section 4, alternative CLSA approaches based on word clustering are elucidated. Experimental details are explained in section 5. Results and discussions are presented in section 6 and section 7 respectively. Finally, section 8 concludes the paper pointing to some future research possibilities. Further, cluster based features are used to address the problem of scarcity of sentiment annotated data in a language. Popular approaches for Cross-Lingual Sentiment Analysis (CLSA) (Wan, 2009; Duh et al., 2011) depend on Machine Translation (MT) for converting the labeled data from one language to the other (Hiroshi et al., 2004; Banea et al., 2008; Wan, 2009). However, many languages which are truly resource scarce, do not have an MT system or existing MT systems are not ripe to be used for CLSA (Balamurali et al., 2013). To perform CLSA, this study leverages unlabelled parallel corpus to generate the word alignments. These word alignments are then used to link cluster based features to obliterate the language gap for performing SA. No MT systems or bilingual dictionaries are use"
P13-1041,P11-2033,0,0.00516662,"mpared to paradigmatic property extraction, syntagmatic processing is relatively light weight. One of the obvious syntagmas is words, and words are grouped into equivalence classes or clusters, thus reducing the model parameters of a statistical NLP system (Brown et al., 1992). When used as an additional feature with word based language models, it has been shown to improve the system performance viz., machine translation (Uszkoreit and Brants, 2008; Stymne, 2012), speech recognition (Martin et al., 1995; Samuelsson and Reichl, 1999), dependency parsing (Koo et al., 2008; Haffari et al., 2011; Zhang and Nivre, 2011; Tratz and Hovy, 2011) and NER (Miller et al., 2004; Faruqui and Pad´o, 2010; Turian et al., 2010; T¨ackstr¨om et al., 2012). In this paper, the focus is on alleviating the data sparsity faced by supervised approaches for SA through the means of cluster based features. As WordNets are essentially word Expensive feature engineering based on WordNet senses has been shown to be useful for document level sentiment classification. A plausible reason for such a performance improvement is the reduction in data sparsity. However, such a reduction could be achieved with a lesser effort through the mea"
P13-1041,D11-1116,0,\N,Missing
P13-1041,P08-1068,0,\N,Missing
P13-1077,N07-1018,0,0.55118,"provements over competitive baseline systems. 1 This paper develops a phrase-based translation model which aims to address the above shortcomings of the phrase-based translation pipeline. Specifically, we formulate translation using inverse transduction grammar (ITG), and seek to learn an ITG from parallel corpora. The novelty of our approach is that we develop a Bayesian prior over the grammar, such that a nonterminal becomes a ‘cache’ learning each production and its complete yield, which in turn is recursively composed of its child constituents. This is closely related to adaptor grammars (Johnson et al., 2007a), which also generate full tree rewrites in a monolingual setting. Our model learns translations of entire sentences while also learning their decomposition into smaller units (phrase-pairs) recursively, terminating at word translations. The model is richly parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrasepairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, whi"
P13-1077,N10-1028,1,0.911056,"Missing"
P13-1077,N03-1017,0,0.289824,"parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrasepairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central"
P13-1077,P09-1088,1,0.944279,"ey are used to infer word-level alignments from sentence aligned parallel data, from 780 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 780–790, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader class of synchronous grammars than ITGs, such as SCFGs (Blunsom et al., 2009b; Levenberg et al., 2012). Conceptually, our work could be readily adapted to general SCFGs using similar techniques. We are not the first to consider this idea; Neubig et al. (2011) developed a similar approach for learning an ITG using a form of Pitman-Yor adaptor grammar. However Neubig et al.’s work was flawed in a number of respects, most notably in terms of their heuristic beam sampling algorithm which does not meet either of the Markov Chain Monte Carlo criteria of ergodicity or detailed balance. Consequently their approach does not constitute a valid Bayesian model. In contrast, this"
P13-1077,P07-2045,0,0.00926249,"the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from 780 Proceedings of the 51st Annual Meeting of the Associa"
P13-1077,J93-2003,0,0.0720143,"ased approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from 780 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 780–790, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader"
P13-1077,D12-1021,0,0.489339,"Missing"
P13-1077,P06-2014,0,0.020981,"ages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-w"
P13-1077,W07-0403,0,0.0200011,"synchronous grammar formalism. Terminal productions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual"
P13-1077,W02-1018,0,0.0608507,"Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-word units. This pioneering approach suffered from intractable inference and moreover, suffers from degenerate solutions (DeNero and Klein, 2010). Our approach is similar to these previous works, except that we impose As mentioned above, ours is not the first work attempting to generalise adaptor grammars for machine translation; (Neubig et al., 2011) also developed a similar approach based around ITG using a Pitman-Yor Process prior. Our approach improves upon theirs in terms of the model and inference, and critically, this is borne o"
P13-1077,J07-2003,0,0.135124,"ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from 780 Proceedings of the 51st Annual Me"
P13-1077,W06-1606,0,0.0289279,"en the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word-based translation models (Brown et al., 1993) remain central to phrase-based model training, where they are used to infer word-level alignments from sentence aligned parallel data, from 780 Proceedings of the 51st Annual Meeting of the Association for Computationa"
P13-1077,P10-1147,0,0.0127123,"te their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-word units. This pioneering approach suffered from intractable inference and moreover, suffers from degenerate solutions (DeNero and Klein, 2010). Our approach is similar to th"
P13-1077,P11-1064,0,0.755949,"780–790, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics additional constraints on how phrase-pairs can be tiled to produce a sentence pair, and moreover, we seek to model the embedding of phrase-pairs in one another, something not considered by this prior work. Another strand of related research is in estimating a broader class of synchronous grammars than ITGs, such as SCFGs (Blunsom et al., 2009b; Levenberg et al., 2012). Conceptually, our work could be readily adapted to general SCFGs using similar techniques. We are not the first to consider this idea; Neubig et al. (2011) developed a similar approach for learning an ITG using a form of Pitman-Yor adaptor grammar. However Neubig et al.’s work was flawed in a number of respects, most notably in terms of their heuristic beam sampling algorithm which does not meet either of the Markov Chain Monte Carlo criteria of ergodicity or detailed balance. Consequently their approach does not constitute a valid Bayesian model. In contrast, this paper provides a more rigorous and theoretically sound method. Moreover our approach results in consistent translation improvements across a number of translation tasks compared to Ne"
P13-1077,P09-1104,0,0.0157684,"ate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM mo"
P13-1077,P03-1021,0,0.072027,"Section 2.14 In the end-to-end MT pipeline we use a standard set of features: relative-frequency and lexical translation model probabilities in both directions; distance-based distortion model; language model and word count. We set the distortion limit to 6 and max-phrase-length to 7 in all experiments. We train 3-gram language models using modified Kneser-Ney smoothing. For AR-EN experiments the language model is trained on English data as (Blunsom et al., 2009a), and for FA-EN and UREN the English data are the target sides of the bilingual training data. We use minimum error rate training (Och, 2003) with nbest list size 100 to optimize the feature weights for maximum development BLEU. 500 iteration Figure 1: Training progress on the UR-EN corpus, showing the posterior probability improving with each full sampling iteration. Different colours denote independent sampling runs. 1e−03 1e−05 time (s) 1e−01 ● ●● ●● ●●●● ● ●● ●● ●● ●● ●● ● ●● ● ● ● ● ●● ● ● ●● ●● ●● ● ● ●● ●● ●● ● ●● ● ● ●● ● ● ● ● ● ● ● ● ●● ● ●● ● ● ●● ●●●● ●● ● ●● ● ● ●● ● ● ●● ● ● ● ●● ● ● ● ●●●● ●● ●● ● ● ● ● ● ● ● ●● ●● ● ● ● ●● ●● ●● ● ● ●● ●●●● ●● ● ● ● ● ● ●● ●● ● ● ● ● ● ●● ● ● ●● ● ●● ● ● ●●● ● ●●● ●● ● ● ● ● ● ●● ●"
P13-1077,N10-1014,0,0.0165898,"roductions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002"
P13-1077,D08-1012,0,0.0211363,"vement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with Marcu and Wong (2002) who developed an extension to IBM model 1 to handle multi-word units. This pioneering approach suffered from"
P13-1077,P06-1124,0,0.109497,"rminating at word translations. The model is richly parameterised, such that it can describe phrase-based phenomena while also explicitly modelling the relationships between phrasepairs and their component expansions, thus ameliorating the disconnect between the treatment of words versus phrases in the current MT pipeline. We develop a Bayesian approach using a PitmanYor process prior, which is capable of modelling a diverse range of geometrically decaying distributions over infinite event spaces (here translation phrase-pairs), an approach shown to be state of the art for language modelling (Teh, 2006). Introduction The phrase-based approach (Koehn et al., 2003) to machine translation (MT) has transformed MT from a narrow research topic into a truly useful technology to end users. Leading translation systems (Chiang, 2007; Koehn et al., 2007; Marcu et al., 2006) all use some kind of multi-word translation unit, which allows translations to be produced from large canned units of text from the training corpus. Larger phrases allow for the lexical context to be considered in choosing the translation, and also limit the number of reordering decisions required to produce a full translation. Word"
P13-1077,J97-3002,0,0.533801,"daptor grammars (Johnson et al., 2007a), a monolingual grammar formalism whereby a non-terminal rewrites in a single step as a complete subtree. The model prior allows for trees to be generated as a mixture of a cache and a base adaptor grammar. In our case, we have generalised to a bilingual setting using an ITG. Additionally, we have extended the model to allow recursive nesting of adapted non-terminals, such that we end up with an infinitely recursive formulation where the top-level and base distributions are explicitly linked together. Related Work Inversion transduction grammar (or ITG) (Wu, 1997) is a well studied synchronous grammar formalism. Terminal productions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang"
P13-1077,P05-1059,0,0.0265561,"1997) is a well studied synchronous grammar formalism. Terminal productions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based"
P13-1077,P08-1012,0,0.0209672,"ormalism. Terminal productions of the form X → e/f generate a word in two languages, and nonterminal productions allow phrasal movement in the translation process. Straight productions, denoted by their non-terminals inside square brackets [...], generate their symbols in the given order in both languages, while inverted productions, indicated by angled brackets h...i, generate their symbols in the reverse order in the target language. In the context of machine translation, ITG has been explored for statistical word alignment in both unsupervised (Zhang and Gildea, 2005; Cherry and Lin, 2007; Zhang et al., 2008; Pauls et al., 2010) and supervised (Haghighi et al., 2009; Cherry and Lin, 2006) settings, and for decoding (Petrov et al., 2008). Our paper fits into the recent line of work for jointly inducing the phrase table and word alignment (DeNero and Klein, 2010; Neubig et al., 2011). The work of DeNero and Klein (2010) presents a supervised approach to this problem, whereas our work is unsupervised hence more closely related to Neubig et al. (2011) which we describe in detail below. A number of other approaches have been developed for learning phrase-based models from bilingual data, starting with"
P13-1109,N09-1014,0,0.0734438,"enerative models based on canonical correlation analysis to extract translation lexicons for non-parallel corpora by learning a matching between source and target lexicons. Using monolingual features to represent words, feature vectors are projected from source and target words into a canonical space to find the appropriate matching between them. Their method relies on context features which need a seed lexicon and orthographic features which only works for phylogenetically related languages. Graph-based semi-supervised methods have been shown to be useful for domain adaptation in MT as well. Alexandrescu and Kirchhoff (2009) applied a graph-based method to determine similarities between sentences and use these similarities to promote similar translations for similar sentences. They used a graph-based semi-supervised model to re-rank the n-best translation hypothesis. Liu et al. (2012) extended Alexandrescu’s model to use translation consensus among similar sentences in bilingual training data by developing a new structured label propagation method. They derived some features to use during decoding process that has been shown useful in improving translation quality. Our graph propagation method connects monolingua"
P13-1109,N06-1003,0,0.762665,"was partially supported by an NSERC, Canada (RGPIN: 264905) grant. The third author was supported by an early career research award from Monash University to visit Simon Fraser University. re-order the words in the target language (Zhang et al., 2012). Increasing the size of the parallel data can reduce the number of oovs. However, there will always be some words or phrases that are new to the system and finding ways to translate such words or phrases will be beneficial to the system. Researchers have applied a number of approaches to tackle this problem. Some approaches use pivot languages (Callison-Burch et al., 2006) while others use lexicon-induction-based approaches from source language monolingual corpora (Koehn and Knight, 2002; Garera et al., 2009; Marton et al., 2009). Pivot language techniques tackle this problem by taking advantage of available parallel data between the source language and a third language. Using a pivot language, oovs are translated into a third language and back into the source language and thereby paraphrases to those oov words are extracted (Callison-Burch et al., 2006). For each oov, the system can be augmented by aggregating the translations of all its paraphrases and assign"
P13-1109,P11-2071,0,0.518554,"Missing"
P13-1109,J93-1003,0,0.034648,"Missing"
P13-1109,R11-1018,0,0.0412122,"1999; Fung and Yee, 1998), or dependency relations (Garera et al., 2009). Laws et al. (2010) used linguistic analysis in the form of graph-based models instead of a vector space. But all of these researches used an available seed lexicon as the basic source of similarity between source and target languages unlike our method which just needs a monolingual corpus of source language which is freely available for many languages and a small bilingual corpora. Some methods tried to alleviate the lack of seed lexicon by using orthographic similarity to extract a seed lexicon (Koehn and Knight, 2002; Fiser and Ljubesic, 2011). But it is not a practical solution in case of unrelated languages. Haghighi et al. (2008) and Daum´e and Jagarlamudi (2011) proposed generative models based on canonical correlation analysis to extract translation lexicons for non-parallel corpora by learning a matching between source and target lexicons. Using monolingual features to represent words, feature vectors are projected from source and target words into a canonical space to find the appropriate matching between them. Their method relies on context features which need a seed lexicon and orthographic features which only works for ph"
P13-1109,P98-1069,0,0.917184,"k paraphrases that have translations in the phrase-table are used to assign translations and scores to each oov word by marginalizing translations over paraphrases: p(t|o) = X p(t|s)p(s|o) s where t is a phrase on the target side, o is the oov word or phrase, and s is a paraphrase of o. p(s|o) is estimated using a similarity measure over DPs and p(t|s) is coming from the phrase-table. We reimplemented this collocational approach for finding translations for oovs and used it as a baseline system. Alternative ways of modeling and comparing distributional profiles have been proposed (Rapp, 1999; Fung and Yee, 1998; Terra and Clarke, 2003; Garera et al., 2009; Marton et al., 2009). We review some of them here and compare their performance in Section 4.3. 2.2 Association Measures Given a word u, its distributional profile DP (u) is constructed by counting surrounding words (in a fixed window size) in a monolingual corpus. 1106 DP (u) = {hA(u, wi )i |wi ∈ V } The counts can be collected in positional3 (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). A(·, ·) is an association measure and can simply be defined as co-occurrence counts within sliding windows. Stro"
P13-1109,W09-1117,0,0.79905,"iversity to visit Simon Fraser University. re-order the words in the target language (Zhang et al., 2012). Increasing the size of the parallel data can reduce the number of oovs. However, there will always be some words or phrases that are new to the system and finding ways to translate such words or phrases will be beneficial to the system. Researchers have applied a number of approaches to tackle this problem. Some approaches use pivot languages (Callison-Burch et al., 2006) while others use lexicon-induction-based approaches from source language monolingual corpora (Koehn and Knight, 2002; Garera et al., 2009; Marton et al., 2009). Pivot language techniques tackle this problem by taking advantage of available parallel data between the source language and a third language. Using a pivot language, oovs are translated into a third language and back into the source language and thereby paraphrases to those oov words are extracted (Callison-Burch et al., 2006). For each oov, the system can be augmented by aggregating the translations of all its paraphrases and assign them to the oov. However, these methods require parallel corpora between the source language and one or multiple pivot languages. Another"
P13-1109,D12-1098,0,0.025325,"eds to be created. The number of possible edges can easily explode in size as there can be as many as O(n2 ) edges where n is the number of nodes. A common practice to control the number of edges is to connect each node to at most k other nodes (k-nearest neighbor). However, finding the top-k nearest nodes to each node requires considering its similarity to all the other nodes which requires O(n2 ) computations and since n is usually very large, doing such is practically intractable. Therefore, researchers usually resort to an approximate k-NN algorithms such as locality-sensitive hashing (?; Goyal et al., 2012). Fortunately, since we use context words as cues for relating their meaning and since the similarity measures are defined based on these cues, the number of neighbors we need to consider for each node is reduced by several orders of magnitude. We incorporate an inverted-index-style data structure which indicates what nodes are neighbors based on each context word. Therefore, the set of neighbors of a node consists of union of all the neighbors bridged by each context word in the DP of the node. However, the number of neighbors to be considered for each node even after this drastic reduction i"
P13-1109,P08-2015,0,0.0438362,"taking advantage of available parallel data between the source language and a third language. Using a pivot language, oovs are translated into a third language and back into the source language and thereby paraphrases to those oov words are extracted (Callison-Burch et al., 2006). For each oov, the system can be augmented by aggregating the translations of all its paraphrases and assign them to the oov. However, these methods require parallel corpora between the source language and one or multiple pivot languages. Another line of work exploits spelling and morphological variants of oov words. Habash (2008) presents techniques for online handling of oov words for Arabic to English such as spelling expansion and morphological expansion. Huang et al. (2011) proposes a method to combine sublexical/constituent translations of an oov word or phrase to generate its translations. Several researchers have applied lexiconinduction methods to create a bilingual lexicon for those oovs. Marton et al. (2009) use a monolingual text on the source side to find paraphrases to oov words for which the translations are available. The translations for these paraphrases are 1105 Proceedings of the 51st Annual Meeting"
P13-1109,P08-1088,0,0.644087,"ble is to be tuned along with the language model on the dev set, and run on the test set. BLEU (Papineni et al., 2002) is still the de facto evaluation metric for machine translation and we use that to measure the quality of our proposed approaches for MT. candiate list particularly specific only particular should and especially support agreement approval accession will approve endorses Table 7: Two examples of oov translations found by our method. 5 Related work There has been a long line of research on learning translation pairs from non-parallel corpora (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Marton et al., 2009; Laws et al., 2010). Most have focused on extracting a translation lexicon by mining monolingual resources of data to find clues, using probabilistic methods to map words, or by exploiting the cross-language evidence of closely related languages. Most of them evaluated only highfrequency words of specific types (nouns or content words) (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Laws et al., 2010) In contrast, we do not consider any constraint on our test data and our data includes many low frequency words. It has"
P13-1109,W11-2123,0,0.00533283,"taset Europarl EMEA Dev types tokens 1893 2229 2325 4317 Test types tokens 1830 2163 2294 4190 Table 2: number of oovs in dev and test sets for Europarl and EMEA systems. For the end-to-end MT pipeline, we used Moses (Koehn et al., 2007) with these standard features: relative-frequency and lexical translation model (TM) probabilities in both directions; distortion model; language model (LM) and word count. Word alignment is done using GIZA++ (Och and Ney, 2003). We used distortion limit of 6 and max-phrase-length of 10 in all the experiments. For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoothing. 4.1.1 Phrase-table Integration Once the translations and their probabilities for each oov are extracted, they are added to the 8 http://www.statmt.org/wpt05/mt-shared-task/ phrase-table that is induced from the parallel text. The probability for new entries are added as a new feature in the log-linear framework to be tuned along with other features. The value of this newly introduced feature for original entries in the phrase-table is set to 1. Similarly, t"
P13-1109,W02-0902,0,0.963888,"rch award from Monash University to visit Simon Fraser University. re-order the words in the target language (Zhang et al., 2012). Increasing the size of the parallel data can reduce the number of oovs. However, there will always be some words or phrases that are new to the system and finding ways to translate such words or phrases will be beneficial to the system. Researchers have applied a number of approaches to tackle this problem. Some approaches use pivot languages (Callison-Burch et al., 2006) while others use lexicon-induction-based approaches from source language monolingual corpora (Koehn and Knight, 2002; Garera et al., 2009; Marton et al., 2009). Pivot language techniques tackle this problem by taking advantage of available parallel data between the source language and a third language. Using a pivot language, oovs are translated into a third language and back into the source language and thereby paraphrases to those oov words are extracted (Callison-Burch et al., 2006). For each oov, the system can be augmented by aggregating the translations of all its paraphrases and assign them to the oov. However, these methods require parallel corpora between the source language and one or multiple piv"
P13-1109,P07-2045,0,0.00763609,"rom the parallel data. From the oovs, we exclude numbers as well as named entities. We apply a simple heuristic to detect named entities: basically words that are capitalized in the original dev/test set that do not appear at the beginning of a sentence are named entities. Table 2 shows the number of oov types and tokens for Europarl and EMEA systems in both dev and test sets. Dataset Europarl EMEA Dev types tokens 1893 2229 2325 4317 Test types tokens 1830 2163 2294 4190 Table 2: number of oovs in dev and test sets for Europarl and EMEA systems. For the end-to-end MT pipeline, we used Moses (Koehn et al., 2007) with these standard features: relative-frequency and lexical translation model (TM) probabilities in both directions; distortion model; language model (LM) and word count. Word alignment is done using GIZA++ (Och and Ney, 2003). We used distortion limit of 6 and max-phrase-length of 10 in all the experiments. For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoothing. 4.1.1 Phrase-table Integration Once the translations and their probabilities for ea"
P13-1109,2005.mtsummit-papers.11,0,0.032002,"Missing"
P13-1109,C10-2070,0,0.235868,"Missing"
P13-1109,P98-2127,0,0.273455,"way (count all the word occurrences within the sliding window). A(·, ·) is an association measure and can simply be defined as co-occurrence counts within sliding windows. Stronger association measures can also be used such as: Conditional probability: the probability for the occurrence of each word in DP given the occurrence of u: CP(u, wi ) = P (wi |u) (Sch¨utze and Pedersen, 1997) Pointwise Mutual Information: this measure is a transformation of the independence assumption into a ratio. Positive values indicate that words co-occur more than what we expect under the independence assumption (Lin, 1998): PMI(u, wi ) = log2 P (u, wi ) P (u)P (wi ) L(P (wi |u); p) ∗ L(P (wi |¬u); p) L(P (wi |u); p1 ) ∗ L(P (wi |¬u); p2 ) where L is likelihood function under the assumption that word counts in text have binomial distributions. The numerator represents the likelihood of the hypothesis that u and wi are independent (P (wi |u) = P (wi |¬u) = p) and the denominator represents the likelihood of the hypothesis that u and wi are dependent (P (wi |u) 6= P (wi |¬u) , P (wi |u) = p1 , P (wi |¬u) = p2 )4 . Chi-square test: is a statistical hypothesis testing method to evaluate independence of two categoric"
P13-1109,P12-1032,0,0.101525,"to a canonical space to find the appropriate matching between them. Their method relies on context features which need a seed lexicon and orthographic features which only works for phylogenetically related languages. Graph-based semi-supervised methods have been shown to be useful for domain adaptation in MT as well. Alexandrescu and Kirchhoff (2009) applied a graph-based method to determine similarities between sentences and use these similarities to promote similar translations for similar sentences. They used a graph-based semi-supervised model to re-rank the n-best translation hypothesis. Liu et al. (2012) extended Alexandrescu’s model to use translation consensus among similar sentences in bilingual training data by developing a new structured label propagation method. They derived some features to use during decoding process that has been shown useful in improving translation quality. Our graph propagation method connects monolingual source phrases with oovs to obtain translation and so is a very different use of graph propagation from these previous works. Recently label propagation has been used for lexicon induction (Tamura et al., 2012). They used a graph based on context similarity as we"
P13-1109,N01-1020,0,0.230157,"s to map words, or by exploiting the cross-language evidence of closely related languages. Most of them evaluated only highfrequency words of specific types (nouns or content words) (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Laws et al., 2010) In contrast, we do not consider any constraint on our test data and our data includes many low frequency words. It has been shown that translation of high-frequency words is easier than low frequency words (Tamura et al., 2012). Some methods have used a third language(s) as pivot or bridge to find translation pairs (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Callison-Burch et al., 2006). 1112 Corpus Europarl EMEA System Baseline Our approach Baseline Our approach MRR – 5.9 – 3.6 Recall – 12.6 – 7.4 Dev Bleu 28.53 28.76 20.05 20.54 Test Bleu 28.97 29.40* 20.34 20.80* * Statistically significant with p < 0.02 using the bootstrap resampling significance test (in Moses). Table 6: Bleu scores for different domains with or without using oov translations. Context similarity has been used effectively in bilingual lexicon induction (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Marton et al.,"
P13-1109,D09-1040,0,0.248917,"on Fraser University. re-order the words in the target language (Zhang et al., 2012). Increasing the size of the parallel data can reduce the number of oovs. However, there will always be some words or phrases that are new to the system and finding ways to translate such words or phrases will be beneficial to the system. Researchers have applied a number of approaches to tackle this problem. Some approaches use pivot languages (Callison-Burch et al., 2006) while others use lexicon-induction-based approaches from source language monolingual corpora (Koehn and Knight, 2002; Garera et al., 2009; Marton et al., 2009). Pivot language techniques tackle this problem by taking advantage of available parallel data between the source language and a third language. Using a pivot language, oovs are translated into a third language and back into the source language and thereby paraphrases to those oov words are extracted (Callison-Burch et al., 2006). For each oov, the system can be augmented by aggregating the translations of all its paraphrases and assign them to the oov. However, these methods require parallel corpora between the source language and one or multiple pivot languages. Another line of work exploits"
P13-1109,J03-1002,0,0.00334896,"beginning of a sentence are named entities. Table 2 shows the number of oov types and tokens for Europarl and EMEA systems in both dev and test sets. Dataset Europarl EMEA Dev types tokens 1893 2229 2325 4317 Test types tokens 1830 2163 2294 4190 Table 2: number of oovs in dev and test sets for Europarl and EMEA systems. For the end-to-end MT pipeline, we used Moses (Koehn et al., 2007) with these standard features: relative-frequency and lexical translation model (TM) probabilities in both directions; distortion model; language model (LM) and word count. Word alignment is done using GIZA++ (Och and Ney, 2003). We used distortion limit of 6 and max-phrase-length of 10 in all the experiments. For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoothing. 4.1.1 Phrase-table Integration Once the translations and their probabilities for each oov are extracted, they are added to the 8 http://www.statmt.org/wpt05/mt-shared-task/ phrase-table that is induced from the parallel text. The probability for new entries are added as a new feature in the log-linear framewor"
P13-1109,P03-1021,0,0.0175584,"Missing"
P13-1109,P02-1040,0,0.0883445,"nder intrinsic and extrinsic evaluation metrics. 1 Introduction Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation. SMT systems usually copy unknown words verbatim to the target language output. Although this is helpful in translating a small fraction of oovs such as named entities for languages with same writing systems, it harms the translation in other types of oovs and distant language pairs. In general, copied-over oovs are a hindrance to fluent, high quality translation, and we can see evidence of this in automatic measures such as BLEU (Papineni et al., 2002) and also in human evaluation scores such as HTER. The problem becomes more severe when only a limited amount of parallel text is available for training or when the training and test data are from different domains. Even noisy translation of oovs can aid the language model to better ∗ This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant. The third author was supported by an early career research award from Monash University to visit Simon Fraser University. re-order the words in the target language (Zhang et al., 2012). Increasing the size of the parallel data can re"
P13-1109,W09-3209,0,0.0184289,"v with different extent for different labeled nodes. The second term (2) enforces the smoothness of the labeling according to the graph structure and edge weights. The last term (3) regularizes the soft labeling for a vertex v to match a priori label Rv , e.g. for high-degree unlabeled nodes (hubs in the graph) we may believe that the neighbors are not going to produce reliable label and hence the probability of undefined label ⊥ should be higher. The optimization problem can be solved with an efficient iterative algorithm which is parallelized in a MapReduce framework (Talukdar et al., 2008; Rao and Yarowsky, 2009). We used the Junto label propagation toolkit (Talukdar and Crammer, 2009) for label propagation. 3.2 Efficient Graph Construction Graph-based approaches can easily become computationally very expensive as the number of nodes grow. In our case, we use phrases in the monolingual text as graph vertices. These phrases are n-grams up to a certain value, which can result in millions of nodes. For each node a distributional profile (DP) needs to be created. The number of possible edges can easily explode in size as there can be as many as O(n2 ) edges where n is the number of nodes. A common practic"
P13-1109,D08-1061,0,0.20752,"Missing"
P13-1109,D12-1003,0,0.379855,"racting a translation lexicon by mining monolingual resources of data to find clues, using probabilistic methods to map words, or by exploiting the cross-language evidence of closely related languages. Most of them evaluated only highfrequency words of specific types (nouns or content words) (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Laws et al., 2010) In contrast, we do not consider any constraint on our test data and our data includes many low frequency words. It has been shown that translation of high-frequency words is easier than low frequency words (Tamura et al., 2012). Some methods have used a third language(s) as pivot or bridge to find translation pairs (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Callison-Burch et al., 2006). 1112 Corpus Europarl EMEA System Baseline Our approach Baseline Our approach MRR – 5.9 – 3.6 Recall – 12.6 – 7.4 Dev Bleu 28.53 28.76 20.05 20.54 Test Bleu 28.97 29.40* 20.34 20.80* * Statistically significant with p < 0.02 using the bootstrap resampling significance test (in Moses). Table 6: Bleu scores for different domains with or without using oov translations. Context similarity has been used effectively in bilingual"
P13-1109,N03-1032,0,0.0251207,"ave translations in the phrase-table are used to assign translations and scores to each oov word by marginalizing translations over paraphrases: p(t|o) = X p(t|s)p(s|o) s where t is a phrase on the target side, o is the oov word or phrase, and s is a paraphrase of o. p(s|o) is estimated using a similarity measure over DPs and p(t|s) is coming from the phrase-table. We reimplemented this collocational approach for finding translations for oovs and used it as a baseline system. Alternative ways of modeling and comparing distributional profiles have been proposed (Rapp, 1999; Fung and Yee, 1998; Terra and Clarke, 2003; Garera et al., 2009; Marton et al., 2009). We review some of them here and compare their performance in Section 4.3. 2.2 Association Measures Given a word u, its distributional profile DP (u) is constructed by counting surrounding words (in a fixed window size) in a monolingual corpus. 1106 DP (u) = {hA(u, wi )i |wi ∈ V } The counts can be collected in positional3 (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). A(·, ·) is an association measure and can simply be defined as co-occurrence counts within sliding windows. Stronger association measure"
P13-1109,P95-1050,0,0.887422,"with target-side translations and their feature values. A graph propagation algorithm is then used to propagate translations from labeled nodes to unlabeled nodes (phrases appearing only in the monolingual text and oovs). This provides a general purpose approach to handle several types of oovs, including morphological variants, spelling variants and synonyms2 . Constructing such a huge graph and propagating messages through it pose severe computational challenges. Throughout the paper, we will see how these challenges are dealt with using scalable algorithms. 2 Collocational Lexicon Induction Rapp (1995) introduced the notion of a distributional profile in bilingual lexicon induction from monolingual data. A distributional profile (DP) of a word or phrase type is a co-occurrence vector created by combining all co-occurrence vectors of the tokens of that phrase type. Each distributional profile can be seen as a point in a |V |-dimensional space where V is the vocabulary where each word type represents a unique axis. Points (i.e. phrase types) that are close to one another in this highdimensional space can represent paraphrases. This approach has also been used in machine translation to find in"
P13-1109,P99-1067,0,0.51044,"2). The top-k paraphrases that have translations in the phrase-table are used to assign translations and scores to each oov word by marginalizing translations over paraphrases: p(t|o) = X p(t|s)p(s|o) s where t is a phrase on the target side, o is the oov word or phrase, and s is a paraphrase of o. p(s|o) is estimated using a similarity measure over DPs and p(t|s) is coming from the phrase-table. We reimplemented this collocational approach for finding translations for oovs and used it as a baseline system. Alternative ways of modeling and comparing distributional profiles have been proposed (Rapp, 1999; Fung and Yee, 1998; Terra and Clarke, 2003; Garera et al., 2009; Marton et al., 2009). We review some of them here and compare their performance in Section 4.3. 2.2 Association Measures Given a word u, its distributional profile DP (u) is constructed by counting surrounding words (in a fixed window size) in a monolingual corpus. 1106 DP (u) = {hA(u, wi )i |wi ∈ V } The counts can be collected in positional3 (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). A(·, ·) is an association measure and can simply be defined as co-occurrence counts within s"
P13-1109,W02-2026,0,0.377279,"loiting the cross-language evidence of closely related languages. Most of them evaluated only highfrequency words of specific types (nouns or content words) (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Laws et al., 2010) In contrast, we do not consider any constraint on our test data and our data includes many low frequency words. It has been shown that translation of high-frequency words is easier than low frequency words (Tamura et al., 2012). Some methods have used a third language(s) as pivot or bridge to find translation pairs (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Callison-Burch et al., 2006). 1112 Corpus Europarl EMEA System Baseline Our approach Baseline Our approach MRR – 5.9 – 3.6 Recall – 12.6 – 7.4 Dev Bleu 28.53 28.76 20.05 20.54 Test Bleu 28.97 29.40* 20.34 20.80* * Statistically significant with p < 0.02 using the bootstrap resampling significance test (in Moses). Table 6: Bleu scores for different domains with or without using oov translations. Context similarity has been used effectively in bilingual lexicon induction (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Marton et al., 2009; Laws et al., 2010). It"
P13-1109,C98-1066,0,\N,Missing
P13-1109,P05-1077,0,\N,Missing
P13-1109,C98-2122,0,\N,Missing
P13-1109,D08-1076,0,\N,Missing
P17-2083,W14-4012,0,0.0499521,"Missing"
P17-2083,N16-1037,1,0.86789,"se innovations. 1 Introduction Dialogue Act (DA) classification is a sequenceto-sequence learning task where a sequence of utterances is mapped into a sequence of DAs. Some works in DA classification treat each utterance as an independent instance (Julia et al., 2010; Gamb¨ack et al., 2011), which leads to ignoring important long-range dependencies in the dialogue history. Other works have captured inter-utterance relationships using models such as Hidden Markov Models (HMMs) (Stolcke et al., 2000; Surendran and Levow, 2006) or Recurrent Neural Networks (RNNs) (Kalchbrenner and Blunsom, 2013; Ji et al., 2016), where RNNs have been particularly successful. In this paper, we present a generative model of utterances and dialogue acts which conditions on the relevant part of the dialogue history. To this effect, we use the attention mechanism (Bahdanau et al., 2014) developed originally for sequence-tosequence models, which has proven effective in Machine Translation (Bahdanau et al., 2014; Luong et al., 2015) and DA classification (Shen and 2 Model Description Assume that we have a training dataset D comprising a collection of dialogues, where each dialogue consists of a sequence of utterances {yt }T"
P17-2083,W13-3214,0,0.0748741,"the effectiveness of each of these innovations. 1 Introduction Dialogue Act (DA) classification is a sequenceto-sequence learning task where a sequence of utterances is mapped into a sequence of DAs. Some works in DA classification treat each utterance as an independent instance (Julia et al., 2010; Gamb¨ack et al., 2011), which leads to ignoring important long-range dependencies in the dialogue history. Other works have captured inter-utterance relationships using models such as Hidden Markov Models (HMMs) (Stolcke et al., 2000; Surendran and Levow, 2006) or Recurrent Neural Networks (RNNs) (Kalchbrenner and Blunsom, 2013; Ji et al., 2016), where RNNs have been particularly successful. In this paper, we present a generative model of utterances and dialogue acts which conditions on the relevant part of the dialogue history. To this effect, we use the attention mechanism (Bahdanau et al., 2014) developed originally for sequence-tosequence models, which has proven effective in Machine Translation (Bahdanau et al., 2014; Luong et al., 2015) and DA classification (Shen and 2 Model Description Assume that we have a training dataset D comprising a collection of dialogues, where each dialogue consists of a sequence of"
P17-2083,D15-1166,0,0.137245,"Missing"
P17-2083,J00-3003,0,0.900694,"Missing"
P17-2083,P16-1122,0,0.0275065,"ut yn , and the previous hidden state hn−1 : αn = g(hn−1 , E yn ) , 2.2 For prediction, we choose the sequence of dialogue acts with the highest posterior probability: 0 0 arg max PΘ (z1:T |y1:T ) = arg max PΘ (z1:T , y1:T ) 0 0 z1:T eαn , αn0 n0 =1 e where g is a non-linear function. Once the attention is defined, the representation of the input is constructed as X c= anh n . (6) n The problem with this traditional attention model is that the final hidden state is a function of all the inputs, hence it is usually more “informative” than the earlier hidden states due to semantic accumulation (Wang et al., 2016). Thus, most of the attention signal is assigned to the hidden states toward the end of a sequence. In DA classification, this may not be desirable, since an important token with respect to a dialogue act can appear anywhere in an utterance. We call this the attention bias problem. We propose a novel gated attention mechanism, which is inspired by the gating mechanism in LSTMs, to fix the attention bias problem. Similar to the forget gate of LSTMs, we use the available information to calculate an attention gate that learns whether to allow the whole input signal to pass through or to forget al"
P17-2083,C10-2150,0,0.0601902,"Missing"
P18-1026,P17-2021,0,0.0266792,"n our work. However, compared to our approach, linearisation incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated data. 7 Acknowledgements This work was supported by the Australian Research Council (DP160102686). The research reported in this paper was partly conducted at the 2017 Frederick Jelinek Memorial Summer Workshop on Speech and Language Technologies, hosted at Carnegie Mellon University and sponsored by Johns Hopkins University with unrestricted gifts from Amazon, Apple, Facebook, Google, and Microsoft. The authors would also like to thank Joost Basti"
P18-1026,N04-1035,0,0.328773,"Missing"
P18-1026,W13-2322,0,0.267685,"Missing"
P18-1026,D17-1209,0,0.130802,"Missing"
P18-1026,W17-4755,0,0.0130573,"available in gold-standard form. Table 2: Results for syntax-based NMT on the test sets. All score differences between our models and the corresponding baselines are significantly different (p&lt;0.05), including the negative CHRF++ result for En-Cs. Interestingly, we found different trends when analysing the CHRF++ numbers. In particular, this metric favours the PB-SMT models for both language pairs, while also showing improved performance for s2s in En-Cs. CHRF++ has been shown to better correlate with human judgments compared to BLEU, both at system and sentence level for both language pairs (Bojar et al., 2017), which motivated our choice as an additional metric. We leave further investigation of this phenomena for future work. Neural networks for graphs Recurrent networks on general graphs were first proposed un280 sation and parameter explosion. In particular, we showed how graph transformations can solve issues with graph-based networks without changing the underlying architecture. This is the case of the proposed Levi graph transformation, which ensures the decoder can attend to edges as well as nodes, but also to the sequential connections added to the dependency trees in the case of NMT. Overa"
P18-1026,W14-3333,0,0.0148702,"emble of the 5 models. Finally, we also report the number of parameters used in each model. Since our encoder architectures are quite different, we try to match the number of parameters between them by changing the dimensionality of the hidden layers (as explained above). We do this to minimise the effects of model capacity as a confounder. Evaluation Following previous work, we evaluate our models using BLEU (Papineni et al., 2001) and perform bootstrap resampling to check statistical significance. However, since recent work has questioned the effectiveness of BLEU with bootstrap resampling (Graham et al., 2014), we also report results using sentence-level CHR F++ (Popovi´c, 2017), using the Wilcoxon signed-rank test to check significance. Evaluation is case-insensitive for both metrics. Recent work has shown that evaluation in neural models can lead to wrong conclusions by just changing the random seed (Reimers and Gurevych, 2017). In an effort to make our conclusions more robust, we run each model 5 times using different seeds. From each pool, we report 4.2 Results and analysis Table 1 shows the results on the test set. For the s2s models, we also report results without the scope marking procedure"
P18-1026,D17-1012,0,0.0183755,"incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated data. 7 Acknowledgements This work was supported by the Australian Research Council (DP160102686). The research reported in this paper was partly conducted at the 2017 Frederick Jelinek Memorial Summer Workshop on Speech and Language Technologies, hosted at Carnegie Mellon University and sponsored by Johns Hopkins University with unrestricted gifts from Amazon, Apple, Facebook, Google, and Microsoft. The authors would also like to thank Joost Bastings for sharing the data from his paper’s experiments. Discussio"
P18-1026,E17-3017,0,0.0312247,", 2005; Scarselli et al., 2009) assume a fixed point representation of the parameters and learn using contraction maps. Li et al. (2016) argues that this restricts the capacity of the model and makes it harder to learn long distance relations between nodes. To tackle these issues, they propose Gated Graph Neural Networks, which extend these architectures with gating mechanisms =σ crv X W`re h(t−1) u + br`e u∈Nv ! ztv = σ czv X W`ze h(t−1) + bz`e u u∈Nv et h v = ρ cv X W`e  rtu h(t−1) u  ! + b`e u∈Nv htv 1 Our implementation uses MXNet (Chen et al., 2015) and is based on the Sockeye toolkit (Hieber et al., 2017). Code is available at github.com/beckdaniel/acl2018_ graph2seq. et = (1 − ztv ) h(i−1) + ztv h v v where e = (u, v, `e ) is the edge between nodes u and v, N (v) is the set of neighbour nodes for v, ρ is a non-linear function, σ is the sigmoid function 274 and cv = czv = crv = |Nv |−1 are normalisation constants. Our formulation differs from the original GGNNs from Li et al. (2016) in some aspects: 1) we add bias vectors for the hidden state, reset gate and update gate computations; 2) labelspecific matrices do not share any components; 3) reset gates are applied to all hidden states before a"
P18-1026,P13-1091,0,0.0244612,"– English-Czech – – – BLEU CHR F++ #params Single models PB-SMT s2s g2s g2s+ 8.6 8.9 8.7 9.8 36.4 33.8 32.3 33.3 – 39.1M 38.4M 38.8M Ensembles s2s g2s g2s+ 11.3 10.4 11.7 36.4 34.7 35.9 195M 192M 194M Results from (Bastings et al., 2017) BoW+GCN 7.5 – BiRNN 8.9 – BiRNN+GCN 9.6 – – – – 6 Related work Graph-to-sequence modelling Early NLP approaches for this problem were based on Hyperedge Replacement Grammars (Drewes et al., 1997, HRGs). These grammars assume the transduction problem can be split into rules that map portions of a graph to a set of tokens in the output sequence. In particular, Chiang et al. (2013) defines a parsing algorithm, followed by a complexity analysis, while Jones et al. (2012) report experiments on semantic-based machine translation using HRGs. HRGs were also used in previous work on AMR parsing (Peng et al., 2015). The main drawback of these grammar-based approaches though is the need for alignments between graph nodes and surface tokens, which are usually not available in gold-standard form. Table 2: Results for syntax-based NMT on the test sets. All score differences between our models and the corresponding baselines are significantly different (p&lt;0.05), including the negat"
P18-1026,C12-1083,0,0.111997,"Missing"
P18-1026,P16-1078,0,0.0208166,"rdamghani et al., 2016) and (Konstas et al., 2017), which showed that graph simplification and anonymisation are key to good performance, a procedure we also employ in our work. However, compared to our approach, linearisation incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated data. 7 Acknowledgements This work was supported by the Australian Research Council (DP160102686). The research reported in this paper was partly conducted at the 2017 Frederick Jelinek Memorial Summer Workshop on Speech and Language Technologies, hosted at Carnegie Mellon Universi"
P18-1026,P17-2012,0,0.0159373,"d to our approach, linearisation incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated data. 7 Acknowledgements This work was supported by the Australian Research Council (DP160102686). The research reported in this paper was partly conducted at the 2017 Frederick Jelinek Memorial Summer Workshop on Speech and Language Technologies, hosted at Carnegie Mellon University and sponsored by Johns Hopkins University with unrestricted gifts from Amazon, Apple, Facebook, Google, and Microsoft. The authors would also like to thank Joost Bastings for sharing the data"
P18-1026,P07-2045,0,0.00451381,"g2s models are almost the same as in the AMR generation experiments (§4.1). The only exception is the GGNN encoder dimensionality, where we use 512 for the experiments with dependency trees only and 448 when the inputs have additional sequential connections. As in the AMR generation setting, we do this to ensure model capacity are comparable in the number of parameters. Another key difference is that the s2s baselines do not use dependency trees: they are trained on the sentences only. In addition to neural models, we also report results for Phrase-Based Statistical MT (PB-SMT), using Moses (Koehn et al., 2007). The PB-SMT models are trained using the same data conditions as s2s (no dependency trees) and use the standard setup in Moses, except for the language model, where we use a 5-gram LM trained on the target side of the respective parallel corpus.8 There ROOT is expl a nsubj deeper punct issue det at amod stake prep . pobj Figure 4: Top: a sentence with its corresponding dependency tree. Bottom: the transformed tree into a Levi graph with additional sequential connections between words (dashed lines). The full graph also contains reverse and self edges, which are omitted in the figure. 5.2 Resu"
P18-1026,N16-1087,0,0.498343,"al language. In particular, many wholesentence semantic frameworks employ directed acyclic graphs as the underlying formalism, while most tree-based syntactic representations can also be seen as graphs. A range of NLP applications can be framed as the process of transducing a graph structure into a sequence. For instance, language generation may involve realising a semantic graph into a surface form and syntactic machine translation involves transforming a tree-annotated source sentence to its translation. Previous work in this setting rely on grammarbased approaches such as tree transducers (Flanigan et al., 2016) and hyperedge replacement gram273 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 273–283 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics want-01 ARG0 G1 AR believe-01 0 boy ARG G1 AR girl Figure 1: Left: the AMR graph representing the sentence “The boy wants the girl to believe him.”. Right: Our proposed architecture using the same AMR graph as input and the surface form as output. The first layer is a concatenation of node and positional embeddings, using distance from the root node as th"
P18-1026,P17-1014,0,0.269851,"d.beck,t.cohn}@unimelb.edu.au ‡ Faculty of Information Technology Monash University, Australia gholamreza.haffari@monash.edu Abstract mars (Jones et al., 2012). A key limitation of these approaches is that alignments between graph nodes and surface tokens are required. These alignments are usually automatically generated so they can propagate errors when building the grammar. More recent approaches transform the graph into a linearised form and use off-the-shelf methods such as phrase-based machine translation (Pourdamghani et al., 2016) or neural sequenceto-sequence (henceforth, s2s) models (Konstas et al., 2017). Such approaches ignore the full graph structure, discarding key information. In this work we propose a model for graph-tosequence (henceforth, g2s) learning that leverages recent advances in neural encoder-decoder architectures. Specifically, we employ an encoder based on Gated Graph Neural Networks (Li et al., 2016, GGNNs), which can incorporate the full graph structure without loss of information. Such networks represent edge information as label-wise parameters, which can be problematic even for small sized label vocabularies (in the order of hundreds). To address this limitation, we also"
P18-1026,P16-1162,0,0.097942,"anistan to block drug supplies. Figure 3: Example showing overgeneration due to reentrancies. Top: original AMR graph with key reentrancies highlighted. Bottom: reference and outputs generated by the s2s and g2s models, highlighting the overgeneration phenomena. 5.1 Experimental setup Data and preprocessing We employ the same data and settings from Bastings et al. (2017),5 which use the News Commentary V11 corpora from the WMT16 translation task.6 English text is tokenised and parsed using SyntaxNet7 while German and Czech texts are tokenised and split into subwords using byte-pair encodings (Sennrich et al., 2016, BPE) (8000 merge operations). ence previously defined concepts in the graph. In the s2s models including Konstas et al. (2017), reentrant nodes are copied in the linearised form, while this is not necessary for our g2s models. We can see that the s2s prediction overgenerates the “India and China” phrase. The g2s prediction avoids overgeneration, and almost perfectly matches the reference. While this is only a single example, it provides evidence that retaining the full graphical structure is beneficial for this task, which is corroborated by our quantitative results. 5 We obtained the data f"
P18-1026,P17-2002,0,0.267833,".3 50.4 28.4M 28.4M 28.3M Ensembles s2s s2s (-s) g2s 52.5 48.9 53.5 142M 142M 141M 26.6 22.0 27.5 Previous work (early AMR treebank versions) KIYCZ17 22.0 – – Previous work (as above + unlabelled data) KIYCZ17 33.8 – – PKH16 26.9 – – SPZWG17 25.6 – – FDSC16 22.0 – – Table 1: Results for AMR generation on the test set. All score differences between our models and the corresponding baselines are significantly different (p&lt;0.05). “(-s)” means input without scope marking. KIYCZ17, PKH16, SPZWG17 and FDSC16 are respectively the results reported in Konstas et al. (2017), Pourdamghani et al. (2016), Song et al. (2017) and Flanigan et al. (2016). results using the median model according to performance on the dev set (simulating what is expected from a single run) and using an ensemble of the 5 models. Finally, we also report the number of parameters used in each model. Since our encoder architectures are quite different, we try to match the number of parameters between them by changing the dimensionality of the hidden layers (as explained above). We do this to minimise the effects of model capacity as a confounder. Evaluation Following previous work, we evaluate our models using BLEU (Papineni et al., 2001)"
P18-1026,P06-1077,0,0.0773311,"ensor factorisation to reduce the number of parameters. Applications Early work on AMR generation employs grammars and transducers (Flanigan et al., 2016; Song et al., 2017). Linearisation approaches include (Pourdamghani et al., 2016) and (Konstas et al., 2017), which showed that graph simplification and anonymisation are key to good performance, a procedure we also employ in our work. However, compared to our approach, linearisation incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated data. 7 Acknowledgements This work was supported by the Australi"
P18-1026,D15-1166,0,0.187779,"Missing"
P18-1026,D17-1159,0,0.510024,"es for v, ρ is a non-linear function, σ is the sigmoid function 274 and cv = czv = crv = |Nv |−1 are normalisation constants. Our formulation differs from the original GGNNs from Li et al. (2016) in some aspects: 1) we add bias vectors for the hidden state, reset gate and update gate computations; 2) labelspecific matrices do not share any components; 3) reset gates are applied to all hidden states before any computation and 4) we add normalisation constants. These modifications were applied based on preliminary experiments and ease of implementation. An alternative to GGNNs is the model from Marcheggiani and Titov (2017), which add edge label information to Graph Convolutional Networks (GCNs). According to Li et al. (2016), the main difference between GCNs and GGNNs is analogous to the difference between convolutional and recurrent networks. More specifically, GGNNs can be seen as multi-layered GCNs where layer-wise parameters are tied and gating mechanisms are added. A large number of layers can propagate node information between longer distances in the graph and, unlike GCNs, GGNNs can have an arbitrary number of layers without increasing the number of parameters. Nevertheless, our architecture borrows idea"
P18-1026,J97-3002,0,0.124119,"is proposed by Schlichtkrull et al. (2017), which uses tensor factorisation to reduce the number of parameters. Applications Early work on AMR generation employs grammars and transducers (Flanigan et al., 2016; Song et al., 2017). Linearisation approaches include (Pourdamghani et al., 2016) and (Konstas et al., 2017), which showed that graph simplification and anonymisation are key to good performance, a procedure we also employ in our work. However, compared to our approach, linearisation incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated"
P18-1026,P01-1067,0,0.342365,"d by Schlichtkrull et al. (2017), which uses tensor factorisation to reduce the number of parameters. Applications Early work on AMR generation employs grammars and transducers (Flanigan et al., 2016; Song et al., 2017). Linearisation approaches include (Pourdamghani et al., 2016) and (Konstas et al., 2017), which showed that graph simplification and anonymisation are key to good performance, a procedure we also employ in our work. However, compared to our approach, linearisation incurs in loss of information. MT has a long history of previous work that aims at incorporating syntax (Wu, 1997; Yamada and Knight, 2001; Galley et al., 2004; Liu et al., 2006, inter alia). This idea has also been investigated in the context of NMT. Bastings et al. (2017) is the most similar work to ours, and we benchmark against their approach in our NMT experiments. Eriguchi et al. (2016) also employs source syntax, but using constituency trees instead. Other approaches have investigated the use of syntax in the target language (Aharoni and Goldberg, 2017; Eriguchi et al., 2017). Finally, Hashimoto and Tsuruoka (2017) treats source syntax as a latent variable, which can be pretrained using annotated data. 7 Acknowledgements"
P18-1026,P02-1038,0,0.0590062,"best results. However, the g2s+ models outperform the baselines in terms of BLEU scores under the same parameter budget, in both single model and ensemble scenarios. This result show that it is possible to incorporate sequential biases in our model without relying on RNNs or any other modification in the architecture. Evaluation We report results in terms of BLEU and CHRF++, using case-sensitive versions of both metrics. Other settings are kept the same as in the AMR generation experiments (§4.1). For PBSMT, we also report the median result of 5 runs, obtained by tuning the model using MERT (Och and Ney, 2002) 5 times. 8 Note that target data is segmented using BPE, which is not the usual setting for PB-SMT. We decided to keep the segmentation to ensure data conditions are the same. 279 We also show some of the results reported by Bastings et al. (2017) in Table 2. Note that their results were based on a different implementation, which may explain some variation in performance. Their BoW+GCN model is the most similar to ours, as it uses only an embedding layer and a GCN encoder. We can see that even our simpler g2s model outperforms their results. A key difference between their approach and ours is"
P18-1026,2001.mtsummit-papers.68,0,0.00892287,"6), Song et al. (2017) and Flanigan et al. (2016). results using the median model according to performance on the dev set (simulating what is expected from a single run) and using an ensemble of the 5 models. Finally, we also report the number of parameters used in each model. Since our encoder architectures are quite different, we try to match the number of parameters between them by changing the dimensionality of the hidden layers (as explained above). We do this to minimise the effects of model capacity as a confounder. Evaluation Following previous work, we evaluate our models using BLEU (Papineni et al., 2001) and perform bootstrap resampling to check statistical significance. However, since recent work has questioned the effectiveness of BLEU with bootstrap resampling (Graham et al., 2014), we also report results using sentence-level CHR F++ (Popovi´c, 2017), using the Wilcoxon signed-rank test to check significance. Evaluation is case-insensitive for both metrics. Recent work has shown that evaluation in neural models can lead to wrong conclusions by just changing the random seed (Reimers and Gurevych, 2017). In an effort to make our conclusions more robust, we run each model 5 times using differ"
P18-1026,K15-1004,0,0.019093,"2017) BoW+GCN 7.5 – BiRNN 8.9 – BiRNN+GCN 9.6 – – – – 6 Related work Graph-to-sequence modelling Early NLP approaches for this problem were based on Hyperedge Replacement Grammars (Drewes et al., 1997, HRGs). These grammars assume the transduction problem can be split into rules that map portions of a graph to a set of tokens in the output sequence. In particular, Chiang et al. (2013) defines a parsing algorithm, followed by a complexity analysis, while Jones et al. (2012) report experiments on semantic-based machine translation using HRGs. HRGs were also used in previous work on AMR parsing (Peng et al., 2015). The main drawback of these grammar-based approaches though is the need for alignments between graph nodes and surface tokens, which are usually not available in gold-standard form. Table 2: Results for syntax-based NMT on the test sets. All score differences between our models and the corresponding baselines are significantly different (p&lt;0.05), including the negative CHRF++ result for En-Cs. Interestingly, we found different trends when analysing the CHRF++ numbers. In particular, this metric favours the PB-SMT models for both language pairs, while also showing improved performance for s2s"
P18-1026,W17-4770,0,0.13252,"Missing"
P18-1026,W16-6603,0,0.571969,"† School of Computing and Information Systems University of Melbourne, Australia {d.beck,t.cohn}@unimelb.edu.au ‡ Faculty of Information Technology Monash University, Australia gholamreza.haffari@monash.edu Abstract mars (Jones et al., 2012). A key limitation of these approaches is that alignments between graph nodes and surface tokens are required. These alignments are usually automatically generated so they can propagate errors when building the grammar. More recent approaches transform the graph into a linearised form and use off-the-shelf methods such as phrase-based machine translation (Pourdamghani et al., 2016) or neural sequenceto-sequence (henceforth, s2s) models (Konstas et al., 2017). Such approaches ignore the full graph structure, discarding key information. In this work we propose a model for graph-tosequence (henceforth, g2s) learning that leverages recent advances in neural encoder-decoder architectures. Specifically, we employ an encoder based on Gated Graph Neural Networks (Li et al., 2016, GGNNs), which can incorporate the full graph structure without loss of information. Such networks represent edge information as label-wise parameters, which can be problematic even for small sized labe"
P18-1026,D17-1035,0,0.0255313,"pacity as a confounder. Evaluation Following previous work, we evaluate our models using BLEU (Papineni et al., 2001) and perform bootstrap resampling to check statistical significance. However, since recent work has questioned the effectiveness of BLEU with bootstrap resampling (Graham et al., 2014), we also report results using sentence-level CHR F++ (Popovi´c, 2017), using the Wilcoxon signed-rank test to check significance. Evaluation is case-insensitive for both metrics. Recent work has shown that evaluation in neural models can lead to wrong conclusions by just changing the random seed (Reimers and Gurevych, 2017). In an effort to make our conclusions more robust, we run each model 5 times using different seeds. From each pool, we report 4.2 Results and analysis Table 1 shows the results on the test set. For the s2s models, we also report results without the scope marking procedure of Konstas et al. (2017). Our approach significantly outperforms the s2s baselines both with individual models and ensembles, while using a comparable number of parameters. In particular, we obtain these results without relying on scoping heuristics. On Figure 3 we show an example where our model outperforms the baseline. Th"
P18-1118,2012.eamt-1.60,0,0.214568,"ical sentencedocument RNNs to transform the document translations into memory cells (similar to what we do for the source memory); however, it would have been computationally expensive and may have resulted in error propagation. We will show in the experiments that our efficient target memory construction is indeed effective. 5 Experiments and Analysis Datasets. We conducted experiments on three language pairs: French-English, German-English and Estonian-English. Table 1 shows the statistics of the datasets used in our experiments. The French-English dataset is based on the TED Talks corpus1 (Cettolo et al., 2012) where each talk is considered a document. The EstonianEnglish data comes from the Europarl v7 corpus2 (Koehn, 2005). Following Smith et al. (2013), we split the speeches based on the SPEAKER tag and treat them as documents. The FrenchEnglish and Estonian-English corpora were randomly split into train/dev/test sets. For GermanEnglish, we use the News Commentary v9 corpus3 for training, news-dev2009 for development, 1 https://wit3.fbk.eu/ http://www.statmt.org/europarl/ 3 http://statmt.org/wmt14/news-commentary-v9-bydocument.tgz 1278 2 # docs # sents doc len src/tgt vocab Fr-En 10/1.2/1.5 123/1"
P18-1118,W14-4012,0,0.185181,"Missing"
P18-1118,P11-2031,0,0.0710704,"we report statistics of the two test sets news-test2011 and news-test2016. and news-test2011 and news-test2016 as the test sets. The news-commentary corpus has document boundaries already provided. We pre-processed all corpora to remove very short documents and those with missing translations. Out-of-vocabulary and rare words (frequency less than 5) are replaced by the &lt;UNK&gt; token, following Cohn et al. (2016).4 Evaluation Measures We use BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores to measure the quality of the generated translations. We use bootstrap resampling (Clark et al., 2011) to measure statistical significance, p &lt; 0.05, comparing to the baselines. Implementation and Hyperparameters We implement our document-level neural machine translation model in C++ using the DyNet library (Neubig et al., 2017), on top of the basic sentence-level NMT implementation in mantis (Cohn et al., 2016). For the source memory, the sentence and document-level bidirectional RNNs use LSTM and GRU units, respectively. The translation model uses GRU units for the bidirectional RNN encoder and the 2-layer RNN decoder. GRUs are used instead of LSTMs to reduce the number of parameters in the"
P18-1118,N16-1102,1,0.938899,"27/19 45.1/34.7 Table 1: Training/dev/test corpora statistics: number of documents (×100) and sentences (×1000), average document length (in sentences) and source/target vocabulary size (×1000). For DeEn, we report statistics of the two test sets news-test2011 and news-test2016. and news-test2011 and news-test2016 as the test sets. The news-commentary corpus has document boundaries already provided. We pre-processed all corpora to remove very short documents and those with missing translations. Out-of-vocabulary and rare words (frequency less than 5) are replaced by the &lt;UNK&gt; token, following Cohn et al. (2016).4 Evaluation Measures We use BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores to measure the quality of the generated translations. We use bootstrap resampling (Clark et al., 2011) to measure statistical significance, p &lt; 0.05, comparing to the baselines. Implementation and Hyperparameters We implement our document-level neural machine translation model in C++ using the DyNet library (Neubig et al., 2017), on top of the basic sentence-level NMT implementation in mantis (Cohn et al., 2016). For the source memory, the sentence and document-level bidirectional RNNs use LS"
P18-1118,W15-4908,0,0.213129,"Missing"
P18-1118,D11-1084,0,0.725746,"rpasses the traditional statistical MT (Luong et al., 2015) while enjoying more flexibility and significantly less manual effort for feature engineering. Despite their flexibility, most neural MT models translate sentences independently. Discourse phenomenon such as pronominal anaphora and lexical consistency, may depend on long-range dependency going farther than a few previous sentences, are neglected in sentencebased translation (Bawden et al., 2017). There are only a handful of attempts to document-wide machine translation in statistical and neural MT camps. Hardmeier and Federico (2010); Gong et al. (2011); Garcia et al. (2014) propose document translation models based on statistical MT but are restrictive in the way they incorporate the document-level information and fail to gain significant improvements. More recently, there have been a few attempts to incorporate source side context into neural MT (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017); however, these works only consider a very local context including a few previous source/target sentences, ignoring the global source and target documental contexts. The latter two report deteriorated performance when using the target-side"
P18-1118,2010.iwslt-papers.10,0,0.438067,"par, and in some cases, even surpasses the traditional statistical MT (Luong et al., 2015) while enjoying more flexibility and significantly less manual effort for feature engineering. Despite their flexibility, most neural MT models translate sentences independently. Discourse phenomenon such as pronominal anaphora and lexical consistency, may depend on long-range dependency going farther than a few previous sentences, are neglected in sentencebased translation (Bawden et al., 2017). There are only a handful of attempts to document-wide machine translation in statistical and neural MT camps. Hardmeier and Federico (2010); Gong et al. (2011); Garcia et al. (2014) propose document translation models based on statistical MT but are restrictive in the way they incorporate the document-level information and fail to gain significant improvements. More recently, there have been a few attempts to incorporate source side context into neural MT (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017); however, these works only consider a very local context including a few previous source/target sentences, ignoring the global source and target documental contexts. The latter two report deteriorated performance when u"
P18-1118,D12-1108,0,0.183699,"attempts to document MT, but they are either restrictive or do not lead to significant improvements. Hardmeier and Federico (2010) identify links among words in the source document using a word-dependency model to improve translation of anaphoric pronouns. Gong et al. (2011) make use of a cache-based system to save relevant information from the previously generated translations and use that to enhance document-level translation. Garcia et al. (2014) propose a two-pass approach to improve the translations already obtained by a sentencelevel model. Docent is an SMT-based document-level decoder (Hardmeier et al., 2012, 2013), which tries to modify the initial translation generated by the Moses decoder (Koehn et al., 2007) through stochastic local search and hill-climbing. Garcia et al. (2015) make use of neural-based continuous word representations to incorporate distributional semantics into Docent. In another work, Garcia et al. (2017) incorporate new word embedding features into Docent to improve the lexical consistency of translations. The proposed methods fail to yield improvements upon automatic evaluation. Larger Context Neural MT 1282 Jean et al. (2017) extend the vanilla attention-based neural MT"
P18-1118,P13-4033,0,0.103616,"Missing"
P18-1118,2005.mtsummit-papers.11,0,0.273692,"ory); however, it would have been computationally expensive and may have resulted in error propagation. We will show in the experiments that our efficient target memory construction is indeed effective. 5 Experiments and Analysis Datasets. We conducted experiments on three language pairs: French-English, German-English and Estonian-English. Table 1 shows the statistics of the datasets used in our experiments. The French-English dataset is based on the TED Talks corpus1 (Cettolo et al., 2012) where each talk is considered a document. The EstonianEnglish data comes from the Europarl v7 corpus2 (Koehn, 2005). Following Smith et al. (2013), we split the speeches based on the SPEAKER tag and treat them as documents. The FrenchEnglish and Estonian-English corpora were randomly split into train/dev/test sets. For GermanEnglish, we use the News Commentary v9 corpus3 for training, news-dev2009 for development, 1 https://wit3.fbk.eu/ http://www.statmt.org/europarl/ 3 http://statmt.org/wmt14/news-commentary-v9-bydocument.tgz 1278 2 # docs # sents doc len src/tgt vocab Fr-En 10/1.2/1.5 123/15/19 123/128/124 25.1/21 Et-En 150/10/18 209/14/25 14/14/14 48.6/24.9 De-En 49/.9/1.1/1.6 191/2/3/3 39/23/27/19 45.1"
P18-1118,P07-2045,0,0.0150483,"r and Federico (2010) identify links among words in the source document using a word-dependency model to improve translation of anaphoric pronouns. Gong et al. (2011) make use of a cache-based system to save relevant information from the previously generated translations and use that to enhance document-level translation. Garcia et al. (2014) propose a two-pass approach to improve the translations already obtained by a sentencelevel model. Docent is an SMT-based document-level decoder (Hardmeier et al., 2012, 2013), which tries to modify the initial translation generated by the Moses decoder (Koehn et al., 2007) through stochastic local search and hill-climbing. Garcia et al. (2015) make use of neural-based continuous word representations to incorporate distributional semantics into Docent. In another work, Garcia et al. (2017) incorporate new word embedding features into Docent to improve the lexical consistency of translations. The proposed methods fail to yield improvements upon automatic evaluation. Larger Context Neural MT 1282 Jean et al. (2017) extend the vanilla attention-based neural MT model (Bahdanau et al., 2015) by conditioning the decoder on the previous sentence via attention over its"
P18-1118,W07-0734,0,0.0957653,"and sentences (×1000), average document length (in sentences) and source/target vocabulary size (×1000). For DeEn, we report statistics of the two test sets news-test2011 and news-test2016. and news-test2011 and news-test2016 as the test sets. The news-commentary corpus has document boundaries already provided. We pre-processed all corpora to remove very short documents and those with missing translations. Out-of-vocabulary and rare words (frequency less than 5) are replaced by the &lt;UNK&gt; token, following Cohn et al. (2016).4 Evaluation Measures We use BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores to measure the quality of the generated translations. We use bootstrap resampling (Clark et al., 2011) to measure statistical significance, p &lt; 0.05, comparing to the baselines. Implementation and Hyperparameters We implement our document-level neural machine translation model in C++ using the DyNet library (Neubig et al., 2017), on top of the basic sentence-level NMT implementation in mantis (Cohn et al., 2016). For the source memory, the sentence and document-level bidirectional RNNs use LSTM and GRU units, respectively. The translation model uses GRU units for the bidirectional RNN"
P18-1118,P02-1040,0,0.101189,"tistics: number of documents (×100) and sentences (×1000), average document length (in sentences) and source/target vocabulary size (×1000). For DeEn, we report statistics of the two test sets news-test2011 and news-test2016. and news-test2011 and news-test2016 as the test sets. The news-commentary corpus has document boundaries already provided. We pre-processed all corpora to remove very short documents and those with missing translations. Out-of-vocabulary and rare words (frequency less than 5) are replaced by the &lt;UNK&gt; token, following Cohn et al. (2016).4 Evaluation Measures We use BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores to measure the quality of the generated translations. We use bootstrap resampling (Clark et al., 2011) to measure statistical significance, p &lt; 0.05, comparing to the baselines. Implementation and Hyperparameters We implement our document-level neural machine translation model in C++ using the DyNet library (Neubig et al., 2017), on top of the basic sentence-level NMT implementation in mantis (Cohn et al., 2016). For the source memory, the sentence and document-level bidirectional RNNs use LSTM and GRU units, respectively. The translation model uses"
P18-1118,P16-1162,0,0.37884,"by the pre-trained sentence-level model7 . This effectively exposes the model to its potential test-time mistakes during the training time, resulting in more robust learned parameters. 5.1 We have three variants of our model, using: (i) only the source memory (S-NMT+src mem), (ii) only the target memory (S-NMT+trg mem), or 5 Training We use a stage-wise method to train the variants of our document context NMT model. Firstly, we pre-train the Memory-toContext/Memory-to-Output models, setting their readings from the source and target memories to 4 We do not split words into subwords using BPE (Sennrich et al., 2016) as that increases sentence lengths resulting in removing long documents due to GPU memory limitations, which would heavily reduce the amount of data that we have. Main Results In our initial experiments, we found SGD to be more effective than Adam/Adagrad; an observation also made by Bahar et al. (2017). 6 For the document NMT model training, we did some preliminary experiments using different learning rates and used the scheme which converged to the best perplexity in the least number of epochs while for sentence-level training we follow Cohn et al. (2016). 7 We report results for two-pass d"
P18-1118,P13-1135,0,0.0176884,"have been computationally expensive and may have resulted in error propagation. We will show in the experiments that our efficient target memory construction is indeed effective. 5 Experiments and Analysis Datasets. We conducted experiments on three language pairs: French-English, German-English and Estonian-English. Table 1 shows the statistics of the datasets used in our experiments. The French-English dataset is based on the TED Talks corpus1 (Cettolo et al., 2012) where each talk is considered a document. The EstonianEnglish data comes from the Europarl v7 corpus2 (Koehn, 2005). Following Smith et al. (2013), we split the speeches based on the SPEAKER tag and treat them as documents. The FrenchEnglish and Estonian-English corpora were randomly split into train/dev/test sets. For GermanEnglish, we use the News Commentary v9 corpus3 for training, news-dev2009 for development, 1 https://wit3.fbk.eu/ http://www.statmt.org/europarl/ 3 http://statmt.org/wmt14/news-commentary-v9-bydocument.tgz 1278 2 # docs # sents doc len src/tgt vocab Fr-En 10/1.2/1.5 123/15/19 123/128/124 25.1/21 Et-En 150/10/18 209/14/25 14/14/14 48.6/24.9 De-En 49/.9/1.1/1.6 191/2/3/3 39/23/27/19 45.1/34.7 Table 1: Training/dev/tes"
P18-1118,D17-1301,0,0.241454,"pendency going farther than a few previous sentences, are neglected in sentencebased translation (Bawden et al., 2017). There are only a handful of attempts to document-wide machine translation in statistical and neural MT camps. Hardmeier and Federico (2010); Gong et al. (2011); Garcia et al. (2014) propose document translation models based on statistical MT but are restrictive in the way they incorporate the document-level information and fail to gain significant improvements. More recently, there have been a few attempts to incorporate source side context into neural MT (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017); however, these works only consider a very local context including a few previous source/target sentences, ignoring the global source and target documental contexts. The latter two report deteriorated performance when using the target-side context. In this paper, we present a document-level machine translation model which combines sentencebased NMT (Bahdanau et al., 2015) with memory networks (Sukhbaatar et al., 2015). We capture the global source and target document context with two memory components, one each for the source and target side, and incorporate it into the"
P18-1118,D15-1166,0,0.218006,"Missing"
P18-1174,D17-1063,0,0.150475,"s rare while unlabelled data is abundant. Active learning (AL) seeks to learn an accurate model with minimum amount of annotation cost. It is inspired by the observation that a model can get better performance if it is allowed to choose the data points on which it is trained. For example, the learner can identify the areas of the space where it does not have enough knowledge, and query those data points which bridge its knowledge gap. Traditionally, AL is performed using engineered heuristics in order to estimate the usefulness of unlabeled data points as queries to an annotator. Recent work (Fang et al., 2017; Bachman et al., 2017; Woodward and Finn, 2017) have focused on learning the AL querying strategy, as engineered heuristics are not flexible to exploit charIn this work, we formulate learning AL strategies as an imitation learning problem. In particular, we consider the popular pool-based AL scenario, where an AL agent is presented with a pool of unlabelled data. Inspired by the Dataset Aggregation (DAGGER) algorithm (Ross et al., 2011), we develop an effective AL policy learning method by designing an efficient and effective algorithmic expert, which provides the AL agent with good decisions"
P18-1174,D08-1112,0,0.935715,"ie sp pt doc. (src/tgt) number avg. len. (tokens) 27k/1k 35/20 24k/2k 140/150 3.6k/4.2k 1.15k/1.35k 3.6k/1.2k 1.15k/1.03k Table 1: The data sets used in sentiment classification (top part) and gender profiling (bottom part). the unlabelled document contains sentiment words or phrases that were returned as rationales for any of the existing labeled documents. For NER, we use the Total Token Entropy (TTE) as the uncertainty sampling method, arg maxx − P|xx |P i=1 yi x, Dlab ) log p(yi |x x, Dlab ) p(yi |x which has been shown to be the best heuristic for this task among 17 different heuristics (Settles and Craven, 2008). • PAL: A reinforcement learning based approach (Fang et al., 2017), which makes use a deep Q-network to make the selection decision for stream-based active learning. 4.1 Text Classification Datasets and Setup. The first task is sentiment classification, in which product reviews express either positive or negative sentiment. The data comes from the Amazon product reviews (McAuley and Yang, 2016); see Table 1 for data statistics. The second task is Authorship Profiling, in which we aim to predict the gender of the text author. The data comes from the gender profiling task in PAN 2017 (Rangel e"
P18-1174,N15-1047,0,0.0639879,"Missing"
P18-1174,W13-3501,0,0.174908,"in Algorithm 1). Related Work Traditional active learning algorithms rely on various heuristics (Settles, 2010), such as uncertainty sampling (Settles and Craven, 2008; Houlsby et al., 2011), query-by-committee (GiladBachrach et al., 2006), and diversity sampling (Brinker, 2003; Joshi et al., 2009; Yang et al., 2015). Apart from these, different heuristics can be combined, thus creating integrated strategy which consider one or more heuristics at the same time. Combined with transfer learning, pre-existing labeled data from related tasks can help improve the performance of an active learner (Xiao and Guo, 2013; Kale and Liu, 2013; Huang and Chen, 2016; Konyushkova et al., 2017). More recently, deep reinforcement learning is used as the framework for learning active learning algorithms, where the active learning cycle is considered as a decision process. (Woodward and Finn, 2017) extended one shot learning to active learning and combined reinforcement learning with a deep recurrent model to make labeling decisions. (Bachman et al., 2017) introduced a policy gradient based method which jointly learns data representation, selection heuristic as well as the model prediction function. (Fang et al., 2017"
P18-2104,I17-1015,0,0.0475706,"Missing"
P18-2104,W17-3204,0,0.0244457,"this issue by extending the recurrent units with multiple blocks along with a trainable routing network. The routing network enables adaptive collaboration by dynamic sharing of blocks conditioned on the task at hand, input, and model state. Empirical evaluation of two low-resource translation tasks, English to Vietnamese and Farsi, show +1 BLEU score improvements compared to strong baselines. 1 Introduction Neural Machine Translation (NMT) has shown remarkable progress in recent years. However, it requires large amounts of bilingual data to learn a translation model with reasonable quality (Koehn and Knowles, 2017). This requirement can be compensated by leveraging curated monolingual linguistic resources in a multi-task learning framework. Essentially, learned knowledge from auxiliary linguistic tasks serves as inductive bias for the translation task to lead to better generalizations. Multi-Task Learning (MTL) is an effective approach for leveraging commonalities of related 2 S EQ 2S EQ MTL Using Recurrent Unit with Adaptive Routed Blocks Our MTL is based on the sequential encoderdecoder architecture with the attention mecha656 Proceedings of the 56th Annual Meeting of the Association for Computational"
P18-2104,P17-1014,0,0.0574551,"Missing"
P18-2104,2006.amta-papers.25,0,0.0215901,"thout any auxiliary task. rates are halved on the decrease in the performance on the dev set of corresponding task. Mini-batch size is set to 32, and dropout rate is 0.5. All models are trained for 50 epochs and the best models are saved based on the perplexity on the dev set of the translation task. For each task, we add special tokens to the beginning of source sequence (similar to (Johnson et al., 2017)) to indicate which task the sequence pair comes from. We used greedy decoding to generate translation. In order to measure translation quality, we use BLEU7 (Papineni et al., 2002) and TER (Snover et al., 2006) scores. • Baseline 2: The MTL architecture proposed in (Niehues and Cho, 2017) which fully shares parameters in components. We have used their best performing architecture with our training schedule. We have extended their work with deep stacked layers for the sake of comparison. • Baseline 3: The MTL architecture proposed in (Zaremoodi and Haffari, 2018) which uses deep stacked layers in the components and shares the parameters of the top two/one stacked layers among encoders/decoders of all tasks6 . 3.4 Results and analysis For the proposed MTL, we use recurrent units with 400 hidden dimens"
P18-2104,2015.iwslt-evaluation.11,0,0.0545596,"amed-Entity Recognition (NER). It is expected that learning to recognize named-entities help the model to learn translation pattern by masking out named-entites. We have used the NER data comes from the CONLL shared task.4 Sentences in this dataset come from a collection of newswire articles from the Reuters Corpus. These sentences are annotated with four types of named entities: persons, locations, organizations and names of miscellaneous entities. Experiments 3.1 • The English-Vietnamese has ∼133K training pairs. It is the preprocessed version of the IWSLT 2015 translation task provided by (Luong and Manning, 2015). It consists of subtitles and their corresponding translations of a collection of public speeches from TED and TEDX talks. The “tst2012” and “tst2013” parts are used as validation and test sets, respectively. We have removed sentence pairs which had more than 300 tokens after applying BPE on either sides. Bilingual Corpora We use two language-pairs, translating from English to Farsi and Vietnamese. We have chosen them to analyze the effect of multi-task learning on languages with different underlying linguistic structures2 . We apply BPE (Sennrich et al., 2016) on the union of source and targ"
P18-2104,tiedemann-2012-parallel,0,0.020875,"yntactic Parsing. By learning the phrase structure of the input sentence, the model would be able to learn better re-ordering. Specially, in the case of language pairs with high level of syntactic divergence (e.g. English-Farsi). We have used Penn Tree Bank parsing data with the standard split for training, development, and test (Marcus et al., 1993). We cast syntactic parsing to a S EQ 2S EQ transduction task by linearizing constituency trees (Vinyals et al., 2015). • The English-Farsi corpus has ∼105K sentence pairs. It is assembled from English-Farsi parallel subtitles from the TED corpus (Tiedemann, 2012), accompanied by all the parallel news text in LDC2016E93 Farsi Representative Language Pack from the Linguistic Data Consortium. The corpus has been normalized using the Hazm toolkit3 . We have removed sentences with more than 80 tokens in either side (before applying BPE). 3k and 4k sentence pairs were held out for the purpose of validation and test. Semantic Parsing. Learning semantic parsing helps the model to abstract away the meaning from the surface in order to convey it in the target translation. For this task, we have used the Abstract Meaning Representation (AMR) corpus Release 2.0 ("
P18-2104,D15-1166,0,0.146277,", learned knowledge from auxiliary linguistic tasks serves as inductive bias for the translation task to lead to better generalizations. Multi-Task Learning (MTL) is an effective approach for leveraging commonalities of related 2 S EQ 2S EQ MTL Using Recurrent Unit with Adaptive Routed Blocks Our MTL is based on the sequential encoderdecoder architecture with the attention mecha656 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 656–661 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics nism (Luong et al., 2015b; Bahdanau et al., 2014). The encoder/decoder consist of recurrent units to read/generate a sentence sequentially. Sharing the parameters of the recurrent units among different tasks is indeed sharing the knowledge for controlling the information flow in the hidden states. Sharing these parameters among all tasks may, however, lead to task interference or inability to leverages commonalities among subsets of tasks. We address this issue by extending the recurrent units with multiple blocks, each of which processing its own information flow through the time. The state of the recurrent unit at"
P18-2104,N18-1123,1,0.753244,"Missing"
P18-2104,J93-2004,0,0.06447,"(Sennrich et al., 2016) on the union of source and target vocabularies for English-Vietnamese, and separate vocabularies for English-Farsi as the alphabets are disjoined (30K BPE operations). Further details about the corpora and their pre-processing is as follows: Syntactic Parsing. By learning the phrase structure of the input sentence, the model would be able to learn better re-ordering. Specially, in the case of language pairs with high level of syntactic divergence (e.g. English-Farsi). We have used Penn Tree Bank parsing data with the standard split for training, development, and test (Marcus et al., 1993). We cast syntactic parsing to a S EQ 2S EQ transduction task by linearizing constituency trees (Vinyals et al., 2015). • The English-Farsi corpus has ∼105K sentence pairs. It is assembled from English-Farsi parallel subtitles from the TED corpus (Tiedemann, 2012), accompanied by all the parallel news text in LDC2016E93 Farsi Representative Language Pack from the Linguistic Data Consortium. The corpus has been normalized using the Hazm toolkit3 . We have removed sentences with more than 80 tokens in either side (before applying BPE). 3k and 4k sentence pairs were held out for the purpose of va"
P18-2104,D16-1160,0,0.0484828,"Missing"
P18-2104,W17-4708,0,0.127861,"convey it in the target translation. For this task, we have used the Abstract Meaning Representation (AMR) corpus Release 2.0 (LDC2017T10)5 . This corpus contains natural language sentences from newswire, weblogs, web discussion forums and broadcast conversations. We cast this task to a S EQ 2S EQ transduction task by linearizing the AMR graphs (Konstas et al., 2017). 2 3 4 English and Vietnamese are SVO, and Farsi is SOV. www.sobhe.ir/hazm 5 658 https://www.clips.uantwerpen.be/conll2003/ner https://catalog.ldc.upenn.edu/LDC2017T10 English → Farsi Method NMT (Luong et al., 2015b) MTL (Full) (Niehues and Cho, 2017) MTL (Partial) (Zaremoodi and Haffari, 2018) Our MTL (Routing) PPL Dev TER BLEU 55.36 87.9 English → Vietnamese Dev Test TER BLEU PPL TER PPL Test TER BLEU PPL 8.57 56.21 88.2 8.35 18.21 64.92 18.39 16.3 61.37 20.18 47.43 85.92 8.97 48.23 87.3 8.73 14.56 61.52 20.55 12.5 57.6 22.6 42.6 80.16 10.58 43.09 81.94 10.54 13.32 59.55 22.2 11.34 55.84 24.65 37.95 76.30 12.06 38.57 78.18 11.95 12.38 58.52 23.06 10.52 54.33 25.65 BLEU Table 1: The performance measures of the baselines vs our MTL architecture on the bilingual datasets. 3.3 0.5 Models and Baselines 0.45 We have implemented the proposed MT"
P18-2104,P02-1040,0,0.100513,"Q model (Luong et al., 2015a) without any auxiliary task. rates are halved on the decrease in the performance on the dev set of corresponding task. Mini-batch size is set to 32, and dropout rate is 0.5. All models are trained for 50 epochs and the best models are saved based on the perplexity on the dev set of the translation task. For each task, we add special tokens to the beginning of source sequence (similar to (Johnson et al., 2017)) to indicate which task the sequence pair comes from. We used greedy decoding to generate translation. In order to measure translation quality, we use BLEU7 (Papineni et al., 2002) and TER (Snover et al., 2006) scores. • Baseline 2: The MTL architecture proposed in (Niehues and Cho, 2017) which fully shares parameters in components. We have used their best performing architecture with our training schedule. We have extended their work with deep stacked layers for the sake of comparison. • Baseline 3: The MTL architecture proposed in (Zaremoodi and Haffari, 2018) which uses deep stacked layers in the components and shares the parameters of the top two/one stacked layers among encoders/decoders of all tasks6 . 3.4 Results and analysis For the proposed MTL, we use recurren"
P18-2104,P16-1162,0,0.0739865,"translation task provided by (Luong and Manning, 2015). It consists of subtitles and their corresponding translations of a collection of public speeches from TED and TEDX talks. The “tst2012” and “tst2013” parts are used as validation and test sets, respectively. We have removed sentence pairs which had more than 300 tokens after applying BPE on either sides. Bilingual Corpora We use two language-pairs, translating from English to Farsi and Vietnamese. We have chosen them to analyze the effect of multi-task learning on languages with different underlying linguistic structures2 . We apply BPE (Sennrich et al., 2016) on the union of source and target vocabularies for English-Vietnamese, and separate vocabularies for English-Farsi as the alphabets are disjoined (30K BPE operations). Further details about the corpora and their pre-processing is as follows: Syntactic Parsing. By learning the phrase structure of the input sentence, the model would be able to learn better re-ordering. Specially, in the case of language pairs with high level of syntactic divergence (e.g. English-Farsi). We have used Penn Tree Bank parsing data with the standard split for training, development, and test (Marcus et al., 1993). We"
P19-1401,W16-2922,0,0.0487528,"Missing"
P19-1401,D17-1063,0,0.184149,"reducing the annotation cost. It is based on the premise that a model can get better performance if it is allowed to prepare its own training data, by choosing the most beneficial data points and querying their annotations from annotators. For example, the learner can identify its knowledge gaps in order to select the most informative query data points. The core AL problem is how to identify the most beneficial query data points. Traditionally, they are identified using various hand crafted heuristics (Settles, 2012). Recent work has investigated learning the AL query strategy from the data (Fang et al., 2017; Bachman et al., 2017; Woodward and Finn, 2017; Contardo et al., 2017; Liu et al., 2018a; Pang et al., 2018), as engineered heuristics are not flexible to exploit characteristics inherent to a given problem. These works are all based on the idea that aims to learn an AL query strategy on a related problem for which enough annotated data exist via AL simulations, and then transfers it to the target AL scenario of interest. The success of this approach, however, highly depends on the relatedness of the source and target AL problems, as the transferred AL strategy is not adapted to the character"
P19-1401,D14-1181,0,0.0023891,"ertainty as the input to the policy network. In the dream phase, the sample pool is constructed randomly as usual. The underlying model mφ is a conditional random field (CRF) treating NER as a sequence labelling task. The prediction is made using the Viterbi algorithm. For the word embeddings, we also use the pretrained multilingual embeddings (Ammar et al., 2016) with 40 dimensions and fix these during policy training. State representation. The input to the policy network is the concatenation of: (i) the representation of the candidate sentence using the sentence convolution network cnnsent (Kim, 2014) (ii) the representation of the labelling marginals using the label-level convolution network cnnlab (Emφ (yy |xx) [yy ]) (Fang et al., 2017) (iii) the bag-of-word representation of sentences in pool of unlabelled data P the sample P pool x0 e (w) where e (w) is emw∈x x 0 ∈Drnd bedding of word w (iv) the representationPof ground-truth labels in the labelled data (xx0 ,yy 0 )∈Dlab cnnlab (yy 0 ) using the empirical distributions (v) the p confidence of the sequential prediction x| |x x) maxy mφ (yy |x (vi) the representation of the entropy sequences for each word label in the sentence using ano"
P19-1401,P18-1174,1,0.121879,"formance if it is allowed to prepare its own training data, by choosing the most beneficial data points and querying their annotations from annotators. For example, the learner can identify its knowledge gaps in order to select the most informative query data points. The core AL problem is how to identify the most beneficial query data points. Traditionally, they are identified using various hand crafted heuristics (Settles, 2012). Recent work has investigated learning the AL query strategy from the data (Fang et al., 2017; Bachman et al., 2017; Woodward and Finn, 2017; Contardo et al., 2017; Liu et al., 2018a; Pang et al., 2018), as engineered heuristics are not flexible to exploit characteristics inherent to a given problem. These works are all based on the idea that aims to learn an AL query strategy on a related problem for which enough annotated data exist via AL simulations, and then transfers it to the target AL scenario of interest. The success of this approach, however, highly depends on the relatedness of the source and target AL problems, as the transferred AL strategy is not adapted to the characteristics of the target AL problem. To address this mismatch challenge, we introduce a new"
P19-1401,K18-1033,1,0.0799074,"formance if it is allowed to prepare its own training data, by choosing the most beneficial data points and querying their annotations from annotators. For example, the learner can identify its knowledge gaps in order to select the most informative query data points. The core AL problem is how to identify the most beneficial query data points. Traditionally, they are identified using various hand crafted heuristics (Settles, 2012). Recent work has investigated learning the AL query strategy from the data (Fang et al., 2017; Bachman et al., 2017; Woodward and Finn, 2017; Contardo et al., 2017; Liu et al., 2018a; Pang et al., 2018), as engineered heuristics are not flexible to exploit characteristics inherent to a given problem. These works are all based on the idea that aims to learn an AL query strategy on a related problem for which enough annotated data exist via AL simulations, and then transfers it to the target AL scenario of interest. The success of this approach, however, highly depends on the relatedness of the source and target AL problems, as the transferred AL strategy is not adapted to the characteristics of the target AL problem. To address this mismatch challenge, we introduce a new"
P19-1401,D08-1112,0,0.0837562,"serve that uncertainty strategy provides a better candidate pool for the AL policy to improve the student learner. Interestingly, random and mixed selection strategy seem to perform better than certainty strategy, especially in the later stages of the AL process where we have a better student learner. This suggests that exploration plays a more important role in strengthening the query policy. 6 Related Works Heuristic-based AL. Traditional active learning algorithms rely on various heuristics (Settles, 2010) to guide the selection of most informative datapoints, such as uncertainty sampling (Settles and Craven, 2008; Houlsby et al., 2011), queryby-committee (Gilad-Bachrach et al., 2006), and diversity sampling (Brinker, 2003; Joshi et al., 2009; Yang et al., 2015). Combined with transfer learning, pre-existing labelled data from related tasks can help improve the performance of an active learner (Xiao and Guo, 2013; Kale and Liu, 2013; Huang and Chen, 2016; Konyushkova 4098 et al., 2017). However, these methods are not flexible to exploit characteristics inherent to a particular problem. Policy-based AL. Recent research has formalized the AL process as a sequential decision process, and applied reinforce"
P19-1401,W13-3501,0,0.0237856,"gests that exploration plays a more important role in strengthening the query policy. 6 Related Works Heuristic-based AL. Traditional active learning algorithms rely on various heuristics (Settles, 2010) to guide the selection of most informative datapoints, such as uncertainty sampling (Settles and Craven, 2008; Houlsby et al., 2011), queryby-committee (Gilad-Bachrach et al., 2006), and diversity sampling (Brinker, 2003; Joshi et al., 2009; Yang et al., 2015). Combined with transfer learning, pre-existing labelled data from related tasks can help improve the performance of an active learner (Xiao and Guo, 2013; Kale and Liu, 2013; Huang and Chen, 2016; Konyushkova 4098 et al., 2017). However, these methods are not flexible to exploit characteristics inherent to a particular problem. Policy-based AL. Recent research has formalized the AL process as a sequential decision process, and applied reinforcement/imitation learning to learn the AL query strategy (Woodward and Finn, 2017; Bachman et al., 2017; Fang et al., 2017; Liu et al., 2018a,b; Contardo et al., 2017). The AL policy learned via simulations on a source task for which enough labeled data exists. It is then transferred to related target task"
Q16-1034,D07-1090,0,0.0624305,"uirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language models (LMs) are fundamental to many NLP tasks, including machine translation and speech recognition. Statistical LMs are probabilistic models that assign a probability to a sequence of words w1N , i"
Q16-1034,buck-etal-2014-n,0,0.0354647,"Missing"
Q16-1034,D07-1021,0,0.0254431,"on time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language models (LMs) are fundamen"
Q16-1034,W09-1505,0,0.0196707,"high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language models (LMs) are fundamental to many NLP tasks, including machine transl"
Q16-1034,D10-1026,0,0.0183067,"te only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. In"
Q16-1034,P13-2121,0,0.0264381,"s much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language models (LMs) are fundamental to many NLP tasks, including machine translation and speech recognition. Statistical LMs are probabilistic models that assign a probability to a seq"
Q16-1034,W11-2123,0,0.443499,"model probabilities on-the-fly. We present several optimisations which improve query runtimes up to 2500×, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang a"
Q16-1034,kennington-etal-2012-suffix,0,0.0228694,"ries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language models (LMs) are fundamental to many NLP tasks, including machine translation and speech recognition. Statistical LMs are probabilistic models that assign a probability to a sequence of words w1N , indicating how likely the sequence is in the language. m-gram LMs are popular, and prove to be accurate when estimated using large corpora. In these LMs, the probabilities of m-grams are often precomputed and stored explicitly. Although widely suc"
Q16-1034,2005.mtsummit-papers.11,0,0.0203928,"ory and time usage, along with the predictive perplexity score of word-level LMs on a number of different corpora varying in size and domain. For all of our word-level LMs, we use m, ¯ m ˆ ≤ 10. We also demonstrate the positive impact of increasing the set limit on m, ¯ m ˆ from 10 to 50 on improving characterlevel LM perplexity. The SDSL library (Gog et al., 2014) is used to implement our data structures. The benchmarking experiments were run on a single core of a Intel Xeon E5-2687 v3 3.10GHz server with 500GiB of RAM. In our word-level experiments, we use the German subset of the Europarl (Koehn, 2005) as a small corpus, which is 382 MiB in size measuring the raw uncompressed text. We also evaluate on much larger corpora, training on 32GiB subsets of the deduplicated English, Spanish, German, and French Common Crawl corpus (Buck et al., 2014). As test sets, we used newstest-2014 for all languages except Spanish, for which we used newstest-2013.11 In our 9 Although the SA can be very large, we need not store it in memory. The DFS traversal in Algorithm 4 (lines 4–16) means that the calls to SA` occur in increasing order of `. Hence, we use on-disk storage for the SA with a small memory mappe"
Q16-1034,D09-1079,0,0.0224297,"y runtimes up to 2500×, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient"
Q16-1034,P11-1027,0,0.0231557,"increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language mod"
Q16-1034,D15-1288,1,0.116456,"ign a probability to a sequence of words w1N , indicating how likely the sequence is in the language. m-gram LMs are popular, and prove to be accurate when estimated using large corpora. In these LMs, the probabilities of m-grams are often precomputed and stored explicitly. Although widely successful, current m-gram LM approaches are impractical for learning high-order LMs on large corpora, due to their poor scaling properties in both training and query phases. Prevailing methods (Heafield, 2011; Stolcke et al., 2011) precompute all m-gram probabilities, and consequently In our previous work (Shareghi et al., 2015), we extended this line of research using a Compressed Suffix Tree (C ST) (Ohlebusch et al., 2010), which provides a considerably more compact searchable means of storing the corpus than an uncompressed suffix array or suffix tree. This approach showed favourable scaling properties with m and had only a modest memory requirement. However, the method only supported Kneser-Ney smoothing, not its modified variant (Chen and Goodman, 1999) which overall performs better and has become the de-facto standard. Additionally, querying was significantly slower than for leading LM toolkits, making the meth"
Q16-1034,P07-1065,0,0.0326882,"sations which improve query runtimes up to 2500×, despite only incurring a modest increase in construction time and memory usage. For large corpora and high Markov orders, our method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text"
Q16-1034,P06-1124,0,0.0649117,"ess, with only a modest increase in construction time and memory usage, yet improving query runtimes up to 2500×. In benchmarking against the state-of-the-art KenLM package on large corpora, our method has superior memory usage and highly competitive runtimes for both querying and training. Our approach allows easy experimentation with high order language models, and our results provide evidence that such high orders are most useful when using large training sets. We posit that further perplexity gains can be realised using richer smoothing techniques, such as a non-parametric Bayesian prior (Teh, 2006; Wood et al., 2011). Our ongoing work will explore this avenue, as well as integrating our language model into the Moses machine translation system, and improving the querying time by caching the lower order probabilities (e.g., m < 4) which we believe can improve query time substantially while maintaining a modest memory footprint. 6 Acknowledgements unit time (s) mem (GiB) m = 5 m = 10 m = 20 m = ∞ word 8164 character 17 935 6.29 18.58 73.45 68.66 3.93 2.69 68.76 2.37 68.80 2.33 Table 4: Perplexity results for the 1 billion word benchmark corpus, showing word based and character based MKN m"
Q16-1034,P09-2086,0,0.0213007,"ur method is highly competitive with the state-of-the-art KenLM package. It imposes much lower memory requirements, often by orders of magnitude, and has runtimes that are either similar (for training) or comparable (for querying). 1 Recent research has attempted to tackle scalability issues through the use of efficient data structures such as tries and hash-tables (Heafield, 2011; Stolcke et al., 2011), lossy compression (Talbot and Osborne, 2007; Levenberg and Osborne, 2009; Guthrie and Hepple, 2010; Pauls and Klein, 2011; Church et al., 2007), compact data structures (Germann et al., 2009; Watanabe et al., 2009; Sorensen and Allauzen, 2011), and distributed computation (Heafield et al., 2013; Brants et al., 2007). Fundamental to all the widely used methods is the precomputation of all probabilities, hence they do not provide an adequate trade-off between space and time for high m, both during training and querying. Exceptions are Kennington et al. (2012) and Zhang and Vogel (2006), who use a suffix-tree or suffix-array over the text for computing the sufficient statistics on-the-fly. Introduction Language models (LMs) are fundamental to many NLP tasks, including machine translation and speech recogn"
U16-1001,P14-1129,0,0.084663,"Missing"
U16-1001,D07-1091,0,0.535235,"). The Trevor Cohn University of Melbourne Melbourne, VIC, Australia t.cohn@unimelb.edu.au use of a RNN provides the ability to memorize longer range dependencies that are impossible with standard n-gram modeling - a core component of the traditional Statistical Machine Translation (SMT) framework (Koehn et al., 2003; Lopez, 2008; Koehn, 2010). Unlike the traditional SMT, NMT offers unique mechanisms to learn translation equivalence without extensive feature engineering efforts. Though promising, NMT still lacks of the ability of modeling deeper semantic and syntactic aspects of the language. Koehn and Hoang (2007) presented a factored translation model to address this issue for the traditional SMT framework (Koehn et al., 2007), where the model incorporates various linguistic annotations for the surface level words. Particularly for low-resource conditions, these extra annotations can lead to better translation of OOVs (or low-count words) and resolve ambiguities, hence increase the generalization capabilities of the model. In machine translation with a low-resource setting, resolving data sparseness and semantic ambiguity problems can help improve its performance. In this paper, we investigate utilizi"
U16-1001,N03-1017,0,0.0679502,"able results and improvements over conventional SMT (Luong et al., 2015). The core idea of NMT is the encoder-decoder framework where an encoder encodes the source sequence into a vector representation, and then a decoder generates the target sequence sequentially via a recurrent neural network (RNN). The Trevor Cohn University of Melbourne Melbourne, VIC, Australia t.cohn@unimelb.edu.au use of a RNN provides the ability to memorize longer range dependencies that are impossible with standard n-gram modeling - a core component of the traditional Statistical Machine Translation (SMT) framework (Koehn et al., 2003; Lopez, 2008; Koehn, 2010). Unlike the traditional SMT, NMT offers unique mechanisms to learn translation equivalence without extensive feature engineering efforts. Though promising, NMT still lacks of the ability of modeling deeper semantic and syntactic aspects of the language. Koehn and Hoang (2007) presented a factored translation model to address this issue for the traditional SMT framework (Koehn et al., 2007), where the model incorporates various linguistic annotations for the surface level words. Particularly for low-resource conditions, these extra annotations can lead to better tran"
U16-1001,2012.eamt-1.60,0,0.0155488,"linguistic layers independently, and compute layer-specific context vectors {c`i }L `=0 and stack them up:  T ci = c0i , . . . , cL i ; c`i = Tx X n=1 i=1 `=0 (n),` where αi is the attention to the layer ` when generating the target word i, and we define (n) (n),` 1 PL ¯ i := L+1 as the average attenα `=0 αi tion across all layers. Essentially, our regularizer penalizes parameters which induce layer-specific attentions deviating from the average attention. ` ` αij hj j=1 α`i = softmax(e`i ) ; 2   e`ij = MLP gi−1 ; h`j 3 Experiments Data. We conducted our experiments on TED Talks datasets (Cettolo et al., 2012) and translate between English (en) ↔ German (de). For training, we used about 200K parallel sentences, and used tst2010 for tuning model parameters (phrasebased SMT) and early stopping (NMT). We evaluated on the official test sets tst2013 and tst2014, where e`ij denotes the alignment score between the annotation at layer ` and the target word. The MLP for each layer has a different parameterization. Global-Local Attention. Finally, we consider a hybrid global-local attention mechanism which 9 dataset train tune-tst2010 test1-tst2013 test2-tst2014 # tokens (K) 4384.68 35.13 22.86 27.40 # types"
U16-1001,P07-2045,0,0.0478143,"ility to memorize longer range dependencies that are impossible with standard n-gram modeling - a core component of the traditional Statistical Machine Translation (SMT) framework (Koehn et al., 2003; Lopez, 2008; Koehn, 2010). Unlike the traditional SMT, NMT offers unique mechanisms to learn translation equivalence without extensive feature engineering efforts. Though promising, NMT still lacks of the ability of modeling deeper semantic and syntactic aspects of the language. Koehn and Hoang (2007) presented a factored translation model to address this issue for the traditional SMT framework (Koehn et al., 2007), where the model incorporates various linguistic annotations for the surface level words. Particularly for low-resource conditions, these extra annotations can lead to better translation of OOVs (or low-count words) and resolve ambiguities, hence increase the generalization capabilities of the model. In machine translation with a low-resource setting, resolving data sparseness and semantic ambiguity problems can help improve its performance. In this paper, we investigate utilizing extra syntactic and semantic linguistic factors in the context of the NMT framework. Linguistic factors can inclu"
U16-1001,2014.iwslt-evaluation.1,0,0.0992157,"64 embedding dimensions for each of lemma, word cluster, Part-of-Speech (POS), and labelled dependency sequences, respectively. For training our neural models, the best perplexity scores on tuning sets were used for early stopping of training, which was usually between 5-8 epochs. For decoding, we used a simple greedy algorithm with length normalization. For evaluation of translations, we applied bootstrapping resampling (Koehn, 2004) to measure the statistical significance (p &lt; 0.05) of BLEU score differences between translation outputs of proposed models compared to the baselines. following Cettolo et al. (2014). We chose a word frequency cut-off of ≥ 5 for limiting the vocabulary when training neural models, resulting in 19K and 26K word types for English and German, respectively. All details of data statistics can be found in Table 1. As linguistic factors, we annotated the source sentences with lemmas,2 word clusters,3 and POS tags. We also annotated with the labelled dependency, i.e. by taking the dependency label between each word and its head (together with its direction, i.e. left or right)4 in the dependency parse tree. Also note that the POS tags and dependency parse trees were extracted fro"
U16-1001,D14-1179,0,0.0503059,"Missing"
U16-1001,W04-3250,0,0.0931051,"ces. For the phrasebased SMT baseline, we used the Moses toolkit (Koehn et al., 2007) with its standard configuration. To encode the linguistic factors, we used 128, 64, 64, 64 embedding dimensions for each of lemma, word cluster, Part-of-Speech (POS), and labelled dependency sequences, respectively. For training our neural models, the best perplexity scores on tuning sets were used for early stopping of training, which was usually between 5-8 epochs. For decoding, we used a simple greedy algorithm with length normalization. For evaluation of translations, we applied bootstrapping resampling (Koehn, 2004) to measure the statistical significance (p &lt; 0.05) of BLEU score differences between translation outputs of proposed models compared to the baselines. following Cettolo et al. (2014). We chose a word frequency cut-off of ≥ 5 for limiting the vocabulary when training neural models, resulting in 19K and 26K word types for English and German, respectively. All details of data statistics can be found in Table 1. As linguistic factors, we annotated the source sentences with lemmas,2 word clusters,3 and POS tags. We also annotated with the labelled dependency, i.e. by taking the dependency label be"
U16-1001,J10-4005,0,0.0186301,"r conventional SMT (Luong et al., 2015). The core idea of NMT is the encoder-decoder framework where an encoder encodes the source sequence into a vector representation, and then a decoder generates the target sequence sequentially via a recurrent neural network (RNN). The Trevor Cohn University of Melbourne Melbourne, VIC, Australia t.cohn@unimelb.edu.au use of a RNN provides the ability to memorize longer range dependencies that are impossible with standard n-gram modeling - a core component of the traditional Statistical Machine Translation (SMT) framework (Koehn et al., 2003; Lopez, 2008; Koehn, 2010). Unlike the traditional SMT, NMT offers unique mechanisms to learn translation equivalence without extensive feature engineering efforts. Though promising, NMT still lacks of the ability of modeling deeper semantic and syntactic aspects of the language. Koehn and Hoang (2007) presented a factored translation model to address this issue for the traditional SMT framework (Koehn et al., 2007), where the model incorporates various linguistic annotations for the surface level words. Particularly for low-resource conditions, these extra annotations can lead to better translation of OOVs (or low-cou"
U16-1001,N16-1102,1,0.834502,"l in resolving ambiguities of source sentences in translation. We formalize sentence complexity by 4 Related Work Recent advances in deep learning research facilitate innovative ideas in machine translation. The attentional encoder-decoder framework pioneered by Bahdanau et al. (2015) is the core, opening a new trend in neural machine translation. Luong et al. (2015) followed the work of (Bahdanau et al., 2015) by experimenting various options on the generation of soft alignments with global and local attention mechanisms. Inspired by remarkable characteristics of state-of-the-art SMT models, Cohn et al. (2016) incorporated structural alignment biases inspired from conventional statistical alignment models (e.g. IBM models 1, 2) to encourage more linguistic structures in the alignment process. Similar in spirit to this, Feng et al. (2016) made use of additional RNN structure for the attention mechanism, hence likely capturing long range dependencies between the attention vectors. Tu et al. (2016) further proposed a socalled coverage vector to trace the attention history for flexibly adjusting future attentions. Though having been developed for almost 2 years, the NMT models are currently competitive"
U16-1001,D15-1166,0,0.276466,"ors into the NMT framework. Evaluating on translating between English and German in two directions with a low resource setting in the domain of TED talks, we obtain promising results in terms of both perplexity reductions and improved BLEU scores over baseline methods. 1 Introduction Neural Machine Translation (NMT) (Devlin et al., 2014; Bahdanau et al., 2015) is a new paradigm in machine translation (MT) powered by recent advances in sequence to sequence learning frameworks (Graves, 2013; Sutskever et al., 2014). NMT has already made remarkable results and improvements over conventional SMT (Luong et al., 2015). The core idea of NMT is the encoder-decoder framework where an encoder encodes the source sequence into a vector representation, and then a decoder generates the target sequence sequentially via a recurrent neural network (RNN). The Trevor Cohn University of Melbourne Melbourne, VIC, Australia t.cohn@unimelb.edu.au use of a RNN provides the ability to memorize longer range dependencies that are impossible with standard n-gram modeling - a core component of the traditional Statistical Machine Translation (SMT) framework (Koehn et al., 2003; Lopez, 2008; Koehn, 2010). Unlike the traditional SM"
U16-1001,P02-1040,0,0.0946994,"Missing"
U16-1001,W11-2155,0,0.0654445,"Missing"
U16-1001,W16-2209,0,0.047294,"Missing"
U16-1001,J82-2005,0,0.770967,"Missing"
U16-1001,D13-1138,0,0.0357817,"Missing"
U16-1001,P07-2046,0,0.0706373,"Missing"
U16-1014,P11-2100,0,0.0341392,"Missing"
U16-1014,D10-1102,0,0.0250829,"ssification has attracted large amounts of attention. Pang(Pang and Lee, 2004) first explored subjectivity extraction methods based on a minimum cut formulation, in which they performed subjectivity detection on individual sentences and implemented document level polarity classification by leveraging those extracted subjective sentences. McDonald(T¨ackstr¨om and McDonald, 2011) proposed a structured model for jointly classifying the sentiment of text at varying levels of granularity, they showed that this task can be reduced to sequential classification with constrained inference. Yessenalina(Yessenalina et al., 2010) described a joint two-level approach for document level sentiment classification that simultaneously extracts useful sentences, and Fang(Fang and Huang, 2012) extended it by incorporating aspect information to the structured model to aspect level sentiment analysis. In this paper, we propose a cascaded latent variable model for biomedical text classification that combines logistic regression and EM, which is trained with a large number of unlabelled but limited amount of labelled biomedical text. ExperMing Liu, Gholamreza Haffari and Wray Buntine. 2016. Learning cascaded latent variable model"
U16-1014,P12-2065,0,0.0172951,"n which they performed subjectivity detection on individual sentences and implemented document level polarity classification by leveraging those extracted subjective sentences. McDonald(T¨ackstr¨om and McDonald, 2011) proposed a structured model for jointly classifying the sentiment of text at varying levels of granularity, they showed that this task can be reduced to sequential classification with constrained inference. Yessenalina(Yessenalina et al., 2010) described a joint two-level approach for document level sentiment classification that simultaneously extracts useful sentences, and Fang(Fang and Huang, 2012) extended it by incorporating aspect information to the structured model to aspect level sentiment analysis. In this paper, we propose a cascaded latent variable model for biomedical text classification that combines logistic regression and EM, which is trained with a large number of unlabelled but limited amount of labelled biomedical text. ExperMing Liu, Gholamreza Haffari and Wray Buntine. 2016. Learning cascaded latent variable models for biomedical text classification. In Proceedings of Australasian Language Technology Association Workshop, pages 128−132. imental results show that the com"
U16-1014,P04-1035,0,0.00630682,"ferent kinds of diseases. In recent years, machine learning methods have been widely used in disease identification from biomedical text(Ehrentraut et al., 2012; Bejan et al., 2012; Martinez et al., 2015; Hassanpour and Langlotz, 2015), which also ask medical experts to do some annotation work for building training data. Unlabelled free biomedical text in hospitals and other clinical organizations is abundant but manual annotation is very expensive. Exploiting fine-grained sentence level properties for coarse-grained document level classification has attracted large amounts of attention. Pang(Pang and Lee, 2004) first explored subjectivity extraction methods based on a minimum cut formulation, in which they performed subjectivity detection on individual sentences and implemented document level polarity classification by leveraging those extracted subjective sentences. McDonald(T¨ackstr¨om and McDonald, 2011) proposed a structured model for jointly classifying the sentiment of text at varying levels of granularity, they showed that this task can be reduced to sequential classification with constrained inference. Yessenalina(Yessenalina et al., 2010) described a joint two-level approach for document le"
U17-1004,D14-1162,0,0.0761409,"ree sources: the word clusters returned by GloVe word embeddings, lexical categories from WordNet, and biomedical concepts from MetaMap. CT reports Additionally, we use 1000 CT scan reports (Martinez et al., 2015) with either positive or negative labels for fungal disease. These reports have technical medical content and highly specialized conventions, which are arguably the most distant genre from the above three datasets. Word clusters from GloVe word embeddings Global K-means clustering algorithm (Likas et al., 2003) is used to create K word clusters from pre-trained GloVe word embeddings (Pennington et al., 2014). The algorithm is conducted in an incremental approach: To create K word clusters, all intermediate problems with 1, 2, ..., K 1 clusters are sequentially solved. The core idea of this method is that an optimal solution for a clustering problem with K clusters can be obtained by using a series of local optimal searches. We tested different K which varies from 50 to 200. 4.2 Preprocessing The same preprocessing steps were used for all the datasets. We lower-cased all the tokens, removed stop words and replaced those low-frequency tokens with a UNK representation. All the numbers were replaced"
U17-1004,P15-1162,0,0.0307386,"Missing"
U17-1004,P13-1045,0,0.0130647,"ng the input is more important than tailoring a network to incorporate word order and syntax. This section describes some related work on deep neural models for text classification and several common knowledge bases. 2.1 Text classification with deep neural models Composition functions play a key role in many deep neural models. Generally, composition functions fall into two categories: unordered and syntactic. Unordered functions regard input text as bags of word embeddings (Iyyer et al., 2015), while syntactic models take word order and sentence structure into account (Mikolov et al., 2010; Socher et al., 2013b). Previously published results have shown that syntactic models have outperformed unordered ones on many tasks. RecNNbased approaches (Socher et al., 2011, 2013a,b) rely on parsing trees to construct the semantic function, in which each leaf node in the tree corresponds to a word. Recursive neural models then compute parent vectors in a bottom up fashion using different types of compositionality functions. While parsing is the first step, RecNNs are restricted to modelling short text like sentences rather than documents. Recurrent neural networks (RNNs) (Mikolov et al., 2010) are another nat"
U17-1004,P14-1062,0,0.0606323,"t due to their capability of processing arbitrary-length sequences. Unfortunately, a problem with RNNs is that the transition function inside can cause the gradient vector to grow or decay exponentially over long sequences. The LSTM architecture (Hochreiter and Schmidhuber, 1997) addresses this problem by introducing a memory cell that is able to preserve state over a long period of time. Tree-LSTM (Tai et al., 2015) is an extension of standard LSTM in that Tree-LSTM computes its hidden state from the current input and the hidden states of arbitrarily many child units. Convolutional networks (Kalchbrenner et al., 2014) also model word order in local windows and have achieved performance comparable or better than that of RecNNs or RNNs on many tasks. While models that use syntactic functions need large training time and data, unordered functions allow a tradeoff between training time and model complexity. Unlike some of the previous syntactic approaches, paragraph vector (Le and Mikolov, 2014) is capable of constructing representations of input sequences of variable length. It does not re2.2 Exploiting linguistic resources Besides distributed word representation, there exist many large-scale knowledge bases"
U17-1004,D11-1014,0,0.0439844,"for text classification and several common knowledge bases. 2.1 Text classification with deep neural models Composition functions play a key role in many deep neural models. Generally, composition functions fall into two categories: unordered and syntactic. Unordered functions regard input text as bags of word embeddings (Iyyer et al., 2015), while syntactic models take word order and sentence structure into account (Mikolov et al., 2010; Socher et al., 2013b). Previously published results have shown that syntactic models have outperformed unordered ones on many tasks. RecNNbased approaches (Socher et al., 2011, 2013a,b) rely on parsing trees to construct the semantic function, in which each leaf node in the tree corresponds to a word. Recursive neural models then compute parent vectors in a bottom up fashion using different types of compositionality functions. While parsing is the first step, RecNNs are restricted to modelling short text like sentences rather than documents. Recurrent neural networks (RNNs) (Mikolov et al., 2010) are another natural choice to model text due to their capability of processing arbitrary-length sequences. Unfortunately, a problem with RNNs is that the transition functi"
U17-1004,D13-1170,0,0.00402323,"ng the input is more important than tailoring a network to incorporate word order and syntax. This section describes some related work on deep neural models for text classification and several common knowledge bases. 2.1 Text classification with deep neural models Composition functions play a key role in many deep neural models. Generally, composition functions fall into two categories: unordered and syntactic. Unordered functions regard input text as bags of word embeddings (Iyyer et al., 2015), while syntactic models take word order and sentence structure into account (Mikolov et al., 2010; Socher et al., 2013b). Previously published results have shown that syntactic models have outperformed unordered ones on many tasks. RecNNbased approaches (Socher et al., 2011, 2013a,b) rely on parsing trees to construct the semantic function, in which each leaf node in the tree corresponds to a word. Recursive neural models then compute parent vectors in a bottom up fashion using different types of compositionality functions. While parsing is the first step, RecNNs are restricted to modelling short text like sentences rather than documents. Recurrent neural networks (RNNs) (Mikolov et al., 2010) are another nat"
U17-1004,P11-1015,0,0.0131144,"ding the newly concatenated word-concept vector hi into the following layers is the same. But not all words contribute equally to the representation of the document meaning, we further introduce an attention mechanism to extract such words that are important to the meaning of the document and aggrez= P|X| i=1 ↵i hi . With z, the final prediction is made with a softmax layer: yˆ = softmax(Ws · z + b). Figure 2 gives the framework of our model. The two variants of the model are neural bag of words with either direct or gated mapping. 37 3.3 Sources of concept information IMDB This core dataset (Maas et al., 2011) contains 50,000 reviews which are divided evenly into 25k train and 25k test sets. The overall distribution of labels is balanced (25k positive and 25k negative). We collect concept annotation from three sources: the word clusters returned by GloVe word embeddings, lexical categories from WordNet, and biomedical concepts from MetaMap. CT reports Additionally, we use 1000 CT scan reports (Martinez et al., 2015) with either positive or negative labels for fungal disease. These reports have technical medical content and highly specialized conventions, which are arguably the most distant genre fr"
U17-1004,P15-1150,0,0.0143268,"s the first step, RecNNs are restricted to modelling short text like sentences rather than documents. Recurrent neural networks (RNNs) (Mikolov et al., 2010) are another natural choice to model text due to their capability of processing arbitrary-length sequences. Unfortunately, a problem with RNNs is that the transition function inside can cause the gradient vector to grow or decay exponentially over long sequences. The LSTM architecture (Hochreiter and Schmidhuber, 1997) addresses this problem by introducing a memory cell that is able to preserve state over a long period of time. Tree-LSTM (Tai et al., 2015) is an extension of standard LSTM in that Tree-LSTM computes its hidden state from the current input and the hidden states of arbitrarily many child units. Convolutional networks (Kalchbrenner et al., 2014) also model word order in local windows and have achieved performance comparable or better than that of RecNNs or RNNs on many tasks. While models that use syntactic functions need large training time and data, unordered functions allow a tradeoff between training time and model complexity. Unlike some of the previous syntactic approaches, paragraph vector (Le and Mikolov, 2014) is capable o"
U18-1001,P17-1175,0,0.0499344,"Missing"
U18-1001,2012.eamt-1.60,0,0.0122068,"classification. Then we study different methods with minimal efforts for incorporating such side information into existing NMT models. 2 Machine Translation Data with Side Information First, let’s explore some realistic scenarios in which the side information is potentially useful for NMT. TED Talks The TED Talks website2 hosts technical videos from influential speakers around the world on various topics or domains, such as: education, business, science, technology, creativity, etc. Thanks to users’ contributions, most of such videos are subtitled in multiple languages. Based on this website, Cettolo et al. (2012) created a parallel corpus for the MT research community. Inspired by this, Chen et al. (2016) further customised this dataset and included an additional sentence-level topic information.3 We consider such topic information as side information. Fig2 https://www.ted.com/talks https://github.com/wenhuchen/ iwslt-2015-de-en-topics 3 Cong Duy Vu Hoang, Gholamreza Haffari and Trevor Cohn. 2018. Improved Neural Machine Translation using Side Information. In Proceedings of Australasian Language Technology Association Workshop, pages 6−16. ure 1 illustrates some examples of this dataset. As can be see"
U18-1001,2016.amta-researchers.10,0,0.0200853,"information into existing NMT models. 2 Machine Translation Data with Side Information First, let’s explore some realistic scenarios in which the side information is potentially useful for NMT. TED Talks The TED Talks website2 hosts technical videos from influential speakers around the world on various topics or domains, such as: education, business, science, technology, creativity, etc. Thanks to users’ contributions, most of such videos are subtitled in multiple languages. Based on this website, Cettolo et al. (2012) created a parallel corpus for the MT research community. Inspired by this, Chen et al. (2016) further customised this dataset and included an additional sentence-level topic information.3 We consider such topic information as side information. Fig2 https://www.ted.com/talks https://github.com/wenhuchen/ iwslt-2015-de-en-topics 3 Cong Duy Vu Hoang, Gholamreza Haffari and Trevor Cohn. 2018. Improved Neural Machine Translation using Side Information. In Proceedings of Australasian Language Technology Association Workshop, pages 6−16. ure 1 illustrates some examples of this dataset. As can be seen, the keywords (second column, treated as side information) contain additional contextual inf"
U18-1001,N16-1102,1,0.66264,"eural machine translation (NMT). We study various kinds of side information, including topical information and personal traits, and then propose different ways of incorporating these information sources into existing NMT models. Our experimental results show the benefits of side information in improving the NMT models. 1 Introduction Neural machine translation is the task of generating a target language sequence given a source language sequence, framed as a neural network (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia). Most research efforts focus on inducing more prior knowledge (Cohn et al., 2016; Zhang et al., 2017; Mi et al., 2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et a"
U18-1001,P15-2139,1,0.840333,"ared information in the encoder states. Our formulation in Equation 5 gives rise to multi-task learning (MTL). Here, we propose the joint learning of two different but related tasks: NMT and multi-label classification (MLC). Here, the MLC task refers to predicting the labels that possibly represent words of the given side information. This is interesting in the sense that the model is capable of not only generating the translated outputs, but also explicitly predicting what the side information is. Here, we adopt a simple instance of MTL for our case, called soft parameter sharing similar to (Duong et al., 2015; Yang and Hospedales, 2016). In our MTL version, the NMT and MLC tasks share the parameters of the encoders. The difference between the two is at the decoder part. In the NMT task, the decoder is kept unchanged. For the MLC task, we define its objective function (or loss), formulated as: where: λ is the coefficient balancing the two task objectives, whose value is fine-tuned based on the development data to optimise for NMT accuracy measured using BLEU (Papineni et al., 2002). The idea of MTL applied for NLP was firstly explored by (Collobert and Weston, 2008), later attracts increasing atten"
U18-1001,K18-1010,0,0.150475,"ts of side information in improving the NMT models. 1 Introduction Neural machine translation is the task of generating a target language sequence given a source language sequence, framed as a neural network (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia). Most research efforts focus on inducing more prior knowledge (Cohn et al., 2016; Zhang et al., 2017; Mi et al., 2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et al., 2017; Matusov et al., 2017; Calixto and Liu, 2017). This task, also known as multi-modal translation, seeks to leverage images which can contain cues representing the perception of the image in source text, and potentially can contribute to resolve ambiguity (e.g., lex"
U18-1001,D17-1105,0,0.0227807,"2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et al., 2017; Matusov et al., 2017; Calixto and Liu, 2017). This task, also known as multi-modal translation, seeks to leverage images which can contain cues representing the perception of the image in source text, and potentially can contribute to resolve ambiguity (e.g., lexical, gender), 1 http://www.statmt.org/wmt17/ multimodal-task.html vagueness, out-of-vocabulary terms, and topic relevancy. Inspired from the idea of multi-modal translation, in our work, we propose the use of another modality, namely metadata or side information. Previously, Hoang et al. (2016a) have shown the usefulness of side information for neural language models. This work"
U18-1001,E17-2101,0,0.101522,"IPC labels. The full meaning of all IPC labels can be found on the official IPC website,6 however we provide in Figure 3 the glossess for each referenced label. Note that those IPC labels form a WordNet style hierarchy (Fellbaum, 1998), and accordingly may be useful in many other deep models of NLP. Personalised Europarl For the second dataset, we evaluate our proposed idea in the context of personality-aware MT. Mirkin et al. (2015) explored whether translation preserves personality information (e.g., demographic and psychometric traits) in statistical MT (SMT); and further Rabinovich et al. (2017) found that personality information like author’s gender is an obvious signal in source text, but it is less clear in human and machine translated texts. As a result, they created a new dataset for personalised MT4 partially based on the original Europarl. The personality such as author’s gender will be regarded as side information in our setup. An excerpt of this dataset is shown in Figure 2. As can be seen from the figure, there exist many kinds of side information pertaining to authors’ traits, including identification (ID, name), native language, gender, date of birth/age, and plenary sess"
U18-1001,P17-1012,0,0.0613326,"Missing"
U18-1001,D16-1096,0,0.0228671,"dy various kinds of side information, including topical information and personal traits, and then propose different ways of incorporating these information sources into existing NMT models. Our experimental results show the benefits of side information in improving the NMT models. 1 Introduction Neural machine translation is the task of generating a target language sequence given a source language sequence, framed as a neural network (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia). Most research efforts focus on inducing more prior knowledge (Cohn et al., 2016; Zhang et al., 2017; Mi et al., 2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et al., 2017; Matusov et al., 2017; Calix"
U18-1001,P18-2050,0,0.0366293,"Missing"
U18-1001,N16-1149,1,0.601696,"on and personal traits, and then propose different ways of incorporating these information sources into existing NMT models. Our experimental results show the benefits of side information in improving the NMT models. 1 Introduction Neural machine translation is the task of generating a target language sequence given a source language sequence, framed as a neural network (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia). Most research efforts focus on inducing more prior knowledge (Cohn et al., 2016; Zhang et al., 2017; Mi et al., 2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et al., 2017; Matusov et al., 2017; Calixto and Liu, 2017). This task, also known as multi-modal translation,"
U18-1001,D15-1130,0,0.0221855,"e pair in this corpus is associated with any number of IPC label(s) as well as other metadata, e.g., patent ID, patent family ID, publication date. In this work, we consider only the IPC labels. The full meaning of all IPC labels can be found on the official IPC website,6 however we provide in Figure 3 the glossess for each referenced label. Note that those IPC labels form a WordNet style hierarchy (Fellbaum, 1998), and accordingly may be useful in many other deep models of NLP. Personalised Europarl For the second dataset, we evaluate our proposed idea in the context of personality-aware MT. Mirkin et al. (2015) explored whether translation preserves personality information (e.g., demographic and psychometric traits) in statistical MT (SMT); and further Rabinovich et al. (2017) found that personality information like author’s gender is an obvious signal in source text, but it is less clear in human and machine translated texts. As a result, they created a new dataset for personalised MT4 partially based on the original Europarl. The personality such as author’s gender will be regarded as side information in our setup. An excerpt of this dataset is shown in Figure 2. As can be seen from the figure, th"
U18-1001,U16-1001,1,0.650663,"on and personal traits, and then propose different ways of incorporating these information sources into existing NMT models. Our experimental results show the benefits of side information in improving the NMT models. 1 Introduction Neural machine translation is the task of generating a target language sequence given a source language sequence, framed as a neural network (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia). Most research efforts focus on inducing more prior knowledge (Cohn et al., 2016; Zhang et al., 2017; Mi et al., 2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et al., 2017; Matusov et al., 2017; Calixto and Liu, 2017). This task, also known as multi-modal translation,"
U18-1001,P02-1040,0,0.101887,"t the side information is. Here, we adopt a simple instance of MTL for our case, called soft parameter sharing similar to (Duong et al., 2015; Yang and Hospedales, 2016). In our MTL version, the NMT and MLC tasks share the parameters of the encoders. The difference between the two is at the decoder part. In the NMT task, the decoder is kept unchanged. For the MLC task, we define its objective function (or loss), formulated as: where: λ is the coefficient balancing the two task objectives, whose value is fine-tuned based on the development data to optimise for NMT accuracy measured using BLEU (Papineni et al., 2002). The idea of MTL applied for NLP was firstly explored by (Collobert and Weston, 2008), later attracts increasing attentions from the NLP community (Ruder, 2017). Specifically, the idea behind MTL is to leverage related tasks which can be learned jointly — potentially introducing an inductive bias (Feinman and Lake, 2018). An alternative explanation of the benefits of MTL is that joint training with multiple tasks acts as an additional regulariser to the model, reducing the risk of overfitting (Collobert and Weston, 2008; Ruder, 2017, inter alia). 4 4.1 LM LC := − M X 1Twm s log ps ; (8) Exper"
U18-1001,W18-1802,0,0.0860953,"nt set, using the value range of {64, 128, 256, 512}. Similarly, the balancing weight in the mtl method is fine-tuned using the value range of {0.001, 0.01, 0.1, 1.0}. For evaluation, we measured the end translation quality with case-sensitive BLEU (Papineni et al., 2002). We averaged 2 runs for each of the method variants. denote the system variants as follows: base refers to the baseline NMT system using the transformer without using any side information. si-src-prefix and si-src-suffix refer to the NMT system using the side information as respective prefix or suffix of the source sequence (Jehl and Riezler, 2018), applied to both training and decoder/inference. 4.3 Results and Analysis The experimental results can be seen in Table 3. Overall, we obtained limited success for the method of adding side information as prefix or suffix for TED Talks and Personalised Europarl datasets. On the PatTR dataset, small improvements (0.1-0.2 BLEU) are observed. We experimented two sets of side information in the PatTR dataset, including PatTR-1 (651 deep labels) and PatTR (8 shallow labels).13 The possible reason for this phenomenon is that the multi-head attention mechanism in the transformer may have some confus"
U18-1001,E17-1101,0,0.241385,"consider only the IPC labels. The full meaning of all IPC labels can be found on the official IPC website,6 however we provide in Figure 3 the glossess for each referenced label. Note that those IPC labels form a WordNet style hierarchy (Fellbaum, 1998), and accordingly may be useful in many other deep models of NLP. Personalised Europarl For the second dataset, we evaluate our proposed idea in the context of personality-aware MT. Mirkin et al. (2015) explored whether translation preserves personality information (e.g., demographic and psychometric traits) in statistical MT (SMT); and further Rabinovich et al. (2017) found that personality information like author’s gender is an obvious signal in source text, but it is less clear in human and machine translated texts. As a result, they created a new dataset for personalised MT4 partially based on the original Europarl. The personality such as author’s gender will be regarded as side information in our setup. An excerpt of this dataset is shown in Figure 2. As can be seen from the figure, there exist many kinds of side information pertaining to authors’ traits, including identification (ID, name), native language, gender, date of birth/age, and plenary sess"
U18-1001,P17-1139,0,0.0188248,"lation (NMT). We study various kinds of side information, including topical information and personal traits, and then propose different ways of incorporating these information sources into existing NMT models. Our experimental results show the benefits of side information in improving the NMT models. 1 Introduction Neural machine translation is the task of generating a target language sequence given a source language sequence, framed as a neural network (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia). Most research efforts focus on inducing more prior knowledge (Cohn et al., 2016; Zhang et al., 2017; Mi et al., 2016, inter alia), incorporating linguistics factors (Hoang et al., 2016b; Sennrich and Haddow, 2016; Garc´ıa-Mart´ınez et al., 2017) or changing the network architecture (Gehring et al., 2017b,a; Vaswani et al., 2017; Elbayad et al., 2018) in order to better exploit the source representation. Consider a different direction, situations in which there exists other modality other than the text of the source sentence. For instance, the WMT 2017 campaign1 proposed to use additional information obtained from images to enrich the neural MT models, as in (Calixto et al., 2017; Matusov et"
U18-1001,N16-1005,0,0.133644,"itional side information we would like to incorporate into NMT model. 3.3 Conditioning on Side Information Keeping in mind that we would like a generic incorporation method so that only minimal modification of NMT model is required, we propose and evaluate different approaches. Side Information as Source Prefix/Suffix The most simple way to include side information is to add the side information as a string prefix or suffix to the source sequence, and letting the NMT model learn from this modified data. This method requires no modification of the NMT model. This method was firstly proposed by Sennrich et al. (2016a) who added the side constraints (e.g., honMulti-task Learning Consider the case where we would like to use existing side information to 9 mation, formulated as: &quot; improve the main NMT task. We can define a generative model p (y, e|x), formulated as: p (y, e|x) := p (y|x, e) · |{z } p (e|x) |{z } ! # 1 X 0 ps = sigmoid W s g (xi ) + bs ; |x| i (7) where x is the source sequence, comprising of x1 , . . . , xi , . . . , x|x |words. Here, we denote a generic function term g0 (.) which refers to a vectorised representation of a specific word depending on designing the network architecture, e.g.,"
U18-1001,N16-1004,0,0.0333152,"Missing"
U18-1001,P16-1162,0,0.57918,"itional side information we would like to incorporate into NMT model. 3.3 Conditioning on Side Information Keeping in mind that we would like a generic incorporation method so that only minimal modification of NMT model is required, we propose and evaluate different approaches. Side Information as Source Prefix/Suffix The most simple way to include side information is to add the side information as a string prefix or suffix to the source sequence, and letting the NMT model learn from this modified data. This method requires no modification of the NMT model. This method was firstly proposed by Sennrich et al. (2016a) who added the side constraints (e.g., honMulti-task Learning Consider the case where we would like to use existing side information to 9 mation, formulated as: &quot; improve the main NMT task. We can define a generative model p (y, e|x), formulated as: p (y, e|x) := p (y|x, e) · |{z } p (e|x) |{z } ! # 1 X 0 ps = sigmoid W s g (xi ) + bs ; |x| i (7) where x is the source sequence, comprising of x1 , . . . , xi , . . . , x|x |words. Here, we denote a generic function term g0 (.) which refers to a vectorised representation of a specific word depending on designing the network architecture, e.g.,"
U18-1001,W13-2236,0,0.114478,"and Hs the dimensionality of the hidden space. These embedding vectors are used for the input to several different neural architectures, which we now outline. Patent MT Collection Another interesting data is patent translation which includes rich side information. PatTR5 is a sentence-parallel corpus which is a subset of the MAREC Patent Corpus (W¨aschle and Riezler, 2012a). In general, PatTR contains millions of parallel sentences collected from all patent text sections (e.g., title, abstract, claims, description) in multiple languages (English, French, German) (W¨aschle and Riezler, 2012b; Simianer and Riezler, 2013). An appealing feature of this corpus is that it provides a labelling at a sentence level, in the form of IPC (International Patent Classification) codes. The IPC 3.2 NMT Model Formulation Recall the general formulation of NMT (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia) as a conditional language model in which the generation of target sequence is conditioned on the source sequence (Sutskever et al., 2014; Bahdanau et al., 2015, inter alia), formulated as: y t+1 ∼ pΘ (yt+1 |y <t , x) = softmax (f Θ (y <t , x)) ; 4 http://cl.haifa.ac.il/projects/pmt/ index.shtml 5 http://www.cl.u"
U18-1001,E12-1083,0,0.061259,"Missing"
U18-1007,J00-3003,0,0.601454,"Missing"
U18-1007,E17-1041,1,0.918862,"echniques have been proposed to capture the semantic correlation between utterances and DAs. Earlier on, statistical techniques such as Hidden Markov Models (HMMs) were widely used to recognise DAs (Stolcke et al., 2000; Julia et al., 2010). Recently, due to the enormous success of neural networks in sequence labeling/transduction tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Popov, 2016), several recurrent neural network (RNN) based architectures have been proposed to conduct DA classification, resulting in ∗ Equal contribution promising outcomes (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Despite the success of previous work in DA classification, there are still several fundamental issues. Firstly, most of the previous works rely on transcriptions (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Fewer of these focus on combining speech and textual signals (Julia et al., 2010), and even then, the textual signals in these works utilise the oracle transcriptions. We argue that in the context of a spoken dialog system, oracle transcriptions of utterances are usually not available, i.e. the agent does not have access to the human transcriptions. Speech and textual data"
U18-1007,D17-1229,1,0.625302,"echniques have been proposed to capture the semantic correlation between utterances and DAs. Earlier on, statistical techniques such as Hidden Markov Models (HMMs) were widely used to recognise DAs (Stolcke et al., 2000; Julia et al., 2010). Recently, due to the enormous success of neural networks in sequence labeling/transduction tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Popov, 2016), several recurrent neural network (RNN) based architectures have been proposed to conduct DA classification, resulting in ∗ Equal contribution promising outcomes (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Despite the success of previous work in DA classification, there are still several fundamental issues. Firstly, most of the previous works rely on transcriptions (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Fewer of these focus on combining speech and textual signals (Julia et al., 2010), and even then, the textual signals in these works utilise the oracle transcriptions. We argue that in the context of a spoken dialog system, oracle transcriptions of utterances are usually not available, i.e. the agent does not have access to the human transcriptions. Speech and textual data"
U18-1007,N16-1037,1,0.904418,"ue (Stolcke et al., 2000), numerous techniques have been proposed to capture the semantic correlation between utterances and DAs. Earlier on, statistical techniques such as Hidden Markov Models (HMMs) were widely used to recognise DAs (Stolcke et al., 2000; Julia et al., 2010). Recently, due to the enormous success of neural networks in sequence labeling/transduction tasks (Sutskever et al., 2014; Bahdanau et al., 2014; Popov, 2016), several recurrent neural network (RNN) based architectures have been proposed to conduct DA classification, resulting in ∗ Equal contribution promising outcomes (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Despite the success of previous work in DA classification, there are still several fundamental issues. Firstly, most of the previous works rely on transcriptions (Ji et al., 2016; Shen and Lee, 2016; Tran et al., 2017a). Fewer of these focus on combining speech and textual signals (Julia et al., 2010), and even then, the textual signals in these works utilise the oracle transcriptions. We argue that in the context of a spoken dialog system, oracle transcriptions of utterances are usually not available, i.e. the agent does not have access to the human"
U18-1007,K16-1028,0,0.0661187,"Missing"
U19-1013,D18-1045,0,0.0281888,"d content words. Hence, an intelligent parser should be able to switch between generation mode and copy mode. In order to address the aforementioned issues, we equip the vanilla seq2seq model with a pointer network and a context-dependent architecture to generate more accurate logical forms. Our model achieves state-of-the-art performance on the email assistant task. 1 Introduction Recently, due to the breakthrough of the deep learning, numerous and various tasks within the filed of natural language processing (NLP) have made impressive achievements (Vaswani et al., 2017; Devlin et al., 2018; Edunov et al., 2018). However, most these achievements are assessed by automatic metrics, which are relatively superficial and brittle, and can be easily tricked (Paulus et al., 2017; Jia and Liang, 2017; L¨aubli et al., 2018). Hence, understanding the underlying meaning of natural language sentences is crucial to NLP tasks. As an appealing direction in natural language understanding, semantic parsing has been widely studied in the NLP community (Ling et al., 2016; Dong and Lapata, 2016; Jia and Liang, 2017). Semantic parsing aims at converting human utterances to machine executable representations. Most existing"
U19-1013,D17-1215,0,0.0186016,"q model with a pointer network and a context-dependent architecture to generate more accurate logical forms. Our model achieves state-of-the-art performance on the email assistant task. 1 Introduction Recently, due to the breakthrough of the deep learning, numerous and various tasks within the filed of natural language processing (NLP) have made impressive achievements (Vaswani et al., 2017; Devlin et al., 2018; Edunov et al., 2018). However, most these achievements are assessed by automatic metrics, which are relatively superficial and brittle, and can be easily tricked (Paulus et al., 2017; Jia and Liang, 2017; L¨aubli et al., 2018). Hence, understanding the underlying meaning of natural language sentences is crucial to NLP tasks. As an appealing direction in natural language understanding, semantic parsing has been widely studied in the NLP community (Ling et al., 2016; Dong and Lapata, 2016; Jia and Liang, 2017). Semantic parsing aims at converting human utterances to machine executable representations. Most existing work focuses on parsing individual utterances independently, even they have an access to the contextual information. In spite of several pioneering efforts (Zettlemoyer and Collins,"
U19-1013,D18-1512,0,0.0361152,"Missing"
U19-1013,P16-1057,0,0.0461794,"Missing"
U19-1013,P17-1105,0,0.0176205,"tanding, semantic parsing has been widely studied in the NLP community (Ling et al., 2016; Dong and Lapata, 2016; Jia and Liang, 2017). Semantic parsing aims at converting human utterances to machine executable representations. Most existing work focuses on parsing individual utterances independently, even they have an access to the contextual information. In spite of several pioneering efforts (Zettlemoyer and Collins, 2009; Srivastava et al., 2017), these pre-neural models suffer from complicated hand-crafted feature engineering, compared to their neural counterparts (Dong and Lapata, 2018; Rabinovich et al., 2017). One notable exception is the work of Suhr et al. (2018), who incorporate context into ATIS data with a neural approach. In this work, we propose a neural semantic parser for email assistant task which incorporates the conversation context as well as a copy mechanism to fill-in the arguments of the logical forms from the input sentence. Our model achieves stateof-the-art (SOTA) performance. We further provide details analysis about where these improvements come from. 2 Models To build our models, we follow a process of errordriven design. We first start with a simple seq2seq model, then we cl"
U19-1013,P17-1099,0,0.0823197,"d be crucial for the model to learn when to copy from the source sentence, and when to generate a new token. Thus, we incorporate the pointer mechanism into our base seq2seq approach. As shown in Figure 1, for an email assistant system, users inputs are usually comprised of a functional part and a content part. A semantic parser should be able to distinguish and handle them in a different way. Specifically, the parser must generate a series of lambda-like functions for the functional part, while the content part should be copied to the argument slot. Our pointer network is inspired by that of See et al. (2017) designed for the summarisation task. Given an utterance x and a logical form y, at each time step t, we have a soft switch which determines the contributions of the token generator and the copier which uses a pointer over the words of the input utterance: X P (yt ) = pgen Pvocab (yt ) + (1 − pgen ) αit i:xi =yt where αit is the attention score over the position i in the t-th generation step, and Pvocab is a probability distribution over the vocabulary. pgen ∈ [0, 1] is the generation probability, modelled as: pgen = σ(wTc ct + wTs st + wTx xt + b) Conditioning on Conversation Context Understa"
U19-1013,D07-1071,0,0.0757121,"es when we compute attention scores. Depending on their distances from the current utterance, we append Epos [0], .., Epos [k] to the previous utterances respectively. 3 Experiments Dataset Semantic paring is crucial to dialogue systems, especially for multi-turn conversations. Additionally, understanding users’ intentions and extracting salient requirements play an important role in the dialogue-related semantic parsing. We use a dataset created by Srivastava et al. (2017) Main Results Prior to this work, Srivastava et al. (2017) also incorporate the conversational context into a CCG parser (Zettlemoyer and Collins, 2007). CCG requires extensive hand-feature engineering to construct text-based features. However, neural semantic parsers have been demonstrating impressive improvement over various and numerous dataset (Suhr et al., 2018; Dong and Lapata, 2018). Hence, we explore both RNN-based (Bahdanau et al., 2014) and transformer-based (Vaswani et al., 2017) architectures for our attentional seq2seq model, denoted as RNNS2S and Transformer respectively. Hyperparameters, architecture details, and other experimental choices are detailed in the supplementary material. Unless otherwise mentioned, we use 3 previous"
U19-1013,P09-1110,0,0.169367,"2017; Jia and Liang, 2017; L¨aubli et al., 2018). Hence, understanding the underlying meaning of natural language sentences is crucial to NLP tasks. As an appealing direction in natural language understanding, semantic parsing has been widely studied in the NLP community (Ling et al., 2016; Dong and Lapata, 2016; Jia and Liang, 2017). Semantic parsing aims at converting human utterances to machine executable representations. Most existing work focuses on parsing individual utterances independently, even they have an access to the contextual information. In spite of several pioneering efforts (Zettlemoyer and Collins, 2009; Srivastava et al., 2017), these pre-neural models suffer from complicated hand-crafted feature engineering, compared to their neural counterparts (Dong and Lapata, 2018; Rabinovich et al., 2017). One notable exception is the work of Suhr et al. (2018), who incorporate context into ATIS data with a neural approach. In this work, we propose a neural semantic parser for email assistant task which incorporates the conversation context as well as a copy mechanism to fill-in the arguments of the logical forms from the input sentence. Our model achieves stateof-the-art (SOTA) performance. We furthe"
U19-1023,I17-3007,0,0.0263041,"Missing"
U19-1023,J92-4003,0,0.353475,"s the output of the lexical simplification module. The choice of the language model is extremely important to ensure that the sentence with the highest score is indeed the simplest. To choose a suitable language model, we ran experiments using a validation set of 2000 sentences from WikiLarge corpus. We found that the best performance with respect to simplicity metrics, was obtained using language models which had been trained on a simple English corpus. This tends to encourage output sentences which are simpler and more common. The best performance was achieved using a 5-gram language model (Brown et al. 1992) trained on the Simple Wikipedia corpus5. Lexical Simplification The lexical module operates in two phases: simpler synonyms extraction, and lexical substitution. In the first phase, the system builds a synonyms dictionary for all the words appearing in the input text and apply a simplicity criterion to only keep synonyms which are simpler than the original words. In the second phase, the system decides which words should be substituted with which synonyms based on the context of the original words in their sentences. Simpler Synonyms Extraction Given an input sentence, this phase starts with"
U19-1023,D14-1082,0,0.0253631,"ement four operations that are arguably the most useful simplification operations. • Splitting conjoint clauses • Splitting relative clauses • Splitting appositive phrases • Changing passive-voice to active-voice To apply the above rules, the sentence is first parsed using the Stanford dependency parser 1 3 We use Stanford tokenizer and PoS-tagger in NLTK Python library 2 An example of an automatically extracted medical dictionary is presented in section 3 of this paper We use Pattern Python package for morphology changes 4 We use News Crawl 2013 corpus in WMT16 Task 5 simple.wikipedia.org 2 (Chen & Manning, 2014) and then three main operations are performed to achieve the final output: Analysis: where the sentence is analyzed in search for simplification clues such as discourse markers for conjoint clauses (ex: “and” or “when”), or relative pronouns for relative clauses (ex: “who”, “which”). Transformation: where the core operations are applied to transform the sentence into a simplified form. It is applied in a recursive manner until the sentence has no more simplification clues. Generation: where the simplified sentences are reconstructed ensuring proper grammatical structure. 3 in a report to the t"
U19-1023,D18-1081,0,0.0227936,"Missing"
U19-1023,P08-1040,0,0.0285573,"tion, where complex sentence structures are split, reordered, or deleted to produce simpler more readable structure. To implement those rewrite rules, researchers employ various methods broadly categorized into two categories: rule-based methods and data-driven methods (Siddharthan, 2014). In rule-based methods, the rules are hand-crafted a priori then applied to new text at simplification time. Examples of such rules include dictionarybased lookups for lexical simplification (Kurohashi & Sakai, 1999) or rules aiming at sentence restructuring into more readable formats. (A. Siddharthan, 2002; Vickrey & Koller, 2008). In contrast, data-driven methods frame the simplification 2 Rule-Based Simplification System In this section, we describe our proposed rulebased simplification system. It comprises two modules corresponding to the two major operations of text simplification: lexical and syntactic simplification. 1 2.1 tion with a language model to produce a set of candidate sentences and select the simplest among them. This process happens in an iterative greedy manner. First, the (word, Pos) pairs of the input sentence are scanned sequentially and for each pair with an entry in the synonyms dictionary, a co"
U19-1023,Q15-1021,0,0.0244447,"indeed yielded a simplified output. Datasets We perform two types of testing: (1) General-purpose Simplification: on WikiSmall (Zhu et al. 2010) and WikiLarge (Zhang & Lapata, 2017) datasets, where the latter is a superset of the former and both are collated by automatically aligning complex and simple sentences from the ordinary and simple English Wikipedia articles. We use the same test splits used in the mentioned study (100 sentences for WikiSmall and 354 sentences for WikiLarge not containing duplicates). This enables us to use their system output directly. We don’t use Newsela dataset (Xu et al 2015), which was used in their study, as it is not publicly available. (2) Medical Simplification: We use the held-out test set (50 sentences) from the medical dataset mentioned in section 3 to test our system. We couldn’t test the DRESS system on our medical dataset due to its extremely limited size leading to non-sensible results when used to train a neuralbased architecture such as DRESS. We, therefore, only compare our results with the baseline in case of medical data. 4 FKGL No Simplification DRESS Rule-Based 9.2 6.58 8.37 WikiLarge Avg. SARI words/sent 7.2 22.61 37.08 16.39 40.42 20.83 FKGL 1"
U19-1023,P07-2045,0,0.00727069,"applied in a recursive manner until the sentence has no more simplification clues. Generation: where the simplified sentences are reconstructed ensuring proper grammatical structure. 3 in a report to the total length of vocabulary. This is to ensure that the selected sentences dataset captures a diverse representation of the underlying medical reports corpus. The simplification was conducted by a medical expert and was targeted to address audience of Grade 6 level on the FleschKincaid scale (Kincaid et al. 1975). Extracting Synonyms After compiling the medical dataset, we used Moses toolkit (Koehn et al. 2007) to train a phrase-based machine translation model using 450 parallel sentences (the remaining 50 sentences were held out to test the system). One of the outputs of the trained model is the PBMT phrase table, which depicts potential mappings between source (i.e. complex) and destination (i.e. simple) phrases accompanied with maximum likelihood alignment scores for each phrase mapping. We used the phrase table to extract a phrase-synonyms dictionary of medical jargon, by scanning through each source phrase and selecting the destination phrase with the highest PBMT alignment score as its synonym"
U19-1023,Q16-1029,0,0.0227545,"ctors. As for the fluency reward, they use an LSTM language model trained on simple sentences to obtain a normalized perplexity score Medical Dataset First, we compiled a small parallel corpus of complex-simple medical text by manually simplifying 500 sentences drawn from “General Medicine” medical summary reports. The 500 sentences were randomly selected from a pool which included reports with the highest lexical diversity in the entire dataset. We calculate the lexical diversity as the ratio of unique word count 3 for the output sentence. For the simplicity reward, they use the SARI metric (Xu et al. 2016) which measures the n-gram overlaps between source, output and reference sentences. SARI will be further elaborated in section 5. Finally, to encourage lexical simplification, they use a separate pre-trained encoder-decoder model, trained in a non-reinforced setting on a parallel corpus of complex-simple sentences, to obtain lexical substitution probabilities based on a given source sentence. Using the latter model favors lexical simplification operations but does not take into account the fluency of the overall output. Therefore, the output of their system is determined by linearly combining"
U19-1023,P99-1062,0,0.0154708,"ical simplification, where difficult words are substituted with more common alternatives; and (2) syntactic simplification, where complex sentence structures are split, reordered, or deleted to produce simpler more readable structure. To implement those rewrite rules, researchers employ various methods broadly categorized into two categories: rule-based methods and data-driven methods (Siddharthan, 2014). In rule-based methods, the rules are hand-crafted a priori then applied to new text at simplification time. Examples of such rules include dictionarybased lookups for lexical simplification (Kurohashi & Sakai, 1999) or rules aiming at sentence restructuring into more readable formats. (A. Siddharthan, 2002; Vickrey & Koller, 2008). In contrast, data-driven methods frame the simplification 2 Rule-Based Simplification System In this section, we describe our proposed rulebased simplification system. It comprises two modules corresponding to the two major operations of text simplification: lexical and syntactic simplification. 1 2.1 tion with a language model to produce a set of candidate sentences and select the simplest among them. This process happens in an iterative greedy manner. First, the (word, Pos)"
U19-1023,D17-1062,0,0.260873,"sk the questions: Could similar success be achieved in Text Simplification by employing neural architectures? Would rule-based methods be more effective since simplification is a fundamentally different task than translation? To answer these questions, first, we propose a non-neural general-purpose rule-based simplification system. We, then, show how it can be adapted to address domain-specific simplification tasks by leveraging a small parallel dataset from the target domain. Subsequently, we compare the output of our system with that of a recently proposed neuralbased simplification system (Zhang & Lapata, 2017). In our study, we focus on two simplification domains: (1) general-purpose English, for which we run our tests using Wikipedia-based datasets; and (2) medical English, for which we compile a small medical parallel corpus of complex-simple pairs and use it to test our systems. We show that our rule-based system outperforms the neural system, in terms of simplicity, both qualitatively and quantitatively. Finally, we reflect on the output of both systems to pinpoint the shortcomings of each approach and encourage researchers to address them in future research. We propose a modular rule-based sys"
U19-1023,P02-1040,0,0.103376,"Missing"
U19-1023,C10-1152,0,0.0406498,"e-based system successfully substitutes difficult words with what seems to be reasonable and easier alternatives. It also splits composite structures into simpler form. For example, the appositive phrase in example 1 (the trickster character) and the conjoint Baseline Our baseline is simply an echo system where the input complex sentence is not simplified but rather passed through as the output. This allows a first-glance evaluation of whether a comparison system has indeed yielded a simplified output. Datasets We perform two types of testing: (1) General-purpose Simplification: on WikiSmall (Zhu et al. 2010) and WikiLarge (Zhang & Lapata, 2017) datasets, where the latter is a superset of the former and both are collated by automatically aligning complex and simple sentences from the ordinary and simple English Wikipedia articles. We use the same test splits used in the mentioned study (100 sentences for WikiSmall and 354 sentences for WikiLarge not containing duplicates). This enables us to use their system output directly. We don’t use Newsela dataset (Xu et al 2015), which was used in their study, as it is not publicly available. (2) Medical Simplification: We use the held-out test set (50 sent"
W11-2167,P09-1088,0,0.583313,"e probability p(e|f ) has to be shared by all the rules having the same source side string f , leading to fragmentation and resulting in many rules having very poor probability. Approaches to improve the inference (the induction of the SCFG rules from the bitext) typically follows two streams. One focusses on filtering the extracted hierarchical rules either by removing redundancy (He et al., 2009) or by filtering rules based on certain patterns (Iglesias et al., 2009), while the other stream is concerned about alternative approaches for learning the synchronous grammar (Blunsom et al., 2008; Blunsom et al., 2009; de Gispert et al., 2010). This paper falls under the latter category and we use a non-parametric Bayesian approach for rule extraction for Hiero-style systems. Our objective in this paper is to provide a principled 533 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 533–541, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics rule extraction method using a Bayesian framework that can extract the minimal SCFG rules without reducing the BLEU score. 2 Motivation and Related Work The large number of rules in Hiero-style systems le"
W11-2167,J07-2003,0,0.908814,"high quality rules leading to improved generalization and the automatic identification of commonly re-used rules. We show that our Bayesian model is able to extract minimal set of hierarchical phrase rules without impacting the translation quality as measured by the BLEU score. 1 While most of the research in Hiero-style systems is focused on the improving the decoder, and in particular the link to the language model, comparatively few papers have considered the inference of the probabilistic SCFG from the word alignments. A majority of the systems employ the classic ruleextraction algorithm (Chiang, 2007) which extracts rules by replacing possible sub-spans (permitted by the word alignments) with a non-terminal and then using relative frequencies to estimate the probabilistic synchronous context-free grammar. One of the issues in building Hiero-style systems is in managing the size of the synchronous grammar. The original approach extracts a larger number of rules when compared to a phrase-based system on the same data leading to practical issues in terms of memory requirements and decoding speed. Introduction Hierarchical phrase-based (Hiero) machine translation (Chiang, 2007) has attracted s"
W11-2167,N09-1062,0,0.0607756,"l maximize the likelihood of producing the entire set of observed phrase pairs). Using Bayes’ rule, the posterior over the derivations r given the phrase pairs Rp can be written as: P (r|Rp ) ∝ P (Rp |r)P (r) (1) where P (Rp |r) is equal to one when the sequence of rules r and phrase-pairs Rp are consistent, i.e. r can be partitioned into derivations to compose the set of phrase-pairs such that the derivations respect the given word alignments; otherwise P (Rp |r) is zero. The overall structure of the model is analogous to the Bayesian model for inducing Tree Substitution Grammars proposed by Cohn et al. (2009). Note that, our model extracts hierarchical rules for the word-aligned phrase pairs and not for the sentences. Similar to the other Hiero-style systems, we use two types of rules: terminal and hierarchical rules. For each phrase-pair, our model either generates a terminal rule by not segmenting the phrase-pair, or decides to segment the phrase-pair and extract some rules. Though it is possible to segment phrase-pairs by two (or more) non-overlapping spans, we propose a simpler model in this paper and restrict the hierarchical rules to contain only one non-terminal (unlike the case of classic"
W11-2167,D10-1053,0,0.0591137,"Missing"
W11-2167,D08-1033,0,0.0400314,"e 1: An example phrase-pair with Viterbi alignments X → (Eighth and Ninth X1 for the financial year, octavo y noveno X1 para el ejercicio) X → (European Development Funds, Fondos Europeos de Desarrollo) X → (Eighth and Ninth X1 , octavo y noveno X1 ) X → (European Development Funds for the financial year, Fondos Europeos de Desarrollo para el ejercicio) Figure 2: Two possible derivations of the phrase-pair in Figure 1 where a is the set of alignments in the given subspan; if the sub-span has multiple Viterbi alignments from different phrase-pairs, we consider the union of all such alignments. DeNero et al. (2008) use a similar prior- geometric mean of the forward and reverse IBM-1 alignments. However, we use the product of geometric means of the forward and reverse alignment scores. We also experimented with the arithmetic mean of the lexical alignment probabilities. The lexical prior lx in the first step can be defined similarly. We found the particular combination of, ‘arithmetic mean’ for the lexical prior lx (in the first step) and ‘geometric mean’ for the base distribution P0 (in the second step) to work better, as we discuss later in Section 5. Assuming the heuristically extracted phrase pairs t"
W11-2167,P07-1019,0,0.0149304,"nslation model and the language model was trained with the same set and an additional 2 million sentences from the UN data, using SRILM toolkit with Knesser-Ney discounting. We tuned the feature weights on the WMT-10 devset using MERT (Och, 2003) and evaluate on the test set by computing lower-cased BLEU score (Papineni et al., 2002) using the WMT-10 standard evaluation script. We use Kriya – an in-house implementation of hierarchical phrase-based translation written predominantly in Python. Kriya supports the entire translation pipeline of SCFG rule extraction and decoding with cube pruning (Huang and Chiang, 2007) and LM integration (Chiang, 2007). We use the 7 features (4 translation model features, extracted rules penalty, word penalty and language model) as is typical in Hiero-style systems. For tuning the feature weights, we have adapted the MERT implementation in Moses1 for use with Kriya as the decoder. We started by training and evaluating the two baseline systems using i) two non-terminals and ii) one non-terminal, which were trained using the conventional heuristic extraction approach. For the baseline with one non-terminal, we modified the heuristic rule extraction algorithm appropriately2 ."
W11-2167,E09-1044,0,0.587594,"Missing"
W11-2167,W02-1018,0,0.0804535,"Missing"
W11-2167,P03-1021,0,0.0416857,",[3,4]) ([2,2],[2,2]) ([0,0],[0,0]) ([1,1],[1,1]) Figure 4: Decomposed alignment tree for the example alignment in Fig. 3. 5 Experiments We use the English-Spanish data from WMT-10 shared task for the experiments to evaluate the effectiveness of our Bayesian rule extraction approach. We used the entire shared task training set except the UN data for training translation model and the language model was trained with the same set and an additional 2 million sentences from the UN data, using SRILM toolkit with Knesser-Ney discounting. We tuned the feature weights on the WMT-10 devset using MERT (Och, 2003) and evaluate on the test set by computing lower-cased BLEU score (Papineni et al., 2002) using the WMT-10 standard evaluation script. We use Kriya – an in-house implementation of hierarchical phrase-based translation written predominantly in Python. Kriya supports the entire translation pipeline of SCFG rule extraction and decoding with cube pruning (Huang and Chiang, 2007) and LM integration (Chiang, 2007). We use the 7 features (4 translation model features, extracted rules penalty, word penalty and language model) as is typical in Hiero-style systems. For tuning the feature weights, we hav"
W11-2167,P02-1040,0,0.0828271,"ent tree for the example alignment in Fig. 3. 5 Experiments We use the English-Spanish data from WMT-10 shared task for the experiments to evaluate the effectiveness of our Bayesian rule extraction approach. We used the entire shared task training set except the UN data for training translation model and the language model was trained with the same set and an additional 2 million sentences from the UN data, using SRILM toolkit with Knesser-Ney discounting. We tuned the feature weights on the WMT-10 devset using MERT (Och, 2003) and evaluate on the test set by computing lower-cased BLEU score (Papineni et al., 2002) using the WMT-10 standard evaluation script. We use Kriya – an in-house implementation of hierarchical phrase-based translation written predominantly in Python. Kriya supports the entire translation pipeline of SCFG rule extraction and decoding with cube pruning (Huang and Chiang, 2007) and LM integration (Chiang, 2007). We use the 7 features (4 translation model features, extracted rules penalty, word penalty and language model) as is typical in Hiero-style systems. For tuning the feature weights, we have adapted the MERT implementation in Moses1 for use with Kriya as the decoder. We started"
W11-2167,C08-1136,0,0.171982,"unts averaged by the number of thin iterations become our translation model. In our model, a sample for a given phrase pair corresponds either to its terminal derivation or two rules in a hierarchical derivation. The model samples a derivation from the space of derivations that are consistent with the word alignments. In order to achieve this, we need an efficient way to enumerate the derivations for a phrase pair such that they are consistent with the alignments. We use the linear time algorithm to maximally decompose a wordaligned phrase pair, so as to encode it as a compact alignment tree (Zhang et al., 2008). e0 e1 e2 e3 e4 f0 f1 f2 f3 f4 e5 Figure 3: Example phrase pair with alignments. For a phrase-pair with a given alignment as shown in Figure 3, Zhang et al. (2008) generalize the O(n+ K) time algorithm for computing all K common intervals of two different permutations of length n. The contiguous blocks of the alignment are captured as the nodes in the alignment tree and the tree structure for the example phrase pair in Figure 3 is shown in Figure 4. The italicized nodes form a leftbranching chain in the alignment tree and the subspans of this chain also lead to alignment nodes that are not ex"
W11-2167,C08-1144,0,0.205434,"om an aligned bitext. The synchronous context-free grammar links non-terminals in source and target languages. Decoding in such systems employ a modified CKYparser that is integrated with a language model. The primary advantage of Hiero-style systems lie in their unsupervised model of syntax for translation: allowing long-distance reordering and capturing certain syntactic constructions, particularly those that involve discontiguous phrases. It has been demonstrated to be a successful framework with comparable performance with other statistical frameworks and suitable for large-scale corpora (Zollmann et al., 2008). However, one of the Extremely large Hiero phrase tables may also lead to statistical issues, where the probability mass has to be shared by more rules: the probability p(e|f ) has to be shared by all the rules having the same source side string f , leading to fragmentation and resulting in many rules having very poor probability. Approaches to improve the inference (the induction of the SCFG rules from the bitext) typically follows two streams. One focusses on filtering the extracted hierarchical rules either by removing redundancy (He et al., 2009) or by filtering rules based on certain pat"
W15-5937,W10-3710,0,0.030442,"Rhyme: Either identical ultimate stressed vowels or identical phoneme sequences following the ultimate stressed vowel, but not both Examples cat-cat, বাঁেক-বাঁেক (baanke-baanke) cat-rat, বাঁেক-থােক (baanke-thaake) stick-picket জবা - অবাক (joba-obaak) queen-afternoon কেলাল - েকালাহল (kallol-kolahol) Table 1: Types of Rhyme (anaadore abohelay). To detect alliteration, we check the beginning sound of each word for every pair of consecutive words in a line. Reduplication refers to the repetition of any linguistic unit such as a phoneme, morpheme, word, phrase, clause or the utterance as a whole (Chakraborty and Bandyopadhyay, 2010). It is mainly used for emphasis, generality, intensity or to show continuation of an act. It may be partial (খাওয়া দাওয়া khaawa daawa) or complete (আকােশ আকােশ akaashe akaashe). We check only for complete reduplication. We use a simple algorithm that basically checks if two consecutive words in the poem are identical. 4.2 Rhyme Scheme Detection A rhyme scheme is the pattern of rhymes at the end of each line of a poem or song. The rhyme scheme of the poem can be determined by looking at the end word in each line of a poem. Various rhyme schemes are used. Ex: abab, aabb, ababcc and so on. In th"
W15-5937,W12-2502,0,0.0521765,"n Natural Language Processing, pages 247–253, c Trivandrum, India. December 2015. 2015 NLP Association of India (NLPAI) cuss the literature in Section 2, and describe our approach in Section 3. The system architecture and its details have been described in 4. The experimental setup and results are covered in sections 5 and 6, respectively. We delve into analysis of the results in Section 7. We conclude our work and discuss scope for future work in Section 8. 2 Related Work Computational understanding of poetry has been previously studied for languages such as English (Kaplan and Blei, 2007), (Kao and Jurafsky, 2012), Chinese (Voigt and Jurafsky, 2013) and Malay (Jamal et al., 2012). Kaplan and Blei (2007) analyse American poems in terms of style and visualise them as clusters. Kao and Jurafsky (2012) use various stylistic features to categorise poems into ones written by professional and amateur poets, and establish the importance of Imagism in poetry of high-quality. Lou et al. (2015) use of a SVM to classify poems in English into 3 main categories and 9 subcategories by combining tf-idf and Latent Dirichlet Allocation. All this work has been done for English. Voigt and Jurafsky (2013) observed through"
W15-5937,W13-1403,0,\N,Missing
W17-4115,D15-1041,0,0.059412,"Missing"
W17-4115,P03-1021,0,0.0525884,"Missing"
W17-4115,P16-2058,0,0.0842576,"Missing"
W17-4115,E17-1048,0,0.100246,"Missing"
W17-4115,P17-1184,0,0.0482079,"Missing"
W17-4115,D13-1176,0,0.123419,"Missing"
W17-4115,2005.mtsummit-papers.11,0,0.0131352,"Missing"
W17-4115,N16-1030,0,0.136004,"Missing"
W17-4115,D15-1176,0,0.0715268,"Missing"
W17-4115,W13-3512,0,0.0493206,"Missing"
W18-2703,W11-2138,0,0.144125,"is used to build better translation systems in forward and backward directions, which in turn is used to reback-translate monolingual data. This process can be “iterated” several times. This is a form of co-training (Blum and Mitchell, 1998) where the two models over both translation directions can be used to train one another. We show that iterative back-translation leads to improved results over simple back-translation, under both high and 2 Related Work The idea of back-translation dates back at least to statistical machine translation, where it has been used for semi-supervised learning (Bojar and Tamchyna, 2011), or self-training (Goutte et al., 2009, ch.12, p.237). In modern NMT research, Sennrich et al. (2017) reported significant gains on the WMT and IWSLT shared tasks. They showed that even simply duplicating the monolingual target data into the source was sufficient to realise some benefits. Currey et al. (2017) reported similar findings for low resource conditions, showing that even poor translations can be beneficial. Gwinnup et al. (2017) mention in their system description iteratively applying back-translation, but did not report successful experiments. A more refined idea of back-translatio"
W18-2703,W16-2323,0,0.520255,"uding the best reported BLEU scores for the WMT 2017 German↔English tasks. 1 real+synthetic reverse system final system synthetic Figure 1: Creating a synthetic parallel corpus through back-translation. First, a system in the reverse direction is trained and then used to translate monolingual data from the target side backward into the source side, to be used in the final system. Introduction low resource conditions, improving over the state of the art. The exploitation of monolingual training data for neural machine translation is an open challenge. One successful method is back-translation (Sennrich et al., 2016b), whereby an NMT system is trained in the reverse translation direction (targetto-source), and is then used to translate target-side monolingual data back into the source language (in the backward direction, hence the name backtranslation). The resulting sentence pairs constitute a synthetic parallel corpus that can be added to the existing training data to learn a source-totarget model. Figure 1 illustrates this idea. In this paper, we show that the quality of backtranslation matters and propose iterative backtranslation, where back-translated data is used to build better translation system"
W18-2703,P16-1009,0,0.485876,"uding the best reported BLEU scores for the WMT 2017 German↔English tasks. 1 real+synthetic reverse system final system synthetic Figure 1: Creating a synthetic parallel corpus through back-translation. First, a system in the reverse direction is trained and then used to translate monolingual data from the target side backward into the source side, to be used in the final system. Introduction low resource conditions, improving over the state of the art. The exploitation of monolingual training data for neural machine translation is an open challenge. One successful method is back-translation (Sennrich et al., 2016b), whereby an NMT system is trained in the reverse translation direction (targetto-source), and is then used to translate target-side monolingual data back into the source language (in the backward direction, hence the name backtranslation). The resulting sentence pairs constitute a synthetic parallel corpus that can be added to the existing training data to learn a source-totarget model. Figure 1 illustrates this idea. In this paper, we show that the quality of backtranslation matters and propose iterative backtranslation, where back-translated data is used to build better translation system"
W18-2703,E17-3017,0,0.0244524,"gs) with droption that the back-translation approach still imout of 0.2 for the RNN parameters, and 0.1 otherproves the translation accuracy in all language wise. Training is smoothed with moving average. pairs with a low-resource setting. In the English– It takes about 2–4 days. French experiments, large improvements over the The deep system uses matches the setup of baseline are observed in both directions, with +3.5 Edinburgh’s WMT 2017 system (Sennrich et al., 3 The difference here is on the NMT toolkit used — we 2017). It uses 4 encoder and 4 decoder layers opted to use Amazon’s Sockeye (Hieber et al., 2017). We (Marian setting best-deep) with LSTM cells. used Sockeye’s default configuration with dropout 0.5. 21 Setting NMT baseline back-translation back-translation iterative+1 back-translation iterative+2 back-translation (w/ Moses) French–English 100K 1M English–French 100K 1M 16.7 22.1 22.5 22.6 23.7 18.0 21.5 22.7 22.6 23.5 24.7 27.8 27.9 25.6 27.0 27.3 Farsi–English 100K English-Farsi 100K 21.7 22.1 22.7 22.6 21.8 16.4 16.7 17.1 17.2 16.8 Table 4: Low Resource setting: Impact of the quality of the back-translation systems on the benefit of the synthetic parallel for the final system in a low"
W18-2703,P16-1162,0,0.808379,"uding the best reported BLEU scores for the WMT 2017 German↔English tasks. 1 real+synthetic reverse system final system synthetic Figure 1: Creating a synthetic parallel corpus through back-translation. First, a system in the reverse direction is trained and then used to translate monolingual data from the target side backward into the source side, to be used in the final system. Introduction low resource conditions, improving over the state of the art. The exploitation of monolingual training data for neural machine translation is an open challenge. One successful method is back-translation (Sennrich et al., 2016b), whereby an NMT system is trained in the reverse translation direction (targetto-source), and is then used to translate target-side monolingual data back into the source language (in the backward direction, hence the name backtranslation). The resulting sentence pairs constitute a synthetic parallel corpus that can be added to the existing training data to learn a source-totarget model. Figure 1 illustrates this idea. In this paper, we show that the quality of backtranslation matters and propose iterative backtranslation, where back-translated data is used to build better translation system"
W18-2703,U16-1001,1,0.520715,"nt of parallel data to reach reasonable performance (Koehn and Knowles, 2017). In a lowthe parallel data and the synthetic data generresource setting, only small amount of parallel ated by the base translation system. For better data exist. Previous work has attempted to inperformance, we train a deep model with 8corporate prior or external knowledge to compencheckpoint ensembling; again we use a beam sate for the lack of parallel data, e.g. injecting insize of 2. ductive bias via linguistic constraints (Cohn et al., The final back-translation systems were trained 2016) or linguistic factors (Hoang et al., 2016). using several different systems: a shallow arHowever, it is much cheaper and easier to obtain chitecture, a deep architecture, and an ensemmonolingual data in either the source or target lanble system of 4 independent training runs. guage. An interesting question is whether the (iterAcross the board, the final systems with reative) back-translation can compensate for the lack back-translation outperform the final systems with of parallel data in such low-resource settings. simple back-translation, by a margin of 0.5–1.1 BLEU. To explore this question, we conducted experiments on two datasets"
W18-2703,P18-4020,0,0.0750003,"Missing"
W18-2703,W17-3204,1,0.74613,"ely) in Table 3. Best WMT 2017 28.3 For all experiments, the true-casing model and Table 3: WMT News Translation Task German– the list of BPE operations is left constant. Both English, comparing the quality of different backwere learned from the original parallel training translation systems with different final system arcorpus. chitectures. *Note that the quality for the backtranslation system (Back) is measured in the op4.2 Experiments on Low Resource Scenario posite language direction. NMT is a data-hungry approach, requiring a large amount of parallel data to reach reasonable performance (Koehn and Knowles, 2017). In a lowthe parallel data and the synthetic data generresource setting, only small amount of parallel ated by the base translation system. For better data exist. Previous work has attempted to inperformance, we train a deep model with 8corporate prior or external knowledge to compencheckpoint ensembling; again we use a beam sate for the lack of parallel data, e.g. injecting insize of 2. ductive bias via linguistic constraints (Cohn et al., The final back-translation systems were trained 2016) or linguistic factors (Hoang et al., 2016). using several different systems: a shallow arHowever, it"
W18-2703,W17-4710,0,0.0258173,"ons. Under high-resource conditions, we improve the state of the art with re-back-translation. Under low-resource conditions, we demonstrate Experiments on High Resource Scenario In §3 we demonstrated that the quality of the backtranslation system has significant impact on the effectiveness of the back-translation approach under high-resource data conditions such as WMT 2017 German–English. Here we ask: how much additional benefit can be realised for repeating this process? Also, do the gains for state-of-the-art systems that use deeper models, i.e., more layers in encoder and decoder (Miceli Barone et al., 2017) still apply in this setting? We evaluate on German–English and English– German, under the same data conditions as in Section 3. We experiment with both shallow and deep stacked-layer encoder/decoder architectures. The base translation system is trained on the parallel data only. We train a shallow system using 4-checkpoint ensembling (Chen et al., 2017). The system is used to translate the monolingual data using a beam size of 2. The first back-translation system is trained on 20 German–English Back* Shallow Deep Ensemble back-translation 23.7 32.5 35.0 35.6 re-back-translation 27.9 33.6 36.1"
W18-6311,D17-1151,0,0.0188792,"nimum overall perplexity on the bilingual dev set. For the source context representations, we use the sentence representations generated by two sentence-level bidirectional RNNLMs (one each for English and Foreign) trained offline. For the target sentence representations, we use the last hidden states of the decoder generated from the pre-trained base model10 . At decoding time, however, we use the last hidden state of the decoder computed by our model (not the base) as the target sentence representations. Further training details are provided in Appendix B. 8 We follow Cohn et al. (2016) and Britz et al. (2017) in choosing hyperparameters for our model. 9 For each language-pair, we use BPE (Sennrich et al., 2016) to obtain a joint vocabulary of size ≈30k. 10 Even though the paramaters of the base model are updated, the target sentence representations are fixed throughout training. We experimented with a scheduled updating scheme in preliminary experiments but it did not yield significant improvement. Base Model Europarl Subtitles En-Fr En-Et En-De En-Ru Overall En→Fr Fr→En Overall En→Et Et→En Overall En→De De→En Overall En→Ru Ru→En 37.36 38.13 36.03 20.68 18.64 26.65 24.74 21.80 27.74 19.05 14.90 23"
W18-6311,W14-4012,0,0.150826,"Missing"
W18-6311,P11-2031,0,0.0694498,"Src-Tgt-Mix 38.76† 39.24† 39.57† 39.51† 39.52† 37.06† 37.35† 37.50† 37.43† 21.84† 21.77† 21.74† 21.68† 19.58† 19.68† 19.60† 19.63† 28.43† 27.86† 27.98† 27.71† 26.49† 26.21† 26.39† 26.37† 23.49† 23.16† 23.28† 23.26† 29.49† 29.26† 29.50† 29.48† 19.09 19.23 18.89 19.26 14.59 14.77 14.52 14.86 22.98 23.23 23.06 23.01 Table 2: BLEU scores for the bilingual test sets. Here all contexts are incorporated as InitDec for Europarl and InitDec+AddDec for Subtitles unless otherwise specified. bold: Best performance, †: Statistically significantly better than the base model, based on bootstrap resampling (Clark et al., 2011) with p &lt; 0.05. 5.1 Results just the turn-level one. Finally, our results with source, target and dual contexts are reported. Interestingly, just using the source context is sufficient for English-Estonian and English-German. For English-French, on the other hand, we see significant improvements for the models using the target-side conversation history over using only the source-side. We attribute this to the base model being more efficient and able to generate better translations for En-Fr as it had been trained on a larger corpus as opposed to the other two language-pairs. Unlike Europarl, f"
W18-6311,2005.mtsummit-papers.11,0,0.0848971,"significant improvements over all baselines when translating subtitles and comparable performance for news and TED talks. Along similar lines, Kuang et al. (2018) propose dynamic and topic caches to capture contextual information either from recently translated sentences or the entire document to model coherence for NMT. Voita et al. (2018) introduce a context3.2 Data Standard machine translation datasets are inappropriate for Bi-MSMT task since they are not composed of conversations or the speaker annotations are missing. In this section, we describe how we extract data from raw Europarl v7 (Koehn, 2005) and OpenSubtitles20161 (Lison and Tiedemann, 2016) for this task2 . Europarl The raw Europarl v7 corpus (Koehn, 2005) contains SPEAKER and LANGUAGE tags where the latter indicates the language the speaker was actually using. The individual files are first split into conversations. The data is tokenised (using scripts by Koehn (2005)), and cleaned (headings and single token sentences removed). Conversations are divided into smaller ones if the number of speakers is greater than 5.3 The corpus is then randomly split into train/dev/test sets with respect to conversations in ratio 100:2:3. The En"
W18-6311,C18-1050,0,0.0491435,"roposed by Jean et al. (2017); Wang et al. (2017). Apart from being difficult to scale, they report deteriorated BLEU scores when using the target-side context. Tu et al. (2017) augment the vanilla NMT model with a continuous cache-like memory, along the same lines as the cache-based system for traditional document MT (Gong et al., 2011), which stores hidden representations of recently generated words as translation history. The proposed approach shows significant improvements over all baselines when translating subtitles and comparable performance for news and TED talks. Along similar lines, Kuang et al. (2018) propose dynamic and topic caches to capture contextual information either from recently translated sentences or the entire document to model coherence for NMT. Voita et al. (2018) introduce a context3.2 Data Standard machine translation datasets are inappropriate for Bi-MSMT task since they are not composed of conversations or the speaker annotations are missing. In this section, we describe how we extract data from raw Europarl v7 (Koehn, 2005) and OpenSubtitles20161 (Lison and Tiedemann, 2016) for this task2 . Europarl The raw Europarl v7 corpus (Koehn, 2005) contains SPEAKER and LANGUAGE t"
W18-6311,N16-1102,1,0.867266,"n training set D. For example, for a conversation having alternating turns of English and Foreign language, the log-likelihood is: |T | −1 2 X |t2k+1 X| k=0 i=1 |t2k+2 | log Pθ (yi |xi , oi ) + X j=1 log Pθ (xj |yj , oj )  where i, j denote sentences belonging to 2k + 1th or 2k + 2th turn; o(.) is a representation of the conversation history, and |T |is the total number of turns (assumed to be even here). 106 Experiments Implementation and Hyperparameters We implement our conversational Bi-MSMT model in C++ using the DyNet library (Neubig et al., 2017). The base model is built using mantis (Cohn et al., 2016) which is an implementation of the generic sentence-level NMT model using DyNet. The base model has single layer bidirectional GRUs in the encoder and 2-layer GRU in the decoder8 . The hidden dimensions and word embedding sizes are set to 256, and the alignment dimension (for the attention mechanism in the decoder) is set to 128. Models and Training We do a stage-wise training for the base model, i.e., we first train the English→Foreign architecture and the Foreign→English architecture, using the sentence-level parallel corpus. Both architectures have the same vocabulary9 but separate paramete"
W18-6311,L16-1147,0,0.024601,"elines when translating subtitles and comparable performance for news and TED talks. Along similar lines, Kuang et al. (2018) propose dynamic and topic caches to capture contextual information either from recently translated sentences or the entire document to model coherence for NMT. Voita et al. (2018) introduce a context3.2 Data Standard machine translation datasets are inappropriate for Bi-MSMT task since they are not composed of conversations or the speaker annotations are missing. In this section, we describe how we extract data from raw Europarl v7 (Koehn, 2005) and OpenSubtitles20161 (Lison and Tiedemann, 2016) for this task2 . Europarl The raw Europarl v7 corpus (Koehn, 2005) contains SPEAKER and LANGUAGE tags where the latter indicates the language the speaker was actually using. The individual files are first split into conversations. The data is tokenised (using scripts by Koehn (2005)), and cleaned (headings and single token sentences removed). Conversations are divided into smaller ones if the number of speakers is greater than 5.3 The corpus is then randomly split into train/dev/test sets with respect to conversations in ratio 100:2:3. The English side of the corpus is set as reference, and 1"
W18-6311,P18-1118,1,0.908436,"on, e.g. metadata, and Tran et al. (2016) use inter-document context to boost the performance 101 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 101–112 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64011 aware NMT model in which they control and analyse the flow of information from the extended context to the translation model. They show that using the previous sentence as context their model is able to implicitly capture anaphora. For the offline setting, Maruf and Haffari (2018) incorporate the global source and target document contexts into the base NMT model via memory networks. They report significant improvements using BLEU and METEOR for the contextual model over the baseline. To the best of our knowledge, there has been no work on Multi-Speaker MT or its variation to date. of RNN language models. For conversational language modelling, Ji and Bilmes (2004) propose a statistical multi-speaker language model (MSLM) that considers words from other speakers when predicting words from the current one. By taking the inter-speaker dependency into account using a normal"
W18-6311,D11-1084,0,0.121514,"generate a summary of three previous source sentences via a hierarchical RNN, which is then added as an auxiliary input to the decoder. Bawden et al. (2017) explore various ways to exploit context from the previous sentence on the source and target-side by extending the models proposed by Jean et al. (2017); Wang et al. (2017). Apart from being difficult to scale, they report deteriorated BLEU scores when using the target-side context. Tu et al. (2017) augment the vanilla NMT model with a continuous cache-like memory, along the same lines as the cache-based system for traditional document MT (Gong et al., 2011), which stores hidden representations of recently generated words as translation history. The proposed approach shows significant improvements over all baselines when translating subtitles and comparable performance for news and TED talks. Along similar lines, Kuang et al. (2018) propose dynamic and topic caches to capture contextual information either from recently translated sentences or the entire document to model coherence for NMT. Voita et al. (2018) introduce a context3.2 Data Standard machine translation datasets are inappropriate for Bi-MSMT task since they are not composed of convers"
W18-6311,2010.iwslt-papers.10,0,0.0683895,"odels. For conversational language modelling, Ji and Bilmes (2004) propose a statistical multi-speaker language model (MSLM) that considers words from other speakers when predicting words from the current one. By taking the inter-speaker dependency into account using a normal trigram context, they report significant reduction in perplexity. Statistical Machine Translation The few SMTbased attempts to document MT are either restrictive or do not lead to significant improvements upon automatic evaluation. Few of these deal with specific discourse phenomena, such as resolving anaphoric pronouns (Hardmeier and Federico, 2010) or lexical consistency of translations (Garcia et al., 2017). Others are based on a twopass approach i.e., to improve the translations already obtained by a sentence-level model (Hardmeier et al., 2012; Garcia et al., 2014). 3 Preliminaries 3.1 Problem Formulation We are given a dataset that comprises parallel conversations, and each conversation consists of turns. Each turn is constituted by sentences spoken by a single speaker, denoted by x or y, if the sentence is in English or Foreign language, respectively. The goal is to learn a model that is able to leverage the mixed-language conversa"
W18-6311,P16-1162,0,0.478312,"the source language is English, otherwise Foreign. The sentences in the source-side of the corpus are kept or swapped with those in the target-side based on this tag. We perform the aforementioned steps for English-French, English-Estonian and EnglishGerman, and obtain the bilingual multi-speaker corpora for the three language pairs. Before splitting into train/dev/test sets, we remove conversations with sentences having more than 100 tokens for English-French, English-German and more than 80 tokens for English-Estonian4 respectively, to limit the sentence-length for using subwords with BPE (Sennrich et al., 2016). The data statistics are given in Table 1 and Appendix A5 . Encoder It maps each source word xm to a distributed representation hm which is the concatenation of the corresponding hidden states of two RNNs running in opposite directions over the source sentence. The forward and backward RNNs are taken to be GRUs (gated-recurrent unit; Cho et al. (2014)) in this work. Decoder The generation of each target word yn is conditioned on all the previously generated words y&lt;n via the state sn of the decoder, and the source sentence via a dynamic context vector cn : yn un sn Subtitles There has been re"
W18-6311,D12-1108,0,0.0178585,"By taking the inter-speaker dependency into account using a normal trigram context, they report significant reduction in perplexity. Statistical Machine Translation The few SMTbased attempts to document MT are either restrictive or do not lead to significant improvements upon automatic evaluation. Few of these deal with specific discourse phenomena, such as resolving anaphoric pronouns (Hardmeier and Federico, 2010) or lexical consistency of translations (Garcia et al., 2017). Others are based on a twopass approach i.e., to improve the translations already obtained by a sentence-level model (Hardmeier et al., 2012; Garcia et al., 2014). 3 Preliminaries 3.1 Problem Formulation We are given a dataset that comprises parallel conversations, and each conversation consists of turns. Each turn is constituted by sentences spoken by a single speaker, denoted by x or y, if the sentence is in English or Foreign language, respectively. The goal is to learn a model that is able to leverage the mixed-language conversation history in order to produce high quality translations. Neural Machine Translation Using contextbased neural models for improving online and offline NMT is a popular trend recently. Jean et al. (201"
W18-6311,N16-1090,1,0.798072,"ng the discourse or document context to improve NMT, in an online setting, by using the past context (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017; Voita 2 Related Work Our research builds upon prior work in the field of context-based language modelling and contextbased machine translation. Language Modelling There have been few works on leveraging context information for language modelling. Ji et al. (2015) introduced Document Context Language Model (DCLM) which incorporates inter and intra-sentential contexts. Hoang et al. (2016) make use of side information, e.g. metadata, and Tran et al. (2016) use inter-document context to boost the performance 101 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 101–112 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64011 aware NMT model in which they control and analyse the flow of information from the extended context to the translation model. They show that using the previous sentence as context their model is able to implicitly capture anaphora. For the offline setting, Maruf and Haffari (2018) incorporate the"
W18-6311,N16-1149,1,0.858868,"ored in the literature. Recently, there has been work focusing on using the discourse or document context to improve NMT, in an online setting, by using the past context (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017; Voita 2 Related Work Our research builds upon prior work in the field of context-based language modelling and contextbased machine translation. Language Modelling There have been few works on leveraging context information for language modelling. Ji et al. (2015) introduced Document Context Language Model (DCLM) which incorporates inter and intra-sentential contexts. Hoang et al. (2016) make use of side information, e.g. metadata, and Tran et al. (2016) use inter-document context to boost the performance 101 Proceedings of the Third Conference on Machine Translation (WMT), Volume 1: Research Papers, pages 101–112 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64011 aware NMT model in which they control and analyse the flow of information from the extended context to the translation model. They show that using the previous sentence as context their model is able to implicitly capture anaphora"
W18-6311,P18-1117,0,0.0617836,"gment the vanilla NMT model with a continuous cache-like memory, along the same lines as the cache-based system for traditional document MT (Gong et al., 2011), which stores hidden representations of recently generated words as translation history. The proposed approach shows significant improvements over all baselines when translating subtitles and comparable performance for news and TED talks. Along similar lines, Kuang et al. (2018) propose dynamic and topic caches to capture contextual information either from recently translated sentences or the entire document to model coherence for NMT. Voita et al. (2018) introduce a context3.2 Data Standard machine translation datasets are inappropriate for Bi-MSMT task since they are not composed of conversations or the speaker annotations are missing. In this section, we describe how we extract data from raw Europarl v7 (Koehn, 2005) and OpenSubtitles20161 (Lison and Tiedemann, 2016) for this task2 . Europarl The raw Europarl v7 corpus (Koehn, 2005) contains SPEAKER and LANGUAGE tags where the latter indicates the language the speaker was actually using. The individual files are first split into conversations. The data is tokenised (using scripts by Koehn ("
W18-6311,N04-4034,0,0.0768255,"he flow of information from the extended context to the translation model. They show that using the previous sentence as context their model is able to implicitly capture anaphora. For the offline setting, Maruf and Haffari (2018) incorporate the global source and target document contexts into the base NMT model via memory networks. They report significant improvements using BLEU and METEOR for the contextual model over the baseline. To the best of our knowledge, there has been no work on Multi-Speaker MT or its variation to date. of RNN language models. For conversational language modelling, Ji and Bilmes (2004) propose a statistical multi-speaker language model (MSLM) that considers words from other speakers when predicting words from the current one. By taking the inter-speaker dependency into account using a normal trigram context, they report significant reduction in perplexity. Statistical Machine Translation The few SMTbased attempts to document MT are either restrictive or do not lead to significant improvements upon automatic evaluation. Few of these deal with specific discourse phenomena, such as resolving anaphoric pronouns (Hardmeier and Federico, 2010) or lexical consistency of translatio"
W18-6311,D17-1301,0,0.191981,"urce and target languages. We investigate neural architectures that exploit the bilingual conversation history for this scenario, which is a challenging problem as the history consists of utterances in both languages. The ultimate aim of all machine translation systems for dialogue is to enable a multi-lingual conversation between multiple speakers. However, translation of such conversations is not wellexplored in the literature. Recently, there has been work focusing on using the discourse or document context to improve NMT, in an online setting, by using the past context (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017; Voita 2 Related Work Our research builds upon prior work in the field of context-based language modelling and contextbased machine translation. Language Modelling There have been few works on leveraging context information for language modelling. Ji et al. (2015) introduced Document Context Language Model (DCLM) which incorporates inter and intra-sentential contexts. Hoang et al. (2016) make use of side information, e.g. metadata, and Tran et al. (2016) use inter-document context to boost the performance 101 Proceedings of the Third Conference on Machine Translation (WMT"
W18-6311,L16-1436,0,0.297821,"enation of the corresponding hidden states of two RNNs running in opposite directions over the source sentence. The forward and backward RNNs are taken to be GRUs (gated-recurrent unit; Cho et al. (2014)) in this work. Decoder The generation of each target word yn is conditioned on all the previously generated words y&lt;n via the state sn of the decoder, and the source sentence via a dynamic context vector cn : yn un sn Subtitles There has been recent work to obtain speaker labels via automatic turn segmentation for the OpenSubtitles2016 corpus (Lison and Meena, 2016; van der Wees et al., 2016; Wang et al., 2016). We obtain the English side of OpenSubtitles2016 corpus annotated with speaker information by Lison and Meena (2016).6 To obtain the parallel corpus, we use the OpenSubtitles alignment links to align foreign subtitles to the annotated English ones. For each subtitle, we extract individual conversations with more than 5 sentences and at least two turns. Conversations with more than 30 turns are discarded. Finally, since subtitles are in a single language, we assign language tag such that the same language occurs in alternating turns. We thus obtain the Bi-MSMT corpus for English-Russian, which"
W18-6311,C16-1242,0,0.0526471,"Missing"
