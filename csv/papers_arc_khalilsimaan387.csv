N18-1092,Deep Generative Model for Joint Alignment and Word Representation,2018,34,0,3,0,25928,miguel rios,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"This work exploits translation data as a source of semantically relevant learning signal for models of word representation. In particular, we exploit equivalence through translation as a form of distributional context and jointly learn how to embed and align with a deep generative model. Our EmbedAlign model embeds words in their complete observed context and learns by marginalisation of latent lexical alignments. Besides, it embeds words as posterior probability densities, rather than point estimates, which allows us to compare words in context using a measure of overlap between distributions (e.g. KL divergence). We investigate our model{'}s performance on a range of lexical semantics tasks achieving competitive results on several standard benchmarks including natural language inference, paraphrasing, and text similarity."
P17-2004,Alternative Objective Functions for Training {MT} Evaluation Metrics,2017,12,0,2,1,10251,milovs stanojevic,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"MT evaluation metrics are tested for correlation with human judgments either at the sentence- or the corpus-level. Trained metrics ignore corpus-level judgments and are trained for high sentence-level correlation only. We show that training only for one objective (sentence or corpus level), can not only harm the performance on the other objective, but it can also be suboptimal for the objective being optimized. To this end we present a metric trained for corpus-level and show empirical comparison against a metric trained for sentence-level exemplifying how their performance may vary per language pair, type and level of judgment. Subsequently we propose a model trained to optimize both objectives simultaneously and show that it is far more stable than{--}and on average outperforms{--}both models on both objectives."
D17-1209,Graph Convolutional Encoders for Syntax-aware Neural Machine Translation,2017,25,9,5,0.952381,10758,jasmijn bastings,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoder-decoder models for machine translation. We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data. Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods. GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks). We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups."
W16-6402,Factoring Adjunction in Hierarchical Phrase-Based {SMT},2016,-1,-1,2,1,5467,sophie arnoult,Proceedings of the 2nd Deep Machine Translation Workshop,0,None
W16-3210,{M}ulti30{K}: Multilingual {E}nglish-{G}erman Image Descriptions,2016,15,51,3,0,2490,desmond elliott,Proceedings of the 5th Workshop on Vision and Language,0,"We introduce the Multi30K dataset to stimulate multilingual multimodal research. Recent advances in image description have been demonstrated on English-language datasets almost exclusively, but image description should not be limited to English. This dataset extends the Flickr30K dataset with i) German translations created by professional translators over a subset of the English descriptions, and ii) descriptions crowdsourced independently of the original English descriptions. We outline how the data can be used for multilingual image description and multimodal machine translation, but we anticipate the data will be useful for a broader range of tasks."
W16-2330,{ILLC}-{U}v{A} Adaptation System (Scorpio) at {WMT}{'}16 {IT}-{DOMAIN} Task,2016,18,3,3,1,23615,hoang cuong,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes Scorpio, the ILLCUvA Adaptation System submitted to the IT-DOMAIN translation task at WMT 2016, which participated with the language pair of English-Dutch. This system consolidates the ideas in our previous work on latent variable models for adaptation, and demonstrates their effectiveness in a competitive setting."
W16-2346,A Shared Task on Multimodal Machine Translation and Crosslingual Image Description,2016,33,74,3,0,2509,lucia specia,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper introduces and summarises the findings of a new shared task at the intersection of Natural Language Processing and Computer Vision: the generation of image descriptions in a target language, given an image and/or one or more descriptions in a different (source) language. This challenge was organised along with the Conference on Machine Translation (WMT16), and called for system submissions for two task variants: (i) a translation task, in which a source language image description needs to be translated to a target language, (optionally) with additional cues from the corresponding image, and (ii) a description generation task, in which a target language description needs to be generated for an image, (optionally) with additional cues from source language descriptions of the same image. In this first edition of the shared task, 16 systems were submitted for the translation task and seven for the image description task, from a total of 10 teams."
W16-2213,Examining the Relationship between Preordering and Word Order Freedom in Machine Translation,2016,53,2,4,1,33919,joachim daiber,"Proceedings of the First Conference on Machine Translation: Volume 1, Research Papers",0,"We study the relationship between word order freedom and preordering in statistical machine translation. To assess word order freedom, we first introduce a novel entropy measure which quantifies how difficult it is to predict word order given a source sentence and its syntactic analysis. We then address preordering for two target languages at the far ends of the word order freedom spectrum, German and Japanese, and argue that for languages with more word order freedom, attempting to predict a unique word order given source clues only is less justified. Subsequently, we examine lattices of n-best word order predictions as a unified representation for languages from across this broad spectrum and present an effective solution to a resulting technical issue, namely how to select a suitable source word order from the lattice during training. Our experiments show that lattices are crucial for good empirical performance for languages with freer word order (Englishxe2x80x90German) and can provide additional improvements for fixed word order languages (Englishxe2x80x90"
Q16-1008,Adapting to All Domains at Once: Rewarding Domain Invariance in {SMT},2016,56,7,2,1,23615,hoang cuong,Transactions of the Association for Computational Linguistics,0,"Existing work on domain adaptation for statistical machine translation has consistently assumed access to a small sample from the test distribution (target domain) at training time. In practice, however, the target domain may not be known at training time or it may change to match user needs. In such situations, it is natural to push the system to make safer choices, giving higher preference to domain-invariant translations, which work well across domains, over risky domain-specific alternatives. We encode this intuition by (1) inducing latent subdomains from the training data only; (2) introducing features which measure how specialized phrases are to individual induced sub-domains; (3) estimating feature weights on out-of-domain data (rather than on the target domain). We conduct experiments on three language pairs and a number of different domains. We observe consistent improvements over a baseline which does not explicitly reward domain invariance."
P16-2028,Word Alignment without {NULL} Words,2016,14,3,3,0,1786,philip schulz,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In word alignment certain source words are only needed for fluency reasons and do not have a translation on the target side. Most word alignment models assume a target NULL word from which they generate these untranslatable source words. Hypothesising a target NULL word is not without problems, however. For example, because this NULL word has a position, it interferes with the distribution over alignment jumps. We present a word alignment model that accounts for untranslatable source words by generating them from preceding source words. It thereby removes the need for a target NULL word and only models alignments between word pairs that are actually observed in the data. Translation experiments on English paired with Czech, German, French and Japanese show that the model outperforms its traditional IBM counterparts in terms of BLEU score."
C16-1204,Hierarchical Permutation Complexity for Word Order Evaluation,2016,22,0,2,1,10251,milovs stanojevic,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Existing approaches for evaluating word order in machine translation work with metrics computed directly over a permutation of word positions in system output relative to a reference translation. However, every permutation factorizes into a permutation tree (PET) built of primal permutations, i.e., atomic units that do not factorize any further. In this paper we explore the idea that permutations factorizing into (on average) shorter primal permutations should represent simpler ordering as well. Consequently, we contribute Permutation Complexity, a class of metrics over PETs and their extension to forests, and define tight metrics, a sub-class of metrics implementing this idea. Subsequently we define example tight metrics and empirically test them in word order evaluation. Experiments on the WMT13 data sets for ten language pairs show that a tight metric is more often than not better than the baselines."
C16-1298,Universal Reordering via Linguistic Typology,2016,21,2,3,1,33919,joachim daiber,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In this paper we explore the novel idea of building a single universal reordering model from English to a large number of target languages. To build this model we exploit typological features of word order for a large number of target languages together with source (English) syntactic features and we train this model on a single combined parallel corpus representing all (22) involved language pairs. We contribute experimental evidence for the usefulness of linguistically defined typological features for building such a model. When the universal reordering model is used for preordering followed by monotone translation (no reordering inside the decoder), our experiments show that this pipeline gives comparable or improved translation performance with a phrase-based baseline for a large number of language pairs (12 out of 22) from diverse language families."
W15-5701,Modelling the Adjunct/Argument Distinction in Hierarchical Phrase-Based {SMT},2015,-1,-1,2,1,5467,sophie arnoult,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-5704,Delimiting Morphosyntactic Search Space with Source-Side Reordering Models,2015,-1,-1,2,1,33919,joachim daiber,Proceedings of the 1st Deep Machine Translation Workshop,0,None
W15-3050,{BEER} 1.1: {ILLC} {U}v{A} submission to metrics and tuning task,2015,13,8,2,1,10251,milovs stanojevic,Proceedings of the Tenth Workshop on Statistical Machine Translation,0,"We describe the submissions of ILLC UvA to the metrics and tuning tasks on WMT15. Both submissions are based on the BEER evaluation metric originally presented on WMT14 (Stanojevic and Simaxe2x80x99an, 2014a). The main changes introduced this year are: (i) extending the learning-to-rank trained sentence level metric to the corpus level (but still decomposable to sentence level), (ii) incorporating syntactic ingredients based on dependency trees, and (iii) a technique for finding parameters of BEER that avoid xe2x80x9cgaming of the metricxe2x80x9d during tuning."
N15-1043,Latent Domain Word Alignment for Heterogeneous Corpora,2015,41,7,2,1,23615,hoang cuong,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"This work focuses on the insensitivity of existing word alignment models to domain differences, which often yields suboptimal results on large heterogeneous data. A novel latent domain word alignment model is proposed, which induces domain-conditioned lexical and alignment statistics. We propose to train the model on a heterogeneous corpus under partial supervision, using a small number of seed samples from different domains. The seed samples allow estimating sharper, domain-conditioned word alignment statistics for sentence pairs. Our experiments show that the derived domain-conditioned statistics, once combined together, produce notable improvements both in word alignment accuracy and in translation accuracy of their resulting SMT systems."
D15-1005,Reordering Grammar Induction,2015,40,4,2,1,10251,milovs stanojevic,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We present a novel approach for unsupervised induction of a Reordering Grammar using a modified form of permutation trees (Zhang and Gildea, 2007), which we apply to preordering in phrase-based machine translation. Unlike previous approaches, we induce in one step both the hierarchical structure and the transduction function over it from word-aligned parallel corpora. Furthermore, our model (1) handles non-ITG reordering patterns (up to 5-ary branching), (2) is learned from all derivations by treating not only labeling but also bracketing as latent variable, (3) is entirely unlexicalized at the level of reordering rules, and (4) requires no linguistic annotation. Our model is evaluated both for accuracy in predicting target order, and for its impact on translation quality. We report significant performance gains over phrase reordering, and over two known preordering baselines for English-Japanese."
2015.tc-1.3,The {EXPERT} project: Advancing the state of the art in hybrid translation technologies,2015,8,0,8,0,12551,constantin orasan,Proceedings of Translating and the Computer 37,0,None
2015.mtsummit-papers.22,Machine translation with source-predicted target morphology,2015,0,2,2,1,33919,joachim daiber,Proceedings of Machine Translation Summit XV: Papers,0,"We propose a novel pipeline for translation into morphologically rich languages which consists of two steps: initially, the source string is enriched with target morphological features and then fed into a translation model which takes care of reordering and lexical choice that matches the provided morphological features. As a proof of concept we xefxacx81rst show improved translation performance for a phrase-based model translating source strings enriched with morphological features projected through the word alignments from target words to source words. Given this potential, we present a model for predicting target morphological features on the source string and its predicate-argument structure, and tackle two major technical challenges: (1) How to xefxacx81t the morphological feature set to training data? and (2) How to integrate the morphology into the back-end phrase-based model such that it can also be trained on projected (rather than predicted) features for a more efxefxacx81cient pipeline? For the xefxacx81rst challenge we present a latent variable model, and show that it learns a feature set with quality comparable to a manually selected set for German. And for the second challenge we present results showing that it is possible to bridge the gap between a model trained on a predicted and another model trained on a projected morphologically enriched parallel corpus. Finally we exhibit xefxacx81nal translation results showing promising improvement over the baseline phrase-based system."
W14-4002,Bilingual {M}arkov Reordering Labels for Hierarchical {SMT},2014,36,5,2,1,15437,gideon wenniger,"Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Earlier work on labeling Hiero grammars with monolingual syntax reports improved performance, suggesting that such labeling may impact phrase reordering as well as lexical selection. In this paper we explore the idea of inducing bilingual labels for Hiero grammars without using any additional resources other than original Hiero itself does. Our bilingual labels aim at capturing salient patterns of phrase reordering in the training parallel corpus. These bilingual labels originate from hierarchical factorizations of the word alignments in Hieroxe2x80x99s own training data. In this paper we take a Markovian view on synchronous top-down derivations over these factorizations which allows us to extract 0th- and 1st-order bilingual reordering labels. Using exactly the same training data as Hiero we show that the Markovian interpretation of word alignment factorization offers major benefits over the unlabeled version. We report extensive experiments with strict and soft bilingual labeled Hiero showing improved performance up to 1 BLEU points for Chinese-English and about 0.1 BLEU points for German-English."
W14-4017,Evaluating Word Order Recursively over Permutation-Forests,2014,19,2,2,1,10251,milovs stanojevic,"Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Automatically evaluating word order of MT system output at the sentence-level is challenging. At the sentence-level, ngram counts are rather sparse which makes it difficult to measure word order quality effectively using lexicalized units. Recent approaches abstract away from lexicalization by assigning a score to the permutation representing how word positions in system output move around relative to a reference translation. Metrics over permutations exist (e.g., Kendal tau or Spearman Rho) and have been shown to be useful in earlier work. However, none of the existing metrics over permutations groups word positions recursively into larger phrase-like blocks, which makes it difficult to account for long-distance reordering phenomena. In this paper we explore novel metrics computed over Permutation Forests (PEFs), packed charts of Permutation Trees (PETs), which are tree decompositions of a permutation into primitive ordering units. We empirically compare PEFs metric against five known reordering metrics on WMT13 data for ten language pairs. The PEFs metric shows better correlation with human ranking than the other metrics almost on all language pairs. None of the other metrics exhibits as stable behavior across language pairs."
W14-4019,How Synchronous are Adjuncts in Translation Data?,2014,19,0,2,1,5467,sophie arnoult,"Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"The argument-adjunct distinction is central to most syntactic and semantic theories. As optional elements that refine (the meaning of) a phrase, adjuncts are important for recursive, compositional accounts of syntax, semantics and translation. In formal accounts of machine translation, adjuncts are often treated as modifiers applying synchronously in source and target derivations. But how well can the assumption of synchronous adjunction explain translation equivalence in actual parallel data? In this paper we present the first empirical study of translation equivalence of adjuncts on a variety of FrenchEnglish parallel corpora, while varying word alignments so we can gauge the effect of errors in them. We show that for proper measurement of the types of translation equivalence of adjuncts, we must work with non-contiguous, many-to-many relations, thereby amending the traditional Direct Correspondence Assumption. Our empirical results show that 70% of manually identified adjuncts have adjunct translation equivalents in training data, against roughly 50% for automatically identified adjuncts."
W14-3354,{BEER}: {BE}tter Evaluation as Ranking,2014,15,41,2,1,10251,milovs stanojevic,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"We present the UvA-ILLC submission of the BEER metric to WMT 14 metrics task. BEER is a sentence level metric that can incorporate a large number of features combined in a linear model. Novel contributions are (1) efficient tuning of a large number of features for maximizing correlation with human system ranking, and (2) novel features that give smoother sentence level scores."
bastings-simaan-2014-fragments,All Fragments Count in Parser Evaluation,2014,15,1,2,0.952381,10758,jasmijn bastings,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"PARSEVAL, the default paradigm for evaluating constituency parsers, calculates parsing success (Precision/Recall) as a function of the number of matching labeled brackets across the test set. Nodes in constituency trees, however, are connected together to reflect important linguistic relations such as predicate-argument and direct-dominance relations between categories. In this paper, we present FREVAL, a generalization of PARSEVAL, where the precision and recall are calculated not only for individual brackets, but also for co-occurring, connected brackets (i.e. fragments). FREVAL fragments precision (FLP) and recall (FLR) interpolate the match across the whole spectrum of fragment sizes ranging from those consisting of individual nodes (labeled brackets) to those consisting of full parse trees. We provide evidence that FREVAL is informative for inspecting relative parser performance by comparing a range of existing parsers."
D14-1025,Fitting Sentence Level Translation Evaluation with Many Dense Features,2014,17,22,2,1,10251,milovs stanojevic,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Sentence level evaluation in MT has turned out far more difficult than corpus level evaluation. Existing sentence level metrics employ a limited set of features, most of which are rather sparse at the sentence level, and their intricate models are rarely trained for ranking. This paper presents a simple linear model exploiting 33 relatively dense features, some of which are novel while others are known but seldom used, and train it under the learning-to-rank framework. We evaluate our metric on the standard WMT12 data showing that it outperforms the strong baseline METEOR. We also analyze the contribution of individual features and the choice of training data, language-pair vs. target-language data, providing new insights into this task."
D14-1062,Latent Domain Phrase-based Models for Adaptation,2014,31,9,2,1,23615,hoang cuong,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Phrase-based models directly trained on mix-of-domain corpora can be sub-optimal. In this paper we equip phrase-based models with a latent domain variable and present a novel method for adapting them to an in-domain task represented by a seed corpus. We derive an EM algorithm which alternates between inducing domain-focused phrase pair estimates, and weights for mix-domain sentence pairs reflecting their relevance for the in-domain task. By embedding our latent domain phrase model in a sentence-level model and training the two in tandem, we are able to adapt all core translation components together xe2x80x90 phrase, lexical and reordering. We show experiments on weighing sentence pairs for relevance as well as adapting phrase-based models, showing significant performance improvement in both tasks."
C14-1182,Latent Domain Translation Models in Mix-of-Domains Haystack,2014,30,23,2,1,23615,hoang cuong,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"This paper addresses the problem of selecting adequate training sentence pairs from a mix-ofdomains parallel corpus for a translation task represented by a small in-domain parallel corpus. We propose a novel latent domain translation model which includes domain priors, domaindependent translation models and language models. The goal of learning is to estimate the probability of a sentence pair in mix-domain corpus to be in- or out-domain using in-domain corpus statistics as prior. We derive an EM training algorithm and provide solutions for estimating out-domain models (given only in- and mix-domain data). We report on experiments in data selection (intrinsic) and machine translation (extrinsic) on a large parallel corpus consisting of a mix of a rather diverse set of domains. Our results show that our latent domain invitation approach outperforms the existing baselines significantly. We also provide analysis of the merits of our approach relative to existing approaches. Large parallel corpora are important for training statistical MT systems. Besides size, the relevance of a parallel training corpus to the translation task at hand can be decisive for system performance, cf. (Axelrod et al., 2011; Koehn and Haddow, 2012). In this paper we look at data selection where we have access to a large parallel data repositoryCmix, representing a rather varied mix of domains, and we are given a sample of in-domain parallel dataCin, exemplifying a target translation task. Simply concatenatingCin withCmix does not always deliver best performance, because including irrelevant sentences might be more harmful than beneficial, cf. (Axelrod et al., 2011). To make the best of available data, we must select sentences fromCmix for their relevance to translating sentences fromCin. Axelrod et al. (2011) and follow-up work, e.g., (Haddow and Koehn, 2012; Koehn and Haddow, 2012), select sentence pairs inCmix using the cross-entropy difference between in- and mix-domain language models, both source and target sides, a modification of the Moore and Lewis method (Moore and Lewis, 2010). In the translation context, however, often a source phrase has different senses/translations in different domains, which cannot be distinguished with monolingual language models. The dependence of translation choice on domain suggests that the word alignments themselves can better be conditioned on domain information. However, in the data selection setting, corpusCmix often does not contain useful domain markers, andCin contains only a small sample of in-domain sentence pairs. In this paper we present a latent domain translation model which weights every sentence pairhf,ei2 Cmix with a probabilityP (D| f,e) for being in-domain (D1) or out-domain (D0). Our model defines P (e,f) = P D2{D1,D0} P (D)P (e,f | D), using a latent domain variable D 2 {D0,D1}. Using bidirectional translation models, this leads to a domain priorP (D), domain-dependent translation models Pt(xc2xb7|xc2xb7,D) and language modelsPlm(xc2xb7|D) as in Equation 1:"
W13-0803,Hierarchical Alignment Decomposition Labels for {H}iero Grammar Rules,2013,26,3,2,1,15437,gideon wenniger,"Proceedings of the Seventh Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Selecting a set of nonterminals for the synchronous CFGs underlying the hierarchical phrase-based models is usually done on the basis of a monolingual resource (like a syntactic parser). However, a standard bilingual resource like word alignments is itself rich with reordering patterns that, if clustered somehow, might provide labels of different (possibly complementary) nature to monolingual labels. In this paper we explore a first version of this idea based on a hierarchical decomposition of word alignments into recursive tree representations. We identify five clusters of alignment patterns in which the children of a node in a decomposition tree are found and employ these five as nonterminal labels for the Hiero productions. Although this is our first non-optimized instantiation of the idea, our experiments show competitive performance with the Hiero baseline, exemplifying certain merits of this novel approach."
W13-0807,A Formal Characterization of Parsing Word Alignments by Synchronous Grammars with Empirical Evidence to the {ITG} Hypothesis.,2013,29,3,2,1,15437,gideon wenniger,"Proceedings of the Seventh Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"Deciding whether a synchronous grammar formalism generates a given word alignment (the alignment coverage problem) depends on finding an adequate instance grammar and then using it to parse the word alignment. But what does it mean to parse a word alignment by a synchronous grammar? This is formally undefined until we define an unambiguous mapping between grammatical derivations and word-level alignments. This paper proposes an initial, formal characterization of alignment coverage as intersecting two partially ordered sets (graphs) of translation equivalence units, one derived by a grammar instance and another defined by the word alignment. As a first sanity check, we report extensive coverage results for ITG on automatic and manual alignments. Even for the ITG formalism, our formal characterization makes explicit many algorithmic choices often left underspecified in earlier work."
2012.eamt-1.63,Adjunct Alignment in Translation Data with an Application to Phrase Based Statistical Machine Translation,2012,13,2,2,1,5467,sophie arnoult,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,None
W11-2911,Learning Structural Dependencies of Words in the {Z}ipfian Tail,2011,41,3,3,1,5203,tejaswini deoskar,Proceedings of the 12th International Conference on Parsing Technologies,0,"Using semi-supervised EM, we learn finegrained but sparse lexical parameters of a generative parsing model (a PCFG) initially estimated over the Penn Treebank. Our lexical parameters employ supertags, which encode complex structural information at the pre-terminal level, and are particularly sparse in labeled data -- our goal is to learn these for words that are unseen or rare in the labeled data. In order to guide estimation from unlabeled data, we incorporate both structural and lexical priors from the labeled data. We get a large error reduction in parsing ambiguous structures associated with unseen verbs, the most important case of learning lexico-structural dependencies. We also obtain a statistically significant improvement in labeled bracketing score of the treebank PCFG, the first successful improvement via semi-supervised EM of a generative structured model already trained over large labeled data."
W11-2150,{ILLC}-{U}v{A} translation system for {EMNLP}-{WMT} 2011,2011,25,1,2,1,17603,maxim khalilov,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"In this paper we describe the Institute for Logic, Language and Computation (University of Amsterdam) phrase-based statistical machine translation system for English-to-German translation proposed within the EMNLP-WMT 2011 shared task. The main novelty of the submitted system is a syntax-driven pre-translation reordering algorithm implemented as source string permutation via transfer of the source-side syntax tree."
P11-1065,Learning Hierarchical Translation Structure with Linguistic Annotations,2011,33,26,2,1,44134,markos mylonakis,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"While it is generally accepted that many translation phenomena are correlated with linguistic structures, employing linguistic syntax for translation has proven a highly non-trivial task. The key assumption behind many approaches is that translation is guided by the source and/or target language parse, employing rules extracted from the parse tree or performing tree transformations. These approaches enforce strict constraints and might overlook important translation phenomena that cross linguistic constituents. We propose a novel flexible modelling approach to introduce linguistic information of varying granularity from the source side. Our method induces joint probability synchronous grammars and estimates their parameters, by selecting and weighing together linguistically motivated rules according to an objective function directly targeting generalisation over future data. We obtain statistically significant improvements across 4 different language pairs with English as source, mounting up to 1.92 BLEU for Chinese as target."
I11-1005,Context-Sensitive Syntactic Source-Reordering by Statistical Transduction,2011,27,8,2,1,17603,maxim khalilov,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"How well can a phrase translation model perform if we permute the source words to fit target word order as perfectly as word alignment might allow? And how well would it perform if we limit the allowed permutations to ITGlike tree-transduction operations on the source parse tree? First we contribute oracle results showing great potential for performance improvement by source-reordering, ranging from 1.5 to 4 BLEU points depending on language pair. Although less outspoken, the potential of tree-based source-reordering is also significant. Our second contribution is a source reordering model that works with two kinds of tree transductions: the one permutes the order of sibling subtrees under a node, and the other first deletes layers in the parse tree in order to exploit sibling permutation at the remaining levels.The statistical parameters of the model we introduce concern individual tree transductions conditioned on contextual features of the tree resulting from all preceding transductions. Experiments in translating from English to Spanish/Dutch/Chinese show significant improvements of respectively 0.6/1.2/2.0 BLEU points."
W10-3812,A Discriminative Syntactic Model for Source Permutation via Tree Transduction,2010,27,7,2,1,17603,maxim khalilov,Proceedings of the 4th Workshop on Syntax and Structure in Statistical Translation,0,"A major challenge in statistical machine translation is mitigating the word order differences between source and target strings. While reordering and lexical translation choices are often conducted in tandem, source string permutation prior to translation is attractive for studying reordering using hierarchical and syntactic structure. This work contributes an approach for learning source string permutation via transfer of the source syntax tree. We present a novel discriminative, probabilistic tree transduction model, and contribute a set of empirical upperbounds on translation performance for English-to-Dutch source string permutation under sequence and parse tree constraints. Finally, the translation performance of our learning model is shown to outperform the state-of-the-art phrase-based system significantly."
W10-2915,Learning Probabilistic Synchronous {CFG}s for Phrase-Based Translation,2010,24,11,2,1,44134,markos mylonakis,Proceedings of the Fourteenth Conference on Computational Natural Language Learning,0,"Probabilistic phrase-based synchronous grammars are now considered promising devices for statistical machine translation because they can express reordering phenomena between pairs of languages. Learning these hierarchical, probabilistic devices from parallel corpora constitutes a major challenge, because of multiple latent model variables as well as the risk of data overfitting. This paper presents an effective method for learning a family of particular interest to MT, binary Synchronous Context-Free Grammars with inverted/monotone orientation (a.k.a. Binary ITG). A second contribution concerns devising a lexicalized phrase reordering mechanism that has complimentary strengths to Chiang's model. The latter conditions reordering decisions on the surrounding lexical context of phrases, whereas our mechanism works with the lexical content of phrase pairs (akin to standard phrase-based systems). Surprisingly, our experiments on French-English data show that our learning method applied to far simpler models exhibits performance indistinguishable from the Hiero system."
W10-1405,Modeling Morphosyntactic Agreement in Constituency-Based Parsing of {M}odern {H}ebrew,2010,18,9,2,0.952381,5249,reut tsarfaty,Proceedings of the {NAACL} {HLT} 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages,0,"We show that naive modeling of morphosyn-tactic agreement in a Constituency-Based (CB) statistical parsing model is worse than none, whereas a linguistically adequate way of modeling inflectional morphology in CB parsing leads to improved performance. In particular, we show that an extension of the Relational-Realizational (RR) model that incorporates agreement features is superior to CB models that treat morphosyntax as state-splits (SP), and that the RR model benefits more from inflectional features. We focus on parsing Hebrew and report the best result to date, F184.13 for parsing off of gold-tagged text, 5% error reduction from previous results."
2010.iwslt-evaluation.27,{ILLC}-{U}v{A} machine translation system for the {IWSLT} 2010 evaluation,2010,0,5,2,1,17603,maxim khalilov,Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign,0,None
2010.eamt-1.28,Source reordering using {M}ax{E}nt classifiers and supertags,2010,23,5,2,1,17603,maxim khalilov,Proceedings of the 14th Annual conference of the European Association for Machine Translation,0,"Source language reordering can be seen as the preprocessing task of permuting the order of the source words in such a way that the resulting permutation allows as monotone a translation process as possible. We explore a simple but effective source reordering algorithm that works as a cascade of source string transforms, each consisting of swapping the positions of a single pair of adjacent words in order to unfold a candidate pair of crossing alignments. The decision to swap a pair of words is modelled as a binary classification task formulated as a log-linear model and trained under maximum entropy (MaxEnt). We experiment with features that consist of the local neighborhood of both words as well as lexico-syntactic representations known as supertags. Our experiments on the English-to-Dutch EuroParl translation task show that the cascaded alignment unfolding slightly improves the performance of a state-of-the-art phrase translation system that uses distance-based and lexicalized block-oriented reordering."
W09-3833,Smoothing fine-grained {PCFG} lexicons,2009,12,1,3,1,5203,tejaswini deoskar,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"We present an approach for smoothing treebank-PCFG lexicons by interpolating treebank lexical parameter estimates with estimates obtained from unannotated data via the Inside-outside algorithm. The PCFG has complex lexical categories, making relative-frequency estimates from a treebank very sparse. This kind of smoothing for complex lexical categories results in improved parsing performance, with a particular advantage in identifying obligatory arguments subcategorized by verbs unseen in the treebank."
R09-1025,Lexicalized Semi-incremental Dependency Parsing,2009,26,7,2,1,6587,hany hassan,Proceedings of the International Conference {RANLP}-2009,0,"Even leaving aside concerns of cognitive plausibility, incremental parsing is appealing for applications such as speech recognition and machine translation because it could allow for incorporating syntactic features into the decoding process without blowing up the search space. Yet, incremental parsing is often associated with greedy parsing decisions and intolerable loss of accuracy. Would the use of lexicalized grammars provide a new perspective on incremental parsing? In this paper we explore incremental left-to-right dependency parsing using a lexicalized grammatical formalism that works with lexical categories (supertags) and a small set of combinatory operators. A strictly incremental parser would conduct only a single pass over the input, use no lookahead and make only local decisions at every word. We show that such a parser suffers heavy loss of accuracy. Instead, we explore the utility of a two-pass approach that incrementally builds a dependency structure by first assigning a supertag to every input word and then selecting an incremental operator that allows assembling every supertag with the dependency structure built so-far to its left. We instantiate this idea in different models that allow a trade-off between aspects of full incrementality and performance, and explore the differences between these models empirically. Our exploration shows that a semi-incremental (two-pass), linear-time parser that employs fixed and limited look-ahead exhibits an appealing balance between the efficiency advantages of incrementality and the achieved accuracy. Surprisingly, taking local or global decisions matters very little for the accuracy of this linear-time parser. Such a parser fits seemlessly with the currently dominant finite-state decoders for machine translation."
D09-1088,An Alternative to Head-Driven Approaches for Parsing a (Relatively) Free Word-Order Language,2009,30,6,2,1,5249,reut tsarfaty,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,Applying statistical parsers developed for English to languages with freer word-order has turned out to be harder than expected. This paper investigates the adequacy of different statistical parsing models for dealing with a (relatively) free word-order language. We show that the recently proposed Relational-Realizational (RR) model consistently outperforms state-of-the-art Head-Driven (HD) models on the Hebrew Treebank. Our analysis reveals a weakness of HD models: their intrinsic focus on configurational information. We conclude that the form-function separation ingrained in RR models makes them better suited for parsing nonconfigurational phenomena.
D09-1123,A Syntactified Direct Translation Model with Linear-time Decoding,2009,23,14,2,1,6587,hany hassan,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Recent syntactic extensions of statistical translation models work with a synchronous context-free or tree-substitution grammar extracted from an automatically parsed parallel corpus. The decoders accompanying these extensions typically exceed quadratic time complexity.n n This paper extends the Direct Translation Model 2 (DTM2) with syntax while maintaining linear-time decoding. We employ a linear-time parsing algorithm based on an eager, incremental interpretation of Combinatory Categorial Grammar (CCG). As every input word is processed, the local parsing decisions resolve ambiguity eagerly, by selecting a single supertag-operator pair for extending the dependency parse incrementally. Alongside translation features extracted from the derived parse tree, we explore syntactic features extracted from the incremental derivation process. Our empirical experiments show that our model significantly outperforms the state-of-the art DTM2 system."
plank-simaan-2008-subdomain,Subdomain Sensitive Statistical Parsing using Raw Corpora,2008,13,6,2,0,106,barbara plank,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Modern statistical parsers are trained on large annotated corpora (treebanks). These treebanks usually consist of sentences addressing different subdomains (e.g. sports, politics, music), which implies that the statistics gathered by current statistical parsers are mixtures of subdomains of language use. In this paper we present a method that exploits raw subdomain corpora gathered from the web to introduce subdomain sensitivity into a given parser. We employ statistical techniques for creating an ensemble of domain sensitive parsers, and explore methods for amalgamating their predictions. Our experiments show that introducing domain sensitivity by exploiting raw corpora can improve over a tough, state-of-the-art baseline."
D08-1066,Phrase Translation Probabilities with {ITG} Priors and Smoothing as Learning Objective,2008,29,14,2,1,44134,markos mylonakis,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"The conditional phrase translation probabilities constitute the principal components of phrase-based machine translation systems. These probabilities are estimated using a heuristic method that does not seem to optimize any reasonable objective function of the word-aligned, parallel training corpus. Earlier efforts on devising a better understood estimator either do not scale to reasonably sized training data, or lead to deteriorating performance. In this paper we explore a new approach based on three ingredients (1) A generative model with a prior over latent segmentations derived from Inversion Transduction Grammar (ITG), (2) A phrase table containing all phrase pairs without length limit, and (3) Smoothing as learning objective using a novel Maximum-A-Posteriori version of Deleted Estimation working with Expectation-Maximization. Where others conclude that latent segmentations lead to overfitting and deteriorating performance, we show here that these three ingredients give performance equivalent to the heuristic method on reasonably sized training data."
C08-1112,Relational-Realizational Parsing,2008,22,35,2,1,5249,reut tsarfaty,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"State-of-the-art statistical parsing models applied to free word-order languages tend to underperform compared to, e.g., parsing English. Constituency-based models often fail to capture generalizations that cannot be stated in structural terms, and dependency-based models employ a 'single-head' assumption that often breaks in the face of multiple exponence. In this paper we suggest that the position of a constituent is a form manifestation of its grammatical function, one among various possible means of realization. We develop the Relational-Realizational approach to parsing in which we untangle the projection of grammatical functions and their means of realization to allow for phrase-structure variability and morphological-syntactic interaction. We empirically demonstrate the application of our approach to parsing Modern Hebrew, obtaining 7% error reduction from previously reported results."
W07-2219,Three-Dimensional Parametrization for Parsing Morphologically Rich Languages,2007,26,14,2,1,5249,reut tsarfaty,Proceedings of the Tenth International Conference on Parsing Technologies,0,"Current parameters of accurate unlexicalized parsers based on Probabilistic Context-Free Grammars (PCFGs) form a two-dimensional grid in which rewrite events are conditioned on both horizontal (head-outward) and vertical (parental) histories. In Semitic languages, where arguments may move around rather freely and phrase-structures are often shallow, there are additional morphological factors that govern the generation process. Here we propose that agreement features percolated up the parse-tree form a third dimension of parametrization that is orthogonal to the previous two. This dimension differs from mere state-splits as it applies to a whole set of categories rather than to individual ones and encodes linguistically motivated co-occurrences between them. This paper presents extensive experiments with extensions of unlexicalized PCFGs for parsing Modern Hebrew in which tuning the parameters in three dimensions gradually leads to improved performance. Our best result introduces a new, stronger, lower bound on the performance of treebank grammars for parsing Modern Hebrew, and is on a par with current results for parsing Modern Standard Arabic obtained by a fully lexicalized parser trained on a much larger treebank."
W07-0813,Smoothing a Lexicon-based {POS} Tagger for {A}rabic and {H}ebrew,2007,10,20,2,0,49039,saib manour,Proceedings of the 2007 Workshop on Computational Approaches to {S}emitic Languages: Common Issues and Resources,0,"We propose an enhanced Part-of-Speech (POS) tagger of Semitic languages that treats Modern Standard Arabic (henceforth Arabic) and Modern Hebrew (henceforth Hebrew) using the same probabilistic model and architectural setting. We start out by porting an existing Hidden Markov Model POS tagger for Hebrew to Arabic by exchanging a morphological analyzer for Hebrew with Buckwalter's (2002) morphological analyzer for Arabic. This gives state-of-the-art accuracy (96.12%), comparable to Habash and Rambow's (2005) analyzer-based POS tagger on the same Arabic datasets. However, further improvement of such analyzer-based tagging methods is hindered by the incomplete coverage of standard morphological analyzer (Bar Haim et al., 2005). To overcome this coverage problem we supplement the output of Buckwalter's analyzer with synthetically constructed analyses that are proposed by a model which uses character information (Diab et al., 2004) in a way that is similar to Nakagawa's (2004) system for Chinese and Japanese. A version of this extended model that (unlike Nakagawa) incorporates synthetically constructed analyses also for known words achieves 96.28% accuracy on the standard Arabic test set."
P07-1037,Supertagged Phrase-Based Statistical Machine Translation,2007,15,61,2,1,6587,hany hassan,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Until quite recently, extending Phrase-based Statistical Machine Translation (PBSMT) with syntactic structure caused system performance to deteriorate. In this work we show that incorporating lexical syntactic descriptions in the form of supertags can yield significantly better PBSMT systems. We describe a novel PBSMT model that integrates supertags into the target language model and the target side of the translation model. Two kinds of supertags are employed: those from Lexicalized Tree-Adjoining Grammar and Combinatory Categorial Grammar. Despite the differences between these two approaches, the supertaggers give similar improvements. In addition to supertagging, we also explore the utility of a surface global grammaticality measure based on combinatory operators. We perform various experiments on the Arabic to English NIST 2005 test set addressing issues such as sparseness, scalability and the utility of system subcomponents. Our best result (0.4688 BLEU) improves by 6.1% relative to a state-of-theart PBSMT model, which compares very favourably with the leading systems on the NIST 2005 task."
2006.amta-papers.9,Corpus Variations for Translation Lexicon Induction,2006,17,2,3,0,238,rebecca hwa,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Lexical mappings (word translations) between languages are an invaluable resource for multilingual processing. While the problem of extracting lexical mappings from parallel corpora is well-studied, the task is more challenging when the language samples are from non-parallel corpora. The goal of this work is to investigate one such scenario: finding lexical mappings between dialects of a diglossic language, in which people conduct their written communications in a prestigious formal dialect, but they communicate verbally in a colloquial dialect. Because the two dialects serve different socio-linguistic functions, parallel corpora do not naturally exist between them. An example of a diglossic dialect pair is Modern Standard Arabic (MSA) and Levantine Arabic. In this paper, we evaluate the applicability of a standard algorithm for inducing lexical mappings between comparable corpora (Rapp, 1999) to such diglossic corpora pairs. The focus of the paper is an in-depth error analysis, exploring the notion of relatedness in diglossic corpora and scrutinizing the effects of various dimensions of relatedness (such as mode, topic, style, and statistics) on the quality of the resulting translation lexicon."
W05-0706,Choosing an Optimal Architecture for Segmentation and {POS}-Tagging of {M}odern {H}ebrew,2005,11,33,2,0,10390,roy barhaim,Proceedings of the {ACL} Workshop on Computational Approaches to {S}emitic Languages,0,"A major architectural decision in designing a disambiguation model for segmentation and Part-of-Speech (POS) tagging in Semitic languages concerns the choice of the input-output terminal symbols over which the probability distributions are defined. In this paper we develop a segmenter and a tagger for Hebrew based on Hidden Markov Models (HMMs). We start out from a morphological analyzer and a very small morphologically annotated corpus. We show that a model whose terminal symbols are word segments (=morphemes), is advantageous over a word-level model for the task of POS tagging. However, for segmentation alone, the morpheme-level model has no significant advantage over the word-level model. Error analysis shows that both models are not adequate for resolving a common type of segmentation ambiguity in Hebrew -- whether or not a word in a written text is prefixed by a definiteness marker. Hence, we propose a morpheme-level model where the definiteness morpheme is treated as a possible feature of morpheme terminals. This model exhibits the best overall performance, both in POS tagging and in segmentation. Despite the small size of the annotated corpus available for Hebrew, the results achieved using our best model are on par with recent results on Modern Standard Arabic."
W04-0505,{B}io{G}rapher: Biography Questions as a Restricted Domain Question Answering Task,2004,12,12,3,0,10723,oren tsur,Proceedings of the Conference on Question Answering in Restricted Domains,0,"We address Question Answering (QA) for biographical questions, i.e., questions asking for biographical facts about persons. The domain of biographical documents differs from other restricted domains in that the available collections of biographies are inherently incomplete: a major challenge is to answer questions about persons for whom biographical information is not present in biography collections. We present BioGrapher, a biographical QA system that addresses this problem by machine learning algorithms for biography classification. BioGrapher first attempts to answer a question by searching in a given collection of biographies, using techniques tailored for the restricted nature of the domain. If a biography is not found, BioGrapher attempts to find an answer on the web: it retrieves documents using a web search engine, filters these using the biography classifier, and then extracts answers from documents classified as biographies. Our empirical results show that biographical classification, prior to answer extraction, improves the results."
W03-3021,On maximizing metrics for syntactic disambiguation,2003,11,7,1,1,29433,khalil simaan,Proceedings of the Eighth International Conference on Parsing Technologies,0,"Given a probabilistic parsing model and an evaluation metric for scoring the match between parse-trees, e.g., PARSEVAL [Black et al., 1991], this paper addresses the problem of how to select the on average best scoring parse-tree for an input sentence. Common wisdom dictates that it is optimal to select the parse with the highest probability, regardless of the evaluation metric. In contrast, the Maximizing Metrics (MM) method [Goodman, 1998, Stolcke et al., 1997] proposes that an algorithm that optimizes the evaluation metric itself constitutes the optimal choice. We study the MM method within parsing. We observe that the MM does not always hold for tree-bank models, and that optimizing weak metrics is not interesting for semantic processing. Subsequently, we state an alternative proposition: the optimal algorithm must maximize the metric that scores parse-trees according to linguistically relevant features. We present new algorithms that optimize metrics that take into account increasingly more linguistic features, and exhibit experiments in support of our claim."
W01-1817,Robust Data Oriented Parsing of Speech Utterances,2001,0,0,1,1,29433,khalil simaan,Proceedings of the Seventh International Workshop on Parsing Technologies,0,None
C96-2215,Computational Complexity of Probabilistic Disambiguation by means of Tree-Grammars,1996,11,91,1,1,29433,khalil simaan,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"This paper studies the computational complexity of disambiguation under probabilistic tree-grammars as in (Bod, 1992; Schabes and Waters, 1993). It presents a proof that the following problems are NP-hard: computing the Most Probable Parse from a sentence or from a word-graph, and computing the Most Probable Sentence (MPS) from a word-graph. The NP-hardness of computing the MPS from a word-graph also holds for Stochastic Context-Free Grammars (SCFGs)."
