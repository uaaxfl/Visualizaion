2010.amta-papers.33,P08-1087,0,0.0943135,"., 2005; Chan et al., 2007). Although their integration efforts have shown promising results, none of these works have focused on the problem of translating from morphologically poor languages into morphologically rich languages. Since such languages generally have complicated rules for generating inflections, modeling a morphologically rich lexicon and disambiguating its senses is particularly challenging. In SMT in general, there has only been a limited amount of work applying morphological processing for translating from English to morphologically rich languages. Of those, Bojar (2007) and Avramidis and Koehn (2008) used morphological features in the factored representation of words implemented in the Moses system. However, this approach is difficult to apply when the syntax-based SMT framework is used, and it tends to focus on simple generative translation models that partition the input into chunks, along with target language models to help model fluency, rather than discriminative models for picking aspects of translation based on varying amounts of context. Other studies closely related to ours are the works on Japanese case-marker generation (Toutanova and Suzuki, 2007) and morphological inflection"
2010.amta-papers.33,P07-1020,0,0.0559774,"h errors introduced by re-ranking while leading to a faster overall pipeline. For the rest of the paper, we first give a review of the literature in Section 2. Section 3 describes our approach in more detail. We describe our experiments with several feature sets and language pairs, both with intrinsic evaluations of lexical selection and in end-to-end BLEU scores in Sections 4 and 5. 2 Related Work Much of the work on discriminative lexicon models has focused on target word selection, under the assumption that sentence-level global information is useful in finding good translation candidates. Bangalore et al. (2007) framed the problem of selecting translated words from a target lexicon as a binary classification task, where each word in the target vocabulary included in or excluded from the translation according to its binary decision. In a similar vein, Mauser et al. (2009) integrated discriminative and trigger-based lexicon models into phrase-based Chinese-English MT system, showing an improvement of the BLEU score. The goal of both of these papers on lexicon modeling was to propose appropriate translated words, looking beyond word- and phrase-based translation pairs by including source context informa"
2010.amta-papers.33,W07-0735,0,0.0760361,"05; Vickrey et al., 2005; Chan et al., 2007). Although their integration efforts have shown promising results, none of these works have focused on the problem of translating from morphologically poor languages into morphologically rich languages. Since such languages generally have complicated rules for generating inflections, modeling a morphologically rich lexicon and disambiguating its senses is particularly challenging. In SMT in general, there has only been a limited amount of work applying morphological processing for translating from English to morphologically rich languages. Of those, Bojar (2007) and Avramidis and Koehn (2008) used morphological features in the factored representation of words implemented in the Moses system. However, this approach is difficult to apply when the syntax-based SMT framework is used, and it tends to focus on simple generative translation models that partition the input into chunks, along with target language models to help model fluency, rather than discriminative models for picking aspects of translation based on varying amounts of context. Other studies closely related to ours are the works on Japanese case-marker generation (Toutanova and Suzuki, 2007"
2010.amta-papers.33,N07-1007,1,0.855715,"es. Of those, Bojar (2007) and Avramidis and Koehn (2008) used morphological features in the factored representation of words implemented in the Moses system. However, this approach is difficult to apply when the syntax-based SMT framework is used, and it tends to focus on simple generative translation models that partition the input into chunks, along with target language models to help model fluency, rather than discriminative models for picking aspects of translation based on varying amounts of context. Other studies closely related to ours are the works on Japanese case-marker generation (Toutanova and Suzuki, 2007) and morphological inflection prediction for Russian and Arabic (Toutanova et al., 2008). They built probabilistic models for morphology generation and applied them to rescore n-best outputs from an SMT engine, augmenting those outputs with additional inflectional variations. In contrast, we focus on direct integration of our discriminative lexicon model with a dependency tree-to-string translation system, in which syntatic features obtained from an English parser are naturally incorporated in the first-pass SMT decoder. 3 Discriminative Lexicon Model This section describes our discriminative"
2010.amta-papers.33,P05-1048,0,0.0278346,"ion pairs by including source context information as bag-of-words features and triggerbased models. The same objective has motivated work on word sense disambiguation (WSD) for machine translation, in which different senses of a word in the source language are defined as its possible translations in the target language (Berger et al., 1996). Again the correct sense, and therefore the correct translation, depends on the specific meaning of the word in context. Recently there has been quite a bit of research on integrating discriminatively trained WSD systems into SMT (Cabezas and Resnik, 2005; Carpuat and Wu, 2005; Vickrey et al., 2005; Chan et al., 2007). Although their integration efforts have shown promising results, none of these works have focused on the problem of translating from morphologically poor languages into morphologically rich languages. Since such languages generally have complicated rules for generating inflections, modeling a morphologically rich lexicon and disambiguating its senses is particularly challenging. In SMT in general, there has only been a limited amount of work applying morphological processing for translating from English to morphologically rich languages. Of those, Bo"
2010.amta-papers.33,J07-2003,0,0.0206916,"anslation pairs and order templates can be combined to form the following transduction rules: (run/Verb (x1: /Noun)) ((x1) 실행하시오) ((x1: /Det) query/Noun) ((x1) 쿼리를) (this) (이) Given rules of this form, finding the best translation is a matter of searching for the best derivation according to a sentence specific grammar. In the absence of a language model or other models that score based on context, we may simply use a standard parsing algorithm such as CKY to find the best derivation. In the presence of context sensitive features, we resort to the approximate search technique of cube pruning (Chiang, 2007). The baseline treelet system scores candidates with a weighted linear combination of features, with weights trained by Minimum Error Rate Training (Och, 2003), hereafter referred to as MERT. The baseline set of feature functions are: log probabilities of the source treelet given the target treelet and vice versa (maximum likelihood estimates); forward and backward lexical weighting; a target language log probability from a Kneser-Ney smoothed language model; word and phrase count feature functions, and order template log probabilities (maximum likelihood estimates). For all features except th"
2010.amta-papers.33,P07-1005,0,0.0389986,"ation as bag-of-words features and triggerbased models. The same objective has motivated work on word sense disambiguation (WSD) for machine translation, in which different senses of a word in the source language are defined as its possible translations in the target language (Berger et al., 1996). Again the correct sense, and therefore the correct translation, depends on the specific meaning of the word in context. Recently there has been quite a bit of research on integrating discriminatively trained WSD systems into SMT (Cabezas and Resnik, 2005; Carpuat and Wu, 2005; Vickrey et al., 2005; Chan et al., 2007). Although their integration efforts have shown promising results, none of these works have focused on the problem of translating from morphologically poor languages into morphologically rich languages. Since such languages generally have complicated rules for generating inflections, modeling a morphologically rich lexicon and disambiguating its senses is particularly challenging. In SMT in general, there has only been a limited amount of work applying morphological processing for translating from English to morphologically rich languages. Of those, Bojar (2007) and Avramidis and Koehn (2008)"
2010.amta-papers.33,erjavec-2004-multext,0,0.0185888,"tWord GramFeat(AlignedWord)+GramFeat(Word-1) +GramFeat(Word+1)+GramFeat(Parent)+GramFeat(Grand parent) & GramFeat(Target) GramFeat(AlignedWord)+GramFeat(Word-1) +GramFeat(Word+1)+GramFeat(Parent)+GramFeat(Grand parent)+AlignedWord & GramFeat(Target) Lemma(AlignedWord) & Lemma(Target) Lemma(Word-1) & Lemma(Target) Lemma(Word+1) & Lemma(Target) Table 1:Features for discriminative lexicon model lemma. The function GramFeat(w) in Table 1 denotes a conjunction of all values of grammatical features as well as the POS tag. For Bulgarian, we created the lexicon from the Multext-East (Version 3) data (Erjavec, 2004), which contains about 41K distinct surface word forms and 23K lemmas. Our Czech lexicon was created using the training and development section of the CzEng corpus data (Bojar and Žabokrtský, 2009), resulting in about 800K distinct word forms and 431K lemmas. For English, we used the lexicon of the dependency Bulgarian Czech Korean Train Dev Test 345K (4.4M) 138K (2.5M) 2.8M (24M) 2K (51K) 2K (39K) 2K (15K) 3K (72K) 3K (60K) 3K (25K) Random/ Oracle Acc@1 5.8 / 96.2 7.8 / 90.5 6.0 / 95.7 Table 2: Parallel data for discriminative lexicon training statistics: # sentence pairs (extracted example s"
2010.amta-papers.33,P07-1104,1,0.852498,"r ranking models is scalability. Training with a large amount of parallel data is the norm in modern SMT systems; therefore, our model needs to accommodate a large set of training examples and features. To do this, we used an online learning approach, namely stochastic gradient descent (SGD) with L1-regularization. In this learning procedure, each example is evaluated sequentially for parameter updates so that the learning algorithm requires a minimal memory footprint. In addition, using an L1-regularizer in the log-linear model has the desirable property of reducing the number of parameters (Gao et al., 2007). We adopt an efficient gradient calculation method for L1regularized log-linear model proposed in (Langford et al., 2009; Tsuruoka et al., 2009). Two hyperparameters, learning rate and regularization prior, are determined using a development set in our experiments. The training time depends on the number of features of the model. On the biggest Korean dataset, using a single CPU, training took about 12 hours per iteration for the local feature set, and about 24 hours per iteration for the local+deptree+morph feature set. About 2 to 4 iterations were sufficient for good performance. The indepe"
2010.amta-papers.33,D09-1022,0,0.0413963,"language pairs, both with intrinsic evaluations of lexical selection and in end-to-end BLEU scores in Sections 4 and 5. 2 Related Work Much of the work on discriminative lexicon models has focused on target word selection, under the assumption that sentence-level global information is useful in finding good translation candidates. Bangalore et al. (2007) framed the problem of selecting translated words from a target lexicon as a binary classification task, where each word in the target vocabulary included in or excluded from the translation according to its binary decision. In a similar vein, Mauser et al. (2009) integrated discriminative and trigger-based lexicon models into phrase-based Chinese-English MT system, showing an improvement of the BLEU score. The goal of both of these papers on lexicon modeling was to propose appropriate translated words, looking beyond word- and phrase-based translation pairs by including source context information as bag-of-words features and triggerbased models. The same objective has motivated work on word sense disambiguation (WSD) for machine translation, in which different senses of a word in the source language are defined as its possible translations in the targ"
2010.amta-papers.33,P03-1021,0,0.00904146,"를) (this) (이) Given rules of this form, finding the best translation is a matter of searching for the best derivation according to a sentence specific grammar. In the absence of a language model or other models that score based on context, we may simply use a standard parsing algorithm such as CKY to find the best derivation. In the presence of context sensitive features, we resort to the approximate search technique of cube pruning (Chiang, 2007). The baseline treelet system scores candidates with a weighted linear combination of features, with weights trained by Minimum Error Rate Training (Och, 2003), hereafter referred to as MERT. The baseline set of feature functions are: log probabilities of the source treelet given the target treelet and vice versa (maximum likelihood estimates); forward and backward lexical weighting; a target language log probability from a Kneser-Ney smoothed language model; word and phrase count feature functions, and order template log probabilities (maximum likelihood estimates). For all features except the language model, we may pre-compute the weighted score of each model. Then, during decoding, we only need to update the score of the language model as larger"
2010.amta-papers.33,P05-1034,1,0.912393,"gth of our model over a plain tree-to-string translation system is that a discriminative model can easily incorporate rich contextual features, such as neighboring words and dependency relations, which are often absent in the generative translation models. 3.1 Treelet Translation System In this work, we build a discriminative lexicon model over a treelet translation system, a depento run this query , 이 쿼리를 실행하려면 [this] [query] [to run] enter 매개 values 변수 [parameter] for its parameters . 값을 입력하십시오 . [value] [enter] Figure 1: Aligned English-Korean sentence pair dency tree-to-string SMT system (Quirk et al., 2005). A treelet is defined as a connected subgraph of a dependency tree, which acts as a unit in the channel model for decoding much like phrases in phrasal SMT models (Koehn et al., 2003). A treelet translation pair is a pair of source and target language treelets, which are extracted from wordaligned sentence pairs. Figure 1 shows an example of an aligned English-Korean sentence pair: a directed arc indicates a dependency relation, which is derived from an English dependency parser and is projected onto the Korean side. For instance, from the phrase “to run this query” in Figure 1, we can extrac"
2010.amta-papers.33,steinberger-etal-2006-jrc,0,0.0191679,"weights that optimize the BLEU score of this development set. 5.3 Data Our full datasets for the three languages are described in Table 7. They consist of training sets (train), dev sets for tuning the weights of the MT component models (MERT dev), and final test sets for evaluating the translation performance (test). As mentioned earlier, the training data for the discriminative lexicon models is a subset of the MT training data: it includes almost all MT training data, excluding 5K sentences for development and testing. For Bulgarian, we used a 300K sentence subset of the JRC-Aquis corpus (Steinberger et al., 2006) for training. The MERT development set and the test sets are from a variety of sources from more general domains and are thus out-of-domain with respect to the training set. The MT system for Bulgarian used a maximum treelet size of 4. For Czech we used data from the EACL 2009 fourth workshop on SMT. Our test set is newsdev2009b, our MERT dev set is 500 sentences from news-dev2009a, and our training set is the union of the news-commentary09 corpus and the news portion of the CzEng corpus data (Bojar and Žabokrtský, 2009) from sections 0 to 7, with duplicates removed. The MT system used a maxi"
2010.amta-papers.33,P09-1054,0,0.0208026,"to accommodate a large set of training examples and features. To do this, we used an online learning approach, namely stochastic gradient descent (SGD) with L1-regularization. In this learning procedure, each example is evaluated sequentially for parameter updates so that the learning algorithm requires a minimal memory footprint. In addition, using an L1-regularizer in the log-linear model has the desirable property of reducing the number of parameters (Gao et al., 2007). We adopt an efficient gradient calculation method for L1regularized log-linear model proposed in (Langford et al., 2009; Tsuruoka et al., 2009). Two hyperparameters, learning rate and regularization prior, are determined using a development set in our experiments. The training time depends on the number of features of the model. On the biggest Korean dataset, using a single CPU, training took about 12 hours per iteration for the local feature set, and about 24 hours per iteration for the local+deptree+morph feature set. About 2 to 4 iterations were sufficient for good performance. The independent models can be straightforwardly parallelized on thousands of CPUs and can thus be much faster. We are currently working on parallelizing th"
2010.amta-papers.33,H05-1097,0,0.0358384,"source context information as bag-of-words features and triggerbased models. The same objective has motivated work on word sense disambiguation (WSD) for machine translation, in which different senses of a word in the source language are defined as its possible translations in the target language (Berger et al., 1996). Again the correct sense, and therefore the correct translation, depends on the specific meaning of the word in context. Recently there has been quite a bit of research on integrating discriminatively trained WSD systems into SMT (Cabezas and Resnik, 2005; Carpuat and Wu, 2005; Vickrey et al., 2005; Chan et al., 2007). Although their integration efforts have shown promising results, none of these works have focused on the problem of translating from morphologically poor languages into morphologically rich languages. Since such languages generally have complicated rules for generating inflections, modeling a morphologically rich lexicon and disambiguating its senses is particularly challenging. In SMT in general, there has only been a limited amount of work applying morphological processing for translating from English to morphologically rich languages. Of those, Bojar (2007) and Avramid"
2010.amta-papers.33,J96-1002,0,\N,Missing
2010.amta-papers.33,P08-1059,1,\N,Missing
2010.amta-papers.33,D08-1076,0,\N,Missing
2020.acl-main.501,P17-1171,0,0.0646925,"multi-instance learning (Surdeanu et al., 2012), assuming at least one supporting evidence (Hoffmann et al., 2011), integration of label-specific priors (Ritter et al., 2013), and adaption to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-supervised QA setting due to the noise in the heuristics-based span labels (Chen et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017). One line of work jointly learns RC and evidence ranking using either a pipeline system (Wang et al., 2018a; Lee et al., 2018; Kratzwald and Feuerriegel, 2018) or an end-to-end model (Lee et al., 2019). Another line of work focuses on improving distantly-supervised RC models by developing learning methods and model architectures that can better use noisy labels. Clark and Gardner (2018) propose a paragraph-pair ranking objective, which has components of both our H2-P and H3-D position-based formulations. They don’t explore multiple"
2020.acl-main.501,P18-1078,0,0.317345,"ons perform better. We therefore study exhaustive combinations of probability space, distant supervision assumption, and training and inference methods. 5 Experiments 5.1 Data and Implementation Two datasets are used in this paper: TriviaQA (Joshi et al., 2017) in its Wikipedia formulation, and NarrativeQA (summaries setting) (Koˇcisk´y et al., 2018). Using the same preprocessing as 5 P For inference with marginal ( ) scoring, we use an approximate scheme where we only aggregate probabilities of candidates strings generated from a 20-best list of begin/end answer positions for each paragraph. Clark and Gardner (2018) for TriviaQA-Wiki6 , we only keep the top 8 ranked paragraphs up to 400 tokens for each document-question pair for both training and evaluation. Following Min et al. (2019), for NarrativeQA we define the possible answer string sets A using Rouge-L (Lin, 2004) similarity with crouwdsourced abstractive answer strings. We use identical data preprocessing and the evaluation script provided by the authors. In this work, we use the BERT-base model for text encoding and train our model with the default configuration as described in (Devlin et al., 2019), fine-tuning all parameters. We fine-tune for"
2020.acl-main.501,N19-1423,1,0.560444,"ons. While distant supervision reduces the annotation cost, increased coverage often comes with increased noise (e.g., expanding entity answer strings with aliases improves coverage but also increases noise). Even for fixed document-level distant supervision in the form of a set of answers A, different interpretations of the partial supervision lead to different points in the coverage/noise space and their relative performance is not well understood. This work systematically studies methods for learning and inference with document-level distantly supervised extractive QA models. Using a BERT (Devlin et al., 2019) joint question-passage encoder, we study the compound impact of: • Probability space (§2): ways to define the model’s probability space based on independent paragraphs or whole documents. • Distant supervision assumptions (§3): ways to translate the supervision from possible strings A to possible locations of answer mentions in the document. • Optimization and inference (§4): ways to define corresponding training objectives (e.g. Hard EM as in Min et al. (2019) vs. Maximum Marginal Likelihood) and make answer string predictions during inference (Viterbi or marginal inference). We show that th"
2020.acl-main.501,P16-1086,0,0.0752964,"Missing"
2020.acl-main.501,Q18-1023,0,0.0895334,"Missing"
2020.acl-main.501,D18-1055,0,0.040217,"Missing"
2020.acl-main.501,D18-1053,0,0.0249577,"to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-supervised QA setting due to the noise in the heuristics-based span labels (Chen et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017). One line of work jointly learns RC and evidence ranking using either a pipeline system (Wang et al., 2018a; Lee et al., 2018; Kratzwald and Feuerriegel, 2018) or an end-to-end model (Lee et al., 2019). Another line of work focuses on improving distantly-supervised RC models by developing learning methods and model architectures that can better use noisy labels. Clark and Gardner (2018) propose a paragraph-pair ranking objective, which has components of both our H2-P and H3-D position-based formulations. They don’t explore multiple inference methods or combinations of objectives and use less powerful representations. In (Lin et al., 2018), a coarse-to-fine model is proposed to handle label noise by aggregating infor"
2020.acl-main.501,P19-1612,1,0.942839,"s consider each paragraph in the document independently, whereas document models integrate some dependencies among paragraphs. To define the model, we need to specify the probability space, consisting of a set of possible outcomes and a way to assign probabilities to individual outcomes. For extractive QA, the probability space outcomes consist of token positions of answer mention spans. The overall model architecture is shown in Fig. 2. We use BERT (Devlin et al., 2019) to derive representations of document tokens. As is standard in state-of-the-art extractive QA models (Devlin et al., 2019; Lee et al., 2019; Min et al., 2019), the BERT model is used to encode a pair of a given question with one paragraph from a given document into neural text representations. These representations are then used to 2 The code is available at https://github.com/ hao-cheng/ds_doc_qa 5658 define scores/probabilities of possible answer begin and end positions, which are in turn used to define probabilities over possible answer spans. Then the answer string probabilities can be defined as the aggregation over all possible answer spans/mentions. In the following, we show that paragraph-level and document-level models d"
2020.acl-main.501,W04-1013,0,0.0436926,"edia formulation, and NarrativeQA (summaries setting) (Koˇcisk´y et al., 2018). Using the same preprocessing as 5 P For inference with marginal ( ) scoring, we use an approximate scheme where we only aggregate probabilities of candidates strings generated from a 20-best list of begin/end answer positions for each paragraph. Clark and Gardner (2018) for TriviaQA-Wiki6 , we only keep the top 8 ranked paragraphs up to 400 tokens for each document-question pair for both training and evaluation. Following Min et al. (2019), for NarrativeQA we define the possible answer string sets A using Rouge-L (Lin, 2004) similarity with crouwdsourced abstractive answer strings. We use identical data preprocessing and the evaluation script provided by the authors. In this work, we use the BERT-base model for text encoding and train our model with the default configuration as described in (Devlin et al., 2019), fine-tuning all parameters. We fine-tune for 3 epochs on TriviaQA and 2 epochs on NarrativeQA. 5.2 Optimization and Inference for Latent Variable Models Here we look at the cross product of optimization (HardEM vs MML) and inference (Max vs Sum) for all distant supervision assumptions that result in mode"
2020.acl-main.501,P18-1161,0,0.0509609,"rns RC and evidence ranking using either a pipeline system (Wang et al., 2018a; Lee et al., 2018; Kratzwald and Feuerriegel, 2018) or an end-to-end model (Lee et al., 2019). Another line of work focuses on improving distantly-supervised RC models by developing learning methods and model architectures that can better use noisy labels. Clark and Gardner (2018) propose a paragraph-pair ranking objective, which has components of both our H2-P and H3-D position-based formulations. They don’t explore multiple inference methods or combinations of objectives and use less powerful representations. In (Lin et al., 2018), a coarse-to-fine model is proposed to handle label noise by aggregating information from relevant paragraphs and then extracting answers from selected ones. Min et al. (2019) propose a hard EM learning scheme which we included in our experimental evaluation. Our work focuses on examining probabilistic assumptions for document-level extractive QA. We provide a unified view of multiple methods in terms of their probability space and distant supervision assumptions and evaluate the impact of their components in combination with optimization and inference methods. To the best of our knowledge, t"
2020.acl-main.501,P11-1055,0,0.0509056,"e Qll likely contains more incorrect answer strings (false aliases). We can observe that the improvement is more significant for these noisier subsets, suggesting document-level modeling is crucial for handling both types of label noise. 6 Related Work Distant supervision has been successfully used for decades for information extraction tasks such as entity tagging and relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009). Several ways have been proposed to learn with DS, e.g., multi-label multi-instance learning (Surdeanu et al., 2012), assuming at least one supporting evidence (Hoffmann et al., 2011), integration of label-specific priors (Ritter et al., 2013), and adaption to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-supervised QA setting due to the noise in the heuristics-based span labels (Chen et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017). One line of work jointly learns R"
2020.acl-main.501,D19-1284,0,0.061792,"ematically studies methods for learning and inference with document-level distantly supervised extractive QA models. Using a BERT (Devlin et al., 2019) joint question-passage encoder, we study the compound impact of: • Probability space (§2): ways to define the model’s probability space based on independent paragraphs or whole documents. • Distant supervision assumptions (§3): ways to translate the supervision from possible strings A to possible locations of answer mentions in the document. • Optimization and inference (§4): ways to define corresponding training objectives (e.g. Hard EM as in Min et al. (2019) vs. Maximum Marginal Likelihood) and make answer string predictions during inference (Viterbi or marginal inference). We show that the choice of probability space puts constraints on the distant supervision assumptions that can be captured, and that all three choices interact, leading to large differences in performance. Specifically, we provide a framework for understanding different distant supervision assumptions and the corresponding trade-off among the coverage, quality and strength of distant supervision signal. The best configuration depends on the properties of the possible annotation"
2020.acl-main.501,P17-1147,0,0.611651,"lves? Answer: in the spring at mount helicon mount helicon : { in the spring at mount helicon, mount helicon } P1: The play begins with three pages … P2: The courtiers … She sentences them to make reparation and to purify themselves by bathing in the spring at mount helicon. The figure of Actaeon in the play may represent ... Figure 1: TriviaQA and NarrativeQA examples. In the TrivIntroduction Distant supervision assumptions have enabled the creation of large-scale datasets that can be used to train fine-grained extractive short answer question answering (QA) systems. One example is TriviaQA (Joshi et al., 2017). There the authors utilized a pre-existing set of Trivia questionanswer string pairs and coupled them with relevant documents, such that, with high likelihood, the documents support answering the questions (see Fig. 1 for an illustration). Another example is the NarrativeQA dataset (Koˇcisk´y et al., 2018), where crowd-sourced abstractive answer strings were used to weakly supervise answer mentions in the text of movie scripts or their summaries. In this work, we focus on the setting of documentlevel extractive QA, where distant supervision is specified as a set A of answer strings for an inp"
2020.acl-main.501,P17-2081,0,0.0255796,"me parameters for the two objectives and the multiobjective formulation does not have more parameters and is no less efficient than the individual models. Second, we use external clean supervision from SQUAD 2.0 (Rajpurkar et al., 2018) to train the BERT-based QA model for 2 epochs. This model matches the P probability space and is able to detect both NULL and extractive answer spans. The resulting network is used to initialize the models for TriviaQA and NarrativeQA. The results are shown in Table 4. It is not surprising that using external clean supervision improves model performance (e.g. (Min et al., 2017)). We note that, interestingly, this external supervision narrows the performance gap between paragraph-level and document-level models, and reduces the difference between the two inference methods. Compared with their single-objective components, multi-objective formulations improve performance on both TriviaQA and NarrativeQA. 5.5 F1 NarrativeQA Summary Multi-objective Par + Doc Verified Test Set Evaluation Table 5 reports test set results on TriviaQA and NarrativeQA for our best models, in comparison to recent state-of-art (SOTA) models. For TriviaQA, we report F1 and EM scores on the full"
2020.acl-main.501,P09-1113,0,0.102905,"nce separately on each subset, as shown in Table 6. In general, we expect Qsl and Qll to be noisier due to the larger I, where Qsl potentially includes many irrelevant mentions while Qll likely contains more incorrect answer strings (false aliases). We can observe that the improvement is more significant for these noisier subsets, suggesting document-level modeling is crucial for handling both types of label noise. 6 Related Work Distant supervision has been successfully used for decades for information extraction tasks such as entity tagging and relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009). Several ways have been proposed to learn with DS, e.g., multi-label multi-instance learning (Surdeanu et al., 2012), assuming at least one supporting evidence (Hoffmann et al., 2011), integration of label-specific priors (Ritter et al., 2013), and adaption to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-superv"
2020.acl-main.501,P19-1220,0,0.0147623,"paragraph-level and document-level models, and reduces the difference between the two inference methods. Compared with their single-objective components, multi-objective formulations improve performance on both TriviaQA and NarrativeQA. 5.5 F1 NarrativeQA Summary Multi-objective Par + Doc Verified Test Set Evaluation Table 5 reports test set results on TriviaQA and NarrativeQA for our best models, in comparison to recent state-of-art (SOTA) models. For TriviaQA, we report F1 and EM scores on the full test set and the verified subset. For NarrativeQA, RougeOurs (H2-P+H2-D) w/o SQUAD 62.9 60.5 (Nishida et al., 2019) w/o external data 59.9 54.7 (Min et al., 2019) 58.8 Table 5: Test set results on TriviaQA Wiki and NarrativeQA Summaries. “w/o SQUAD” refers to our best model without pretraining on SQUAD 2.0. “w/o external data” refers to the model from (Nishida et al., 2019) without using MS MARCO data (Bajaj et al., 2018). L scores are reported. Compared to recent TriviaQA SOTA (Wang et al., 2018b), our best models achieve 4.9 F1 and 5.5 EM improvement on the full test set, and 6.8 F1 and 7.4 EM improvement on the verified subset. On the NarrativeQA test set, we improve Rouge-L by 3.0 over (Nishida et al.,"
2020.acl-main.501,P18-2124,0,0.190125,"obabilities (?? , ?? ) … (“Joan Rivers” |?? ) BERT BERT ? ?? … … … … Contextualized Representation … ? ?? … … Figure 2: The document-level QA model as used for test-time inference. The lower part is a BERT-based paragraph-level answer scoring component, and the upper part illustrates the probability aggregation across answer spans sharing the same answer string. Ξ refers to either a sum or a max operator. In the given example, “John Rivers” is derived from two paragraphs. tasks. Results are further strengthened by transfer learning from fully labeled short-answer extraction data in SQuAD 2.0 (Rajpurkar et al., 2018), leading to a final state-of-the-art performance of 76.3 F1 on TriviaQA-Wiki and 62.9 on the NarrativeQA summaries task.2 2 Probability Space Here, we first formalize both paragraph-level and document-level models, which have been previously used for document-level extractive QA. Typically, paragraph-level models consider each paragraph in the document independently, whereas document models integrate some dependencies among paragraphs. To define the model, we need to specify the probability space, consisting of a set of possible outcomes and a way to assign probabilities to individual outcome"
2020.acl-main.501,Q13-1030,0,0.0175032,"ases). We can observe that the improvement is more significant for these noisier subsets, suggesting document-level modeling is crucial for handling both types of label noise. 6 Related Work Distant supervision has been successfully used for decades for information extraction tasks such as entity tagging and relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009). Several ways have been proposed to learn with DS, e.g., multi-label multi-instance learning (Surdeanu et al., 2012), assuming at least one supporting evidence (Hoffmann et al., 2011), integration of label-specific priors (Ritter et al., 2013), and adaption to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-supervised QA setting due to the noise in the heuristics-based span labels (Chen et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017). One line of work jointly learns RC and evidence ranking using either a pipeline system (Wang"
2020.acl-main.501,D12-1042,0,0.0302617,"ger I, where Qsl potentially includes many irrelevant mentions while Qll likely contains more incorrect answer strings (false aliases). We can observe that the improvement is more significant for these noisier subsets, suggesting document-level modeling is crucial for handling both types of label noise. 6 Related Work Distant supervision has been successfully used for decades for information extraction tasks such as entity tagging and relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009). Several ways have been proposed to learn with DS, e.g., multi-label multi-instance learning (Surdeanu et al., 2012), assuming at least one supporting evidence (Hoffmann et al., 2011), integration of label-specific priors (Ritter et al., 2013), and adaption to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-supervised QA setting due to the noise in the heuristics-based span labels (Chen et al., 2017; Joshi et al., 2017; Dunn et"
2020.acl-main.501,P18-1158,0,0.220499,"hods to further improve weakly supervised QA models. First, we combine two distant supervision objectives in a multitask manner, i.e. H2-P and H3-D for TriviaQA, and H2-P and H2-D for NarrativeQA, chosen based on the results in §5.3. H2 objectives have higher coverage than H3 while being more susceptible 5663 Objective Clean Infer TriviaQA F1 NarrativeQA EM Rouge-L TriviaQA Wiki Full Single-objective X Max Sum 71.9 73.0 67.7 69.0 59.2 57.8 X Max Sum 74.2 74.9 70.1 70.9 61.7 61.7 X Max Sum 75.1 75.3 70.6 70.8 60.1 59.9 X Max Sum 75.5 75.5 70.8 70.9 62.8 62.9 Par Doc Ours (H2-P+H3-D) w/o SQUAD (Wang et al., 2018b) (Clark and Gardner, 2018) (Min et al., 2019) X Max Sum 75.6 75.9 71.2 71.6 60.5 60.5 X Max Sum 75.8 76.2 71.2 71.7 63.0 63.1 EM F1 EM 76.3 75.7 71.4 68.9 67.1 72.1 71.6 66.6 64.0 – 85.5 83.6 78.7 72.9 – 82.2 79.6 74.8 68.0 – Rouge-L Table 4: Dev set results comparing multi-objectives and clean supervison. X indicates the QA model is pre-trained on SQUAD. to noise. Paragraph-level models have the advantage of learning to score irrelevant paragraphs (via NULL outcomes). Note that we use the same parameters for the two objectives and the multiobjective formulation does not have more parameters"
2020.acl-main.501,D19-1397,0,0.01441,"these noisier subsets, suggesting document-level modeling is crucial for handling both types of label noise. 6 Related Work Distant supervision has been successfully used for decades for information extraction tasks such as entity tagging and relation extraction (Craven and Kumlien, 1999; Mintz et al., 2009). Several ways have been proposed to learn with DS, e.g., multi-label multi-instance learning (Surdeanu et al., 2012), assuming at least one supporting evidence (Hoffmann et al., 2011), integration of label-specific priors (Ritter et al., 2013), and adaption to shifted label distributions (Ye et al., 2019). Recent work has started to explore distant supervision to scale up QA systems, particularly for open-domain QA where the evidence has to be retrieved rather than given as input. Reading comprehension (RC) with evidence retrieved from information retrieval systems establishes a weakly-supervised QA setting due to the noise in the heuristics-based span labels (Chen et al., 2017; Joshi et al., 2017; Dunn et al., 2017; Dhingra et al., 2017). One line of work jointly learns RC and evidence ranking using either a pipeline system (Wang et al., 2018a; Lee et al., 2018; Kratzwald and Feuerriegel, 201"
2021.acl-long.75,2020.acl-main.676,0,0.366849,"on-synthetic datasets by focusing on the template splits proposed by Finegan-Dollak et al. (2018), demonstrating improvements over standard seq2seq models. The effect of large-scale pre-training on compositional generalization ability has also been studied. Furrer et al. (2020) finds that pre-training alone cannot solve several compositional generalization challenges, despite its effectiveness across NLP tasks such as question answering (Raffel et al., 2020). While our work focuses on modeling approaches, compositional data augmentation techniques have also been proposed (Jia and Liang, 2016; Andreas, 2020). NQG-T5 outperforms previously reported results for these methods, but more in-depth analysis is needed. 3 Target Maximum Compound Divergence (TMCD) Splits The existing evaluations targeting compositional generalization for non-synthetic tasks are template splits and length splits. Here we propose an additional method which expands the set of available evaluations by generating data splits that maximize compound divergence over non-synthetic datasets, termed Target Maximum Compound Divergence (TMCD) splits. As we show in § 6, it results in a generalization problem with different characteristi"
2021.acl-long.75,P13-2009,0,0.191301,"our induced grammar G.5 Our grammar G contains a single non-terminal symbol, N T . We restrict source rules to ones containing at most 2 non-terminal symbols, and do not allow unary productions as source rules. This enables efficient parsing using an algorithm similar to CKY (Cocke, 1969; Kasami, 1965; Younger, 1967) that does not require binarization of the grammar. NQG Component NQG is inspired by more traditional approaches to semantic parsing based on grammar formalisms such as CCG (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2013) and SCFG (Wong and Mooney, 2006, 2007; Andreas et al., 2013; Li 925 Induction Procedure To induce G from the training data, we propose a QCFG induction algorithm that does not rely on task-specific heuristics or pre-computed word alignments. Notably, our approach makes no explicit assumptions about the source or target languages, beyond those implicit in the QCFG formalism. Table 1 shows examples of induced rules. Our grammar induction algorithm is guided by the principle of Occam’s razor, which leads us to 5 See Appendix A.1 for additional background on QCFGs. is the set of derivations that yield source string x and any target string. The constants l"
2021.acl-long.75,D15-1170,0,0.0504146,"Missing"
2021.acl-long.75,P07-1121,0,0.0298222,"Missing"
2021.acl-long.75,N06-1056,0,0.377494,"2006), or QCFGs, to refer to our induced grammar G.5 Our grammar G contains a single non-terminal symbol, N T . We restrict source rules to ones containing at most 2 non-terminal symbols, and do not allow unary productions as source rules. This enables efficient parsing using an algorithm similar to CKY (Cocke, 1969; Kasami, 1965; Younger, 1967) that does not require binarization of the grammar. NQG Component NQG is inspired by more traditional approaches to semantic parsing based on grammar formalisms such as CCG (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010, 2013) and SCFG (Wong and Mooney, 2006, 2007; Andreas et al., 2013; Li 925 Induction Procedure To induce G from the training data, we propose a QCFG induction algorithm that does not rely on task-specific heuristics or pre-computed word alignments. Notably, our approach makes no explicit assumptions about the source or target languages, beyond those implicit in the QCFG formalism. Table 1 shows examples of induced rules. Our grammar induction algorithm is guided by the principle of Occam’s razor, which leads us to 5 See Appendix A.1 for additional background on QCFGs. is the set of derivations that yield source string x and any ta"
2021.acl-long.75,D18-1425,0,0.564354,"-trained seq2seq models that perform well on in-distribution evaluations do not address most of the compositional generalization challenges proposed in SCAN (Furrer et al., 2020). Add Primitive (Lake and Baroni, 2018) Length TMCD Template (Finegan-Dollak et al., 2018) Length Random SYNTHETIC NON - SYNTHETIC NATURAL LANGUAGE VARIATION Figure 2: We evaluate semantic parsing approaches across a diverse set of evaluations focused on natural language variation, compositional generalization, or both. We add TMCD splits to complement existing evaluations. Ordering within each cell is arbitrary. DER (Yu et al., 2018). Our results indicate that NQG-T5 is a strong baseline for our challenge of developing approaches that perform well across a diverse set of evaluations focusing on either natural language variation, compositional generalization, or both. Comparing five approaches across eight evaluations on SCAN and G EO Q UERY, its average rank is 1, with the rank of the best previous approach (T5) being 2.9; performance is also competitive across several evaluations on S PIDER. While still far from affirmatively answering our research question, our study highlights the importance of a diverse set of evaluat"
2021.acl-long.75,D07-1071,0,0.0339813,"Missing"
2021.eacl-main.253,2020.findings-emnlp.91,0,0.174899,"e (if anywhere). In addition, very often information presented in tables is compact and abbreviated. The associated text can potentially provide rich context that can be used to enhance the representation of the table for more robust question answering. The main focus of this paper is to investigate how to improve question answering on documents that contain both text and tables. While recently there has been a lot of interest in reading comprehension for both text and tables, little research has been done in combining the two sources of information. The only prior study we are aware of is by Chen et al. (2020) who introduced a new dataset for multi-hop QA over tabular and textual data. In their work, the authors heavily rely on the assumption that the questions would be unanswerable if either text or table information is missing. Here we investigate a more realistic scenario of naturally occurring questions, where the answer can be found in either text, tables, both, or none. We evaluate our approach on the Natural Questions corpus (Kwiatkowski et al., 2019) which consists of real anonymized queries issued to the Google search engine and corresponding Wikipedia articles, simulating a real use case"
2021.eacl-main.253,P19-1285,0,0.0155666,"on of the table for QA. Recent work has explored building encoders over large input sequences. Ravula et al. (2020) scaled input sequence length to more than 8,000 tokens for the NQ dataset. However, to make the model efficient, encodings of individual text or table segments communicate through single-vector global memories. Here, we take the approach of using asymmetric attention from table token representations to a small number of relevant text token representations, that are precomputed independently. Our approach is more similar to the handling of prior segment context in Transformer-XL (Dai et al., 2019), but relevant context is selected based on word overlap and not contiguity. The two components of our approach, described next, include the definition of relevant text context for table elements and the mechanism for using contextualized embeddings of the relevant text to enrich the table token representations. 3.2.1 Table-Textual Context Linking Let a table cell that contains a sequence of input tokens be defined as (ut0 . . . utK ), with the corresponding s sub-word units (wordpiecies in BERT or byte-pair encodings in RoBERTa) for the k-th t,k word to be defined as (xt,k 0 . . . xSk ), and"
2021.eacl-main.253,N19-1423,1,0.0923115,"uestions, where the answer can be found in either text, tables, both, or none. We evaluate our approach on the Natural Questions corpus (Kwiatkowski et al., 2019) which consists of real anonymized queries issued to the Google search engine and corresponding Wikipedia articles, simulating a real use case of such a system. Prior work on the Natural Questions dataset has treated text and tables uniformly, linearizing tables and representing them and text segments using the same contextual token representations (for example, starting from pre-trained transformers (Vaswani et al., 2017) like BERT (Devlin et al., 2019)). However, representations developed for text are sub-optimal for tables, since they do not account for the special relationships between table cells, defined by the row and column structure. In this work, we extend the BERT architecture to account for inter-cell relationships in tables. This approach is motivated by Graph Neural Networks with a transformer (Shaw et al., 2018) and is 2895 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2895–2906 April 19 - 23, 2021. ©2021 Association for Computational Linguistics closely relat"
2021.eacl-main.253,2020.acl-main.210,0,0.0221193,"pretraining method are comparable to those in recent and concurrent work. Leveraging tables is a hard problem. However, most studies on table-based QA omit an important additional information source: the text in the article discussing the table. Prior attempts at integrating a KB and text use early fusion of document text and KG information (Sun et al., 2018), where they integrate text and a KG sub-graph in a single graph, from which an entity is selected to answer the question. Structured KGs are often easier to interpret than tables, which have a wide variety of possible schemas. InfoTabS (Gupta et al., 2020) introduced a dataset for the natural language inference task based on premises that are tables, where the authors explore multiple table representations, including a key-value approach and linearized representations with table rows corresponding to ”sentences.” Hypothesis representations are calculated separately. Recently, TaBERT (Yin et al., 2020) introduced a joint table-utterance representation approach, where a table row is concatenated with a short text utterance, such as the query in question answering, and passed as an input to a BERT-based model. Such an approach relies on the initia"
2021.eacl-main.253,2020.acl-main.398,0,0.0446239,"Missing"
2021.eacl-main.253,D17-1160,0,0.0266503,"s. Finally, this is the first model that uses the Natural Questions corpus for question answering on tables, improving the baseline in Alberti et al. (2019) that does not distinguish between tables and text. 2 Related Work Most work on QA with tables prior to BERT involves first converting the table to a Knowledge Graph (KG) where cell entries are entities with row/column relations, then using entity linking to identify spans in the question that match an entity in the knowledge graph, and finally parsing the question to generate a SQL query using some variant of a sequence-to-sequence model (Krishnamurthy et al., 2017). Due to the advances in contextualized word embeddings, more recent work proposed a modification of the BERT transformer architecture to be used for representing tables. Hwang et al. (2019) proposed the usage of additional [SEP] tokens between headers of the table to make a BERT model more suitable for the tables. Recently, Yin et al. (2020) introduced a pretraining procedure for joint representation of tabular data paired with an utterance, where the approach is to linearize the structure of tables to be compatible with a BERT model. Our approach for table encoding is most similar to that of"
2021.eacl-main.253,Q19-1026,1,0.813839,"nsion for both text and tables, little research has been done in combining the two sources of information. The only prior study we are aware of is by Chen et al. (2020) who introduced a new dataset for multi-hop QA over tabular and textual data. In their work, the authors heavily rely on the assumption that the questions would be unanswerable if either text or table information is missing. Here we investigate a more realistic scenario of naturally occurring questions, where the answer can be found in either text, tables, both, or none. We evaluate our approach on the Natural Questions corpus (Kwiatkowski et al., 2019) which consists of real anonymized queries issued to the Google search engine and corresponding Wikipedia articles, simulating a real use case of such a system. Prior work on the Natural Questions dataset has treated text and tables uniformly, linearizing tables and representing them and text segments using the same contextual token representations (for example, starting from pre-trained transformers (Vaswani et al., 2017) like BERT (Devlin et al., 2019)). However, representations developed for text are sub-optimal for tables, since they do not account for the special relationships between tab"
2021.eacl-main.253,2020.acl-main.604,0,0.074858,"select the table rows most relevant to the query. In contrast, we enrich the table representation using an attention mechanism with the representations of the most relevant parts of the context of the article in which the table appears. The Natural Questions is a large corpus that contains real user queries along with their corresponding Wikipedia articles, which may or may not contain an answer anywhere in the article. Alberti et al. (2019) provided a BERT-based baseline that treats both table and text segments like text: a sequence of tokens with word and position embeddings. Re2896 cently, Liu et al. (2020) improved this baseline by using dynamic dual-attention over paragraphs and cascade answer predictor. In another direction, Ravula et al. (2020) used an extended transformer architecture that models extra-long documents with limited propagation of information among different segments. All three approaches did not distinguish between text and table input, treating tables as text while not taking into account table structure. To the best of our knowledge, this is the first work that focuses on table-based QA for the real user queries in the Natural Questions corpus, and shows that table-based an"
2021.eacl-main.253,D19-1603,0,0.0226677,"Missing"
2021.eacl-main.253,P15-1142,0,0.0807757,"Missing"
2021.eacl-main.253,P19-1010,0,0.0345868,"Missing"
2021.eacl-main.253,N18-2074,0,0.386156,"t and tables uniformly, linearizing tables and representing them and text segments using the same contextual token representations (for example, starting from pre-trained transformers (Vaswani et al., 2017) like BERT (Devlin et al., 2019)). However, representations developed for text are sub-optimal for tables, since they do not account for the special relationships between table cells, defined by the row and column structure. In this work, we extend the BERT architecture to account for inter-cell relationships in tables. This approach is motivated by Graph Neural Networks with a transformer (Shaw et al., 2018) and is 2895 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2895–2906 April 19 - 23, 2021. ©2021 Association for Computational Linguistics closely related to the one in M¨ueller et al. (2019). In our work, we pretrain parameters for the new relationships using a large table corpus extracted from Wikipedia (Bhagavatula et al., 2013). In addition, we present a novel approach that refines table representations by attending to related representations of text in the surrounding article. This allows information to propagate from the"
2021.eacl-main.253,D18-1455,0,0.0246856,"table representation learning, TaPaS (Herzig et al., 2020) and GraPPa (Yu et al., 2020), also use pretraining on the Wikitables dataset (Bhagavatula et al., 2013) that we use in our work. Therefore, our table representations based on transformers and our pretraining method are comparable to those in recent and concurrent work. Leveraging tables is a hard problem. However, most studies on table-based QA omit an important additional information source: the text in the article discussing the table. Prior attempts at integrating a KB and text use early fusion of document text and KG information (Sun et al., 2018), where they integrate text and a KG sub-graph in a single graph, from which an entity is selected to answer the question. Structured KGs are often easier to interpret than tables, which have a wide variety of possible schemas. InfoTabS (Gupta et al., 2020) introduced a dataset for the natural language inference task based on premises that are tables, where the authors explore multiple table representations, including a key-value approach and linearized representations with table rows corresponding to ”sentences.” Hypothesis representations are calculated separately. Recently, TaBERT (Yin et a"
2021.eacl-main.253,2020.acl-main.745,0,0.088425,"ties with row/column relations, then using entity linking to identify spans in the question that match an entity in the knowledge graph, and finally parsing the question to generate a SQL query using some variant of a sequence-to-sequence model (Krishnamurthy et al., 2017). Due to the advances in contextualized word embeddings, more recent work proposed a modification of the BERT transformer architecture to be used for representing tables. Hwang et al. (2019) proposed the usage of additional [SEP] tokens between headers of the table to make a BERT model more suitable for the tables. Recently, Yin et al. (2020) introduced a pretraining procedure for joint representation of tabular data paired with an utterance, where the approach is to linearize the structure of tables to be compatible with a BERT model. Our approach for table encoding is most similar to that of M¨ueller et al. (2019), where the authors generalized the BERT architecture similarly to Shaw et al. (2018) with new types of relations to encode table-specific relationships. The main differences between our table representation and M¨ueller et al. (2019) is that in our representation we use 5 types of relations, cell-column, cell-row, in-c"
2021.emnlp-main.560,D13-1160,0,0.0653188,"tive is the sum of −logP (p|q, B) of the passages including any valid answer to q. At inference, I NDEP PR outputs the top k passages based the logit values of the passage indices. We compare mainly to I N DEP PR because it is the strict non-autoregressive version of JPR, and is empirically better than or comparable to Nogueira et al. (2020) (Section 5.1). We train and evaluate on three datasets that provide a set of distinct answers for each question. Statistics of each dataset are provided in Table 2. W EB QSP (Yih et al., 2016) consists of questions from Google Suggest API, originally from Berant et al. (2013). The answer is a set of distinct entities in Freebase; we recast this problem as textual question answering based on Wikipedia. A MBIG QA (Min et al., 2020) consists of questions mined from Google search queries, originally from NQ (Kwiatkowski et al., 2019). Each question is paired with an annotated set of distinct answers 4.4 Implementation Details that are equally valid based on Wikipedia. TREC (Baudiš and Šediv`y, 2015) contains ques- We use the English Wikipedia from 12/20/2018 tions curated from TREC QA tracks, along with as the retrieval corpus C, where each article is regular expressi"
2021.emnlp-main.560,P17-1171,0,0.0188345,"ropose JPR, a joint passage retrieval model that integrates dependencies among selected passages, along with new training and decoding algorithms. 3. On three multi-answer QA datasets, JPR significantly outperforms a range of baselines with independent scoring of passages, both in retrieval recall and answer accuracy. 2 2.1 Background Review: Single-Answer Retrieval In a typical single-answer retrieval problem, a model is given a natural language question q and retrieves k passages {p1 ...pk } from a large text corpus C (Voorhees et al., 1999; Ramos et al., 2003; Robertson and Zaragoza, 2009; Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020; Luan et al., 2020). The goal is to retrieve at least one passage that contains the answer to q. During training, question-answer pairs (q, a) are given to the model. Task Single-answer Retrieval Multi-answer Retrieval Train Data (q, a) (q, {a1 ...an }) Inference q → {p1 ...pk } q → {p1 ...pk } Evaluation R ECALL( a, {p1 ...pk }) MR ECALL( {a1 ...an }, {p1 ...pk }) Appropriate Model P (pi |q) P (p1 ...pk |q) Table 1: A comparison of single-answer and multianswer retrieval tasks. Previous work has used independent ranking models P (pi |q) for multi-ans"
2021.emnlp-main.560,2021.eacl-main.74,0,0.444028,"Inference q → {p1 ...pk } q → {p1 ...pk } Evaluation R ECALL( a, {p1 ...pk }) MR ECALL( {a1 ...an }, {p1 ...pk }) Appropriate Model P (pi |q) P (p1 ...pk |q) Table 1: A comparison of single-answer and multianswer retrieval tasks. Previous work has used independent ranking models P (pi |q) for multi-answer retrieval because the inference-time inputs and outputs are the same. We propose JPR as an instance of P (p1 ...pk |q). trieval successful if the answer a is included in {p1 ...pk }. Extrinsic evaluation uses the retrieved passages as input to an answer generation model such as the model in Izacard and Grave (2021) and evaluates final question answering performance. Reranking Much prior work (Liu, 2011; Asadi and Lin, 2013; Nogueira et al., 2020) found an effective strategy in using a two-step approach of (1) retrieving a set of candidate passages B from the corpus C (k &lt; |B | |C|) and (2) using another model to rerank the passages, obtaining a final top k. A reranker could be more expressive than the first-stage model (e.g. by using cross-attention), as it needs to process much fewer candidates. Most prior work in reranking, including the current stateof-the-art (Nogueira et al., 2020), scores each pa"
2021.emnlp-main.560,2020.emnlp-main.550,1,0.91278,"model that integrates dependencies among selected passages, along with new training and decoding algorithms. 3. On three multi-answer QA datasets, JPR significantly outperforms a range of baselines with independent scoring of passages, both in retrieval recall and answer accuracy. 2 2.1 Background Review: Single-Answer Retrieval In a typical single-answer retrieval problem, a model is given a natural language question q and retrieves k passages {p1 ...pk } from a large text corpus C (Voorhees et al., 1999; Ramos et al., 2003; Robertson and Zaragoza, 2009; Chen et al., 2017; Lee et al., 2019; Karpukhin et al., 2020; Luan et al., 2020). The goal is to retrieve at least one passage that contains the answer to q. During training, question-answer pairs (q, a) are given to the model. Task Single-answer Retrieval Multi-answer Retrieval Train Data (q, a) (q, {a1 ...an }) Inference q → {p1 ...pk } q → {p1 ...pk } Evaluation R ECALL( a, {p1 ...pk }) MR ECALL( {a1 ...an }, {p1 ...pk }) Appropriate Model P (pi |q) P (p1 ...pk |q) Table 1: A comparison of single-answer and multianswer retrieval tasks. Previous work has used independent ranking models P (pi |q) for multi-answer retrieval because the inference-time i"
2021.emnlp-main.560,Q19-1026,1,0.820022,"ve version of JPR, and is empirically better than or comparable to Nogueira et al. (2020) (Section 5.1). We train and evaluate on three datasets that provide a set of distinct answers for each question. Statistics of each dataset are provided in Table 2. W EB QSP (Yih et al., 2016) consists of questions from Google Suggest API, originally from Berant et al. (2013). The answer is a set of distinct entities in Freebase; we recast this problem as textual question answering based on Wikipedia. A MBIG QA (Min et al., 2020) consists of questions mined from Google search queries, originally from NQ (Kwiatkowski et al., 2019). Each question is paired with an annotated set of distinct answers 4.4 Implementation Details that are equally valid based on Wikipedia. TREC (Baudiš and Šediv`y, 2015) contains ques- We use the English Wikipedia from 12/20/2018 tions curated from TREC QA tracks, along with as the retrieval corpus C, where each article is regular expressions as answers. Prior work uses split into passages with up to 288 wordpieces, All this data as a task of finding a single answer (where rerankers are based on T5 (Raffel et al., 2020), retrieving any of the correct answers is sufficient), a pretrained encode"
2021.emnlp-main.560,P19-1612,1,0.924355,"e reof passages relevant to a natural language question call, and keeping passages relevant to the question. from a large text corpus. Most prior work focuses In this work, we introduce Joint Passage Reon single-answer retrieval, which scores passages trieval (JPR), a new model that addresses these independently from each other according to their challenges. To jointly score passages, JPR emrelevance to the given question, assuming there is ploys an encoder-decoder reranker and autoregresa single answer (Voorhees et al., 1999; Chen et al., sively generates passage references by modeling 2017; Lee et al., 2019). However, questions posed the probability of each passage as a function of by humans are often open-ended and ambiguous, previously retrieved passages. Since there is no leading to multiple valid answers (Min et al., 2020). ground truth ordering of passages, we employ a For example, for the question in Figure 1, “What new training method that dynamically forms suwas Eli Whitney’s job?”, an ideal retrieval system pervision to drive the model to prefer passages should provide passages covering all professions of with answers not already covered by previously seEli Whitney. This introduces the p"
2021.emnlp-main.560,2020.emnlp-main.466,1,0.870763,"ieval, which scores passages trieval (JPR), a new model that addresses these independently from each other according to their challenges. To jointly score passages, JPR emrelevance to the given question, assuming there is ploys an encoder-decoder reranker and autoregresa single answer (Voorhees et al., 1999; Chen et al., sively generates passage references by modeling 2017; Lee et al., 2019). However, questions posed the probability of each passage as a function of by humans are often open-ended and ambiguous, previously retrieved passages. Since there is no leading to multiple valid answers (Min et al., 2020). ground truth ordering of passages, we employ a For example, for the question in Figure 1, “What new training method that dynamically forms suwas Eli Whitney’s job?”, an ideal retrieval system pervision to drive the model to prefer passages should provide passages covering all professions of with answers not already covered by previously seEli Whitney. This introduces the problem of multilected passages. Furthermore, we introduce a new answer retrieval—retrieval of multiple passages tree-decoding algorithm to allow flexibility in the ∗ Work done while interning at Google. degree of diversity."
2021.emnlp-main.560,P19-1416,1,0.828576,"ncreasing the distances between retrieved passages does not help.7 Second, multi-answer retrieval uses a clear notion of “answers”; “sub-topics” in diverse IR are more subjective and hard to enumerate fully. Multi-hop passage retrieval Recent work studies multi-hop passage retrieval, where a passage containing the answer is the destination of a chain of multiple hops (Asai et al., 2020; Xiong et al., 2021; Khattab et al., 2021). This is a difficult problem as passages in a chain are dissimilar to each other, but existing datasets often suffer from annotation artifacts (Chen and Durrett, 2019; Min et al., 2019), resulting in strong lexical cues for each hop. We study an orthogonal problem of finding multiple answers, where the challenge is in controlling the trade-off between relevance and diversity. 7 Conclusion We introduce JPR, an autoregressive passage reranker designed to address the multi-answer retrieval problem. On three multi-answer datasets, JPR significantly outperforms a range of baselines 7 Diverse retrieval Studies on diverse retrieval in In our preliminary experiment, we tried increasing diversity based on Maximal Marginal Relevance (Carbonell and the context of information retrieval"
2021.emnlp-main.560,2020.findings-emnlp.63,0,0.293556,"pi |q) P (p1 ...pk |q) Table 1: A comparison of single-answer and multianswer retrieval tasks. Previous work has used independent ranking models P (pi |q) for multi-answer retrieval because the inference-time inputs and outputs are the same. We propose JPR as an instance of P (p1 ...pk |q). trieval successful if the answer a is included in {p1 ...pk }. Extrinsic evaluation uses the retrieved passages as input to an answer generation model such as the model in Izacard and Grave (2021) and evaluates final question answering performance. Reranking Much prior work (Liu, 2011; Asadi and Lin, 2013; Nogueira et al., 2020) found an effective strategy in using a two-step approach of (1) retrieving a set of candidate passages B from the corpus C (k &lt; |B | |C|) and (2) using another model to rerank the passages, obtaining a final top k. A reranker could be more expressive than the first-stage model (e.g. by using cross-attention), as it needs to process much fewer candidates. Most prior work in reranking, including the current stateof-the-art (Nogueira et al., 2020), scores each passage independently, modeling P (p|q). 2.2 Multi-Answer Retrieval We now formally define the task of multi-answer retrieval. A model i"
2021.emnlp-main.560,P16-2033,1,0.83115,"rained to output a single token i (1 ≤ i ≤ |B|) rather than a sequence. The objective is the sum of −logP (p|q, B) of the passages including any valid answer to q. At inference, I NDEP PR outputs the top k passages based the logit values of the passage indices. We compare mainly to I N DEP PR because it is the strict non-autoregressive version of JPR, and is empirically better than or comparable to Nogueira et al. (2020) (Section 5.1). We train and evaluate on three datasets that provide a set of distinct answers for each question. Statistics of each dataset are provided in Table 2. W EB QSP (Yih et al., 2016) consists of questions from Google Suggest API, originally from Berant et al. (2013). The answer is a set of distinct entities in Freebase; we recast this problem as textual question answering based on Wikipedia. A MBIG QA (Min et al., 2020) consists of questions mined from Google search queries, originally from NQ (Kwiatkowski et al., 2019). Each question is paired with an annotated set of distinct answers 4.4 Implementation Details that are equally valid based on Wikipedia. TREC (Baudiš and Šediv`y, 2015) contains ques- We use the English Wikipedia from 12/20/2018 tions curated from TREC QA"
C02-2025,J97-4005,0,0.0188522,"d information about the words. Well-understood statistical part-of-speech tagging technology is sufficient for this approach. In order to use more information about the parse, we might examine the entire derivation of the string. Most probabilistic parsing research – including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these pr"
C02-2025,W97-1502,0,0.122641,"nks, there is no need to define a (new) form of grammatical representation specific to the treebank. Instead, the treebank records complete syntactosemantic analyses as defined by the LinGO ERG and provide tools to extract different types of linguistic information at varying granularity. The treebanking environment, building on the [incr tsdb()] profiling environment (Oepen & Callmeier, 2000), presents annotators, one sentence at a time, with the full set of analyses produced by the grammar. Using a pre-existing tree comparison tool in the LKB (similar in kind to the SRI Cambridge TreeBanker; Carter, 1997), annotators can quickly navigate through the parse forest and identify the correct or preferred analysis in the current context (or, in rare cases, reject all analyses proposed by the grammar). The tree selection tool presents users, who need little expert knowledge of the underlying grammar, with a range of basic properties that distinguish competing analyses and that are relatively easy to judge. All disambiguating decisions made by annotators are recorded in the [incr tsdb()] database and thus become available for (i) later dynamic extraction from the annotated profile or (ii) dynamic prop"
C02-2025,P97-1003,0,0.0128863,"t the simplest end, we might look only at the lexical type sequence assigned to the words by each parse and rank the parse based on the likelihood of that sequence. These lexical types – the preterminals in the derivation – are essentially part-of-speech tags, but encode considerably finer-grained information about the words. Well-understood statistical part-of-speech tagging technology is sufficient for this approach. In order to use more information about the parse, we might examine the entire derivation of the string. Most probabilistic parsing research – including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). The"
C02-2025,P01-1019,1,0.256862,"with an enhanced version of the grammar in an automated fashion, viz. by re-applying the disambiguating decisions on the corpus with an updated version of the grammar. Depth of Representation and Transformation of Information Internally, the [incr tsdb()] database records analyses in three different formats, viz. (i) as a derivation tree composed of identifiers of lexical items and constructions used to build the analysis, (ii) as a traditional phrase structure tree labeled with an inventory of some fifty atomic labels (of the type ‘S’, ‘NP’, ‘VP’ et al.), and (iii) as an underspecified MRS (Copestake, Lascarides, & Flickinger, 2001) meaning representation. While representation (ii) will in many cases be similar to the representation found in the Penn Treebank, representation (iii) subsumes the functor – argument (or tectogrammatical) structure advocated in the Prague Dependency Treebank or the German TiGer corpus. Most importantly, however, representation (i) provides all the information required to replay the full HPSG analysis (using the original grammar and one of the open-source HPSG processing environments, e.g., the LKB or PET, which already have been interfaced to [incr tsdb()]). Using the latter approach, users"
C02-2025,W00-1908,0,0.0559015,"Missing"
C02-2025,P99-1069,0,0.0373806,"including, for example, work by by Collins (1997), and Charniak (1997) – is based on branching process models (Harris, 1963). The HPSG derivations that the treebank makes available can be viewed as just such a branching process, and a stochastic model of the trees can be built as a probabilistic context-free grammar (PCFG) model. Abney (1997) notes important problems with the soundness of the approach when a unification-based grammar is actually determining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these proof-of-concept experiments (more recently, we have begun work on building log-linear models over HPSG signs (Toutanova & Manning, 2002)). The learned models were used to rank possible parses of unseen test sentences according to the probabilities they assign to them. We report parse selection perfo"
C02-2025,2000.iwpt-1.19,1,0.696136,"SG framework and a generally-available broad-coverage grammar of English, the LinGO English Resource Grammar (Flickinger, 2000) as implemented with the LKB grammar development environment (Copestake, 2002). Unlike existing treebanks, there is no need to define a (new) form of grammatical representation specific to the treebank. Instead, the treebank records complete syntactosemantic analyses as defined by the LinGO ERG and provide tools to extract different types of linguistic information at varying granularity. The treebanking environment, building on the [incr tsdb()] profiling environment (Oepen & Callmeier, 2000), presents annotators, one sentence at a time, with the full set of analyses produced by the grammar. Using a pre-existing tree comparison tool in the LKB (similar in kind to the SRI Cambridge TreeBanker; Carter, 1997), annotators can quickly navigate through the parse forest and identify the correct or preferred analysis in the current context (or, in rare cases, reject all analyses proposed by the grammar). The tree selection tool presents users, who need little expert knowledge of the underlying grammar, with a range of basic properties that distinguish competing analyses and that are relat"
C02-2025,W02-2030,1,0.780762,"ermining the derivations, motivating the use of log-linear models (Agresti, 1990) for parse ranking that Johnson and colleagues further developed (Johnson, Geman, Canon, Chi, & Riezler, 1999). These models can deal with the many interacting dependencies and the structural complexity found in constraint-based or unification-based theories of syntax. Nevertheless, the naive PCFG approach has the advantage of simplicity, so we pursue it and the tagging approach to parse ranking in these proof-of-concept experiments (more recently, we have begun work on building log-linear models over HPSG signs (Toutanova & Manning, 2002)). The learned models were used to rank possible parses of unseen test sentences according to the probabilities they assign to them. We report parse selection performance as percentage of test sentences for which the correct parse was highest ranked by the model. (We restrict attention in the test corpus to sentences that are ambiguous according to the grammar, that is, for which the parse selection task is nontrivial.) We examine four models: an HMM tagging model, a simple PCFG, a PCFG with grandparent annotation, and a hybrid model that combines predictions from the PCFG and the tagger. Thes"
C08-1128,W06-1655,0,0.0211706,"e 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Microsoft Corporation∗ One Microsoft Way Redmond, WA 98052, USA {jfgao,kristout}@microsoft.com frequencies are obtained using manually annotated data. This method is sub-optimal for MT. For example, d(paper) and d(card) can be two words or composed into one word dd(cards). Since d ddoes not exist in the manual lexicon, it cannot be generated by this method. In addition to unigram segmentation, other methods have been proposed. For example, (Gao et al., 2005) described an adaptive CWS system, and (Andrew, 2006) employed a conditional random field model for sequence segmentation. However, these methods are not specifically developed for the MT application, and significant improvements in translation performance need to be shown. In (Xu et al., 2004) and (Xu et al., 2005), word segmentations are integrated into MT systems during model training and translation. We refine the method in training using a Bayesian semisupervised CWS approach motivated by (Goldwater et al., 2006). We describe a generative model which consists of a word model and two alignment models, representing the monolingual and bilingu"
C08-1128,J93-2003,0,0.0237847,"ach table with infinite number of seats (approximately corresponding to Chinese word frequencies). The Dirichlet Process model can be viewed intuitively as a cache model (Goldwater et al., 2006). Each word fj in the corpus is either retrieved from a cache or generated anew given the previously observed words f−j : where the maximization is taken over Chinese word sequences whose character sequence is cK 1 . 2.2 Translation system Once we have segmented the Chinese sentences into words, we train standard alignment models in both directions with GIZA++ (Och and Ney, 2002) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993). Our MT system uses a phrase-based decoder and the log-linear model described in (Zens and Ney, 2004). Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The feature weights are tuned on the development set using a downhill simplex algorithm (Press et al., 2002). The language model is a statistical ngram model estimated using modified Kneser-Ney smoothing. 3 Unigram Dirichlet Process Model for CWS The simplest version of our model is based on"
C08-1128,P02-1038,1,0.192261,"ly corresponding to Chinese word types), each table with infinite number of seats (approximately corresponding to Chinese word frequencies). The Dirichlet Process model can be viewed intuitively as a cache model (Goldwater et al., 2006). Each word fj in the corpus is either retrieved from a cache or generated anew given the previously observed words f−j : where the maximization is taken over Chinese word sequences whose character sequence is cK 1 . 2.2 Translation system Once we have segmented the Chinese sentences into words, we train standard alignment models in both directions with GIZA++ (Och and Ney, 2002) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993). Our MT system uses a phrase-based decoder and the log-linear model described in (Zens and Ney, 2004). Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The feature weights are tuned on the development set using a downhill simplex algorithm (Press et al., 2002). The language model is a statistical ngram model estimated using modified Kneser-Ney smoothing. 3 Unigram Dirichlet Process Model for CWS Th"
C08-1128,P02-1040,0,0.110305,"step is done according to the algorithm in Table 1. Using this algorithm, we obtain a new segmentation of the Chinese data and train the translation models using this segmentation as in the baseline MT system. To segment the test data for translation, we use a unigram model, trained with maximum likelihood estimation off of the final segmentation of the training corpus FT . 6 Translation Experiments We performed experiments using our models for CWS on a large and a small data track. We evaluated performance by measuring WER (word error rate), PER (position-independent word error rate), BLEU (Papineni et al., 2002) and TER (translation error rate) (Snover et al., 2006) using multiple references. 6.1 Translation Task: Large Track NIST We first report the experiments using our monolingual unigram Dirichlet Process model for word segmentation on the NIST machine translation task (NIST, 2005). Because of the computational requirements, we only employed the monolingual word model for this large data track, i.e. the feature weights were λ1 = 1, λ2 = 0, λ3 = 0. Therefore, no alignment information needs to be maintained in this case. The bilingual training corpus is a superset of corpora in the news domain coll"
C08-1128,2006.amta-papers.25,0,0.0127207,"g this algorithm, we obtain a new segmentation of the Chinese data and train the translation models using this segmentation as in the baseline MT system. To segment the test data for translation, we use a unigram model, trained with maximum likelihood estimation off of the final segmentation of the training corpus FT . 6 Translation Experiments We performed experiments using our models for CWS on a large and a small data track. We evaluated performance by measuring WER (word error rate), PER (position-independent word error rate), BLEU (Papineni et al., 2002) and TER (translation error rate) (Snover et al., 2006) using multiple references. 6.1 Translation Task: Large Track NIST We first report the experiments using our monolingual unigram Dirichlet Process model for word segmentation on the NIST machine translation task (NIST, 2005). Because of the computational requirements, we only employed the monolingual word model for this large data track, i.e. the feature weights were λ1 = 1, λ2 = 0, λ3 = 0. Therefore, no alignment information needs to be maintained in this case. The bilingual training corpus is a superset of corpora in the news domain collected from different sources. We took LDC (LDC, 2003) a"
C08-1128,C96-2141,1,0.472239,"mber of seats (approximately corresponding to Chinese word frequencies). The Dirichlet Process model can be viewed intuitively as a cache model (Goldwater et al., 2006). Each word fj in the corpus is either retrieved from a cache or generated anew given the previously observed words f−j : where the maximization is taken over Chinese word sequences whose character sequence is cK 1 . 2.2 Translation system Once we have segmented the Chinese sentences into words, we train standard alignment models in both directions with GIZA++ (Och and Ney, 2002) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993). Our MT system uses a phrase-based decoder and the log-linear model described in (Zens and Ney, 2004). Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The feature weights are tuned on the development set using a downhill simplex algorithm (Press et al., 2002). The language model is a statistical ngram model estimated using modified Kneser-Ney smoothing. 3 Unigram Dirichlet Process Model for CWS The simplest version of our model is based on a unigram Dirichlet Proce"
C08-1128,W04-1118,1,0.723099,"ted data. This method is sub-optimal for MT. For example, d(paper) and d(card) can be two words or composed into one word dd(cards). Since d ddoes not exist in the manual lexicon, it cannot be generated by this method. In addition to unigram segmentation, other methods have been proposed. For example, (Gao et al., 2005) described an adaptive CWS system, and (Andrew, 2006) employed a conditional random field model for sequence segmentation. However, these methods are not specifically developed for the MT application, and significant improvements in translation performance need to be shown. In (Xu et al., 2004) and (Xu et al., 2005), word segmentations are integrated into MT systems during model training and translation. We refine the method in training using a Bayesian semisupervised CWS approach motivated by (Goldwater et al., 2006). We describe a generative model which consists of a word model and two alignment models, representing the monolingual and bilingual information, respectively. In our methods, we first segment Chinese text using a unigram segmenter, and then learn new word types and word distributions, which are suitable for MT. Our experiments on both large (NIST) and small (IWSLT) dat"
C08-1128,2005.iwslt-1.18,1,0.694023,"is sub-optimal for MT. For example, d(paper) and d(card) can be two words or composed into one word dd(cards). Since d ddoes not exist in the manual lexicon, it cannot be generated by this method. In addition to unigram segmentation, other methods have been proposed. For example, (Gao et al., 2005) described an adaptive CWS system, and (Andrew, 2006) employed a conditional random field model for sequence segmentation. However, these methods are not specifically developed for the MT application, and significant improvements in translation performance need to be shown. In (Xu et al., 2004) and (Xu et al., 2005), word segmentations are integrated into MT systems during model training and translation. We refine the method in training using a Bayesian semisupervised CWS approach motivated by (Goldwater et al., 2006). We describe a generative model which consists of a word model and two alignment models, representing the monolingual and bilingual information, respectively. In our methods, we first segment Chinese text using a unigram segmenter, and then learn new word types and word distributions, which are suitable for MT. Our experiments on both large (NIST) and small (IWSLT) data tracks of Chinese-to"
C08-1128,N04-1033,1,0.749729,"ache model (Goldwater et al., 2006). Each word fj in the corpus is either retrieved from a cache or generated anew given the previously observed words f−j : where the maximization is taken over Chinese word sequences whose character sequence is cK 1 . 2.2 Translation system Once we have segmented the Chinese sentences into words, we train standard alignment models in both directions with GIZA++ (Och and Ney, 2002) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993). Our MT system uses a phrase-based decoder and the log-linear model described in (Zens and Ney, 2004). Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The feature weights are tuned on the development set using a downhill simplex algorithm (Press et al., 2002). The language model is a statistical ngram model estimated using modified Kneser-Ney smoothing. 3 Unigram Dirichlet Process Model for CWS The simplest version of our model is based on a unigram Dirichlet Process (DP) model, using only monolingual information. Different from a standard unigram model for CWS, our model can introduce new Chine"
C08-1128,W03-1730,0,0.127363,"Missing"
C08-1128,J05-4005,1,0.845047,"the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Microsoft Corporation∗ One Microsoft Way Redmond, WA 98052, USA {jfgao,kristout}@microsoft.com frequencies are obtained using manually annotated data. This method is sub-optimal for MT. For example, d(paper) and d(card) can be two words or composed into one word dd(cards). Since d ddoes not exist in the manual lexicon, it cannot be generated by this method. In addition to unigram segmentation, other methods have been proposed. For example, (Gao et al., 2005) described an adaptive CWS system, and (Andrew, 2006) employed a conditional random field model for sequence segmentation. However, these methods are not specifically developed for the MT application, and significant improvements in translation performance need to be shown. In (Xu et al., 2004) and (Xu et al., 2005), word segmentations are integrated into MT systems during model training and translation. We refine the method in training using a Bayesian semisupervised CWS approach motivated by (Goldwater et al., 2006). We describe a generative model which consists of a word model and two align"
C08-1128,P06-1085,0,\N,Missing
C08-1128,P02-1017,0,\N,Missing
D09-1125,D08-1011,1,0.954325,"d order is determined by the backbone, and the set of possible words at each position is determined by alignment. Since the space of possible alignments is extremely large, approximate and heuristic techniques have been employed to derive them. In pair-wise alignment, each hypothesis is aligned to the backbone in turn, with separate processing to combine the multiple alignments. Several models have been used for pair-wise alignment, starting with TER and proceeding with more sophisticated techniques such as HMM models, ITG, and IHMM (Rosti et. al 2007a, Matusov et al 2008, Krakos et al. 2008, He et al. 2008). A major problem with such methods is that each hypothesis is aligned to the backbone independently, leading to suboptimal behavior. For example, suppose that we use a state-of-the-art word alignment model for pairs of hypotheses, such as the IHMM. Figure 1 shows likely alignment links between every pair of hypotheses. If Hypothesis 1 is aligned to Hypothesis 2 (the backbone), Jeep is likely to align to SUV because they express similar Chinese content. Hypothesis 3 is separately aligned to the backbone and since the alignment is constrained to be one-to-one, SUV is aligned to SUV and Jeep to"
D09-1125,P02-1040,0,0.110034,"Missing"
D09-1125,P05-3026,0,0.0952961,"al., 2006, He et al 08). Some work has made such decisions in a more principled fashion by computing model-based scores (Matusov et al. 2008), but still specialpurpose algorithms and heuristics are needed and a single alignment is fixed. In our approach, no heuristics are used to convert alignments and no concept of a backbone is used. Instead, the globally highest scoring combination of alignment, order, and lexical choice is selected (subject to search error). Other than confusion-network-based algorithms, work most closely related to ours is the method of MT system combination proposed in (Jayaraman and Lavie 2005), which we will refer to as J&L. Like our method, this approach performs word-level system combination and is not limited to following the word order of a single backbone hypothesis; it also allows more flexibility in the selection of correspondence sets during decoding, compared to a confusionnetwork-based approach. Even though their algorithm and ours are broadly similar, there are several important differences. Firstly, the J&L approach is based on pairwise alignments between words in different hypotheses, which are hard and do not have associated probabilities. Every word in every hypothes"
D09-1125,N07-1029,0,0.499518,"Missing"
D09-1125,P08-2021,0,0.141318,"Missing"
D09-1125,W04-3250,0,0.0577903,"Missing"
D09-1125,P07-1040,0,0.182496,"Missing"
D09-1125,koen-2004-pharaoh,0,0.225819,", ??+1 ∈ ? ) ?=1 |?|−1 The joint decoding framework chooses optimal output according to the following log-linear model: ? ?(?) ?(??,? ? = ??,? ? ) ???? ?, ?, ?, ? = ??? ? ?? , ??+1 ? ?=1 Distortion model: Unlike in the conventional CN-based system combination, flexible orders of CS are allowed in this joint decoding framework. In order to model the distortion of different orderings, a distortion model between two CS is defined as follows: First we define the distortion cost between two words at a single hypothesis. Similarly to the distortion penalty in the conventional phrasebased decoder (Koehn 2004b), the distortion cost of jumping from a word at position i to another word at position j, d(i,j), is proportional to the distance between i and j, e.g., |i-j|. Then, the distortion cost of jumping from one CS, which has a position vector recording the original position of each word in that CS, to another CS is a weighted sum of single-hypothesis-based distortion costs: ? Word posterior model: The word posterior feature is the same as the one proposed by Rosti et. al. (2007a). i.e., ? ??? ?, ?, ?, ? = ??? ? ?? ??? ? =1 where the posterior of a single word in a CS is ?(??? , ??? +1 ) = ?(?) ∙"
D09-1125,W08-0329,0,0.101884,"Hypothesis 2 (the backbone), Jeep is likely to align to SUV because they express similar Chinese content. Hypothesis 3 is separately aligned to the backbone and since the alignment is constrained to be one-to-one, SUV is aligned to SUV and Jeep to an empty word which is inserted after SUV. The network in Figure 2a) is the result of this process. An undesirable property of this CN is that the two instances of Jeep are placed in separate columns and cannot vote to reinforce each other. Incremental alignment methods have been proposed to relax the independence assumption of pair-wise alignment (Rosti et al. 2008, Li et al. 2009). Such methods align hypotheses to a partially constructed CN in some order. For example, if in such method, Hypothesis 3 is first aligned to the backbone, followed by Hypothesis 1, we are likely to arrive at the CN in Figure 2b) in which the two instances of Jeep are aligned. However, if Hypothesis 1 is aligned to the backbone first, we would still get the CN in Figure 2a). Notice that the desirable output “She bought the Jeep SUV” cannot be generated from either of the confusion networks because a rereordering of columns would be required. A common characteristic of CN-based"
D09-1125,N09-2052,1,0.685122,"e not collapsed to be one single candidate since they have different original word positions. We need to trace each of them separately during the decoding process. computed based on a weighted voting score: 4 And the global bi-gram voting feature is defined as: A Joint Optimization Framework For System Combination ? ??,? ? ?? = ? ??,? ? ?? ?1 , … , ?? ? = ?=1 and M is the number of CS generated. Note that M may be larger than the length of the output word sequence w since some CS may generate empty words. Bi-gram voting model: The second feature we used is a bi-gram voting feature proposed by Zhao and He (2009), i.e., for each bi-gram ?? , ??+1 , a weighted position-independent voting score is computed: ? ? ?? , ??+1 ? = ?∗ = argmax ??? ? ∈?,?∈?,?∈? ?? ⋅ ?? (?, ?, ?, ?) ?=1 where we denote by C the set of all possible valid arrangements of CS, O the set of all possible orders of CS, W the set of all possible word sequences, consisting of words from the input hypotheses. {?? (?, ?, ?, ?)} are the features and {?? } are the feature weights in the log-linear model, respectively. 4.1 Features A set of features are used in this framework. Each of them models one or more of the alignment, ordering, and le"
D09-1125,E06-1005,0,0.209061,"Missing"
D09-1125,C08-1074,0,0.00875285,"the joint decoding approach. Some of these features are constant across decoding hypotheses and can be ignored. The non-constant features are word posterior, bigram voting, language model score, and word count. They are computed in the same way as for the joint decoding approach. System weights and feature weights are trained together using Powell's search for the IHMM-based approach. Then the same system weights are applied to both IncHMM and Joint Decoding -based approaches, and the feature weights of them are trained using the max-BLEU training method proposed by Och (2003) and refined by Moore and Quirk (2008). Table 1: Performance of individual systems on the dev and test set System ID System A System B System C System D System E 6.2 dev 32.88 32.82 32.16 31.40 27.44 test 31.81 32.03 31.87 31.32 27.67 Comparison against baselines Table 2 lists the BLEU scores achieved by the two baselines and the joint decoding approach. Both baselines surpass the best individual system 1209 significantly. However, the gain of incremental HMM over IHMM is smaller than that reported in Li et al. (2009). One possible reason of such discrepancy could be that fewer hypotheses are used for combination in this experimen"
D09-1125,D07-1029,0,\N,Missing
D09-1125,W09-0408,0,\N,Missing
D09-1125,W05-0801,0,\N,Missing
D09-1125,W07-0711,1,\N,Missing
D09-1125,2005.eamt-1.20,0,\N,Missing
D09-1125,2008.amta-srw.3,0,\N,Missing
D09-1125,P09-1107,1,\N,Missing
D09-1125,P03-1021,0,\N,Missing
D09-1125,N06-1014,0,\N,Missing
D09-1125,P04-1066,0,\N,Missing
D10-1025,P98-1069,0,0.301965,"Missing"
D10-1025,P08-1088,0,0.0132652,"l., 2009) generalized LDA to tuples of documents from multiple languages. The experiments in section 3 use CL-LSI and an algorithm similar to PLTM as benchmarks. The closest previous work to this paper is the use of Canonical Correlation Analysis (CCA) to find projections for multiple languages whose results are maximally correlated with each other (Vinokourov et al., 2003). PLSA-, LDA-, and CCA-based cross-lingual models have also been trained without the use of parallel or comparable documents, using only knowledge from a translation dictionary to achieve sharing of topics across languages (Haghighi et al., 2008; Jagarlamudi and Daum´e, 2010; Zhang et al., 2010). 252 Such work is complementary to ours and can be used to extend the models to domains lacking parallel documents. Outside of NLP, researchers have designed algorithms to find discriminative projections. We build on the Oriented Principal Component Analysis (OPCA) algorithm (Diamantaras and Kung, 1996), which finds projections that maximize a signal-tonoise ratio (as defined by the user). OPCA has been used to create discriminative features for audio fingerprinting (Burges et al., 2003). 1.2 Structure of paper This paper now presents two alg"
D10-1025,W07-0711,0,0.0109621,"it on the final test set. The regularization γ was tuned for CCA: γ = 10 for Europarl, and γ = 3 for Wikipedia. Figure 3: Mean reciprocal rank versus dimension for Europarl Figure 4: Mean reciprocal rank versus dimension for Wikipedia In the two figures, we evaluate the five projection methods, as well as a word-by-word translation method (denoted by WbW in the graphs). Here “word-by-word” refers to using cosine distance after applying a word-by-word translation model to the Spanish documents. The word-by-word translation model was trained on the Europarl training set, using the WDHMM model (He, 2007), which performs similarly to IBM 258 Model 4. The probability matrix of generating English words from Spanish words was multiplied by each document’s log(tf)-idf vector to produce a translated document vector. We found that multiplying the probability matrix to the log(tf)-idf vector was more accurate on the development set than multiplying the tf vector directly. This vector was either tested as-is, or mapped through LSA learned from the English training set of the corpus. In the figures, the dimensionality of WbW translation refers to the dimensionality of monolingual LSA. The overall order"
D10-1025,D09-1092,0,0.769618,"ors as low-rank Gaussians (Deerwester et al., 1990). Subsequent projection algorithms were based on generative models of individual terms in the documents, including Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Work on cross-lingual projections followed a similar pattern of moving from Gaussian models to term-wise generative models. Cross-language Latent Semantic Indexing (CL-LSI) (Dumais et al., 1997) applied LSA to concatenated comparable documents from multiple languages. Similarly, Polylingual Topic Models (PLTM) (Mimno et al., 2009) generalized LDA to tuples of documents from multiple languages. The experiments in section 3 use CL-LSI and an algorithm similar to PLTM as benchmarks. The closest previous work to this paper is the use of Canonical Correlation Analysis (CCA) to find projections for multiple languages whose results are maximally correlated with each other (Vinokourov et al., 2003). PLSA-, LDA-, and CCA-based cross-lingual models have also been trained without the use of parallel or comparable documents, using only knowledge from a translation dictionary to achieve sharing of topics across languages (Haghighi"
D10-1025,J05-4003,0,0.198249,"Missing"
D10-1025,W02-1011,0,0.0142509,"t pairs to have similar vector representations. We evaluate these algorithms on two tasks: parallel document retrieval for Wikipedia and Europarl documents, and cross-lingual text classification on Reuters. The two discriminative variants, OPCA and CPLSA, significantly outperform their corresponding baselines. The largest differences in performance are observed on the task of retrieval when the documents are only comparable and not parallel. The OPCA method is shown to perform best. 1 • Cross-language text categorization — Applications of text categorization, such as sentiment classification (Pang et al., 2002), are now required to run on multiple languages. Categorization is usually trained on the language of the developer: it needs to be easily extended to other languages. Introduction Given the growth of multiple languages on the Internet, Natural Language Processing must operate on dozens of languages. It is becoming critical that computers reach high performance on the following two tasks: • Comparable and parallel document retrieval — Cross-language information retrieval and text categorization have become important with the growth of the Web (Oard and Diekema, 1998). In addition, machine tran"
D10-1025,P99-1067,0,0.492858,"Missing"
D10-1025,P10-1115,0,0.0709661,"Missing"
D10-1025,1998.amta-tutorials.5,0,\N,Missing
D10-1025,C98-1066,0,\N,Missing
D10-1025,W07-0724,0,\N,Missing
D13-1201,J93-2003,0,0.0480096,"g to compute w0 in the first place is a bit of a disadvantage compared to standard MERT, the need for good initializer is hardly surprising in the context of non-convex optimization. Other non-convex problems in machine learning, such as deep neural networks (DNN) and word alignment models, commonly require such initializers in order to obtain decent performance. In the case of DNN, extensive research is devoted to the problem of finding good initializers.10 In the case of word alignment, it is common practice to initialize search in non-convex optimization problems—such as IBM Model 3 and 4 (Brown et al., 1993)—with solutions of simpler models—such as IBM Model 1. 7 Related work the weights in the context of MERT, (Cer et al., 2008) achieves a related effect. Cer et al.’s goal is to achieve a more regular or smooth objective function, while ours is to obtain a more regular set of parameters. The two approaches may be complementary. More recently, new research has explored direction finding using a smooth surrogate loss function (Flanigan et al., 2013). Although this method is successful in helping MERT find better directions, it also exacerbates the tendency of MERT to overfit.11 As an indirect way"
D13-1201,W08-0304,0,0.721887,"ularized objective function along the line. Finally, we address the issue of searching in a high-dimensional space by using the gradient of expected BLEU (Smith and Eisner, 2006) to find better search directions for our line searches. This direction finder addresses one of the serious concerns raised by Hopkins and May (2011): MERT widely failed to reach the optimum of a synthetic linear objective function. In replicating Hopkins and May’s experiments, we confirm that existing search algorithms for MERT—including coordinate ascent, Powell’s algorithm (Powell, 1964), and random direction sets (Cer et al., 2008)—perform poorly in this experimental condition. However, when using our gradient-based direction finder, MERT has no problem finding the true optimum even in a 1000-dimensional space. Our results suggest that the combination of a regularized objective function and a gradient-informed line search algorithm enables MERT to scale well with a large number of features. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO (Hopkins and May, 2011), a parameter tuning method known to be effective with large feature sets. 2 Unregularized MERT Prior to i"
D13-1201,N12-1047,1,0.910733,"Microsoft Research National Research Council Kristina Toutanova Microsoft Research mgalley@microsoft.com chrisq@microsoft.com kristout@microsoft.com colin.cherry@nrc-cnrc.gc.ca Abstract Secondly, it offers a globally optimal line search. Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization. These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization. In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation. On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method appears at least as good as MERT on small feature se"
D13-1201,N13-1003,1,0.916487,"g to w∗ has a BLEU score of 1, and so that the translation with lowest model score for the sentence gets a BLEU of zero. This normalization has no impact on search, but makes results more interpretable. For our translation experiments, we use multistack phrase-based decoding (Koehn et al., 2007). We report results for two feature sets: non-linear features induced using Gradient Boosting Machines (Toutanova and Ahn, 2013) and sparse lexicalized 7 The objective function remains piecewise constant, and the plateau containing w∗ maps to the optimal value of the function. 1954 reordering features (Cherry, 2013). We exploit these feature sets (GBM and SparseHRM, respectively) in two distinct experimental conditions, which we detail in the two next paragraphs. Both GBM and SparseHRM augment baseline features similar to Moses’: relative frequency and lexicalized phrase translation scores for both translation directions; one or two language model features, depending on the language pair; distortion penalty; word and phrase count; six lexicalized reordering features. For both experimental conditions, phrase tables have maximum phrase length of 7 words on either side. In reference to Table 1, we used the"
D13-1201,D08-1024,0,0.392627,"Missing"
D13-1201,N09-1025,0,0.103202,"Microsoft Research Chris Quirk Colin Cherry Microsoft Research National Research Council Kristina Toutanova Microsoft Research mgalley@microsoft.com chrisq@microsoft.com kristout@microsoft.com colin.cherry@nrc-cnrc.gc.ca Abstract Secondly, it offers a globally optimal line search. Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization. These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization. In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation. On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method app"
D13-1201,J07-2003,0,0.09406,"tion can drive the regularization term down to zero by scaling down w. As special treatments for `2 , we evaluate three linear transforms of the weight vector, where the vector w of the regularization term ||w||22 /2σ 2 is replaced with either: 1. an affine transform: w − w0 2. a vector with only (D − 1) free parameters, e.g., 0 ) (1, w20 , · · · , wD 3. an `1 renormalization: w/||w||1 In (1), regularization is biased towards w0 , a weight vector previously optimized using a competitive yet much smaller feature set, such as core features of a phrase-based (Koehn et al., 2007) or hierarchical (Chiang, 2007) system. The requirement that this feature set be small is to prevent overfitting. Otherwise, any regularization toward an overfit parameter vector w0 would defeat the purpose of introducing a regularization term in the first place.3 In (2), the transformation is motivated by the observation that the D-parameter linear model of Equation 2 only needs (D − 1) degrees of freedom. Fixing one of the components of w to any non-zero constant and allowing the others to vary, the new linear model retains the same modeling power, but the (D − 1) free parameters are no longer scale invariant, i.e., scali"
D13-1201,N13-1025,0,0.320973,"Missing"
D13-1201,D11-1004,1,0.672028,"search towards the greatest increase of expected BLEU score. While our best results are comparable to PRO and not significantly better, we think that this paper provides a deeper understanding of why standard MERT can fail when handling an increasingly larger number of features. Furthermore, this paper complements the analysis by Hopkins and May (2011) of the differences between MERT and optimization with a surrogate loss function. MERT and its extensions have been the target of extensive research (Och, 2003; Macherey et al., 2008; Cer et al., 2008; Moore and Quirk, 2008; Kumar et al., 2009; Galley and Quirk, 2011). More recent work has focused on replacing MERT with a linearly decomposable approximations of the evaluation metric (Smith and Eisner, 2006; Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011; Rosti et al., 2011; Gimpel and Smith, 2012; Cherry and Foster, 2012), which generally involve a surrogate loss function incorporating a regularization term such as the `2 -norm. While we are not aware of any previous work adding a penalty on We thank the anonymous reviewers for their helpful comments and suggestions. 10 For example, (Larochelle et al., 2009) presents"
D13-1201,N12-1023,0,0.787448,"one of the components of w to any non-zero constant and allowing the others to vary, the new linear model retains the same modeling power, but the (D − 1) free parameters are no longer scale invariant, i.e., scaling the (D − 1)-dimensional vector now has an effect on linear model predictions. In (3), the weight vector is normalized as to have an `1 -norm equal to 1. In contrast, the `0 norm is scale insensitive, thus not affected by this problem. 3.1 Exact line search with regularization Optimizing with a regularized error surface requires a change in the line search algorithm presented in 3 (Gimpel and Smith, 2012, footnote 6) briefly mentions the use of such a regularizer with its ramp loss objective function. 1951 Section 2, but the other aspects of MERT remain the same, and we can still use global search algorithms such as coordinate ascent, Powell, and random directions exactly the same way as with unregularized MERT. Line search with a regularization term is still as efficient as in (Och, 2003), and it is still guaranteed to find the optimum of the (now regularized) objective function along the line. Considering again a given point wt and a given direction dt at line search iteration t, finding th"
D13-1201,P12-1031,0,0.190707,"Missing"
D13-1201,D11-1125,0,0.578272,"ris Quirk Colin Cherry Microsoft Research National Research Council Kristina Toutanova Microsoft Research mgalley@microsoft.com chrisq@microsoft.com kristout@microsoft.com colin.cherry@nrc-cnrc.gc.ca Abstract Secondly, it offers a globally optimal line search. Unfortunately, there are several potential difficulties in scaling MERT to larger numbers of features, due to its non-convex loss function and its lack of regularization. These challenges have prompted some researchers to move away from MERT, in favor of linearly decomposable approximations of the evaluation metric (Chiang et al., 2009; Hopkins and May, 2011; Cherry and Foster, 2012), which correspond to easier optimization problems and which naturally incorporate regularization. In particular, recent work (Chiang et al., 2009) has shown that adding thousands or tens of thousands of features can improve MT quality when weights are optimized using a margin-based approximation. On simulated datasets, Hopkins and May (2011) found that conventional MERT struggles to find reasonable parameter vectors, where a smooth loss function based on Pairwise Ranking Optimization (PRO) performs much better; on real data, this PRO method appears at least as good a"
D13-1201,P07-2045,0,0.0817011,"constant intervals of the corpus-level error function, and by selecting the one that has the lowest error count (or, correspondingly, highest BLEU score). Assuming the optimum is found in the interval [γk−1 , γk ], we define γopt = (γk−1 + γk )/2 and change the parameters using the update wt+1 = wt + γopt · dt . Finally, this method is turned into a global Ddimensional search using algorithms that repeatedly use the aforementioned exact line search algorithm. Och (2003) first advocated the use of Powell’s method (Powell, 1964; Press et al., 2007). Pharaoh (Koehn, 2004) and subsequently Moses (Koehn et al., 2007) instead use coordinate ascent, and more recent work often uses random search directions (Cer et al., 2008; Macherey et al., 2008). In Section 4, we will present a novel direction finder for maximum-BLEU optimization, which uses the gradient of expected BLEU to find directions where the BLEU score is most likely to increase. 3 Regularization for MERT Because MERT is prone to overfitting when a large number of parameters must be optimized, we study the addition of a regularization term to the objective function. One conventional approach is to regularize the objective function with a penalty ba"
D13-1201,koen-2004-pharaoh,0,0.526758,"omputed by enumerating all piecewise constant intervals of the corpus-level error function, and by selecting the one that has the lowest error count (or, correspondingly, highest BLEU score). Assuming the optimum is found in the interval [γk−1 , γk ], we define γopt = (γk−1 + γk )/2 and change the parameters using the update wt+1 = wt + γopt · dt . Finally, this method is turned into a global Ddimensional search using algorithms that repeatedly use the aforementioned exact line search algorithm. Och (2003) first advocated the use of Powell’s method (Powell, 1964; Press et al., 2007). Pharaoh (Koehn, 2004) and subsequently Moses (Koehn et al., 2007) instead use coordinate ascent, and more recent work often uses random search directions (Cer et al., 2008; Macherey et al., 2008). In Section 4, we will present a novel direction finder for maximum-BLEU optimization, which uses the gradient of expected BLEU to find directions where the BLEU score is most likely to increase. 3 Regularization for MERT Because MERT is prone to overfitting when a large number of parameters must be optimized, we study the addition of a regularization term to the objective function. One conventional approach is to regular"
D13-1201,P09-1019,0,0.704285,"rches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g., BLEU) instead of some surrogate loss. In this paper, we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search. The idea of adding a regularization term to the MERT objective function can be perplexing at first, because the most common regularizers, such as `1 and `2 , are not directly applicable to MERT. Indeed, these regul"
D13-1201,P06-1096,0,0.129495,"Missing"
D13-1201,C04-1072,0,0.0497267,"our optimizer featuring both regularization and the gradient-based direction finder. All variants of MERT are initialized with a single starting point, which is either uniform weight or w0 . Instead of providing MERT with additional random starting points as in Moses, we use random walks as in (Moore and Quirk, 2008) to attempt to move out of local optima.8 Since PRO and our optimizer have hyperparameters, we use a held-out set (Dev) for adjusting them. For PRO, we adjust three parameters: a regularization penalty for `2 , the parameter α in the add-α smoothed sentence-level version of BLEU (Lin and Och, 2004), and a parameter for scaling the corpus-level length of the references. The latter scaling parameter is discussed in (He and 8 In the case of the gradient-based direction finder, we also use the following strategy whenever optimization converges to a (possibly local) optimum. We run one round of coordinate ascent, and continue with the gradient direction finder as soon as the optimum improves. If the none of the coordinate directions helped, we stop the search. Method MERT MERT MERT PRO `2 MERT (v1: ||w − w0 ||) `2 MERT (v2: D − 1 dimensions) `2 MERT (v3: `1 -renormalized) `0 MERT Starting pt"
D13-1201,D08-1076,0,0.707037,"ed BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g., BLEU) instead of some surrogate loss. In this paper, we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search. The idea of adding a regularization term to the MERT objective function can be perplexing at first, because the most common regularizers, such as `1 and `2 , are not directly app"
D13-1201,C08-1074,1,0.906037,"Finally, Figure 3 shows our rate of convergence compared to coordinate ascent. Our experimental results with the GBM feature set data are shown in Table 2. Each table is divided into three sections corresponding respectively to MERT (Och, 2003) with Koehn-style coordinate ascent (Koehn, 2004), PRO, and our optimizer featuring both regularization and the gradient-based direction finder. All variants of MERT are initialized with a single starting point, which is either uniform weight or w0 . Instead of providing MERT with additional random starting points as in Moses, we use random walks as in (Moore and Quirk, 2008) to attempt to move out of local optima.8 Since PRO and our optimizer have hyperparameters, we use a held-out set (Dev) for adjusting them. For PRO, we adjust three parameters: a regularization penalty for `2 , the parameter α in the add-α smoothed sentence-level version of BLEU (Lin and Och, 2004), and a parameter for scaling the corpus-level length of the references. The latter scaling parameter is discussed in (He and 8 In the case of the gradient-based direction finder, we also use the following strategy whenever optimization converges to a (possibly local) optimum. We run one round of coo"
D13-1201,C12-1121,0,0.0629013,"on the Tune set. For PRO and regularized MERT, we optimized with different hyperparameters (regularization weight, etc.), and retained for each experimental condition the model that worked best on Dev. The table shows the performance of these retained models. 52.6 52.4 BLEU 52.2 52 51.8 51.6 51.4 51.2 1e-05 expected BLEU gradient coordinate ascent 0.0001 0.001 0.01 0.1 regularization weight 1 10 Figure 4: BLEU score on the Finnish Dev set (GBM) with different values for the 1/2σ 2 regularization weight. To enable comparable results, the other hyperparameter (length) is kept fixed. Deng, 2012; Nakov et al., 2012) and addresses the problem that systems tuned with PRO tend to produce sentences that are too short. On the other hand, regularized MERT only requires one hyperparameter to tune: a regularization penalty for `2 or `0 . However, since PRO optimizes translation length on the Dev dataset and MERT does so using the Tune set, a comparison of the two systems would yield a discrepancy in length that would be undesirable. Therefore, we add another hyperparameter to regularized MERT to tune length in the same manner using the Dev set. Table 2 offers several findings. First, unregularized MERT can achie"
D13-1201,P02-1038,0,0.192281,"integrating them during search. To improve search in large parameter spaces, we also present a new direction finding algorithm that uses the gradient of expected BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g., BLEU) instead of some surrogate loss. In this paper, we seek to preserve the advantages of MERT while addressing its shortcomings in terms of regularization and search. The idea of add"
D13-1201,P03-1021,0,0.145765,"s `2 are inapplicable to MERT due to the scale invariance of its objective function, we turn to two regularizers—`0 and a modification of `2 — and present methods for efficiently integrating them during search. To improve search in large parameter spaces, we also present a new direction finding algorithm that uses the gradient of expected BLEU to orient MERT’s exact line searches. Experiments with up to 3600 features show that these extensions of MERT yield results comparable to PRO, a learner often used with large feature sets. 1 Introduction Minimum Error Rate Training emerged a decade ago (Och, 2003) as a superior training method for small numbers of linear model parameters of machine translation systems, improving over prior work using maximum likelihood criteria (Och and Ney, 2002). This technique quickly rose to prominence, becoming standard in many research and commercial MT systems. Variants operating over lattices (Macherey et al., 2008) or hypergraphs (Kumar et al., 2009) were subsequently developed, with the benefit of reducing the approximation error from n-best lists. The primary advantages of MERT are twofold. It directly optimizes the evaluation metric under consideration (e.g"
D13-1201,2001.mtsummit-papers.68,0,0.129733,"umber of parameters must be optimized, we study the addition of a regularization term to the objective function. One conventional approach is to regularize the objective function with a penalty based on the qP 2 Euclidean norm ||w||2 = i wi , also known as `2 regularization. In the case of MERT, this yields the following objective function:2 ˆ = arg min w w X S s=1 ||w||22 E(rs , ˆ e(fs ; w)) + 2σ 2  (4) 1 This assumes that the sufficient statistics of the metric under consideration are additively decomposable by sentence, which is the case with most popular evaluation metrics such as BLEU (Papineni et al., 2001). 2 The `2 regularizer is often used in conjunction with loglikelihood objectives. The regularization term of Equation 4 could similarly be added to the log of an objective—e.g., log(BLEU) instead of BLEU—but we found that the distinction doesn’t have much of an impact in practice. 1950 1.4 1.2 1 0.8 0.6 0.4 0.2 0 -0.2 -0.4 -0.3 -0.2 -0.1 MERT Max at 0.225 × × 0 0.1 0.2 0.3 0.4 1.4 MERT − `2 1.2 Max at -0.018 × 1 −`2 0.8 0.6 × 0.4 0.2 0 -0.2 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 0.4 × 1.4 MERT − `0 1.2 Max at 0 × 1 `0 0.8 0.6 0.4 0.2 0 -0.2 -0.4 -0.3 -0.2 -0.1 0 0.1 0.2 0.3 γ, the step size in the"
D13-1201,W11-2119,0,0.0513815,"larger number of features. Furthermore, this paper complements the analysis by Hopkins and May (2011) of the differences between MERT and optimization with a surrogate loss function. MERT and its extensions have been the target of extensive research (Och, 2003; Macherey et al., 2008; Cer et al., 2008; Moore and Quirk, 2008; Kumar et al., 2009; Galley and Quirk, 2011). More recent work has focused on replacing MERT with a linearly decomposable approximations of the evaluation metric (Smith and Eisner, 2006; Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011; Rosti et al., 2011; Gimpel and Smith, 2012; Cherry and Foster, 2012), which generally involve a surrogate loss function incorporating a regularization term such as the `2 -norm. While we are not aware of any previous work adding a penalty on We thank the anonymous reviewers for their helpful comments and suggestions. 10 For example, (Larochelle et al., 2009) presents a pre-trained DNN that outperforms a shallow network, but the performance of the DNN becomes much worse relative to the shallow network once pre-training is turned off. 11 Indeed, in their Table 3, a comparison between HILS and HOLS suggests tuning"
D13-1201,P06-2101,0,0.503336,"l Language Processing, pages 1948–1959, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics regularization, where we apply `2 regularization to scale-senstive linear transforms of the original linear model. In addition, we introduce efficient methods of incorporating regularization in Och (2003)’s exact line searches. For all of these regularizers, our methods let us find the true optimum of the regularized objective function along the line. Finally, we address the issue of searching in a high-dimensional space by using the gradient of expected BLEU (Smith and Eisner, 2006) to find better search directions for our line searches. This direction finder addresses one of the serious concerns raised by Hopkins and May (2011): MERT widely failed to reach the optimum of a synthetic linear objective function. In replicating Hopkins and May’s experiments, we confirm that existing search algorithms for MERT—including coordinate ascent, Powell’s algorithm (Powell, 1964), and random direction sets (Cer et al., 2008)—perform poorly in this experimental condition. However, when using our gradient-based direction finder, MERT has no problem finding the true optimum even in a 1"
D13-1201,P13-2072,1,0.846348,"ector hs,m . By this linear construction, w∗ is guaranteed to be a global optimum.7 The pseudo-BLEU score is normalized for each M -best list, so that the translation with highest model score according to w∗ has a BLEU score of 1, and so that the translation with lowest model score for the sentence gets a BLEU of zero. This normalization has no impact on search, but makes results more interpretable. For our translation experiments, we use multistack phrase-based decoding (Koehn et al., 2007). We report results for two feature sets: non-linear features induced using Gradient Boosting Machines (Toutanova and Ahn, 2013) and sparse lexicalized 7 The objective function remains piecewise constant, and the plateau containing w∗ maps to the optimal value of the function. 1954 reordering features (Cherry, 2013). We exploit these feature sets (GBM and SparseHRM, respectively) in two distinct experimental conditions, which we detail in the two next paragraphs. Both GBM and SparseHRM augment baseline features similar to Moses’: relative frequency and lexicalized phrase translation scores for both translation directions; one or two language model features, depending on the language pair; distortion penalty; word and p"
D13-1201,D07-1080,0,0.35769,"tanding of why standard MERT can fail when handling an increasingly larger number of features. Furthermore, this paper complements the analysis by Hopkins and May (2011) of the differences between MERT and optimization with a surrogate loss function. MERT and its extensions have been the target of extensive research (Och, 2003; Macherey et al., 2008; Cer et al., 2008; Moore and Quirk, 2008; Kumar et al., 2009; Galley and Quirk, 2011). More recent work has focused on replacing MERT with a linearly decomposable approximations of the evaluation metric (Smith and Eisner, 2006; Liang et al., 2006; Watanabe et al., 2007; Chiang et al., 2008; Hopkins and May, 2011; Rosti et al., 2011; Gimpel and Smith, 2012; Cherry and Foster, 2012), which generally involve a surrogate loss function incorporating a regularization term such as the `2 -norm. While we are not aware of any previous work adding a penalty on We thank the anonymous reviewers for their helpful comments and suggestions. 10 For example, (Larochelle et al., 2009) presents a pre-trained DNN that outperforms a shallow network, but the performance of the DNN becomes much worse relative to the shallow network once pre-training is turned off. 11 Indeed, in t"
D13-1201,P02-1040,0,\N,Missing
D14-1018,islam-mehler-2012-customization,0,0.342427,"ng non-domain-specific bilingual features at the sentence level. In addition to improving accuracy, these fine-grained features may be better able to confirm existing theories or discover new linguistic phenomena that occur in the translation process. We use a fast linear classifier trained with online learning, Vowpal Wabbit (Langford et al., 2007). The Hansard FrenchEnglish dataset (Kurokawa et al., 2009) is used for training and test data in all experiments. sky et al., 2012a; Lembersky et al., 2013; Lembersky et al., 2012b). Few parallel corpora including a customized version of EuroParl (Islam and Mehler, 2012) and a processed version of Hansard (Kurokawa et al., 2009) are labeled for translated versus original text. Using these limited resources, it has been shown that taking the translation direction into account when training a statistical machine translation system can improve translation quality (Lembersky et al., 2013). However, improving statistical machine translation using translation direction information has been limited by several factors. 1. Limited Labeled Data: The amount of labeled data is limited by language and domain and therefore by itself is not enough to make a significant impr"
D14-1018,2005.mtsummit-papers.11,0,0.238909,"Missing"
D14-1018,P11-1132,0,0.260512,"size of a word in the “Translated” section is proportional to the difference between the frequency of the word in original and in the translated text (Fellows, 2013). For example, it is apparent that the word “the” is over-represented in translated English as noted by other research (Volansky et al., 2013). In addition, cohesive markers are clearly more common in translated text. In the past few years there has been work on machine learning techniques for identifying Translationese. Standard machine learning algorithms like SVMs (Baroni and Bernardini, 2006) and Bayesian Logistic Regression (Koppel and Ordan, 2011) have been employed to train classifiers for one of the following tasks: Introduction It has been known for many years in linguistics that translated text has distinct patterns compared to original or authored text (Baker, 1993). The term “Translationese” is often used to refer to the characteristics of translated text. Patterns of Translationese can be categorized as follows (Volansky et al., 2013): 1. Simplification: The process of translation is often coupled with a simplification process at several levels. For example, there tends to be less lexical variety in translated text and rare word"
D14-1018,2009.mtsummit-papers.9,0,0.88001,"nslated text (Baroni and Bernardini, 2006). While many characteristics of translated text are more apparent in comparison to the original text, most of the prior research has focused on monolingual features of translated and original text. The contribution of this work is introducing bilingual features that are capable of explaining differences in translation direction using localized linguistic phenomena at the phrase or sentence level, rather than using monolingual statistics at the document level. We show that these bilingual features outperform the monolingual features used in prior work (Kurokawa et al., 2009) for the task of classifying translation direction. 1 Kristina Toutanova Microsoft Research Redmond, WA kristout@microsoft.com In Figure 1 the size of a word in the “Translated” section is proportional to the difference between the frequency of the word in original and in the translated text (Fellows, 2013). For example, it is apparent that the word “the” is over-represented in translated English as noted by other research (Volansky et al., 2013). In addition, cohesive markers are clearly more common in translated text. In the past few years there has been work on machine learning techniques f"
D14-1018,E12-1026,0,0.0606931,"mitations, in this work we focus on improving sentence-level classification accuracy by using non-domain-specific bilingual features at the sentence level. In addition to improving accuracy, these fine-grained features may be better able to confirm existing theories or discover new linguistic phenomena that occur in the translation process. We use a fast linear classifier trained with online learning, Vowpal Wabbit (Langford et al., 2007). The Hansard FrenchEnglish dataset (Kurokawa et al., 2009) is used for training and test data in all experiments. sky et al., 2012a; Lembersky et al., 2013; Lembersky et al., 2012b). Few parallel corpora including a customized version of EuroParl (Islam and Mehler, 2012) and a processed version of Hansard (Kurokawa et al., 2009) are labeled for translated versus original text. Using these limited resources, it has been shown that taking the translation direction into account when training a statistical machine translation system can improve translation quality (Lembersky et al., 2013). However, improving statistical machine translation using translation direction information has been limited by several factors. 1. Limited Labeled Data: The amount of labeled data is lim"
D14-1018,J12-4004,0,0.409839,"mitations, in this work we focus on improving sentence-level classification accuracy by using non-domain-specific bilingual features at the sentence level. In addition to improving accuracy, these fine-grained features may be better able to confirm existing theories or discover new linguistic phenomena that occur in the translation process. We use a fast linear classifier trained with online learning, Vowpal Wabbit (Langford et al., 2007). The Hansard FrenchEnglish dataset (Kurokawa et al., 2009) is used for training and test data in all experiments. sky et al., 2012a; Lembersky et al., 2013; Lembersky et al., 2012b). Few parallel corpora including a customized version of EuroParl (Islam and Mehler, 2012) and a processed version of Hansard (Kurokawa et al., 2009) are labeled for translated versus original text. Using these limited resources, it has been shown that taking the translation direction into account when training a statistical machine translation system can improve translation quality (Lembersky et al., 2013). However, improving statistical machine translation using translation direction information has been limited by several factors. 1. Limited Labeled Data: The amount of labeled data is lim"
D14-1018,J13-4007,0,0.197715,"y2 Motivated by these limitations, in this work we focus on improving sentence-level classification accuracy by using non-domain-specific bilingual features at the sentence level. In addition to improving accuracy, these fine-grained features may be better able to confirm existing theories or discover new linguistic phenomena that occur in the translation process. We use a fast linear classifier trained with online learning, Vowpal Wabbit (Langford et al., 2007). The Hansard FrenchEnglish dataset (Kurokawa et al., 2009) is used for training and test data in all experiments. sky et al., 2012a; Lembersky et al., 2013; Lembersky et al., 2012b). Few parallel corpora including a customized version of EuroParl (Islam and Mehler, 2012) and a processed version of Hansard (Kurokawa et al., 2009) are labeled for translated versus original text. Using these limited resources, it has been shown that taking the translation direction into account when training a statistical machine translation system can improve translation quality (Lembersky et al., 2013). However, improving statistical machine translation using translation direction information has been limited by several factors. 1. Limited Labeled Data: The amoun"
D14-1018,J93-2004,0,0.0456,"ion filter5 and length ratio filter to clean the data. After filtering we were left with 1,890,603 English-French sentence pairs and 640,117 French-English sentence pairs. The Stanford POS tagger (Toutanova and Manning, 2000) was used to tag the English and the French sides of the corpus. The HMM alignment model (Vogel et al., 1996) trained on 4 A character n-gram language model is used to detect the language of source and target side text and filter them out if they do not match their annotated language. 5 Duplicate sentences pairs are filtered out. 6 For description of English POS tags see (Marcus et al., 1993) and (Abeill´e et al., 2003) for French 162 POS MTU (E⇒F) FE# EF# Example NNPS⇒(N,C) 336 12 quebecers(NNPS) ⇒ qu´eb´ecoises(N) et(C) des qu´eb´ecois IN⇒(CL,V) 69 1027 a few days ago(IN) ⇒ il y(CL) a(V) quelques PRP⇒(N,V) 18 663 he(PRP) is ⇒ le d´eput´e(N) a` (V) (NNP,POS)⇒A 155 28 quebec(NNP) ’s(POS) history ⇒ histoire qu´eb´ecoises(A) (FW,FW)⇒ADV 7 195 pro(FW) bono(FW) work ⇒ b´en´evolement(ADV) travailler (RB,MD)⇒V 2 112 money alone(RB) could(MD) solve ⇒ argent suffirait(V) a` r´esoudre 1 2 3 4 5 6 Table 1: POS MTU features with highest weight. FE# indicates the number of times this feature"
D14-1018,N06-1002,0,0.130257,") as well as HMM distortion (word position difference between the target of a link and the target of the previous link). We bin the distortions into three bins: “= 0”, “&gt; 0” and “&lt; 0”, to reduce sparsity. Bilingual Features for Translation Direction Classification We are interested in learning common localized linguistic phenomena that occur during the translation process when translating in one direction but not the other. 3.1 Distortion POS Tag MTUs Minimal translation units (MTUs) for a sentence pair are defined as pairs of source and target word sets that satisfy the following conditions (Quirk and Menezes, 2006). 1. No alignment links between distinct MTUs. 2. MTUs are not decomposable into smaller MTUs without violating the previous rule. We use POS tags to capture linguistic structures and MTUs to map linguistic structures of 4 Experimental Setup For the translation direction detection task explained in section 1, we use a fast linear classifier trained with online learning, Vowpal Wabbit (Langford et al., 2007). Training data and classification features are explained in section 4.1 and 4.2. 3 Only replacing content words with their POS tags while leaving function words as is. 161 Figure 4: Sentenc"
D14-1018,W00-1308,0,0.224179,"the advantage of bilingual features over prior work using only a union of monolingual features (reproduced by the “English-POS + French-POS” configuration). While higher order features generally show better in-domain accuracy, the advantage of low-order bilingual features might be even higher in cross-domain classification. Preprocessing and Feature Extraction We used a language filter4 , deduplication filter5 and length ratio filter to clean the data. After filtering we were left with 1,890,603 English-French sentence pairs and 640,117 French-English sentence pairs. The Stanford POS tagger (Toutanova and Manning, 2000) was used to tag the English and the French sides of the corpus. The HMM alignment model (Vogel et al., 1996) trained on 4 A character n-gram language model is used to detect the language of source and target side text and filter them out if they do not match their annotated language. 5 Duplicate sentences pairs are filtered out. 6 For description of English POS tags see (Marcus et al., 1993) and (Abeill´e et al., 2003) for French 162 POS MTU (E⇒F) FE# EF# Example NNPS⇒(N,C) 336 12 quebecers(NNPS) ⇒ qu´eb´ecoises(N) et(C) des qu´eb´ecois IN⇒(CL,V) 69 1027 a few days ago(IN) ⇒ il y(CL) a(V) que"
D14-1018,C96-2141,0,\N,Missing
D14-1018,D11-1034,0,\N,Missing
D15-1174,D14-1165,0,0.0272354,"follows: ef (es ,r,eo ;Θ) f (es ,r,e0 ;Θ) e0 ∈N eg(es ,r,?) e p(eo |es , r; Θ) = P Conditional probabilities for subject entities p(es |eo , r; Θ) are defined analogously. Here Θ denotes all the parameters of latent features. The denominator is defined using a set of entities that do not fill the object position in any relation triple (es , r, ?) in the training knowledge graph. Since the number of such entities is impractically large, we sample negative triples from the full set. We also limit the candidate entities to ones that have types consistent with the position in the relation triple (Chang et al., 2014; Yang et al., 2015), where the types are approximated following Toutanova and Chen (2015). Additionally, since the task of predicting textual relations is auxiliary to the main task, we use a weighting factor τ for the loss on predicting the arguments of textual relations (Toutanova and Chen, 2015). Denote T as a set of triples, we define the loss L(T ; Θ) as: X L(T ; Θ) = − log p(eo |es , r; Θ) (es ,r,eo )∈T − log p(es |eo , r; Θ) (es ,r,eo )∈T Let TKB and Ttext represent the set of knowledge base triples and textual relation triples respectively. The final training loss function is de2 All"
D15-1174,D13-1080,0,0.025555,"from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textua"
D15-1174,D14-1044,0,0.362479,"ss of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations. Weston et"
D15-1174,D15-1205,0,0.0233037,"use scoring function f (es , r, eo ) to represent the model’s confidence in the existence of the triple. We present the models and then the loss function used to train Continuous representations for supervised relation extraction In contrast to the work reviewed so far, work on sentence-level relation extraction using direct supervision has focused heavily on representing sentence context. Models using hand-crafted features have evolved for more than a decade, and recently, models using continuous representations have been found to achieve new state-of-the-art performance (Zeng et al., 2014; Gormley et al., 2015). Compared to work on representation learning for sentence-level context, such as this recent work using LSTM models on constituency or dependency trees (Tai et al., 2015), our approach using a one-hidden-layer convolutional neural network is relatively simple. However, even such a simple approach has been shown to be very competitive (Kim, 2014). 3 1501 nsubj r tween entities and the subject and object positions of relations. For each relation type r, the model learns two latent feature vectors v(rs ) and v(ro ) of dimension K. For each entity (node) ei , the model also learns a latent featur"
D15-1174,P11-1055,0,0.224699,"of textual relations. 1 http://lemurproject.org/clueweb12/ FACC1/ Relation extraction using distant supervision A number of works have focused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base. Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context. Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of te"
D15-1174,D14-1181,0,0.00486836,"Missing"
D15-1174,D11-1049,0,0.132637,"ithout requiring sentence-level annotations of textual mentions at training time. We group such related work into three groups based on whether KB, text, or both sources of information are used. Additionally, we discuss related work in the area of supervised relation extraction using continuous representations of text, even though we do not use supervision at the level of textual mentions. Knowledge base completion Nickel et al. (2015) provide a broad overview of machine learning models for knowledge graphs, including models based on observed graph features such as the path ranking algorithm (Lao et al., 2011), models based on continuous representations (latent features), and model combinations (Dong et al., 2014). These models predict new facts in a given knowledge base, based on information from existing entities and relations. From this line of work, most relevant to our study is prior work evaluating continuous representation models on the FB15k dataset. Yang et al. (2015) showed that a simple variant of a bilinear model D IST M ULT outperformed T RANS E (Bordes et al., 2013) and more richly parameterized models on this dataset. We therefore build upon the best performing prior model D IST M UL"
D15-1174,D12-1093,0,0.0611575,"iables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the"
D15-1174,P09-1113,0,0.533576,"ed models on this dataset. We therefore build upon the best performing prior model D IST M ULT from this line of work, as well as additional models E and F developed in the context of text-augmented knowledge graphs (Riedel et al., 2013), and extend them to incorporate compositional representations of textual relations. 1 http://lemurproject.org/clueweb12/ FACC1/ Relation extraction using distant supervision A number of works have focused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base. Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context. Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowled"
D15-1174,P15-1016,0,0.389139,"rmation was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations. Weston et al. (2013) combined continuous representations from a knowledge base and textual mentions for prediction of new relations. The two representations were trained independently of each other and using different loss functions, and were only combined at inference time. Additionally, the employed representations of text were non-composit"
D15-1174,N13-1008,0,0.290368,"of Text and Knowledge Bases Kristina Toutanova Microsoft Research Redmond, WA, USA Danqi Chen∗ Computer Science Department Stanford University Patrick Pantel Microsoft Research Redmond, WA, USA Hoifung Poon Microsoft Research Redmond, WA, USA Pallavi Choudhury Microsoft Research Redmond, WA, USA Michael Gamon Microsoft Research Redmond, WA, USA Abstract Knowledge Base Models that learn to represent textual and knowledge base relations in the same continuous latent space are able to perform joint inferences among the two kinds of relations and obtain high accuracy on knowledge base completion (Riedel et al., 2013). In this paper we propose a model that captures the compositional structure of textual relations, and jointly optimizes entity, knowledge base, and textual relation representations. The proposed model significantly improves performance over a model that does not share parameters among textual relations with common sub-structure. 1 place_of_birth Honolulu Barack Obama city_of United States nationality Textual Mentions Barack Obama is the 44th and current President of United States. Obama was born in the United States just as he has always said. … ClueWeb Figure 1: A knowledge base fragment cou"
D15-1174,Q13-1030,0,0.00682238,"cused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base. Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context. Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or cont"
D15-1174,D12-1042,0,0.309075,"1 http://lemurproject.org/clueweb12/ FACC1/ Relation extraction using distant supervision A number of works have focused on extracting new instances of relations using information from textual mentions, without sophisticated modeling of prior knowledge from the knowledge base. Mintz et al. (2009) demonstrated that both surface context and dependency path context were helpful for the task, but did not model the compositional sub-structure of this context. Other work proposed more sophisticated models that reason about sentence-level hidden variables (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) or model the noise arising from the incompleteness of knowledge bases and text collections (Ritter et al., 2013), inter alia. Our work focuses on representing the compositional structure of sentential context for learning joint continuous representations of text and knowledge bases. Combining knowledge base and text information A combination of knowledge base and textual information was first shown to outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising"
D15-1174,P15-1150,0,0.0210913,"uous representations for supervised relation extraction In contrast to the work reviewed so far, work on sentence-level relation extraction using direct supervision has focused heavily on representing sentence context. Models using hand-crafted features have evolved for more than a decade, and recently, models using continuous representations have been found to achieve new state-of-the-art performance (Zeng et al., 2014; Gormley et al., 2015). Compared to work on representation learning for sentence-level context, such as this recent work using LSTM models on constituency or dependency trees (Tai et al., 2015), our approach using a one-hidden-layer convolutional neural network is relatively simple. However, even such a simple approach has been shown to be very competitive (Kim, 2014). 3 1501 nsubj r tween entities and the subject and object positions of relations. For each relation type r, the model learns two latent feature vectors v(rs ) and v(ro ) of dimension K. For each entity (node) ei , the model also learns a latent feature vector of the same dimensionality. The score of a candidate triple (es , r, eo ) is defined as f (es , r, eo ) = v(rs ) |v(es ) + v(ro ) |v(eo ). It can be seen that whe"
D15-1174,W15-4007,1,0.610739,"odel this sub-structure and share parameters among related dependency paths, using a unified loss function learning entity and relation representations to maximize performance on the knowledge base link prediction task. We evaluate our approach on the FB15k-237 dataset, a knowledge base derived from the Free1499 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1499–1509, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. base subset FB15k (Bordes et al., 2013) and filtered to remove highly redundant relations (Toutanova and Chen, 2015). The knowledge base is paired with textual mentions for all entity pairs derived from ClueWeb121 with Freebase entity mention annotations (Gabrilovich et al., 2013). We show that using a convolutional neural network to derive continuous representations for textual relations boosts the overall performance on link prediction, with larger improvement on entity pairs that have textual mentions. 2 Related Work There has been a growing body of work on learning to predict relations between entities without requiring sentence-level annotations of textual mentions at training time. We group such relat"
D15-1174,P10-1040,0,0.0350795,"rformance. L-BFGS (Liu and Nocedal, 1989) and RProp (Riedmiller and Braun, 1993) were found to converge to similar function values, with RProp converging significantly faster. We thus used RProp for optimization. We initialized the KB+text models from the KB-only models and also from random initial values (sampled from a Gaussian distribution), and stopped optimization when the overall MRR on the validation set decreased. For each model type, we chose the better of random and KB-only initialization. The word embeddings in the C ONV models were initialized using the 50-dimensional vectors from Turian et al. (2010) in the main experiments, with a slight positive impact. The effect of initialization is discussed at the end of the section. The number of negative examples for each triple was set to 200. Performance improved substantially when the number of negative examples was increased and reached a plateau around 200. We chose the optimal number of latent feature dimensions via a grid search to optimize MRR on the validation set, testing the values 5, 10, 15, 35, 50, 100, 200 and 500. We also performed a grid search over the values of the parameter τ , testing values in the set {0.01, 0.1, 0.25, 0.5, 1}"
D15-1174,D14-1167,0,0.541781,"outperform either source alone in the framework of path-ranking algorithms in a combined knowledge base and text graph (Lao et al., 2012). To alleviate the sparsity of textual relations arising in such a combined graph, (Gardner et al., 2013; Gardner et al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations. Weston et al. (2013) combined continuous representations from a knowledge base and textual mentions for prediction of new relations. The two representations were trained independently of each other and using different loss functions, and were only combined at inference time. Additionally, the employed representations of text were non-compositional. In this work"
D15-1174,D13-1136,0,0.0553672,"al., 2014) showed how to incorporate clusters or continuous representations of textual relations. Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure. Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015). Wang et al. (2014a) combined knowledge base and text information by embedding knowledge base entities and the words in their names in the same vector space, but did not model the textual cooccurrences of entity pairs and the expressed textual relations. Weston et al. (2013) combined continuous representations from a knowledge base and textual mentions for prediction of new relations. The two representations were trained independently of each other and using different loss functions, and were only combined at inference time. Additionally, the employed representations of text were non-compositional. In this work we train continuous representations of knowledge base and textual relations jointly, which allows for deeper interactions between the 1500 sources of information. We directly build on the universal schema approach of Riedel et al. (2013) as well as the uni"
D15-1174,C14-1220,0,0.0333623,"triple. The models use scoring function f (es , r, eo ) to represent the model’s confidence in the existence of the triple. We present the models and then the loss function used to train Continuous representations for supervised relation extraction In contrast to the work reviewed so far, work on sentence-level relation extraction using direct supervision has focused heavily on representing sentence context. Models using hand-crafted features have evolved for more than a decade, and recently, models using continuous representations have been found to achieve new state-of-the-art performance (Zeng et al., 2014; Gormley et al., 2015). Compared to work on representation learning for sentence-level context, such as this recent work using LSTM models on constituency or dependency trees (Tai et al., 2015), our approach using a one-hidden-layer convolutional neural network is relatively simple. However, even such a simple approach has been shown to be very competitive (Kim, 2014). 3 1501 nsubj r tween entities and the subject and object positions of relations. For each relation type r, the model learns two latent feature vectors v(rs ) and v(ro ) of dimension K. For each entity (node) ei , the model also"
D16-1033,W00-1401,0,0.151229,"al., 2012). A news-domain parallel sentence corpus containing 1,496 parallel examples has been culled from multireference Chinese-English translations by Ganitkevitch et al. (2011). The only publicly-available manually-created abstractive compression corpus is that described by Cohn and Lapata (2008), which comprises 575 single-reference sentence pairs. Automatic metrics: Early automatic metrics for evaluation of compressions include success rate (Jing, 2000), defined as accuracy of individual word or constituent deletion decisions; Simple String Accuracy (string edit distance), introduced by Bangalore et al. (2000) for natural language generation tasks; and Word Accuracy (Chiori and Furui, 2004), which generalizes Bangalore et al. (2000) to multiple references. Riezler et al. (2003) introduced the use of F-measure over grammatical relations. Word unigram and word-bigram F-measure have also been used (Unno et al., 2006; Filippova et al., 2015). Variants of ROUGE (Lin, 2004), used for summarization evaluation, have also been applied to sentence compressions (Rush et al., 2015). Riezler et al. (2003) show that F-measure over grammatical relations agrees with human ratings on the relative ranking of three s"
D16-1033,P11-1049,0,0.0139397,"urnals, and technical documents sampled from the Open American National Corpus (OANC1 ). • Each source text is accompanied by up to five crowd-sourced rewrites constrained to a preset compression ratio and annotated with quality judgments. Multiple rewrites permit study of the impact of operations on human compression quality and facilitate automatic evaluation. Introduction Automated sentence compression condenses a sentence or paragraph to its most important content in order to enhance writing quality, meet document length constraints, and build more accurate document summarization systems (Berg-Kirkpatrick et al., 2011; Vanderwende et al., 2007). Though word deletion is extensively used (e.g., (Clarke and Lapata, 2008)), state-of-the-art compression models (Cohn and Lapata, 2008; Rush et al., 2015) benefit crucially from data that can represent complex abstractive compression operations, including substitution of words and phrases and reordering. ∗ This research was conducted during the author’s internship at Microsoft Research. • This dataset is the first to provide compressions at the multi-sentence (two-sentence paragraph) level, which may present a stepping stone to whole document summarization. Many of"
D16-1033,briscoe-carroll-2002-robust,0,0.0155411,"metrics based on surface uni-grams (LR -1), bi-grams (LR -2), tri-grams (LR -3), and four-grams (LR -4), as well skip bi-grams (with a maximum of four intervening words as in ROUGE-S4) (SKIP -2), and dependency tree triples obtained from collapsed dependencies output from the Stanford parser (PARSE 2).7 The second criterion is the scoring measure used to evaluate the match between two sets of linguistic units corresponding to a system output and a reference compression. We compare Precision, Recall, F-measure, and Precision+Brevity penalty (as 7 Clarke and Lapata (2006) used the RASP parser (Briscoe and Carroll, 2002), but we expect that the Stanford parser is similarly robust and would lead to similar correlations. in BLEU). The third criterion is whether multiple references or a single reference is used, and in the case of multiple references, the method used to aggregate information from multiple references. We investigate two previously applied methods and introduce a novel approach that often outperforms the standard methods. To illustrate, we introduce some notation and use a simple example. Consider a sub-phrase of one of the sentences in Table 1, think about your household, as an input text to comp"
D16-1033,P06-1048,0,0.0963999,"matically-mined deletion corpora are single-reference and have varying (uncontrolled) compression rates. Knight and Marcu (2002) automatically mined a small parallel corpus (1,035 training and 32 test sentences) by aligning abstracts to sentences in articles. Filippova and Altun (2013) extracted deletion-based compressions by aligning news headlines to first sentences, yielding a corpus of 250,000 parallel sentences. The same approach was used by Filippova et al. (2015) to create a set of 2M sentence pairs. Only a subset of 10,000 parallel sentences from the latter has been publicly released. Clarke and Lapata (2006) and Clarke and Lapata (2008) provide two manually-created two-reference corpora for deletion-based compression:2 their sizes are 1,370 and 1,433 sentences, respectively. 2 http://jamesclarke.net/research/ resources 341 Abstractive compression corpora: Rush et al. (2015) have mined 4 million compression pairs from news articles and released their code to extract data from the Annotated Gigaword (Napoles et al., 2012). A news-domain parallel sentence corpus containing 1,496 parallel examples has been culled from multireference Chinese-English translations by Ganitkevitch et al. (2011). The only"
D16-1033,C08-1018,0,0.127462,"to a preset compression ratio and annotated with quality judgments. Multiple rewrites permit study of the impact of operations on human compression quality and facilitate automatic evaluation. Introduction Automated sentence compression condenses a sentence or paragraph to its most important content in order to enhance writing quality, meet document length constraints, and build more accurate document summarization systems (Berg-Kirkpatrick et al., 2011; Vanderwende et al., 2007). Though word deletion is extensively used (e.g., (Clarke and Lapata, 2008)), state-of-the-art compression models (Cohn and Lapata, 2008; Rush et al., 2015) benefit crucially from data that can represent complex abstractive compression operations, including substitution of words and phrases and reordering. ∗ This research was conducted during the author’s internship at Microsoft Research. • This dataset is the first to provide compressions at the multi-sentence (two-sentence paragraph) level, which may present a stepping stone to whole document summarization. Many of these two-sentence paragraphs are compressed both as paragraphs and separately sentence-bysentence, offering data that may yield insights into the impact of multi"
D16-1033,D13-1155,0,0.101382,"tituent type. Jing and McKeown (1999) identified abstractive operations (other than word deletion) employed by professional writers, including paraphrasing and re-ordering of phrases, and merging and reordering sentences, but did not quantify their impact on compression quality. Deletion-based compression corpora: Currently available automatically-mined deletion corpora are single-reference and have varying (uncontrolled) compression rates. Knight and Marcu (2002) automatically mined a small parallel corpus (1,035 training and 32 test sentences) by aligning abstracts to sentences in articles. Filippova and Altun (2013) extracted deletion-based compressions by aligning news headlines to first sentences, yielding a corpus of 250,000 parallel sentences. The same approach was used by Filippova et al. (2015) to create a set of 2M sentence pairs. Only a subset of 10,000 parallel sentences from the latter has been publicly released. Clarke and Lapata (2006) and Clarke and Lapata (2008) provide two manually-created two-reference corpora for deletion-based compression:2 their sizes are 1,370 and 1,433 sentences, respectively. 2 http://jamesclarke.net/research/ resources 341 Abstractive compression corpora: Rush et a"
D16-1033,P15-2073,1,0.283431,"Missing"
D16-1033,D11-1108,0,0.0317678,"Missing"
D16-1033,N15-1072,0,0.041093,"Missing"
D16-1033,D15-1013,0,0.189886,"r grammatical relations agrees with human ratings on the relative ranking of three systems at the corpus level. Clarke and Lapata (2006) evaluate two deletion-based automatic compression systems against a deletion-based gold-standard on sets of 20 sentences. Parse-based F-1 was shown to have high sentence-level Pearson’s ρ correlation with human judgments of overall quality, and to have higher ρ than Simple String Accuracy. Napoles et al. (2011) have pointed to the need of multiple references and studies of evaluation metrics. For the related tasks of document and multidocument summarization, Graham (2015) provides a fine-grained comparison of automated evaluation methods. However, to the best of our knowledge, no studies of automatic evaluation metrics exist for abstractive compression of shorter texts. Length 1-Sent Source Ref-1 2-Sent Ref-2 Source Text Think of all the ways everyone in your household will benefit from your membership in Audubon. Imagine how your household will benefit from your Audubon membership. Ref-1 Everyone in your household will benefit from membership in Audubon. Will the administration live up to its environmental promises? Can we save the last of our ancient forests"
D16-1033,ide-etal-2008-masc,0,0.0437419,"Missing"
D16-1033,P10-2013,0,0.0508029,"Missing"
D16-1033,A00-1043,0,0.0856246,"l. (2015) have mined 4 million compression pairs from news articles and released their code to extract data from the Annotated Gigaword (Napoles et al., 2012). A news-domain parallel sentence corpus containing 1,496 parallel examples has been culled from multireference Chinese-English translations by Ganitkevitch et al. (2011). The only publicly-available manually-created abstractive compression corpus is that described by Cohn and Lapata (2008), which comprises 575 single-reference sentence pairs. Automatic metrics: Early automatic metrics for evaluation of compressions include success rate (Jing, 2000), defined as accuracy of individual word or constituent deletion decisions; Simple String Accuracy (string edit distance), introduced by Bangalore et al. (2000) for natural language generation tasks; and Word Accuracy (Chiori and Furui, 2004), which generalizes Bangalore et al. (2000) to multiple references. Riezler et al. (2003) introduced the use of F-measure over grammatical relations. Word unigram and word-bigram F-measure have also been used (Unno et al., 2006; Filippova et al., 2015). Variants of ROUGE (Lin, 2004), used for summarization evaluation, have also been applied to sentence com"
D16-1033,P03-1054,0,0.0242237,"graph input/output pairs split into sentences by heuristically aligning source to target sentences in the paragraphs. T3: We use the authors’ implementation of the tree transducer system described in Cohn and Lapata (2008). T3 similarly requires sentence-level input/output pairs, but can also learn from abstractive compressions. We thus used a larger set of approximately 28,000 examples (single sentences with abstractive compressions taken directly from the data or as a result of heuristic sentence-level alignment of two-sentence paragraphs). We obtained parse trees using the Stanford parser (Klein and Manning, 2003), and used Jacana (Yao et al., 2013) for word alignment. The performance obtained by T3 in our experiments is substantially weaker (relative to ILP) than that reported in prior work (Cohn and Lapata, 2008). We therefore interpret this system output solely as data for evaluating automatic metrics. NAMAS: We run the publicly available implementation of NAMAS12 with the settings described by Rush et al. (2015). We modified the beam search algorithm to produce output with a compression ratio similar to that of the human references, since this ratio is a large factor in compression quality (Napoles"
D16-1033,W04-1013,0,0.043578,"ly automatic metrics for evaluation of compressions include success rate (Jing, 2000), defined as accuracy of individual word or constituent deletion decisions; Simple String Accuracy (string edit distance), introduced by Bangalore et al. (2000) for natural language generation tasks; and Word Accuracy (Chiori and Furui, 2004), which generalizes Bangalore et al. (2000) to multiple references. Riezler et al. (2003) introduced the use of F-measure over grammatical relations. Word unigram and word-bigram F-measure have also been used (Unno et al., 2006; Filippova et al., 2015). Variants of ROUGE (Lin, 2004), used for summarization evaluation, have also been applied to sentence compressions (Rush et al., 2015). Riezler et al. (2003) show that F-measure over grammatical relations agrees with human ratings on the relative ranking of three systems at the corpus level. Clarke and Lapata (2006) evaluate two deletion-based automatic compression systems against a deletion-based gold-standard on sets of 20 sentences. Parse-based F-1 was shown to have high sentence-level Pearson’s ρ correlation with human judgments of overall quality, and to have higher ρ than Simple String Accuracy. Napoles et al. (2011)"
D16-1033,W11-1611,0,0.209708,"Missing"
D16-1033,W12-3018,0,0.037565,"Missing"
D16-1033,Q16-1005,0,0.0328221,"Missing"
D16-1033,N03-1026,0,0.127073,"(2011). The only publicly-available manually-created abstractive compression corpus is that described by Cohn and Lapata (2008), which comprises 575 single-reference sentence pairs. Automatic metrics: Early automatic metrics for evaluation of compressions include success rate (Jing, 2000), defined as accuracy of individual word or constituent deletion decisions; Simple String Accuracy (string edit distance), introduced by Bangalore et al. (2000) for natural language generation tasks; and Word Accuracy (Chiori and Furui, 2004), which generalizes Bangalore et al. (2000) to multiple references. Riezler et al. (2003) introduced the use of F-measure over grammatical relations. Word unigram and word-bigram F-measure have also been used (Unno et al., 2006; Filippova et al., 2015). Variants of ROUGE (Lin, 2004), used for summarization evaluation, have also been applied to sentence compressions (Rush et al., 2015). Riezler et al. (2003) show that F-measure over grammatical relations agrees with human ratings on the relative ranking of three systems at the corpus level. Clarke and Lapata (2006) evaluate two deletion-based automatic compression systems against a deletion-based gold-standard on sets of 20 sentenc"
D16-1033,D15-1044,0,0.719526,"n ratio and annotated with quality judgments. Multiple rewrites permit study of the impact of operations on human compression quality and facilitate automatic evaluation. Introduction Automated sentence compression condenses a sentence or paragraph to its most important content in order to enhance writing quality, meet document length constraints, and build more accurate document summarization systems (Berg-Kirkpatrick et al., 2011; Vanderwende et al., 2007). Though word deletion is extensively used (e.g., (Clarke and Lapata, 2008)), state-of-the-art compression models (Cohn and Lapata, 2008; Rush et al., 2015) benefit crucially from data that can represent complex abstractive compression operations, including substitution of words and phrases and reordering. ∗ This research was conducted during the author’s internship at Microsoft Research. • This dataset is the first to provide compressions at the multi-sentence (two-sentence paragraph) level, which may present a stepping stone to whole document summarization. Many of these two-sentence paragraphs are compressed both as paragraphs and separately sentence-bysentence, offering data that may yield insights into the impact of multi-sentence operations"
D16-1033,P06-2109,0,0.0309074,"ses 575 single-reference sentence pairs. Automatic metrics: Early automatic metrics for evaluation of compressions include success rate (Jing, 2000), defined as accuracy of individual word or constituent deletion decisions; Simple String Accuracy (string edit distance), introduced by Bangalore et al. (2000) for natural language generation tasks; and Word Accuracy (Chiori and Furui, 2004), which generalizes Bangalore et al. (2000) to multiple references. Riezler et al. (2003) introduced the use of F-measure over grammatical relations. Word unigram and word-bigram F-measure have also been used (Unno et al., 2006; Filippova et al., 2015). Variants of ROUGE (Lin, 2004), used for summarization evaluation, have also been applied to sentence compressions (Rush et al., 2015). Riezler et al. (2003) show that F-measure over grammatical relations agrees with human ratings on the relative ranking of three systems at the corpus level. Clarke and Lapata (2006) evaluate two deletion-based automatic compression systems against a deletion-based gold-standard on sets of 20 sentences. Parse-based F-1 was shown to have high sentence-level Pearson’s ρ correlation with human judgments of overall quality, and to have hig"
D16-1033,Q16-1029,0,0.0351268,"ls are evaluated on the test set portion of our dataset. All models use the training portion of the data for training, and two models (Seq2Seq and NAMAS10 ) additionally use external training data. The external data is summarized in Table 7. The Gigaword set was extracted from the Annotated Gigaword (Napoles et al., 2012), using the implementation provided by Rush et al. (2015). The Headline data was extracted in similar fashion using an in-house news collection. 9 A similar insight was used in one of the component metrics of the SARI evaluation metric used for text simplification evaluation (Xu et al., 2016). 10 The original works introducing these models employed much larger training corpora, believed to be key to improving the accuracy of neutral network models with large parameter spaces. Data Abstractive Deletion-based Gigaword Headline Gigaword Headline #src tokens 114.1M 6.0M 1,353K 59K #trg tokens 30.0M 1.4M 329K 11K #sents 3.6M 0.2M 47K 2K Table 7: External data statistics. ILP: We use an open-source implementation11 of the semi-supervised ILP model described in (Clarke and Lapata, 2008). The model uses a trigram language model trained on a 9 million token subset of the OANC corpus. The I"
D16-1033,P11-1019,0,0.0125601,"ter if they compressed each sentence in a sequence independently. 15 This method has been standard for ROUGE, but has not for BLEU . We find that averaging sentence-level metrics is also advantageous for BLEU . 14 System T3 NAMAS Seq2Seq ILP Meaning 1.14 1.56 1.64 2.28 Grammar 1.40 1.30 1.51 2.22 Combined 1.26 1.43 1.57 2.25 4.3.2 Table 8: Average human ratings of system outputs for meaning and grammar separately and in combination. between dependent Pearson correlations with human judgments (Williams, 1959) as recommended for summarization evaluation (Graham, 2015) and other NLP tasks (e.g. (Yannakoudakis et al., 2011)). 4.3.1 Corpus-level metrics Table 8 shows the average human ratings of the four systems, separately in meaning and grammar, as well as the combined measure (an arithmetic mean of meaning and grammar judgments). Even though the performance of some systems is similar, the differences between all pairs of systems in meaning and grammar are significant p &lt; 0.0001 according to a paired t-test. It is interesting to note that ILP outperforms the more recently developed neural network systems Seq2Seq and NAMAS. This might seem to contradict recent results showing that the new models are superior to"
D16-1033,P13-2123,0,0.15085,"Missing"
D16-1033,D15-1042,0,\N,Missing
J08-2002,P98-1013,0,0.3465,"Missing"
J08-2002,W04-2412,0,0.0487494,"Missing"
J08-2002,W05-0620,0,0.4847,"Missing"
J08-2002,A00-2018,0,0.874707,"the February 2004 data. For the February 2004 data, we used the standard split into training, development, and test sets—the annotations from sections 02–21 formed the training set, section 24 the development, and section 23 the test set. The set of argument labels considered is the set of core argument labels (ARG0 through ARG5) plus the modiﬁer labels (see Figure 1). The training set contained 85,392 propositions, the test set 4,615, and the development set 2,626. We evaluate semantic role labeling models on gold-standard parse trees and parse trees produced by Charniak’s automatic parser (Charniak 2000). For gold-standard parse trees, we preprocess the trees to discard empty constituents and strip functional tags. Using the trace information provided by empty constituents is very useful for improving performance (Palmer, Gildea, and Kingsbury 2005; Pradhan, Ward et al. 2005), but we have not used this information so that we can compare our results to previous work and since automatic systems that recover it are not widely available. 3.2 Evaluation Measures Since 2004, there has been a precise, standard evaluation measure for semantic role labeling, formulated by the organizers of the CoNLL s"
J08-2002,P05-1022,0,0.0428178,"f forward quotes.8 We ﬁrst present results of our local and joint model using the parses provided as part of the CoNLL 2005 data (and having wrong forward quotes) in Figure 18. We then report results from the same local and joint model, and the joint model using the top ﬁve Charniak parses, where the parses have correct representation of the forward quotes in Figure 19. For these results we used the version of the Charniak parser from 4 May 2005. The results were very similar to the results we obtained with the version from 18 March 2005. We did not experiment with the new re-ranking model of Charniak and Johnson (2005), even though it improves upon Charniak (2000) signiﬁcantly. For comparison, the system we submitted to CoNLL 2005 had an F-Measure of 78.45 on the WSJ Test set. The winning system (Punyakanok, Roth, and Yih 2005) had an F-Measure of 79.44 and our current system has an F-Measure of 80.32. For the Brown Test set, our submitted version had an F-Measure of 67.71, the winning system had 67.75, and our current system has 68.81. Figure 20 shows the per-label performance of our joint model using the top ﬁve Charniak parse trees on the Test WSJ test set. The columns show the Precision, Recall, F-Measu"
J08-2002,W05-0622,0,0.524278,"this role. We propose such a model, with a very rich graphical model structure, which is globally conditioned on the observation (the parse tree).2 Such a model is formally a Conditional Random Field (CRF) (Lafferty, McCallum, and Pereira 2001). However, note that in practice this term has previously been used almost exclusively to describe the restricted case of linear chain Conditional Markov Random Fields (sequence models) (Lafferty, McCallum, and Pereira 2001; Sha and Pereira 2003), or at least models that have strong Markov properties, which allow efﬁcient dynamic programming algorithms (Cohn and Blunsom 2005). Instead, we consider a densely connected CRF structure, with no Markov properties, and use approximate inference by re-ranking the n-best solutions of a simpler model with stronger independence assumptions (for which exact inference is possible). Such a rich graphical model can represent many dependencies but there are two dangers—one is that the computational complexity of training the model and searching for the most likely labeling given the tree can be prohibitive, and the other is that if too many dependencies are encoded, the model will over-ﬁt the training data and will not generalize"
J08-2002,W06-1673,1,0.810223,"Missing"
J08-2002,J02-3001,0,0.938356,"y motivated dependencies in features over sets of the random variables. Our re-ranking approach, like the approach to parse re-ranking of Collins (2000), employs a simpler model—a local semantic role labeling algorithm—as a ﬁrst pass to generate a set of n likely complete assignments of labels to all parse tree nodes. The joint model is restricted to these n assignments and does not have to search the exponentially large space of all possible joint labelings. 2. Related Work There has been a substantial amount of work on automatic semantic role labeling, starting with the statistical model of Gildea and Jurafsky (2002). Researchers have worked on deﬁning new useful features, and different system architectures and models. Here we review the work most closely related to ours, concentrating on methods for incorporating joint information and for increasing robustness to parser error. 2 That is, it deﬁnes a conditional distribution of labels of all nodes given the parse tree. 163 Computational Linguistics Volume 34, Number 2 2.1 Methods for Incorporating Joint Information Gildea and Jurafsky (2002) propose a method to model global dependencies by including a probability distribution over multi-sets of semantic r"
J08-2002,W05-0623,1,0.666119,"Missing"
J08-2002,P04-1042,1,0.876043,"Missing"
J08-2002,H05-1081,0,0.104916,"Missing"
J08-2002,J05-1004,0,0.561776,"Missing"
J08-2002,N04-1030,0,0.662826,"We use log-linear models for multi-class classiﬁcation for the local models. Because they produce probability distributions, identiﬁcation and classiﬁcation models can be chained in a principled way, as in Equation (1). The baseline features we used for the local identiﬁcation and classiﬁcation models are outlined in Figure 3. These features are a subset of the features used in previous work. The standard features at the top of the ﬁgure were deﬁned by Gildea and Jurafsky (2002), and the rest are other useful lexical and structural features identiﬁed in more recent work (Surdeanu et al. 2003; Pradhan et al. 2004; Xue and Palmer 2004). We also incorporated several novel features which we describe next. Figure 3 Baseline features. 172 Toutanova, Haghighi, and Manning A Global Joint Model for SRL Figure 4 Example of displaced arguments. 4.1 Additional Features for Displaced Constituents We found that a large source of errors for ARG0 and ARG1 stemmed from cases such as those illustrated in Figure 4, where arguments were dislocated by raising or control verbs. Here, the predicate, expected, does not have a subject in the typical position— indicated by the empty NP—because the auxiliary is has raised the"
J08-2002,P05-1072,0,0.0975432,"guments. For instance, the dependency between the meal and the children for the sentence in example (4) will not be captured because these phrases are not in the same local tree according to Penn Treebank syntax. 2.2 Increasing Robustness to Parser Error There have been multiple approaches to reducing the sensitivity of semantic role labeling systems to syntactic parser error. Promising approaches have been to consider multiple syntactic analyses—the top k parses from a single or multiple full parsers (Punyakanok, Roth, and Yih 2005), or a shallow parse and a full parse (M`arquez et al. 2005; Pradhan et al. 2005), or several types of full syntactic parses (Pradhan, Ward et al. 2005). Such techniques are important for achieving good performance: The top four systems in the CoNLL 2005 shared task competition all used multiple syntactic analyses (Carreras and M`arquez 2005). These previous methods develop special components to combine the labeling decisions obtained using different syntactic annotation. The method of Punyakanok, Roth, and Yih (2005) uses ILP to derive a consistent set of arguments, each of which could be derived using a different parse tree. Pradhan, Ward et al. (2005) use stacking to tr"
J08-2002,W05-0639,0,0.0394384,"Missing"
J08-2002,W04-2421,0,0.290116,"ns over a baseline system using only the features of Gildea and Jurafsky (2002). The performance gain due to joint information over a system using all features was not reported. The joint information captured by this model is limited by the n-gram Markov assumption of the language model over labels. In our work, we improve the modeling of joint dependencies by looking at longer-distance context, by deﬁning richer features over the sequence of labels and input features, and by estimating the model parameters discriminatively. A system which can integrate longer-distance dependencies is that of Punyakanok et al. (2004) and Punyakanok, Roth, and Yih (2005). The idea is to build a semantic role labeling system that is based on local classiﬁers but also uses a global component that ensures that several linguistically motivated global constraints on argument frames are satisﬁed. The constraints are categorical and speciﬁed by hand. For example, one global constraint is that the argument phrases cannot overlap—that is, if a node is labeled with a non-NONE label, all of its descendants have to be labeled NONE. The proposed framework is integer linear programming (ILP), which makes it possible to ﬁnd the most like"
J08-2002,N03-1028,0,0.0262055,". To estimate the probability that a certain node gets the role AGENT, we need to know if any of the other nodes were labeled with this role. We propose such a model, with a very rich graphical model structure, which is globally conditioned on the observation (the parse tree).2 Such a model is formally a Conditional Random Field (CRF) (Lafferty, McCallum, and Pereira 2001). However, note that in practice this term has previously been used almost exclusively to describe the restricted case of linear chain Conditional Markov Random Fields (sequence models) (Lafferty, McCallum, and Pereira 2001; Sha and Pereira 2003), or at least models that have strong Markov properties, which allow efﬁcient dynamic programming algorithms (Cohn and Blunsom 2005). Instead, we consider a densely connected CRF structure, with no Markov properties, and use approximate inference by re-ranking the n-best solutions of a simpler model with stronger independence assumptions (for which exact inference is possible). Such a rich graphical model can represent many dependencies but there are two dangers—one is that the computational complexity of training the model and searching for the most likely labeling given the tree can be prohi"
J08-2002,P03-1002,0,0.297495,"izer of Equation (1). We use log-linear models for multi-class classiﬁcation for the local models. Because they produce probability distributions, identiﬁcation and classiﬁcation models can be chained in a principled way, as in Equation (1). The baseline features we used for the local identiﬁcation and classiﬁcation models are outlined in Figure 3. These features are a subset of the features used in previous work. The standard features at the top of the ﬁgure were deﬁned by Gildea and Jurafsky (2002), and the rest are other useful lexical and structural features identiﬁed in more recent work (Surdeanu et al. 2003; Pradhan et al. 2004; Xue and Palmer 2004). We also incorporated several novel features which we describe next. Figure 3 Baseline features. 172 Toutanova, Haghighi, and Manning A Global Joint Model for SRL Figure 4 Example of displaced arguments. 4.1 Additional Features for Displaced Constituents We found that a large source of errors for ARG0 and ARG1 stemmed from cases such as those illustrated in Figure 4, where arguments were dislocated by raising or control verbs. Here, the predicate, expected, does not have a subject in the typical position— indicated by the empty NP—because the auxilia"
J08-2002,W04-3212,0,0.834943,"They can be obtained by ﬁrst mapping all non-core argument labels in the guessed and correct labelings to NONE. Coarse Modiﬁer Argument Measures (C OARSE ARGM). Sometimes it is sufﬁcient to know a given span has a modiﬁer role, without knowledge of the speciﬁc role label. In addition, deciding exact modiﬁer argument labels was one of the decisions with highest disagreement among annotators (Palmer, Gildea, and Kingsbury 2005). To estimate performance under this setting, we relabel all ARGM - X arguments to ARGM in the proposed and correct labeling. Such a performance measure was also used by Xue and Palmer (2004). Note that these measures do not exclude the core arguments but instead consider the core plus a coarse version of the modiﬁer arguments. Thus for C OARSE ARGM 169 Computational Linguistics Volume 34, Number 2 ALL we count {0} as a true positive span, {1, 2} , {3, 4}, and {7, 8, 9} as false positive, and {1, 2, 3, 4} and {7, 8, 9} as false negative. Identiﬁcation Measures (I D). These measure how well we do on the ARG vs. NONE distinction. For the purposes of this evaluation, all spans labeled with a non-NONE label are considered to have the generic label ARG. For example, to compute CORE I D"
J08-2002,C98-1013,0,\N,Missing
K15-1036,N10-1083,0,0.0546078,"Missing"
K15-1036,W06-2920,0,0.00708715,"nd a weight vector λ. We use the feature set described in (Li et al., 2012): transition features, word-tag features (hyi , xi i) (lowercased words with frequency greater than a threshold), whether the word contains a hyphen and/or starts with a capital letter, character suffixes, and whether the word contains a digit. We initialize the transition and emission distributions of the HMM using unambiguous words as proposed by (Zhang and DeNero, 2014). Data. We use the Danish, Dutch, German, Greek, English, Italian, Portuguese, Spanish and Swedish datasets from CoNLL-X and CoNLL-2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). We map the POS labels in the CoNLL datasets to the universal POS tagset (Petrov et al., 2012). We use the tag dictionaries provided by Li et al. (2012). Model configurations. For each c(x, ym ) × 1(x ∈ lexdev , ygold ∈ lexdev [x]) |lexdev [x]| We then compute the expected accuracy as evaldevlex = ED [1(ygold = ym )], and select the hyperparameter configuration m ˆ which maximizes this criterion, then re-train the model with the full lexicon lex.1 3 Experiments How to best use a small labeled set TL ? Several prior works used a labeled set for supervised hyper-parameter s"
K15-1036,D14-1096,0,0.024493,"Missing"
K15-1036,N13-1014,0,0.0498354,"Missing"
K15-1036,S14-1001,0,0.0579129,"Missing"
K15-1036,D12-1127,0,0.0269241,"Missing"
K15-1036,P09-1113,0,0.140129,"Missing"
K15-1036,P11-2009,0,0.0646388,"Missing"
K15-1036,Q13-1001,0,0.0343057,"Missing"
K15-1036,P10-2039,0,0.0899185,"e-level supervision, for POS tagging, super sense tagging, NER, and relation extraction (Craven et al., 1999; Smith and Eisner, 2005; Carlson et al., 2009; Mintz et al., 2009; Johannsen et al., 2014), inter alia, focussing on parametric forms and loss functions for model training. However, there has been little research on the practically important aspect of model selection for type-supervised learning. While some previous work used criteria based on the type-level supervision only (Smith and Eisner, 2005; Goldwater and Griffiths, 2007), much prior work used a labeled set for model selection (Vaswani et al., 2010; Soderland and Weld, 2014). We are not aware of any prior work aiming to compare or improve existing type-supervised model selection criteria. For POS tagging, there is also work on using both type-level supervision from lexicons, and projection from another language (T¨ackstr¨om et al., 2013). Methods for training with a small labeled set have also been developed (Søgaard, 2011; Garrette and Baldridge, 2013; Duong et al., 2014), but there have not been studies on the utility of a small labeled set for model selection versus model training. Our contributions are: 1) a simple and generally app"
K15-1036,petrov-etal-2012-universal,0,0.0180167,", xi i) (lowercased words with frequency greater than a threshold), whether the word contains a hyphen and/or starts with a capital letter, character suffixes, and whether the word contains a digit. We initialize the transition and emission distributions of the HMM using unambiguous words as proposed by (Zhang and DeNero, 2014). Data. We use the Danish, Dutch, German, Greek, English, Italian, Portuguese, Spanish and Swedish datasets from CoNLL-X and CoNLL-2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). We map the POS labels in the CoNLL datasets to the universal POS tagset (Petrov et al., 2012). We use the tag dictionaries provided by Li et al. (2012). Model configurations. For each c(x, ym ) × 1(x ∈ lexdev , ygold ∈ lexdev [x]) |lexdev [x]| We then compute the expected accuracy as evaldevlex = ED [1(ygold = ym )], and select the hyperparameter configuration m ˆ which maximizes this criterion, then re-train the model with the full lexicon lex.1 3 Experiments How to best use a small labeled set TL ? Several prior works used a labeled set for supervised hyper-parameter selection even when only type-level supervision is assumed to be available for training (Vaswani et al., 2010; Soderl"
K15-1036,P05-1044,0,0.303908,"n University Redmond, WA, USA Abstract form displayed in Fig. 1 are available for many more languages, either in commercial dictionaries or community created resources such as Wiktionary. Tag dictionaries provide type-level supervision for word types in the lexicon. Similarly, while sentences labeled with named entities are scarce, gazetteers and databases are more readily available (Bollacker et al., 2008). There has been substantial research on how best to build models using such type-level supervision, for POS tagging, super sense tagging, NER, and relation extraction (Craven et al., 1999; Smith and Eisner, 2005; Carlson et al., 2009; Mintz et al., 2009; Johannsen et al., 2014), inter alia, focussing on parametric forms and loss functions for model training. However, there has been little research on the practically important aspect of model selection for type-supervised learning. While some previous work used criteria based on the type-level supervision only (Smith and Eisner, 2005; Goldwater and Griffiths, 2007), much prior work used a labeled set for model selection (Vaswani et al., 2010; Soderland and Weld, 2014). We are not aware of any prior work aiming to compare or improve existing type-super"
K15-1036,P14-2132,0,0.0164611,"ultinomial distribution for the local emissions and transitions, this model uses a log-linear distribution (i.e., p(xi |yi ) ∝ exp λ> f (xi , yi )) with a feature vector f and a weight vector λ. We use the feature set described in (Li et al., 2012): transition features, word-tag features (hyi , xi i) (lowercased words with frequency greater than a threshold), whether the word contains a hyphen and/or starts with a capital letter, character suffixes, and whether the word contains a digit. We initialize the transition and emission distributions of the HMM using unambiguous words as proposed by (Zhang and DeNero, 2014). Data. We use the Danish, Dutch, German, Greek, English, Italian, Portuguese, Spanish and Swedish datasets from CoNLL-X and CoNLL-2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). We map the POS labels in the CoNLL datasets to the universal POS tagset (Petrov et al., 2012). We use the tag dictionaries provided by Li et al. (2012). Model configurations. For each c(x, ym ) × 1(x ∈ lexdev , ygold ∈ lexdev [x]) |lexdev [x]| We then compute the expected accuracy as evaldevlex = ED [1(ygold = ym )], and select the hyperparameter configuration m ˆ which maximizes this criterion, then"
K15-1036,P07-1094,0,\N,Missing
K15-1036,D14-1203,0,\N,Missing
K15-1036,D07-1096,0,\N,Missing
L16-1106,de-marneffe-etal-2006-generating,0,0.138525,"Missing"
L16-1106,P12-2047,0,0.0582166,"Missing"
N03-1033,A00-1031,0,0.11107,"Missing"
N03-1033,P98-1029,0,0.133187,"Missing"
N03-1033,J95-4004,0,0.136762,"2 ), or centered tags (t−1 , t0 , t+1 ) respectively. Again, with roughly equivalent feature sets, the left context is better than the right, and the centered context is better than either unidirectional context. line for this task high, while substantial annotator noise creates an unknown upper bound on the task. 3.2 Lexicalization Lexicalization has been a key factor in the advance of statistical parsing models, but has been less exploited for tagging. Words surrounding the current word have been occasionally used in taggers, such as (Ratnaparkhi, 1996), Brill’s transformation based tagger (Brill, 1995), and the HMM model of Lee et al. (2000), but nevertheless, the only lexicalization consistently included in tagging models is the dependence of the part of speech tag of a word on the word itself. In maximum entropy models, joint features which look at surrounding words and their tags, as well as joint features of the current word and surrounding words are in principle straightforward additions, but have not been incorporated into previous models. We have found these features to be very useful. We explore here lexicalization both alone and in combination with preceding and following tag histo"
N03-1033,A88-1019,0,0.138214,"Missing"
N03-1033,W02-1001,0,0.159577,"ned to maximize the conditional likelihood over the training data of that node. At test time, the sequence with the highest product of local conditional scores is calculated and returned. We can always find the exact maximizing sequence, but only in the case of an acyclic net is it guaranteed to be the maximum likelihood sequence. 3 Experiments The part of speech tagged data used in our experiments is the Wall Street Journal data from Penn Treebank III (Marcus et al., 1994). We extracted tagged sentences from the parse trees.5 We split the data into training, development, and test sets as in (Collins, 2002). Table 1 lists character5 Note that these tags (and sentences) are not identical to those obtained from the tagged/pos directories of the same disk: hundreds of tags in the RB/RP/IN set were changed to be more consistent in the parsed/mrg version. Maybe we were the last to discover this, but we’ve never seen it in print. istics of the three splits.6 Except where indicated for the model BEST, all results are on the development set. One innovation in our reporting of results is that we present whole-sentence accuracy numbers as well as the traditional per-tag accuracy measure (over all tokens,"
N03-1033,P99-1069,0,0.144081,"Missing"
N03-1033,W02-1002,1,0.136285,"he tag at a certain position, the obvious thing to do is to explicitly include in the local model all predictive features, no matter on which side of the target position they lie. There are two good formal reasons to expect that a model explicitly conditioning on both sides at each position, like figure 1(c) could be advantageous. First, because of smoothing effects and interaction with other conditioning features (like the words), left-to-right factors like P(t0 |t−1 , w0 ) do not always suffice when t0 is implicitly needed to determine t−1 . For example, consider a case of observation bias (Klein and Manning, 2002) for a first-order left-toright CMM. The word to has only one tag (TO) in the PTB tag set. The TO tag is often preceded by nouns, but rarely by modals (MD). In a sequence will to fight, that trend indicates that will should be a noun rather than a modal verb. However, that effect is completely lost in a CMM like (a): P(twill |will, hstar ti) prefers the modal tagging, and P(TO |to, twill ) is roughly 1 regardless of twill . While the model has an arrow between the two tag positions, that path of influence is severed.3 The same problem exists in the other direction. If we use the symmetric righ"
N03-1033,P00-1034,0,0.118545,"Missing"
N03-1033,W96-0213,0,0.139267,"one wants to find a coherent sentence interpretation). Further, while some tag errors matter much more than others, to a first cut getting a single tag wrong in many of the more common ways (e.g., proper noun vs. common noun; noun vs. verb) would lead to errors in a subsequent processor such as an information extraction system or a parser that would greatly degrade results for the entire sentence. Finally, the fact that the measure has much more dynamic range has some appeal when reporting tagging results. The per-state models in this paper are log-linear models, building upon the models in (Ratnaparkhi, 1996) and (Toutanova and Manning, 2000), though some models are in fact strictly simpler. The features in the models are defined using templates; there are different templates for rare words aimed at learning the correct tags for unknown words.7 We present the results of three classes of experiments: experiments with directionality, experiments with lexicalization, and experiments with smoothing. 3.1 Experiments with Directionality In this section, we report experiments using log-linear CMMs to populate nets with various structures, exploring the relative value of neighboring words’ tags. Table 2 l"
N03-1033,P99-1023,0,0.086251,"Missing"
N03-1033,W00-1308,1,0.229218,"erent sentence interpretation). Further, while some tag errors matter much more than others, to a first cut getting a single tag wrong in many of the more common ways (e.g., proper noun vs. common noun; noun vs. verb) would lead to errors in a subsequent processor such as an information extraction system or a parser that would greatly degrade results for the entire sentence. Finally, the fact that the measure has much more dynamic range has some appeal when reporting tagging results. The per-state models in this paper are log-linear models, building upon the models in (Ratnaparkhi, 1996) and (Toutanova and Manning, 2000), though some models are in fact strictly simpler. The features in the models are defined using templates; there are different templates for rare words aimed at learning the correct tags for unknown words.7 We present the results of three classes of experiments: experiments with directionality, experiments with lexicalization, and experiments with smoothing. 3.1 Experiments with Directionality In this section, we report experiments using log-linear CMMs to populate nets with various structures, exploring the relative value of neighboring words’ tags. Table 2 lists the discussed networks. All n"
N03-1033,W99-0606,1,\N,Missing
N03-1033,J93-2004,0,\N,Missing
N03-1033,C98-1029,0,\N,Missing
N06-5006,D07-1002,0,\N,Missing
N06-5006,N07-1069,0,\N,Missing
N06-5006,A00-2031,0,\N,Missing
N06-5006,copestake-flickinger-2000-open,0,\N,Missing
N06-5006,W04-2401,0,\N,Missing
N06-5006,Y01-1001,0,\N,Missing
N06-5006,N06-1055,0,\N,Missing
N06-5006,N03-2009,0,\N,Missing
N06-5006,C04-1100,0,\N,Missing
N06-5006,D07-1062,0,\N,Missing
N06-5006,W05-0628,0,\N,Missing
N06-5006,J08-2003,0,\N,Missing
N06-5006,C02-1064,0,\N,Missing
N06-5006,W03-1008,0,\N,Missing
N06-5006,W06-1617,0,\N,Missing
N06-5006,W04-3212,0,\N,Missing
N06-5006,W05-0639,0,\N,Missing
N06-5006,N07-1070,0,\N,Missing
N06-5006,W05-0623,1,\N,Missing
N06-5006,W06-1619,0,\N,Missing
N06-5006,C04-1197,0,\N,Missing
N06-5006,W03-1006,0,\N,Missing
N06-5006,H90-1020,0,\N,Missing
N06-5006,W05-0625,0,\N,Missing
N06-5006,H94-1039,0,\N,Missing
N06-5006,W06-1601,0,\N,Missing
N06-5006,N06-1056,0,\N,Missing
N06-5006,P06-2113,0,\N,Missing
N06-5006,P07-1025,0,\N,Missing
N06-5006,J93-4001,0,\N,Missing
N06-5006,D07-1071,0,\N,Missing
N06-5006,P05-1072,0,\N,Missing
N06-5006,J05-1003,0,\N,Missing
N06-5006,P05-1011,0,\N,Missing
N06-5006,P07-1121,0,\N,Missing
N06-5006,N06-1024,0,\N,Missing
N06-5006,W05-0620,0,\N,Missing
N06-5006,P05-1073,1,\N,Missing
N06-5006,W05-0622,0,\N,Missing
N06-5006,P06-2057,0,\N,Missing
N06-5006,P02-1035,0,\N,Missing
N06-5006,P03-1002,0,\N,Missing
N06-5006,W05-0602,0,\N,Missing
N06-5006,J02-3001,0,\N,Missing
N06-5006,W05-0634,0,\N,Missing
N06-5006,P02-1031,0,\N,Missing
N06-5006,P06-1117,0,\N,Missing
N06-5006,P06-1132,1,\N,Missing
N06-5006,J05-1004,0,\N,Missing
N06-5006,P96-1008,0,\N,Missing
N06-5006,P07-1027,0,\N,Missing
N06-5006,N04-1030,0,\N,Missing
N06-5006,N01-1025,0,\N,Missing
N06-5006,P06-2010,0,\N,Missing
N07-1007,P05-1066,0,0.0163684,"0.92 BLEU points improvement over the baseline. Considering more case variations (20 or 40), and more SMT candidates (100) resulted in a similar but slightly lower performance in BLEU. This is presumably because the case model does affect the choice of content words as well, but this influence is limited and can be best captured when using a small number (n=20) of baseline system candidates. Based on these results on the dev-1K set, we chose the best model (i.e., 20-best-10case) and evaluated it on the test-2K set against the baseline. Using the pair-wise statistical test design described in Collins et al. (2005), the BLEU improvement (35.53 vs. 36.29) was statistically significant (p < .01) according to the Wilcoxon signed-rank test. 5.3 Human evaluation These results demonstrate that the proposed model is effective at improving the translation quality according to the BLEU score. In this section, we report the results of human evaluation to ensure that the improvements in BLEU lead to better translations according to human evaluators. We performed human evaluation on the 20best-10case (n=20, k=10) and 1best-40case (n=1, k=40) models against the baseline using our final test set, the test-2K data. Th"
N07-1007,P05-1033,0,0.0204136,"n) Introduction Generation of grammatical elements such as inflectional endings and case markers is an important component technology for machine translation (MT). Statistical machine translation (SMT) systems, however, have not yet successfully incorporated components that generate grammatical elements in the target language. Most stateof-the-art SMT systems treat grammatical elements in exactly the same way as content words, and rely on general-purpose phrasal translations and target language models to generate these elements (e.g., Och and Ney, 2002; Koehn et al., 2003; Quirk et al., 2005; Chiang, 2005; Galley et al., 2006). However, since these grammatical elements in the target language often correspond to long-range dependencies and/or do not have any words corresponding in the source, they may be difficult to model, and the output of an SMT system is often ungrammatical. For example, Figure 1 shows an output from our baseline English-to-Japanese SMT system on a sentence from a computer domain. The SMT system, trained on this domain, produces a natural lexical translation for the English word patch as correction program, and translates replace In this paper, we explore the use of a stati"
N07-1007,P06-1121,0,0.0268297,"n Generation of grammatical elements such as inflectional endings and case markers is an important component technology for machine translation (MT). Statistical machine translation (SMT) systems, however, have not yet successfully incorporated components that generate grammatical elements in the target language. Most stateof-the-art SMT systems treat grammatical elements in exactly the same way as content words, and rely on general-purpose phrasal translations and target language models to generate these elements (e.g., Och and Ney, 2002; Koehn et al., 2003; Quirk et al., 2005; Chiang, 2005; Galley et al., 2006). However, since these grammatical elements in the target language often correspond to long-range dependencies and/or do not have any words corresponding in the source, they may be difficult to model, and the output of an SMT system is often ungrammatical. For example, Figure 1 shows an output from our baseline English-to-Japanese SMT system on a sentence from a computer domain. The SMT system, trained on this domain, produces a natural lexical translation for the English word patch as correction program, and translates replace In this paper, we explore the use of a statistical model for case"
N07-1007,N03-1017,0,0.00589951,"; O: output of MT; C: correct translation) Introduction Generation of grammatical elements such as inflectional endings and case markers is an important component technology for machine translation (MT). Statistical machine translation (SMT) systems, however, have not yet successfully incorporated components that generate grammatical elements in the target language. Most stateof-the-art SMT systems treat grammatical elements in exactly the same way as content words, and rely on general-purpose phrasal translations and target language models to generate these elements (e.g., Och and Ney, 2002; Koehn et al., 2003; Quirk et al., 2005; Chiang, 2005; Galley et al., 2006). However, since these grammatical elements in the target language often correspond to long-range dependencies and/or do not have any words corresponding in the source, they may be difficult to model, and the output of an SMT system is often ungrammatical. For example, Figure 1 shows an output from our baseline English-to-Japanese SMT system on a sentence from a computer domain. The SMT system, trained on this domain, produces a natural lexical translation for the English word patch as correction program, and translates replace In this pa"
N07-1007,P03-1021,0,0.0109215,"e j are the model parameters and fj(t) is the value of the feature function j on the candidate t. There are ten feature functions in the treelet system, including log-probabilities according to inverted and direct channel models estimated by relative frequency, lexical weighting channel models following Vogel et al. (2003), a trigram target language model, an order model, word count, phrase count, average phrase size functions, and whole-sentence IBM Model 1 logprobabilities in both directions (Och et al. 2004). The weights of these models are determined using the max-BLEU method described in Och (2003). As we describe in Section 4, the case prediction model is integrated into the system as an additional feature function. The treelet translation model is estimated using a parallel corpus. First, the corpus is wordaligned using GIZA++ (Och and Ney, 2000); then the source sentences are parsed into a dependency structure, and the dependency is projected onto the target side following the heuristics described in Quirk et al. (2005). Figure 2 shows an example of an aligned sentence pair: on the source (English) side, POS tags and word dependency structure are assigned (solid arcs); the word align"
N07-1007,P00-1056,0,0.008771,"ive frequency, lexical weighting channel models following Vogel et al. (2003), a trigram target language model, an order model, word count, phrase count, average phrase size functions, and whole-sentence IBM Model 1 logprobabilities in both directions (Och et al. 2004). The weights of these models are determined using the max-BLEU method described in Och (2003). As we describe in Section 4, the case prediction model is integrated into the system as an additional feature function. The treelet translation model is estimated using a parallel corpus. First, the corpus is wordaligned using GIZA++ (Och and Ney, 2000); then the source sentences are parsed into a dependency structure, and the dependency is projected onto the target side following the heuristics described in Quirk et al. (2005). Figure 2 shows an example of an aligned sentence pair: on the source (English) side, POS tags and word dependency structure are assigned (solid arcs); the word alignments between English and Japanese words are indicated by the dotted lines. On the target (Japanese) side, projected word dependencies (solid arcs) are available. Additional annotations in Figure 2, namely the POS tags and the bunsetsu dependency structur"
N07-1007,P02-1038,0,0.0484646,"e of SMT (S: source; O: output of MT; C: correct translation) Introduction Generation of grammatical elements such as inflectional endings and case markers is an important component technology for machine translation (MT). Statistical machine translation (SMT) systems, however, have not yet successfully incorporated components that generate grammatical elements in the target language. Most stateof-the-art SMT systems treat grammatical elements in exactly the same way as content words, and rely on general-purpose phrasal translations and target language models to generate these elements (e.g., Och and Ney, 2002; Koehn et al., 2003; Quirk et al., 2005; Chiang, 2005; Galley et al., 2006). However, since these grammatical elements in the target language often correspond to long-range dependencies and/or do not have any words corresponding in the source, they may be difficult to model, and the output of an SMT system is often ungrammatical. For example, Figure 1 shows an output from our baseline English-to-Japanese SMT system on a sentence from a computer domain. The SMT system, trained on this domain, produces a natural lexical translation for the English word patch as correction program, and translate"
N07-1007,N04-1021,0,0.380653,"an inanimate subject in Japanese. 49 Proceedings of NAACL HLT 2007, pages 49–56, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics ever, these components have not actually been integrated in an MT system. To our knowledge, this is the first work to integrate a grammatical element production model in an SMT system and to evaluate its impact in the context of end-toend MT. A common approach of integrating new models with a statistical MT system is to add them as new feature functions which are used in decoding or in models which re-rank n-best lists from the MT system (Och et al., 2004). In this paper we propose an extension of the n-best re-ranking approach, where we expand n-best candidate lists with multiple case assignment variations, and define new feature functions on this expanded candidate set. We show that expanding the n-best lists significantly outperforms standard n-best reranking. We also show that integrating our case prediction model improves the quality of translation according to BLEU (Papineni et al., 2002) and human evaluation. 2 Background In this section, we provide necessary background of the current work. 2.1 case markers ga wo no ni kara to de e made"
N07-1007,P02-1040,0,0.0744987,"s with a statistical MT system is to add them as new feature functions which are used in decoding or in models which re-rank n-best lists from the MT system (Och et al., 2004). In this paper we propose an extension of the n-best re-ranking approach, where we expand n-best candidate lists with multiple case assignment variations, and define new feature functions on this expanded candidate set. We show that expanding the n-best lists significantly outperforms standard n-best reranking. We also show that integrating our case prediction model improves the quality of translation according to BLEU (Papineni et al., 2002) and human evaluation. 2 Background In this section, we provide necessary background of the current work. 2.1 case markers ga wo no ni kara to de e made yori wa が を の に から と で へ まで より は grammatical functions subject; object object; path genitive; subject dative object, location source quotative, reciprocal, as location,instrument, cause goal, direction goal (up to, until) source, comparison target Topic +wa ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 1. Case markers to be predicted The case markers we used for the prediction task are the same as those defined in Suzuki and Toutatnova (2006), and are summarized in Tab"
N07-1007,P05-1034,0,0.506373,": correct translation) Introduction Generation of grammatical elements such as inflectional endings and case markers is an important component technology for machine translation (MT). Statistical machine translation (SMT) systems, however, have not yet successfully incorporated components that generate grammatical elements in the target language. Most stateof-the-art SMT systems treat grammatical elements in exactly the same way as content words, and rely on general-purpose phrasal translations and target language models to generate these elements (e.g., Och and Ney, 2002; Koehn et al., 2003; Quirk et al., 2005; Chiang, 2005; Galley et al., 2006). However, since these grammatical elements in the target language often correspond to long-range dependencies and/or do not have any words corresponding in the source, they may be difficult to model, and the output of an SMT system is often ungrammatical. For example, Figure 1 shows an output from our baseline English-to-Japanese SMT system on a sentence from a computer domain. The SMT system, trained on this domain, produces a natural lexical translation for the English word patch as correction program, and translates replace In this paper, we explore the"
N07-1007,P06-1132,1,0.919261,"benefit from a similar approach to modeling grammatical elements. Our model uses a rich set of syntactic features of both the source (English) and the target (Japanese) sentences, using context which is broader than that utilized by existing SMT systems. We show that the use of such features results in very high case assignment quality and also leads to a notable improvement in MT quality. Previous work has discussed the building of special-purpose classifiers which generate grammatical elements such as prepositions (Hajič et al. 2002), determiners (Knight and Chander, 1994) and case markers (Suzuki and Toutanova, 2006) with an eye toward improving MT output. How1 There is a strong tendency to avoid transitive sentences with an inanimate subject in Japanese. 49 Proceedings of NAACL HLT 2007, pages 49–56, c Rochester, NY, April 2007. 2007 Association for Computational Linguistics ever, these components have not actually been integrated in an MT system. To our knowledge, this is the first work to integrate a grammatical element production model in an SMT system and to evaluate its impact in the context of end-toend MT. A common approach of integrating new models with a statistical MT system is to add them as n"
N07-1007,2003.mtsummit-papers.53,0,0.0125643,"-2K, which are used to integrate and evaluate the case prediction model in an end-to-end MT scenario. Some characteristics of these data sets are given in Table 2. We will refer to this table as we describe our experiments in later sections. Figure 2. Aligned English-Japanese sentence pair where j are the model parameters and fj(t) is the value of the feature function j on the candidate t. There are ten feature functions in the treelet system, including log-probabilities according to inverted and direct channel models estimated by relative frequency, lexical weighting channel models following Vogel et al. (2003), a trigram target language model, an order model, word count, phrase count, average phrase size functions, and whole-sentence IBM Model 1 logprobabilities in both directions (Och et al. 2004). The weights of these models are determined using the max-BLEU method described in Och (2003). As we describe in Section 4, the case prediction model is integrated into the system as an additional feature function. The treelet translation model is estimated using a parallel corpus. First, the corpus is wordaligned using GIZA++ (Och and Ney, 2000); then the source sentences are parsed into a dependency st"
N07-1007,D08-1076,0,\N,Missing
N09-1024,W06-1655,0,0.0176511,"overlapping contextual features previously have been used in directed generative models (in the form of Markov models) for unsupervised morphological segmentation (Creutz and Lagus, 2007) or word segmentation (Goldwater et al., 2007). In terms of feature sets, our model is most closely related to the constituent-context model proposed by Klein and Manning (2001) for grammar induction. If we exclude the priors, our model can also be seen as a semi-Markov conditional random field (CRF) model (Sarawagi and Cohen, 2004). Semi-Markov CRFs previously have been used for supervised word segmentation (Andrew, 2006), but not for unsupervised morphological segmentation. Unsupervised learning with log-linear models has received little attention in the past. Two notable exceptions are Smith & Eisner (2005) for POS tagging, and Poon & Domingos (2008) for coreference resolution. Learning with log-linear models requires computing the normalization constant (a.k.a. the partition function) Z. This is already challenging in supervised learning. In unsupervised learning, the difficulty is further compounded by the absence of supervised labels. Smith & Eisner (2005) proposed contrastive estimation, which uses a sma"
N09-1024,N07-1020,0,0.25384,"soft Research. NLP applications, including machine translation, speech recognition and question answering. Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). While successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems use directed generative models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008b), making it difficult to extend them with arbitrary overlapping dependencies that are potentially helpful to segmentation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such"
N09-1024,P07-1116,0,0.0818885,"resentations of the lexicon and corpus (Brent et al., 1995; Goldsmith, 2001; Creutz and Lagus, 2007). Other approaches use statistics on morpheme context, such as conditional entropy between adjacent n-grams, to identify morpheme candidates (Harris, 1955; Keshava and Pitler, 2006). In this paper, we incorporate both intuitions into a simple yet powerful model, and show that each contributes significantly to performance. Unsupervised morphological segmentation systems also differ from the engineering perspective. Some adopt a pipeline approach (Schone and Jurafsky, 2001; Dasgupta and Ng, 2007; Demberg, 2007), which works by first extracting candidate affixes and stems, and then segmenting the words based on the candidates. Others model segmentation using a joint probabilistic distribution (Goldwater et al., 2006; Creutz and Lagus, 2007; Snyder and 210 Barzilay, 2008b); they learn the model parameters from unlabeled data and produce the most probable segmentation as the final output. The latter approach is arguably more appealing from the modeling standpoint and avoids error propagation along the pipeline. However, most existing systems use directed generative models; Creutz & Lagus (2007) used an"
N09-1024,J01-2001,0,0.879538,"ternship at Microsoft Research. NLP applications, including machine translation, speech recognition and question answering. Past approaches include rule-based morphological analyzers (Buckwalter, 2004) and supervised learning (Habash and Rambow, 2005). While successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems use directed generative models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008b), making it difficult to extend them with arbitrary overlapping dependencies that are potentially helpful to segmentation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as ov"
N09-1024,P05-1071,0,0.0464249,"when computing the expected counts, we initialize the sampler with the most probable segmentation output by annealing. 7 Experiments We evaluated our system on two datasets. Our main evaluation is on a multi-lingual dataset constructed by Snyder & Barzilay (2008a; 2008b). It consists of 6192 short parallel phrases in Hebrew, Arabic, Aramaic (a dialect of Arabic), and English. The parallel phrases were extracted from the Hebrew Bible and its translations via word alignment and postprocessing. For Arabic, the gold segmentation was obtained using a highly accurate Arabic morphological analyzer (Habash and Rambow, 2005); for Hebrew, from a Bible edition distributed by Westminster Hebrew Institute (Groves and Lowery, 2006). There is no gold segmentation for English and Aramaic. Like Snyder & Barzilay, we evaluate on the Arabic and Hebrew portions only; unlike their approach, our system does not use any bilingual information. We refer to this dataset as S&B . We also report our results on the Arabic Penn Treebank (ATB), which provides gold segmentations for an 214 Arabic corpus with about 120,000 Arabic words. As in previous work, we report recall, precision, and F1 over segmentation points. We used 500 phrase"
N09-1024,D08-1068,1,0.212887,", 2007). In terms of feature sets, our model is most closely related to the constituent-context model proposed by Klein and Manning (2001) for grammar induction. If we exclude the priors, our model can also be seen as a semi-Markov conditional random field (CRF) model (Sarawagi and Cohen, 2004). Semi-Markov CRFs previously have been used for supervised word segmentation (Andrew, 2006), but not for unsupervised morphological segmentation. Unsupervised learning with log-linear models has received little attention in the past. Two notable exceptions are Smith & Eisner (2005) for POS tagging, and Poon & Domingos (2008) for coreference resolution. Learning with log-linear models requires computing the normalization constant (a.k.a. the partition function) Z. This is already challenging in supervised learning. In unsupervised learning, the difficulty is further compounded by the absence of supervised labels. Smith & Eisner (2005) proposed contrastive estimation, which uses a small neighborhood to compute Z. The neighborhood is carefully designed so that it not only makes computation easier but also offers sufficient contrastive information to aid unsupervised learning. Poon & Domingos (2008), on the other han"
N09-1024,N01-1024,0,0.522611,"z and Lagus, 2007). Our system can be readily applied to supervised and semi-supervised learning. Using a fraction of the labeled data, it already outperforms Snyder & Barzilay’s supervised results (2008a), which further demonstrates the benefit of using a log-linear model. 2 Related Work There is a large body of work on the unsupervised learning of morphology. In addition to morphological segmentation, there has been work on unsupervised morpheme analysis, where one needs to determine features of word forms (Kurimo et al., 2007) or identify words with the same lemma by modeling stem changes (Schone and Jurafsky, 2001; Goldsmith, 2001). However, we focus our review specifically on morphological segmentation. In the absence of labels, unsupervised learning must incorporate a strong learning bias that reflects prior knowledge about the task. In morphological segmentation, an often-used bias is the minimum description length (MDL) principle, which favors compact representations of the lexicon and corpus (Brent et al., 1995; Goldsmith, 2001; Creutz and Lagus, 2007). Other approaches use statistics on morpheme context, such as conditional entropy between adjacent n-grams, to identify morpheme candidates (Harris"
N09-1024,P05-1044,0,0.807103,"007) or word segmentation (Goldwater et al., 2007). In terms of feature sets, our model is most closely related to the constituent-context model proposed by Klein and Manning (2001) for grammar induction. If we exclude the priors, our model can also be seen as a semi-Markov conditional random field (CRF) model (Sarawagi and Cohen, 2004). Semi-Markov CRFs previously have been used for supervised word segmentation (Andrew, 2006), but not for unsupervised morphological segmentation. Unsupervised learning with log-linear models has received little attention in the past. Two notable exceptions are Smith & Eisner (2005) for POS tagging, and Poon & Domingos (2008) for coreference resolution. Learning with log-linear models requires computing the normalization constant (a.k.a. the partition function) Z. This is already challenging in supervised learning. In unsupervised learning, the difficulty is further compounded by the absence of supervised labels. Smith & Eisner (2005) proposed contrastive estimation, which uses a small neighborhood to compute Z. The neighborhood is carefully designed so that it not only makes computation easier but also offers sufficient contrastive information to aid unsupervised learni"
N09-1024,P08-1084,0,0.766837,"e successful, these require deep language expertise and a long and laborious process in system building or labeling. Unsupervised approaches are attractive due to the the availability of large quantities of unlabeled text, and unsupervised morphological segmentation has been extensively studied for a number of languages (Brent et al., 1995; Goldsmith, 2001; Dasgupta and Ng, 2007; Creutz and Lagus, 2007). The lack of supervised labels makes it even more important to leverage rich features and global dependencies. However, existing systems use directed generative models (Creutz and Lagus, 2007; Snyder and Barzilay, 2008b), making it difficult to extend them with arbitrary overlapping dependencies that are potentially helpful to segmentation. In this paper, we present the first log-linear model for unsupervised morphological segmentation. Our model incorporates simple priors inspired by the minimum description length (MDL) principle, as well as overlapping features such as morphemes and their contexts (e.g., in Arabic, the string Al is likely a morpheme, as is any string between Al and a word boundary). We develop efficient learning and inference algorithms using a novel combination of two ideas from previous"
N09-1024,D08-1109,0,\N,Missing
N10-1063,W06-2810,0,0.673272,"Missing"
N10-1063,P06-1009,0,0.00624231,"entence alignment model is able to capture this phenomenon. For both parallel and comparable corpora, global sentence alignments have been used, though the alignments were monotonic (Gale and Church, 1991; Moore, 2002; Zhao and Vogel, 2002). Our model is a first order linear chain Conditional Random Field (CRF) (Lafferty et al., 2001). The set of source and target sentences are observed. For each source sentence, we have a hidden variable indicating the corresponding target sentence to which it is aligned (or null). The model is similar to the discriminative CRF-based word alignment model of (Blunsom and Cohn, 2006). 3.3 Features Our features can be grouped into four categories. 3.1 Binary Classifiers and Rankers Much of the previous work involves building a binary classifier for sentence pairs to determine whether or not they are parallel (Munteanu and Marcu, 2005; Tillmann, 2009). The training data usually comes from a standard parallel corpus. There is a substantial class imbalance (O(n) positive examples, and O(n2 ) negative examples), and various heuristics are used to mitigate this problem. Munteanu and Marcu (2005) filter out negative examples with high length difference or low word overlap (based"
N10-1063,J93-2003,0,0.0430686,"ine translation system, the size of the parallel corpus used for training is a major factor in its performance. For some language pairs, such as Chinese-English and Arabic-English, large amounts of parallel data are readily available, but for most language pairs this is not the case. The ∗ This research was conducted during the author’s internship at Microsoft Research. Once promising document pairs are identified, the next step is to extract parallel sentences. Usually, some seed parallel data is assumed to be available. This data is used to train a word alignment model, such as IBM Model 1 (Brown et al., 1993) or HMM-based word alignment (Vogel et al., 1996). Statistics from this word alignment model are used to train a classifier which identifies bilingual sentence pairs as parallel or not parallel. This classifier is applied to all sentence pairs in documents which were found to be similar. Typically, some pruning is done to reduce the number of sen403 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 403–411, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tence pairs that need to be classified. While thes"
N10-1063,C04-1151,0,0.608848,"d by users. This is an extremely valuable resource when extracting parallel sentences, as the document alignment is already provided. Table 1 shows how many of these “interwiki” links are present between the English Wikipedia and the 16 largest non-English Wikipedias. Wikipedia’s markup contains other useful indicators for parallel sentence extraction. The many hyperlinks found in articles have previously been used as a valuable source of information. (Adafre and de Rijke, 2006) use matching hyperlinks to identify similar sentences. Two links match if the arti404 Types of Non-Parallel Corpora Fung and Cheung (2004) give a more fine-grained description of the types of non-parallel corpora, which we will briefly summarize. A noisy parallel corpus has documents which contain many parallel sentences in roughly the same order. Comparable corpora contain topic aligned documents which are not translations of each other. The corpora Fung and Cheung (2004) examine are quasi-comparable: they contain bilingual documents which are not necessarily on the same topic. Wikipedia is a special case, since the aligned article pairs may range from being almost completely parallel (e.g., the Spanish and English entries for"
N10-1063,H91-1026,0,0.452331,"llmann (2009). Since our corpus already contains document alignments, we sidestep this problem, and will not discuss further details of this issue. That said, we believe that our methods will be effective in corpora without document alignments when combined with one of the aforementioned algorithms. 3.2 Sequence Models In Wikipedia article pairs, it is common for parallel sentences to occur in clusters. A global sentence alignment model is able to capture this phenomenon. For both parallel and comparable corpora, global sentence alignments have been used, though the alignments were monotonic (Gale and Church, 1991; Moore, 2002; Zhao and Vogel, 2002). Our model is a first order linear chain Conditional Random Field (CRF) (Lafferty et al., 2001). The set of source and target sentences are observed. For each source sentence, we have a hidden variable indicating the corresponding target sentence to which it is aligned (or null). The model is similar to the discriminative CRF-based word alignment model of (Blunsom and Cohn, 2006). 3.3 Features Our features can be grouped into four categories. 3.1 Binary Classifiers and Rankers Much of the previous work involves building a binary classifier for sentence pair"
N10-1063,P08-1088,0,0.206055,"rcu, 2005). However, a selftrained sentence pair extraction system is only able to acquire new lexical items that occur in parallel sentences. Within Wikipedia, many linked article pairs do not contain any parallel sentences, yet con406 tain many words and phrases that are good translations of each other. In this paper we explore an alternative approach to lexicon acquisition for use in parallel sentence extraction. We build a lexicon model using an approach similar to ones developed for unsupervised lexicon induction from monolingual or comparable corpora (Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). We briefly describe the lexicon model and its use in sentence-extraction. The lexicon model is based on a probabilistic model P (wt |ws , T, S) where wt is a word in the target language, ws is a word in the source language, and T and S are linked articles in the target and source languages, respectively. We train this model similarly to the sentenceextraction ranking model, with the difference that we are aligning word pairs and not sentence pairs. The model is trained from a small set of annotated Wikipedia article pairs, where for some words in the source language we have marked one or mor"
N10-1063,W02-0902,0,0.601618,", as in (Munteanu and Marcu, 2005). However, a selftrained sentence pair extraction system is only able to acquire new lexical items that occur in parallel sentences. Within Wikipedia, many linked article pairs do not contain any parallel sentences, yet con406 tain many words and phrases that are good translations of each other. In this paper we explore an alternative approach to lexicon acquisition for use in parallel sentence extraction. We build a lexicon model using an approach similar to ones developed for unsupervised lexicon induction from monolingual or comparable corpora (Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). We briefly describe the lexicon model and its use in sentence-extraction. The lexicon model is based on a probabilistic model P (wt |ws , T, S) where wt is a word in the target language, ws is a word in the source language, and T and S are linked articles in the target and source languages, respectively. We train this model similarly to the sentenceextraction ranking model, with the difference that we are aligning word pairs and not sentence pairs. The model is trained from a small set of annotated Wikipedia article pairs, where for some words in the source language w"
N10-1063,N03-1017,0,0.00903619,"based features can be dramatic as in the case of Bulgarian (the CRF model average precision increased by nearly 9 points). The lower gains on Spanish and German may be due in part to the lack of language-specific training data. These results are very promising and motivate further exploration. We also note that this is perhaps the first successful practical application of an automatically induced word translation lexicon. 4.3 SMT Evaluation We also present results in the context of a full machine translation system to evaluate the potential utility of this data. A standard phrasal SMT system (Koehn et al., 2003) serves as our testbed, using a conventional set of models: phrasal models of source given target and target given source; lexical weighting models in both directions, language model, word count, phrase count, distortion penalty, and a lexicalized reordering model. Given that the extracted Wikipedia data takes the standard form of parallel sentences, it would be easy to exploit this same data in a number of systems. For each language pair we explored two training conditions. The “Medium” data condition used easily downloadable corpora: Europarl for GermanEnglish and Spanish-English, and JRC/Ac"
N10-1063,2005.mtsummit-papers.11,0,0.331918,"s annotated with possible parallel sentences in the target language (the target language was English in all experiments). The pairs were annotated with a quality level: 1 if the sentences contained some parallel fragments, 2 if the sentences were mostly parallel with some missing words, and 3 if the sentences appeared to be direct translations. In all experiments, sentence pairs with quality 2 or 3 were taken as positive examples. The resulting datasets are available at http://research.microsoft.com/enus/people/chrisq/wikidownload.aspx. For our seed parallel data, we used the Europarl corpus (Koehn, 2005) for Spanish and German and the JRC-Aquis corpus for Bulgarian, plus the article titles for parallel Wikipedia documents, and translations available from Wiktionary entries.3 4.2 Intrinsic Evaluation Using 5-fold cross-validation on the 20 document pairs for each language condition, we compared the binary classifier, ranker, and CRF models for parallel sentence extraction. To tune for precision/recall, we used minimum Bayes risk decoding. We define the loss L(τ, µ) of picking target sentence τ when the correct target sentence is µ as 0 if τ = µ, λ if τ = NULL and µ 6= NULL, and 1 otherwise. By"
N10-1063,moore-2002-fast,0,0.383005,"r corpus already contains document alignments, we sidestep this problem, and will not discuss further details of this issue. That said, we believe that our methods will be effective in corpora without document alignments when combined with one of the aforementioned algorithms. 3.2 Sequence Models In Wikipedia article pairs, it is common for parallel sentences to occur in clusters. A global sentence alignment model is able to capture this phenomenon. For both parallel and comparable corpora, global sentence alignments have been used, though the alignments were monotonic (Gale and Church, 1991; Moore, 2002; Zhao and Vogel, 2002). Our model is a first order linear chain Conditional Random Field (CRF) (Lafferty et al., 2001). The set of source and target sentences are observed. For each source sentence, we have a hidden variable indicating the corresponding target sentence to which it is aligned (or null). The model is similar to the discriminative CRF-based word alignment model of (Blunsom and Cohn, 2006). 3.3 Features Our features can be grouped into four categories. 3.1 Binary Classifiers and Rankers Much of the previous work involves building a binary classifier for sentence pairs to determin"
N10-1063,J05-4003,0,0.774258,"llel corpus also strongly influences the quality of translations produced. Many parallel corpora are taken from the news domain, or from parliamentary proceedings. Translation quality suffers when a system is not trained on any data from the domain it is tested on. The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training. In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages. We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity. We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model. Results for both accuracy in sentence extraction and downstrea"
N10-1063,P03-1021,0,0.0038481,"in training data is unlikely to provide appropriate phrasal translations. Therefore, we experimented with two broad domain test sets. First, Bing Translator provided a sample of translation requests along with translations in GermanEnglish and Spanish-English, which acted our standard development and test set. Unfortunately no such tagged set was available in Bulgarian-English, so we held out a portion of the large system’s training data to use for development and test. In each language pair, the test set was split into a devel409 opment portion (“Dev A”) used for minimum error rate training (Och, 2003) and a test set (“Test A”) used for final evaluation. Second, we created new test sets in each of the three language pairs by sampling parallel sentences from held out Wikipedia articles. To ensure that this test data was clean, we manually filtered the sentence pairs that were not truly parallel and edited them as necessary to improve adequacy. We called this “Wikitest”. This test set is available at http://research.microsoft.com/enus/people/chrisq/wikidownload.aspx. Characteristics of these test sets are summarized in Table 5. We evaluated the resulting systems using BLEU4 (Papineni et al.,"
N10-1063,P02-1040,0,0.0927712,"ining (Och, 2003) and a test set (“Test A”) used for final evaluation. Second, we created new test sets in each of the three language pairs by sampling parallel sentences from held out Wikipedia articles. To ensure that this test data was clean, we manually filtered the sentence pairs that were not truly parallel and edited them as necessary to improve adequacy. We called this “Wikitest”. This test set is available at http://research.microsoft.com/enus/people/chrisq/wikidownload.aspx. Characteristics of these test sets are summarized in Table 5. We evaluated the resulting systems using BLEU4 (Papineni et al., 2002); the results are presented in Table 6. First we note that the extracted Wikipedia data are very helpful in medium data conditions, significantly improving translation performance in all conditions. Furthermore we found that the extracted Wikipedia sentences substantially improved translation quality on held-out Wikipedia articles. In every case, training on medium data plus Wikipedia extracts led to equal or better translation quality than the large system alone. Furthermore, adding the Wikipedia data to the large data condition still made substantial improvements. 5 Conclusions Our first sub"
N10-1063,P99-1067,0,0.346802,"elf-training, as in (Munteanu and Marcu, 2005). However, a selftrained sentence pair extraction system is only able to acquire new lexical items that occur in parallel sentences. Within Wikipedia, many linked article pairs do not contain any parallel sentences, yet con406 tain many words and phrases that are good translations of each other. In this paper we explore an alternative approach to lexicon acquisition for use in parallel sentence extraction. We build a lexicon model using an approach similar to ones developed for unsupervised lexicon induction from monolingual or comparable corpora (Rapp, 1999; Koehn and Knight, 2002; Haghighi et al., 2008). We briefly describe the lexicon model and its use in sentence-extraction. The lexicon model is based on a probabilistic model P (wt |ws , T, S) where wt is a word in the target language, ws is a word in the source language, and T and S are linked articles in the target and source languages, respectively. We train this model similarly to the sentenceextraction ranking model, with the difference that we are aligning word pairs and not sentence pairs. The model is trained from a small set of annotated Wikipedia article pairs, where for some words"
N10-1063,J03-3002,0,0.875704,"roduced. Many parallel corpora are taken from the news domain, or from parliamentary proceedings. Translation quality suffers when a system is not trained on any data from the domain it is tested on. The quality of a statistical machine translation (SMT) system is heavily dependent upon the amount of parallel sentences used in training. In recent years, there have been several approaches developed for obtaining parallel sentences from non-parallel, or comparable data, such as news articles published within the same time period (Munteanu and Marcu, 2005), or web pages with a similar structure (Resnik and Smith, 2003). One resource not yet thoroughly explored is Wikipedia, an online encyclopedia containing linked articles in many languages. We advance the state of the art in parallel sentence extraction by modeling the document level alignment, motivated by the observation that parallel sentence pairs are often found in close proximity. We also include features which make use of the additional annotation given by Wikipedia, and features using an automatically induced lexicon model. Results for both accuracy in sentence extraction and downstream improvement in an SMT system are presented. 1 While parallel c"
N10-1063,N09-2024,0,0.108475,"ese 252K Czech 87K Table 1: Number of aligned bilingual articles in Wikipedia by language (paired with English). write the content themselves. Furthermore, even articles created through translations may later diverge due to independent edits in either language. 3 Models for Parallel Sentence Extraction In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents. The related problem of automatic document alignment in news and web corpora has been explored by a number of researchers, including Resnik and Smith (2003), Munteanu and Marcu (2005), Tillmann and Xu (2009), and Tillmann (2009). Since our corpus already contains document alignments, we sidestep this problem, and will not discuss further details of this issue. That said, we believe that our methods will be effective in corpora without document alignments when combined with one of the aforementioned algorithms. 3.2 Sequence Models In Wikipedia article pairs, it is common for parallel sentences to occur in clusters. A global sentence alignment model is able to capture this phenomenon. For both parallel and comparable corpora, global sentence alignments have been used, though the alignments were mon"
N10-1063,P09-2057,0,0.287582,"Number of aligned bilingual articles in Wikipedia by language (paired with English). write the content themselves. Furthermore, even articles created through translations may later diverge due to independent edits in either language. 3 Models for Parallel Sentence Extraction In this section, we will focus on methods for extracting parallel sentences from aligned, comparable documents. The related problem of automatic document alignment in news and web corpora has been explored by a number of researchers, including Resnik and Smith (2003), Munteanu and Marcu (2005), Tillmann and Xu (2009), and Tillmann (2009). Since our corpus already contains document alignments, we sidestep this problem, and will not discuss further details of this issue. That said, we believe that our methods will be effective in corpora without document alignments when combined with one of the aforementioned algorithms. 3.2 Sequence Models In Wikipedia article pairs, it is common for parallel sentences to occur in clusters. A global sentence alignment model is able to capture this phenomenon. For both parallel and comparable corpora, global sentence alignments have been used, though the alignments were monotonic (Gale and Chur"
N10-1063,C96-2141,0,0.887352,"corpus used for training is a major factor in its performance. For some language pairs, such as Chinese-English and Arabic-English, large amounts of parallel data are readily available, but for most language pairs this is not the case. The ∗ This research was conducted during the author’s internship at Microsoft Research. Once promising document pairs are identified, the next step is to extract parallel sentences. Usually, some seed parallel data is assumed to be available. This data is used to train a word alignment model, such as IBM Model 1 (Brown et al., 1993) or HMM-based word alignment (Vogel et al., 1996). Statistics from this word alignment model are used to train a classifier which identifies bilingual sentence pairs as parallel or not parallel. This classifier is applied to all sentence pairs in documents which were found to be similar. Typically, some pruning is done to reduce the number of sen403 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 403–411, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tence pairs that need to be classified. While these methods have been applied to news corpora and w"
N12-3006,D09-1111,1,0.814882,"Missing"
N12-3006,J93-2004,0,0.04636,"the NLP group at Microsoft Research. The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty,"
N12-3006,J05-1004,0,0.0277178,". The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty, and shared task organizers are now"
N12-3006,P06-1055,0,0.0153341,"Missing"
N12-3006,C08-1094,0,0.0225179,"Missing"
N12-3006,J08-2002,1,0.0600596,"abels, like ARG0, ARG1, …, ARG5 for core arguments, and labels like ARGMTMP,ARGM-LOC, etc. for adjunct-like arguments. The meaning of the numbered arguments is verb-specific, with ARG0 typically representing an agent-like role, and ARG1 a patient-like role. This implementation of an SRL system follows the approach described in (Xue and Palmer, 04), and includes two log-linear models for argument identification and classification. A single syntax tree generated by the MSR SPLAT split-merge parser is used as input. Non-overlapping arguments are derived using the dynamic programming algorithm by Toutanova et al. (2008). 3 3.1 Other Language Analysis Functionality Sentence Boundary / Tokenization This analyzer identifies sentence boundaries and breaks the input into tokens. Both are represented as offsets of character ranges. Each token has both a raw form from the string and a normalized form in the PTB specification, e.g., open and close parentheses are replaced by -LRB- and -RRB-, respectively, to remove ambiguity with parentheses indicating syntactic structure. A finite state machine using simple rules and abbreviations detects sentence boundaries with high accuracy, and a set of regular expressions toke"
N12-3006,W04-3212,0,0.0291436,"Missing"
N12-3006,J03-4003,0,\N,Missing
N13-1002,P11-2031,0,0.0145787,"5-gram models being better than the 3-gram. Here the individual 3-gram models are better than the baseline at significance level 0.02 and their combination is better than the baseline at our earlier defined threshold of 0.01. The withinphrase MTU MMs (results shown in the last two rows) improve upon the baseline slightly, but here again the improvements mostly stem from the use of context across phrase boundaries. Our final results on German-English are better than the best result of 27.30 from the shared task (Koehn and Monz, 2006). Thanks to the reviewers for referring us to recent work by (Clark et al., 2011) that pointed out problems with significance tests for machine translation, where the randomness and local optima in the MERT weight tuning method lead to a large variance in development and test set performance across different runs of optimization (using a different random seed or starting point). (Clark et al., 2011) proposed a stratified approximate randomization statistical significance test, which controls for optimizer instability. Using this test, for the English-Bulgarian system, we confirmed that the combination of four 3-gram MMs and the combination of 5-gram MMs is better than the"
N13-1002,P11-1105,0,0.148685,"nguage pairs, looking at distinct orders in isolation and combination. 12 Proceedings of NAACL-HLT 2013, pages 12–21, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics 2 Related work Marino et al. (2006) proposed a translation model using a Markov model of bilingual n-grams, demonstrating state-of-the-art performance compared to conventional phrase-based models. Crego and Yvon (2010) further explored factorized n-gram approaches, though both models considered rather large n-grams; this paper focuses on small units with asynchronous orders in source and target. Durrani et al. (2011) developed a joint model that captures translation of contiguous and gapped units as well as reordering. Two prior approaches explored similar models in syntax based systems. MTUs have been used in dependency translation models (Quirk and Menezes, 2006) to augment syntax directed translation systems. Likewise in target language syntax systems, one can consider Markov models over minimal rules, where the translation probability of each rule is adjusted to include context information from parent rules (Vaswani et al., 2011). Most prior work tends to replace the existing probabilities rather than"
N13-1002,D09-1117,0,0.259413,"similar to recombination in a standard-phrase based decoder with the difference that it is not always the last two target MTUs that define the context needed by future extensions. The weights λ of different models are trained on a development set using MER training to maximize the BLEU score of the resulting model. Note that this method of model combination was not considered in any of the previous works comparing different decompositions. The system combination method is motivated by prior work in machine translation which combined left-to-right and right-to-left machine translation systems (Finch and Sumita, 2009). Similarly, we perform sentence-level system combination between systems using different MTU Markov models to come up with most likely translations. If we have k systems guessing hypotheses based on MM1 , . . . , MMk respectively, we generate 1000best lists from each system, resulting in a pool of up to 1000k possible distinct translations. Each of the candidate hypotheses from MMi is scored with its Markov model log-probability logP MMi (h). We compute normalized probabilities for each system’s n-best by exponentiating and normalizing: Pi (h) ∝ P MMi (h). If a hypothesis h is not in system i"
N13-1002,W07-0711,0,0.0146267,"lopment set consists of 1,497 sentences, the English side from WMT 2009 news test data, and the Bulgarian side a human translation thereof. The test set comes from the same mixture of sources as the training set. For this system we used a single four-gram target language model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). 19 3-gram models Dev Test 32.58 31.78 33.05 32.78* 33.05 32.96* 32.90 33.00* 32.94 32.98* 33.22 33.07* 32.58 31.78 5-gram models Dev Test 32.58 31.78 33.16 32.88* 33.16 32.81* 32.98 32.98* 33.09 32.96* 33.37 33.00* 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starre"
N13-1002,W06-3114,0,0.190924,"T 2002 and 2003 test sets as the development set and the NIST 2005 test set as our test set. The baseline phrasal system uses a 5-gram language model with modified Kneser-Ney smoothing (Kenser and Ney, 1995), trained on the Xinhua portion of the English Gigaword corpus (238M English words). For German-English we used the dataset from Language Chs-En Deu-En En-Bgr Training 1 Mln 751 K 4 Mln Dev NIST02+03 WMT06dev 1,497 Test NIST05 WMT06test 2,498 Model Baseline L2RT R2LT L2RS R2LS 4 MMs 4 MMs phrs Table 3: Data sets for different language pairs. the WMT 2006 shared task on machine translation (Koehn and Monz, 2006). The parallel training set contains approximately 751K sentences. We also used the English monolingual data of around 1 million sentences for language model training. The development set contains 2000 sentences. The final test set (the in-domain test set for the shared task) also contains 2000 sentences. Two Kneser-Ney language models were used as separate features: a 4gram LM trained on the parallel portion of the data, and a 5-gram LM trained on the monolingual corpus. For English-Bulgarian we used a dataset containing sentences from several data sources: JRCAcquis (Steinberger et al., 2006"
N13-1002,N03-1017,0,0.189237,"“that the source and target side of a tuple of words are synchronized, i.e. that they occur in the same order in their respective languages” (Crego and Yvon, 2010). For language pairs with significant typological divergences, such as Chinese-English, it is quite difficult to extract a synchronized sequence of units; in the limit, the smallest synchronized unit may be the whole sentence. Other approaches explore incorporation into syntax-based MT systems or replacing the phrasal translation system altogether. Introduction The translation procedure of a classical phrasebased translation model (Koehn et al., 2003) first divides the input sentence into a sequence of phrases, translates each phrase, explores reorderings of these translations, and then scores the resulting candidates with a linear combination of models. Conventional models include phrase-based channel models that effectively model each phrase as a large unigram, reordering models, and target language models. Of these models, only the target language model ∗ This research was conducted during the author’s internship at Microsoft Research We investigate the addition of MTUs to a phrasal translation system to improve modeling of context and"
N13-1002,W04-3250,0,0.0168628,"ate prediction. The best decomposition order varies from language to language: right-to-left in source order is best for Chinese-English, right-to-left in target order is best for German-English and left-to-right or rightto-left in target order are best in English-Bulgarian. We computed statistical significance tests, testing the difference between the L2RT model (the standard in prior work) and models achieving higher test set performance. The models that are significantly better at significance α &lt; 0.01 are marked with a star in the table. We used a paired bootstrap test with 10,000 trials (Koehn, 2004). Next we evaluate the methods for combining decomposition orders introduced in Sections 4.1 and 4.2. The results are reported in Table 2. The upper part of the table focuses on combining different 18 Model Baseline-1 TgtProduct TgtSysComb TgtDynamic Baseline-2 AllProduct AllSyscomb Chs-En Dev Test 24.04 25.09 25.27 25.84* 24.49 25.27 24.07 25.10 26.48 27.96 28.68 29.59* 27.02 28.30 Deu-En Dev Test 30.14 30.14 30.47 30.49 30.20 30.15 30.60 30.41 30.14 30.14 31.54 31.36* 30.20 30.17 En-Bgr Dev Test 49.86 46.45 51.04 47.27* 50.46 46.31 49.99 46.52 49.86 46.45 51.50 48.10* 50.90 46.53 Table 2: Le"
N13-1002,J06-4004,0,0.727376,"Missing"
N13-1002,C08-1074,1,0.791289,"me mixture of sources as the training set. For this system we used a single four-gram target language model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). 19 3-gram models Dev Test 32.58 31.78 33.05 32.78* 33.05 32.96* 32.90 33.00* 32.94 32.98* 33.22 33.07* 32.58 31.78 5-gram models Dev Test 32.58 31.78 33.16 32.88* 33.16 32.81* 32.98 32.98* 33.09 32.96* 33.37 33.00* 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starred results on the test set indicate significantly better performance than the baseline. 6.3 MT reranking experiments We first report detailed experiments on ChineseEnglish, and then ve"
N13-1002,P02-1038,0,0.0382882,"approximations to the trigram MTU Markov model scores as future scores, since not all needed context is available for a hypothesis at the time of construction. As additional context becomes available, the exact score can be computed. 2 4.1 Basic decomposition order combinations We first introduce two methods of combining different decomposition orders: product and system combination. The product method arises naturally in the machine translation setting, where probabilities from different models are multiplied together and further weighted to form the log-linear model for machine translation (Och and Ney, 2002). We define a similar scoring function using a set of MTU Markov models MM 1 , ..., MM k for a hypothesis h as follows: Score(h) = λ1 logP MM1 (h) + ... + λk logP MMk (h) Figure 2: Lexical selection. 2 We use this constrained MT setting to evaluate the performance of models using different MTU decomposition orders and models using combinations of decomposition orders. The simplified setting allows 15 We apply hypothesis recombination, which can merge hypotheses that are indistinguishable with respect to future continuations. This is similar to recombination in a standard-phrase based decoder w"
N13-1002,P03-1021,0,0.0140661,"f. The test set comes from the same mixture of sources as the training set. For this system we used a single four-gram target language model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). 19 3-gram models Dev Test 32.58 31.78 33.05 32.78* 33.05 32.96* 32.90 33.00* 32.94 32.98* 33.22 33.07* 32.58 31.78 5-gram models Dev Test 32.58 31.78 33.16 32.88* 33.16 32.81* 32.98 32.98* 33.09 32.96* 33.37 33.00* 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starred results on the test set indicate significantly better performance than the baseline. 6.3 MT reranking experiments We first report detail"
N13-1002,P02-1040,0,0.102213,"uage model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). 19 3-gram models Dev Test 32.58 31.78 33.05 32.78* 33.05 32.96* 32.90 33.00* 32.94 32.98* 33.22 33.07* 32.58 31.78 5-gram models Dev Test 32.58 31.78 33.16 32.88* 33.16 32.81* 32.98 32.98* 33.09 32.96* 33.37 33.00* 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starred results on the test set indicate significantly better performance than the baseline. 6.3 MT reranking experiments We first report detailed experiments on ChineseEnglish, and then verify our main conclusions on the other language pairs. Table 4 looks at the impact of individual"
N13-1002,N06-1002,1,0.92188,"anslation model using a Markov model of bilingual n-grams, demonstrating state-of-the-art performance compared to conventional phrase-based models. Crego and Yvon (2010) further explored factorized n-gram approaches, though both models considered rather large n-grams; this paper focuses on small units with asynchronous orders in source and target. Durrani et al. (2011) developed a joint model that captures translation of contiguous and gapped units as well as reordering. Two prior approaches explored similar models in syntax based systems. MTUs have been used in dependency translation models (Quirk and Menezes, 2006) to augment syntax directed translation systems. Likewise in target language syntax systems, one can consider Markov models over minimal rules, where the translation probability of each rule is adjusted to include context information from parent rules (Vaswani et al., 2011). Most prior work tends to replace the existing probabilities rather than augmenting them. We believe that Markov rules provide an additional signal but are not a replacement. Their distributions should be more informative than the so-called “lexical weighting” models, and less sparse than relative frequency estimates, thoug"
N13-1002,N03-1033,1,0.0127916,"text for disambiguation and it is not clear apriori which would perform best. We compare all four decomposition orders (source order left-to-right and right-to-left, and target order left-to-right and rightto-left). Although the independence assumptions of left-to-right and right-to-left are the same, the resulting models may be different due to smoothing. In addition to studying these four basic decomposition orders, we report performance of two cyclic orders: cyclic in source or target sentence order. These models are inspired by the cyclic dependency network model proposed for POS tagging (Toutanova et al., 2003) and also used as a baseline in previous work on dynamic decomposition orders (Tsuruoka and Tsujii, 2005). 1 The probability according to the cyclic orders is defined by conditioning each MTU on both its left and right neighbor MTUs. For example, the probability of the sentence pair in Figure 1 under the source cyclic order, using a 3-gram model is defined as: P(M1|M2) · P(M2|M1, M3) · P(M3|M2, M4) · P(M4|M3, M5) · P(M5|M4). All n-gram Markov models over MTUs are esti1 The correct application of such models requires sampling to find the highest scoring sequence, but we apply the max product ap"
N13-1002,H05-1059,0,0.328021,"unigrams to capture contextual information, n-grams of minimal translation units allow a robust contextual model that is less constrained by segmentation. 3.2 MTU enumeration orders When defining a joint probability distribution over MTUs of an aligned sentence pair, it is necessary to define a decomposition, or generation order for the sentence pair. For a single sequence in language modeling or synchronized sequences in channel modeling, the default enumeration order has been left-to-right. Different decomposition orders have been used in part-of-speech tagging and named entity recognition (Tsuruoka and Tsujii, 2005). Intuitively, information from the left or right could be more useful for particular disambiguation choices. Our research on different decomposition orders was motivated by this work. When applying such ideas to machine translation, there are additional challenges and opportunities. The task exhibits much more ambiguity – the number of possible MTUs is in the millions. An opportunity arises from the reordering phenomenon in machine translation: while in POS tagging the natural decomposition orders to study are only left-to-right and right-to-left, in machine translation we can further disting"
N13-1002,P11-1086,0,0.364188,"er focuses on small units with asynchronous orders in source and target. Durrani et al. (2011) developed a joint model that captures translation of contiguous and gapped units as well as reordering. Two prior approaches explored similar models in syntax based systems. MTUs have been used in dependency translation models (Quirk and Menezes, 2006) to augment syntax directed translation systems. Likewise in target language syntax systems, one can consider Markov models over minimal rules, where the translation probability of each rule is adjusted to include context information from parent rules (Vaswani et al., 2011). Most prior work tends to replace the existing probabilities rather than augmenting them. We believe that Markov rules provide an additional signal but are not a replacement. Their distributions should be more informative than the so-called “lexical weighting” models, and less sparse than relative frequency estimates, though potentially not as effective for truly non-compositional units. Therefore, we explore the inclusion of all such information. Also, unlike prior work, we explore combinations of multiple decomposition orders, as well as dynamic decompositions. The most useful context for t"
N13-1002,steinberger-etal-2006-jrc,0,\N,Missing
N15-1077,D11-1039,0,0.0132022,"arser require annotated examples, ∗ This research was conducted during the author’s internship at Microsoft Research. which are expensive and time-consuming to acquire (Zelle and Mooney, 1993; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). As a result, there has been rising interest in learning semantic parsers from indirect supervision. Examples include unsupervised approaches that leverage distributional similarity by recursive clustering (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011), semi-supervised approaches that learn from dialog context (Artzi and Zettlemoyer, 2011), grounded approaches that learn from annotated question-answer pairs (Clarke et al., 2010; Liang et al., 2011) or virtual worlds (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Such progress is exciting, but most applications focus on question answering, where the semantic parser is used to convert natural-language questions into formal queries. In contrast, complex knowledge extraction represents a relatively untapped ap756 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 756–766, c Denver, Colorado, May 31 – June 5, 2015. 2015 Ass"
N15-1077,Q13-1005,0,0.0325673,"ney, 1993; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). As a result, there has been rising interest in learning semantic parsers from indirect supervision. Examples include unsupervised approaches that leverage distributional similarity by recursive clustering (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011), semi-supervised approaches that learn from dialog context (Artzi and Zettlemoyer, 2011), grounded approaches that learn from annotated question-answer pairs (Clarke et al., 2010; Liang et al., 2011) or virtual worlds (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Such progress is exciting, but most applications focus on question answering, where the semantic parser is used to convert natural-language questions into formal queries. In contrast, complex knowledge extraction represents a relatively untapped ap756 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 756–766, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics requires POS-REG stimulates POS-REG THEME CAUSE (POS-REG,BCL,(NEG-REG,IL-10,RFLAT)) (NEG-REG,TP53,(POS-REG,BCL,IL-2)) BCL CAUSE (POS-REG,AKT2,("
N15-1077,N10-1083,0,0.0900935,"Missing"
N15-1077,W09-1402,0,0.0491907,"systems, outperforming 19 out of 24 teams that participated in the GENIA event extraction shared task. With significant information loss (skipping event triggers and, most importantly, the nested event structures), it is possible to reduce GENIA events to binary relations so that existing distant-supervision methods are applicable. Yet even in such an evaluation tailored for existing methods, our system still outperformed them by a wide margin. 2 Related Work Existing approaches for GENIA event extraction are supervised methods that either used a carefully engineered classification pipeline (Bjorne et al., 2009; Quirk et al., 2011) or applied joint inference (Riedel et al., 2009; Poon and Vanderwende, 2010; Riedel and McCallum, 2011). Poon and Vanderwende (2010) used a dependency-based formulation that resembled our semantic parsing one, but learned from supervised data. Classification approaches first need to classify words into event triggers, where distant supervision is not directly applicable. In distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), if two entities are known to have a binary relation in the database, their cooccurrence in a sentence justifies labeling the instance"
N15-1077,W10-2903,0,0.160263,"icrosoft Research. which are expensive and time-consuming to acquire (Zelle and Mooney, 1993; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). As a result, there has been rising interest in learning semantic parsers from indirect supervision. Examples include unsupervised approaches that leverage distributional similarity by recursive clustering (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011), semi-supervised approaches that learn from dialog context (Artzi and Zettlemoyer, 2011), grounded approaches that learn from annotated question-answer pairs (Clarke et al., 2010; Liang et al., 2011) or virtual worlds (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Such progress is exciting, but most applications focus on question answering, where the semantic parser is used to convert natural-language questions into formal queries. In contrast, complex knowledge extraction represents a relatively untapped ap756 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 756–766, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics requires POS-REG stimulates POS-REG THEME CAUSE (PO"
N15-1077,de-marneffe-etal-2006-generating,0,0.196531,"Missing"
N15-1077,N06-1041,0,0.0386742,". Simple events do not admit multiple arguments, so they appear less often in the virtual evidence, and grounded learning has difficulty learning these event types, especially their triggers. In light of this, it’s actually remarkable that GUSPEE still learned a substantial portion of them. 4.2 Prototype-Driven Learning While full-blown annotations are undoubtedly expensive and time-consuming to generate, it is rather easy for a domain expert to provide a few trigger words per event type, such as “expression”, “expressed” for Expression. This motivates us to explore prototype-driven learning (Haghighi and Klein, 2006) in combination with grounded learning. Specifically, we simulated expert selection by picking the top five most frequent trigger words for each event type from training data. We then augmented grounded learning in GUSPEE by incorporating word emission features for each prototype word and the corresponding event state, e.g., I[lemma = express, zm = Expression]. The weights are fixed to a large number (five in our case). Table 3 shows the results with prototypes, which improved substantially. Not surprisingly, simple Event Type Expression Transcription Catabolism Phosphorylation Localization Bi"
N15-1077,P11-1055,0,0.0488258,"dency-based formulation that resembled our semantic parsing one, but learned from supervised data. Classification approaches first need to classify words into event triggers, where distant supervision is not directly applicable. In distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), if two entities are known to have a binary relation in the database, their cooccurrence in a sentence justifies labeling the instance with the relation. This assumption is often incorrect, and Riedel et al. (2010) introduced latent variables to model the uncertainty; the model was later improved by Hoffmann et al. (2011). GUSPEE generalizes this idea to structured prediction where the latent annotations are not simple classification decisions, but nested events. Krishnamurthy and Mitchell (2012) and Reddy et al. (2014) took an important step toward this direction, by learning a semantic parser based on combinatorial categorial grammar (CCG) from Freebase and web sentences. However, Krishnamurthy and Mitchell (2012) still learned from binary relations, using only simple sentences (of length ten or less). Reddy et al. (2014) learned from n-ary relations as well, yet their formulation only allows relations betwe"
N15-1077,W09-1401,0,0.636644,"e sentence also discloses important contextual information, i.e., BCL regulates RFLAT by stimulating the inhibitive effect of IL-10, and likewise the inhibition of RFLAT by IL-10 is controlled by BCL. Such context-specific knowledge is crucial in translational medicine: imagine a targeted therapy that tries to suppress RFLAT by inducing either BCL or IL-10, without taking into account their interdependency. As Figure 1 shows, this knowledge can be represented by events with nested structures (e.g., the THEME argument of E1 is an event E2), as exemplified by the GENIA event extraction dataset (Kim et al., 2009). Complex knowledge extraction can be naturally framed as a semantic parsing problem, with the event structure represented by a semantic parse; see 757 Figure 2. However, annotating example sentences is expensive and time-consuming. GENIA is the only corpus of its kind by far; its annotation took years and its scope is limited to the narrow domain of transcription in human blood cells. In contrast, databases are usually available. For example, due to the central importance of biological pathways in understanding diseases and developing drug targets, there exist many pathway databases (Schaefer"
N15-1077,D12-1069,0,0.521457,"iggers, where distant supervision is not directly applicable. In distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), if two entities are known to have a binary relation in the database, their cooccurrence in a sentence justifies labeling the instance with the relation. This assumption is often incorrect, and Riedel et al. (2010) introduced latent variables to model the uncertainty; the model was later improved by Hoffmann et al. (2011). GUSPEE generalizes this idea to structured prediction where the latent annotations are not simple classification decisions, but nested events. Krishnamurthy and Mitchell (2012) and Reddy et al. (2014) took an important step toward this direction, by learning a semantic parser based on combinatorial categorial grammar (CCG) from Freebase and web sentences. However, Krishnamurthy and Mitchell (2012) still learned from binary relations, using only simple sentences (of length ten or less). Reddy et al. (2014) learned from n-ary relations as well, yet their formulation only allows relations between entities, not relations between relations. Thus their approach 758 cannot represent nested events, let alone extracting them. And like Krishnamurthy and Mitchell (2012), Reddy"
N15-1077,P11-1060,0,0.720796,"ich are expensive and time-consuming to acquire (Zelle and Mooney, 1993; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). As a result, there has been rising interest in learning semantic parsers from indirect supervision. Examples include unsupervised approaches that leverage distributional similarity by recursive clustering (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011), semi-supervised approaches that learn from dialog context (Artzi and Zettlemoyer, 2011), grounded approaches that learn from annotated question-answer pairs (Clarke et al., 2010; Liang et al., 2011) or virtual worlds (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Such progress is exciting, but most applications focus on question answering, where the semantic parser is used to convert natural-language questions into formal queries. In contrast, complex knowledge extraction represents a relatively untapped ap756 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 756–766, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics requires POS-REG stimulates POS-REG THEME CAUSE (POS-REG,BCL,(NEG-REG,IL"
N15-1077,P09-1113,0,0.384084,"is limited to the narrow domain of transcription in human blood cells. In contrast, databases are usually available. For example, due to the central importance of biological pathways in understanding diseases and developing drug targets, there exist many pathway databases (Schaefer et al., 2009; Kanehisa, 2002; Cerami et al., 2011). Limited by manual curation, they are incomplete and not up-to-date, thereby the need for automated extraction. But compared to question answering, knowledge extraction can derive more leverage from such databases via distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009). The key insight is that databases can be used to automatically annotate sentences with a relation if the arguments of a known instance co-occur in the sentence. This learning paradigm, however, has never been applied to extracting nested events. In this paper, we propose the first approach to learn a semantic parser from a database of complex events and unannotated text, by generalizing distant supervision to complex knowledge extraction. The key idea is to recover the latent annotations via EM, guided by a structured prior that favors semantic parses containing known events in the database,"
N15-1077,D09-1001,1,0.869064,"parsing is to map text into a complete and detailed meaning representation (Mooney, 2007). Supervised approaches for learning a semantic parser require annotated examples, ∗ This research was conducted during the author’s internship at Microsoft Research. which are expensive and time-consuming to acquire (Zelle and Mooney, 1993; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). As a result, there has been rising interest in learning semantic parsers from indirect supervision. Examples include unsupervised approaches that leverage distributional similarity by recursive clustering (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011), semi-supervised approaches that learn from dialog context (Artzi and Zettlemoyer, 2011), grounded approaches that learn from annotated question-answer pairs (Clarke et al., 2010; Liang et al., 2011) or virtual worlds (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Such progress is exciting, but most applications focus on question answering, where the semantic parser is used to convert natural-language questions into formal queries. In contrast, complex knowledge extraction represents a relatively untapped ap756 Human Language Techno"
N15-1077,N10-1123,1,0.847244,"shared task. With significant information loss (skipping event triggers and, most importantly, the nested event structures), it is possible to reduce GENIA events to binary relations so that existing distant-supervision methods are applicable. Yet even in such an evaluation tailored for existing methods, our system still outperformed them by a wide margin. 2 Related Work Existing approaches for GENIA event extraction are supervised methods that either used a carefully engineered classification pipeline (Bjorne et al., 2009; Quirk et al., 2011) or applied joint inference (Riedel et al., 2009; Poon and Vanderwende, 2010; Riedel and McCallum, 2011). Poon and Vanderwende (2010) used a dependency-based formulation that resembled our semantic parsing one, but learned from supervised data. Classification approaches first need to classify words into event triggers, where distant supervision is not directly applicable. In distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), if two entities are known to have a binary relation in the database, their cooccurrence in a sentence justifies labeling the instance with the relation. This assumption is often incorrect, and Riedel et al. (2010) introduced laten"
N15-1077,P13-1092,1,0.838068,"alized distant supervision to n-ary relations for extracting template-based events, but similar to Reddy et al. (2014), they did not consider nested events. Distant supervision can be viewed as a special case of the more general paradigm of grounded learning from a database. Clarke et al. (2010) and Liang et al. (2011) used the database to determine if a candidate semantic parse would yield the annotated answer, whereas distant supervision uses the database to determine if a relation instance is contained therein. Our GUSPEE system is inspired by grounded unsupervised semantic parsing (GUSP) (Poon, 2013) and shares a similar semantic representation. GUSP, like most grounded learning approaches, applied to question answering and did not leverage distant supervision. GUSPEE can thus be viewed as an extension of GUSP to leverage distant supervision for complex knowledge extraction. Grounding in GUSPEE is materialized by virtual evidence favoring semantic structures that conform with the database. The idea of virtual evidence was first introduced by Pearl (1988) and later applied in several applications such as Subramanya and Bilmes (2007). Unlike in prior work, the virtual evidence in GUSPEE inv"
N15-1077,W11-1825,0,0.099754,"ng 19 out of 24 teams that participated in the GENIA event extraction shared task. With significant information loss (skipping event triggers and, most importantly, the nested event structures), it is possible to reduce GENIA events to binary relations so that existing distant-supervision methods are applicable. Yet even in such an evaluation tailored for existing methods, our system still outperformed them by a wide margin. 2 Related Work Existing approaches for GENIA event extraction are supervised methods that either used a carefully engineered classification pipeline (Bjorne et al., 2009; Quirk et al., 2011) or applied joint inference (Riedel et al., 2009; Poon and Vanderwende, 2010; Riedel and McCallum, 2011). Poon and Vanderwende (2010) used a dependency-based formulation that resembled our semantic parsing one, but learned from supervised data. Classification approaches first need to classify words into event triggers, where distant supervision is not directly applicable. In distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), if two entities are known to have a binary relation in the database, their cooccurrence in a sentence justifies labeling the instance with the relation. T"
N15-1077,N12-3006,1,0.591731,"ulation Total Event F1 Table 1: GENIA event extraction results of GUSPEE evaluation methodology enables us to assess the true accuracy and compare head-to-head with supervised methods. GENIA contains 800 abstracts for training and 150 for development. It also has a test set, but its annotation is not made public. Therefore, we used the training set for grounded learning and development, and reserved the development set for testing. The majority events are Regulation (including Positive regulation, Negative regulation). See Kim et al. (2009) for details. We processed all sentences using SPLAT (Quirk et al., 2012), to conduct tokenization, part-of-speech tagging, and constituency parsing. We then postprocessed the parses to obtain Stanford dependencies (de Marneffe et al., 2006). During development on the training data, we found the following parameters (Section 3) to perform quite well and used them in all subsequent experiments: κ = 20, WNULL = 4, WRAISE−P = 2, WRAISE−E = −6, L2 prior = 0.1. Interestingly, we found that encouraging protein RAISING is beneficial, which probably stems from the fact that proteins are often separated from event triggers by noun modifiers, such as “the BCL gene”, “IL-10 p"
N15-1077,reschke-etal-2014-event,0,0.0239246,"s well, yet their formulation only allows relations between entities, not relations between relations. Thus their approach 758 cannot represent nested events, let alone extracting them. And like Krishnamurthy and Mitchell (2012), Reddy et al. (2014) focused on simple text and excluded sentences where entities were not dependency neighbors (i.e., not directly connected in the ungrounded graph), as well as sentences with unknown entities. While such restrictions do not impede parsing simple questions in their evaluation, their approach is not directly applicable to complex knowledge extraction. Reschke et al. (2014) also generalized distant supervision to n-ary relations for extracting template-based events, but similar to Reddy et al. (2014), they did not consider nested events. Distant supervision can be viewed as a special case of the more general paradigm of grounded learning from a database. Clarke et al. (2010) and Liang et al. (2011) used the database to determine if a candidate semantic parse would yield the annotated answer, whereas distant supervision uses the database to determine if a relation instance is contained therein. Our GUSPEE system is inspired by grounded unsupervised semantic parsi"
N15-1077,D11-1001,0,0.0216553,"nt information loss (skipping event triggers and, most importantly, the nested event structures), it is possible to reduce GENIA events to binary relations so that existing distant-supervision methods are applicable. Yet even in such an evaluation tailored for existing methods, our system still outperformed them by a wide margin. 2 Related Work Existing approaches for GENIA event extraction are supervised methods that either used a carefully engineered classification pipeline (Bjorne et al., 2009; Quirk et al., 2011) or applied joint inference (Riedel et al., 2009; Poon and Vanderwende, 2010; Riedel and McCallum, 2011). Poon and Vanderwende (2010) used a dependency-based formulation that resembled our semantic parsing one, but learned from supervised data. Classification approaches first need to classify words into event triggers, where distant supervision is not directly applicable. In distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), if two entities are known to have a binary relation in the database, their cooccurrence in a sentence justifies labeling the instance with the relation. This assumption is often incorrect, and Riedel et al. (2010) introduced latent variables to model the unc"
N15-1077,W09-1406,0,0.0199018,"ENIA event extraction shared task. With significant information loss (skipping event triggers and, most importantly, the nested event structures), it is possible to reduce GENIA events to binary relations so that existing distant-supervision methods are applicable. Yet even in such an evaluation tailored for existing methods, our system still outperformed them by a wide margin. 2 Related Work Existing approaches for GENIA event extraction are supervised methods that either used a carefully engineered classification pipeline (Bjorne et al., 2009; Quirk et al., 2011) or applied joint inference (Riedel et al., 2009; Poon and Vanderwende, 2010; Riedel and McCallum, 2011). Poon and Vanderwende (2010) used a dependency-based formulation that resembled our semantic parsing one, but learned from supervised data. Classification approaches first need to classify words into event triggers, where distant supervision is not directly applicable. In distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), if two entities are known to have a binary relation in the database, their cooccurrence in a sentence justifies labeling the instance with the relation. This assumption is often incorrect, and Riedel et"
N15-1077,P13-1045,0,0.0822634,"Missing"
N15-1077,N07-2042,0,0.790714,"to automatically annotate sentences with a relation if the arguments of a known instance co-occur in the sentence. This learning paradigm, however, has never been applied to extracting nested events. In this paper, we propose the first approach to learn a semantic parser from a database of complex events and unannotated text, by generalizing distant supervision to complex knowledge extraction. The key idea is to recover the latent annotations via EM, guided by a structured prior that favors semantic parses containing known events in the database, in the form of virtual evidence (Pearl, 1988; Subramanya and Bilmes, 2007). Experiments on the GENIA dataset demonstrate the promise of this direction. Our GUSPEE (GroUnded Semantic Parsing for Event Extraction) system can successfully learn from and extract complex events, without requiring textual annotations (Figure 2). Moreover, after incorporating prototype-driven learning using just five example words for each event type, GUSPEE becomes competitive even among supervised systems, outperforming 19 out of 24 teams that participated in the GENIA event extraction shared task. With significant information loss (skipping event triggers and, most importantly, the nest"
N15-1077,P11-1145,0,0.0165211,"d meaning representation (Mooney, 2007). Supervised approaches for learning a semantic parser require annotated examples, ∗ This research was conducted during the author’s internship at Microsoft Research. which are expensive and time-consuming to acquire (Zelle and Mooney, 1993; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). As a result, there has been rising interest in learning semantic parsers from indirect supervision. Examples include unsupervised approaches that leverage distributional similarity by recursive clustering (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011), semi-supervised approaches that learn from dialog context (Artzi and Zettlemoyer, 2011), grounded approaches that learn from annotated question-answer pairs (Clarke et al., 2010; Liang et al., 2011) or virtual worlds (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Such progress is exciting, but most applications focus on question answering, where the semantic parser is used to convert natural-language questions into formal queries. In contrast, complex knowledge extraction represents a relatively untapped ap756 Human Language Technologies: The 2015 Annual Conference of the North Americ"
N15-1077,D07-1071,0,0.0534467,"simple relations among entities (top), whereas ideally we want to extract the complex event that captures important contextual information (middle), as exemplified by the GENIA event annotation (bottom). Introduction The goal of semantic parsing is to map text into a complete and detailed meaning representation (Mooney, 2007). Supervised approaches for learning a semantic parser require annotated examples, ∗ This research was conducted during the author’s internship at Microsoft Research. which are expensive and time-consuming to acquire (Zelle and Mooney, 1993; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007). As a result, there has been rising interest in learning semantic parsers from indirect supervision. Examples include unsupervised approaches that leverage distributional similarity by recursive clustering (Poon and Domingos, 2009; Poon and Domingos, 2010; Titov and Klementiev, 2011), semi-supervised approaches that learn from dialog context (Artzi and Zettlemoyer, 2011), grounded approaches that learn from annotated question-answer pairs (Clarke et al., 2010; Liang et al., 2011) or virtual worlds (Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013). Such progress is exciting, but most applic"
N15-1077,P10-1031,1,\N,Missing
N15-1077,Q14-1030,0,\N,Missing
N15-2014,J92-4003,0,0.124231,"Hansard Hansard Hansard-Committees Hansard-Committees Authored Language English French English French English French Translation Language Training Sentences Test Sentences French 62k 6k English 43k 4k French 1,697k 169k English 567k 56k French 2,930k 292k English 636k 63k Table 2: Cross-Domain Data Sets make a small modification to the feature used to obtain the best previously reported sentence level performance (Eetemadi and Toutanova, 2014) to derive a new type of features. POS MTU n-gram features are the most linguistically informed features amongst prior work. We introduce Brown cluster (Brown et al., 1992) MTUs instead. Our use of Brown clusters is inspired by recent success on their use in statistical machine translation systems (Bhatia et al., 2014; Durrani et al., 2014). Finally, we also include source and target Brown cluster n-grams as a comparison point to better understand their effectiveness compared to POS n-grams and their contribution to the effectiveness of Brown cluster MTUs. Given these 8 feature types summarized in Table 1, n-gram lengths of up to 5 and the 3 × 3 data matrix explained in the next section, we run 360 experiments for this cross-domain study. 4 Data, Preprocessing a"
N15-2014,C14-1041,0,0.0136543,"ces French 62k 6k English 43k 4k French 1,697k 169k English 567k 56k French 2,930k 292k English 636k 63k Table 2: Cross-Domain Data Sets make a small modification to the feature used to obtain the best previously reported sentence level performance (Eetemadi and Toutanova, 2014) to derive a new type of features. POS MTU n-gram features are the most linguistically informed features amongst prior work. We introduce Brown cluster (Brown et al., 1992) MTUs instead. Our use of Brown clusters is inspired by recent success on their use in statistical machine translation systems (Bhatia et al., 2014; Durrani et al., 2014). Finally, we also include source and target Brown cluster n-grams as a comparison point to better understand their effectiveness compared to POS n-grams and their contribution to the effectiveness of Brown cluster MTUs. Given these 8 feature types summarized in Table 1, n-gram lengths of up to 5 and the 3 × 3 data matrix explained in the next section, we run 360 experiments for this cross-domain study. 4 Data, Preprocessing and Feature Extraction We chose the English-French language pair for our cross-domain experiments based on prior work and availability of labeled data. Existing sentencepa"
N15-2014,D14-1018,1,0.804549,"ive) experiments for each feature set mentioned above. In addition to the features mentioned above, we 3 Minimal Translation Units (Quirk and Menezes, 2006) Corpus EuroParl EuroParl Hansard Hansard Hansard-Committees Hansard-Committees Authored Language English French English French English French Translation Language Training Sentences Test Sentences French 62k 6k English 43k 4k French 1,697k 169k English 567k 56k French 2,930k 292k English 636k 63k Table 2: Cross-Domain Data Sets make a small modification to the feature used to obtain the best previously reported sentence level performance (Eetemadi and Toutanova, 2014) to derive a new type of features. POS MTU n-gram features are the most linguistically informed features amongst prior work. We introduce Brown cluster (Brown et al., 1992) MTUs instead. Our use of Brown clusters is inspired by recent success on their use in statistical machine translation systems (Bhatia et al., 2014; Durrani et al., 2014). Finally, we also include source and target Brown cluster n-grams as a comparison point to better understand their effectiveness compared to POS n-grams and their contribution to the effectiveness of Brown cluster MTUs. Given these 8 feature types summarize"
N15-2014,islam-mehler-2012-customization,0,0.02027,"rmation, and additional processing is necessary to compile a dataset with such information (labels). Kurokawa et al (2009) extract translation direction information from the English-French Hansard parallel dataset using speaker language tags. We use this dataset, and treat the two sections “main parliamentary proceedings” and “committee hearings” as two different corpora. These two corpora have slightly different domains, although they share many common topics as well. We additionally choose a third corpus, whose domain is more distinct from these two, from the EuroParl English-French corpus. Islam and Mehler (2012) provided a customized version of Europarl 105 with translation direction labels, but this dataset only contains sentences that were authored in English and translated to French, and does not contain examples for which the original language of authoring was French. We thus prepare a new dataset from EuroParl and will make it publicly available for use. The original unprocessed version of EuroParl (Koehn, 2005) contains speaker language tags (original language of authoring) for the French and English sides of the parallel corpus. We filter out inconsistencies in the corpus. First, we filter out"
N15-2014,2005.mtsummit-papers.11,0,0.196317,"Missing"
N15-2014,P11-1132,0,0.216523,"Missing"
N15-2014,2009.mtsummit-papers.9,0,0.904779,"d type of features to outperform the best previously proposed features in detecting translation direction and achieve 0.80 precision with 0.85 recall. 1 Introduction Translated text differs from authored text (Baker, 1993). The main differences are simplification, explicitation, normalization and interference (Volansky et al., 2013). Statistical classifiers have been trained to detect Translationese1 . Volansky et al. (2013) state two motivations for automatic detection of Translationese: empirical validation of Translationese linguistic theories and improving statistical machine translation (Kurokawa et al., 2009). 1 Translated text is often referred to as “Translationese” (Volansky et al., 2013). Kristina Toutanova Microsoft Research Redmond, WA kristout@microsoft.com Most of the prior work focus on in-domain Translationese detection (Baroni and Bernardini, 2006; Kurokawa et al., 2009). That is, the training and test set come from the same, usually narrow, domain. Cross-domain Translationese detection serves the two stated motivations better than in-domain detection. First, automatic classification validates linguistic theories only if it works independent of the domain. Otherwise, the classifier coul"
N15-2014,N06-1002,0,0.0204867,"Missing"
N15-2014,W00-1308,0,0.349493,"Missing"
N15-2014,C96-2141,0,0.635669,"Missing"
N15-2014,W14-3315,0,\N,Missing
N15-2014,W13-2201,0,\N,Missing
N19-1300,Q19-1026,1,0.809429,"s a result, they can be used to construct highly inferential reading comprehension datasets that have the added benefit of being directly related to the practical end-task of answering user yes/no questions. Yes/No questions do appear as a subset of some existing datasets (Reddy et al., 2018; Choi et al., 2018; Yang et al., 2018). However, these datasets are primarily intended to test other aspects of question answering (QA), such as conversational QA or multi-step reasoning, and do not contain naturally occurring questions. We follow the data collection method used by Natural Questions (NQ) (Kwiatkowski et al., 2019) to gather 16,000 naturally occurring yes/no questions into a dataset we call BoolQ (for Boolean Questions). Each question is paired with a paragraph from Wikipedia that an independent annotator has marked as containing the answer. The task is then to take a question and passage as input, and to return “yes” or “no” as output. Figure 1 contains some examples, and Appendix A.1 contains additional randomly selected examples. Following recent work (Wang et al., 2018), we focus on using transfer learning to establish baselines for our dataset. Yes/No QA is closely related to many other NLP tasks,"
N19-1300,D15-1075,0,0.265278,"Missing"
N19-1300,P17-1152,0,0.0881736,"Missing"
N19-1300,D18-1241,0,0.0608786,"Missing"
N19-1300,L18-1269,0,0.0405572,"Missing"
N19-1300,N18-2017,0,0.0305137,"consists of a question (Q), an excerpt from a passage (P), and an answer (A) with an explanation added for clarity. Introduction 1 Has the UK been hit by a hurricane? The Great Storm of 1987 was a violent extratropical cyclone which caused casualties in England, France and the Channel Islands . . . Yes. [An example event is given.] ference (NLI) proposed the task of labeling candidate statements as being entailed or contradicted by a given passage. However, in practice, generating candidate statements that test for complex inferential abilities is challenging. For instance, evidence suggests (Gururangan et al., 2018; Jia and Liang, 2017; McCoy et al., 2019) that simply asking human annotators to write candidate statements will result in examples that typically only require surface-level reasoning. In this paper we propose an alternative: we test models on their ability to answer naturally occurring yes/no questions. That is, questions that were authored by people who were not prompted to write particular kinds of questions, including even being required to write yes/no questions, and who did not know the answer to the question they were asking. Figure 1 contains some examples from our dataset. We find su"
N19-1300,D17-1215,0,0.0358614,"), an excerpt from a passage (P), and an answer (A) with an explanation added for clarity. Introduction 1 Has the UK been hit by a hurricane? The Great Storm of 1987 was a violent extratropical cyclone which caused casualties in England, France and the Channel Islands . . . Yes. [An example event is given.] ference (NLI) proposed the task of labeling candidate statements as being entailed or contradicted by a given passage. However, in practice, generating candidate statements that test for complex inferential abilities is challenging. For instance, evidence suggests (Gururangan et al., 2018; Jia and Liang, 2017; McCoy et al., 2019) that simply asking human annotators to write candidate statements will result in examples that typically only require surface-level reasoning. In this paper we propose an alternative: we test models on their ability to answer naturally occurring yes/no questions. That is, questions that were authored by people who were not prompted to write particular kinds of questions, including even being required to write yes/no questions, and who did not know the answer to the question they were asking. Figure 1 contains some examples from our dataset. We find such questions often qu"
N19-1300,P17-1147,0,0.100114,"Missing"
N19-1300,P19-1334,0,0.0296322,"Missing"
N19-1300,D18-1260,0,0.0376092,"Missing"
N19-1300,L18-1008,0,0.0292594,"Missing"
N19-1300,D16-1244,0,0.0948182,"Missing"
N19-1300,N18-1202,1,0.559304,"e detail: Our Recurrent model follows a standard recurrent plus attention architecture for text-pair classification (Wang et al., 2018). It embeds the premise/hypothesis text using fasttext word vectors (Mikolov et al., 2018) and learned character vectors, applies a shared bidirectional LSTM to both parts, applies co-attention (Parikh et al., 2016) to share information between the two parts, applies another bi-LSTM to both parts, pools the result, and uses the pooled representation to predict the final class. See Appendix A.2 for details. Our Recurrent +ELMo model uses the language model from Peters et al. (2018) to provide contextualized embeddings to the baseline model outlined above, as recommended by the authors. Our OpenAI GPT model fine-tunes the 12 layer 768 dimensional uni-directional transformer from Radford et al. (2018), which has been pretrained as a language model on the Books corpus (Zhu et al., 2015). Our BERTL model fine-tunes the 24 layer 1024 dimensional transformer from Devlin et al. (2018), which has been trained on next-sentence-selection and masked language modelling on the Book Corpus and Wikipedia. We fine-tune the BERTL and the OpenAI GPT models using the optimizers recommende"
N19-1300,W18-5441,0,0.0742579,"Missing"
N19-1300,P18-2124,0,0.099408,"Missing"
N19-1300,D18-1233,0,0.064836,"Missing"
N19-1300,W18-5446,0,0.39751,"ning, and do not contain naturally occurring questions. We follow the data collection method used by Natural Questions (NQ) (Kwiatkowski et al., 2019) to gather 16,000 naturally occurring yes/no questions into a dataset we call BoolQ (for Boolean Questions). Each question is paired with a paragraph from Wikipedia that an independent annotator has marked as containing the answer. The task is then to take a question and passage as input, and to return “yes” or “no” as output. Figure 1 contains some examples, and Appendix A.1 contains additional randomly selected examples. Following recent work (Wang et al., 2018), we focus on using transfer learning to establish baselines for our dataset. Yes/No QA is closely related to many other NLP tasks, including other forms of question answering, entailment, and paraphrasing. Therefore, it is not clear what the best data sources to transfer from are, or if it will be sufficient to just transfer from powerful pretrained language models such as BERT (Devlin et al., 2018) or ELMo (Peters et al., 2018). We experiment with state-of-the-art unsupervised approaches, using existing entailment datasets, three methods of leveraging extractive QA data, and using a few othe"
N19-1300,Q18-1021,0,0.0822727,"Missing"
N19-1300,N18-1101,0,0.16728,"Missing"
N19-1300,D18-1259,0,0.0909819,"Missing"
N19-1300,D18-1009,0,0.09126,"Missing"
N19-1423,C18-1139,0,0.153493,"a, June 2 - June 7, 2019. 2019 Association for Computational Linguistics word based only on its context. Unlike left-toright language model pre-training, the MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. In addition to the masked language model, we also use a “next sentence prediction” task that jointly pretrains text-pair representations. The contributions of our paper are as follows: • We demonstrate the importance of bidirectional pre-training for language representations. Unlike Radford et al. (2018), which uses unidirectional language models for pre-training, BERT uses masked language models to enable pretrained deep bidirectional representations. This is also in contrast to Peters et al. (2018a), which uses a shallow concatenation of independently trained left-to-right and right-to-left LMs. • We show that pre-trained representations reduce the need for many heavily-engineered taskspecific architectures. BERT is the first finetuning based representation model that achieves state-of-the-art performance on a large suite of sentence-level and token-level tasks, outperforming many task-spec"
P02-1019,P00-1037,1,0.649272,"ing corrections for words that are not found in a dictionary. Notice, however, that the noisy channel model offers the possibility of correcting misspellings without a dictionary, as long as sufficient data is available to estimate the source model facOsama bin Laden and tors. For example, if r w Ossama bin Laden, the model will predict that the correct spelling r is more likely than the incorrect spelling w, provided that () ( ) () = ( ) = P (w) P (wjr) &lt; P (r) P (wjw) ( ) ( ) where P wjr =P wjw would be approximately the odds of doubling the s in Osama. We do not pursue this, here, however. Brill and Moore (2000) present an improved error model for noisy channel spelling correction that goes beyond single insertions, deletions, substitutions, and transpositions. The model has a set of parameters P ! for letter sequences of lengths up to . An extension they presented has refined parameters P ! jP SN which also depend on the position of the substitution in the source word. According to this model, the misspelling is generated by the correct word as follows: First, a person picks a partition of the correct word and then types each partition independently, possibly making some errors. The probability for"
P02-1019,C90-2036,0,0.944079,"Missing"
P05-1073,P98-1013,0,0.0556257,"parse trees on PropBank. We propose a discriminative log-linear joint model for semantic role labeling, which incorporates more global features and achieves superior performance in comparison to state-of-the-art models. To deal with the computational complexity of the task, we employ dynamic programming and reranking approaches. We present performance results on the February 2004 version of PropBank on gold-standard parse trees as well as results on automatic parses generated by Charniak’s parser (Charniak, 2000). 1 Introduction The release of semantically annotated corpora such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2003) has made it possible to develop high-accuracy statistical models for automated semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004). Such systems have identified several linguistically motivated features for discriminating arguments and their labels (see Table 1). These features usually characterize aspects of individual arguments and the predicate. It is evident that the labels and the features of arguments are highly correlated. For example, there are hard constraints – that arguments cannot overlap 2 Semantic Role"
P05-1073,W04-2412,0,0.192254,"Missing"
P05-1073,A00-2018,0,0.169312,"Missing"
P05-1073,J02-3001,0,0.906079,"e Stanford University Stanford, CA, 94305 Aria Haghighi Dept of Computer Science Stanford University Stanford, CA, 94305 Christopher D. Manning Dept of Computer Science Stanford University Stanford, CA, 94305 kristina@cs.stanford.edu aria42@stanford.edu manning@cs.stanford.edu Abstract with each other or the predicate, and also soft constraints – for example, is it unlikely that a predicate will have two or more AGENT arguments, or that a predicate used in the active voice will have a THEME argument prior to an AGENT argument. Several systems have incorporated such dependencies, for example, (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Thompson et al., 2003) and several systems submitted in the CoNLL-2004 shared task (Carreras and M`arquez, 2004). However, we show that there are greater gains to be had by modeling joint information about a verb’s argument structure. Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding. This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between ar"
P05-1073,J93-2004,0,0.0342212,"eling, we assume the existence of a separate parsing model that can assign a parse tree t to each sentence, and the task then is to label each node in the parse tree with the semantic role of the phrase it dominates, or NONE, if the phrase does not fill any role. We do stress however that the joint framework and features proposed here can also be used when only a shallow parse (chunked) representation is available as in the CoNLL-2004 shared task (Carreras and M`arquez, 2004). In the February 2004 version of the PropBank corpus, annotations are done on top of the Penn TreeBank II parse trees (Marcus et al., 1993). Possible labels of arguments in this corpus are the core argument labels ARG [0, 1, 2, 3, 4, 5], and the modifier argument labels. The core arguments ARG [3, 4, 5] do not have consistent global roles and tend to be verb specific. There are about 14 modifier labels such as ARGM - LOC and ARGM - TMP, for location and temporal modifiers respectively.1 Figure 1 shows an example parse tree annotated with semantic roles. We distinguish between models that learn to label nodes in the parse tree independently, called local models, and models that incorporate dependencies among the labels of multiple"
P05-1073,N04-1030,0,0.77673,"ord, CA, 94305 Aria Haghighi Dept of Computer Science Stanford University Stanford, CA, 94305 Christopher D. Manning Dept of Computer Science Stanford University Stanford, CA, 94305 kristina@cs.stanford.edu aria42@stanford.edu manning@cs.stanford.edu Abstract with each other or the predicate, and also soft constraints – for example, is it unlikely that a predicate will have two or more AGENT arguments, or that a predicate used in the active voice will have a THEME argument prior to an AGENT argument. Several systems have incorporated such dependencies, for example, (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Thompson et al., 2003) and several systems submitted in the CoNLL-2004 shared task (Carreras and M`arquez, 2004). However, we show that there are greater gains to be had by modeling joint information about a verb’s argument structure. Despite much recent progress on accurate semantic role labeling, previous work has largely used independent classifiers, possibly combined with separate label sequence models via Viterbi decoding. This stands in stark contrast to the linguistic observation that a core argument frame is a joint structure, with strong dependencies between arguments. We show how t"
P05-1073,W04-2421,0,0.0181359,"tion for CORE arguments in F-Measure and Frame Accuracy respectively. Model Local Joint C ORE F1 84.1 85.8 Acc. 66.5 72.7 F1 81.4 82.9 A RGM Acc. 55.6 60.8 Table 5: Performance of local and joint models on identification+classification on section 23, using Charniak automatically generated parse trees. References 6 Related Work Several semantic role labeling systems have successfully utilized joint information. (Gildea and Jurafsky, 2002) used the empirical probability of the set of proposed arguments as a prior distribution. (Pradhan et al., 2004) train a language model over label sequences. (Punyakanok et al., 2004) use a linear programming framework to ensure that the only argument frames which get probability mass are ones that respect global constraints on argument labels. The key differences of our approach compared to previous work are that our model has all of the following properties: (i) we do not assume a finite Markov horizon for dependencies among node labels, (ii) we include features looking at the labels of multiple argument nodes and internal features of these nodes, and (iii) we train a discriminative model capable of incorporating these long-distance dependencies. 7 Conclusions Reflecting"
P05-1073,P03-1002,0,0.537983,"his way the training set for the classification models is smaller. Note that we don’t do any hard pruning at the identification stage in testing and can find the exact labeling of the complete parse tree, which is the maximizer of Equation 1. Thus we do not have accuracy loss as in the two-pass hard prune strategy described in (Pradhan et al., 2005). In previous work, various machine learning methods have been used to learn local classifiers for role labeling. Examples are linearly interpolated relative frequency models (Gildea and Jurafsky, 2002), SVMs (Pradhan et al., 2004), decision trees (Surdeanu et al., 2003), and log-linear models (Xue and Palmer, 2004). In this work we use log-linear models for multi-class classification. One advantage of log-linear models over SVMs for us is that they produce probability distributions and thus identification A problem with this approach is that a maximizing labeling of the nodes could possibly violate the constraint that argument nodes should not overlap with each other. Therefore, to produce a consistent set of arguments with local classifiers, we must have a way of enforcing the non-overlapping constraint. Standard Features (Gildea and Jurafsky, 2002) P HRASE"
P05-1073,W04-3212,0,0.684627,"e-art models. To deal with the computational complexity of the task, we employ dynamic programming and reranking approaches. We present performance results on the February 2004 version of PropBank on gold-standard parse trees as well as results on automatic parses generated by Charniak’s parser (Charniak, 2000). 1 Introduction The release of semantically annotated corpora such as FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2003) has made it possible to develop high-accuracy statistical models for automated semantic role labeling (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Xue and Palmer, 2004). Such systems have identified several linguistically motivated features for discriminating arguments and their labels (see Table 1). These features usually characterize aspects of individual arguments and the predicate. It is evident that the labels and the features of arguments are highly correlated. For example, there are hard constraints – that arguments cannot overlap 2 Semantic Role Labeling: Task Definition and Architectures Consider the pair of sentences, • [The GM-Jaguar pact] AGENT gives [the car market]RECIPIENT [a much-needed boost] THEME • [A much-needed boost] THEME was given to"
P05-1073,A00-2031,0,\N,Missing
P05-1073,C04-1197,0,\N,Missing
P05-1073,J03-4003,0,\N,Missing
P05-1073,C98-1013,0,\N,Missing
P05-1073,W05-0620,0,\N,Missing
P05-1073,J05-1004,0,\N,Missing
P06-1132,W04-0907,0,0.0227215,"Missing"
P06-1132,A00-2031,0,0.139592,"Missing"
P06-1132,W05-0620,0,0.0686402,"Missing"
P06-1132,P02-1004,0,0.0571065,"Missing"
P06-1132,J02-3001,0,0.344957,"cles. These particles are used to conjoin words and phrases, corresponding to English ""and"" and ""or"". As their occurrence is not predictable from the sentence structure alone, we did not include them in the current prediction task. Figure 1. Example of case markers in Japanese (taken from the Kyoto Corpus). Square brackets indicate bunsetsu (phrase) boundaries, to be discussed below. Arrows between phrases indicate dependency relations. using Japanese sentences and their dependency structure alone. We formulated this task after the well-studied task of semantic role labeling in English (e.g., Gildea and Jurafsky, 2002; Carreras and Màrques, 2005), whose goal is to assign one of 20 semantic role labels to each phrase in a sentence with respect to a given predicate, based on the annotations provided by PropBank (Palmer et al., 2005). Though the task of case marker prediction is more ambiguous and subject to uncertainty than the semantic role labeling task, we obtained some encouraging results which we present in Section 4. Next, in Section 5, we describe the bilingual task, in which information about case assignment can be extracted from a corresponding source language sentence. Though the process of MT intr"
P06-1132,C04-1186,0,0.0159687,"various machine learning methods that can be used to train the classifiers, we chose log-linear models for both identification and classification tasks, as they 4.1 Features The basic local model features we used for the identification and classification models are listed in Table 2. They consist of features for a phrase, for its parent phrase and for their relations. Only one feature (GrandparentNounSubPos) currently refers to the grandparent of the phrase; all other features are between the phrase, its parent and its sibling nodes, and are a superset of the dependency-based features used by Hacioglu (2004) for the semantic labeling task. In addition to these basic features, we added 20 combined features, some of which are shown at the bottom of Table 2. For the joint model, we implemented only two types of features: sequence of non-NONE case markers for a set of sister phrases, and repetition of non-NONE case markers. These features are intended to capture regularities in the sequence of case markers of phrases that modify the same head phrase. All of these features are represented as binary features: that is, when the value of a feature is not binary, we have treated the combination of the fea"
P06-1132,P00-1056,0,0.0250547,"ate translation using a case prediction model; the experiments described in this section on reference translations serve as an important preliminary step toward achieving that final goal. We will show in this section that even the automatically derived syntactic information is very useful in assigning case markers in 5.1 Data and task set-up The dataset we used is a collection of parallel English-Japanese sentences from a technical (computer) domain. We used 15,000 sentence pairs for training, 5,000 for development, and 4,241 for testing. The parallel sentences were word-aligned using GIZA++ (Och and Ney, 2000), and submitted to a tree-to-string-based MT system (Quirk et al., 2005) which utilizes the dependency structure of the source language and projects dependency structure to the target language. Figure 3 shows an example of an aligned sentence pair: on the source (English) side, part-of-speech (POS) tags and word dependency structure are assigned (solid arcs). The alignments between English and Japanese words are indicated by the dotted lines. In order to create phrase-level dependency structures like the ones utilized in the Kyoto Corpus monolingual task, we derived some additional information"
P06-1132,J05-1004,0,0.00908893,"ion task. Figure 1. Example of case markers in Japanese (taken from the Kyoto Corpus). Square brackets indicate bunsetsu (phrase) boundaries, to be discussed below. Arrows between phrases indicate dependency relations. using Japanese sentences and their dependency structure alone. We formulated this task after the well-studied task of semantic role labeling in English (e.g., Gildea and Jurafsky, 2002; Carreras and Màrques, 2005), whose goal is to assign one of 20 semantic role labels to each phrase in a sentence with respect to a given predicate, based on the annotations provided by PropBank (Palmer et al., 2005). Though the task of case marker prediction is more ambiguous and subject to uncertainty than the semantic role labeling task, we obtained some encouraging results which we present in Section 4. Next, in Section 5, we describe the bilingual task, in which information about case assignment can be extracted from a corresponding source language sentence. Though the process of MT introduces uncertainties in generating the features we use, we show that the benefit of using dependency structure in our models is far greater than not using it even when the assigned structure is not perfect. 2 The task"
P06-1132,N04-1030,0,0.0214283,"loy a much richer set of features. 3 Classifiers for case prediction We implemented two types of models for the task of case prediction: local models, which choose the case marker of each phrase independently of the case markers of other phrases, and joint models, which incorporate dependencies among the case markers of dependents of the same head phrase. We describe the two types of models in turn. 3.1 Local classifiers Following the standard practice in semantic role labeling, we divided the case prediction task into the tasks of identification and classification (Gildea and Jurafsky, 2002; Pradhan et al., 2004). In the identification task, we assign to each phrase one of two labels: HASCASE, meaning that the phrase has a case marker, or NONE, meaning that it does not have a case. In the 1051 produce probability distributions which allows chaining of the two component models and easy integration into an MT system. Basic features for phrases (self, parent) HeadPOS, PrevHeadPOS, NextHeadPOS PrevPOS,Prev2POS,NextPOS,Next2POS HeadNounSubPos: time, formal nouns, adverbial HeadLemma HeadWord, PrevHeadWord, NextHeadWord PrevWord, Prev2Word, NextWord, Next2Word LastWordLemma (excluding case markers) LastWord"
P06-1132,P05-1034,0,0.266412,"cles in Kyoto Corpus. 1050 case markers ga wo 4 no ni kara to de e made yori wa        grammatical functions (e.g.) subject; object object; path genitive; subject dative object, location source quotative, reciprocal, as location, instrument, cause goal, direction goal (up to, until) source, object of comparison topic +wa        Table 1. Case markers included in this study with a dependency structure. For our monolingual experiments, we used the dependency structure annotation in the Kyoto Corpus; for our bilingual experiments, we used automatically derived dependency structure (Quirk et al., 2005). Each bunsetsu (or simply phrase in this paper) is defined as consisting of one content word (or n-content words in the case of compounds with n-components) plus any number of function words (including particles, auxiliaries and affixes). Case markers are classified as function words, and there is at most one case marker per phrase.3 In testing, the case marker for each phrase is hidden; the task is to assign to each phrase one of the 18 case markers defined above or NONE; NONE indicates that the phrase does not have a case marker. 2.3 Related work Though the task of case marker prediction as"
P06-1132,P05-1073,1,0.893667,"edicateNominal HasNominalizer HasPunctuation: comma, period HasFiniteClausalModifier RelativePosition: sole, first, mid, last NSiblings (number of siblings) Position (absolute position among siblings) Voice: pass, caus, passcaus Negation Basic features for phrase relations (parent-child pair) DependencyType: D,P,A,I Distance: linear distance in bunsetsu, 1, 2-5, >6 Subcat: POS tag of parent + POS tag of all children + indication for current Combined features (selected) HeadPOS + HeadLemma ParentLemma + HeadLemma Position + NSiblings IsFiniteClause + GrandparentNounSubPos 3.2 Joint classifiers Toutanova et al. (2005) report a substantial improvement in performance on the semantic role labeling task by building a joint classifier, which takes the labels of other phrases into account when classifying a given phrase. This is motivated by the fact that the argument structure is a joint structure, with strong dependencies among arguments. Since the case markers also reflect the argument structure to some extent, we implemented a joint classifier for the case prediction task as well. We applied the joint classifiers in the framework of N-best reranking (Collins, 2000), following Toutanova et al. (2005). That is"
P06-1132,C02-1064,0,0.202662,"Missing"
P07-1002,P06-1067,0,0.210214,"how that combining discriminative training with features to detect these two different kinds of movement phenomena leads to substantial improvements in word ordering performance over strong baselines. Integrating this word order model in a baseline MT system results in a 2.4 points improvement in BLEU for English to Japanese translation. 1 On the other hand, there is also significant amount of information in the surface strings of the source and target and their alignment. Many state-of-the-art SMT systems do not use trees and base the ordering decisions on surface phrases (Och and Ney, 2004; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). In this paper we develop an order model for machine translation which makes use of both syntactic and surface information. Introduction The machine translation task can be viewed as consisting of two subtasks: predicting the collection of words in a translation, and deciding the order of the predicted words. For some language pairs, such as English and Japanese, the ordering problem is especially hard, because the target word order differs significantly from the source word order. Previous work has shown that it is useful to model target language order in terms of movemen"
P07-1002,P05-1033,0,0.0689778,"some language pairs, such as English and Japanese, the ordering problem is especially hard, because the target word order differs significantly from the source word order. Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees (Yamada and Knight, 2001; Galley et al., 2006) or dependency trees (Quirk et al., 2005), which are obtained using a parser trained to determine linguistic constituency. Alternatively, order is modelled in terms of movement of automatically induced hierarchical structure of sentences (Chiang, 2005; Wu, 1997). ∗ This research was conducted during the author’s internship at Microsoft Research. The framework for our statistical model is as follows. We assume the existence of a dependency tree for the source sentence, an unordered dependency tree for the target sentence, and a word alignment between the target and source sentences. Figure 1 (a) shows an example of aligned source and target dependency trees. Our task is to order the target dependency tree. We train a statistical model to select the best order of the unordered target dependency tree. An important advantage of our model is th"
P07-1002,W02-1039,0,0.0585169,"Missing"
P07-1002,P06-1121,0,0.0120856,"anslation which makes use of both syntactic and surface information. Introduction The machine translation task can be viewed as consisting of two subtasks: predicting the collection of words in a translation, and deciding the order of the predicted words. For some language pairs, such as English and Japanese, the ordering problem is especially hard, because the target word order differs significantly from the source word order. Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees (Yamada and Knight, 2001; Galley et al., 2006) or dependency trees (Quirk et al., 2005), which are obtained using a parser trained to determine linguistic constituency. Alternatively, order is modelled in terms of movement of automatically induced hierarchical structure of sentences (Chiang, 2005; Wu, 1997). ∗ This research was conducted during the author’s internship at Microsoft Research. The framework for our statistical model is as follows. We assume the existence of a dependency tree for the source sentence, an unordered dependency tree for the target sentence, and a word alignment between the target and source sentences. Figure 1 (a"
P07-1002,koen-2004-pharaoh,0,0.150487,"of possible orders. Since the space of all possible orders of an unordered dependency tree is factorially large, we train our model on N-best lists of possible orders. These N-best lists are generated using approximate search and simpler models, as in the re-ranking approach of (Collins, 2000). We first evaluate our model on the task of ordering target sentences, given correct (reference) unordered target dependency trees. Our results show that combining features derived from the source and target dependency trees, distortion surface order-based features (like the distortion used in Pharaoh (Koehn, 2004)) and language model-like features results in a model which significantly outperforms models using only some of the information sources. We also evaluate the contribution of our model to the performance of an MT system. We integrate our order model in the MT system, by simply re-ordering the target translation sentences output by the system. The model resulted in an improvement from 33.6 to 35.4 BLEU points in English-toJapanese translation on a computer domain. 2 Task Setup The ordering problem in MT can be formulated as the task of ordering a target bag of words, given a source sentence and"
P07-1002,N06-1004,0,0.0522181,"ve training with features to detect these two different kinds of movement phenomena leads to substantial improvements in word ordering performance over strong baselines. Integrating this word order model in a baseline MT system results in a 2.4 points improvement in BLEU for English to Japanese translation. 1 On the other hand, there is also significant amount of information in the surface strings of the source and target and their alignment. Many state-of-the-art SMT systems do not use trees and base the ordering decisions on surface phrases (Och and Ney, 2004; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). In this paper we develop an order model for machine translation which makes use of both syntactic and surface information. Introduction The machine translation task can be viewed as consisting of two subtasks: predicting the collection of words in a translation, and deciding the order of the predicted words. For some language pairs, such as English and Japanese, the ordering problem is especially hard, because the target word order differs significantly from the source word order. Previous work has shown that it is useful to model target language order in terms of movement of syntactic const"
P07-1002,P02-1038,0,0.0133185,"s based on treelet pairs. A treelet is a connected subgraph of the source or target dependency tree. A treelet translation pair is a pair of word-aligned source and target treelets. The baseline SMT model combines this treelet translation model with other feature functions — a target language model, a tree order model, lexical weighting features to smooth the translation probabilities, word count feature, and treelet-pairs count feature. These models are combined as feature functions in a (log)linear model for predicting a target sentence given a source sentence, in the framework proposed by (Och and Ney, 2002). The weights of this model are trained to maximize BLEU (Och and Ney, 2004). The SMT system is trained using the same form of data as our order model: parallel source and target dependency trees as in Figure 2. Of particular interest are the components in the baseline SMT system contributing most to word order decisions. The SMT system uses the same target language trigram model and local tree order model, as we are using for generating N-best orders for reranking. Thus the baseline system already uses our first-pass order models and only lacks the additional information provided by our re-ra"
P07-1002,J04-4002,0,0.8239,"e word orders. We show that combining discriminative training with features to detect these two different kinds of movement phenomena leads to substantial improvements in word ordering performance over strong baselines. Integrating this word order model in a baseline MT system results in a 2.4 points improvement in BLEU for English to Japanese translation. 1 On the other hand, there is also significant amount of information in the surface strings of the source and target and their alignment. Many state-of-the-art SMT systems do not use trees and base the ordering decisions on surface phrases (Och and Ney, 2004; Al-Onaizan and Papineni, 2006; Kuhn et al., 2006). In this paper we develop an order model for machine translation which makes use of both syntactic and surface information. Introduction The machine translation task can be viewed as consisting of two subtasks: predicting the collection of words in a translation, and deciding the order of the predicted words. For some language pairs, such as English and Japanese, the ordering problem is especially hard, because the target word order differs significantly from the source word order. Previous work has shown that it is useful to model target lan"
P07-1002,2001.mtsummit-papers.68,0,0.0333828,"iation of Computational Linguistics, pages 9–16, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics all [ࠫપ] constraints [㦕ٙ] are [圹] [圣坃地][㻫圩土] [坖坈圡圩] “restriction”“condition” TOPIC “all” c d satisfied e f “satisfy” PASSIVE-PRES g d e c f g h h d e c f g h g h d e c f (a) (b) Figure 1: (a) A sentence pair with source dependency tree, projected target dependency tree, and word alignments. (b) Example orders violating the target tree projectivity constraints. models. Our model is discriminatively trained to select the best order (according to the BLEU measure) (Papineni et al., 2001) of an unordered target dependency tree from the space of possible orders. Since the space of all possible orders of an unordered dependency tree is factorially large, we train our model on N-best lists of possible orders. These N-best lists are generated using approximate search and simpler models, as in the re-ranking approach of (Collins, 2000). We first evaluate our model on the task of ordering target sentences, given correct (reference) unordered target dependency trees. Our results show that combining features derived from the source and target dependency trees, distortion surface order"
P07-1002,P05-1034,0,0.684095,"c and surface information. Introduction The machine translation task can be viewed as consisting of two subtasks: predicting the collection of words in a translation, and deciding the order of the predicted words. For some language pairs, such as English and Japanese, the ordering problem is especially hard, because the target word order differs significantly from the source word order. Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees (Yamada and Knight, 2001; Galley et al., 2006) or dependency trees (Quirk et al., 2005), which are obtained using a parser trained to determine linguistic constituency. Alternatively, order is modelled in terms of movement of automatically induced hierarchical structure of sentences (Chiang, 2005; Wu, 1997). ∗ This research was conducted during the author’s internship at Microsoft Research. The framework for our statistical model is as follows. We assume the existence of a dependency tree for the source sentence, an unordered dependency tree for the target sentence, and a word alignment between the target and source sentences. Figure 1 (a) shows an example of aligned source and"
P07-1002,P06-1123,0,0.0130003,"ces an order 10 on the target sentence words. The dependency tree constrains the possible orders of the target sentence only to the ones that are projective with respect to the tree. An order of the sentence is projective with respect to the tree if each word and its descendants form a contiguous subsequence in the ordered sentence. Figure 1(b) shows several orders of the sentence which violate this constraint.1 Previous studies have shown that if both the source and target dependency trees represent linguistic constituency, the alignment between subtrees in the two languages is very complex (Wellington et al., 2006). Thus such parallel trees would be difficult for MT systems to construct in translation. In this work only the source dependency trees are linguistically motivated and constructed by a parser trained to determine linguistic structure. The target dependency trees are obtained through projection of the source dependency trees, using the word alignment (we use GIZA++ (Och and Ney, 2004)), ensuring better parallelism of the source and target structures. 2.1 Obtaining Target Dependency Trees Through Projection Our algorithm for obtaining target dependency trees by projection of the source trees vi"
P07-1002,J97-3002,0,0.0333648,"pairs, such as English and Japanese, the ordering problem is especially hard, because the target word order differs significantly from the source word order. Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees (Yamada and Knight, 2001; Galley et al., 2006) or dependency trees (Quirk et al., 2005), which are obtained using a parser trained to determine linguistic constituency. Alternatively, order is modelled in terms of movement of automatically induced hierarchical structure of sentences (Chiang, 2005; Wu, 1997). ∗ This research was conducted during the author’s internship at Microsoft Research. The framework for our statistical model is as follows. We assume the existence of a dependency tree for the source sentence, an unordered dependency tree for the target sentence, and a word alignment between the target and source sentences. Figure 1 (a) shows an example of aligned source and target dependency trees. Our task is to order the target dependency tree. We train a statistical model to select the best order of the unordered target dependency tree. An important advantage of our model is that it is gl"
P07-1002,P06-1066,0,0.0451715,"ce sentence, an unordered dependency tree for the target sentence, and a word alignment between the target and source sentences. Figure 1 (a) shows an example of aligned source and target dependency trees. Our task is to order the target dependency tree. We train a statistical model to select the best order of the unordered target dependency tree. An important advantage of our model is that it is global, and does not decompose the task of ordering a target sentence into a series of local decisions, as in the recently proposed order models for Machine Transition (Al-Onaizan and Papineni, 2006; Xiong et al., 2006; Kuhn et al., 2006). Thus we are able to define features over complete target sentence orders, and avoid the independence assumptions made by these 9 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 9–16, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics all [ࠫપ] constraints [㦕ٙ] are [圹] [圣坃地][㻫圩土] [坖坈圡圩] “restriction”“condition” TOPIC “all” c d satisfied e f “satisfy” PASSIVE-PRES g d e c f g h h d e c f g h g h d e c f (a) (b) Figure 1: (a) A sentence pair with source dependency tree, projected target dependency"
P07-1002,P01-1067,0,0.11175,"rder model for machine translation which makes use of both syntactic and surface information. Introduction The machine translation task can be viewed as consisting of two subtasks: predicting the collection of words in a translation, and deciding the order of the predicted words. For some language pairs, such as English and Japanese, the ordering problem is especially hard, because the target word order differs significantly from the source word order. Previous work has shown that it is useful to model target language order in terms of movement of syntactic constituents in constituency trees (Yamada and Knight, 2001; Galley et al., 2006) or dependency trees (Quirk et al., 2005), which are obtained using a parser trained to determine linguistic constituency. Alternatively, order is modelled in terms of movement of automatically induced hierarchical structure of sentences (Chiang, 2005; Wu, 1997). ∗ This research was conducted during the author’s internship at Microsoft Research. The framework for our statistical model is as follows. We assume the existence of a dependency tree for the source sentence, an unordered dependency tree for the target sentence, and a word alignment between the target and source"
P07-1002,2003.mtsummit-papers.53,0,\N,Missing
P07-1002,P02-1040,0,\N,Missing
P07-1002,P05-1066,0,\N,Missing
P07-1002,2001.mtsummit-papers.45,0,\N,Missing
P07-1017,C04-1022,0,0.0201475,"lving person, number and gender, it also requires a determiner for each word in a definite noun phrase with adjectival modifiers; in a noun compound, a determiner is attached to the last noun in the chain. Also, non-human subject plural nouns require the verb to be inflected in a singular feminine form. Generating these morphologically complex languages is therefore more difficult than generating English in terms of capturing the agreement phenomena. 3 Related Work The use of morphological features in language modelling has been explored in the past for morphologyrich languages. For example, (Duh and Kirchhoff, 2004) showed that factored language models, which consider morphological features and use an optimized backoff policy, yield lower perplexity. In the area of MT, there has been a large body of work attempting to modify the input to a translation system in order to improve the generated alignments for particular language pairs. For example, it has been shown (Lee, 2004) that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment, thus leading to improved overall translation quality. Another work (Koehn and Knight, 2003) showed"
P07-1017,W06-3102,0,0.0776228,"sative), gen(itive) yes, no yes, no wa, fa, none bi, ka, li, none yes, no Pers/Numb/Gend of pronoun, none Same as ObjectPronoun PossessivePronoun Table 1: Morphological features used for Russian and Arabic set of possible source transformations, incorporating morphology. In general, this line of work focused on translating from morphologically rich languages into English; there has been limited research in MT in the opposite direction. Koehn (2005) includes a survey of statistical MT systems in both directions for the Europarl corpus, and points out the challenges of this task. A recent work (El-Kahlout and Oflazer, 2006) experimented with English-toTurkish translation with limited success, suggesting that inflection generation given morphological features may give positive results. In the current work, we suggest a probabilistic framework for morphology generation performed as post-processing. It can therefore be considered as complementary to the techniques described above. Our approach is general in that it is not specific to a particular language pair, and is novel in that it allows modelling of agreement on the target side. The framework suggested here is most closely related to (Suzuki and Toutanova, 200"
P07-1017,H05-1085,0,0.348961,"ntence alignment, thus leading to improved overall translation quality. Another work (Koehn and Knight, 2003) showed improvements by splitting compounds in German. (Nießen and Ney, 2004) demonstrated that a similar level of alignment quality can be achieved with smaller corpora applying morpho-syntactic source restructuring, using hierarchical lexicon models, in translating from German into English. (Popovi´c and Ney, 2004) experimented successfully with translating from inflectional languages into English making use of POS tags, word stems and suffixes in the source language. More recently, (Goldwater and McClosky, 2005) achieved improvements in Czech-English MT, optimizing a Features POS Person Number Gender Tense Mood Case Russian (11 categories) Arabic (18 categories) Both 1,2,3 sing(ular), pl(ural) masc(uline), fem(inine) present, past, future, imperative dual neut(er) gerund subjunctive, jussive dat(ive), prep(ositional), instr(umental) Negation Determiner Conjunction Preposition ObjectPronoun nom(inative), acc(usative), gen(itive) yes, no yes, no wa, fa, none bi, ka, li, none yes, no Pers/Numb/Gend of pronoun, none Same as ObjectPronoun PossessivePronoun Table 1: Morphological features used for Russian"
P07-1017,P05-1071,0,0.0221425,"and 89,360 inflected forms. For the generation of word features, we only consider one dominant analysis for any surface word for simplicity. In case of ambiguity, we considered only the first (arbitrary) analysis for Russian. For Arabic, we apply the following heuristic: use the most frequent analysis estimated from the gold standard labels in the Arabic Treebank (Maamouri et al., 2005); if a word does not appear in the treebank, we choose the first analysis returned by the Buckwalter analyzer. Ideally, the best word analysis should be provided as a result of contextual disambiguation (e.g., (Habash and Rambow, 2005)); we leave this for future work. 6.3 Baseline As a baseline, we pick a morphological inflection yt at random from It . This random baseline serves as an indicator for the difficulty of the problem. Another more competitive baseline we implemented is a word trigram language model (LM). The LMs were trained using the CMU language modelling toolkit (Clarkson and Rosenfeld, 1997) with default settings on the training data described in Table 3. 6.4 Experiments In the experiments, our primary goal is to evaluate the effectiveness of the proposed model using all features available to us. Additionall"
P07-1017,E03-1076,0,0.0316642,"mple, (Duh and Kirchhoff, 2004) showed that factored language models, which consider morphological features and use an optimized backoff policy, yield lower perplexity. In the area of MT, there has been a large body of work attempting to modify the input to a translation system in order to improve the generated alignments for particular language pairs. For example, it has been shown (Lee, 2004) that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment, thus leading to improved overall translation quality. Another work (Koehn and Knight, 2003) showed improvements by splitting compounds in German. (Nießen and Ney, 2004) demonstrated that a similar level of alignment quality can be achieved with smaller corpora applying morpho-syntactic source restructuring, using hierarchical lexicon models, in translating from German into English. (Popovi´c and Ney, 2004) experimented successfully with translating from inflectional languages into English making use of POS tags, word stems and suffixes in the source language. More recently, (Goldwater and McClosky, 2005) achieved improvements in Czech-English MT, optimizing a Features POS Person Num"
P07-1017,2005.mtsummit-papers.11,0,0.0761755,"We generate the inflected forms of words in the target sentence using all of the available information, using a log-linear model that learns the relevant mapping functions. As a case study, we focus on the English-Russian and English-Arabic language pairs. Unlike English, Russian and Arabic have very rich systems of morphology, each with distinct characteristics. Translating from a morphology-poor to a morphologyrich language is especially challenging since detailed morphological information needs to be decoded from a language that does not encode this information or does so only implicitly (Koehn, 2005). We believe that these language pairs are represen128 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 128–135, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics tative in this respect and therefore demonstrate the generality of our approach. There are several contributions of this work. First, we propose a general approach that shows promise in addressing the challenges of MT into morphologically rich languages. We show that the use of both syntactic and morphological information improves translation quality. We a"
P07-1017,N04-4015,0,0.0883215,"ult than generating English in terms of capturing the agreement phenomena. 3 Related Work The use of morphological features in language modelling has been explored in the past for morphologyrich languages. For example, (Duh and Kirchhoff, 2004) showed that factored language models, which consider morphological features and use an optimized backoff policy, yield lower perplexity. In the area of MT, there has been a large body of work attempting to modify the input to a translation system in order to improve the generated alignments for particular language pairs. For example, it has been shown (Lee, 2004) that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment, thus leading to improved overall translation quality. Another work (Koehn and Knight, 2003) showed improvements by splitting compounds in German. (Nießen and Ney, 2004) demonstrated that a similar level of alignment quality can be achieved with smaller corpora applying morpho-syntactic source restructuring, using hierarchical lexicon models, in translating from German into English. (Popovi´c and Ney, 2004) experimented successfully with translating from inflect"
P07-1017,J04-2003,0,0.0894976,"sider morphological features and use an optimized backoff policy, yield lower perplexity. In the area of MT, there has been a large body of work attempting to modify the input to a translation system in order to improve the generated alignments for particular language pairs. For example, it has been shown (Lee, 2004) that determiner segmentation and deletion in Arabic sentences in an Arabic-to-English translation system improves sentence alignment, thus leading to improved overall translation quality. Another work (Koehn and Knight, 2003) showed improvements by splitting compounds in German. (Nießen and Ney, 2004) demonstrated that a similar level of alignment quality can be achieved with smaller corpora applying morpho-syntactic source restructuring, using hierarchical lexicon models, in translating from German into English. (Popovi´c and Ney, 2004) experimented successfully with translating from inflectional languages into English making use of POS tags, word stems and suffixes in the source language. More recently, (Goldwater and McClosky, 2005) achieved improvements in Czech-English MT, optimizing a Features POS Person Number Gender Tense Mood Case Russian (11 categories) Arabic (18 categories) Bot"
P07-1017,P00-1056,0,0.0157758,"ese experiments thus constitute a preliminary step for tackling the real task of inflecting words in MT. 6.1 Data We used a corpus of approximately 1 million aligned sentence pairs for English-Russian, and 0.5 million pairs for English-Arabic. Both corpora are from a technical (software manual) domain, which we believe is somewhat restricted along some morphological dimensions, such as tense and person. We used 1,000 sentence pairs each for development and testing for both language pairs. The details of the datasets used are given in Table 3. The sentence pairs were word-aligned using GIZA++ (Och and Ney, 2000) and submitted to a treelet-based MT system (Quirk et al., 2005), which uses the word dependency structure of the source language and projects word dependency structure to the target language, creating the structure shown in Figure 1 above. 6.2 Lexicon Table 4 gives some relevant statistics of the lexicons we used. For Russian, a general-domain lexicon was available to us, consisting of about 80,000 lemmas (stems) and 9.4 inflected forms per stem.6 Limiting the lexicon to word types that are seen in the training set reduces its size substantially to about 14,000 stems, and an average of 3.8 in"
P07-1017,popovic-ney-2004-towards,0,0.389358,"Missing"
P07-1017,P05-1034,0,0.0355404,"the real task of inflecting words in MT. 6.1 Data We used a corpus of approximately 1 million aligned sentence pairs for English-Russian, and 0.5 million pairs for English-Arabic. Both corpora are from a technical (software manual) domain, which we believe is somewhat restricted along some morphological dimensions, such as tense and person. We used 1,000 sentence pairs each for development and testing for both language pairs. The details of the datasets used are given in Table 3. The sentence pairs were word-aligned using GIZA++ (Och and Ney, 2000) and submitted to a treelet-based MT system (Quirk et al., 2005), which uses the word dependency structure of the source language and projects word dependency structure to the target language, creating the structure shown in Figure 1 above. 6.2 Lexicon Table 4 gives some relevant statistics of the lexicons we used. For Russian, a general-domain lexicon was available to us, consisting of about 80,000 lemmas (stems) and 9.4 inflected forms per stem.6 Limiting the lexicon to word types that are seen in the training set reduces its size substantially to about 14,000 stems, and an average of 3.8 inflections per stem. We will use this latter “domain-adapted” lex"
P07-1017,P06-1132,1,0.81474,"ahlout and Oflazer, 2006) experimented with English-toTurkish translation with limited success, suggesting that inflection generation given morphological features may give positive results. In the current work, we suggest a probabilistic framework for morphology generation performed as post-processing. It can therefore be considered as complementary to the techniques described above. Our approach is general in that it is not specific to a particular language pair, and is novel in that it allows modelling of agreement on the target side. The framework suggested here is most closely related to (Suzuki and Toutanova, 2006), which uses a probabilistic model to generate Japanese case markers for English-to-Japanese MT. This work can be viewed as a generalization of (Suzuki and Toutanova, 2006) in that our model generates inflected forms of words, and is not limited to generating a small, closed set of case markers. In addition, the morphology generation problem is more challenging in that it requires handling of complex agreement phenomena along multiple morphological dimensions. 4 Inflection Prediction Framework lexical operations relevant for the task. 4.1 Morphology Analysis and Generation Morphological analys"
P07-1017,W06-1668,1,0.876884,"Oflazer, 2006) experimented with English-toTurkish translation with limited success, suggesting that inflection generation given morphological features may give positive results. In the current work, we suggest a probabilistic framework for morphology generation performed as post-processing. It can therefore be considered as complementary to the techniques described above. Our approach is general in that it is not specific to a particular language pair, and is novel in that it allows modelling of agreement on the target side. The framework suggested here is most closely related to (Suzuki and Toutanova, 2006), which uses a probabilistic model to generate Japanese case markers for English-to-Japanese MT. This work can be viewed as a generalization of (Suzuki and Toutanova, 2006) in that our model generates inflected forms of words, and is not limited to generating a small, closed set of case markers. In addition, the morphology generation problem is more challenging in that it requires handling of complex agreement phenomena along multiple morphological dimensions. 4 Inflection Prediction Framework lexical operations relevant for the task. 4.1 Morphology Analysis and Generation Morphological analys"
P07-1104,W06-1655,1,0.814278,"Missing"
P07-1104,A00-2018,0,0.0713209,"ion (1) is used to discriminatively re-rank the candidate list using additional features which may or may not be included in the baseline model. Since 828 We follow the experimental paradigm of parse re-ranking outlined in Charniak and Johnson (2005), and fed the features extracted by their program to the five rerankers we developed. Each uses a linear model trained using one of the five estimators. These rerankers attempt to select the best parse ? for a sentence ? from the 50-best list of possible parses ??? ? for the sentence. The linear model combines the log probability calculated by the Charniak (2000) parser as a feature with 1,219,272 additional features. We trained the feaBaseline ME/L2 ME/L1 AP Boosting BLasso F-Score 0.8986 0.9176 0.9165 0.9164 0.9131 0.9133 # features time (min) # train iter 1,211,026 19,121 939,248 6,714 8,085 62 37 2 495 239 129 174 8 92,600 56,500 Table 1: Performance summary of estimators on parsing re-ranking (ME/L2: ME with L2 regularization; ME/L1: ME with L1 regularization) ME/L2 ME/L2 ME/L1 AP Boost Blasso &lt;&lt; ~ &lt;&lt; &lt;&lt; ME/L1 &gt;&gt; ~ &lt; ~ AP ~ ~ &lt;&lt; &lt; Boost &gt;&gt; &gt; &gt;&gt; BLasso &gt;&gt; ~ &gt; ~ ~ Table 2: Statistical significance test results (“&gt;&gt;” or “&lt;&lt;” means P-value &lt; 0.01; &gt;"
P07-1104,P05-1022,1,0.17373,"6), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularizat"
P07-1104,W02-1001,0,0.121044,"ures are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boosting in the LM adaptation task. Ng (2004) showed that for logistic regression, L1 regularization outperforms L2 regularization on artificial datasets which contain many completely irrelevant features. Goodman (2003) showed that in two out of three tasks, an ME estimator with a one-sided Laplacian prior ("
P07-1104,P06-1029,1,0.803398,"ely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boosting in the LM adaptation task. Ng (2004) showed that for logistic regression, L1 regularization outperforms L2 regularization on artificial datasets which contain many completely irrelevant features. Goodman (2003) showed that in two out of three tasks, an ME estimator with a one-sided Laplacian prior (i.e., L1 regularization with the constraint that all feature weights are positive) outperformed a comparable estimator using a Gaussian prior (i.e., L2 regularization). Riezler and Vasserman (2004) showed that a"
P07-1104,N04-1039,0,0.300415,"Missing"
P07-1104,P99-1069,1,0.742056,"n into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boos"
P07-1104,W03-1018,0,0.505913,"ith L1 regularization This estimator also minimizes the negative conditional log-likelihood, but uses an L1 (or Lasso) penalty. That is, ?(?) in Equation (2) is defined according to ? ? = ? ? ?? . L1 regularization typically leads to sparse solutions in which many feature weights are exactly zero, so it is a natural candidate when feature selection is desirable. By contrast, L2 regularization produces solutions in which most weights are small but non-zero. Optimizing the L1-regularized objective function is challenging because its gradient is discontinuous whenever some parameter equals zero. Kazama and Tsujii (2003) described an estimation method that constructs an equivalent constrained optimization problem with twice the number of variables. However, we found that this method is impractically slow for large-scale NLP tasks. In this work we use the orthant-wise limited-memory quasi-Newton algorithm (OWL-QN), which is a modification of L-BFGS that allows it to effectively handle the discontinuity of the gradient (Andrew and Gao 2007). We provide here a high-level description of the algorithm. A quasi-Newton method such as L-BFGS uses first order information at each iterate to build an approximation to th"
P07-1104,W02-2018,0,0.215148,"the parameters. Here,  is a parameter that controls the amount of regularization, optimized on held-out data. This is one of the most popular estimators, largely due to its appealing computational properties: both ? ? and ?(?) are convex and differentiable, so gradient-based numerical algorithms can be used to find the global minimum efficiently. In our experiments, we used the limited memory quasi-Newton algorithm (or L-BFGS, Nocedal and Wright 1999) to find the optimal ? because this method has been shown to be substantially faster than other methods such as Generalized Iterative Scaling (Malouf 2002). Because for some sentences there are multiple best parses (i.e., parses with the same F-Score), we used the variant of ME estimator described in Riezler et al. (2002), where ? ? is defined as the likelihood of the best parses ? ∈ ?(?) relative to the n-best parser output ??? ? , (i.e., ? ? ⊑ ???(?)): ? ? = − ??=1 log ? ? ∈?(? ? ) ?(?? |?? ). We applied this variant in our experiments of parse re-ranking and LM adaptation, and found that on both tasks it leads to a significant improvement in performance for the L2-regularied ME estimator but not for the L1-regularied ME estimator. 2.2 ME esti"
P07-1104,W04-3223,0,0.0632296,"ver a large number of weakly informative features. The first intuition motivates feature selection methods such as Boosting and BLasso (e.g., Collins 2000; Zhao and Yu, 2004), which usually work best when many features are completely irrelevant. L1 or Lasso regularization of linear models, introduced by Tibshirani (1996), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low comput"
P07-1104,P02-1035,1,0.821172,"gely due to its appealing computational properties: both ? ? and ?(?) are convex and differentiable, so gradient-based numerical algorithms can be used to find the global minimum efficiently. In our experiments, we used the limited memory quasi-Newton algorithm (or L-BFGS, Nocedal and Wright 1999) to find the optimal ? because this method has been shown to be substantially faster than other methods such as Generalized Iterative Scaling (Malouf 2002). Because for some sentences there are multiple best parses (i.e., parses with the same F-Score), we used the variant of ME estimator described in Riezler et al. (2002), where ? ? is defined as the likelihood of the best parses ? ∈ ?(?) relative to the n-best parser output ??? ? , (i.e., ? ? ⊑ ???(?)): ? ? = − ??=1 log ? ? ∈?(? ? ) ?(?? |?? ). We applied this variant in our experiments of parse re-ranking and LM adaptation, and found that on both tasks it leads to a significant improvement in performance for the L2-regularied ME estimator but not for the L1-regularied ME estimator. 2.2 ME estimation with L1 regularization This estimator also minimizes the negative conditional log-likelihood, but uses an L1 (or Lasso) penalty. That is, ?(?) in Equation (2) is"
P07-1104,N03-1033,1,0.14229,"ontext are ME models. Following previous work (Ratnaparkhi, 1996), we assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words (i.e., ?=2 in the equation above). The local models at each position include features of the current word, the previous word, the next word, and features of the previous two tags. In addition to lexical identity of the words, we used features of word suffixes, capitalization, and number/special character signatures of the words. We used the standard splits of the Penn Treebank from the tagging literature (Toutanova et al. 2003) for training, development and test sets. The training set comprises Sections 0-18, the development set — Sections 19-21, and the test set — Sections 22-24. We compared training the ME models using L1 and L2 regularization. For each of the two types of regularization we selected the best value of the regularization constant using grid search to optimize the accuracy on the development set. We report final accuracy measures on the test set in Table 6. The results on this task confirm the trends we have seen so far. There is almost no difference in 3 Only the L2 vs. AP comparison is significant"
P07-1104,J05-1003,0,\N,Missing
P08-1059,W06-1673,0,0.00864483,"rget language model has almost solely been responsible for addressing the second aspect. Minkov et al. (2007) introduced a way to address these problems by using a rich featurebased model, but did not apply the model to MT. In this paper, we integrate a model that predicts target word inflection in the translations of English into two morphologically complex languages (Russian and Arabic) and show improvements in the MT output. We study several alternative methods for integration and show that it is best to propagate uncertainty among the different components as shown by other research, e.g. (Finkel et al., 2006), and in some cases, to factor the translation problem so that the baseline MT system can take advantage of the reduction in sparsity by being able to work on word stems. We also demonstrate that our independently trained models are portable, showing that they can improve both syntactic and phrasal SMT systems. 514 Proceedings of ACL-08: HLT, pages 514–522, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2 Related work There has been active research on incorporating morphological knowledge in SMT. Several approaches use pre-processing schemes, including segment"
P08-1059,J07-3002,0,0.0103036,"model improves the quality of SMT over both phrasal and syntax-based SMT systems according to BLEU and human judgements. 1 Introduction One of the outstanding problems for further improving machine translation (MT) systems is the difficulty of dividing the MT problem into sub-problems and tackling each sub-problem in isolation to improve the overall quality of MT. Evidence for this difficulty is the fact that there has been very little work investigating the use of such independent subcomponents, though we started to see some successful cases in the literature, for example in word alignment (Fraser and Marcu, 2007), target language capitalization (Wang et al., 2006) and case marker generation (Toutanova and Suzuki, 2007). This paper describes a successful attempt to integrate a subcomponent for generating word inflections into a statistical machine translation (SMT) system. Our research is built on previous work in the area of using morpho-syntactic information for improving SMT. Work in this area is motivated by two advantages offered by morphological analysis: (1) it provides linguistically motivated clustering of words and makes the data less sparse; (2) it captures morphological constraints applicab"
P08-1059,H05-1085,0,0.239268,"inguistically motivated clustering of words and makes the data less sparse; (2) it captures morphological constraints applicable on the target side, such as agreement phenomena. This second problem is very difficult to address with wordbased translation systems, when the relevant morphological information in the target language is either non-existent or implicitly encoded in the source language. These two aspects of morphological processing have often been addressed separately: for example, morphological pre-processing of the input data is a common method of addressing the first aspect, e.g. (Goldwater and McClosky, 2005), while the application of a target language model has almost solely been responsible for addressing the second aspect. Minkov et al. (2007) introduced a way to address these problems by using a rich featurebased model, but did not apply the model to MT. In this paper, we integrate a model that predicts target word inflection in the translations of English into two morphologically complex languages (Russian and Arabic) and show improvements in the MT output. We study several alternative methods for integration and show that it is best to propagate uncertainty among the different components as"
P08-1059,N06-2013,0,0.0277457,"r the translation problem so that the baseline MT system can take advantage of the reduction in sparsity by being able to work on word stems. We also demonstrate that our independently trained models are portable, showing that they can improve both syntactic and phrasal SMT systems. 514 Proceedings of ACL-08: HLT, pages 514–522, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2 Related work There has been active research on incorporating morphological knowledge in SMT. Several approaches use pre-processing schemes, including segmentation of clitics (Lee, 2004; Habash and Sadat, 2006), compound splitting (Nießen and Ney, 2004) and stemming (Goldwater and McClosky, 2005). Of these, the segmentation approach is difficult to apply when the target language is morphologically rich as the segmented morphemes must be put together in the output (El-Kahlout and Oflazer, 2006); and in fact, most work using pre-processing focused on translation into English. In recent work, Koehn and Hoang (2007) proposed a general framework for including morphological features in a phrase-based SMT system by factoring the representation of words into a vector of morphological features and allowing a"
P08-1059,W07-0711,0,0.0134153,"similar to the ones used in phrasal systems, and their weights are trained using max-BLEU training (Och, 2003). There are nine feature functions in the treelet system, including log-probabilities according to inverted and direct channel models estimated by relative frequency, lexical weighting channel models following Vogel et al. (2003), a trigram target language model, two order models, word count, phrase count, and average phrase size functions. The treelet translation model is estimated using a parallel corpus. First, the corpus is word-aligned using an implementation of lexicalized-HMMs (He, 2007); then the source sentences are parsed into a dependency structure, and the dependency is projected onto the target side following the heuristics described in (Quirk et al., 2005). These aligned sentence pairs form the training data of the inflection models as well. An example was given in Figure 1. 4.2 Phrasal translation system This is a re-implementation of the Pharaoh translation system (Koehn, 2004). It uses the same lexicalized-HMM model for word alignment as the treelet system, and uses the standard extraction heuristics to extract phrase pairs using forward and backward alignments. In"
P08-1059,D07-1091,0,0.216689,"s 2 Related work There has been active research on incorporating morphological knowledge in SMT. Several approaches use pre-processing schemes, including segmentation of clitics (Lee, 2004; Habash and Sadat, 2006), compound splitting (Nießen and Ney, 2004) and stemming (Goldwater and McClosky, 2005). Of these, the segmentation approach is difficult to apply when the target language is morphologically rich as the segmented morphemes must be put together in the output (El-Kahlout and Oflazer, 2006); and in fact, most work using pre-processing focused on translation into English. In recent work, Koehn and Hoang (2007) proposed a general framework for including morphological features in a phrase-based SMT system by factoring the representation of words into a vector of morphological features and allowing a phrase-based MT system to work on any of the factored representations, which is implemented in the Moses system. Though our motivation is similar to that of Koehn and Hoang (2007), we chose to build an independent component for inflection prediction in isolation rather than folding morphological information into the main translation model. While this may lead to search errors due to the fact that the mode"
P08-1059,koen-2004-pharaoh,0,0.00988604,"phrase count, and average phrase size functions. The treelet translation model is estimated using a parallel corpus. First, the corpus is word-aligned using an implementation of lexicalized-HMMs (He, 2007); then the source sentences are parsed into a dependency structure, and the dependency is projected onto the target side following the heuristics described in (Quirk et al., 2005). These aligned sentence pairs form the training data of the inflection models as well. An example was given in Figure 1. 4.2 Phrasal translation system This is a re-implementation of the Pharaoh translation system (Koehn, 2004). It uses the same lexicalized-HMM model for word alignment as the treelet system, and uses the standard extraction heuristics to extract phrase pairs using forward and backward alignments. In decoding, the system uses a linear combination of feature functions whose 517 Data sets For our English-Russian and English-Arabic experiments, we used data from a technical (computer) domain. For each language pair, we used a set of parallel sentences (train) for training the MT system submodels (e.g., phrase tables, language model), a set of parallel sentences (lambda) for training the combination weig"
P08-1059,N04-4015,0,0.0138062,"s, to factor the translation problem so that the baseline MT system can take advantage of the reduction in sparsity by being able to work on word stems. We also demonstrate that our independently trained models are portable, showing that they can improve both syntactic and phrasal SMT systems. 514 Proceedings of ACL-08: HLT, pages 514–522, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2 Related work There has been active research on incorporating morphological knowledge in SMT. Several approaches use pre-processing schemes, including segmentation of clitics (Lee, 2004; Habash and Sadat, 2006), compound splitting (Nießen and Ney, 2004) and stemming (Goldwater and McClosky, 2005). Of these, the segmentation approach is difficult to apply when the target language is morphologically rich as the segmented morphemes must be put together in the output (El-Kahlout and Oflazer, 2006); and in fact, most work using pre-processing focused on translation into English. In recent work, Koehn and Hoang (2007) proposed a general framework for including morphological features in a phrase-based SMT system by factoring the representation of words into a vector of morphologica"
P08-1059,P07-1017,1,0.911177,"such as agreement phenomena. This second problem is very difficult to address with wordbased translation systems, when the relevant morphological information in the target language is either non-existent or implicitly encoded in the source language. These two aspects of morphological processing have often been addressed separately: for example, morphological pre-processing of the input data is a common method of addressing the first aspect, e.g. (Goldwater and McClosky, 2005), while the application of a target language model has almost solely been responsible for addressing the second aspect. Minkov et al. (2007) introduced a way to address these problems by using a rich featurebased model, but did not apply the model to MT. In this paper, we integrate a model that predicts target word inflection in the translations of English into two morphologically complex languages (Russian and Arabic) and show improvements in the MT output. We study several alternative methods for integration and show that it is best to propagate uncertainty among the different components as shown by other research, e.g. (Finkel et al., 2006), and in some cases, to factor the translation problem so that the baseline MT system can"
P08-1059,J04-2003,0,0.00686992,"e MT system can take advantage of the reduction in sparsity by being able to work on word stems. We also demonstrate that our independently trained models are portable, showing that they can improve both syntactic and phrasal SMT systems. 514 Proceedings of ACL-08: HLT, pages 514–522, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2 Related work There has been active research on incorporating morphological knowledge in SMT. Several approaches use pre-processing schemes, including segmentation of clitics (Lee, 2004; Habash and Sadat, 2006), compound splitting (Nießen and Ney, 2004) and stemming (Goldwater and McClosky, 2005). Of these, the segmentation approach is difficult to apply when the target language is morphologically rich as the segmented morphemes must be put together in the output (El-Kahlout and Oflazer, 2006); and in fact, most work using pre-processing focused on translation into English. In recent work, Koehn and Hoang (2007) proposed a general framework for including morphological features in a phrase-based SMT system by factoring the representation of words into a vector of morphological features and allowing a phrase-based MT system to work on any of t"
P08-1059,P03-1021,0,0.00639994,"the inflection prediction model with two types of machine translation systems: systems that make use of syntax and surface phrase-based systems. 4.1 Treelet translation system This is a syntactically-informed MT system, designed following (Quirk et al., 2005). In this approach, translation is guided by treelet translation pairs, where a treelet is a connected subgraph of a syntactic dependency tree. Translations are scored according to a linear combination of feature functions. The features are similar to the ones used in phrasal systems, and their weights are trained using max-BLEU training (Och, 2003). There are nine feature functions in the treelet system, including log-probabilities according to inverted and direct channel models estimated by relative frequency, lexical weighting channel models following Vogel et al. (2003), a trigram target language model, two order models, word count, phrase count, and average phrase size functions. The treelet translation model is estimated using a parallel corpus. First, the corpus is word-aligned using an implementation of lexicalized-HMMs (He, 2007); then the source sentences are parsed into a dependency structure, and the dependency is projected o"
P08-1059,P05-1034,0,0.0119487,"-best list from the treelet system, every translation hypothesis contains the annotations A that our model needs, because the system maintains the alignment, parse trees, etc., as part of its search space. Thus we do not need to do anything further to obtain input of the form necessary for application of the inflection model. For the phrase-based system, we generated the annotations needed by first parsing the source sentence e, aligning the source and candidate translations with the word-alignment model used in training, and projected the dependency tree to the target using the algorithm of (Quirk et al., 2005). Note that it may be better to use the word alignment maintained as part of the translation hypotheses during search, but our solution is more suitable to situations where these can not be easily obtained. For all methods, we study two settings for integration. In the first, we only consider (n=1) hypotheses from the base MT system. In the second setting, we allow the model to use up to 100 translations, and to automatically select the best number to use. As seen in Table 3, (n=16) translations were chosen for Russian and as seen in Table 5, (n=2) were chosen for Arabic for this method. 5.2 M"
P08-1059,N07-1007,1,0.9279,"human judgements. 1 Introduction One of the outstanding problems for further improving machine translation (MT) systems is the difficulty of dividing the MT problem into sub-problems and tackling each sub-problem in isolation to improve the overall quality of MT. Evidence for this difficulty is the fact that there has been very little work investigating the use of such independent subcomponents, though we started to see some successful cases in the literature, for example in word alignment (Fraser and Marcu, 2007), target language capitalization (Wang et al., 2006) and case marker generation (Toutanova and Suzuki, 2007). This paper describes a successful attempt to integrate a subcomponent for generating word inflections into a statistical machine translation (SMT) system. Our research is built on previous work in the area of using morpho-syntactic information for improving SMT. Work in this area is motivated by two advantages offered by morphological analysis: (1) it provides linguistically motivated clustering of words and makes the data less sparse; (2) it captures morphological constraints applicable on the target side, such as agreement phenomena. This second problem is very difficult to address with wo"
P08-1059,N06-1001,0,0.0385931,"syntax-based SMT systems according to BLEU and human judgements. 1 Introduction One of the outstanding problems for further improving machine translation (MT) systems is the difficulty of dividing the MT problem into sub-problems and tackling each sub-problem in isolation to improve the overall quality of MT. Evidence for this difficulty is the fact that there has been very little work investigating the use of such independent subcomponents, though we started to see some successful cases in the literature, for example in word alignment (Fraser and Marcu, 2007), target language capitalization (Wang et al., 2006) and case marker generation (Toutanova and Suzuki, 2007). This paper describes a successful attempt to integrate a subcomponent for generating word inflections into a statistical machine translation (SMT) system. Our research is built on previous work in the area of using morpho-syntactic information for improving SMT. Work in this area is motivated by two advantages offered by morphological analysis: (1) it provides linguistically motivated clustering of words and makes the data less sparse; (2) it captures morphological constraints applicable on the target side, such as agreement phenomena."
P08-1059,W06-3102,0,\N,Missing
P08-1059,D08-1076,0,\N,Missing
P09-1055,P08-1103,1,0.624672,"cess to tags and ask it to predict a single lemma for each word in testing. Our joint model, described in Section 5, is defined in a re-ranking framework, and can choose from among k-best predictions of tag-sets and lemmas generated from the component tagger and lemmatizer models. 4.1 Morphological analyser We employ a discriminative character transducer as a component morphological analyzer. The input to the transducer is an inflected word (the source) and possibly an estimated part-of-speech; the output is the lemma of the word (the target). The transducer is similar to the one described by Jiampojamarn et al. (2008) for letter-to-phoneme conversion, but extended to allow for whole-word features on both the input and the output. The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). The main feature of this algorithm is its capability to transduce many consecutive characters with a single operation; the same algorithm is employed to tag subsequences in semi-Markov CRFs (Sarawagi and Cohen, 2004). We employ three main categories of features: context, transition, and vocabulary (rootlist) features. The first two are described in detail by Jiampojamarn"
P09-1055,P08-1083,0,0.0228182,"sducer which is related to these approaches and draws on previous work in this and related string transduction areas. Our transducer is described in detail in Section 4.1. Another related line of work approaches the disambiguation problem directly, where the task is to predict the correct analysis of word-forms in context (in sentences), and not all possible analyses. In such work it is often assumed that the correct POS tags can be predicted with high accuracy using labeled POS-disambiguated sentences (Erjavec and Dˇzeroski, 2004; Habash and Rambow, 2005). A notable exception is the work of (Adler et al., 2008), which uses unlabeled data and a morphological analyzer to learn a semi-supervised HMM model for disambiguation in context, and also guesses analyses for unknown words using a guesser of likely POS-tags. It is most closely related to our work, but does not attempt to predict all possible analyses, and does not have to tackle a complex string transduction problem for lemmatization since segmentation is mostly sufficient for the focus language of that study (Hebrew). The idea of solving two related tasks jointly to improve performance on both has been successful for other pairs of tasks (e.g.,"
P09-1055,J93-2004,0,0.0308704,"to all words in the graph.4 We 6 Experiments 6.1 Data We use datasets for four languages: English, Bulgarian, Slovene, and Czech. For each of the languages, we need a lexicon with morphological analyses L and unlabeled text. For English we derive the lexicon from CELEX (Baayen et al., 1995), and for the other languages we use the Multext-East resources (Erjavec, 2004). For English we use only open-class words (nouns, verbs, adjectives, and adverbs), and for the other languages we use words of all classes. The unlabeled data for English we use is the union of the Penn Treebank tagged WSJ data (Marcus et al., 1993) and the BLLIP corpus.5 For the rest of the languages we use only the text of George Orwell’s novel 1984, which is provided in morphologically disambiguated form as part of MultextEast (but we don’t use the annotations). Table 2 4 We start the Gibbs sampler by the assignments found by the pipeline method and then use an annealing schedule to find a neighborhood of high-likelihood assignments, before taking about 10 complete samples from the graph to compute expectations. 5 The BLLIP corpus contains approximately 30 million words of automatically parsed WSJ data. We used these corpora as plain"
P09-1055,W04-3220,0,0.0153107,"which uses unlabeled data and a morphological analyzer to learn a semi-supervised HMM model for disambiguation in context, and also guesses analyses for unknown words using a guesser of likely POS-tags. It is most closely related to our work, but does not attempt to predict all possible analyses, and does not have to tackle a complex string transduction problem for lemmatization since segmentation is mostly sufficient for the focus language of that study (Hebrew). The idea of solving two related tasks jointly to improve performance on both has been successful for other pairs of tasks (e.g., (Andrew et al., 2004)). Doing joint inference instead of taking a pipeline approach has also been shown useful for other problems (e.g., (Finkel et al., 2006; Cohen and Smith, 2007)). Related work In work on morphological analysis using machine learning, the task is rarely addressed in the form described above. Some exceptions are the work (Bosch and Daelemans, 1999) which presents a model for segmenting, stemming, and tagging words in Dutch, and requires the prediction of all possible analyses, and (Antal van den Bosch and Soudi, 2007) which similarly requires the prediction of all morpho-syntactically annotated"
P09-1055,P99-1037,0,0.120438,"Missing"
P09-1055,P02-1065,0,0.0249792,"T but there are no guarantees about the coverage of the target lemmas or the number of noise words which may occur in T (see Table 2 for data statistics). Our setting is thus more realistic since it is what one would have in a real application scenario. 3 our work, these approaches do not make use of unlabeled data and make predictions for each word type in isolation. In machine learning work on lemmatization for highly inflective languages, it is most often assumed that a word form and a POS tag are given, and the task is to predict the set of corresponding lemma(s) (Mooney and Califf, 1995; Clark, 2002; Wicentowski, 2002; Erjavec and Dˇzeroski, 2004; Dreyer et al., 2008). In our task setting, we do not assume the availability of gold-standard POS tags. As a component model, we use a lemmatizing string transducer which is related to these approaches and draws on previous work in this and related string transduction areas. Our transducer is described in detail in Section 4.1. Another related line of work approaches the disambiguation problem directly, where the task is to predict the correct analysis of word-forms in context (in sentences), and not all possible analyses. In such work it is of"
P09-1055,N04-1033,0,0.0189474,"from the component tagger and lemmatizer models. 4.1 Morphological analyser We employ a discriminative character transducer as a component morphological analyzer. The input to the transducer is an inflected word (the source) and possibly an estimated part-of-speech; the output is the lemma of the word (the target). The transducer is similar to the one described by Jiampojamarn et al. (2008) for letter-to-phoneme conversion, but extended to allow for whole-word features on both the input and the output. The core of our engine is the dynamic programming algorithm for monotone phrasal decoding (Zens and Ney, 2004). The main feature of this algorithm is its capability to transduce many consecutive characters with a single operation; the same algorithm is employed to tag subsequences in semi-Markov CRFs (Sarawagi and Cohen, 2004). We employ three main categories of features: context, transition, and vocabulary (rootlist) features. The first two are described in detail by Jiampojamarn et al. (2008), while the final is novel to this work. Context features are centered around a transduction operation such as es → e, as employed in gives → give. Context features include an indicator for the operation itself,"
P09-1055,D07-1022,0,0.0151303,"own words using a guesser of likely POS-tags. It is most closely related to our work, but does not attempt to predict all possible analyses, and does not have to tackle a complex string transduction problem for lemmatization since segmentation is mostly sufficient for the focus language of that study (Hebrew). The idea of solving two related tasks jointly to improve performance on both has been successful for other pairs of tasks (e.g., (Andrew et al., 2004)). Doing joint inference instead of taking a pipeline approach has also been shown useful for other problems (e.g., (Finkel et al., 2006; Cohen and Smith, 2007)). Related work In work on morphological analysis using machine learning, the task is rarely addressed in the form described above. Some exceptions are the work (Bosch and Daelemans, 1999) which presents a model for segmenting, stemming, and tagging words in Dutch, and requires the prediction of all possible analyses, and (Antal van den Bosch and Soudi, 2007) which similarly requires the prediction of all morpho-syntactically annotated segmentations of words for Arabic. As opposed to 2 These settings refer to the availability of a set of word forms which are possible lemmas; in the no rootlist"
P09-1055,W02-1001,0,0.134599,"the target context tracked by our dynamic programming chart, we can efficiently track these frequencies during transduction. We incorporate the source part-of-speech tag by appending it to each feature, thus the context feature es → e may become es → e, VBZ. To enable communication between the various parts-ofspeech, a universal set of unannotated features also fires, regardless of the part-of-speech, acting as a back-off model of how words in general behave during stemming. Linear weights are assigned to each of the transducer’s features using an averaged perceptron for structure prediction (Collins, 2002). Note that our features are defined in terms of the operations employed during transduction, therefore to create gold-standard feature vectors, we require not only target outputs, but also derivations to produce those outputs. We employ a deterministic heuristic to create these derivations; given a goldstandard source-target pair, we construct a derivation that uses only trivial copy operations until the first character mismatch. The remainder of the transduction is performed with a single multicharacter replacement. For example, the derivation for living → live would be l → l , i → i , v → v"
P09-1055,P00-1035,0,0.0347005,"r instead of a Naive Bayes model, and second, we use features derived from related words appearing in T. The possible classes predicted by the classifier are as many as the observed tag-sets in L. The sparsity is relieved by adding features for individual tags t which get shared across tag-sets containing t. There are two types of features in the model: (i) word-internal features: word suffixes, capitalization, existence of hyphen, and word prefixes (such features were also used in (Toutanova and Johnson, 2008)), and (ii) features based on related words. These latter features are inspired by (Cucerzan and Yarowsky, 2000) and are defined as follows: for a word w such as telling, there is an indicator feature for every combination of two suffixes α and β, such that there is a prefix p where telling= pα and pβ exists in T. For example, if the word tells is found in T, there would be a feature for the suffixes α=ing,β=s that fires. The suffixes are defined as all character suffixes up to length three which occur with at least 100 words. 489 for word wi , for j = 1 . . . k. Also, let li (t)j denote the top lemmas for word wi given tag t. An assignment of a tag-set and lemmas to a word wi consists of a choice of a"
P09-1055,D08-1113,0,0.0803994,"Missing"
P09-1055,erjavec-2004-multext,0,0.0466541,"tion to the lexicon, we are allowed to make use of unannotated text T in the language. We will predict morphological analyses for words which occur in T. Note that the task is defined on word types and not on words in context. A morphological analysis of a word w consists of a (possibly structured) POS tag t, together with one or several lemmas, which are the possible basic forms of w when it has tag t. As an example, Table 1 illustrates the morphological analyses of several words taken from the CELEX lexical database of English (Baayen et al., 1995) and the Multext-East lexicon of Bulgarian (Erjavec, 2004). The Bulgarian words are transcribed in 1 Tag sets are useful, for example, as a basis of sparsityreducing features for text labeling tasks; lemmatization is useful for information retrieval and machine translation from a morphologically rich to a morphologically poor language, where full analysis may not be important. 486 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 486–494, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Word Forms tell told tells telling izpravena izpraviha Morphological Analyses verb base (VB), tell verb past tense (VBD"
P09-1055,W06-1673,0,0.0247872,"Missing"
P09-1055,P05-1071,0,0.0282082,"S tags. As a component model, we use a lemmatizing string transducer which is related to these approaches and draws on previous work in this and related string transduction areas. Our transducer is described in detail in Section 4.1. Another related line of work approaches the disambiguation problem directly, where the task is to predict the correct analysis of word-forms in context (in sentences), and not all possible analyses. In such work it is often assumed that the correct POS tags can be predicted with high accuracy using labeled POS-disambiguated sentences (Erjavec and Dˇzeroski, 2004; Habash and Rambow, 2005). A notable exception is the work of (Adler et al., 2008), which uses unlabeled data and a morphological analyzer to learn a semi-supervised HMM model for disambiguation in context, and also guesses analyses for unknown words using a guesser of likely POS-tags. It is most closely related to our work, but does not attempt to predict all possible analyses, and does not have to tackle a complex string transduction problem for lemmatization since segmentation is mostly sufficient for the focus language of that study (Hebrew). The idea of solving two related tasks jointly to improve performance on"
P09-1055,P08-1000,0,\N,Missing
P11-1090,N10-1083,0,0.0114887,"ion modeling distribution we use. 1 Traditional distortion models represent P (aj |aj−1 , e), the probability of an alignment given the previous alignment, to bias the model away from placing large distances between the aligned tokens of consecutively sequenced tokens. In addition to modeling a larger state space to also predict morpheme types, we extend this model by using a special log-linear model form which allows the integration of rich morpho-syntactic context. Log-linear models have been previously used in unsupervised learning for local multinomial distributions like this one in e.g. (Berg-Kirkpatrick et al., 2010), and for global distributions in (Poon et al., 2009). The special log-linear form allows the inclusion of features targeted at learning the transitions among morpheme types and the transitions between corresponding source morphemes. The set of features with example values for this model is depicted in Table 3. The example is focussed on the features firing for the transition from the Bulgarian suffix te aligned to the first English morpheme µi−1 = te, ti−1 =suffix, ai−1 =1, to the Bulgarian root tsvet aligned to the third English morpheme µi = tsvet, ti =root, ai =3. The first feature is the"
P11-1090,D09-1075,0,0.0911234,"emes, morphemetransitions, and word boundaries. These additional sources of information provide powerful bias for unsupervised learning, without increasing the asymptotic running time of the inference algorithm. Used as a monolingual model, our system significantly improves the state-of-the-art segmentation performance on three Arabic and Hebrew datasets. Used as a bilingual model, our system outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER). In agreement with some previous work on tokenization/morpheme segmentation for alignment (Chung and Gildea, 2009; Habash and Sadat, 2006), we find that the best segmentation for alignment does not coincide with the gold-standard segmenta896 tion and our bilingual model does not outperform our monolingual model in segmentation F-Measure. 2 Model Our model defines the probability of a target language sequence of words (each consisting of a sequence of morphemes), and alignment from target to source morphemes, given a source language sequence of words (each consisting of a sequence of morphemes). An example morpheme segmentation and alignment of phrases in English-Arabic and EnglishBulgarian is shown in Fi"
P11-1090,N06-2013,0,0.0465407,"s, and word boundaries. These additional sources of information provide powerful bias for unsupervised learning, without increasing the asymptotic running time of the inference algorithm. Used as a monolingual model, our system significantly improves the state-of-the-art segmentation performance on three Arabic and Hebrew datasets. Used as a bilingual model, our system outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER). In agreement with some previous work on tokenization/morpheme segmentation for alignment (Chung and Gildea, 2009; Habash and Sadat, 2006), we find that the best segmentation for alignment does not coincide with the gold-standard segmenta896 tion and our bilingual model does not outperform our monolingual model in segmentation F-Measure. 2 Model Our model defines the probability of a target language sequence of words (each consisting of a sequence of morphemes), and alignment from target to source morphemes, given a source language sequence of words (each consisting of a sequence of morphemes). An example morpheme segmentation and alignment of phrases in English-Arabic and EnglishBulgarian is shown in Figure 1. In our task setti"
P11-1090,W07-0711,0,0.0985228,"orpheme segmentation over the target language. In a monolingual setting we introduce effective use of context by feature-rich modeling of the probabilities of morphemes, morphemetransitions, and word boundaries. These additional sources of information provide powerful bias for unsupervised learning, without increasing the asymptotic running time of the inference algorithm. Used as a monolingual model, our system significantly improves the state-of-the-art segmentation performance on three Arabic and Hebrew datasets. Used as a bilingual model, our system outperforms the state-of-the-art WDHMM (He, 2007) word alignment model as measured by alignment error rate (AER). In agreement with some previous work on tokenization/morpheme segmentation for alignment (Chung and Gildea, 2009; Habash and Sadat, 2006), we find that the best segmentation for alignment does not coincide with the gold-standard segmenta896 tion and our bilingual model does not outperform our monolingual model in segmentation F-Measure. 2 Model Our model defines the probability of a target language sequence of words (each consisting of a sequence of morphemes), and alignment from target to source morphemes, given a source languag"
P11-1090,koen-2004-pharaoh,0,0.0534045,"es and the transitions between corresponding source morphemes. The set of features with example values for this model is depicted in Table 3. The example is focussed on the features firing for the transition from the Bulgarian suffix te aligned to the first English morpheme µi−1 = te, ti−1 =suffix, ai−1 =1, to the Bulgarian root tsvet aligned to the third English morpheme µi = tsvet, ti =root, ai =3. The first feature is the absolute difference between ai and ai−1 + 1 and is similar to information used in other HMM word alignment models (Och and Ney, 2000) as well as phrasetranslation models (Koehn, 2004). The alignment positions ai are defined as indices of the aligned source morphemes. We additionally compute distortion in terms of distance in number of source words that are skipped. This distance corresponds to the feature name WORD DISTANCE. Looking at both kinds of distance is useful to capture the intuition that consecutive morphemes in the same target word should prefer to have a higher proximity of their aligned source words, as compared to consecutive morphemes which are not part of the same target word. The binned distances look at the sign of the distortion and bin the jumps into 5"
P11-1090,N09-1069,0,0.0356755,"t dependency relation between the source words containing the aligned source morphemes, if such relationship exists. We also represent alignments to null and have one null for each source word, similarly to (Och and Ney, 2000) and have a feature to indicate null. Additionally, we make use of several feature conjunctions involving the null, same target word, and distance features. 2.4 To reduce complexity of exposition we have omitted the final transition to a special state beyond the source sentence end after the last target morpheme. 899 Length Penalty Following (Chung and Gildea, 2009) and (Liang and Klein, 2009) we use an exponential length penalty on morpheme lengths to bias the model away from the maximum likelihood under-segmentation solution. The form of the penalty is: LP (|µi |) = |µ1|lp e i Here lp is a hyper-parameter indicating the power that the morpheme length is raised to. We fit this parameter using an annotated development set, to optimize morpheme-segmentation F1. The model is extremely sensitive to this value and performs quite poorly if such penalty is not used. 2.5 1 Value 1 1 fore1 fore1 suffix-root False DET-NN DET←NN False ... Inference We perform inference by EM training on the"
P11-1090,C10-1092,0,0.033577,"ired to make a general claim. We should note that the Arabic dataset used for word-alignment evaluation is unconventionally small and noisy (the sentences are very short phrases, automatically extracted using GIZA++). Thus the phrases might not be really translations, and the sentence length is much smaller than in standard parallel corpora. This warrants further model evaluation in a large-scale alignment setting. 5 Related Work This work is most closely related to the unsupervised tokenization and alignment models of Chung and Gildea (2009), Xu et al. (2008), Snyder and Barzilay (2008), and Nguyen et al. (2010). Chung & Gildea (2009) introduce a unigram model of tokenization based on IBM Model-1,which is a special case of our model. Snyder and Barzi903 lay (2008) proposes a hierarchical Bayesian model that combines the learning of monolingual segmentations and a cross-lingual alignment; their model is very different from ours. Incorporating morphological information into MT has received reasonable attention. For example, Goldwater & McClosky (2005) show improvements when preprocessing Czech input to reflect a morphological decomposition using combinations of lemmatization, pseudowords, and morphemes"
P11-1090,P00-1056,0,0.669313,"get language and the alignments between source and target morphemes are hidden. The source-side input, which we assume to be English, is processed with a gold morphological segmentation, part-of-speech, and dependency tree analysis. While these tools are unavailable in resource-poor languages, they are often available for at least one of the modeled languages in common translation tasks. This additional information then provides a source of features and conditioning information for the translation model. Our model is derived from the hidden-markov model for word alignment (Vogel et al., 1996; Och and Ney, 2000). Based on it, we define a dynamic the red a1 t1 μ1 b1 =2 = stem = &apos;cherven&apos; = OFF flower a2 t2 μ2 b2 =4 s a3 t3 = suffix = &apos;i&apos; = OFF μ3 b3 =1 = suffix = &apos;te&apos; = ON cherven.i.te Figure 2: A graphical depiction of the model generating the transliteration of the first Bulgarian word from Figure 1. Trigram dependencies and some incoming/outgoing arcs have been omitted for clarity. morpheme µi . For our example, b3 = 1, b5 = 1, and the other bi are 0. Let c1 , c2 , . . . , cT denote the non-space characters in the target string, and wb1 , . . . , wbT denote Bernoulli variables indicating whether th"
P11-1090,N09-1024,1,0.736739,"nyder and Barzilay, 2008). Further experimentation is required to make a general claim. We should note that the Arabic dataset used for word-alignment evaluation is unconventionally small and noisy (the sentences are very short phrases, automatically extracted using GIZA++). Thus the phrases might not be really translations, and the sentence length is much smaller than in standard parallel corpora. This warrants further model evaluation in a large-scale alignment setting. 5 Related Work This work is most closely related to the unsupervised tokenization and alignment models of Chung and Gildea (2009), Xu et al. (2008), Snyder and Barzilay (2008), and Nguyen et al. (2010). Chung & Gildea (2009) introduce a unigram model of tokenization based on IBM Model-1,which is a special case of our model. Snyder and Barzi903 lay (2008) proposes a hierarchical Bayesian model that combines the learning of monolingual segmentations and a cross-lingual alignment; their model is very different from ours. Incorporating morphological information into MT has received reasonable attention. For example, Goldwater & McClosky (2005) show improvements when preprocessing Czech input to reflect a morphological decom"
P11-1090,W00-0712,0,0.154772,"om the dynamic program for the IBM-1 like tokenization model of (Chung and Gildea, 2009) and extending it to account for the higher order on morphemes and the factored alignment state space. Even though the inference algorithm is low polynomial it is still much more expensive than the inference for an HMM model for word-alignment without segmentation. To reduce the running time of the model we limit the space of considered morpheme boundaries as follows: Given the target side of the corpus, we derive a list of K most frequent prefixes and suffixes using a simple trie-based method proposed by (Schone and Jurafsky, 2000).2 After we determine a list of allowed prefixes and suffixes we restrict our model to allow only segmentations of the form : ((p*)r(s*))+ where p and s belong to the allowed prefixes and suffixes and r can match any substring. We determine the number of prefixes and suffixes to consider using the maximum recall achievable by limiting the segmentation points in this way. Restricting the allowable segmentations in this way not only improves the speed of inference but also leads to improvements in segmentation accuracy. 2 Words are inserted into a trie with each complete branch naturally identif"
P11-1090,P08-1084,0,0.592998,"maximum recall achievable by limiting the segmentation points in this way. Restricting the allowable segmentations in this way not only improves the speed of inference but also leads to improvements in segmentation accuracy. 2 Words are inserted into a trie with each complete branch naturally identifying a potential suffix, inclusive of its subbranches. The list comprises of the K most frequent of these complete branches. Inserting the reversed words will then yield potential prefixes. 900 3 Evaluation For a majority of our testing we borrow the parallel phrases corpus used in previous work (Snyder and Barzilay, 2008), which we refer to as S&B. The corpus consists of 6,139 short phrases drawn from English, Hebrew, and Arabic translations of the Bible. We use an unmodified version of this corpus for the purpose of comparing morphological segmentation accuracy. For evaluating morpheme alignment accuracy, we have also augmented the English/Arabic subset of the corpus with a gold standard alignment between morphemes. Here morphological segmentations were obtained using the previously-annotated gold standard Arabic morphological segmentation, while the English was preprocessed with a morphological analyzer and"
P11-1090,C96-2141,0,0.519502,"mentation of the target language and the alignments between source and target morphemes are hidden. The source-side input, which we assume to be English, is processed with a gold morphological segmentation, part-of-speech, and dependency tree analysis. While these tools are unavailable in resource-poor languages, they are often available for at least one of the modeled languages in common translation tasks. This additional information then provides a source of features and conditioning information for the translation model. Our model is derived from the hidden-markov model for word alignment (Vogel et al., 1996; Och and Ney, 2000). Based on it, we define a dynamic the red a1 t1 μ1 b1 =2 = stem = &apos;cherven&apos; = OFF flower a2 t2 μ2 b2 =4 s a3 t3 = suffix = &apos;i&apos; = OFF μ3 b3 =1 = suffix = &apos;te&apos; = ON cherven.i.te Figure 2: A graphical depiction of the model generating the transliteration of the first Bulgarian word from Figure 1. Trigram dependencies and some incoming/outgoing arcs have been omitted for clarity. morpheme µi . For our example, b3 = 1, b5 = 1, and the other bi are 0. Let c1 , c2 , . . . , cT denote the non-space characters in the target string, and wb1 , . . . , wbT denote Bernoulli variables i"
P11-1090,C08-1128,1,0.407922,"d Barzilay, 2008). Further experimentation is required to make a general claim. We should note that the Arabic dataset used for word-alignment evaluation is unconventionally small and noisy (the sentences are very short phrases, automatically extracted using GIZA++). Thus the phrases might not be really translations, and the sentence length is much smaller than in standard parallel corpora. This warrants further model evaluation in a large-scale alignment setting. 5 Related Work This work is most closely related to the unsupervised tokenization and alignment models of Chung and Gildea (2009), Xu et al. (2008), Snyder and Barzilay (2008), and Nguyen et al. (2010). Chung & Gildea (2009) introduce a unigram model of tokenization based on IBM Model-1,which is a special case of our model. Snyder and Barzi903 lay (2008) proposes a hierarchical Bayesian model that combines the learning of monolingual segmentations and a cross-lingual alignment; their model is very different from ours. Incorporating morphological information into MT has received reasonable attention. For example, Goldwater & McClosky (2005) show improvements when preprocessing Czech input to reflect a morphological decomposition using com"
P11-1090,P10-1047,0,0.0288547,"Chung & Gildea (2009) introduce a unigram model of tokenization based on IBM Model-1,which is a special case of our model. Snyder and Barzi903 lay (2008) proposes a hierarchical Bayesian model that combines the learning of monolingual segmentations and a cross-lingual alignment; their model is very different from ours. Incorporating morphological information into MT has received reasonable attention. For example, Goldwater & McClosky (2005) show improvements when preprocessing Czech input to reflect a morphological decomposition using combinations of lemmatization, pseudowords, and morphemes. Yeniterzi and Oflazer (2010) bridge the morphological disparity between languages in a unique way by effectively aligning English syntactic elements (function words connected by dependency relations) to Turkish morphemes, using rule-based postprocessing of standard word alignment. Our work is partly inspired by that work and attempts to automate both the morpho-syntactic alignment and morphological analysis tasks. 6 Conclusion We have described an unsupervised model for morpheme segmentation and alignment based on Hidden Semi-Markov Models. Our model makes use of linguistic information to improve alignment quality. On th"
P11-2081,N10-1083,0,0.0618786,"Missing"
P11-2081,J93-2003,0,0.0914619,"Missing"
P11-2081,P00-1056,0,0.144713,", but it catches up and surpasses the random models at data sizes greater than 100 sentence pairs. To further evaluate the impact of initialization for IBM Model 1, we report on a set of experiments looking at alignment error rate achieved by different models. We report the performance of Model 1, as well as the performance of the more competitive HMM alignment model (Vogel et al., 1996), initialized from IBM-1 parameters. The dataset for these experiments is English-French parallel data from Hansards. The manually aligned data for evaluation consists of 137 sentences (a development set from (Och and Ney, 2000)). We look at two different training set sizes, a small set consisting of 1000 sentence pairs, and a reasonably-sized dataset containing 100,000 sentence pairs. In each data size condition, we report on the performance achieved by IBM-1, and the performance achieved by HMM initialized from the IBM1 parameters. For IBM Model 1 training, we either perform only 5 EM iterations (the standard setting in GIZA++), or run it to convergence. For each of these two settings, we either start training from uniform t(f |e) parameters, or random parameters. Table 2 details the results of these experiments. E"
P11-2081,C96-2141,0,0.944716,"as well as multiple random trials, and demonstrate that it results in variance in test set log-likelihood and alignment error rate. 1 Introduction Statistical alignment models have become widely used in machine translation, question answering, textual entailment, and non-NLP application areas such as information retrieval (Berger and Lafferty, 1999) and object recognition (Duygulu et al., 2002). The complexity of the probabilistic models needed to explain the hidden correspondence among words has necessitated the development of highly non-convex and difficult to optimize models, such as HMMs (Vogel et al., 1996) and IBM Models 3 and higher (Brown et al., 1993). To reduce the impact of getting stuck in bad local optima the original IBM paper (Brown et al., 1993) proposed the idea of training a sequence of models from simpler to complex, and using the simpler models to initialize the more complex ones. IBM Model 1 was the first model in this sequence and was considered a reliable initializer due to its convexity. In this paper we show that although IBM Model 1 is convex, it is not strictly convex, and there is a large 461 Michel Galley Microsoft Research Redmond, WA 98005, USA mgalley@microsoft.com spa"
P11-2081,steinberger-etal-2006-jrc,0,\N,Missing
P12-1073,N10-1015,0,0.0373069,"offered hope for creating NER analyzers in many languages. The first has been to devise an algorithm to tag foreign language entities using metadata from the semi-structured Wikipedia repository: inter-wiki links, article categories, and crosslanguage links (Richman and Schone, 2008). The second has been to use parallel English-foreign language data, a high-quality NER tagger for English, and projected annotations for the foreign language (Yarowsky et al., 2001; Das and Petrov, 2011). Parallel data has also been used to improve existing monolingual taggers or other analyzers in two languages (Burkett et al., 2010a; Burkett et al., 2010b). ∗ This research was conducted during the author’s internship at Microsoft Research The goal of this work is to create high-accuracy NER annotated data for foreign languages. Here we combine elements of both Wikipedia metadatabased approaches and projection-based approaches, making use of parallel sentences extracted from Wikipedia. We propose a statistical model which can combine the two types of information. Similarly to the joint model of Burkett et al. (2010a), our model can incorporate both monolingual and bilingual features in a log-linear framework. The advanta"
P12-1073,W10-2906,0,0.364915,"offered hope for creating NER analyzers in many languages. The first has been to devise an algorithm to tag foreign language entities using metadata from the semi-structured Wikipedia repository: inter-wiki links, article categories, and crosslanguage links (Richman and Schone, 2008). The second has been to use parallel English-foreign language data, a high-quality NER tagger for English, and projected annotations for the foreign language (Yarowsky et al., 2001; Das and Petrov, 2011). Parallel data has also been used to improve existing monolingual taggers or other analyzers in two languages (Burkett et al., 2010a; Burkett et al., 2010b). ∗ This research was conducted during the author’s internship at Microsoft Research The goal of this work is to create high-accuracy NER annotated data for foreign languages. Here we combine elements of both Wikipedia metadatabased approaches and projection-based approaches, making use of parallel sentences extracted from Wikipedia. We propose a statistical model which can combine the two types of information. Similarly to the joint model of Burkett et al. (2010a), our model can incorporate both monolingual and bilingual features in a log-linear framework. The advanta"
P12-1073,D09-1111,0,0.0164464,"f posterior probabilities of links from words insidePone entity to P words outside another entity i∈i1 ...im (1 − j∈j1 ...jn P (ai = j|s, t)). Probabilities from the other HMM direction are estimated analogously. • Indicator feature for whether the source and target entity can be extracted as a phrase pair according to the combined Viterbi alignments (grow-diag-final) and the standard phrase extraction heuristic (Koehn et al., 2003). Phonetic similarity features These features measure the similarity between a source and target entity based on pronunciation. We utilize a transliteration model (Cherry and Suzuki, 2009), trained from pairs of English person names and corresponding foreign language names, extracted from Wikipedia. The transliteration model can return an n-best list of transliterations of a foreign string, together with scores. For example the top 3 transliterations in English of the Bulgarian equivalent of “Igor Tudor” from Figure 1 are Igor Twoodor, Igor Twoodore, and Igore Twoodore. We estimate phonetic similarity between a source and target entity by computing Levenshtein and other distance metrics between the source entity and the closest transliteration of the target (out of a 10-best li"
P12-1073,P11-1061,0,0.155991,"rly impossible to build high-accuracy models for a large number of languages. Recently, there have been two lines of work which have offered hope for creating NER analyzers in many languages. The first has been to devise an algorithm to tag foreign language entities using metadata from the semi-structured Wikipedia repository: inter-wiki links, article categories, and crosslanguage links (Richman and Schone, 2008). The second has been to use parallel English-foreign language data, a high-quality NER tagger for English, and projected annotations for the foreign language (Yarowsky et al., 2001; Das and Petrov, 2011). Parallel data has also been used to improve existing monolingual taggers or other analyzers in two languages (Burkett et al., 2010a; Burkett et al., 2010b). ∗ This research was conducted during the author’s internship at Microsoft Research The goal of this work is to create high-accuracy NER annotated data for foreign languages. Here we combine elements of both Wikipedia metadatabased approaches and projection-based approaches, making use of parallel sentences extracted from Wikipedia. We propose a statistical model which can combine the two types of information. Similarly to the joint model"
P12-1073,W04-3248,0,0.24091,"ion filter for Korean which improved precision for Bulgarian and English. The Stanford tagger is worse than the Wiki-based tagger, but it is different enough that it contributes useful information to the task. 4 Projection Model From Table 2 we can see that the English Wikibased taggers are better than the Bulgarian and Korean ones, which is due to the abundance and completeness of English data in Wikipedia. In such circumstances, previous research has shown that one can project annotations from English to the more resource-poor language (Yarowsky et al., 2001). Here we follow the approach of Feng et al. (2004) to train a log-linear model for projection. Note that the Wiki-based taggers do not require training data and can be applied to any sentences from Wikipedia articles. The projection model described in this section and the Semi-CRF model described in Section 5 are trained using annotated data. They can be applied to tag foreign sentences in English-foreign sentence pairs extracted from Wikipedia. The task of projection is re-cast as a ranking task, where for each source entity Si , we rank all possible candidate target entity spans Tj and select the best span as corresponding to this source en"
P12-1073,P05-1045,0,0.00252135,"it has no concept of capitalization. Global foreign Wiki-based tagger The global and local+global taggers are analogous, using the categorization of foreign articles as above. Figure 2 shows the tags assigned to English and Bulgarian strings according to the local and global Wiki-based taggers. The global Wiki-based tagger could assign multiple labels to the same string (corresponding to different senses in different occurrences). In case of multiple possible labels, the most frequent one is denoted by * in the Figure. The Figure also shows the results of the Stanford NER tagger for English (Finkel et al., 2005) (we used the MUC-7 classifier). Table 2 reports the performance of the local (L Wiki-tagger), local+global (LG Wiki tagger) and the Stanford tagger. We can see that the local Wiki taggers have higher precision but lower recall than the local+global Wiki taggers. The local+global taggers Language English Bulgarian English Korean LG Wiki-tagger Stanford Tagger Prec L Wiki-tagger Rec F1 Prec Rec F1 Prec Rec F1 92.8 94.1 92.6 89.5 75.1 48.7 75.6 57.3 83.0 64.2 83.2 69.9 79.7 86.8 84.1 43.2 89.5 79.9 86.7 78.0 84.3 83.2 85.4 55.6 86.5 77.5 81.7 82.2 71.9 76.7 Table 2: English-Bulgarian and English"
P12-1073,N03-1017,0,0.012809,"es is aligned to a word from the other entity, estimated as: P Q i∈i1 ...im j∈j1 ...jn P (ai = j|s, t) We use an analogous estimate for the probability in the other direction. • Sum of posterior probabilities of links from words insidePone entity to P words outside another entity i∈i1 ...im (1 − j∈j1 ...jn P (ai = j|s, t)). Probabilities from the other HMM direction are estimated analogously. • Indicator feature for whether the source and target entity can be extracted as a phrase pair according to the combined Viterbi alignments (grow-diag-final) and the standard phrase extraction heuristic (Koehn et al., 2003). Phonetic similarity features These features measure the similarity between a source and target entity based on pronunciation. We utilize a transliteration model (Cherry and Suzuki, 2009), trained from pairs of English person names and corresponding foreign language names, extracted from Wikipedia. The transliteration model can return an n-best list of transliterations of a foreign string, together with scores. For example the top 3 transliterations in English of the Bulgarian equivalent of “Igor Tudor” from Figure 1 are Igor Twoodor, Igor Twoodore, and Igore Twoodore. We estimate phonetic si"
P12-1073,E03-1035,0,0.0121089,"sentences in English-foreign sentence pairs. The next step for this work is to train monolingual NE taggers for the foreign languages, which can work on text within or outside of Wikipedia. Preliminary results show performance of over 80 Fmeasure for such monolingual models. 6 Related Work As discussed throughout the paper, our model builds upon prior work on Wikipedia metadata-based NE tagging (Richman and Schone, 2008) and crosslingual projection for named entities (Feng et al., 2004). Other interesting work on aligning named entities in two languages is reported in (Huang and Vogel, 2002; Moore, 2003). Our direct semi-CRF tagging approach is related to bilingual labeling models presented in previous work (Burkett et al., 2010a; Smith and Smith, 2004; Snyder and Barzilay, 2008). All of these models jointly label aligned source and target sentences. In contrast, our model is not concerned with tagging English sentences but only tags foreign sentences in the context of English sentences. Compared to the joint log-linear model of Burkett et al. (2010a), our semi-CRF approach does not require enumeration of n-best candidates for the English sentence and is not limited to n-best candidates for t"
P12-1073,P00-1056,0,0.0783294,"aset from Table 1. For training, we use the humanannotated gold English entities and the manuallyspecified entity alignments to derive corresponding target entities. At test time we use the local+global Wiki-based tagger to define the English entities and we don’t use the manually annotated alignments. 4.1 Features We present the features for this model in a lot of detail since analogous feature types are also used in our final direct semi-CRF model. The features are grouped into four categories. Word alignment features We exploit a feature set based on HMM word alignments in both directions (Och and Ney, 2000). To define the features we make use of the posterior alignment link probabilities as well as the most likely (Viterbi) alignments. The posterior probabilities are the probabilities of links in both directions given the source and target sentences: P (ai = j|s, t) and P (aj = i|s, t). If a source entity consists of positions i1 , . . . , im and a potential corresponding target entity consists of positions j1 , . . . , jn , the word-alignment derived features are: • Probability that each word from one of the entities is aligned to a word from the other entity, estimated as: P Q i∈i1 ...im j∈j1"
P12-1073,P08-1001,0,0.0914453,"tly needed technology in NLP applications. State-ofthe-art statistical models for NER typically require a large amount of training data and linguistic expertise to be sufficiently accurate, which makes it nearly impossible to build high-accuracy models for a large number of languages. Recently, there have been two lines of work which have offered hope for creating NER analyzers in many languages. The first has been to devise an algorithm to tag foreign language entities using metadata from the semi-structured Wikipedia repository: inter-wiki links, article categories, and crosslanguage links (Richman and Schone, 2008). The second has been to use parallel English-foreign language data, a high-quality NER tagger for English, and projected annotations for the foreign language (Yarowsky et al., 2001; Das and Petrov, 2011). Parallel data has also been used to improve existing monolingual taggers or other analyzers in two languages (Burkett et al., 2010a; Burkett et al., 2010b). ∗ This research was conducted during the author’s internship at Microsoft Research The goal of this work is to create high-accuracy NER annotated data for foreign languages. Here we combine elements of both Wikipedia metadatabased approa"
P12-1073,W04-3207,0,0.0247097,"can work on text within or outside of Wikipedia. Preliminary results show performance of over 80 Fmeasure for such monolingual models. 6 Related Work As discussed throughout the paper, our model builds upon prior work on Wikipedia metadata-based NE tagging (Richman and Schone, 2008) and crosslingual projection for named entities (Feng et al., 2004). Other interesting work on aligning named entities in two languages is reported in (Huang and Vogel, 2002; Moore, 2003). Our direct semi-CRF tagging approach is related to bilingual labeling models presented in previous work (Burkett et al., 2010a; Smith and Smith, 2004; Snyder and Barzilay, 2008). All of these models jointly label aligned source and target sentences. In contrast, our model is not concerned with tagging English sentences but only tags foreign sentences in the context of English sentences. Compared to the joint log-linear model of Burkett et al. (2010a), our semi-CRF approach does not require enumeration of n-best candidates for the English sentence and is not limited to n-best candidates for the foreign sentence. It enables the use of multiple unweighted and overlapping entity annotations on the English sentence. 7 Conclusions In this paper"
P12-1073,N10-1063,1,0.300202,"and foreign language sentences that comprise our training and test data are extracted from Wikipedia (http://www.wikipedia.org). Currently there are more than 3.8 million articles in the English Wikipedia, 125,000 in the Bulgarian Wikipedia, and 131,000 in the Korean Wikipedia. 694 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 694–702, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Figure 1: A parallel sentence-pair showing gold-standard NE labels and word alignments. To create our dataset, we followed Smith et al. (2010) to find parallel-foreign sentences using comparable documents linked by inter-wiki links. The approach uses a small amount of manually annotated article-pairs to train a document-level CRF model for parallel sentence extraction. A total of 13,410 English-Bulgarian and 8,832 English-Korean sentence pairs were extracted. Of these, we manually annotated 91 EnglishBulgarian and 79 English-Korean sentence pairs with source and target named entities as well as word-alignment links among named entities in the two languages. Figure 1 illustrates a BulgarianEnglish sentence pair with alignment. The na"
P12-1073,H01-1035,0,0.734627,"ate, which makes it nearly impossible to build high-accuracy models for a large number of languages. Recently, there have been two lines of work which have offered hope for creating NER analyzers in many languages. The first has been to devise an algorithm to tag foreign language entities using metadata from the semi-structured Wikipedia repository: inter-wiki links, article categories, and crosslanguage links (Richman and Schone, 2008). The second has been to use parallel English-foreign language data, a high-quality NER tagger for English, and projected annotations for the foreign language (Yarowsky et al., 2001; Das and Petrov, 2011). Parallel data has also been used to improve existing monolingual taggers or other analyzers in two languages (Burkett et al., 2010a; Burkett et al., 2010b). ∗ This research was conducted during the author’s internship at Microsoft Research The goal of this work is to create high-accuracy NER annotated data for foreign languages. Here we combine elements of both Wikipedia metadatabased approaches and projection-based approaches, making use of parallel sentences extracted from Wikipedia. We propose a statistical model which can combine the two types of information. Simil"
P13-2072,P02-1040,0,\N,Missing
P13-2072,W07-0710,0,\N,Missing
P13-2072,D08-1024,0,\N,Missing
P13-2072,N09-1025,0,\N,Missing
P13-2072,D08-1064,0,\N,Missing
P13-2072,P02-1038,0,\N,Missing
P13-2072,N12-1047,0,\N,Missing
P13-2072,P11-2071,0,\N,Missing
P13-2072,D11-1125,0,\N,Missing
P13-2072,2012.amta-papers.4,0,\N,Missing
P13-2072,D07-1080,0,\N,Missing
P14-1064,W13-2233,0,0.0514209,"ys (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use higher order n-grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhancement alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates (§2.1), and are embedded in a target graph. Introduction Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn transla"
P14-1064,N09-1014,0,0.020533,"Missing"
P14-1064,N13-1056,0,0.495081,"ys (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use higher order n-grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhancement alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates (§2.1), and are embedded in a target graph. Introduction Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn transla"
P14-1064,E12-1014,0,0.0149254,"uct a more extensive evaluation of their graph-based BLI techniques, where the emphasis and end-to-end BLEU evaluations concentrated on OOVs, i.e., unigrams, and not on enriching the entire translation model. As with previous BLI work, these approaches only take into account source-side similarity of words; only moderate gains (and in the latter work, on a subset of language pairs evaluated) are obtained. Additionally, because of our structured propagation algorithm, our approach is better at handling multiple translation candidates and does not need to restrict itself to the top translation. Klementiev et al. (2012) propose a method that utilizes a pre-existing phrase table and a small bilingual lexicon, and performs BLI using monolingual corpora. The operational scope of their approach is limited in that they assume a scenario where unknown phrase pairs are provided (thereby sidestepping the issue of translation candidate generation for completely unknown phrases), and what remains is the estimation of phrasal probabilities. In our case, we obtain the phrase pairs from the graph structure (and therefore indirectly from the monolingual data) and a separate generation step, which plays an important role i"
P14-1064,N06-1003,0,0.182311,"corpora. Next, graph propagation identifies translations of phrases that were not observed in the bilingual corpus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets. 1 The challenge of learning translations from monolingual data is of long standing interest, and has been approached in several ways (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara e"
P14-1064,W02-0902,0,0.141694,"are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. The idea presented in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context. This line of work, initiated by Rapp (1995) and continued by others (Fung and Yee, 1998; Koehn and Knight, 2002) (inter alia) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only. Recent improvements to BLI (Tamura et al., 2012; Irvine and Callison-Burch, 2013b) have contained a graph-based flavor by presenting label propagation-based approaches using a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams. Razmara et al. (2013) and Irvine and CallisonBurch (2013a) conduct a more extensive evaluation of their graph-based BLI techniques, where th"
P14-1064,D13-1174,0,0.019094,"e 677 of the sizes of these spaces on the length of instances is to blame. Thus, the target phrase inventory from the parallel corpus may be inadequate for unlabeled instances. We therefore need to enrich the target or label space for unknown phrases. A na¨ıve way to achieve this goal would be to extract all n-grams, from n = 1 to a maximum ngram order, from the monolingual data, but this strategy would lead to a combinatorial explosion in the number of target phrases. Instead, by intelligently expanding the target space using linguistic information such as morphology (Toutanova et al., 2008; Chahuneau et al., 2013), or relying on the baseline system to generate candidates similar to self-training (McClosky et al., 2006), we can tractably propose novel translation candidates (white nodes in Fig. 1’s target graph) whose probabilities are then estimated during propagation. We refer to these additional candidates as “generated” candidates. To generate new translation candidates using the baseline system, we decode each unlabeled source bigram to generate its m-best translations. This set of candidate phrases is filtered to include only n-grams occurring in the target monolingual corpus, and helps to prune p"
P14-1064,N03-1017,0,0.149168,"ach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhancement alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates (§2.1), and are embedded in a target graph. Introduction Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn translation rules along with their probabilities. With large amounts of data, phrase-based translation systems (Koehn et al., 2003; Chiang, 2007) achieve state-of-the-art results in many typologically diverse language pairs (Bojar et al., 2013). However, the limiting factor in the success of these techniques is parallel data availability. Even in resource-rich languages, learning reliable translations of multiword phrases is a challenge, and an adequate phrasal inventory is crucial We then limit the set of translation options for each unlabeled source phrase (§2.3), and using a structured graph propagation algorithm, where translation information is propagated from labeled to unlabeled phrases proportional to both source"
P14-1064,J07-2003,0,0.0595165,"mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhancement alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates (§2.1), and are embedded in a target graph. Introduction Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn translation rules along with their probabilities. With large amounts of data, phrase-based translation systems (Koehn et al., 2003; Chiang, 2007) achieve state-of-the-art results in many typologically diverse language pairs (Bojar et al., 2013). However, the limiting factor in the success of these techniques is parallel data availability. Even in resource-rich languages, learning reliable translations of multiword phrases is a challenge, and an adequate phrasal inventory is crucial We then limit the set of translation options for each unlabeled source phrase (§2.3), and using a structured graph propagation algorithm, where translation information is propagated from labeled to unlabeled phrases proportional to both source and target phr"
P14-1064,P12-1032,0,0.243142,"of a certain number of classes. In our problem, the “label” for each node is actually a probability distribution over a set of translation candidates (target phrases). For a given node f , let e refer to a candidate in the label set for node f ; then in graph propagation, the probability of candidate e given source phrase f in iteration t + 1 is: X Pt+1 (e|f ) = Ts (j|f )Pt (e|j) (1) j2N (f ) 2.4.1 Structured Label Propagation The label set we are considering has a similarity structure encoded by the target graph. How can we exploit this structure in graph propagation on the source graph? In Liu et al. (2012), the authors generalize label propagation to structured label propagation (SLP) in an effort to work more elegantly with structured labels. In particular, the definition of target similarity is similar to that of source similarity: where the set N (f ) contains the (labeled and unlabeled) neighbors of node f , and Ts (j|f ) is a term that captures how similar nodes f and j are. This quantity is also known as the propagation probability, and its exact form will depend on the type of graph propagation algorithm used. For our purposes, node f is a source phrasal node, the set N (f ) refers to ot"
P14-1064,D12-1025,0,0.0314991,"veral different angles. Paraphrases extracted by “pivoting” via a third language (Callison-Burch et al., 2006) can be derived solely from monolingual corpora using distributional similarity (Marton et al., 2009). Snover et al. (2008) use cross-lingual information retrieval techniques to find potential sentence-level translation candidates among comparable corpora. In this case, the goal is to try and construct a corpus as close to parallel as possible from comparable corpora, and is a fairly different take on the problem we are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. The idea presented in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context. This line of work, initiated by Rapp (1995) and continued by others (Fung and Yee, 1998; Koehn and Knight, 2002) (inter alia) is limited from a downstream perspective, as translatio"
P14-1064,D09-1040,0,0.0271007,"he level of sentences, and while the latter does extend the framework to sub-spans of sentences, they do not discover new translation pairs or phrasal probabilities for new pairs at all, but instead re-estimate phrasal probabilities using the graph structure and add this score as an additional feature during decoding. The goal of leveraging non-parallel data in machine translation has been explored from several different angles. Paraphrases extracted by “pivoting” via a third language (Callison-Burch et al., 2006) can be derived solely from monolingual corpora using distributional similarity (Marton et al., 2009). Snover et al. (2008) use cross-lingual information retrieval techniques to find potential sentence-level translation candidates among comparable corpora. In this case, the goal is to try and construct a corpus as close to parallel as possible from comparable corpora, and is a fairly different take on the problem we are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. The idea presented in this paper is similar in"
P14-1064,P98-1069,0,0.0208295,"ke on the problem we are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. The idea presented in this paper is similar in spirit to bilingual lexicon induction (BLI), where a seed lexicon in two different languages is expanded with the help of monolingual corpora, primarily by extracting distributional similarities from the data using word context. This line of work, initiated by Rapp (1995) and continued by others (Fung and Yee, 1998; Koehn and Knight, 2002) (inter alia) is limited from a downstream perspective, as translations for only a small number of words are induced and oftentimes for common or frequently occurring ones only. Recent improvements to BLI (Tamura et al., 2012; Irvine and Callison-Burch, 2013b) have contained a graph-based flavor by presenting label propagation-based approaches using a seed lexicon, but evaluation is once again done on top-1 or top-3 accuracy, and the focus is on unigrams. Razmara et al. (2013) and Irvine and CallisonBurch (2013a) conduct a more extensive evaluation of their graph-based"
P14-1064,N06-1020,0,0.0144871,"from the parallel corpus may be inadequate for unlabeled instances. We therefore need to enrich the target or label space for unknown phrases. A na¨ıve way to achieve this goal would be to extract all n-grams, from n = 1 to a maximum ngram order, from the monolingual data, but this strategy would lead to a combinatorial explosion in the number of target phrases. Instead, by intelligently expanding the target space using linguistic information such as morphology (Toutanova et al., 2008; Chahuneau et al., 2013), or relying on the baseline system to generate candidates similar to self-training (McClosky et al., 2006), we can tractably propose novel translation candidates (white nodes in Fig. 1’s target graph) whose probabilities are then estimated during propagation. We refer to these additional candidates as “generated” candidates. To generate new translation candidates using the baseline system, we decode each unlabeled source bigram to generate its m-best translations. This set of candidate phrases is filtered to include only n-grams occurring in the target monolingual corpus, and helps to prune passed-through OOV words and invalid translations. To generate new translation candidates using morphologica"
P14-1064,D08-1089,0,0.0304745,"propagation step to sum to one over the fixed list of translation candidates, and run the SLP algorithm to convergence.3 2.5 Phrase-based SMT Expansion After graph propagation, each unlabeled phrase is labeled with a categorical distribution over the set of translation candidates defined in §2.3. In order to utilize these newly acquired phrase pairs, we need to compute their relevant features. The phrase pairs have four log-probability features with two likelihood features and two lexical weighting features. In addition, we use a sophisticated lexicalized hierarchical reordering model (HRM) (Galley and Manning, 2008) with five features for each phrase pair. We utilize the graph propagation-estimated forward phrasal probabilities P(e|f ) as the forward likelihood probabilities for the acquired phrases; to obtain the backward phrasal probability for a given phrase pair, we make use of Bayes’ Theorem: P(f |e) = P(e|f )P(f ) P(e) where the marginal probabilities of source and target phrases e and f are obtained from the counts extracted from the monolingual data. The baseline system’s lexical models are used for the forward and backward lexical scores. The HRM probabilities for the new phrase pairs are estima"
P14-1064,P03-1021,0,0.0354415,"learn from noisy parallel data compared to the traditional SMT system. Baseline phrasal systems are used both for comparison and for generating translation candidates for unlabeled phrases as described in §2.1. The baseline is a state-of-the-art phrase-based system; we perform word alignment using a lexicalized hidden Markov model, and then the phrase table is extracted using the grow-diag-final heuristic (Koehn et al., 2003). The 13 baseline features (2 lexical, 2 phrasal, 5 HRM, and 1 language model, word penalty, phrase length feature and distortion penalty feature) were tuned using MERT (Och, 2003), which is also used to tune the 4 feature weights introduced by the secondary phrase table (2 lexical and 2 phrasal, other features being shared between the two tables). For all systems, we use a distortion limit of 4. We use case-insensitive BLEU (Papineni et al., 2002) to evaluate translation quality. Tt (e0 |e)Pt (e0 |j) (5) e0 2H(j) With this formulation, even if e 6= e0 , the similarity Tt (e0 |e) as determined by the target phrase graph will dictate propagation probability. We renormalize the probability distributions after each propagation step to sum to one over the fixed list of tran"
P14-1064,P08-1088,0,0.184622,"tion identifies translations of phrases that were not observed in the bilingual corpus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets. 1 The challenge of learning translations from monolingual data is of long standing interest, and has been approached in several ways (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use hi"
P14-1064,P02-1040,0,0.0915957,"system; we perform word alignment using a lexicalized hidden Markov model, and then the phrase table is extracted using the grow-diag-final heuristic (Koehn et al., 2003). The 13 baseline features (2 lexical, 2 phrasal, 5 HRM, and 1 language model, word penalty, phrase length feature and distortion penalty feature) were tuned using MERT (Och, 2003), which is also used to tune the 4 feature weights introduced by the secondary phrase table (2 lexical and 2 phrasal, other features being shared between the two tables). For all systems, we use a distortion limit of 4. We use case-insensitive BLEU (Papineni et al., 2002) to evaluate translation quality. Tt (e0 |e)Pt (e0 |j) (5) e0 2H(j) With this formulation, even if e 6= e0 , the similarity Tt (e0 |e) as determined by the target phrase graph will dictate propagation probability. We renormalize the probability distributions after each propagation step to sum to one over the fixed list of translation candidates, and run the SLP algorithm to convergence.3 2.5 Phrase-based SMT Expansion After graph propagation, each unlabeled phrase is labeled with a categorical distribution over the set of translation candidates defined in §2.3. In order to utilize these newly"
P14-1064,P95-1050,0,0.650341,"monolingual corpora. Next, graph propagation identifies translations of phrases that were not observed in the bilingual corpus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets. 1 The challenge of learning translations from monolingual data is of long standing interest, and has been approached in several ways (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Cal"
P14-1064,P11-1002,0,0.127132,"tions of phrases that were not observed in the bilingual corpus, assuming that similar phrases have similar translations. We report results on a large Arabic-English system and a medium-sized Urdu-English system. Our proposed approach significantly improves the performance of competitive phrasebased systems, leading to consistent improvements between 1 and 4 BLEU points on standard evaluation sets. 1 The challenge of learning translations from monolingual data is of long standing interest, and has been approached in several ways (Rapp, 1995; Callison-Burch et al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use higher order n-grams inste"
P14-1064,P13-1109,0,0.432707,"al., 2006; Haghighi et al., 2008; Ravi and Knight, 2011). Our work introduces a new take on the problem using graphbased semi-supervised learning to acquire translation rules and probabilities by leveraging both monolingual and parallel data resources. On the source side, labeled phrases (those with known translations) are extracted from bilingual corpora, and unlabeled phrases are extracted from monolingual corpora; together they are embedded as nodes in a graph, with the monolingual data determining edge strengths between nodes (§2.2). Unlike previous work (Irvine and Callison-Burch, 2013a; Razmara et al., 2013), we use higher order n-grams instead of restricting to unigrams, since our approach goes beyond OOV mitigation and can enrich the entire translation model by using evidence from monolingual text. This enhancement alone results in an improvement of almost 1.4 BLEU points. On the target side, phrases initially consisting of translations from the parallel data are selectively expanded with generated candidates (§2.1), and are embedded in a target graph. Introduction Statistical approaches to machine translation (SMT) use sentence-aligned, parallel corpora to learn translation rules along with th"
P14-1064,D08-1090,0,0.023427,"and while the latter does extend the framework to sub-spans of sentences, they do not discover new translation pairs or phrasal probabilities for new pairs at all, but instead re-estimate phrasal probabilities using the graph structure and add this score as an additional feature during decoding. The goal of leveraging non-parallel data in machine translation has been explored from several different angles. Paraphrases extracted by “pivoting” via a third language (Callison-Burch et al., 2006) can be derived solely from monolingual corpora using distributional similarity (Marton et al., 2009). Snover et al. (2008) use cross-lingual information retrieval techniques to find potential sentence-level translation candidates among comparable corpora. In this case, the goal is to try and construct a corpus as close to parallel as possible from comparable corpora, and is a fairly different take on the problem we are looking at. Decipherment-based approaches (Ravi and Knight, 2011; Dou and Knight, 2012) have generally taken a monolingual view to the problem and combine phrase tables through the log-linear model during feature weight training. The idea presented in this paper is similar in spirit to bilingual le"
P14-1064,D12-1003,0,0.0728179,"embed these nodes in their graphs, we utilize the monolingual corpora on both the 1 The q most frequent words in the monolingual corpus were removed as keys from this mapping, as these high entropy features do not provide much information. 2 We also obtained the k-nearest neighbors of the translation candidates generated through these methods by utilizing the target graph, but this had minimal impact. 678 decoder output, or from a morphological generator (e.g., a cat and catlike in Fig. 1). A classic propagation algorithm that has been suitably modified for use in bilingual lexicon induction (Tamura et al., 2012; Razmara et al., 2013) is the label propagation (LP) algorithm of Zhu et al. (2003). In this case, Ts (f, j) is chosen to be: s wf,j Ts (j|f ) = P (2) s j 0 2N (f ) wf,j 0 The morphologically-generated candidates for a given source unlabeled phrase are initially defined as the target word sequences in the monolingual data that have the same stem sequence as one of the baseline’s target translations for a source phrase which has the same stem sequence as the unlabeled source phrase. These candidates are scored using stem-level translation probabilities, morpheme-level lexical weighting probabi"
P14-1064,P08-1059,1,0.812209,"he exponential dependence 677 of the sizes of these spaces on the length of instances is to blame. Thus, the target phrase inventory from the parallel corpus may be inadequate for unlabeled instances. We therefore need to enrich the target or label space for unknown phrases. A na¨ıve way to achieve this goal would be to extract all n-grams, from n = 1 to a maximum ngram order, from the monolingual data, but this strategy would lead to a combinatorial explosion in the number of target phrases. Instead, by intelligently expanding the target space using linguistic information such as morphology (Toutanova et al., 2008; Chahuneau et al., 2013), or relying on the baseline system to generate candidates similar to self-training (McClosky et al., 2006), we can tractably propose novel translation candidates (white nodes in Fig. 1’s target graph) whose probabilities are then estimated during propagation. We refer to these additional candidates as “generated” candidates. To generate new translation candidates using the baseline system, we decode each unlabeled source bigram to generate its m-best translations. This set of candidate phrases is filtered to include only n-grams occurring in the target monolingual cor"
P14-1064,P13-1140,0,0.157296,"g phrase table and a small bilingual lexicon, and performs BLI using monolingual corpora. The operational scope of their approach is limited in that they assume a scenario where unknown phrase pairs are provided (thereby sidestepping the issue of translation candidate generation for completely unknown phrases), and what remains is the estimation of phrasal probabilities. In our case, we obtain the phrase pairs from the graph structure (and therefore indirectly from the monolingual data) and a separate generation step, which plays an important role in good performance of the method. Similarly, Zhang and Zong (2013) present a series of heuristics that are applicable in a fairly narrow setting. The notion of translation consensus, wherein similar sentences on the source side are encour5 Conclusion In this work, we presented an approach that can expand a translation model extracted from a sentence-aligned, bilingual corpus using a large amount of unstructured, monolingual data in both source and target languages, which leads to improvements of 1.4 and 1.2 BLEU points over strong baselines on evaluation sets, and in some scenarios gains in excess of 4 BLEU points. In the future, we plan to estimate the grap"
P14-1064,C98-1066,0,\N,Missing
P14-1064,W13-2201,0,\N,Missing
P16-1136,D13-1160,0,0.0328171,"ons. We conduct a theoretical analysis of the efficiency gain from the approach. Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion. 1 Wen-tau Yih Microsoft Research Redmond, WA, USA Introduction Intelligent applications benefit from structured knowledge about the entities and relations in their domains. For example, large-scale knowledge bases (KB), such as Freebase (Bollacker et al., 2008) or DBPedia (Auer et al., 2007), have proven to be important resources for supporting open-domain question answering (Berant et al., 2013; Sun et al., 2015; Yih et al., 2015). In biomedicine, KBs such as the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) are crucial for understanding complex diseases such as cancer and for advancing precision medicine. ∗ This research was conducted during the author’s internship at Microsoft Research. While these knowledge bases are often carefully curated, they are far from complete. In non-static domains, new facts become true or are discovered at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 200"
P16-1136,D14-1165,1,0.764736,"d at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 2009; Surdeanu et al., 2012; Poon et al., 2015) or inferring facts from the relationships among known entities (Lao and Cohen, 2010) are thus important approaches for populating existing knowledge bases. Originally proposed as an alternative statistical relational learning method, the knowledge base embedding approach has gained a significant amount of attention, due to its simple prediction time computation and strong empirical performance (Nickel et al., 2011; Chang et al., 2014). In this framework, entities and relations in a knowledge base are represented in a continuous space, such as vectors and matrices. Whether two entities have a previously unknown relationship can be predicted by simple functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015). In c"
P16-1136,D15-1034,0,0.442388,"known relationship can be predicted by simple functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015). In contrast, using multi-step relation paths (e.g., husband(barack, michelle) ∧ mother(michelle, sasha) to train KB embeddings has been proposed very recently (Guu et al., 2015; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015). While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened w"
P16-1136,D13-1080,0,0.0302143,"oses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propo"
P16-1136,D14-1044,0,0.0461743,"number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propose a novel dynamic programming method that"
P16-1136,D15-1038,0,0.26163,"Missing"
P16-1136,D12-1093,0,0.0097446,"ormance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact"
P16-1136,D15-1082,0,0.816198,"redicted by simple functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015). In contrast, using multi-step relation paths (e.g., husband(barack, michelle) ∧ mother(michelle, sasha) to train KB embeddings has been proposed very recently (Guu et al., 2015; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015). While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is a"
P16-1136,P09-1113,0,0.0591613,"rant et al., 2013; Sun et al., 2015; Yih et al., 2015). In biomedicine, KBs such as the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) are crucial for understanding complex diseases such as cancer and for advancing precision medicine. ∗ This research was conducted during the author’s internship at Microsoft Research. While these knowledge bases are often carefully curated, they are far from complete. In non-static domains, new facts become true or are discovered at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 2009; Surdeanu et al., 2012; Poon et al., 2015) or inferring facts from the relationships among known entities (Lao and Cohen, 2010) are thus important approaches for populating existing knowledge bases. Originally proposed as an alternative statistical relational learning method, the knowledge base embedding approach has gained a significant amount of attention, due to its simple prediction time computation and strong empirical performance (Nickel et al., 2011; Chang et al., 2014). In this framework, entities and relations in a knowledge base are represented in a continuous space, such as vectors"
P16-1136,P15-1016,0,0.505284,"functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015). In contrast, using multi-step relation paths (e.g., husband(barack, michelle) ∧ mother(michelle, sasha) to train KB embeddings has been proposed very recently (Guu et al., 2015; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015). While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled tex"
P16-1136,N13-1008,0,0.861813,"cal challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propose a novel dynamic pr"
P16-1136,D12-1042,0,0.0364585,"un et al., 2015; Yih et al., 2015). In biomedicine, KBs such as the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) are crucial for understanding complex diseases such as cancer and for advancing precision medicine. ∗ This research was conducted during the author’s internship at Microsoft Research. While these knowledge bases are often carefully curated, they are far from complete. In non-static domains, new facts become true or are discovered at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 2009; Surdeanu et al., 2012; Poon et al., 2015) or inferring facts from the relationships among known entities (Lao and Cohen, 2010) are thus important approaches for populating existing knowledge bases. Originally proposed as an alternative statistical relational learning method, the knowledge base embedding approach has gained a significant amount of attention, due to its simple prediction time computation and strong empirical performance (Nickel et al., 2011; Chang et al., 2014). In this framework, entities and relations in a knowledge base are represented in a continuous space, such as vectors and matrices. Whether"
P16-1136,W15-4007,1,0.757471,"lation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propose a novel dynamic programming method that enables efficient modeling"
P16-1136,D15-1174,1,0.720524,"uded textual relations that occur at least 5 times between mentions of two genes that have a KB relation. This resulted in 3,827 distinct textual relations and 1,244,186 mentions.6 The number of textual relations is much larger than that of KB relations, and it helped induce much larger connectivity among genes (390,338 pairs of genes are directly connected in text versus 12,100 pairs in KB). Systems A LL - PATHS denotes our compositional learning approach that sums over all paths using 6 Modeling such a large number of textual relations introduces sparsity, which necessitates models such as (Toutanova et al., 2015; Verga et al., 2015) to derive composed representations of text. We leave integration with such methods for future work. 1440 dynamic programming; A LL - PATHS + NODES additionally models nodes in the paths. P RUNED PATHS denotes the traditional approach that learns from sampled paths detailed in §3.1.2; paths with occurrence less than a cutoff are pruned (c = 1 in Table 1 means that all sampled paths are used). The most relevant prior approach is Guu et al. (2015). We ran experiments using both their publicly available code and our re-implementation. We also included the B ILINEAR - DIAG bas"
P16-1136,P15-1128,1,0.122455,"of the efficiency gain from the approach. Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion. 1 Wen-tau Yih Microsoft Research Redmond, WA, USA Introduction Intelligent applications benefit from structured knowledge about the entities and relations in their domains. For example, large-scale knowledge bases (KB), such as Freebase (Bollacker et al., 2008) or DBPedia (Auer et al., 2007), have proven to be important resources for supporting open-domain question answering (Berant et al., 2013; Sun et al., 2015; Yih et al., 2015). In biomedicine, KBs such as the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) are crucial for understanding complex diseases such as cancer and for advancing precision medicine. ∗ This research was conducted during the author’s internship at Microsoft Research. While these knowledge bases are often carefully curated, they are far from complete. In non-static domains, new facts become true or are discovered at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 2009; Surdeanu et al., 2012; Poon et al."
P16-1136,N16-1103,0,\N,Missing
P17-1070,buck-etal-2014-n,0,0.0428013,"Missing"
P17-1070,P15-1001,0,0.145211,"d. The sequence of embeddings for our example three-word sequence becomes: x = (x1 , x2 , hc M ). We use the same dimensionality for word embedding vectors xi and composed character sequence vectors hc M to ensure the two ways to define embeddings are compatible. Our hybrid source encoder architecture is similar to the one proposed by Luong and Manning (2016). Nested attention hybrid decoder In traditional word-based sequence-to-sequence models special target UNK tokens are used to represent outputs that are outside the target vocabulary. A post-processing UNK-replacement method is then used (Cho et al., 2015; Yuan and Briscoe, 2016) to replace these special tokens with target words. The hybrid model of (Luong and Manning, 2016) uses a jointly trained character-level decoder to generate target words corresponding to UNK tokens, and outperforms the traditional approach in the machine translation task. However, unlike machine translation, models for grammar correction conduct “translation” in the same language, and often need to apply a small number of local edits to the character sequence of a source word corresponding to the target UNK word. For example, rare but correct words such as entity names"
P17-1070,P06-1032,0,0.145092,"ificantly outperforms previous neural models for GEC as measured on the standard CoNLL14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography. 1 Introduction One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016). Such systems, which are based on phrasebased MT models that are typically trained on large sets of sentence-correction pairs, can correct global errors such as word order and usage and local errors in spelling and inflection. The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016). ∗ This work was conducted while the third author worked at Microsoft Research. Recently, neural machine translation (NMT) systems h"
P17-1070,W13-1703,0,0.636911,"0 ¯ , yc GRUc decNested (dc n−1 ) n>0 n−1 4.1 where cc n is the context vector obtained using character-level attention on the sequence ec and the last state of the character-level decoder dc n , computed following equations 2, 3 and 4, but using a different set of parameters. These equations show that the character-level decoder with nested attention can use both the wordlevel state dˆs , and the character-level context cc n Experiments Dataset and Evaluation We use standard publicly available datasets for training and evaluation. One data source is the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), which is provided as a training set for the CoNLL-13 and CoNLL-14 shared tasks. From the original corpus of size about 60K parallel sentences, we randomly selected close to 5K sentence pairs for use as a validation set, and 45K parallel sentences for use in training. A second data source 757 #Sent pairs Training 2,608,679 Validation 4,771 Development 1,381 Test get OOV, but follow Cho et al. (2015), Section 1,312 3.3 to use the NMT system’s attention weights instead. The target OOV is then replaced by the most likely correction of the source word from the wordcorrection lexicon, or by the so"
P17-1070,C10-1041,1,0.783437,"revious neural models for GEC as measured on the standard CoNLL14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography. 1 Introduction One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016). Such systems, which are based on phrasebased MT models that are typically trained on large sets of sentence-correction pairs, can correct global errors such as word order and usage and local errors in spelling and inflection. The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016). ∗ This work was conducted while the third author worked at Microsoft Research. Recently, neural machine translation (NMT) systems have achieved subst"
P17-1070,D16-1161,0,0.626732,"els for GEC as measured on the standard CoNLL14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography. 1 Introduction One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016). Such systems, which are based on phrasebased MT models that are typically trained on large sets of sentence-correction pairs, can correct global errors such as word order and usage and local errors in spelling and inflection. The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016). ∗ This work was conducted while the third author worked at Microsoft Research. Recently, neural machine translation (NMT) systems have achieved substantial improvements in translation qualit"
P17-1070,Q17-1026,0,0.00769883,"elp of character and word-level encoders and decoders with two nested levels of attention. Our model is inspired by advances in sub-word level modeling in neural machine translation. We build mostly on the hybrid model of Luong and Manning (2016) to expand its capability to correct rare words by fine-grained character-level attention. We directly compare our model to the one of Luong and Manning (2016) on the grammar correction task. Alternative methods for MT include modeling of word pieces to achieve open vocabulary (Sennrich et al., 2016), and more recently, fully character-level modeling (Lee et al., 2017). None of these models integrate two nested levels of attention although an empirical evaluation of these approaches for GEC would also be interesting. 3 specific sequence of hidden state vectors e: e = (h1 , . . . , hT ) The hidden state ht at time t is computed as: ft = GRUencf (ft−1 , xt ) , bt = GRUencb (bt+1 , xt ), ht = [ft ; bt ], where GRUencf and GRUencb stand for gated recurrent unit functions as described in Cho et al. (2014). We use the symbol GRU with different subscripts to represent GRU functions using different sets of parameters (for example, we used the encf and encb subscrip"
P17-1070,P16-1100,0,0.12905,"ctors and attention at both word and character levels, we model all contextual words, including OOVs, in a unified context vector representation. In particular, as we will discuss in Section 5, the character-level attention layer captures most useful information for correcting local errors that involve small edits in orthography. Our model differs substantially from the wordlevel S2S model of Yuan and Briscoe (2016) and the character-level S2S model of Xie et al. (2016) in the way we infuse information at both the word level and the character level. We extend the wordcharacter hybrid model of Luong and Manning (2016), which was originally developed for machine translation, by introducing a character attention layer. This allows the model to learn substitution patterns at both the character level and the word level in an end-to-end fashion, using sentencecorrection pairs. We validate the effectiveness of our model on the CoNLL-14 benchmark dataset (Ng et al., 2014). Results show that the proposed model outperforms all previous neural models for GEC, including the hybrid model of Luong and Manning (2016), which we apply to GEC for the first time. When integrated with a large word-based n-gram language model"
P17-1070,W14-1701,0,0.533419,"y from the wordlevel S2S model of Yuan and Briscoe (2016) and the character-level S2S model of Xie et al. (2016) in the way we infuse information at both the word level and the character level. We extend the wordcharacter hybrid model of Luong and Manning (2016), which was originally developed for machine translation, by introducing a character attention layer. This allows the model to learn substitution patterns at both the character level and the word level in an end-to-end fashion, using sentencecorrection pairs. We validate the effectiveness of our model on the CoNLL-14 benchmark dataset (Ng et al., 2014). Results show that the proposed model outperforms all previous neural models for GEC, including the hybrid model of Luong and Manning (2016), which we apply to GEC for the first time. When integrated with a large word-based n-gram language model, our GEC system achieves an F0.5 of 45.15 on CoNLL-14, substantially exceeding the previFigure 1: Architecture of Nested Attention Hybrid Model ously reported top performance of 40.56 achieved by using a neural model and an external language model (Xie et al., 2016). 2 Related Work A variety of classifier-based and MT-based techniques have been applie"
P17-1070,P16-1208,0,0.266305,"he problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016). Such systems, which are based on phrasebased MT models that are typically trained on large sets of sentence-correction pairs, can correct global errors such as word order and usage and local errors in spelling and inflection. The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016). ∗ This work was conducted while the third author worked at Microsoft Research. Recently, neural machine translation (NMT) systems have achieved substantial improvements in translation quality over phrase-based MT systems (Sutskever et al., 2014; Bahdanau et al., 2015). Thus, there is growing interest in applying neural systems to GEC (Yuan and Briscoe, 2016; Xie et al., 2016). In this paper, we significantly extend previous work, and explore new neural models to meet the unique challenges of GEC. The core component of most NMT systems is a sequence-to-sequence (S2S) model which encodes a seq"
P17-1070,Q16-1013,0,0.0537991,"ural systems to GEC (Yuan and Briscoe, 2016; Xie et al., 2016). In this paper, we significantly extend previous work, and explore new neural models to meet the unique challenges of GEC. The core component of most NMT systems is a sequence-to-sequence (S2S) model which encodes a sequence of source words into a vector and then generates a sequence of target words from the vector. Unlike the phrase-based MT models, the S2S model can capture long-distance, or even global, word dependencies, which are crucial to correcting global grammatical errors and helping users achieve native speaker fluency (Sakaguchi et al., 2016). Thus, the S2S model is expected to perform better on GEC than phrase-based models. However, as we will show in this paper, to achieve the best performance on GEC, we still need to extend the standard S2S model to address several task-specific challenges, which we will describe below. First, a GEC model needs to deal with an extremely large vocabulary that consists of a large number of words and their (mis)spelling variations. Second, the GEC model needs to capture structure at different levels of granularity in order to correct errors of different types. For example, while correcting spellin"
P17-1070,W16-0528,0,0.0121622,"s, and multiple large language models. Neural approaches to the task are less explored. We believe that the advances from Junczys-Dowmunt and Grundkiewicz (2016) are complementary to the ones we propose for neural MT, and could be integrated with neural models to achieve even higher performance. Two prior works explored sequence to sequence neural models for GEC (Xie et al., 2016; Yuan and Briscoe, 2016), while Chollampatt et al. (2016) integrated neural features in a phrase-based system for the task. Neural models were also applied to the related sub-task of grammatical error identification (Schmaltz et al., 2016). Yuan and Briscoe (2016) demonstrated the promise of neural MT for GEC but did not adapt the basic sequence-to-sequence with attention to its unique challenges, falling back to traditional word-alignment models to address vocabulary coverage with a post-processing heuristic. Xie et al. (2016) built a character-level sequence 754 to sequence model, which achieves open vocabulary and character-level modeling, but has difficulty with global word-level decisions. The primary focus of our work is integration of character and word-level reasoning in neural models for GEC, to capture global fluency"
P17-1070,P16-1162,0,0.119168,"ants, while obtaining open vocabulary coverage. This is achieved with the help of character and word-level encoders and decoders with two nested levels of attention. Our model is inspired by advances in sub-word level modeling in neural machine translation. We build mostly on the hybrid model of Luong and Manning (2016) to expand its capability to correct rare words by fine-grained character-level attention. We directly compare our model to the one of Luong and Manning (2016) on the grammar correction task. Alternative methods for MT include modeling of word pieces to achieve open vocabulary (Sennrich et al., 2016), and more recently, fully character-level modeling (Lee et al., 2017). None of these models integrate two nested levels of attention although an empirical evaluation of these approaches for GEC would also be interesting. 3 specific sequence of hidden state vectors e: e = (h1 , . . . , hT ) The hidden state ht at time t is computed as: ft = GRUencf (ft−1 , xt ) , bt = GRUencb (bt+1 , xt ), ht = [ft ; bt ], where GRUencf and GRUencb stand for gated recurrent unit functions as described in Cho et al. (2014). We use the symbol GRU with different subscripts to represent GRU functions using differe"
P17-1070,P12-2039,0,0.0500203,"system’s attention weights instead. The target OOV is then replaced by the most likely correction of the source word from the wordcorrection lexicon, or by the source word itself if there are no available corrections. Table 1: Overview of the datasets used. Source NUCLE CLC lang-8 Total #Sent pairs 45,422 1,517,174 1,046,083 2,608,679 4.3 Table 2: Training data by source. is the Cambridge Learner Corpus (CLC) (Nicholls, 2003), from which we extracted a substantially larger set of parallel sentences. Finally, we used additional training examples from the Lang-8 Corpus of Learner English v1.0 (Tajiri et al., 2012). As Lang-8 data is crowd-sourced, we used heuristics to filter out noisy examples: we removed sentences longer than 100 words and sentence pairs where the correction was substantially shorter than the input text. Table 2 shows the number of sentence pairs from each source used for training. We evaluate the performance of the models on the standard sets from the CoNLL-14 shared task (Ng et al., 2014). We report final performance on the CoNLL-14 test set without alternatives, and analyze model performance on the CoNLL-13 development set (Dahlmeier et al., 2013). We use the development and valid"
P17-1070,N16-1042,0,0.203389,"errors such as word order and usage and local errors in spelling and inflection. The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016). ∗ This work was conducted while the third author worked at Microsoft Research. Recently, neural machine translation (NMT) systems have achieved substantial improvements in translation quality over phrase-based MT systems (Sutskever et al., 2014; Bahdanau et al., 2015). Thus, there is growing interest in applying neural systems to GEC (Yuan and Briscoe, 2016; Xie et al., 2016). In this paper, we significantly extend previous work, and explore new neural models to meet the unique challenges of GEC. The core component of most NMT systems is a sequence-to-sequence (S2S) model which encodes a sequence of source words into a vector and then generates a sequence of target words from the vector. Unlike the phrase-based MT models, the S2S model can capture long-distance, or even global, word dependencies, which are crucial to correcting global grammatical errors and helping users achieve native speaker fluency (Sakaguchi et al., 2016). Thus, the S2S mode"
P19-1335,N19-1423,1,0.727081,"t al. (2011); Chen et al. (2012); Yang and Eisenstein (2015), inter alia, pre-trained on the source and target domain unlabeled data jointly with the goal of discovering features that generalize across domains. After pre-training, the model is fine-tuned on the source-domain labeled data.6 Open-corpus pre-training Instead of explicitly adapting to a target domain, this approach simply applies unsupervised pre-training to large corpora before fine-tuning on the source-domain labeled data. Examples of this approach include ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). 6 In many works, the learned representations are kept fixed and only higher layers are updated. 3453 Intuitively, the target-domain distribution is likely to be partially captured by pre-training if the open corpus is sufficiently large and diverse. Indeed, open-corpus pre-training has been shown to benefit out-of-domain performance far more than indomain performance (He et al., 2018). Domain-adaptive pre-training In addition to pre-training stages from other approaches, we propose to insert a penultimate domain adaptive pre-training (DAP) stage, where the model is pre-trained only on the ta"
P19-1335,D17-1277,0,0.200861,"Missing"
P19-1335,D18-1498,0,0.048978,"Missing"
P19-1335,D17-1284,0,0.326376,"etween EL datasets (Bunescu and Pasca, 2006; Ling et al., 2015), most focus on a standard setting where mentions from a comprehensive test entity dictionary (often Wikipedia) are seen during training, and rich statistics and meta-data can be utilized (Roth et al., 2014). Labeled in-domain documents with mentions are also assumed to be available. Cross-Domain EL Recent work has also generalized to a cross-domain setting, linking entity mentions in different types of text, such as blogposts and news articles to the Wikipedia KB, while only using labeled mentions in Wikipedia for training (e.g., Gupta et al. (2017); Le and Titov (2018), inter alia). Linking to Any DB Sil et al. (2012) proposed a task setup very similar to ours, and later work (Wang et al., 2015) has followed a similar setting. The main difference between zero-shot EL and these works is that they assumed either a highcoverage alias table or high-precision token overlap heuristics to reduce the size of the entity candidate set (i.e., to less than four in Sil et al. (2012)) and relied on structured data to help disambiguation. By compiling and releasing a multi-world dataset focused on learning from textual information, we hope to help dri"
P19-1335,P18-2058,1,0.846885,"es unsupervised pre-training to large corpora before fine-tuning on the source-domain labeled data. Examples of this approach include ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). 6 In many works, the learned representations are kept fixed and only higher layers are updated. 3453 Intuitively, the target-domain distribution is likely to be partially captured by pre-training if the open corpus is sufficiently large and diverse. Indeed, open-corpus pre-training has been shown to benefit out-of-domain performance far more than indomain performance (He et al., 2018). Domain-adaptive pre-training In addition to pre-training stages from other approaches, we propose to insert a penultimate domain adaptive pre-training (DAP) stage, where the model is pre-trained only on the target-domain data. As usual, DAP is followed by a final fine-tuning stage on the source-domain labeled data. The intuition for DAP is that representational capacity is limited, so models should prioritize the quality of target domain representations above all else. We introduce notation to describe various ways in which pre-training stages can be composed. • Usrc denotes text segments fr"
P19-1335,P18-1031,0,0.049145,"Missing"
P19-1335,P18-1148,0,0.0731455,"and Pasca, 2006; Ling et al., 2015), most focus on a standard setting where mentions from a comprehensive test entity dictionary (often Wikipedia) are seen during training, and rich statistics and meta-data can be utilized (Roth et al., 2014). Labeled in-domain documents with mentions are also assumed to be available. Cross-Domain EL Recent work has also generalized to a cross-domain setting, linking entity mentions in different types of text, such as blogposts and news articles to the Wikipedia KB, while only using labeled mentions in Wikipedia for training (e.g., Gupta et al. (2017); Le and Titov (2018), inter alia). Linking to Any DB Sil et al. (2012) proposed a task setup very similar to ours, and later work (Wang et al., 2015) has followed a similar setting. The main difference between zero-shot EL and these works is that they assumed either a highcoverage alias table or high-precision token overlap heuristics to reduce the size of the entity candidate set (i.e., to less than four in Sil et al. (2012)) and relied on structured data to help disambiguation. By compiling and releasing a multi-world dataset focused on learning from textual information, we hope to help drive progress in linkin"
P19-1335,P18-1010,0,0.0448191,"respectively the normalized and unnormalized accuracies. 7 Related Work We discussed prior entity linking task definitions and compared them to our task in section 2. Here, we briefly overview related entity linking models and unsupervised domain adaptation methods. Entity linking models Entity linking given mention boundaries as input can be broken into the tasks of candidate generation and candidate ranking. When frequency information or alias tables are unavailable, prior work has used measures of similarity of the mention string to entity names for candidate generation (Sil et al., 2012; Murty et al., 2018). For candidate ranking, recent work employed distributed representations of mentions in context and entity candidates and neural models to score their compatibility. Mentions in context have been represented using e.g., CNN (Murty et al., 2018), LSTM (Gupta et al., 2017), or bag-of-word embeddings (Ganea and Hofmann, 2017). Entity descriptions have been represented using similar architectures. To the best of our knowledge, while some models allow for crossattention between single-vector entity embeddings and mention-in-context token representations, no prior works have used full cross-attenti"
P19-1335,N18-1202,1,0.490024,"to the two existing approaches. Task-adaptive pre-training Glorot et al. (2011); Chen et al. (2012); Yang and Eisenstein (2015), inter alia, pre-trained on the source and target domain unlabeled data jointly with the goal of discovering features that generalize across domains. After pre-training, the model is fine-tuned on the source-domain labeled data.6 Open-corpus pre-training Instead of explicitly adapting to a target domain, this approach simply applies unsupervised pre-training to large corpora before fine-tuning on the source-domain labeled data. Examples of this approach include ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). 6 In many works, the learned representations are kept fixed and only higher layers are updated. 3453 Intuitively, the target-domain distribution is likely to be partially captured by pre-training if the open corpus is sufficiently large and diverse. Indeed, open-corpus pre-training has been shown to benefit out-of-domain performance far more than indomain performance (He et al., 2018). Domain-adaptive pre-training In addition to pre-training stages from other approaches, we propose to insert a penultimate domain adaptive pre-"
P19-1335,D15-1081,0,0.407235,"dictionaries such as legal cases, company project descriptions, the set of characters in a novel, or a terminology glossary. Unfortunately, labeled data are not readily available and are often expensive to obtain for these specialized entity dictionaries. Therefore, we need to develop entity linking systems that can generalize to unseen specialized entities. Without frequency statistics and meta-data, the task becomes substantially more challenging. Some prior works have pointed out the importance of building entity linking systems that can generalize to unseen entity sets (Sil et al., 2012; Wang et al., 2015), but adopt an additional set of assumptions. 3449 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3449–3460 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics In this work, we propose a new zero-shot entity linking task, and construct a new dataset for it.2 The target dictionary is simply defined as a set of entities, each with a text description (from a canonical entity page, for example). We do not constrain mentions to named entities, unlike some prior work, which makes the task harder due to large numb"
P19-1335,N15-1069,0,0.0314569,"similar to Pool-Transformer and Cand-Pool-Transformer respectively but with different neural architectures for encoding. 5 Adapting to the Target World We focus on using unsupervised pre-training to ensure that downstream models are robust to target domain data. There exist two general strategies for pre-training: (1) task-adaptive pre-training, and (2) open-corpus pre-training. We describe these below, and also propose a new strategy: domainadaptive pre-training (DAP), which is complementary to the two existing approaches. Task-adaptive pre-training Glorot et al. (2011); Chen et al. (2012); Yang and Eisenstein (2015), inter alia, pre-trained on the source and target domain unlabeled data jointly with the goal of discovering features that generalize across domains. After pre-training, the model is fine-tuned on the source-domain labeled data.6 Open-corpus pre-training Instead of explicitly adapting to a target domain, this approach simply applies unsupervised pre-training to large corpora before fine-tuning on the source-domain labeled data. Examples of this approach include ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019). 6 In many works, the learned represent"
P19-1335,P14-6004,1,0.925416,"the zero-shot entity linking task and discuss its relationship to prior work. 2 Existing datasets are either unsuitable or would have to be artificially partitioned to construct a dataset for this task. 2.1 Review: Entity linking Entity linking (EL) is the task of grounding entity mentions by linking them to entries in a given database or dictionary of entities. Formally, given a mention m and its context, an entity linking system links m to the corresponding entity in an entity set E = {ei }i=1,...,K , where K is the number of entities. The standard definition of EL (Bunescu and Pasca, 2006; Roth et al., 2014; Sil et al., 2018) assumes that mention boundaries are provided by users or a mention detection system. The entity set E can contain tens of thousands or even millions of entities, making this a challenging task. In practice, many entity linking systems rely on the following resources or assumptions: Single entity set This assumes that there is a single comprehensive set of entities E shared between training and test examples. Alias table An alias table contains entity candidates for a given mention string and limits the possibilities to a relatively small set. Such tables are often compiled"
P19-1335,P18-5008,0,0.0129955,"y linking task and discuss its relationship to prior work. 2 Existing datasets are either unsuitable or would have to be artificially partitioned to construct a dataset for this task. 2.1 Review: Entity linking Entity linking (EL) is the task of grounding entity mentions by linking them to entries in a given database or dictionary of entities. Formally, given a mention m and its context, an entity linking system links m to the corresponding entity in an entity set E = {ei }i=1,...,K , where K is the number of entities. The standard definition of EL (Bunescu and Pasca, 2006; Roth et al., 2014; Sil et al., 2018) assumes that mention boundaries are provided by users or a mention detection system. The entity set E can contain tens of thousands or even millions of entities, making this a challenging task. In practice, many entity linking systems rely on the following resources or assumptions: Single entity set This assumes that there is a single comprehensive set of entities E shared between training and test examples. Alias table An alias table contains entity candidates for a given mention string and limits the possibilities to a relatively small set. Such tables are often compiled from a labeled trai"
P19-1335,D12-1011,0,0.496517,"Missing"
P19-1335,E06-1002,0,\N,Missing
P19-1335,Q15-1023,0,\N,Missing
P19-1612,Q13-1005,0,0.0263304,"concepts that sparse representations can cleanly separate. These errors indicate that a hybrid approach would be promising future work. 10 Related Work Recent progress has been made towards improving evidence retrieval (Wang et al., 2018; Kratzwald and Feuerriegel, 2018; Lee et al., 2018; Das et al., 2019) by learning to aggregate from multiple retrieval steps. They re-rank evidence candidates from a closed set, and we aim to integrate these complementary approaches in future work. Our approach is also reminiscent of weakly supervised semantic parsing (Clarke et al., 2010; Liang et al., 2013; Artzi and Zettlemoyer, 2013; Fader et al., 2014; Berant et al., 2013; Kwiatkowski et al., 2013), with which we share similar challenges—(1) inference and learning are tightly coupled, (2) latent derivations must be discovered, and (3) strong inductive biases are needed to find positive learning signal while avoiding spurious ambiguities. While we motivate ICT from first principles as an unsupervised proxy for evidence retrieval, it is closely related to existing representation learning literature. ICT can be considered a generalization of the skip-gram objective (Mikolov et al., 2013), with a coarser granularity, deep a"
P19-1612,P17-1147,0,0.580471,"put. This presents a more realistic scenario for practical applications. Current approaches require a blackbox information retrieval (IR) system to do much of the heavy lifting, even though it cannot be fine-tuned on the downstream task. In the strongly supervised setting popularized by DrQA (Chen et al., 2017), they also assume a reading comprehension model trained on question-answer-evidence triples, such as SQuAD (Rajpurkar et al., 2016). The IR system is used at test time to generate evidence candidates in place of the gold evidence. In the weakly supervised setting, proposed by TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), and Quasar (Dhingra et al., 2017), the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence. These approaches rely on the IR system to massively reduce the search space and/or reduce spurious ambiguity. However, QA is fundamentally different from IR (Singh, 2012). Whereas IR is concerned with lexical and semantic matching, questions are by definition under-specified and require more language understanding, since users are explicitly looking for unknown information. Instead of being subject to the recall ceiling"
P19-1612,D18-1055,0,0.150956,"Missing"
P19-1612,D13-1161,0,0.0262235,"s indicate that a hybrid approach would be promising future work. 10 Related Work Recent progress has been made towards improving evidence retrieval (Wang et al., 2018; Kratzwald and Feuerriegel, 2018; Lee et al., 2018; Das et al., 2019) by learning to aggregate from multiple retrieval steps. They re-rank evidence candidates from a closed set, and we aim to integrate these complementary approaches in future work. Our approach is also reminiscent of weakly supervised semantic parsing (Clarke et al., 2010; Liang et al., 2013; Artzi and Zettlemoyer, 2013; Fader et al., 2014; Berant et al., 2013; Kwiatkowski et al., 2013), with which we share similar challenges—(1) inference and learning are tightly coupled, (2) latent derivations must be discovered, and (3) strong inductive biases are needed to find positive learning signal while avoiding spurious ambiguities. While we motivate ICT from first principles as an unsupervised proxy for evidence retrieval, it is closely related to existing representation learning literature. ICT can be considered a generalization of the skip-gram objective (Mikolov et al., 2013), with a coarser granularity, deep architecture, and in-batch negative sampling from Logeswaran and Lee"
P19-1612,D13-1160,0,0.941992,"rs are observed during training, and evidence retrieval is learned in a completely end-to-end manner. ing the marginal log-likelihood of correct answers that were found. We evaluate ORQA on open versions of five existing QA datasets. On datasets where the question writers already know the answer—SQuAD (Rajpurkar et al., 2016) and TriviaQA (Joshi et al., 2017)—the retrieval problem resembles traditional IR, and BM25 (Robertson et al., 2009) provides state-of-the-art retrieval. On datasets where question writers do not know the answer— Natural Questions (Kwiatkowski et al., 2019), WebQuestions (Berant et al., 2013), and CuratedTrec (Baudis and Sediv´y, 2015)—we show that learned retrieval is crucial, providing improvements of 6 to 19 points in exact match over BM25. 2 During inference, the model outputs the answer string of the highest scoring derivation: a∗ = TEXT(argmax S(b, s, q)) Task In open domain question answering, the input q is a question string, and the output a is an answer string. Unlike reading comprehension, the source of evidence is a modeling choice rather than a part of the task definition. We compare the assumptions made by variants of reading comprehension and question answering task"
P19-1612,P17-1171,0,0.122006,"val is crucial, outperforming BM25 by up to 19 points in exact match. 1 Introduction Due to recent advances in reading comprehension systems, there has been a revival of interest in open domain question answering (QA), where the evidence must be retrieved from an open corpus, rather than being given as input. This presents a more realistic scenario for practical applications. Current approaches require a blackbox information retrieval (IR) system to do much of the heavy lifting, even though it cannot be fine-tuned on the downstream task. In the strongly supervised setting popularized by DrQA (Chen et al., 2017), they also assume a reading comprehension model trained on question-answer-evidence triples, such as SQuAD (Rajpurkar et al., 2016). The IR system is used at test time to generate evidence candidates in place of the gold evidence. In the weakly supervised setting, proposed by TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), and Quasar (Dhingra et al., 2017), the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence. These approaches rely on the IR system to massively reduce the search space and/or reduce spurious ambiguity. However"
P19-1612,W10-2903,1,0.667621,"recisely representing extremely specific concepts that sparse representations can cleanly separate. These errors indicate that a hybrid approach would be promising future work. 10 Related Work Recent progress has been made towards improving evidence retrieval (Wang et al., 2018; Kratzwald and Feuerriegel, 2018; Lee et al., 2018; Das et al., 2019) by learning to aggregate from multiple retrieval steps. They re-rank evidence candidates from a closed set, and we aim to integrate these complementary approaches in future work. Our approach is also reminiscent of weakly supervised semantic parsing (Clarke et al., 2010; Liang et al., 2013; Artzi and Zettlemoyer, 2013; Fader et al., 2014; Berant et al., 2013; Kwiatkowski et al., 2013), with which we share similar challenges—(1) inference and learning are tightly coupled, (2) latent derivations must be discovered, and (3) strong inductive biases are needed to find positive learning signal while avoiding spurious ambiguities. While we motivate ICT from first principles as an unsupervised proxy for evidence retrieval, it is closely related to existing representation learning literature. ICT can be considered a generalization of the skip-gram objective (Mikolov"
P19-1612,D18-1053,0,0.517125,"by definition under-specified and require more language understanding, since users are explicitly looking for unknown information. Instead of being subject to the recall ceiling from blackbox IR systems, we should directly learn to retrieve using question-answering data. In this work, we introduce the first OpenRetrieval Question Answering system (ORQA). ORQA learns to retrieve evidence from an open corpus, and is supervised only by questionanswer string pairs. While recent work on improving evidence retrieval has made significant progress (Wang et al., 2018; Kratzwald and Feuerriegel, 2018; Lee et al., 2018; Das et al., 2019), they still only rerank a closed evidence set. The main challenge to fully end-to-end learning is that retrieval over the open corpus must be considered a latent variable that would be impractical to train from scratch. IR systems offer a reasonable but potentially suboptimal starting point. The key insight of this work is that end-toend learning is possible if we pre-train the retriever with an unsupervised Inverse Cloze Task (ICT). In ICT, a sentence is treated as a pseudoquestion, and its context is treated as pseudoevidence. Given a pseudo-question, ICT requires selecti"
P19-1612,J13-2005,0,0.0275901,"extremely specific concepts that sparse representations can cleanly separate. These errors indicate that a hybrid approach would be promising future work. 10 Related Work Recent progress has been made towards improving evidence retrieval (Wang et al., 2018; Kratzwald and Feuerriegel, 2018; Lee et al., 2018; Das et al., 2019) by learning to aggregate from multiple retrieval steps. They re-rank evidence candidates from a closed set, and we aim to integrate these complementary approaches in future work. Our approach is also reminiscent of weakly supervised semantic parsing (Clarke et al., 2010; Liang et al., 2013; Artzi and Zettlemoyer, 2013; Fader et al., 2014; Berant et al., 2013; Kwiatkowski et al., 2013), with which we share similar challenges—(1) inference and learning are tightly coupled, (2) latent derivations must be discovered, and (3) strong inductive biases are needed to find positive learning signal while avoiding spurious ambiguities. While we motivate ICT from first principles as an unsupervised proxy for evidence retrieval, it is closely related to existing representation learning literature. ICT can be considered a generalization of the skip-gram objective (Mikolov et al., 2013), with"
P19-1612,D16-1261,0,0.0229849,"be discovered, and (3) strong inductive biases are needed to find positive learning signal while avoiding spurious ambiguities. While we motivate ICT from first principles as an unsupervised proxy for evidence retrieval, it is closely related to existing representation learning literature. ICT can be considered a generalization of the skip-gram objective (Mikolov et al., 2013), with a coarser granularity, deep architecture, and in-batch negative sampling from Logeswaran and Lee (2018). Consulting external evidence sources with latent retrieval has also been explored in information extraction (Narasimhan et al., 2016). In comparison, we are able to learn a much more expressive retriever due to the strong inductive biases from ICT pre-training. 11 Conclusion We presented ORQA, the first open domain question answering system where the retriever and reader are jointly learned end-to-end using only question-answer pairs and without any IR system. This is made possible by pre-training the retriever using an Inverse Cloze Task (ICT). Experiments show that learning to retrieve is crucial when the questions reflect an information need, i.e. the question writers do not already know the answer. Acknowledgements We t"
P19-1612,N18-1202,1,0.417885,"ementation is based on Lucene.3 Language Models While unsupervised neural retrieval is notoriously difficult to improve over traditional IR (Lin, 2019), we include them as baselines for comparison. We experiment with unsupervised pooled representations from neural language models (LM), which has been shown to be state-of-the-art unsupervised representations (Perone et al., 2018). We compare with two widely-used 128-dimensional representations: (1) NNLM, context-independent embeddings from a feed-forward LMs (Bengio et al., 2003),4 and (2) ELM O (small), a context-dependent bidirectional LSTM (Peters et al., 2018).5 As with ICT, we use the alternate encoders to pre-compute the encoded evidence blocks hb and to initialize the question encoding hq , which is fine-tuned. Based on existing IR literature and the intuition that LMs do not explicitly optimize for retrieval, we do not expect these to be strong baselines, but they demonstrate the difficulty of encoding blocks of text into 128 dimensions. 8.2 Results The main results are show in Table 5. The first result to note is that BM25 is a powerful retrieval system. Word matching is important, and 3 https://lucene.apache.org/ https://tfhub.dev/google/nnlm"
P19-1612,D16-1264,0,0.80349,"nsion systems, there has been a revival of interest in open domain question answering (QA), where the evidence must be retrieved from an open corpus, rather than being given as input. This presents a more realistic scenario for practical applications. Current approaches require a blackbox information retrieval (IR) system to do much of the heavy lifting, even though it cannot be fine-tuned on the downstream task. In the strongly supervised setting popularized by DrQA (Chen et al., 2017), they also assume a reading comprehension model trained on question-answer-evidence triples, such as SQuAD (Rajpurkar et al., 2016). The IR system is used at test time to generate evidence candidates in place of the gold evidence. In the weakly supervised setting, proposed by TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), and Quasar (Dhingra et al., 2017), the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence. These approaches rely on the IR system to massively reduce the search space and/or reduce spurious ambiguity. However, QA is fundamentally different from IR (Singh, 2012). Whereas IR is concerned with lexical and semantic matching, questions are by"
P19-1612,D12-1116,0,0.0330258,"n model trained on question-answer-evidence triples, such as SQuAD (Rajpurkar et al., 2016). The IR system is used at test time to generate evidence candidates in place of the gold evidence. In the weakly supervised setting, proposed by TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), and Quasar (Dhingra et al., 2017), the dependency on strong supervision is removed by assuming that the IR system provides noisy gold evidence. These approaches rely on the IR system to massively reduce the search space and/or reduce spurious ambiguity. However, QA is fundamentally different from IR (Singh, 2012). Whereas IR is concerned with lexical and semantic matching, questions are by definition under-specified and require more language understanding, since users are explicitly looking for unknown information. Instead of being subject to the recall ceiling from blackbox IR systems, we should directly learn to retrieve using question-answering data. In this work, we introduce the first OpenRetrieval Question Answering system (ORQA). ORQA learns to retrieve evidence from an open corpus, and is supervised only by questionanswer string pairs. While recent work on improving evidence retrieval has made"
P19-1612,N19-4013,0,0.226803,"Missing"
P19-1612,P11-1060,0,\N,Missing
P19-1612,Q19-1026,1,\N,Missing
Q17-1008,P98-1013,0,0.0391779,"2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure. Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer. N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998). Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker et al., 1998) style are also instances of n-ary relation extraction, with extraction of events expressed in a single sentence. McDonald et al. (2005) extract n-ary relations in a biomedical domain, by first factoring the n-ary relation into pair-wise relations between all entity pairs, and then constructing maximal cliques of related entities. Recently, neural models have been applied to semantic role labeling (FitzGerald et al., 2015; Roth 111 and Lapata, 2016). These works learned neural representations by effectively decomposing the n-ary relation into binary relations between the predicate and each arg"
Q17-1008,H05-1091,0,0.217196,"rst review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of"
Q17-1008,P16-1072,0,0.0800421,"g powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure. Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer. N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998). Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker et al., 1998) styl"
Q17-1008,C10-1018,0,0.0365625,"cularly beneficial. 7 Related Work Most work on relation extraction has been applied to binary relations of entities in a single sentence. We first review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity"
Q17-1008,M98-1001,0,0.238877,"-21 was noted in 10. NEXTSENT All patients were treated with gefitinib and showed a partial response. DET AUXPASS NSUBJPASS ROOT AMOD DET DOBJ PREP WITH CONJ AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory netwo"
Q17-1008,de-marneffe-etal-2006-generating,0,0.120922,"Missing"
Q17-1008,D15-1112,0,0.0433277,"Missing"
Q17-1008,P10-1160,0,0.211478,"AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory networks (graph LSTMs). By adopting the graph formulation, our framework subsumes prior approaches based on chain or tree LSTMs, and can incorporate a rich set of linguis"
Q17-1008,P05-1053,0,0.36783,"tic parses, suggesting that encoding high-quality analysis is particularly beneficial. 7 Related Work Most work on relation extraction has been applied to binary relations of entities in a single sentence. We first review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue)"
Q17-1008,W09-1401,0,0.0236307,"to the n-ary setting is challenging, as there are n2 paths. One apparent solution is inspired by Davidsonian semantics: first, identify a single trigger phrase that signifies the whole relation, then reduce the n-ary relation to n binary relations between the trigger and an argument. However, challenges remain. It is often hard to specify a single trigger, as the relation is manifested by several words, often not contiguous. Moreover, it is expensive and time-consuming to annotate training examples, especially if triggers are required, as is evident in prior annotation efforts such as GENIA (Kim et al., 2009). The realistic and widely adopted paradigm is to leverage indirect supervision, such as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), where triggers are not available. Additionally, lexical and syntactic patterns signifying the relation will be sparse. To handle such sparsity, traditional feature-based approaches require extensive engineering and large data. Unfortunately, this challenge becomes much more severe in crosssentence extraction when the text spans multiple sentences. To overcome these challenges, we explore a general relation extraction framework based on gra"
Q17-1008,J13-4004,0,0.0121401,"ected acyclic graphs (bottom); the graph LSTMs is constructed by a forward pass (Left to Right) followed by a backward pass (Right to Left). Note that information goes from dependency child to parent. 3.1 Document Graph To model various dependencies from linguistic analysis at our disposal, we follow Quirk and Poon (2017) and introduce a document graph to capture intra- and inter-sentential dependencies. A document graph consists of nodes that represent words and edges that represent various dependencies such as linear context (adjacent words), syntactic dependencies, and discourse relations (Lee et al., 2013; Xue et al., 2015). Figure 1 shows the document graph for our running example; this instance suggests that tumors with L858E mutation in EGFR gene responds to the drug gefitinib. This document graph acts as the backbone upon which a graph LSTM is constructed. If it con104 3.2 Backpropagation in Graph LSTMs Conventional LSTMs are essentially very deep feedforward neural networks. For example, a left-to-right linear LSTM has one hidden vector for each word. This vector is generated by a neural network (recurrent unit) that takes as input the embedding of the given word and the hidden vector of"
Q17-1008,P14-5010,0,0.00700447,"ody of biomedical papers is exactly the challenge. As we will see in later subsections, distant supervision enables us to generate a sizable training set from a small number of manually curated facts, and the learned model was able to extract orders of magnitude more facts. In future work, we will explore incorporating more known facts for distant supervision and extracting from more full-text articles. We conducted tokenization, part-of-speech tagging, and syntactic parsing using SPLAT (Quirk et al., 2012), and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP (Manning et al., 2014). We used the entity taggers from Literome (Poon et al., 2014) to identify drug, gene and mutation mentions. We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) and the Clinical Interpretations of Variants In Cancer (CIVIC) knowledge base6 for distant supervision. The knowledge bases distinguish fine-grained interaction types, which we do not use in this paper. 5 6 Distant Supervision After identifying drug, gene and mutation mentions in the text, co-occurring triples with known interactions were chosen as positive examples. However, unlike the single-sentence setting in"
Q17-1008,P05-1061,0,0.489844,"tation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10. NEXTSENT All patients were treated with gefitinib and showed a partial response. DET AUXPASS NSUBJPASS ROOT AMOD DET DOBJ PREP WITH CONJ AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework"
Q17-1008,P09-1113,0,0.740799,"phrase that signifies the whole relation, then reduce the n-ary relation to n binary relations between the trigger and an argument. However, challenges remain. It is often hard to specify a single trigger, as the relation is manifested by several words, often not contiguous. Moreover, it is expensive and time-consuming to annotate training examples, especially if triggers are required, as is evident in prior annotation efforts such as GENIA (Kim et al., 2009). The realistic and widely adopted paradigm is to leverage indirect supervision, such as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), where triggers are not available. Additionally, lexical and syntactic patterns signifying the relation will be sparse. To handle such sparsity, traditional feature-based approaches require extensive engineering and large data. Unfortunately, this challenge becomes much more severe in crosssentence extraction when the text spans multiple sentences. To overcome these challenges, we explore a general relation extraction framework based on graph LSTMs. By learning a continuous representation for words and entities, LSTMs can handle sparsity effectively without requiring intense feature engineeri"
Q17-1008,P16-1105,0,0.26527,"sing a series of gates (input, forget and output) to avoid amplifying or suppressing gradients during backpropagation. Consequently, LSTMs are much more effective in capturing long-distance dependencies, and have been applied to a variety of NLP tasks. However, most approaches are based on linear chains and only explicitly model the linear context, which ignores a variety of linguistic analyses, such as syntactic and discourse dependencies. In this section, we propose a general framework that generalizes LSTMs to graphs. While there is some prior work on learning tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), to the best of our knowledge, graph LSTMs have not been applied to any NLP task yet. Figure 2 shows the architecture of this approach. The input layer is the word embedding of input text. Next is the graph LSTM which learns a contextual representation for each word. For the entities in question, their contextual representations are concatenated and become the input to the relation classifiers. For a multi-word entity, we simply used the average of its word representations and leave the exploration of more sophisticated aggregation approaches to future work. The layers are trained jointly wit"
Q17-1008,P14-2012,0,0.0332938,"Related Work Most work on relation extraction has been applied to binary relations of entities in a single sentence. We first review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such repre"
Q17-1008,J05-1004,0,0.184752,"P DEP The deletion mutation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10. NEXTSENT All patients were treated with gefitinib and showed a partial response. DET AUXPASS NSUBJPASS ROOT AMOD DET DOBJ PREP WITH CONJ AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we expl"
Q17-1008,P16-2025,1,0.847562,"dge-type embedding. Edge-Type Embedding To reduce the number of parameters and leverage potential correlation among fine-grained edge types, we learned a lowdimensional embedding of the edge types, and conducted an outer product of the predecessor’s hidden vector and the edge-type embedding to generate a “typed hidden representation”, which is a matrix. The new computation is as follows: 3.5 X Multi-task Learning with Sub-relations Multi-task learning has been shown to be beneficial in training neural networks (Caruana, 1998; Collobert ftj = σ(Wf xt + Uf ×T (hj ⊗ ej ) + bf ) and Weston, 2008; Peng and Dredze, 2016). By X learning contextual entity representations, our frameUo ×T (hj ⊗ ej ) + bo ) ot = σ(Wo xt + j∈P (t) work makes it straightforward to conduct multi-task X Uc ×T (hj ⊗ ej ) + bc ) learning. The only change is to add a separate classic˜t = tanh(Wc xt + j∈P (t) X fier for each related auxiliary relation. All classifiers ftj cj ct = it c˜t + share the same graph LSTMs representation learner j∈P (t) and word embeddings, and can potentially help each ht = ot tanh(ct ) other by pooling their supervision signals. U ’s are now l × l × d tensors (l is the dimension of In the molecular tumor board"
Q17-1008,D14-1162,0,0.0882041,"Missing"
Q17-1008,C08-1088,0,0.0226594,"n the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolu"
Q17-1008,E17-1110,1,0.202831,"icity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory networks (graph LSTMs). By adopting the graph formulation, our framework subsumes prior approaches based on chain or tree LSTMs, and can incorporate a rich set of linguistic analyses to aid relation extraction. Relation classification takes as input the entity representations learned from the entire text, and can be easily extended for arbitrary re"
Q17-1008,N12-3006,1,0.827666,"f papers contain knowledge about drug-gene-mutation interactions. Extracting such knowledge from the vast body of biomedical papers is exactly the challenge. As we will see in later subsections, distant supervision enables us to generate a sizable training set from a small number of manually curated facts, and the learned model was able to extract orders of magnitude more facts. In future work, we will explore incorporating more known facts for distant supervision and extracting from more full-text articles. We conducted tokenization, part-of-speech tagging, and syntactic parsing using SPLAT (Quirk et al., 2012), and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP (Manning et al., 2014). We used the entity taggers from Literome (Poon et al., 2014) to identify drug, gene and mutation mentions. We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) and the Clinical Interpretations of Variants In Cancer (CIVIC) knowledge base6 for distant supervision. The knowledge bases distinguish fine-grained interaction types, which we do not use in this paper. 5 6 Distant Supervision After identifying drug, gene and mutation mentions in the text, co-occurring trip"
Q17-1008,reschke-etal-2014-event,0,0.0164171,"ent, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions. Recently, cross-sentence relation extraction models have been learned with distant supervision, and used integrated contextual evidence of diverse types without reliance on these assumptions (Quirk and Poon, 2017), but that work focused on binary relations only and explicitly engineered sparse indicator features. Relation extraction using distant supervision Distant supervision has been applied to extraction of binary (Mintz et al., 2009; Poon et al., 2015) and n-ary (Reschke et al., 2014; Li et al., 2015) relations, traditionally using hand-engineered features. Neural architectures have recently been applied to distantly supervised extraction of binary relations (Zeng et al., 2015). Our work is the first to propose a neural architecture for n-ary relation extraction, where the representation of a tuple of entities is not decomposable into independent representations of the individual entities or entity pairs, and which integrates diverse information from multi-sentential context. To utilize training data more effectively, we show how multitask learning for component binary su"
Q17-1008,P16-1113,0,0.055469,"Missing"
Q17-1008,P15-1061,0,0.43501,"iLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-directional LSTM (BiLSTM). Following Wang et al. (2016), we used input attention for the CNN and a input window size of 5. Quirk and Poon (2017) only extracted binary relations. We extended it to ternary relations by deriving features for each entity pair (with added annotation to signify the two entity types), and pooling the features 108 from all pairs. For binary relation extraction, prior syntax-aware approaches are directly applicable. So we also compared with a state-of-the-art tree LSTM system (Miwa and Bansal, 2016) and a BiLSTM on the shortest dependency"
Q17-1008,D12-1110,0,0.158707,"automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure. Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer. N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998). Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker"
Q17-1008,R11-1004,0,0.156856,"gument, by embedding the dependency path between each pair, or by combining features of the two using a feed-forward network. Although some re-ranking or joint inference models have been employed, the representations of the individual arguments do not influence each other. In contrast, we propose a neural architecture that jointly represents n entity mentions, taking into account long-distance dependencies and inter-sentential information. Cross-sentence relation extraction Several relation extraction tasks have benefited from crosssentence extraction, including MUC fact and event extraction (Swampillai and Stevenson, 2011), record extraction from web pages (Wick et al., 2006), extraction of facts for biomedical domains (Yoshikawa et al., 2011), and extensions of semantic role labeling to cover implicit inter-sentential arguments (Gerber and Chai, 2010). These prior works have either relied on explicit co-reference annotation, or on the assumption that the whole document refers to a single coherent event, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions. Recently, cross-sentence relation extraction models have been learned with distant super"
Q17-1008,P15-1150,0,0.811543,"hese problems by using a series of gates (input, forget and output) to avoid amplifying or suppressing gradients during backpropagation. Consequently, LSTMs are much more effective in capturing long-distance dependencies, and have been applied to a variety of NLP tasks. However, most approaches are based on linear chains and only explicitly model the linear context, which ignores a variety of linguistic analyses, such as syntactic and discourse dependencies. In this section, we propose a general framework that generalizes LSTMs to graphs. While there is some prior work on learning tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), to the best of our knowledge, graph LSTMs have not been applied to any NLP task yet. Figure 2 shows the architecture of this approach. The input layer is the word embedding of input text. Next is the graph LSTM which learns a contextual representation for each word. For the entities in question, their contextual representations are concatenated and become the input to the relation classifiers. For a multi-word entity, we simply used the average of its word representations and leave the exploration of more sophisticated aggregation approaches to future work. The layers"
Q17-1008,P16-1123,0,0.436558,"Missing"
Q17-1008,W06-1671,0,0.152345,"ombining features of the two using a feed-forward network. Although some re-ranking or joint inference models have been employed, the representations of the individual arguments do not influence each other. In contrast, we propose a neural architecture that jointly represents n entity mentions, taking into account long-distance dependencies and inter-sentential information. Cross-sentence relation extraction Several relation extraction tasks have benefited from crosssentence extraction, including MUC fact and event extraction (Swampillai and Stevenson, 2011), record extraction from web pages (Wick et al., 2006), extraction of facts for biomedical domains (Yoshikawa et al., 2011), and extensions of semantic role labeling to cover implicit inter-sentential arguments (Gerber and Chai, 2010). These prior works have either relied on explicit co-reference annotation, or on the assumption that the whole document refers to a single coherent event, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions. Recently, cross-sentence relation extraction models have been learned with distant supervision, and used integrated contextual evidence of div"
Q17-1008,D15-1062,0,0.540872,"able 1: Average test accuracy in five-fold crossvalidation for drug-gene-mutation ternary interactions. Feature-Based used the best performing model in (Quirk and Poon, 2017) with features derived from shortest paths between all entity pairs. Model Single-Sent. Cross-Sent. Feature-Based 73.9 75.2 CNN BiLSTM BiLSTM-Shortest-Path Tree LSTM Graph LSTM-EMBED Graph LSTM-FULL 73.0 73.9 70.2 75.9 74.3 75.6 74.9 76.0 71.7 75.9 76.5 76.7 Table 2: Average test accuracy in five-fold crossvalidation for drug-mutation binary relations, with an extra baseline using a BiLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-dire"
Q17-1008,D15-1206,0,0.709967,"able 1: Average test accuracy in five-fold crossvalidation for drug-gene-mutation ternary interactions. Feature-Based used the best performing model in (Quirk and Poon, 2017) with features derived from shortest paths between all entity pairs. Model Single-Sent. Cross-Sent. Feature-Based 73.9 75.2 CNN BiLSTM BiLSTM-Shortest-Path Tree LSTM Graph LSTM-EMBED Graph LSTM-FULL 73.0 73.9 70.2 75.9 74.3 75.6 74.9 76.0 71.7 75.9 76.5 76.7 Table 2: Average test accuracy in five-fold crossvalidation for drug-mutation binary relations, with an extra baseline using a BiLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-dire"
Q17-1008,C16-1138,0,0.0397931,"egrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchi"
Q17-1008,K15-2001,0,0.021367,"hs (bottom); the graph LSTMs is constructed by a forward pass (Left to Right) followed by a backward pass (Right to Left). Note that information goes from dependency child to parent. 3.1 Document Graph To model various dependencies from linguistic analysis at our disposal, we follow Quirk and Poon (2017) and introduce a document graph to capture intra- and inter-sentential dependencies. A document graph consists of nodes that represent words and edges that represent various dependencies such as linear context (adjacent words), syntactic dependencies, and discourse relations (Lee et al., 2013; Xue et al., 2015). Figure 1 shows the document graph for our running example; this instance suggests that tumors with L858E mutation in EGFR gene responds to the drug gefitinib. This document graph acts as the backbone upon which a graph LSTM is constructed. If it con104 3.2 Backpropagation in Graph LSTMs Conventional LSTMs are essentially very deep feedforward neural networks. For example, a left-to-right linear LSTM has one hidden vector for each word. This vector is generated by a neural network (recurrent unit) that takes as input the embedding of the given word and the hidden vector of the previous word."
Q17-1008,C14-1220,0,0.860338,"baseline using a BiLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-directional LSTM (BiLSTM). Following Wang et al. (2016), we used input attention for the CNN and a input window size of 5. Quirk and Poon (2017) only extracted binary relations. We extended it to ternary relations by deriving features for each entity pair (with added annotation to signify the two entity types), and pooling the features 108 from all pairs. For binary relation extraction, prior syntax-aware approaches are directly applicable. So we also compared with a state-of-the-art tree LSTM system (Miwa and Bansal, 2016) and a BiLSTM on th"
Q17-1008,D15-1203,0,0.237949,"h distant supervision, and used integrated contextual evidence of diverse types without reliance on these assumptions (Quirk and Poon, 2017), but that work focused on binary relations only and explicitly engineered sparse indicator features. Relation extraction using distant supervision Distant supervision has been applied to extraction of binary (Mintz et al., 2009; Poon et al., 2015) and n-ary (Reschke et al., 2014; Li et al., 2015) relations, traditionally using hand-engineered features. Neural architectures have recently been applied to distantly supervised extraction of binary relations (Zeng et al., 2015). Our work is the first to propose a neural architecture for n-ary relation extraction, where the representation of a tuple of entities is not decomposable into independent representations of the individual entities or entity pairs, and which integrates diverse information from multi-sentential context. To utilize training data more effectively, we show how multitask learning for component binary sub-relations can improve performance. Our learned representation combines information sources within a single sentence in a more integrated and generalizable fashion than prior approaches, and can al"
Q17-1008,Y15-1009,0,0.0475333,"fully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on mo"
Q17-1008,C98-1013,0,\N,Missing
Q19-1026,D15-1075,0,0.0508562,"ent set also appear in the training set, we implement two ‘‘copying’’ baselines. The first of these simply selects the most frequent annotation applied to a given page in the training set. The second selects the annotation given to the training set question closest to the eval set question according to TFIDF weighted word overlap. These three baselines are reported as First paragraph, Most frequent, and Closest question in Table 3, respectively. 6.3 Custom Pipeline (DecAtt + DocReader) One view of the long answer selection task is that it is more closely related to natural language inference (Bowman et al., 2015; Williams et al., 2018) than short answer extraction. A valid long answer must contain all of the information required to infer the answer. Short answers do not need to contain this information—they need to be surrounded by it. Motivated by this intuition, we implement a pipelined approach that uses a model drawn from the natural language interference literature to select long answers. Then short answers are selected from these using a model drawn from the short answer extraction literature. 6.2 Document-QA We adapt the reference implementation12 of Document-QA (Clark and Gardner, 2018) for t"
Q19-1026,P17-1171,0,0.275889,"Missing"
Q19-1026,D18-1241,0,0.0611339,"choice of supporting facts is somewhat subjective. They set high human upper bounds by selecting, for each example, the score maximizing partition of four annotations into one prediction and three references. The reference labels chosen by this maximization are not representative of the reference labels in HotpotQA’s evaluation set, and it is not clear that the upper bounds are achievable. A more robust approach is to keep the evaluation distribution fixed, and calculate an acheivable upper bound by approximating the expectation over annotations—as we have done for NQ in Section 5. The QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018) data sets contain dialogues between a questioner, who is trying to learn about a text, and an answerer. QuAC also prevents the questioner from seeing the evidence text. Conversational QA is an exciting new area, but it is significantly different from the single turn QA task in NQ. In both QuAC and CoQA, conversations tend to explore evidence texts incrementally, progressing from the start to the end of the text. 3 Task Definition and Data Collection Natural Questions contains (question, wikipedia page, long answer, short answer) quadruples where: the question see"
Q19-1026,P18-1078,0,0.217148,"(P), recall (R), and the harmonic mean of these (F1) of all baselines, a single annotator, and the super-annotator upper bound. The human performances marked with † are evaluated on a sample of five annotations from the 25-way annotated data introduced in Section 5. To address (ii), we tried adding special NULL passages to represent the lack of answer. However, we achieved better performance by training on the subset of questions with answers and then only predicting those answers whose scores exceed a threshold. With these two modifications, we are able to apply Document-QA to NQ. We follow Clark and Gardner (2018) in pruning documents down to the set of passages that have highest TFIDF similarity with the question. Under this approach, we consider the top 16 passages as long answers. We consider short answers containing up to 17 words. We train Document-QA for 30 epochs with batches containing 15 examples. The post hoc score threshold is set to 3.0. All of these values were chosen on the basis of development set performance. 6.1 Untrained Baselines NQ’s long answer selection task admits several untrained baselines. The first paragraph of a Wikipedia page commonly acts as a summary of the most important"
Q19-1026,D17-1082,0,0.109935,"annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should n"
Q19-1026,W18-2605,0,0.0359237,"ions. MS Marco contains 100,000 questions with freeform answers. For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages. Free-form text answers allow more flexibility in providing abstractive answers, but lead to difficulties in evaluation (BLEU score [Papineni et al., 2002] is used). MS Marco’s authors do not discuss issues of variability or report quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and ans"
Q19-1026,D18-1260,0,0.0463566,"variability or report quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the ins"
Q19-1026,C92-2082,0,0.0914034,"elines and tooling divide the annotation task into three conceptual stages, where all three stages are completed by a single annotator in succession. The decision flow through these is illustrated in Figure 2 and the instructions given to annotators are summarized below. Question Identification: Contributors determine whether the given question is good or bad. A good question is a fact-seeking question that can be answered with an entity or explanation. A bad question is ambigous, incomprehensible, 3 We pre-define the set of categorical noun phrases used in 4 and 5 by running Hearst patterns (Hearst, 1992) to find a broad set of hypernyms. Part of speech tags and entities are identified using Google’s Cloud NLP API: https://cloud. google.com/natural-language. 456 2) k -way annotations (with k = 25) on a subset of the data. Post hoc evaluation of non-null answers leads directly to a measure of annotation precision. As is common in information-retrieval style problems such as long-answer identification, measuring recall is more challenging. However, we describe how 25-way annotated data provide useful insights into recall, particularly when combined with expert judgments. dependent on clear false"
Q19-1026,D16-1241,0,0.0190295,"nts to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should not change the answer, and SQuAD 2.0 introduces questions that are designed to be unanswerable. However, we argue that questions written to be unanswerable can be identified as such with little reasoning, in contrast to NQ’s task of deciding whether a pa"
Q19-1026,P16-1144,0,0.031038,"to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should not change the answer, and SQuAD 2.0 introduces questions that are designed to be unanswerable. However, we argue that questions written to be unanswerable can be identified as such with little reasoning, in contrast to NQ’s task of"
Q19-1026,D17-1215,0,0.0458559,"questions from the query stream. Thus the questions are ‘‘natural’’ in that they represent real queries from people seeking information. 2 Related Work The SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Kocisky et al., 2018), and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer. The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as test data. We justify the use of 5-way annotation for evaluation in Section 5. 454 This contrasts with NQ, where individual questions often require reasoning over large bodies of text. The WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine. WikiQA contains only 3,047 questions. MS Marco contains 100,000 questions with free"
Q19-1026,P17-1147,0,0.26169,"ssues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sentences that should not change the answer, and SQuAD 2.0 introduc"
Q19-1026,P02-1040,0,0.108627,"asoning over large bodies of text. The WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine. WikiQA contains only 3,047 questions. MS Marco contains 100,000 questions with freeform answers. For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages. Free-form text answers allow more flexibility in providing abstractive answers, but lead to difficulties in evaluation (BLEU score [Papineni et al., 2002] is used). MS Marco’s authors do not discuss issues of variability or report quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading co"
Q19-1026,N18-1101,0,0.0325746,"n the training set, we implement two ‘‘copying’’ baselines. The first of these simply selects the most frequent annotation applied to a given page in the training set. The second selects the annotation given to the training set question closest to the eval set question according to TFIDF weighted word overlap. These three baselines are reported as First paragraph, Most frequent, and Closest question in Table 3, respectively. 6.3 Custom Pipeline (DecAtt + DocReader) One view of the long answer selection task is that it is more closely related to natural language inference (Bowman et al., 2015; Williams et al., 2018) than short answer extraction. A valid long answer must contain all of the information required to infer the answer. Short answers do not need to contain this information—they need to be surrounded by it. Motivated by this intuition, we implement a pipelined approach that uses a model drawn from the natural language interference literature to select long answers. Then short answers are selected from these using a model drawn from the short answer extraction literature. 6.2 Document-QA We adapt the reference implementation12 of Document-QA (Clark and Gardner, 2018) for the NQ task. This system"
Q19-1026,P18-2124,0,0.322524,"answer (s) can be a span or set of spans (typically entities) within l that answer the question, a boolean yes or no answer, or NULL. If l = NULL then s = NULL, necessarily. Figure 1 shows examples. Natural Questions has the following properties: Source of questions The questions consist of real anonymized, aggregated queries issued to the Google search engine. Simple heuristics are used to filter questions from the query stream. Thus the questions are ‘‘natural’’ in that they represent real queries from people seeking information. 2 Related Work The SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Kocisky et al., 2018), and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer. The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as"
Q19-1026,D16-1264,0,0.822388,"chine translation, speech recognition, and image recognition. One major factor in these successes has been the development of neural methods that far exceed the performance of previous approaches. A second major factor has been the existence of large quantities of training data for these systems. Open-domain question answering (QA) is a benchmark task in natural language understanding (NLU), which has significant utility to users, and in addition is potentially a challenge task that can drive the development of methods for NLU. Several pieces of recent work have introduced QA data sets (e.g., Rajpurkar et al., 2016; Reddy et al., 2018). However, in contrast to tasks where it is relatively easy to gather naturally occurring examples,1 the definition of a suitable QA task, and the development of a methodology for annotation and evaluation, is challenging. Key issues include the methods and sources used to obtain questions; the methods used to annotate and collect answers; the methods used to measure and ensure annotation quality; and the metrics used for evaluation. For more discussion of the limitations of previous work with respect to these issues, see Section 2 of this paper. This paper introduces Natu"
Q19-1026,D15-1237,0,0.10531,"D data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as test data. We justify the use of 5-way annotation for evaluation in Section 5. 454 This contrasts with NQ, where individual questions often require reasoning over large bodies of text. The WikiQA (Yang et al., 2015) and MS Marco (Nguyen et al., 2016) data sets contain queries sampled from the Bing search engine. WikiQA contains only 3,047 questions. MS Marco contains 100,000 questions with freeform answers. For each question, the annotator is presented with 10 passages returned by the search engine, and is asked to generate an answer to the query, or to say that the answer is not contained within the passages. Free-form text answers allow more flexibility in providing abstractive answers, but lead to difficulties in evaluation (BLEU score [Papineni et al., 2002] is used). MS Marco’s authors do not discus"
Q19-1026,D18-1259,0,0.167869,"answer the question, a boolean yes or no answer, or NULL. If l = NULL then s = NULL, necessarily. Figure 1 shows examples. Natural Questions has the following properties: Source of questions The questions consist of real anonymized, aggregated queries issued to the Google search engine. Simple heuristics are used to filter questions from the query stream. Thus the questions are ‘‘natural’’ in that they represent real queries from people seeking information. 2 Related Work The SQuAD (Rajpurkar et al., 2016), SQuAD 2.0 (Rajpurkar et al., 2018), NarrativeQA (Kocisky et al., 2018), and HotpotQA (Yang et al., 2018) data sets contain questions and answers written by annotators who have first read a short text containing the answer. The SQuAD data sets contain questions/paragraph/answer triples from Wikipedia. In the original SQuAD data set, annotators often borrow part of the evidence paragraph to create a question. Jia and Liang (2017) Number of items The public release contains 307,373 training examples with single annotations, 7,830 examples with 5-way annotations for development data, and 7,842 5-way annotated items sequestered as test data. We justify the use of 5-way annotation for evaluation in Se"
Q19-1026,D13-1020,0,0.136104,"quality metrics for their annotations. From our experience, these issues are critical. DuReader (He et al., 2018) is a Chinese language data set containing queries from Baidu search logs. Like NQ, DuReader contains real user queries; it requires systems to read entire documents to find answers; and it identifies acceptable variability in answers. However, as with MS Marco, DuReader is reliant on BLEU for answer scoring, and systems already outperform a humans according to this metric. There are a number of reading comprehension benchmarks based on multiple choice tests (Mihaylov et al., 2018; Richardson et al., 2013; Lai et al., 2017). The TriviaQA data set (Joshi et al., 2017) contains questions and answers taken from trivia quizzes found online. A number of Clozestyle tasks have also been proposed (Hermann et al., 2015; Hill et al., 2015; Paperno et al., 2016; Onishi et al., 2016). We believe that all of these tasks are related to, but distinct from, answering information-seeking questions. We also believe that, because a solution to NQ will have genuine utility, it is better equipped as a benchmark for NLU. showed that systems trained on SQuAD could be easily fooled by the insertion of distractor sent"
Q19-1026,D16-1244,1,\N,Missing
Q19-1026,Q18-1023,0,\N,Missing
S01-1021,J96-1002,0,0.00227623,"Missing"
S01-1021,W96-0208,0,0.0412505,"Missing"
W02-0811,J96-1002,0,0.00125762,"each (classifier, chosen sense, correct sense) triple. However, most senses are rarely chosen and rarely correct, so most features had zero or singleton support. f i (s, s1 , . . . , sk ) = 1 ⇐⇒ s = si v(s) = P i λi δ(si = s) The indicators δ are true for exactly one sense, and correspond to the simple f i defined above.4 The sense with the largest vote v(s) will be the sense with the highest posterior probability P(s|s 1 , . . . sk ) and will be chosen. For the maximum entropy classifier, we estimate the weights by maximizing the likelihood of a heldout set, using the standard IIS algorithm (Berger et al., 1996). For both weighted schemes, we found that stopping the iterative procedures before convergence gave better results. IIS was halted after 50 rounds, while EM was halted after a single round. Both methods were initialized to uniform starting weights. More importantly than changing the exact weight estimates, moving from method to method triggers broad qualitative changes in what kind of weights are allowed. With majority voting, classifiers all have equal, positive weights. With weighted voting, the weights are no longer required to be equal, but are still non-negative. With maximum entropy wei"
W02-0811,W96-0208,0,0.00775637,"scoring teams’ systems, the combination achieves high performance. We discuss trade-offs and empirical performance. Finally, we present an analysis of the combination, examining how ensemble performance depends on error independence and task difficulty. 1 Introduction The problem of supervised word sense disambiguation (WSD) has been approached using many different classification algorithms, including naive-Bayes, decision trees, decision lists, and memory-based learners. While it is unquestionable that certain algorithms are better suited to the WSD problem than others (for a comparison, see Mooney (1996)), it seems that, given similar input features, various algorithms exhibit roughly similar accuracies. 1 This was supported by the S ENSEVAL -2 results, where a This paper is based on work supported in part by the National Science Foundation under Grants IIS-0085896 and IIS9982226, by an NSF Graduate Fellowship, and by the Research Collaboration between NTT Communication Science Laboratories, Nippon Telegraph and Telephone Corporation and CSLI, Stanford University. 1 In fact, we have observed that differences between implementations of a single classifier type, such as smoothing or window size"
W02-0811,S01-1040,0,\N,Missing
W02-0811,W00-1304,0,\N,Missing
W02-0811,W02-1005,0,\N,Missing
W02-0811,P00-1027,0,\N,Missing
W02-0811,P00-1035,0,\N,Missing
W02-0811,N01-1006,0,\N,Missing
W02-0811,J01-3001,0,\N,Missing
W02-0811,J95-4004,0,\N,Missing
W02-0811,P98-1081,0,\N,Missing
W02-0811,C98-1078,0,\N,Missing
W02-0811,W02-1004,0,\N,Missing
W02-0811,P98-1029,0,\N,Missing
W02-0811,C98-1029,0,\N,Missing
W02-0811,A00-2009,0,\N,Missing
W02-1012,1995.tmi-1.18,0,0.0499084,"Missing"
W02-1012,J00-2004,0,0.0876766,"Missing"
W02-1012,C00-2163,0,0.235451,"Missing"
W02-1012,P00-1056,0,0.359957,"Missing"
W02-1012,W99-0604,0,0.112323,"Missing"
W02-1012,C96-2141,0,0.842545,"Missing"
W02-1012,P01-1067,0,0.170331,"Missing"
W02-1012,J93-2003,0,\N,Missing
W02-2030,J99-2004,0,0.0349187,"n the present experiments we have not explored this fully. The nodes in the derivation trees represent combining rule schemas of the HPSG grammar, and not IMPER HCOMP HCOMP BSE_VERB_INFL BSE_VERB_INFL US LET_V1 us SEE_V3 see let Figure 1: Derivation tree for the sentence Let us see phrasal categories of the standard sort. The whole HPSG analyses can be recreated from the derivation trees, using the grammar. The preterminals of the derivation trees are lexical labels. These are much finer grained than Penn Treebank preterminals tags, and more akin to those used in TreeAdjoining Grammar models (Bangalore and Joshi, 1999). There are a total of about 8, 000 lexical labels occurring in the treebank. One might conjecture that a supertagging approach could go a long way toward parse disambiguation. However, an upper bound for such an approach for our corpus is below 55 percent parse selection accuracy, which is the accuracy of an oracle tagger that chooses at random among the parses having the correct tag sequence (Oepen et al., 2002). The semantic dependency trees are labelled with relations most of which correspond to words in the sentence. These labels provide some abstraction because some classes of words have"
W02-2030,P97-1003,0,0.487196,"e use PCFG models to select features for log linear models. Even though this method may be expected to be suboptimal, it proves to be useful. We select features for PCFGs using decision trees and use the same features in a conditional log linear model. We compare the performance of the two models using equivalent features. Our PCFG models are comparable to branching process models for parsing the Penn Treebank, in which the next state of the model depends on a history of features. In most recent parsing work the history consists of a small number of manually selected features (Charniak, 1997; Collins, 1997). Other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the best predictors for each situation. Decision trees have been applied for feature selection for statistical parsing models by Magerman (1995) and Haruno et al. (1998). Another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very rich feature set"
W02-2030,P01-1019,0,0.0436747,"lickinger, 2000). The current preliminary version contains 10,000 sentences of spoken dialog material drawn from the Verbmobil project. The Redwoods treebank makes available the entire HPSG signs for sentence analyses, but we have used in our experiments only small subsets of this representation. These are (i) derivation trees composed of identifiers of lexical items and constructions used to build the analysis, and (ii) semantic dependency trees which encode semantic head-tohead relations. The Redwoods treebank provides deeper semantics expressed in the Minimum Recursion Semantics formalism (Copestake et al., 2001), but in the present experiments we have not explored this fully. The nodes in the derivation trees represent combining rule schemas of the HPSG grammar, and not IMPER HCOMP HCOMP BSE_VERB_INFL BSE_VERB_INFL US LET_V1 us SEE_V3 see let Figure 1: Derivation tree for the sentence Let us see phrasal categories of the standard sort. The whole HPSG analyses can be recreated from the derivation trees, using the grammar. The preterminals of the derivation trees are lexical labels. These are much finer grained than Penn Treebank preterminals tags, and more akin to those used in TreeAdjoining Grammar m"
W02-2030,P98-1083,0,0.0152973,"s models for parsing the Penn Treebank, in which the next state of the model depends on a history of features. In most recent parsing work the history consists of a small number of manually selected features (Charniak, 1997; Collins, 1997). Other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the best predictors for each situation. Decision trees have been applied for feature selection for statistical parsing models by Magerman (1995) and Haruno et al. (1998). Another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very rich feature set (Hermjakob and Mooney, 1997). Our experiments in feature selection using decision trees suggest that single decision trees may not be able to make optimal use of a large number of relevant features. This may be due to the greedy search procedures or to the fact that trees combine information from different features only through partitioning of the space. For example they have di"
W02-2030,P97-1062,0,0.0230583,"Other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the best predictors for each situation. Decision trees have been applied for feature selection for statistical parsing models by Magerman (1995) and Haruno et al. (1998). Another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very rich feature set (Hermjakob and Mooney, 1997). Our experiments in feature selection using decision trees suggest that single decision trees may not be able to make optimal use of a large number of relevant features. This may be due to the greedy search procedures or to the fact that trees combine information from different features only through partitioning of the space. For example they have difficulty in weighing evidence from different features without fully partitioning the space. A common approach to overcoming some of the problems with decision trees – such as reducing their variance or increasing their representational power – has"
W02-2030,P99-1069,0,0.561774,"en et al., 2002). HPSG (Head-driven Phrase Structure Grammar) is a modern constraintbased lexicalist (unification) grammar, described in Pollard and Sag (1994). The Redwoods treebank makes available syntactic and semantic analyses of much greater depth than, for example, the Penn Treebank. Therefore there are a large number of features available that could be used by stochastic models for disambiguation. Other researchers have worked on extracting features useful for disambiguation from unification grammar analyses and have built log linear models a.k.a. Stochastic Unification Based Grammars (Johnson et al., 1999; Riezler et al., 2000). Here we also use log linear models to estimate conditional probabilities of sentence analyses. Since feature selection is almost prohibitive for these models, because of high computational costs, we use PCFG models to select features for log linear models. Even though this method may be expected to be suboptimal, it proves to be useful. We select features for PCFGs using decision trees and use the same features in a conditional log linear model. We compare the performance of the two models using equivalent features. Our PCFG models are comparable to branching process m"
W02-2030,J98-4004,0,0.100644,"Missing"
W02-2030,P95-1037,0,0.0890931,"to branching process models for parsing the Penn Treebank, in which the next state of the model depends on a history of features. In most recent parsing work the history consists of a small number of manually selected features (Charniak, 1997; Collins, 1997). Other researchers have proposed automatically selecting the conditioning information for various states of the model, thus potentially increasing greatly the space of possible features and selectively choosing the best predictors for each situation. Decision trees have been applied for feature selection for statistical parsing models by Magerman (1995) and Haruno et al. (1998). Another example of automatic feature selection for parsing is in the context of a deterministic parsing model that chooses parse actions based on automatically induced decision structures over a very rich feature set (Hermjakob and Mooney, 1997). Our experiments in feature selection using decision trees suggest that single decision trees may not be able to make optimal use of a large number of relevant features. This may be due to the greedy search procedures or to the fact that trees combine information from different features only through partitioning of the space."
W02-2030,C02-2025,1,0.896002,"learned PCFG grammars and log linear models over the same features. 1 Introduction Hand-built NLP grammars frequently have a depth of linguistic representation and constraints not present in current treebanks, giving them potential importance for tasks requiring deeper processing. On the other hand, these manually built grammars need to solve the disambiguation problem to be practically usable. This paper presents work on the problem of probabilistic parse selection from among a set of alternatives licensed by a hand-built grammar in the context of the newly developed Redwoods HPSG treebank (Oepen et al., 2002). HPSG (Head-driven Phrase Structure Grammar) is a modern constraintbased lexicalist (unification) grammar, described in Pollard and Sag (1994). The Redwoods treebank makes available syntactic and semantic analyses of much greater depth than, for example, the Penn Treebank. Therefore there are a large number of features available that could be used by stochastic models for disambiguation. Other researchers have worked on extracting features useful for disambiguation from unification grammar analyses and have built log linear models a.k.a. Stochastic Unification Based Grammars (Johnson et al.,"
W02-2030,P00-1061,0,0.0767778,"(Head-driven Phrase Structure Grammar) is a modern constraintbased lexicalist (unification) grammar, described in Pollard and Sag (1994). The Redwoods treebank makes available syntactic and semantic analyses of much greater depth than, for example, the Penn Treebank. Therefore there are a large number of features available that could be used by stochastic models for disambiguation. Other researchers have worked on extracting features useful for disambiguation from unification grammar analyses and have built log linear models a.k.a. Stochastic Unification Based Grammars (Johnson et al., 1999; Riezler et al., 2000). Here we also use log linear models to estimate conditional probabilities of sentence analyses. Since feature selection is almost prohibitive for these models, because of high computational costs, we use PCFG models to select features for log linear models. Even though this method may be expected to be suboptimal, it proves to be useful. We select features for PCFGs using decision trees and use the same features in a conditional log linear model. We compare the performance of the two models using equivalent features. Our PCFG models are comparable to branching process models for parsing the P"
W02-2030,C98-1080,0,\N,Missing
W04-3222,A00-2018,0,0.388938,"s to devise a kernel function measures the similarity between inputs and . In addition to achieving efficient computation in high dimensional representation spaces, the use of kernels allows for an alternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking    models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuk"
W04-3222,P97-1003,0,0.182217,"rnel function measures the similarity between inputs and . In addition to achieving efficient computation in high dimensional representation spaces, the use of kernels allows for an alternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking    models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 2003)."
W04-3222,W01-1802,0,0.0505021,"Missing"
W04-3222,C02-2025,1,0.832357,"IMPER verb HCOMPverb HCOMP verb HCOMP verb HCOMP verb HCOMP verb HCOMP verb HCOMP verb LET V1 PLAN ON V2 HCOMP prep* let (v sorb) plan (v e p itrs) ON HCOMPverb LET V1 US let us HCOMP verb PLAN ON V2 plan HCOMP prep* ON THAT DEIX on that Figure 1: Derivation tree for the sentence Let us plan on that. paths (strings) allows us to explore string kernels on these paths and combine them into tree kernels. We apply these ideas in the context of parse disambiguation for sentence analyses produced by a Head-driven Phrase Structure Grammar (HPSG), the grammar formalism underlying the Redwoods corpus (Oepen et al., 2002). HPSG is a modern constraint-based lexicalist (or “unification”) grammar formalism.1 We build discriminative models using Support Vector Machines for ranking (Joachims, 1999). We compare our proposed representation to previous approaches and show that it leads to substantial improvements in accuracy. 2 The Leaf Projection Paths View of Parse Trees 2.1 Representing HPSG Signs In HPSG, sentence analyses are given in the form of HPSG signs, which are large feature structures containing information about syntactic and semantic properties of the phrases. As in some of the previous work on the Redw"
W04-3222,N04-1012,0,0.0559444,"Missing"
W04-3222,P00-1061,0,0.0341975,"lternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking    models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 2003). Many other interesting kernels have been devised for sequences and trees, with application to sequence classification and parsing. A good overview of kernels for structured data can be foun"
W04-3222,W03-0402,0,0.0182056,"class of machine learning algorithms, such an explicit representation is not necessary, and which it suffices to devise a kernel function measures the similarity between inputs and . In addition to achieving efficient computation in high dimensional representation spaces, the use of kernels allows for an alternative view on the modelling problem as defining a similarity between inputs rather than a set of relevant features. In previous work on discriminative natural language parsing, one approach has been to define features centered around lexicalized local rules in the trees (Collins, 2000; Shen and Joshi, 2003), similar to the features of the best performing lexicalized generative parsing models (Charniak, 2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking    models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees"
W04-3222,P03-1005,0,0.0165955,"2000; Collins, 1997). Additionally non-local features have been defined measuring e.g. parallelism and complexity of phrases in discriminative log-linear parse ranking    models (Riezler et al., 2000). Another approach has been to define tree kernels: for example, in (Collins and Duffy, 2001), the allsubtrees representation of parse trees (Bod, 1998) is effectively utilized by the application of a fast dynamic programming algorithm for computing the number of common subtrees of two trees. Another tree kernel, more broadly applicable to Hierarchical Directed Graphs, was proposed in (Suzuki et al., 2003). Many other interesting kernels have been devised for sequences and trees, with application to sequence classification and parsing. A good overview of kernels for structured data can be found in (Gaertner et al., 2002). Here we propose a new representation of parse trees which (i) allows the localization of broader useful context, (ii) paves the way for exploring kernels, and (iii) achieves superior disambiguation accuracy compared to models that use tree representations centered around context-free rules. Compared to the usual notion of discriminative models (placing classes on rich observed"
W04-3222,W02-2030,1,0.94334,"exicalist (or “unification”) grammar formalism.1 We build discriminative models using Support Vector Machines for ranking (Joachims, 1999). We compare our proposed representation to previous approaches and show that it leads to substantial improvements in accuracy. 2 The Leaf Projection Paths View of Parse Trees 2.1 Representing HPSG Signs In HPSG, sentence analyses are given in the form of HPSG signs, which are large feature structures containing information about syntactic and semantic properties of the phrases. As in some of the previous work on the Redwoods corpus (Toutanova et al., 2002; Toutanova and Manning, 2002), we use the derivation trees as the main representation for disambiguation. Derivation trees record the combining rule schemas of the HPSG grammar which were used to license the sign by combining initial lexical types. The derivation tree is also the fundamental data stored in the Redwoods treebank, since the full sign can be reconstructed from it by reference to the grammar. The internal nodes represent, for example, head-complement, head-specifier, and head-adjunct schemas, which were used to license larger signs out of component parts. A derivation tree for the 1 For an introduction to HPS"
W05-0623,W04-2412,0,0.287637,"Missing"
W05-0623,A00-2018,0,0.0651367,"Missing"
W05-0623,J02-3001,0,0.294789,"rn input to a joint model which can Our local model labels nodes in a parse tree independently. We decompose the probability over labels (all argument labels plus NONE), into a product of the probability over ARG and NONE, and a probability over argument labels given that a node is an ARG. This can be seen as chaining an identification and a classification model. The identification model classifies each phrase as either an argument or nonargument and our classification model labels each potential argument with a specific argument label. The two models use the same features. Previous research (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Carreras and M`arquez, 2004) has identified many useful features for local identification and classification. Below we list the features and hand-picked conjunctions of features used in our local models. The ones denoted with asterisks (*) were not present in (Toutanova et al., 2005). Although most of these features have been described in previous work, some features, described in the next section, are – to our knowledge – novel. 173 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 173–176, Ann Arbor, June 2005. 2005 Associat"
W05-0623,N04-1030,0,0.13848,"hich can Our local model labels nodes in a parse tree independently. We decompose the probability over labels (all argument labels plus NONE), into a product of the probability over ARG and NONE, and a probability over argument labels given that a node is an ARG. This can be seen as chaining an identification and a classification model. The identification model classifies each phrase as either an argument or nonargument and our classification model labels each potential argument with a specific argument label. The two models use the same features. Previous research (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Carreras and M`arquez, 2004) has identified many useful features for local identification and classification. Below we list the features and hand-picked conjunctions of features used in our local models. The ones denoted with asterisks (*) were not present in (Toutanova et al., 2005). Although most of these features have been described in previous work, some features, described in the next section, are – to our knowledge – novel. 173 Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL), c pages 173–176, Ann Arbor, June 2005. 2005 Association for Computational"
W05-0623,P05-1073,1,0.899946,"Missing"
W06-1668,W05-0620,0,0.02819,"Missing"
W06-1668,P05-1022,0,0.0251739,"en the generative models have a good model structure, they can perform quite well. Introduction Discriminative models have become the models of choice for NLP tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). In this paper, we look differently at comparing generative and discriminative models. We ask the question: given the same set of input features, what is the best a generative model can do if it is allowed to learn an optimal structure for the joint distribution, and what is the best a discriminative model can do if it is also allowed to learn an optimal structure. That is, we do not impose any independence assumptions on the generative or discriminative models and let them learn the best representation of the data they can. The superiority of discriminative models has been shown on many task"
W06-1668,A00-2018,0,0.0129515,"a learning a more complex structure, it can achieve very similar or better performance than a corresponding discriminative model. In addition, as structure learning for generative models is far more efficient, they may be preferable for some tasks. 1 Generative models may often perform poorly due to making strong independence assumptions about the joint distribution of features and classes. To avoid this problem, generative models for NLP tasks have often been manually designed to achieve an appropriate representation of the joint distribution, such as in the parsing models of (Collins, 1997; Charniak, 2000). This shows that when the generative models have a good model structure, they can perform quite well. Introduction Discriminative models have become the models of choice for NLP tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Pen"
W06-1668,W95-0103,0,0.0492145,"We did not preform search for the hyper-parameter σ when evaluating models. We fit σ by optimizing the development set accuracy after a model was selected. Note that our feature selection algorithm adds an input variable or a variable conjunction with all of its possible values in a single step of the search. Therefore we are adding hundreds or thousands of binary features at each step, as opposed to only one as in (Della Pietra et al., 1997). This is why we can afford to perform complete re-estimation of the parameters of the model at each step. 3 Training 20,801 173,514 parkhi et al., 1994; Collins and Brooks, 1995). It is extracted from the the Penn Treebank Wall Street Journal data (Ratnaparkhi et al., 1994). Table 1 shows summary statistics for the dataset. The second task we concentrate on is semantic role labeling in the context of PropBank (Palmer et al., 2005). The PropBank corpus annotates phrases which fill semantic roles for verbs on top of Penn Treebank parse trees. The annotated roles specify agent, patient, direction, etc. The labels for semantic roles are grouped into two groups, core argument labels and modifier argument labels, which correspond approximately to the traditional distinction"
W06-1668,P97-1003,0,0.434906,"assumptions via learning a more complex structure, it can achieve very similar or better performance than a corresponding discriminative model. In addition, as structure learning for generative models is far more efficient, they may be preferable for some tasks. 1 Generative models may often perform poorly due to making strong independence assumptions about the joint distribution of features and classes. To avoid this problem, generative models for NLP tasks have often been manually designed to achieve an appropriate representation of the joint distribution, such as in the parsing models of (Collins, 1997; Charniak, 2000). This shows that when the generative models have a good model structure, they can perform quite well. Introduction Discriminative models have become the models of choice for NLP tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al"
W06-1668,J02-3001,0,0.0795664,"et. The second task we concentrate on is semantic role labeling in the context of PropBank (Palmer et al., 2005). The PropBank corpus annotates phrases which fill semantic roles for verbs on top of Penn Treebank parse trees. The annotated roles specify agent, patient, direction, etc. The labels for semantic roles are grouped into two groups, core argument labels and modifier argument labels, which correspond approximately to the traditional distinction between arguments and adjuncts. There has been plenty of work on machine learning models for semantic role labeling, starting with the work of Gildea and Jurafsky (2002), and including CoNLL shared tasks (Carreras and M`arquez, 2005). The most successful formulation has been as learning to classify nodes in a syntactic parse tree. The possible labels are NONE, meaning that the corresponding phrase has no semantic role and the set of core and modifier labels. We concentrate on the subproblem of classification for core argument nodes. The problem is, given that a node has a core argument label, decide what the correct label is. Other researchers have also looked at this subproblem (Gildea and Jurafsky, 2002; Toutanova et al., 2005; Pradhan et al., 2005a; Xue an"
W06-1668,J93-1005,0,0.0161114,"no semantic role and the set of core and modifier labels. We concentrate on the subproblem of classification for core argument nodes. The problem is, given that a node has a core argument label, decide what the correct label is. Other researchers have also looked at this subproblem (Gildea and Jurafsky, 2002; Toutanova et al., 2005; Pradhan et al., 2005a; Xue and Palmer, 2004). Experiments 3.1 Problems and Datasets We study two classification problems – prepositional phrase (PP) attachment, and semantic role labeling. Following most of the literature on prepositional phrase attachment (e.g., (Hindle and Rooth, 1993; Collins and Brooks, 1995; Vanschoenwinkel and Manderick, 2003)), we focus on the most common configuration that leads to ambiguities: V NP PP. Here, we are given a verb phrase with a following noun phrase and a prepositional phrase. The goal is to determine if the PP should be attached to the verb or to the object noun phrase. For example, in the sentence: Never [hang]V [a painting]NP [with a peg]PP , the prepositional phrase with a peg can either modify the verb hang or the object noun phrase a painting. Here, clearly, with a peg modifies the verb hang. Many features have been proposed for"
W06-1668,P01-1042,0,0.0274662,"s the log-linear model is not significantly better than the Bayes Net models. We can see that the generative model was able to learn a structure with a set of independence assumptions which are not as strong as the ones the Naive Bayes model makes, thus resulting in a model with performance competitive with the discriminative model. 4 Comparison to Related Work Previous work has compared generative and discriminative models having the same structure, such as the Naive Bayes and Logistic regression models (Ng and Jordan, 2002; Klein and Manning, 2002) and other models (Klein and Manning, 2002; Johnson, 2001). Figures 2(a) and 2(b) show the Bayesian Networks learned for PP Attachment and Semantic Role Labeling. Table 7 shows the conjunctions 3 This result is on an older version of Propbank from July 2002. 582 maximizing conditional likelihood), as representatives of discriminative models. We study more general log-linear models, equivalent to Markov Random Fields. Our models are more general in that their parameters do not need to be interpretable as probabilities (sum to 1 and between 0 and 1), and the structures do not need to correspond to Bayes Net structures. For discriminative classifiers, i"
W06-1668,W02-1002,0,0.141886,". We ask the question: given the same set of input features, what is the best a generative model can do if it is allowed to learn an optimal structure for the joint distribution, and what is the best a discriminative model can do if it is also allowed to learn an optimal structure. That is, we do not impose any independence assumptions on the generative or discriminative models and let them learn the best representation of the data they can. The superiority of discriminative models has been shown on many tasks when the discriminative and generative models use exactly the same model structure (Klein and Manning, 2002). However, the advantage of the discriminative modStructure learning is very efficient for generative models in the form of directed graphical models (Bayesian Networks (Pearl, 1988)), since the optimal parameters for such models can be estimated in closed form. We compare Bayesian Net576 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 576–584, c Sydney, July 2006. 2006 Association for Computational Linguistics works with structure learning to their closely related discriminative counterpart – conditional loglinear models with structur"
W06-1668,P02-1038,0,0.0714475,"a Microsoft Research Redmond, WA kristout@microsoft.com Abstract els can be very slight (Johnson, 2001) and for small training set sizes generative models can be better because they need fewer training samples to converge to the optimal parameter setting (Ng and Jordan, 2002). Additionally, many discriminative models use a generative model as a base model and add discriminative features with reranking (Collins, 2000; Charniak and Johnson, 2005; Roark et al., 2004), or train discriminatively a small set of weights for features which are generatively estimated probabilities (Raina et al., 2004; Och and Ney, 2002). Therefore it is important to study generative models and to find ways of making them better even when they are used only as components of discriminative models. In this paper we show that generative models are competitive with and sometimes superior to discriminative models, when both kinds of models are allowed to learn structures that are optimal for discrimination. In particular, we compare Bayesian Networks and Conditional loglinear models on two NLP tasks. We observe that when the structure of the generative model encodes very strong independence assumptions (a la Naive Bayes), a discri"
W06-1668,J05-1004,0,0.0135686,"s possible values in a single step of the search. Therefore we are adding hundreds or thousands of binary features at each step, as opposed to only one as in (Della Pietra et al., 1997). This is why we can afford to perform complete re-estimation of the parameters of the model at each step. 3 Training 20,801 173,514 parkhi et al., 1994; Collins and Brooks, 1995). It is extracted from the the Penn Treebank Wall Street Journal data (Ratnaparkhi et al., 1994). Table 1 shows summary statistics for the dataset. The second task we concentrate on is semantic role labeling in the context of PropBank (Palmer et al., 2005). The PropBank corpus annotates phrases which fill semantic roles for verbs on top of Penn Treebank parse trees. The annotated roles specify agent, patient, direction, etc. The labels for semantic roles are grouped into two groups, core argument labels and modifier argument labels, which correspond approximately to the traditional distinction between arguments and adjuncts. There has been plenty of work on machine learning models for semantic role labeling, starting with the work of Gildea and Jurafsky (2002), and including CoNLL shared tasks (Carreras and M`arquez, 2005). The most successful"
W06-1668,P05-1072,0,0.0806966,"Collins, 1997; Charniak, 2000). This shows that when the generative models have a good model structure, they can perform quite well. Introduction Discriminative models have become the models of choice for NLP tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). In this paper, we look differently at comparing generative and discriminative models. We ask the question: given the same set of input features, what is the best a generative model can do if it is allowed to learn an optimal structure for the joint distribution, and what is the best a discriminative model can do if it is also allowed to learn an optimal structure. That is, we do not impose any independence assumptions on the generative or discriminative models and let them learn the best representation of the data they can. The superio"
W06-1668,W05-0639,0,0.0120248,"n the parsing models of (Collins, 1997; Charniak, 2000). This shows that when the generative models have a good model structure, they can perform quite well. Introduction Discriminative models have become the models of choice for NLP tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). In this paper, we look differently at comparing generative and discriminative models. We ask the question: given the same set of input features, what is the best a generative model can do if it is allowed to learn an optimal structure for the joint distribution, and what is the best a discriminative model can do if it is also allowed to learn an optimal structure. That is, we do not impose any independence assumptions on the generative or discriminative models and let them learn the best representation of the data"
W06-1668,H94-1048,0,0.0216103,"ng the development set accuracy after a model was selected. Note that our feature selection algorithm adds an input variable or a variable conjunction with all of its possible values in a single step of the search. Therefore we are adding hundreds or thousands of binary features at each step, as opposed to only one as in (Della Pietra et al., 1997). This is why we can afford to perform complete re-estimation of the parameters of the model at each step. 3 Training 20,801 173,514 parkhi et al., 1994; Collins and Brooks, 1995). It is extracted from the the Penn Treebank Wall Street Journal data (Ratnaparkhi et al., 1994). Table 1 shows summary statistics for the dataset. The second task we concentrate on is semantic role labeling in the context of PropBank (Palmer et al., 2005). The PropBank corpus annotates phrases which fill semantic roles for verbs on top of Penn Treebank parse trees. The annotated roles specify agent, patient, direction, etc. The labels for semantic roles are grouped into two groups, core argument labels and modifier argument labels, which correspond approximately to the traditional distinction between arguments and adjuncts. There has been plenty of work on machine learning models for se"
W06-1668,P04-1007,0,0.0532355,"Missing"
W06-1668,N03-1033,1,0.0321548,"presentation of the joint distribution, such as in the parsing models of (Collins, 1997; Charniak, 2000). This shows that when the generative models have a good model structure, they can perform quite well. Introduction Discriminative models have become the models of choice for NLP tasks, because of their ability to easily incorporate non-independent features and to more directly optimize classification accuracy. State of the art models for many NLP tasks are either fully discriminative or trained using discriminative reranking (Collins, 2000). These include models for part-of-speech tagging (Toutanova et al., 2003), semantic-role labeling (Punyakanok et al., 2005; Pradhan et al., 2005b) and Penn Treebank parsing (Charniak and Johnson, 2005). In this paper, we look differently at comparing generative and discriminative models. We ask the question: given the same set of input features, what is the best a generative model can do if it is allowed to learn an optimal structure for the joint distribution, and what is the best a discriminative model can do if it is also allowed to learn an optimal structure. That is, we do not impose any independence assumptions on the generative or discriminative models and l"
W06-1668,P05-1073,1,0.855331,"starting with the work of Gildea and Jurafsky (2002), and including CoNLL shared tasks (Carreras and M`arquez, 2005). The most successful formulation has been as learning to classify nodes in a syntactic parse tree. The possible labels are NONE, meaning that the corresponding phrase has no semantic role and the set of core and modifier labels. We concentrate on the subproblem of classification for core argument nodes. The problem is, given that a node has a core argument label, decide what the correct label is. Other researchers have also looked at this subproblem (Gildea and Jurafsky, 2002; Toutanova et al., 2005; Pradhan et al., 2005a; Xue and Palmer, 2004). Experiments 3.1 Problems and Datasets We study two classification problems – prepositional phrase (PP) attachment, and semantic role labeling. Following most of the literature on prepositional phrase attachment (e.g., (Hindle and Rooth, 1993; Collins and Brooks, 1995; Vanschoenwinkel and Manderick, 2003)), we focus on the most common configuration that leads to ambiguities: V NP PP. Here, we are given a verb phrase with a following noun phrase and a prepositional phrase. The goal is to determine if the PP should be attached to the verb or to the"
W06-1668,W04-3212,0,\N,Missing
W06-3124,P02-1040,0,0.0950288,"ent source and target trees; s and t represent source and target treelets (connected subgraphs of the dependency tree). The expression ∀t∈ T refers to all the lexical items in the target language tree T and |T| refers to the count of lexical items in T. We use subscripts to indicate selected words: Tn represents th the n lexical item in an in-order traversal of T. 2.1. Training We use the broad coverage dependency parser NLPWIN [3] to obtain source language dependency trees, and we use GIZA++ [4] to produce word alignments. The GIZA++ training regimen and parameters are tuned to optimize BLEU [5] scores on held-out data. Using the word alignments, we follow a set of dependency tree projection heuristics [1] to construct target dependency trees, producing a word-aligned parallel dependency tree corpus. Treelet translation pairs are extracted by enumerating all source treelets (to a maximum size) aligned to a target treelet. 2.2. Decoding We use a tree-based decoder, inspired by dynamic programming. It searches for an approximation of 158 Proceedings of the Workshop on Statistical Machine Translation, pages 158–161, c New York City, June 2006. 2006 Association for Computational Linguist"
W06-3124,2004.tmi-1.14,1,0.8501,"r> m4 = <the / el> m5 = <Rio / Rio> m6 = <agenda / programa> m7 = <NULL / de> We can then predict the probability of each MTU in the context of (a) the previous MTUs in source order, (b) the previous MTUs in target order, or (c) the ancestor MTUs in the tree. We consider all of these traversal orders, each acting as a separate feature function in the log linear combination. For source and target traversal order we use a trigram model, and a bigram model for tree order. 2.3.3. Target language models We use both a surface level trigram language model and a dependency-based bigram language model [7], similar to the bilexical dependency modes used in some English Treebank parsers (e.g. [8]). ∣T∣ P surf T = ∏ P trisurf T i∣T i−2 , T i−1  i=1 ∣T∣ P bilex T =∏ P bidep T i∣ parent T i  i=1 Ptrisurf is a Kneser-Ney smoothed trigram language model trained on the target side of the training corpus, and Pbilex is a Kneser-Ney smoothed bigram language model trained on target language dependencies extracted from the aligned parallel dependency tree corpus. 2.3.4. Order model The order model assigns a probability to the position (pos) of each target node relative to its head based on infor"
W06-3124,P97-1003,0,0.0512225,"redict the probability of each MTU in the context of (a) the previous MTUs in source order, (b) the previous MTUs in target order, or (c) the ancestor MTUs in the tree. We consider all of these traversal orders, each acting as a separate feature function in the log linear combination. For source and target traversal order we use a trigram model, and a bigram model for tree order. 2.3.3. Target language models We use both a surface level trigram language model and a dependency-based bigram language model [7], similar to the bilexical dependency modes used in some English Treebank parsers (e.g. [8]). ∣T∣ P surf T = ∏ P trisurf T i∣T i−2 , T i−1  i=1 ∣T∣ P bilex T =∏ P bidep T i∣ parent T i  i=1 Ptrisurf is a Kneser-Ney smoothed trigram language model trained on the target side of the training corpus, and Pbilex is a Kneser-Ney smoothed bigram language model trained on target language dependencies extracted from the aligned parallel dependency tree corpus. 2.3.4. Order model The order model assigns a probability to the position (pos) of each target node relative to its head based on information in both the source and target trees: P order  order T ∣S ,T = ∏ P  pos  t , pa"
W06-3124,P05-1034,1,0.922582,"n the source dependency tree. These models are combined with several other knowledge sources in a log-linear manner. The weights of the individual components in the loglinear model are set by an automatic parametertuning method. We give a brief overview of the components of the system and discuss our experience with the Europarl data translating from English to Spanish. 1. Introduction The dependency treelet translation system developed at MSR is a statistical MT system that takes advantage of linguistic tools, namely a source language dependency parser, as well as a word alignment component. [1] To train a translation system, we require a sentence-aligned parallel corpus. First the source side is parsed to obtain dependency trees. Next the corpus is word-aligned, and the source dependencies are projected onto the target sentences using the word alignments. From the aligned dependency corpus we extract all treelet translation pairs, and train an order model and a bi-lexical dependency model. To translate, we parse the input sentence, and employ a decoder to find a combination and ordering of treelet translation pairs that cover the source tree and are optimal according to a set of mod"
W06-3124,P02-1038,0,0.0323972,"ain dependency trees. Next the corpus is word-aligned, and the source dependencies are projected onto the target sentences using the word alignments. From the aligned dependency corpus we extract all treelet translation pairs, and train an order model and a bi-lexical dependency model. To translate, we parse the input sentence, and employ a decoder to find a combination and ordering of treelet translation pairs that cover the source tree and are optimal according to a set of models. In a now-common generalization of the classic noisy-channel framework, we use a loglinear combination of models [2], as in below: translation S , F ,Λ=argmax T {∑ f ∈F λ f  S ,T  f } Such an approach toward translation scoring has proven very effective in practice, as it allows a translation system to incorporate information from a variety of probabilistic or non-probabilistic sources. The weights Λ = { λf } are selected by discriminatively training against held out data. 2. System Details A brief word on notation: s and t represent source and target lexical nodes; S and T represent source and target trees; s and t represent source and target treelets (connected subgraphs of the dependency tree). The e"
W06-3124,J03-1002,0,0.00362208,"em Details A brief word on notation: s and t represent source and target lexical nodes; S and T represent source and target trees; s and t represent source and target treelets (connected subgraphs of the dependency tree). The expression ∀t∈ T refers to all the lexical items in the target language tree T and |T| refers to the count of lexical items in T. We use subscripts to indicate selected words: Tn represents th the n lexical item in an in-order traversal of T. 2.1. Training We use the broad coverage dependency parser NLPWIN [3] to obtain source language dependency trees, and we use GIZA++ [4] to produce word alignments. The GIZA++ training regimen and parameters are tuned to optimize BLEU [5] scores on held-out data. Using the word alignments, we follow a set of dependency tree projection heuristics [1] to construct target dependency trees, producing a word-aligned parallel dependency tree corpus. Treelet translation pairs are extracted by enumerating all source treelets (to a maximum size) aligned to a target treelet. 2.2. Decoding We use a tree-based decoder, inspired by dynamic programming. It searches for an approximation of 158 Proceedings of the Workshop on Statistical Machi"
W06-3124,N04-1021,0,0.0445594,"Missing"
W06-3124,2004.iwslt-evaluation.13,0,0.0239717,"Missing"
W06-3124,P03-1021,0,0.0398057,"Missing"
W06-3124,N06-1002,1,0.775668,"a number of theoretical problems, such as the ad hoc estimation of phrasal probability, the failure to model the partition probability, and the tenuous connection between the phrases and the underlying word-based alignment model. In string-based SMT systems, these problems are outweighed by the key role played by phrases in capturing “local” order. In the absence of good global ordering models, this has led to an 159 inexorable push towards longer and longer phrases, resulting in serious practical problems of scale, without, in the end, obviating the need for a real global ordering story. In [13] we discuss these issues in greater detail and also present our approach to this problem. Briefly, we take as our basic unit the Minimal Translation Unit (MTU) which we define as a set of source and target word pairs such that there are no word alignment links between distinct MTUs, and no smaller MTUs can be extracted without violating the previous constraint. In other words, these are the minimal non-compositional phrases. We then build models based on n-grams of MTUs in source string, target string and source dependency tree order. These bilingual n-gram models in combination with our globa"
W06-3124,J93-2003,0,\N,Missing
W11-0329,N09-1003,0,0.0187707,"able to efficiently handle a large number of training examples in the highdimensional space. Evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient. 1 Introduction Measures of text similarity have many applications and have been studied extensively in both the NLP and IR communities. For example, a combination of corpus and knowledge based methods have been invented for judging word similarity (Lin, 1998; Agirre et al., 2009). Similarity derived from a largescale Web corpus has been used for automatically extending lists of typed entities (Vyas and Pantel, 2009). Judging the degree of similarity between documents is also fundamental to classical IR problems such as document retrieval (Manning et al., 2008). In all these applications, the vector-based similarity method is the most widely used. Term vectors are first constructed to represent the original text objects, where each term is associated with However, the main weakness of this term-vector representation is that different but semantically related terms are"
W11-0329,W05-0601,0,0.0226363,"eval recall by leveraging Wikipedia. In a companion paper, we also demonstrated that various topic mod254 els including S2Net can enhance the ranking function (Gao et al., 2011). For text categorization, similarity between terms is often encoded as kernel functions embedded in the learning algorithms, and thus increase the classification accuracy. Representative approaches include latent semantic kernels (Cristianini et al., 2002), which learns an LSA-based kernel function from a document collection, and work that computes term-similarity based on the linguistic knowledge provided by WordNet (Basili et al., 2005; Bloehdorn and Moschitti, 2007). 6 Conclusions In this paper, we presented S2Net, a discriminative approach for learning a projection matrix that maps raw term-vectors to a low-dimensional space. Our learning method directly optimizes the model so that the cosine score of the projected vectors can become a reliable similarity measure. The strength of this model design has been shown empirically in two very different tasks. For cross-lingual document retrieval, S2Net significantly outperforms OPCA, which is the best prior approach. For ad selection and filtering, S2Net also outperforms all met"
W11-0329,P98-1069,0,0.0171388,"sed in the experiments. 4 Experiments We compare S2Net experimentally with existing approaches on two very different tasks: cross-lingual document retrieval and ad relevance measures. 4.1 Comparable Document Retrieval With the growth of multiple languages on the Web, there is an increasing demand of processing crosslingual documents. For instance, machine translation (MT) systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the Web (Munteanu and Marcu, 2005). Word-level translation lexicons can also be learned from comparable documents (Fung and Yee, 1998; Rapp, 1999). In this cross-lingual document retrieval task, given a query document in one language, the goal is to find the most similar document from the corpus in another language. 4.1.1 Data & Setting We followed the comparable document retrieval setting described in (Platt et al., 2010) and evaluated S2Net on the Wikipedia dataset used in that paper. This data set consists of Wikipedia documents 1 Without the γ parameter, the model still outperforms other baselines in our experiments, but with a much smaller gain. in two languages, English and Spanish. An article in English is paired wit"
W11-0329,P98-2127,0,0.025292,"ors, and is able to efficiently handle a large number of training examples in the highdimensional space. Evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient. 1 Introduction Measures of text similarity have many applications and have been studied extensively in both the NLP and IR communities. For example, a combination of corpus and knowledge based methods have been invented for judging word similarity (Lin, 1998; Agirre et al., 2009). Similarity derived from a largescale Web corpus has been used for automatically extending lists of typed entities (Vyas and Pantel, 2009). Judging the degree of similarity between documents is also fundamental to classical IR problems such as document retrieval (Manning et al., 2008). In all these applications, the vector-based similarity method is the most widely used. Term vectors are first constructed to represent the original text objects, where each term is associated with However, the main weakness of this term-vector representation is that different but semantica"
W11-0329,D09-1092,0,0.12033,"PLSA to a proper generative model for documents and places Dirichlet priors over the parameters θ and φ. In the experiments in this paper, our implementation of PLSA is LDA with maximum a posteriori (MAP) inference, which was shown to be comparable to the current best Bayesian inference methods for LDA (Asuncion et al., 2009). Recently, these topic models have been generalized to handle pairs or tuples of corresponding documents, which could be translations in multiple languages, or documents in the same language that are considered similar. For instance, the Poly-lingual Topic Model (PLTM) (Mimno et al., 2009) is an extension to LDA that views documents in a tuple as having a shared topic vector θ. Each of the documents in the tuple uses θ to select the topics z of tokens, but could use a different (languagespecific) word-topic-distribution M ULTI(φL z ). Two additional models, Joint PLSA (JPLSA) and Coupled PLSA (CPLSA) were introduced in (Platt et al., 2010). JPLSA is a close variant of PLTM when documents of all languages share the same word-topic distribution parameters, and MAP inference is performed instead of Bayesian. CPLSA extends JPLSA by constraining paired documents to not only share th"
W11-0329,J05-4003,0,0.0121379,"int (A0 ), or simply by early stopping. Empirically we found that the latter is more effective and it is used in the experiments. 4 Experiments We compare S2Net experimentally with existing approaches on two very different tasks: cross-lingual document retrieval and ad relevance measures. 4.1 Comparable Document Retrieval With the growth of multiple languages on the Web, there is an increasing demand of processing crosslingual documents. For instance, machine translation (MT) systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the Web (Munteanu and Marcu, 2005). Word-level translation lexicons can also be learned from comparable documents (Fung and Yee, 1998; Rapp, 1999). In this cross-lingual document retrieval task, given a query document in one language, the goal is to find the most similar document from the corpus in another language. 4.1.1 Data & Setting We followed the comparable document retrieval setting described in (Platt et al., 2010) and evaluated S2Net on the Wikipedia dataset used in that paper. This data set consists of Wikipedia documents 1 Without the γ parameter, the model still outperforms other baselines in our experiments, but w"
W11-0329,D10-1025,1,0.901798,"s have been generalized to handle pairs or tuples of corresponding documents, which could be translations in multiple languages, or documents in the same language that are considered similar. For instance, the Poly-lingual Topic Model (PLTM) (Mimno et al., 2009) is an extension to LDA that views documents in a tuple as having a shared topic vector θ. Each of the documents in the tuple uses θ to select the topics z of tokens, but could use a different (languagespecific) word-topic-distribution M ULTI(φL z ). Two additional models, Joint PLSA (JPLSA) and Coupled PLSA (CPLSA) were introduced in (Platt et al., 2010). JPLSA is a close variant of PLTM when documents of all languages share the same word-topic distribution parameters, and MAP inference is performed instead of Bayesian. CPLSA extends JPLSA by constraining paired documents to not only share the same prior topic distribution θ, but to also have similar fractions of tokens assigned to each topic. This constraint is enforced on expectation using posterior regularization (Ganchev et al., 2009). In the rest of the paper, we first survey some existing work in Sec. 2, with an emphasis on approaches included in our experimental comparison. We present"
W11-0329,P99-1067,0,0.0156585,"ts. 4 Experiments We compare S2Net experimentally with existing approaches on two very different tasks: cross-lingual document retrieval and ad relevance measures. 4.1 Comparable Document Retrieval With the growth of multiple languages on the Web, there is an increasing demand of processing crosslingual documents. For instance, machine translation (MT) systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the Web (Munteanu and Marcu, 2005). Word-level translation lexicons can also be learned from comparable documents (Fung and Yee, 1998; Rapp, 1999). In this cross-lingual document retrieval task, given a query document in one language, the goal is to find the most similar document from the corpus in another language. 4.1.1 Data & Setting We followed the comparable document retrieval setting described in (Platt et al., 2010) and evaluated S2Net on the Wikipedia dataset used in that paper. This data set consists of Wikipedia documents 1 Without the γ parameter, the model still outperforms other baselines in our experiments, but with a much smaller gain. in two languages, English and Spanish. An article in English is paired with a Spanish a"
W11-0329,P10-1040,0,0.0085695,"of S2Net are all different compared to previous work. For example, targeting the application of face verification, Chopra et al. (2005) used a convolutional network and designed a contrastive loss function for optimizing a Eucliden distance metric. In contrast, the network of S2Net is equivalent to a linear projection matrix and has a pairwise loss function. In terms of the learning framework, S2Net is closely related to several neural network based approaches, including autoencoders (Hinton and Salakhutdinov, 2006) and finding low-dimensional word representations (Collobert and Weston, 2008; Turian et al., 2010). Architecturally, S2Net is also similar to RankNet (Burges et al., 2005), which can be viewed as a Siamese neural network that learns a ranking function. The strategy that S2Net takes to learn from labeled pairs of documents can be analogous to the work of distance metric learning. Although high dimensionality is not a problem to algorithms like HDLR, it suffers from a different scalability issue. As we have observed in our experiments, the algorithm can only handle a small number of similarity/dissimilarity constraints (i.e., the labeled examples), and is not able to use a large number of ex"
W11-0329,N09-1033,0,0.00773383,"s-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient. 1 Introduction Measures of text similarity have many applications and have been studied extensively in both the NLP and IR communities. For example, a combination of corpus and knowledge based methods have been invented for judging word similarity (Lin, 1998; Agirre et al., 2009). Similarity derived from a largescale Web corpus has been used for automatically extending lists of typed entities (Vyas and Pantel, 2009). Judging the degree of similarity between documents is also fundamental to classical IR problems such as document retrieval (Manning et al., 2008). In all these applications, the vector-based similarity method is the most widely used. Term vectors are first constructed to represent the original text objects, where each term is associated with However, the main weakness of this term-vector representation is that different but semantically related terms are not matched and cannot influence the final similarity score. As an illustrative example, suppose the two compared term-vectors are: {purcha"
W11-0329,C98-1066,0,\N,Missing
W11-0329,C98-2122,0,\N,Missing
W15-4007,D14-1165,0,0.0137256,"n many cases, while still maintaining a high upper bound on achievable performance. Details on the impact of usage of types are presented in Section 4. . Here the denominator is defined using a set of entities that do not fill the object position in any relation triple (e1 , r, ?) in the training knowledge graph. Since the number of such entities is impractically large, we sample negative triples from the full set (we use 200 negative examples in our experiments). In some settings we also limit the candidate entities to ones that have types consistent with the position in the relation triple (Chang et al., 2014; Yang et al., 2015). We derive approximate type information automatically (as discussed below), but such information could also be present in the knowledge graphs. Conditional probabilities for subject entities given relation and object are defined analogously, as follows: p(e1 |rk , e2 ; Θ) = 3.4 Representation and loss for text-augmented knowledge graphs In addition to knowledge graphs containing only relations r from a given manually developed ontology, we consider knowledge graphs augmented with textual relations derived from sentence cooccurrences of entity pairs. This follows the approa"
W15-4007,D13-1080,0,0.0879155,"Missing"
W15-4007,D14-1044,0,0.048295,"the same loss function (as a function of triple scores) for training all models in this study. Knowledge Base Honolulu city_of place_of_birth Barack Obama United States nationality Textual Mentions Barack Obama is the 44th and current President of United States. Obama was born in the United States just as he has always said. … ClueWeb Figure 1: A knowledge base fragment coupled with textual mentions of pairs of entities. els (Dong et al., 2014; Nickel et al., 2014), and work using inference from both text and knowledge base relations (Lao et al., 2012; Riedel et al., 2013; Dong et al., 2014; Gardner et al., 2014). Our work differs from this prior work in that we compare a very simple form of observed feature models, based on using only direct links between candidate entity pairs, to state-of-the-art latent feature models on two benchmark datasets, with surprising results. Our work on using textual mentions for knowledge base inference differs from prior work in the scale and richness of the knowledge base and textual relations used, as well as in that we evaluate the impact of text not only on mentioned entity pairs like (Gardner et al., 2014; Riedel et al., 2013) but on all links. We represent knowle"
W15-4007,D11-1049,0,0.0702807,"ase and textual patterns in a single knowledge graph, like Lao et al. (2012) and Riedel et al. (2013), but refine the learning method to treat textual relations differently in the loss function, to maximize predictive performance on the knowledge base relations. We show the impact of observed and latent feature models and their combination in knowledge graphs with and without textual relations. 3 3.1 Observed feature models We consider an extremely simple form of observed feature models, which can be seen as an impoverished variant of path ranking (PRA) for KB completion (Lao and Cohen, 2010; Lao et al., 2011). In particular we define features for existing paths of length one for candidate triples (ei , rk , ej ). These can be paths from ei to ej or from ej to ei . Length one paths from ei to ej : we define binary features of the form 1(r0 &rk ), which fire when the triple ei , r0 , ej exists in the training knowledge graph, and r0 6= rk . This feature type captures correlations among multiple relation types for the same entity pair – for example, if someone lives in a certain city, they might be likely to work in the same city. Length one Models for knowledge base completion We begin by introducin"
W15-4007,D12-1093,0,0.108058,"loss functions used for training model parameters. We use the same loss function (as a function of triple scores) for training all models in this study. Knowledge Base Honolulu city_of place_of_birth Barack Obama United States nationality Textual Mentions Barack Obama is the 44th and current President of United States. Obama was born in the United States just as he has always said. … ClueWeb Figure 1: A knowledge base fragment coupled with textual mentions of pairs of entities. els (Dong et al., 2014; Nickel et al., 2014), and work using inference from both text and knowledge base relations (Lao et al., 2012; Riedel et al., 2013; Dong et al., 2014; Gardner et al., 2014). Our work differs from this prior work in that we compare a very simple form of observed feature models, based on using only direct links between candidate entity pairs, to state-of-the-art latent feature models on two benchmark datasets, with surprising results. Our work on using textual mentions for knowledge base inference differs from prior work in the scale and richness of the knowledge base and textual relations used, as well as in that we evaluate the impact of text not only on mentioned entity pairs like (Gardner et al., 2"
W15-4007,N13-1008,0,0.108243,"ed for training model parameters. We use the same loss function (as a function of triple scores) for training all models in this study. Knowledge Base Honolulu city_of place_of_birth Barack Obama United States nationality Textual Mentions Barack Obama is the 44th and current President of United States. Obama was born in the United States just as he has always said. … ClueWeb Figure 1: A knowledge base fragment coupled with textual mentions of pairs of entities. els (Dong et al., 2014; Nickel et al., 2014), and work using inference from both text and knowledge base relations (Lao et al., 2012; Riedel et al., 2013; Dong et al., 2014; Gardner et al., 2014). Our work differs from this prior work in that we compare a very simple form of observed feature models, based on using only direct links between candidate entity pairs, to state-of-the-art latent feature models on two benchmark datasets, with surprising results. Our work on using textual mentions for knowledge base inference differs from prior work in the scale and richness of the knowledge base and textual relations used, as well as in that we evaluate the impact of text not only on mentioned entity pairs like (Gardner et al., 2014; Riedel et al., 2"
