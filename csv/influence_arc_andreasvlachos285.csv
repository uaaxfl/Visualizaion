2020.emnlp-main.580,D19-6609,0,0.028413,", so fine-tuning for question answering first on a much larger dataset is useful. 7 Related Work Previous work in NLP has focused on claim veracity. It has been treated as a classification problem (Wang, 2017), often using stance detection (Riedel et al., 2017). The FEVER Challenge (Thorne et al., 2018) proposed providing provenance for a decision along with classification, and various approaches developed combine information retrieval with stance detection or question answering (Li et al., 2018; Lee et al., 2018). Question generation and answering has been considered in the context of FEVER (Jobanputra, 2019) — the focus was on eliciting the right answer from a question answering system rather than improving the accuracy and efficiency of human fact checkers. However, FEVER is based on modified Wikipedia sentences, not real world claims, which are arguably more difficult. To address this Hanselowski et al. (2019) considered the claims fact checked by the website Snopes, but used the reports accompanying them as evidence instead of finding the evidence directly. Popat et al. (2018) and Augenstein et al. (2019) used search engines, but without ensuring that they provide evidence supporting/refuting"
2020.emnlp-main.580,P17-1147,0,0.103053,"Missing"
2021.acl-long.120,N19-1423,0,0.192867,"kens) as its definition includes different entity classes and tokens in training and testing. It is possible that words observed as non-entities during training belong to one of the test classes, as seen in Figure 1: both Huaqiao Park, in training, and Shantou Harbour, during testing, are entities of the class Facility, however, Huaqiao Park is labelled as a non-entity in the former. Based on this insight we propose several architectures for NERC based on cross-attention between the sentence and the entity type descriptions using transformers (Vaswani et al., 2017) combined with pre-training (Devlin et al., 2019). We 1516 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 1516–1528 August 1–6, 2021. ©2021 Association for Computational Linguistics Dev Test Train sentence str with annotation Shantou Harbour , a natural river seaport , The Shantou Science and Technology Museum O B-ORG I-ORG I-ORG I-ORG I-ORG completed construction in November of O O O B-DATE I-DATE I-DATE O O ORG O O Used to classify a companies, government reference to a date or period ... agencies ... PERSON O O O O O"
2021.acl-long.120,C18-1161,0,0.033749,"Missing"
2021.acl-long.120,W18-5036,0,0.021525,"(Obamuyide and Vlachos, 2018) who pose the task as one of textual entailment. Obeidat et al. (2019) use descriptions for zero-shot NET, however, similar to a previous attempt by Ma et al. (2016), they use the underlying hierarchy to only include unseen classes in the leaves of the hierarchy to reduce the relevant unseen classes to only two or three. The only work on zero-shot word sequence labelling (Rei and Søgaard, 2018) explores the transfer from labels on a sentence level objective (e.g. sentiment analysis) to a token or phrase-based annotation, similar to T¨ackstr¨om and McDonald (2011). Guerini et al. (2018) label their approach zero-shot named entity recognition, however, they focus on recognizing unseen entities not entity classes. Finally, Fritzler et al. (2019) focused on few-shot NERC using prototypical networks (Snell et al., 2017). They tested their model in the zero-shot setting, but concluded that their approach is not suitable for zero-shot learning as the results on OntoNotes were too low. 6 Conclusions & Future work This paper explored the task of zero-shot NERC with entity type descriptions to transfer knowledge from observed to unseen classes. We addressed the zero-shot NERC specifi"
2021.acl-long.120,P19-1356,0,0.0147284,"1,neg , ..., vn,neg = ENC(s), We label this model Sequential Multiclass Crossattention Model (SMXM). Referring to the initial example, cross-attention enables Shantou Harbour to attend to infrastructure in the type description of the class Facility, generating a representation for this token based on the type description in the context of the sentence. Cross-attention Encoder The cross-attention model is based on the pre-trained transformer encoder BERT (Devlin et al., 2019) which allows the model to capture surface-level information as well as semantic aspects between all words of the input (Jawahar et al., 2019). For X-ENC the input tuple (s, dc ) is structured in the form: xX-ENC = [CLS] s [SEP] dc [SEP]. 2.1 Modelling the negative class As discussed in Section 1, the non-entity class creates a challenging setting it is possible that words observed as non-entities during training belong to one of the test classes. We explore three approaches to modelling the negative class: (i) using a (textual) description for the negative class, (ii) modelling the negative class directly, (iii) modelling the negative class using the representations generated for the classes corresponding to types. Description-base"
2021.acl-long.120,N16-1030,0,0.0279013,"An analysis on the classification and recognition task in isolation highlights the importance of the description choice, finding that annotation guidelines result in higher scores than the class name itself or Wikipedia passages. 2 Zero-shot NERC In NERC, given a sentence s = w1 , ..., wn of length n and a description dc for each class c ∈ Cts in the test set, we predict a sequence of labels y ˆ ∈ (Cts )n , with n being the length of the sentence. We model the task as multiclass classification, which despite ignoring the sequential structure of the output, it has been found to be competitive (Lample et al., 2016; Rei, 2017). Thus, we predict the correct class for each token w at position t: arg maxc∈Cts F (s, wt , dc ), using a suitable function F modelling the semantic affinity between wt and dc in the context of s. The parameters of F need to be learned without annotated data for Cts , but with annotated data and descriptions for the training C tr classes. To model F we focus on the use of crossattention (Humeau et al., 2019; Wolf et al., 2019b) in the form of a transformer encoder (Vaswani et al., 2017). For each type description dc , the cross-attention encoder (X-ENC) generates a vector represen"
2021.acl-long.120,D17-1282,0,0.0502386,"Missing"
2021.acl-long.120,2020.acl-main.519,0,0.443333,"ntations generated for the classes corresponding to types. For evaluation we introduce zero-shot adaptations to two real-world NERC datasets with distinct properties: the OntoNotes (Pradhan et al., 2013) as well as the highly domain-specific MedMentions dataset (Mohan and Li, 2019). The adaptations adhere to recommendations to zeroshot evaluation (Xian et al., 2018) by evaluating models on the rarest classes while ensuring that all class sets are disjoint. Our best model achieves a macro F1 of 0.45 on OntoNotes-ZS and 0.38 on MedMentions-ZS, outperforming a state-ofthe-art MRC model for NERC (Li et al., 2020; Sun et al., 2020) and an adapted zero-shot text classification model (Yin et al., 2019). An analysis on the classification and recognition task in isolation highlights the importance of the description choice, finding that annotation guidelines result in higher scores than the class name itself or Wikipedia passages. 2 Zero-shot NERC In NERC, given a sentence s = w1 , ..., wn of length n and a description dc for each class c ∈ Cts in the test set, we predict a sequence of labels y ˆ ∈ (Cts )n , with n being the length of the sentence. We model the task as multiclass classification, which des"
2021.acl-long.120,P19-1335,0,0.128727,"assifying these spans from a set of pre-defined entity classes. A prevalent issue for many real-world applications is that annotated data does not readily exist. This motivates the focus on the zero-shot setting (Xian et al., 2018; Wang et al., 2019), where annotated data is not available for the classes of interest. Instead, information available from observed classes must be transferred to unseen target classes. Recently zero-shot approaches making use of textual representations to represent entity classes ∗ Work done when author was working at Google. were explored for entity linking (EL) (Logeswaran et al., 2019; Wu et al., 2020) and named entity typing (NET) (Obeidat et al., 2019), which are similar to the NERC subtask of named entity classification (NEC). However, no previous work has addressed the task of zero-shot NERC, which additionally requires the detection of which tokens make up an entity in addition to its type, i.e. Named Entity Recognition (NER). This paper is the first to study zero-shot NERC, by leveraging entity type descriptions. The task is illustrated in Figure 1. During testing, the input is a sentence and a set of target entity classes. each accompanied by its description, and th"
2021.acl-long.120,C16-1017,0,0.0142431,"isolation which appears to help the model extract more useful supervision signal, as indicated by the higher validation F1 achieved. When trained with masking, SMXM’s training loss closely follows the trend of the validation F1 , indicating good transfer learning from the model’s training objective to the zero-shot evaluation. 5 Related Work fined class descriptions have also been explored for relation classification (Obamuyide and Vlachos, 2018) who pose the task as one of textual entailment. Obeidat et al. (2019) use descriptions for zero-shot NET, however, similar to a previous attempt by Ma et al. (2016), they use the underlying hierarchy to only include unseen classes in the leaves of the hierarchy to reduce the relevant unseen classes to only two or three. The only work on zero-shot word sequence labelling (Rei and Søgaard, 2018) explores the transfer from labels on a sentence level objective (e.g. sentiment analysis) to a token or phrase-based annotation, similar to T¨ackstr¨om and McDonald (2011). Guerini et al. (2018) label their approach zero-shot named entity recognition, however, they focus on recognizing unseen entities not entity classes. Finally, Fritzler et al. (2019) focused on f"
2021.acl-long.120,W18-5511,1,0.733454,"providing additional implicit supervision to the model: masked tokens cannot be the non-entity class. For these masked tokens the model can focus on the entity classification in isolation which appears to help the model extract more useful supervision signal, as indicated by the higher validation F1 achieved. When trained with masking, SMXM’s training loss closely follows the trend of the validation F1 , indicating good transfer learning from the model’s training objective to the zero-shot evaluation. 5 Related Work fined class descriptions have also been explored for relation classification (Obamuyide and Vlachos, 2018) who pose the task as one of textual entailment. Obeidat et al. (2019) use descriptions for zero-shot NET, however, similar to a previous attempt by Ma et al. (2016), they use the underlying hierarchy to only include unseen classes in the leaves of the hierarchy to reduce the relevant unseen classes to only two or three. The only work on zero-shot word sequence labelling (Rei and Søgaard, 2018) explores the transfer from labels on a sentence level objective (e.g. sentiment analysis) to a token or phrase-based annotation, similar to T¨ackstr¨om and McDonald (2011). Guerini et al. (2018) label t"
2021.acl-long.120,N19-1087,0,0.0447731,"nt issue for many real-world applications is that annotated data does not readily exist. This motivates the focus on the zero-shot setting (Xian et al., 2018; Wang et al., 2019), where annotated data is not available for the classes of interest. Instead, information available from observed classes must be transferred to unseen target classes. Recently zero-shot approaches making use of textual representations to represent entity classes ∗ Work done when author was working at Google. were explored for entity linking (EL) (Logeswaran et al., 2019; Wu et al., 2020) and named entity typing (NET) (Obeidat et al., 2019), which are similar to the NERC subtask of named entity classification (NEC). However, no previous work has addressed the task of zero-shot NERC, which additionally requires the detection of which tokens make up an entity in addition to its type, i.e. Named Entity Recognition (NER). This paper is the first to study zero-shot NERC, by leveraging entity type descriptions. The task is illustrated in Figure 1. During testing, the input is a sentence and a set of target entity classes. each accompanied by its description, and the goal is to recognize and classify entities in these target classes. D"
2021.acl-long.120,W13-3516,0,0.0351582,"Missing"
2021.acl-long.120,P17-1194,0,0.0247211,"assification and recognition task in isolation highlights the importance of the description choice, finding that annotation guidelines result in higher scores than the class name itself or Wikipedia passages. 2 Zero-shot NERC In NERC, given a sentence s = w1 , ..., wn of length n and a description dc for each class c ∈ Cts in the test set, we predict a sequence of labels y ˆ ∈ (Cts )n , with n being the length of the sentence. We model the task as multiclass classification, which despite ignoring the sequential structure of the output, it has been found to be competitive (Lample et al., 2016; Rei, 2017). Thus, we predict the correct class for each token w at position t: arg maxc∈Cts F (s, wt , dc ), using a suitable function F modelling the semantic affinity between wt and dc in the context of s. The parameters of F need to be learned without annotated data for Cts , but with annotated data and descriptions for the training C tr classes. To model F we focus on the use of crossattention (Humeau et al., 2019; Wolf et al., 2019b) in the form of a transformer encoder (Vaswani et al., 2017). For each type description dc , the cross-attention encoder (X-ENC) generates a vector representation vt,c"
2021.acl-long.120,N18-1027,0,0.01483,"F1 , indicating good transfer learning from the model’s training objective to the zero-shot evaluation. 5 Related Work fined class descriptions have also been explored for relation classification (Obamuyide and Vlachos, 2018) who pose the task as one of textual entailment. Obeidat et al. (2019) use descriptions for zero-shot NET, however, similar to a previous attempt by Ma et al. (2016), they use the underlying hierarchy to only include unseen classes in the leaves of the hierarchy to reduce the relevant unseen classes to only two or three. The only work on zero-shot word sequence labelling (Rei and Søgaard, 2018) explores the transfer from labels on a sentence level objective (e.g. sentiment analysis) to a token or phrase-based annotation, similar to T¨ackstr¨om and McDonald (2011). Guerini et al. (2018) label their approach zero-shot named entity recognition, however, they focus on recognizing unseen entities not entity classes. Finally, Fritzler et al. (2019) focused on few-shot NERC using prototypical networks (Snell et al., 2017). They tested their model in the zero-shot setting, but concluded that their approach is not suitable for zero-shot learning as the results on OntoNotes were too low. 6 Co"
2021.acl-long.120,2020.emnlp-main.519,0,0.013732,"m a set of pre-defined entity classes. A prevalent issue for many real-world applications is that annotated data does not readily exist. This motivates the focus on the zero-shot setting (Xian et al., 2018; Wang et al., 2019), where annotated data is not available for the classes of interest. Instead, information available from observed classes must be transferred to unseen target classes. Recently zero-shot approaches making use of textual representations to represent entity classes ∗ Work done when author was working at Google. were explored for entity linking (EL) (Logeswaran et al., 2019; Wu et al., 2020) and named entity typing (NET) (Obeidat et al., 2019), which are similar to the NERC subtask of named entity classification (NEC). However, no previous work has addressed the task of zero-shot NERC, which additionally requires the detection of which tokens make up an entity in addition to its type, i.e. Named Entity Recognition (NER). This paper is the first to study zero-shot NERC, by leveraging entity type descriptions. The task is illustrated in Figure 1. During testing, the input is a sentence and a set of target entity classes. each accompanied by its description, and the goal is to recog"
2021.acl-long.120,D19-1404,0,0.0289075,"Missing"
2021.acl-long.120,N18-1101,0,0.0162435,"set, to classify whether a class description (The text is about X) is entailed by the text. To adapt this model to NERC, we modify the description to The word is of type X with X being the entity class name, and classify each word instead of the entire sentence. Since their model generates a binary output for each class, the negative prediction for all classes predicts the negative class. By treating each sentence-description pair independently, the relationship between classes as well as the complexity of the negative class in zero-shot evaluation is ignored. We fine-tune BERT-Large on MNLI (Williams et al., 2018), as it performed best in the experiments of (Yin et al., 2019), before training BEM on the zero-shot datasets using adjusted class weights, which has been crucial for successful training of the model; not using it resulted in degenerated solutions in preliminary experiments. The proposed entity masking objective is not suitable for BEM’s binary classification approach as it would simply learns to predict the masked token to be an entity during training. MRC for NERC is an approach by Li et al. (2020) who construct queries for entity classes and transform NERC to a machine reading comprehensio"
2021.acl-long.256,2020.acl-main.656,0,0.0189718,"rections to claims, in contrast to fact verification, which instead classifies the veracity of the claim. Introduction Fact verification is the task of predicting whether claims are true or false using evidence. With the availability of a number of resources (Wang, 2017; Karadzhov et al., 2017; Thorne et al., 2018; Augenstein et al., 2019; Wadden et al., 2020), the task has attracted significant attention and spawned the development of new models, architectures and approaches. With potentially sensitive applications, recent works have focused on building explainable variants of fact checking (Atanasova et al., 2020; Stammbach and Ash, 2020; Kotonya and Toni, 2020). Exposing the evidence source and 1 https://github.com/j6mes/ 2021-acl-factual-error-correction Similar to other recluse spider bites, their bite sometimes requires medical attention. decision making process may help the reader uncover subtle issues that cause automated systems to fail. Additionally, using such evidence to continuously update news articles as facts change forms part of the vision outlined by Cohen et al. (2011) for automated newsrooms. In this paper, we propose Factual Error Correction, as an explainable alternative for fact v"
2021.acl-long.256,D19-1475,0,0.0501646,"Missing"
2021.acl-long.256,P17-1152,0,0.0375196,"st is a BERT classifier where evidence and the claim are concatenated in the input. This is referred to as black-box because the model does not undergo modification and no information about internal values or states is exposed. White-box masker In contrast, to obtain whitebox model explanations, the model has undergone modification to expose internal information. We use the Neutrality Masker from (Shah et al., 2020) to predict which tokens, when masked, are likely to cause a label flip from supports or refuted to not enough information. This masker exposes encoded input of an ESIM classifier (Chen et al., 2017), and adds a linear classifier over the hidden states to predict per-token masking probability. At test time, masks can be generated through a single query to the model (unlike LIME in the black-box masker which requires multiple queries to the model), however this requires an additional step to train, using predictions from the classifier as signal. Language model masker We evaluate whether it is possible to generate masks without the need for a fact verification model. We use a BERT pretrained language model (Devlin et al., 2019) to measure the surprisal of tokens in the claim. Our intuition"
2021.acl-long.256,N19-1423,0,0.0114822,"mation. This masker exposes encoded input of an ESIM classifier (Chen et al., 2017), and adds a linear classifier over the hidden states to predict per-token masking probability. At test time, masks can be generated through a single query to the model (unlike LIME in the black-box masker which requires multiple queries to the model), however this requires an additional step to train, using predictions from the classifier as signal. Language model masker We evaluate whether it is possible to generate masks without the need for a fact verification model. We use a BERT pretrained language model (Devlin et al., 2019) to measure the surprisal of tokens in the claim. Our intuition is to identify tokens which introduce misinformation under the hypothesis that the world knowledge (Petroni et al., 2019) captured in retraining would assign lower probabilities to tokens contradictory to the world state. This language model has no additional task-specific fine-tuning. We independently predict the cross-entropy for each token under a masked language modelling objective using BERT and return the top-k tokens. Baselines We additionally consider two simple baseline maskers: random masking of a subset of tokens and al"
2021.acl-long.256,han-etal-2010-using,0,0.106095,"Missing"
2021.acl-long.256,karadzhov-etal-2017-fully,0,0.0633883,"Missing"
2021.acl-long.256,2020.emnlp-main.623,0,0.0290134,"ion, which instead classifies the veracity of the claim. Introduction Fact verification is the task of predicting whether claims are true or false using evidence. With the availability of a number of resources (Wang, 2017; Karadzhov et al., 2017; Thorne et al., 2018; Augenstein et al., 2019; Wadden et al., 2020), the task has attracted significant attention and spawned the development of new models, architectures and approaches. With potentially sensitive applications, recent works have focused on building explainable variants of fact checking (Atanasova et al., 2020; Stammbach and Ash, 2020; Kotonya and Toni, 2020). Exposing the evidence source and 1 https://github.com/j6mes/ 2021-acl-factual-error-correction Similar to other recluse spider bites, their bite sometimes requires medical attention. decision making process may help the reader uncover subtle issues that cause automated systems to fail. Additionally, using such evidence to continuously update news articles as facts change forms part of the vision outlined by Cohen et al. (2011) for automated newsrooms. In this paper, we propose Factual Error Correction, as an explainable alternative for fact verification. Rather than merely assigning a truth"
2021.acl-long.256,2020.fever-1.5,0,0.0303979,"ep for outputs from abstractive summarization so that they are consistent with the source text. Their approach uses a sequence-tosequence model trained to restore artificially generated corruptions of a reference summary. One potential way to introduce knowledge is to use information stored in the parameters of largescale pre-trained language models (Petroni et al., 2019). The language model can be used recover tokens responsible for causing factual errors that are masked out as a variant of cloze-style evaluation (Taylor, 1953). While such approaches have been employed for fact verification (Lee et al., 2020), these approaches share the following limitations. Without explicit control (Nie et al., 2019), the most likely token when decoded may not be factually accurate, or supported by the retrieved evidence, commonly referred to as a hallucination (Rohrbach et al., 2018; Zhou et al., 2020). Furthermore, even if the information stored within language model parameters could be reliably retrieved for factual error correction, facts change over time and the need to obtain information from up-to-date sources becomes greater as the state of the world diverges from the information captured within the mode"
2021.acl-long.256,W04-1013,0,0.0297741,"tasks are assigned to two raters to measure inter-annotator agreement. We used 4 expert participants from our lab (none of them co-authors of the paper) who were familiar with fact verifica3302 tion, but not with error correction. Responses were calibrated using a pilot study on the validation set. For automated evaluation, we use SARI (Xu et al., 2016) which is a metric used for sentence simplification. SARI considers ngrams retained from the source as well added or deleted ngrams through comparison against a reference sentence. We additionally report BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) to indicate precision and recall of the correction. In Section 9, we report correlation of automated metrics against our manual evaluation. 8 Implementation T5 Masker-Corrector We fine-tuned the T5base pre-trained models released by HuggingFace (Wolf et al., 2020). The number of training epochs and learning rate was selected through optimizing the overall SARI score. The search space for learning rate was {10−5 , 5 · 10−5 , 104 , 5 · 10−4 }. We used 5 · 10−5 for all experiments. We found diminishing returns in SARI after 4 epochs and stopped training. Fully Supervised Ceiling We use this mode"
2021.acl-long.256,W14-1701,0,0.0733618,"Missing"
2021.acl-long.256,P19-1256,0,0.0169278,"Their approach uses a sequence-tosequence model trained to restore artificially generated corruptions of a reference summary. One potential way to introduce knowledge is to use information stored in the parameters of largescale pre-trained language models (Petroni et al., 2019). The language model can be used recover tokens responsible for causing factual errors that are masked out as a variant of cloze-style evaluation (Taylor, 1953). While such approaches have been employed for fact verification (Lee et al., 2020), these approaches share the following limitations. Without explicit control (Nie et al., 2019), the most likely token when decoded may not be factually accurate, or supported by the retrieved evidence, commonly referred to as a hallucination (Rohrbach et al., 2018; Zhou et al., 2020). Furthermore, even if the information stored within language model parameters could be reliably retrieved for factual error correction, facts change over time and the need to obtain information from up-to-date sources becomes greater as the state of the world diverges from the information captured within the model parameters. Recent language models augmented with a retrieval component such as REALM (Guu et"
2021.acl-long.256,P02-1040,0,0.109729,"Missing"
2021.acl-long.256,D19-1250,0,0.269934,"replaces the masks with ground truth evidence. In this approach, token salience is predicted by querying a model that is trained to perform fact verification for a claim against evidence. Cao et al. (2020) generate corrections as a post-editing step for outputs from abstractive summarization so that they are consistent with the source text. Their approach uses a sequence-tosequence model trained to restore artificially generated corruptions of a reference summary. One potential way to introduce knowledge is to use information stored in the parameters of largescale pre-trained language models (Petroni et al., 2019). The language model can be used recover tokens responsible for causing factual errors that are masked out as a variant of cloze-style evaluation (Taylor, 1953). While such approaches have been employed for fact verification (Lee et al., 2020), these approaches share the following limitations. Without explicit control (Nie et al., 2019), the most likely token when decoded may not be factually accurate, or supported by the retrieved evidence, commonly referred to as a hallucination (Rohrbach et al., 2018; Zhou et al., 2020). Furthermore, even if the information stored within language model para"
2021.acl-long.256,D18-1437,0,0.0267805,"e is to use information stored in the parameters of largescale pre-trained language models (Petroni et al., 2019). The language model can be used recover tokens responsible for causing factual errors that are masked out as a variant of cloze-style evaluation (Taylor, 1953). While such approaches have been employed for fact verification (Lee et al., 2020), these approaches share the following limitations. Without explicit control (Nie et al., 2019), the most likely token when decoded may not be factually accurate, or supported by the retrieved evidence, commonly referred to as a hallucination (Rohrbach et al., 2018; Zhou et al., 2020). Furthermore, even if the information stored within language model parameters could be reliably retrieved for factual error correction, facts change over time and the need to obtain information from up-to-date sources becomes greater as the state of the world diverges from the information captured within the model parameters. Recent language models augmented with a retrieval component such as REALM (Guu et al., 2020) and RAG (Lewis et al., 2020) could be applied, however, task-specific fine-tuning would still be required to condition the generation based on the factual err"
2021.acl-long.256,P17-1099,0,0.0241167,"are not in common between the claim and the retrieved evidence. 5.3 Corrections We train an encoder-decoder transformer model to generate corrections from masked claims and 3301 evidence. Our model uses a pre-trained T5 transformer (Raffel et al., 2020) which we fine-tune with the distant supervision protocol described in Section 4.1. This model jointly encodes the masked claim and evidence by concatenating these two inputs in the input. We also compare against a baseline model from a related task of fact guided sentence modification (Shah et al., 2020) which uses a pointer generator network (See et al., 2017). Unlike our model, which captures long-range dependencies between claim and evidence through the transformer selfattention (Vaswani et al., 2017), the baseline independently encodes the evidence and masked claim using LSTMs (Hochreiter and Schmidhuber, 1997) before decoding using a pointer-copy mechanism. In order to evaluate the impact of conditioning on evidence, we decode tokens from masked claims using a language model without fine-tuning or conditioning, similar to the Language Models as Knowledge Bases hypothesis introduced by Petroni et al. (2019). This would consider correcting claims"
2021.acl-long.256,N18-1074,1,0.741616,"Missing"
2021.acl-long.256,2020.emnlp-main.609,0,0.0379734,"Missing"
2021.acl-long.256,P17-2067,0,0.0279889,"ed on a recent fact verification shared task and we release it to enable further work on the task.1 1 Information Retrieval Retrieved Evidence Wikipedia System Outputs Fact Veriﬁcation REFUTED Error Correction The brown recluse spider&apos;s bite sometimes requires medical attention. Figure 1: Factual Error Correction uses evidence to make corrections to claims, in contrast to fact verification, which instead classifies the veracity of the claim. Introduction Fact verification is the task of predicting whether claims are true or false using evidence. With the availability of a number of resources (Wang, 2017; Karadzhov et al., 2017; Thorne et al., 2018; Augenstein et al., 2019; Wadden et al., 2020), the task has attracted significant attention and spawned the development of new models, architectures and approaches. With potentially sensitive applications, recent works have focused on building explainable variants of fact checking (Atanasova et al., 2020; Stammbach and Ash, 2020; Kotonya and Toni, 2020). Exposing the evidence source and 1 https://github.com/j6mes/ 2021-acl-factual-error-correction Similar to other recluse spider bites, their bite sometimes requires medical attention. decision maki"
2021.acl-long.256,Q16-1029,0,0.0422217,"Missing"
2021.adaptnlp-1.15,2020.acl-main.236,0,0.0446908,"Missing"
2021.adaptnlp-1.15,Q17-1010,0,0.317708,"hich convey important information for downstream tasks; for example, drug names are key in the biomedical domain. However, the amount of downstream language data is typically much smaller than the corpus used for training word embeddings, thus methods that rely on distributional properties of words across large amounts of data perform poorly (Herbelot and Baroni, 2017). Researchers often assign OOV words to random embeddings or to an “unknown” embedding, however these solutions fail to capture the distributional properties of words. Zero-shot approaches (Pinter et al., 2017; Kim et al., 2016; Bojanowski et al., 2017) attempt to predict the embeddings for OOV words from their characters alone. These approaches rely on inferring the meaning of a word from its subword information, such as morphemes or WordPiece tokens used in BERT (Devlin et al., 2019). While this works well for many words, it performs poorly for names and words where morphology is not informative. Given that an OOV word occurs once, the chance of a second occurrence is much higher than the first (Church, 2000). Hence while OOV words can be rare and not seen in training, it is reasonable to expect that a limited number of occurrences will be"
2021.adaptnlp-1.15,C00-1027,0,0.100432,"ions fail to capture the distributional properties of words. Zero-shot approaches (Pinter et al., 2017; Kim et al., 2016; Bojanowski et al., 2017) attempt to predict the embeddings for OOV words from their characters alone. These approaches rely on inferring the meaning of a word from its subword information, such as morphemes or WordPiece tokens used in BERT (Devlin et al., 2019). While this works well for many words, it performs poorly for names and words where morphology is not informative. Given that an OOV word occurs once, the chance of a second occurrence is much higher than the first (Church, 2000). Hence while OOV words can be rare and not seen in training, it is reasonable to expect that a limited number of occurrences will be present in the data of a downstream application. Few-shot approaches (Garneau et al., 2018; Khodak et al., 2018; Hu et al., 2019) leveraged this to predict the embeddings for OOV words from just a few contexts, often in conjunction with their morphological information. Hu et al. (2019) proposed an attention-based architecture for OOV word embedding learning as a few-shot regression problem. The model is trained to predict the embedding of a word based on a few c"
2021.adaptnlp-1.15,W04-1213,0,0.106288,"seeds. While were able to obtain similar results for HiCE+MAML in some of our experiments, they were outside the confidence intervals we obtained, illustrating the relative instability in training with MAML (Antoniou et al., 2019). The results of Hu et al. (2019) are also lower than the highest results we obtained with HiCE+Leap (0.3896, 0.4116 and 0.4395 for 2-, 4- and 6-shot). 4.2 Extrinsic Evaluation To gauge the quality of the OOV embeddings for downstream tasks we evaluate their performance when applied to NER. For this purpose we use the JNLPBA 2004 Bio-Entity Recognition Task dataset (Collier and Kim, 2004). We choose this dataset as the biomedical domain differs significantly from the domain of Wikipedia that HiCE is pre-trained on, and contains many OOV technical terms. Hu et al. (2019) use this dataset also but did not provide their datasplits; thus. while we were able to confirm their results, we cannot compare against them directly. The JNLPBA dataset is constructed from 2000 abstracts for training and 404 abstracts for testing, each extracted from a bibliographic database of biomedical information and hand annotated with 36 classes corresponding to chemical classifications. These classific"
2021.adaptnlp-1.15,N19-1423,0,0.0152336,"us methods that rely on distributional properties of words across large amounts of data perform poorly (Herbelot and Baroni, 2017). Researchers often assign OOV words to random embeddings or to an “unknown” embedding, however these solutions fail to capture the distributional properties of words. Zero-shot approaches (Pinter et al., 2017; Kim et al., 2016; Bojanowski et al., 2017) attempt to predict the embeddings for OOV words from their characters alone. These approaches rely on inferring the meaning of a word from its subword information, such as morphemes or WordPiece tokens used in BERT (Devlin et al., 2019). While this works well for many words, it performs poorly for names and words where morphology is not informative. Given that an OOV word occurs once, the chance of a second occurrence is much higher than the first (Church, 2000). Hence while OOV words can be rare and not seen in training, it is reasonable to expect that a limited number of occurrences will be present in the data of a downstream application. Few-shot approaches (Garneau et al., 2018; Khodak et al., 2018; Hu et al., 2019) leveraged this to predict the embeddings for OOV words from just a few contexts, often in conjunction with"
2021.adaptnlp-1.15,W18-5439,0,0.019462,"alone. These approaches rely on inferring the meaning of a word from its subword information, such as morphemes or WordPiece tokens used in BERT (Devlin et al., 2019). While this works well for many words, it performs poorly for names and words where morphology is not informative. Given that an OOV word occurs once, the chance of a second occurrence is much higher than the first (Church, 2000). Hence while OOV words can be rare and not seen in training, it is reasonable to expect that a limited number of occurrences will be present in the data of a downstream application. Few-shot approaches (Garneau et al., 2018; Khodak et al., 2018; Hu et al., 2019) leveraged this to predict the embeddings for OOV words from just a few contexts, often in conjunction with their morphological information. Hu et al. (2019) proposed an attention-based architecture for OOV word embedding learning as a few-shot regression problem. The model is trained to predict the embedding of a word based on a few contexts and its character sequence. Such a model is trained by simulating OOV words in the training corpus, with their target embeddings provided by learning them on the same corpus. As OOV words must have their embeddings i"
2021.adaptnlp-1.15,D17-1030,0,0.0251315,"an embedding for every word is obtained, in practice outof-vocabulary (OOV) words do occur in the downstream applications embeddings are used, for example due to domain-specific terminology. Nevertheless, OOV words are often content words such as names which convey important information for downstream tasks; for example, drug names are key in the biomedical domain. However, the amount of downstream language data is typically much smaller than the corpus used for training word embeddings, thus methods that rely on distributional properties of words across large amounts of data perform poorly (Herbelot and Baroni, 2017). Researchers often assign OOV words to random embeddings or to an “unknown” embedding, however these solutions fail to capture the distributional properties of words. Zero-shot approaches (Pinter et al., 2017; Kim et al., 2016; Bojanowski et al., 2017) attempt to predict the embeddings for OOV words from their characters alone. These approaches rely on inferring the meaning of a word from its subword information, such as morphemes or WordPiece tokens used in BERT (Devlin et al., 2019). While this works well for many words, it performs poorly for names and words where morphology is not informa"
2021.adaptnlp-1.15,P19-1402,0,0.221326,"he meaning of a word from its subword information, such as morphemes or WordPiece tokens used in BERT (Devlin et al., 2019). While this works well for many words, it performs poorly for names and words where morphology is not informative. Given that an OOV word occurs once, the chance of a second occurrence is much higher than the first (Church, 2000). Hence while OOV words can be rare and not seen in training, it is reasonable to expect that a limited number of occurrences will be present in the data of a downstream application. Few-shot approaches (Garneau et al., 2018; Khodak et al., 2018; Hu et al., 2019) leveraged this to predict the embeddings for OOV words from just a few contexts, often in conjunction with their morphological information. Hu et al. (2019) proposed an attention-based architecture for OOV word embedding learning as a few-shot regression problem. The model is trained to predict the embedding of a word based on a few contexts and its character sequence. Such a model is trained by simulating OOV words in the training corpus, with their target embeddings provided by learning them on the same corpus. As OOV words must have their embeddings inferred from contexts outside the train"
2021.adaptnlp-1.15,W18-3407,0,0.0218482,"s sampled from some large training corpus DT . Each word (either a training target or in a context) is represented as a pre-trained embedding in the input to Hθ . We note that Hθ can be trained only on words with sufficient occurrences in DT such that their pre-trained embeddings can be accurately learned. The trained Hθ can then be used to infer the embeddings for OOV words, for which we do not have a pre-trained embedding. However, OOV words often form part of domain-specific vocabularies, the semantics of which are not captured by word embeddings trained on generic large corpora (Kameswara Sarma et al., 2018). To counteract this, we adapt the trained parameters θT of the word embedding regression function to the domain in which we infer the OOV embeddings. When adapting θT we consider both DT , and the corpus on which we wish to infer OOV embeddings, DN . We wish to transfer the knowledge encoded in θT to the task of predicting OOV embeddings for words in DN . One approach would be to simply fine-tune θT on DN ; that is, sample words and their contexts from DN which have their pre-trained embeddings from DT known and train Hθ as before on these words. This ignores that DN is much smaller than a co"
2021.adaptnlp-1.15,P18-1002,0,0.0177854,"s rely on inferring the meaning of a word from its subword information, such as morphemes or WordPiece tokens used in BERT (Devlin et al., 2019). While this works well for many words, it performs poorly for names and words where morphology is not informative. Given that an OOV word occurs once, the chance of a second occurrence is much higher than the first (Church, 2000). Hence while OOV words can be rare and not seen in training, it is reasonable to expect that a limited number of occurrences will be present in the data of a downstream application. Few-shot approaches (Garneau et al., 2018; Khodak et al., 2018; Hu et al., 2019) leveraged this to predict the embeddings for OOV words from just a few contexts, often in conjunction with their morphological information. Hu et al. (2019) proposed an attention-based architecture for OOV word embedding learning as a few-shot regression problem. The model is trained to predict the embedding of a word based on a few contexts and its character sequence. Such a model is trained by simulating OOV words in the training corpus, with their target embeddings provided by learning them on the same corpus. As OOV words must have their embeddings inferred from contexts"
2021.adaptnlp-1.15,N16-1030,0,0.0102392,"56 10-shot 0.7209 0.7232 0.7269 0.7282 † Table 2: Micro-averaged F1 score for each of the 2shot, 6-shot and 10-shot settings. Bold indicates the best results and (†) indicates the result is better than random embeddings at a 0.05 significance level. for 4643 OOV distinct words (types) in the 2-shot case, 1310 in the 6-shot case and 702 in the 10-shot case.2 The contexts used to infer the embedding of a word are chosen at random from the contexts that word appears. The inferred embeddings, alongside the embeddings for in-vocabulary words, are used as input to train the LSTM-CRF architecture of Lample et al. (2016). For each of the k-shot cases a separate test set is created by subsampling, such that the respective test set contains only those sentences with an OOV word whose embeddings has been inferred. This ensures that the test sets focus on the quality of the inferred OOV embeddings. For the 2-shot case there are 2876 test sentences; 2451 for 6-shot; and 2134 for 10-shot. The results for each k-shot setting are given in Table 2, reported in micro-averaged F1 score. The results obtained using random OOV embeddings as input are given as a baseline. Results are marginally improved with any of the prop"
2021.adaptnlp-1.15,D17-1010,0,0.0173995,"are often content words such as names which convey important information for downstream tasks; for example, drug names are key in the biomedical domain. However, the amount of downstream language data is typically much smaller than the corpus used for training word embeddings, thus methods that rely on distributional properties of words across large amounts of data perform poorly (Herbelot and Baroni, 2017). Researchers often assign OOV words to random embeddings or to an “unknown” embedding, however these solutions fail to capture the distributional properties of words. Zero-shot approaches (Pinter et al., 2017; Kim et al., 2016; Bojanowski et al., 2017) attempt to predict the embeddings for OOV words from their characters alone. These approaches rely on inferring the meaning of a word from its subword information, such as morphemes or WordPiece tokens used in BERT (Devlin et al., 2019). While this works well for many words, it performs poorly for names and words where morphology is not informative. Given that an OOV word occurs once, the chance of a second occurrence is much higher than the first (Church, 2000). Hence while OOV words can be rare and not seen in training, it is reasonable to expect"
2021.eacl-main.173,2020.sigdial-1.8,0,0.149228,"25 of these. Combined with the low estimates of antisocial behaviour on the site (Wulczyn et al., 2017), we draw the inference that the non-escalated class contains more constructive disagreements than the escalated ones. Throughout the remainder of this paper, we use these terms interchangeably. 4 Modelling constructive disagreement 4.1 Feature-based models We develop our feature-based models using feature sets suggested in previous research on tasks related to conversational analysis. These include: • Politeness: The politeness strategies from Zhang et al. (2018) as implemented in Convokit (Chang et al., 2020), which capture greetings, apologies, directness, and saying “please”, etc. • Collaboration: Conversation markers in collaborative discussions (Niculae and DanescuNiculescu-Mizil, 2016), which capture the introduction and adoption of ideas, uncertainty and confidence terms, pronoun usage, and linguistic style accommodation. • Toxicity: Toxicity and severe toxicity scores as estimated by the Perspective API (Wulczyn et al., 2017) and included in WikiConv (Hua et al., 2018). • Sentiment: Positive and negative sentiment word counts, as per the lexicon of Liu et al. (2005) and implemented in Convo"
2021.eacl-main.173,D19-1481,0,0.388974,"Missing"
2021.eacl-main.173,P19-1057,0,0.021835,"sonal attacks (Wulczyn et al., 2017). Recent works by Zhang et al. (2018) and Chang and Danescu-Niculescu-Mizil (2019) study conversations on Wikipedia talk pages that start out as civil but derail into personal attacks, using linguisFigure 1: An example of a dispute on Wikipedia, showing the dispute tag on the article, talk page posts, and summaries of edits occurring during the discussion. tic markers and deep learning approaches, respectively. An alternative approach is to study good faith disagreement through debates on online platforms such as ChangeMyView (Tan et al., 2016), debate.org (Durmus and Cardie, 2019) and Kialo (Boschi et al., 2019), or formal Oxford-style debates (Zhang et al., 2016a). While this has benefits, such as readily available annotation of stances and indication of winning and losing sides, it does not mirror the way people naturally converse. For example, in formal debates, temporal and structural 2017 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2017–2027 April 19 - 23, 2021. ©2021 Association for Computational Linguistics constraints are imposed. Moreover, in Oxford-style debates, participants are not motiv"
2021.eacl-main.173,D18-1305,0,0.0980027,"de an audience of a predefined stance (Zhang et al., 2016a). Finally, the task of detecting disagreement in conversations has been studied by a number of authors, e.g. Wang and Cardie (2014) and Rosenthal and McKeown (2015), without attempting to analyze it further. We are interested instead in constructive disagreement in an uncoerced setting. To this end, we present WikiDisputes1 ; a corpus of 7 425 disagreements (totalling 99 907 utterances) mined from Wikipedia Talk pages. To construct it, we map dispute tags in the platform’s edit history (shown in Figure 1) to conversations in WikiConv (Hua et al., 2018) to locate conversations that relate to content disputes. Observing that conversations are often conducted in the Talk pages and the edit summaries simultaneously, as illustrated in Figure 1, we augment the WikiConv conversations with edit summaries that occur concurrently. To investigate the factors that make a disagreement constructive, we define the task of predicting whether a dispute is eventually considered resolved by its participants, or it was escalated by them to mediation by a moderator, and therefore considered unconstructive. We evaluate both feature-based and neural models for th"
2021.eacl-main.173,W15-4625,0,0.0192567,"rally converse. For example, in formal debates, temporal and structural 2017 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2017–2027 April 19 - 23, 2021. ©2021 Association for Computational Linguistics constraints are imposed. Moreover, in Oxford-style debates, participants are not motivated to come to a consensus but rather to persuade an audience of a predefined stance (Zhang et al., 2016a). Finally, the task of detecting disagreement in conversations has been studied by a number of authors, e.g. Wang and Cardie (2014) and Rosenthal and McKeown (2015), without attempting to analyze it further. We are interested instead in constructive disagreement in an uncoerced setting. To this end, we present WikiDisputes1 ; a corpus of 7 425 disagreements (totalling 99 907 utterances) mined from Wikipedia Talk pages. To construct it, we map dispute tags in the platform’s edit history (shown in Figure 1) to conversations in WikiConv (Hua et al., 2018) to locate conversations that relate to content disputes. Observing that conversations are often conducted in the Talk pages and the edit summaries simultaneously, as illustrated in Figure 1, we augment the"
2021.eacl-main.173,N16-1070,0,0.0413373,"Missing"
2021.eacl-main.173,D14-1162,0,0.0860801,"cted by the mean. For instance, a disagreement might start out very politely and end impolitely, or the other way around, and would have the same mean. Logistic regression is used to infer linear relationships between the linguistic features and disagreement outcomes. 2020 4.2 Neural models Dialogue structure has been difficult to capture in feature based models due to sparsity. For this reason, we implement a number of neural models with increasing capacities for modelling conversation structure to assess its importance. Averaged embeddings Our simplest variant averages the GloVe embeddings (Pennington et al., 2014) of all words in a conversation, and uses a fully connected layer for classification. It ignores both utterance hierarchy and word ordering and can be seen as a bag-of-word-embeddings approach. LSTM Instead of averaging the GloVe embeddings, we use a bidirectional LSTM-based model (Hochreiter and Schmidhuber, 1997) to process the sequence of words in the conversation; ignoring utterance hierarchy but preserving word order. HAN We use a Hierarchical Attention Network (HAN) (Yang et al., 2016) to model both word order and utterance hierarchy. HANs have recently been used to predict emotion in co"
2021.eacl-main.173,N16-3020,0,0.0116152,"e second utterance, in response to the introductory comment (as occurs in Figure 4). From there, the dispute is either resolved or eventually escalated, and uncertainty decreases almost linearly as the model is exposed to new data. 6.5 Identifying inflection points Having ascertained that model accuracy improves and uncertainty decreases as a model is exposed to more information, we are interested in factors that cause the predicted class to change, and how the model predictions relate to the feature-based model coefficients. Although methods for interpreting neural network predictions exist (Ribeiro et al., 2016, 2018), these are not easily extendable to process inter-utterance dependencies as observed with the HAN. We instead analyse a conversation from our dataset to observe inflection points and what may have caused them. We show HAN model predictions and uncertainty values in Figure 4. The HAN model initially predicts escalation, with a relatively low uncertainty value. The conversation remains confrontational for the first seven utterances with a high escalation score. There are two “I” pronouns in the second utterance and two “you” pronouns in the third utterance, which were also associated wit"
2021.eacl-main.173,N16-1174,0,0.074227,"eements. We find that incorporating edit summaries (which have been ignored in previous work on Wikipedia discussions) improves model performance on predicting escalation. We further find that including information about the conversation structure aids model performance. We observe this in two ways. Firstly, we include gradient features, which capture changes in linguistic markers throughout the conversations as opposed to averaged values. Secondly, we experiment with adding sequential and hierarchical conversation structure to our neural models, finding that a Hierarchical Attention Network (Yang et al., 2016) provides the best performance on our task. We further evaluate this model in terms of its predictive accuracy when exposed to the beginnings of the conversation as opposed to completed conversations, as well as its uncertainty (more details in Section 6.4). Our results indicate that model performance is reduced from a PR-AUC of 0.29 1 github.com/christinedekock11/ wikidisputes to 0.19 when exposed to only the first half of the conversation. However, this reduced model still outperforms toxicity and sentiment models predicting on full conversations. Model uncertainty decreases roughly linearly"
2021.eacl-main.173,N16-1000,0,0.0640635,"nty 0.26 PR-AUC with psychotherapy research on disagreements in relationships (e.g. Gottman and Krokoff (1989)), which emphasises avoiding ‘you’-messages which might be perceived as blameful. Hedging is associated with constructive disagreements, which corroborates the findings of Zhang et al. (2016a). An intuitive understanding of this result is that using hedging terms shows that the speaker is more open to adjusting their opinion or compromising. The certainty gradient and the adoption of new ideas with certainty are also associated with constructiveness. Niculae and DanescuNiculescu-Mizil (2016) found no significant correlation between either certainty or hedging and successful collaboration. We attribute the observed differences in feature associations to the fact that WikiDisputes are sourced from an uncoerced setting, where users feel more invested in the outcome of a disagreement and may need to balance the use of certainty and hedging to negotiate compromises. The setting of Niculae and Danescu-NiculescuMizil (2016) obligates volunteers to participate in a photo geolocation game, where it is unlikely that interlocutors are as invested in the task as collaborators working on a wi"
2021.eacl-main.173,P18-1125,0,0.620991,"s models are exposed to more information. 1 Introduction Disagreements online are a familiar occurrence for any internet user. While they are often perceived as a negative phenomenon, disagreements can be useful; as is illustrated in Figure 1, disagreements can lead to an improved understanding of a topic by introducing and evaluating different perspectives. Research on online disagreements has focused on mostly negative aspects such as trolling (Cheng et al., 2017), hate speech (Waseem and Hovy, 2016), harassment (Yin et al., 2009) and personal attacks (Wulczyn et al., 2017). Recent works by Zhang et al. (2018) and Chang and Danescu-Niculescu-Mizil (2019) study conversations on Wikipedia talk pages that start out as civil but derail into personal attacks, using linguisFigure 1: An example of a dispute on Wikipedia, showing the dispute tag on the article, talk page posts, and summaries of edits occurring during the discussion. tic markers and deep learning approaches, respectively. An alternative approach is to study good faith disagreement through debates on online platforms such as ChangeMyView (Tan et al., 2016), debate.org (Durmus and Cardie, 2019) and Kialo (Boschi et al., 2019), or formal Oxfor"
2021.eacl-main.173,N16-1017,0,0.320283,"nescu-Niculescu-Mizil (2019) study conversations on Wikipedia talk pages that start out as civil but derail into personal attacks, using linguisFigure 1: An example of a dispute on Wikipedia, showing the dispute tag on the article, talk page posts, and summaries of edits occurring during the discussion. tic markers and deep learning approaches, respectively. An alternative approach is to study good faith disagreement through debates on online platforms such as ChangeMyView (Tan et al., 2016), debate.org (Durmus and Cardie, 2019) and Kialo (Boschi et al., 2019), or formal Oxford-style debates (Zhang et al., 2016a). While this has benefits, such as readily available annotation of stances and indication of winning and losing sides, it does not mirror the way people naturally converse. For example, in formal debates, temporal and structural 2017 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 2017–2027 April 19 - 23, 2021. ©2021 Association for Computational Linguistics constraints are imposed. Moreover, in Oxford-style debates, participants are not motivated to come to a consensus but rather to persuade an audience of a predefined stanc"
2021.eacl-main.219,N04-1022,0,0.0658664,"structured information (e.g. a database record or a meaning representation) that is both fluent and contains the right information. Sequence-to-sequence models (seq2seq) have been effective on many tasks in NLG (for example: Wen et al., 2015; Duˇsek and Jurˇc´ıcˇ ek, 2016). These systems first create an embedding for the input information. This embedding is used incrementally during decoding, generating one token To mitigate the limitations of beam search, it is common practice to apply a reranker to the final set of hypotheses. This can be done by defining a reranking criterion (for example: Kumar and Byrne, 2004; Blain et al., 2017; Borgeaud and Emerson, 2020) or by training a reranker to predict the best hypothesis in a beam (for example: Duˇsek and Jurˇc´ıcˇ ek, 2016; Agarwal et al., 2018). Training a reranker allows us to take into account information from outside the model and mitigate model errors. However, rerankers can only choose a hypothesis from the final beam, which limits their potential. To quantify this, we trained the seq2seq model proposed by Duˇsek and Jurˇc´ıcˇ ek (2016), and applied it to the E2E validation set (Novikova et al., 2017b). For each instance, we recorded the point at w"
2021.eacl-main.219,W18-6322,0,0.0996605,"ng potential to improve performance. In this paper, we propose a method for manipulating which items are pruned from the beam at each stage of decoding. We then present evidence that this is a successful approach: it led to an improvement of 1.93, and 5.82 BLEU points over vanilla beam search on the E2E and WebNLG challenges, respectively. When comparing to a strong reranker, the performance of incremental beam manipulation was similar on the WebNLG dataset, whilst increasing the performance on the E2E challenge by 1.04 points. We also applied beam manipulation on top of length normalisation (Murray and Chiang, 2018), and incremental beam manipulation was able to improve its performance. 1 For larger beam sizes, the same general trends were observed. See Appendix A for beam size 10. Length normalisation (Murray and Chiang, 2018) is widely used strategy that often improves the performance of a beam search decoder, by mitigating the fact that seq2seq models are biased towards generating shorter sequences. Rather than directly using model probabilities to order the hypotheses in the beam, each probability is normalised according to the length of the hypothesis, so that shorter hypotheses are penalised. Howev"
2021.eacl-main.219,D17-1238,0,0.0329052,"Missing"
2021.eacl-main.219,W17-5525,0,0.0466451,"Missing"
2021.eacl-main.219,P02-1040,0,0.114323,"wever, using it to provide a coarse partial ordering (as described above) gave more promising results. 3.3 Training the Reranker We trained the reranker on completed hypotheses that were ranked from best to worst. The sequences were produced by generating text sequences using the seq2seq model. A beam search decoding, with a large beam size, was applied to each instance in the training set. 3 The set of hypotheses present in the final beam of the search was ranked from best to worst and recorded. The notion of the best hypothesis was simplified to the one that received the highest BLEU score (Papineni et al., 2002) against the manually written references. BLEU was chosen due to its wide adoption as an automatic evaluation measure, but any automatic metric could have been used in its place. As discussed at the end of the previous section, we need the reranker to distinguish between hypotheses that should be pruned from those that should be kept. Furthermore, for the purpose of reranking, only relative differences in BLEU score matter, not absolute values. Therefore, when generating training data, hypotheses in the bottom section of the beam (according to BLEU) were assigned the 3 In preliminary experimen"
2021.eacl-main.219,D19-1331,0,0.122274,"ith it on the WebNLG dataset. 1 The performance of NLG systems can plateau or even decrease when beam sizes larger than 10 are used, which is counter-intuitive since larger beams produce more likely sequences according to the model. For example, Duˇsek and Jurˇc´ıcˇ ek (2016) used a beam size of 10, and Asghar et al. (2017) found a size of 5 to be optimal. Decreasing performance has been found across a range of tasks including (Cohen and Beck, 2019). Moreover, it and was given by Koehn and Knowles (2017) as one of the six main challenges facing neural machine translation. To investigate this, Stahlberg and Byrne (2019) presented an exact search algorithm to find the most likely output according to a seq2seq model. However, this performed poorly compared to beam search, demonstrating that search errors (from beam search) can mask model errors (from the seq2seq model). Introduction In natural language generation (NLG), the goal is to generate text representing structured information (e.g. a database record or a meaning representation) that is both fluent and contains the right information. Sequence-to-sequence models (seq2seq) have been effective on many tasks in NLG (for example: Wen et al., 2015; Duˇsek and"
2021.eacl-main.219,W15-4639,0,0.062308,"Missing"
2021.eacl-main.219,D16-1137,0,0.0644492,"Missing"
2021.eacl-main.82,D19-1341,0,0.1747,"ining dataset. In this paper, we show that elastic weight consolidation (EWC) allows finetuning of models to mitigate biases while being less susceptible to catastrophic forgetting. In our evaluation on fact verification and NLI stress tests, we show that fine-tuning with EWC dominates standard fine-tuning, yielding models with lower levels of forgetting on the original (biased) dataset for equivalent gains in accuracy on the fine-tuning (unbiased) dataset. 1 Evidence His debut solo studio album [...] was released on 28 April 2014 Model Predicts SUPPORTS ✔ Symmetric (Counterfactual) Evidence (Schuster et al., 2019) Evidence His debut solo studio album [...] was released on 28 April 2011 Model Predicts SUPPORTS ⚠ 1) Train model using FEVER dataset only 2) Illustrate test set accuracies on FEVER + Symmetric Before mitigating biases High accuracy on original task Low accuracy on counterfactual task Mitigating biases by fine-tuning on Symmetric Higher accuracy on counterfactual task Original task exhibits catatstophic forgetting Fine-tuning with EWC on Symmetric Higher accuracy on counterfactual task Catastrophic forgetting mitigated Introduction A number of recent works have illustrated shortcomings in sen"
2021.eacl-main.82,P19-1022,0,0.0551058,"Missing"
2021.emnlp-main.678,D15-1075,0,0.229624,"you’re aged 60 or over), if you’re moving. You won’t usually get any money before you move. Decomposition AND Q0: Is it up to 13 weeks in advance? Q1: Is it up to 17 weeks in advance? Q2: Are you aged 60 or over? Q3: Are you moving? Q3 OR Q0 AND Q1 Q2 Scenario: My partner and I will be moving shortly. I had my 40th birthday last week. Direct Label: NEI Label via Decomposition: A0: NEI, A1: NEI, A2: No, A3: Yes (A3 and (A0 or (A1 and A2) )) = NEI NEI = Not Enough Information/Unresolved Figure 1: A policy with its decomposition (questions and expression tree), scenario and label.1 et al., 2009; Bowman et al., 2015), i.e. whether a scenario complies with a certain policy, this often results in poor accuracy due to the complexity of the policy descriptions, which often contain multiple clauses connected with each other (Holzenberger et al., 2020). Moreover, even though some 1 Introduction types of policy compliance detection can be tackPolicy compliance detection is the task of ensuring led via supervised classification, e.g. recognizing that a scenario conforms to a policy. It is a task hateful memes (Kiela et al., 2020) and other forms that occurs in various contexts, e.g. in ensuring of hate speech or"
2021.emnlp-main.678,N19-1300,0,0.10723,"22 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8622–8632 c November 7–11, 2021. 2021 Association for Computational Linguistics pliance detection via decomposing it to question answering (QA) and an expression tree. Each policy description is converted into a set of questions whose answers are combined in a logical form represented by the expression tree to produce the label. This allows for better handling of complex policies, and to take advantage of the large-scale datasets and models developed for question answering (Rajpurkar et al., 2016; Clark et al., 2019). In addition, the questioning of the scenario with respect to parts of the policy identifies which parts were used in order to determine compliance and which information is missing in case the scenario could not be resolved. This in turn, enhances the interpretability of the model’s decisions and can identify areas for improvement. Figure 1 shows an example application of our approach in the context of a government policy where the missing date of the move in the scenario results in its compliance with the policy being unresolved. We demonstrate the benefits of policy compliance detection via"
2021.emnlp-main.678,N16-1138,1,0.819888,"rsational question answering based on government policies. In ShARC, the task is to predict what a system should output given a policy and a conversation history, which can be an answer or a follow-up clarification question. The task is evaluated in an end-to-end fashion, measuring the system’s ability to provide the correct conversation utterance, However, policy compliance detection is neither investigated nor evaluated. Similarity search or matching has been used to match a new post to known violating posts on social media such as hoaxes, an objectionable video or image, or a hateful meme (Ferreira and Vlachos, 2016; Wang et al., 2018; fbm). For textual content, this can be compared to an entailment task. This approach requires a bank of existing violating content for each policy. By using the policy description and decomposing it into several QA tasks, breaches for new policies can be detected, as we show in our experiments. Much work has focused on learning important elements of privacy policies to assist users in understanding the terms of the policies devised by different websites. Shvartzshanider et al. (2018) uses question answering to extract different elements of privacy policies that are informa"
2021.emnlp-main.678,2021.ccl-1.108,0,0.07869,"Missing"
2021.emnlp-main.678,P19-1613,0,0.0209011,"d on a passage. Our work is similar to this work since we use policy descriptions to learn PCD and conduct evaluation in a cross-policy setting. While they answer questions from passage descriptions, we focus on learning policy compliance given a policy and a scenario. Description of relations has been used in (Obamuyide and Vlachos, 2018) to perform zero-shot relation classification. Question Decomposition Answering complex questions has been a long-standing challenge in natural language processing. Many researchers explored decomposing questions into simpler questions (Wolfson et al., 2020; Min et al., 2019; Fer7 Discussion rucci et al., 2010; Perez et al., 2020). In question deAssumptions In this work, we assume that we composition, the objective is to convert a complex can decompose policy descriptions into indepen- question into a list of inter-related sub-questions. dent questions that can be executed in parallel. While in question decomposition questions are genHowever, designing independent questions is not erated automatically, in our work, we consider them always straightforward. E.g. it could be useful given. On the other hand, the answers to the decomto have a question in an expression"
2021.emnlp-main.678,W18-5511,1,0.845643,"s is that it can be done jointly with the task of policy compliance detection, such that we generate trees that result in a higher accuracy on the downstream task. Use of Task Descriptions The use of task descriptions was recently studied in Weller et al. (2020) to answer questions based on a passage. Our work is similar to this work since we use policy descriptions to learn PCD and conduct evaluation in a cross-policy setting. While they answer questions from passage descriptions, we focus on learning policy compliance given a policy and a scenario. Description of relations has been used in (Obamuyide and Vlachos, 2018) to perform zero-shot relation classification. Question Decomposition Answering complex questions has been a long-standing challenge in natural language processing. Many researchers explored decomposing questions into simpler questions (Wolfson et al., 2020; Min et al., 2019; Fer7 Discussion rucci et al., 2010; Perez et al., 2020). In question deAssumptions In this work, we assume that we composition, the objective is to convert a complex can decompose policy descriptions into indepen- question into a list of inter-related sub-questions. dent questions that can be executed in parallel. While i"
2021.emnlp-main.678,2020.emnlp-main.713,0,0.0125875,"we use policy descriptions to learn PCD and conduct evaluation in a cross-policy setting. While they answer questions from passage descriptions, we focus on learning policy compliance given a policy and a scenario. Description of relations has been used in (Obamuyide and Vlachos, 2018) to perform zero-shot relation classification. Question Decomposition Answering complex questions has been a long-standing challenge in natural language processing. Many researchers explored decomposing questions into simpler questions (Wolfson et al., 2020; Min et al., 2019; Fer7 Discussion rucci et al., 2010; Perez et al., 2020). In question deAssumptions In this work, we assume that we composition, the objective is to convert a complex can decompose policy descriptions into indepen- question into a list of inter-related sub-questions. dent questions that can be executed in parallel. While in question decomposition questions are genHowever, designing independent questions is not erated automatically, in our work, we consider them always straightforward. E.g. it could be useful given. On the other hand, the answers to the decomto have a question in an expression tree that is a posed questions are combined in a more co"
2021.emnlp-main.678,2020.emnlp-demos.6,0,0.0189003,"Missing"
2021.emnlp-main.678,2020.tacl-1.13,0,0.014555,"answer questions based on a passage. Our work is similar to this work since we use policy descriptions to learn PCD and conduct evaluation in a cross-policy setting. While they answer questions from passage descriptions, we focus on learning policy compliance given a policy and a scenario. Description of relations has been used in (Obamuyide and Vlachos, 2018) to perform zero-shot relation classification. Question Decomposition Answering complex questions has been a long-standing challenge in natural language processing. Many researchers explored decomposing questions into simpler questions (Wolfson et al., 2020; Min et al., 2019; Fer7 Discussion rucci et al., 2010; Perez et al., 2020). In question deAssumptions In this work, we assume that we composition, the objective is to convert a complex can decompose policy descriptions into indepen- question into a list of inter-related sub-questions. dent questions that can be executed in parallel. While in question decomposition questions are genHowever, designing independent questions is not erated automatically, in our work, we consider them always straightforward. E.g. it could be useful given. On the other hand, the answers to the decomto have a questio"
2021.emnlp-main.678,P18-2124,0,0.0488722,"Missing"
2021.emnlp-main.678,D16-1264,0,0.0235362,"to address policy com8622 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8622–8632 c November 7–11, 2021. 2021 Association for Computational Linguistics pliance detection via decomposing it to question answering (QA) and an expression tree. Each policy description is converted into a set of questions whose answers are combined in a logical form represented by the expression tree to produce the label. This allows for better handling of complex policies, and to take advantage of the large-scale datasets and models developed for question answering (Rajpurkar et al., 2016; Clark et al., 2019). In addition, the questioning of the scenario with respect to parts of the policy identifies which parts were used in order to determine compliance and which information is missing in case the scenario could not be resolved. This in turn, enhances the interpretability of the model’s decisions and can identify areas for improvement. Figure 1 shows an example application of our approach in the context of a government policy where the missing date of the move in the scenario results in its compliance with the policy being unresolved. We demonstrate the benefits of policy com"
2021.emnlp-main.678,D18-1233,1,0.920197,"missing in case the scenario could not be resolved. This in turn, enhances the interpretability of the model’s decisions and can identify areas for improvement. Figure 1 shows an example application of our approach in the context of a government policy where the missing date of the move in the scenario results in its compliance with the policy being unresolved. We demonstrate the benefits of policy compliance detection via QA using a dataset that contains policies, decomposition questions and expression trees and scenarios. While the policies and scenarios in the dataset are taken from ShARC (Saeidi et al., 2018), we augment them with expression trees and answers to each question for all the scenarios and policies to create the Question Answering for Policy Compliance (QA4PC ) dataset. The results of our experiments demonstrate that we can achieve an accuracy of 0.69 for policies unseen during the training (an increase of 25 absolute points over the entailment approach) and an accuracy of 0.59 (an increase of 22 absolute points) when no in-domain data is available for training. We also show that our approach is more robust compared to entailment when faced with policies of increasing complexity. In ad"
2021.emnlp-main.678,W18-2608,0,0.0161604,"ing posts on social media such as hoaxes, an objectionable video or image, or a hateful meme (Ferreira and Vlachos, 2016; Wang et al., 2018; fbm). For textual content, this can be compared to an entailment task. This approach requires a bank of existing violating content for each policy. By using the policy description and decomposing it into several QA tasks, breaches for new policies can be detected, as we show in our experiments. Much work has focused on learning important elements of privacy policies to assist users in understanding the terms of the policies devised by different websites. Shvartzshanider et al. (2018) uses question answering to extract different elements of privacy policies that are informative to the users. Nejad et al. (2020) and Mustapha et al. (2020) assign pre-defined categories to privacy policy para2 Previous Work on Policy Compliance graphs using supervised classification. While these works aim to help users in understanding complex Policy compliance as an entailment task has been text of privacy policies, they do not aim to idenstudied by Holzenberger et al. (2020) and referred tify compliance and they mainly focus on privacy to as statutory reasoning entailment. They find that po"
2021.emnlp-main.678,N16-2013,0,0.0146421,"ompliance detection is the task of ensuring led via supervised classification, e.g. recognizing that a scenario conforms to a policy. It is a task hateful memes (Kiela et al., 2020) and other forms that occurs in various contexts, e.g. in ensuring of hate speech or abusive language on social platcorrect application of government policies (Saeidi forms (Davidson et al., 2017), this requires substanet al., 2018; Holzenberger et al., 2020), enforcing tial amounts of training data for one specific policy community guidelines on social media platforms which is expensive and time-consuming. Further(Waseem and Hovy, 2016) or the resolution of legal more, the requirement for policy-specific training cases (Zhong et al., 2020), etc. data renders supervised classification difficult to While the task can be modelled as a text-pair adapt when policies change, e.g. customs rules classification similar to textual entailment (Dagan changing with Brexit, novel forms of hate speech in the context of the Covid pandemic, etc. 1 Policy source can be found here https://www.gov. uk/housing-benefit/how-to-claim In this paper we propose to address policy com8622 Proceedings of the 2021 Conference on Empirical Methods in Natura"
2021.emnlp-main.678,2020.emnlp-main.105,0,0.0297696,"ach is offset by the gains in performance compare to the entailment approach. Nevertheless, on-boarding all the policies of an existing organisation can be time-consuming and can benefit from automation, and with human-inthe-loop approaches we can ensure the accuracy of the trees. Finally, a potential benefit of automating the generation of expression trees is that it can be done jointly with the task of policy compliance detection, such that we generate trees that result in a higher accuracy on the downstream task. Use of Task Descriptions The use of task descriptions was recently studied in Weller et al. (2020) to answer questions based on a passage. Our work is similar to this work since we use policy descriptions to learn PCD and conduct evaluation in a cross-policy setting. While they answer questions from passage descriptions, we focus on learning policy compliance given a policy and a scenario. Description of relations has been used in (Obamuyide and Vlachos, 2018) to perform zero-shot relation classification. Question Decomposition Answering complex questions has been a long-standing challenge in natural language processing. Many researchers explored decomposing questions into simpler question"
2021.fever-1.1,2020.acl-main.210,0,0.0582401,"Missing"
2021.fever-1.1,2021.naacl-main.43,0,0.0299773,"Funkquist used vanilla TF-IDF matching. Papelo and Albatross, following the baseline, combined TF-IDF matching with entity matching, and EURECOM_Fever reranked its results with a BERT model pre-trained on MSMARCO (Nguyen et al., 2016). Overall, focusing on the entities in the claim for page retrieval was found to be beneficial by the participants. Table/cell selection Table selection was often done term-based using the same approaches as for page selection, i.e. TF-IDF as in the baseline (Papelo, EURECOM_Fever) and BM25 (Bust a move!), while Martin Funkquist used the dense table retriever of Herzig et al. (2021). NCU considers all tables in retrieved documents for cell extracSentence selection In order to select sentences to used as evidence, many teams used continuous representations in order to capture semantic affinity with the claim. Bust a move! applied a three 4 Team Name Bust a move! Papelo NCU Z team EURECOM_Fever Baseline Saturday_Night_Fever Martin Funkquist Albatross METUIS ChaCha seda_kaist qmul_uou_iiith Numerical Reasoning Multi-hop Reasoning Entity Disambiguation Search terms not in Claim 0.11 0.21 0.10 0.10 0.07 0.07 0.07 0.01 0.02 0.04 0.03 0.02 0.02 0.14 0.10 0.14 0.13 0.12 0.11 0.1"
2021.fever-1.1,2020.acl-main.441,0,0.038689,"Missing"
2021.fever-1.1,2020.acl-main.398,0,0.0267587,"roach used by teams to select sentences from documents but trained on tabular data from the task (Bust a move!, Papelo, NCU, EURECOM_Fever). Cells are treated as text by linearzing them through concatenating their content and context with special markup. The teams considered as context a table’s caption, table headers, and the page name, with the latter improving scores substantially for NCU. While Bust a move! and Papelo train a separate model for sentence and cell retrieval, NCU trains a single model on the joint tabular and textual data. Martin Funkquist and METUIS used the TAPAS QA model (Herzig et al., 2020) for retrieving cells. Using continuous representations to retrieve sentences and cells from retrieved documents have generally been successful, however, using specialised methods (i.e. TAPAS) explored by participants for table retrieval and cell selection seems to be have been less successful. Instead, term-based table retrieval, and treating tables as sequences of cells was overall more successful. cases. Verdict prediction For verdict prediction, the top two teams developed models taking into account the fact that the evidence during testing will be noisy given that retrieval is imperfect."
2021.fever-1.1,D19-1341,0,0.063682,"Missing"
2021.findings-acl.104,P15-2030,0,0.0148901,"he event of interest occurs after the fifth utterance, and we remove conversations longer than the 95th percentile as these are often flame wars which may have confounding impacts. Data is split into training, development and test sets with ratios 75:10:15. 3.1 Metrics Two metrics are calculated to evaluate model performance: mean absolute error and concordance index. The mean absolute error (MAE) for a dataset 1222 of n test samples is defined as Pn |yi − yˆi | M AE = i=1 . n (9) This metric provides an easily interpretable score, and it is commonly used in evaluating regression models, e.g. Bitvai and Cohn (2015). However, MAE is not robust to outliers; large errors on a few values can outweigh many correct predictions.2 MAE is also ill-defined in the presence of censoring as there is no event time to compare against, and it cannot be used to compare model performance between different datasets. For these reasons, we also include the concordance index (Harrell Jr et al., 1996), which is concerned with ordering rather than absolute values. A pair of observations i, j is considered concordant if the prediction and the ground truth have the same inequality relation, i.e. (yi &gt; yj , yˆi &gt; yˆj ) or (yi &lt; y"
2021.findings-acl.104,2020.sigdial-1.8,0,0.0149526,"nce it is known in that case that the uncensored sample should be assigned a later event time. A disadvantage of the CI score is that it does not reflect how accurate the predictions are in absolute terms, meaning that good CI scores can be achieved with predictions in the wrong range. The two scores thus provide complementing views on model performance. 3.2 Features The features we consider are based on previous work on conversation length prediction and predicting personal attacks. These are: • Politeness (POL): The politeness strategies from Zhang et al. (2018a) as implemented in Convokit (Chang et al., 2020), which capture greetings, apologies, and saying “please”, etc. • Arrival sequences (ARR): The order in which speakers partake in the first 5 utterances, defined by Backstrom et al. (2013). • Hypergraph (HYP): Conversation structure features based on the reply tree, proposed by 2 Zhang et al. (2018b) and implemented in Convokit (Chang et al., 2020). These features capture dynamics between participants, such as engagement and reciprocity. This issue is even more pronounced in the root meansquared-error, which is another popular metric for regression (Hyndman and Koehler, 2006). • Sentiment (SEN"
2021.findings-acl.104,2021.eacl-main.173,1,0.774111,"Missing"
2021.findings-acl.104,D19-1481,0,0.0460214,"Missing"
2021.findings-acl.104,D19-5001,0,0.0146968,"ihood and a ranking loss. The ranking loss ensures that earlier events are predicted to happen before later events based on their CIF, but does not penalise models for mispredicting the times in absolute terms. The event time likelihood maximises the probability of the (i) event occurring at the right time (yT (i) ), or, in the case of censoring, it maximises the probability of the event not happening before the censoring time (1 − F (T (i) |x(i) , θ). 2.5 Previous applications in NLP A small number of NLP studies have employed techniques from survival analysis for timedependent tasks. Navaki Arefi et al. (2019) use survival regression to investigate factors that result in posts being censored on a Chinese social media platform, finding that negative sentiment is associated with shorter lifetimes. Stewart and Eisenstein (2018) use a linear Cox model to infer factors that are predictive of non-standard words falling out of use in online discourse, finding that words that appear in more linguistic contexts survive longer. Other applications include modelling fixation times in reading (Nilsson and Nivre, 2011) and evaluating dialogue systems (Deriu et al., 2020). However, none of these studies considere"
2021.findings-acl.104,P18-1125,0,0.186304,"ng in conversations as regression tasks. We focus on a family of regression techniques known as survival regression, which are commonly used in the context of healthcare and reliability engineering. We adapt these models to time-to-event prediction in conversations, using linguistic markers as features. On three datasets, we demonstrate that they outperform commonly considered text regression methods and comparable classification models. 1 Introduction The task of predicting when an event will occur in a conversation frequently arises in NLP research. For instance, Backstrom et al. (2013) and Zhang et al. (2018b) predict when a conversation thread will terminate. Danescu-Niculescu-Mizil et al. (2013) define the task of forecasting when users will cease to interact on a social network based on their language use. Although these questions naturally lend themselves to regression, this presents some difficulties: datasets may be highly skewed towards shorter durations (Zhang et al., 2018b) and samples with a longer duration can contribute inordinately to error terms during training. Furthermore, classical regression models do not explicitly consider the effect of time as distinct from other features. Th"
2021.findings-acl.104,P15-1159,0,0.0145952,"ic for regression (Hyndman and Koehler, 2006). • Sentiment (SENT): Positive and negative sentiment word counts, as per the lexicon of Liu et al. (2005), also implemented in Convokit. • Time features (TIME): Log mean time between utterances and time between last two utterances, inspired by Backstrom et al. (2013). • Utterance lengths (LEN): Log mean utterance length features, measured in tokens. • Number of participants (PART): Also used in Backstrom et al. (2013) and Zhang et al. (2018b). • Turn-taking features (TURNS): The fraction of turns and tokens contributed by the top user, inspired by Niculae et al. (2015). For the POL, SENT, TIME and LEN features, we include both the mean value throughout the conversation and the gradient of a straight-line fit to capture how the feature changes throughout it. All features are calculated up to the point of prediction, and not for the full conversation. 4 4.1 Results Experimental setup We use partly conditional training (Zheng and Heagerty, 2005) to account for using features that change over time, such as politeness, in contrast with static features like the arrival sequence. Under partly conditional training, a feature measured at time t predicts the risk of"
2021.findings-acl.104,W11-0612,0,0.032197,"mber of NLP studies have employed techniques from survival analysis for timedependent tasks. Navaki Arefi et al. (2019) use survival regression to investigate factors that result in posts being censored on a Chinese social media platform, finding that negative sentiment is associated with shorter lifetimes. Stewart and Eisenstein (2018) use a linear Cox model to infer factors that are predictive of non-standard words falling out of use in online discourse, finding that words that appear in more linguistic contexts survive longer. Other applications include modelling fixation times in reading (Nilsson and Nivre, 2011) and evaluating dialogue systems (Deriu et al., 2020). However, none of these studies considered time-to-event prediction tasks based on conversations. 3 Survival regression in conversations We evaluate survival models on two tasks, predicting conversation length and predicting when personal attacks will occur, where each conversation is a subject and the time is measured in utterances.1 1 The task of predicting when users would cease to use a platform would also have been an interesting case for this study; however, the datasets of Danescu-Niculescu-Mizil et al. (2013) are no longer available"
2021.findings-acl.104,D18-1467,0,0.0138848,". The event time likelihood maximises the probability of the (i) event occurring at the right time (yT (i) ), or, in the case of censoring, it maximises the probability of the event not happening before the censoring time (1 − F (T (i) |x(i) , θ). 2.5 Previous applications in NLP A small number of NLP studies have employed techniques from survival analysis for timedependent tasks. Navaki Arefi et al. (2019) use survival regression to investigate factors that result in posts being censored on a Chinese social media platform, finding that negative sentiment is associated with shorter lifetimes. Stewart and Eisenstein (2018) use a linear Cox model to infer factors that are predictive of non-standard words falling out of use in online discourse, finding that words that appear in more linguistic contexts survive longer. Other applications include modelling fixation times in reading (Nilsson and Nivre, 2011) and evaluating dialogue systems (Deriu et al., 2020). However, none of these studies considered time-to-event prediction tasks based on conversations. 3 Survival regression in conversations We evaluate survival models on two tasks, predicting conversation length and predicting when personal attacks will occur, w"
C16-1105,D10-1049,0,0.0352949,"ctions (e.g. “at”, “the”, “side”, “of”, “the”, “river”) followed by the ENDword action denoted by “|” (top part of Fig. 3). Content prediction actions are only used indirectly to generate the sentence; thus different sequences can result in the same sentence, as shown in the bottom part of Fig. 3. The NLG process defined in this section assumes we train two types of classifiers, one for the content prediction actions and one for word prediction actions for each attribute. If we had alignment information, it would be possible to extract data to train both types of models, either independently (Angeli et al., 2010) or jointly. However, we do not assume access to such information. Instead, we take advantage of the ability of imitation learning algorithms such as LOLS to learn with non-decomposable loss functions by only needing to evaluate complete output predictions instead of individual actions. In NLG’s case, this means we do not require explicit supervision for the content and word prediction actions, but only a way to evaluate complete generated sentences against the reference using measures such as BLEU. 3 Locally Optimal Learning to Search Here we describe how we learn the content and word classif"
C16-1105,P04-1015,0,0.028796,"ng the enumeration of all outputs, while also ameliorating the issue of error propagation. Unlike other structured prediction frameworks, LOLS is able to learn using non-decomposable loss functions. Thus it is well-suited to learning NLG systems where measures such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) that do not decompose over single words are commonly used for evaluation. Thus we are able to learn the interactions of the word currently being predicted with those to be predicted later in the sentence. We further propose a variant of LOLS using sequence correction and updates (Collins and Roark, 2004), and an exponential decay schedule (Daum´e III et al., 2009) . We compare against the systems by Duˇsek and Jurc´ıcek (2015) and Wen et al. (2015) on their respective datasets (three datasets across two domains) and show that the NLG system proposed achieves comparable results in both automatic and human evaluations.1 2 Natural language generation The NLG process takes as input a meaning representation (MR) consisting of a predicate followed by an unordered set of attributes and corresponding values; the output is a NL sentence. Figures 1 and 2 show samples from the BAGEL (Mairesse et al., 20"
C16-1105,P13-1123,0,0.0389788,"Missing"
C16-1105,P15-1044,0,0.0549497,"Missing"
C16-1105,Q13-1033,0,0.0170446,"et al. (2016) introduced an encoder-aligner-decoder model to perform content selection and surface realization without pre-aligned data. Their work employs bidirectional LSTM-RNN models, similarly to the work of Wen et al. (2015), and a coarse-to-fine aligner. Unfortunately, they do not report results in the datasets we performed our evaluation on, do not compare against Wen et al. (2015), and their code was unavailable when we were preparing this article. Imitation learning algorithms for structured prediction have been applied successfully to a variety of tasks, such as dependency parsing (Goldberg and Nivre, 2013) and dynamic feature selection (He et al., 2013). Vlachos and Clark (2014) applied a variant of DAGGER (Ross et al., 2011) to learning a semantic parser from unaligned training examples, which is the reverse task to NLG, i.e. predicting the MR given the NL utterance. To circumvent the lack of alignment information they resorted to defining a randomized expert policy similar to the heuristic one we define, but NLG poses a greater challenge since the output space is all English sentences possible given the vocabulary considered. Finally, we believe that the main benefit of our imitation learning"
C16-1105,D15-1013,0,0.0243371,"Missing"
C16-1105,D13-1152,0,0.0346938,"Missing"
C16-1105,P09-1011,0,0.0307329,"n-overlapping MRs (see section 4.2); we see that the differences between the systems are much clearer but again we must state that no safe conclusions can be drawn from that small a subset. Fluency Informativeness SF R ESTAURANT Wen et al. (2015) LOLS 3.53 3.23 4.90 5.23 SF H OTEL Wen et al. (2015) 2.38 4.65 LOLS 4.22 4.30 Table 4: Human evaluation on the non-overlapping MRs of the SF datasets. 5 Related work Apart from the works we compared against, only Konstas and Lapata (2013) and Mei et al. (2016) do not need pre-aligned data. Konstas and Lapata (2013)’s approach incorporates the work of Liang et al. (2009) that learns a generative semi-Markov model to calculate the alignments. We note that this alignment model is developed on the datasets considered, and does not generalize equally well to other datasets (Angeli et al., 2010). On the other hand, the naive alignments we infer are much simpler and we improve them by joint learning of word and content action prediction with respect to the sentence-level evaluation via BLEU and ROUGE. Concurrently, Mei et al. (2016) introduced an encoder-aligner-decoder model to perform content selection and surface realization without pre-aligned data. Their work"
C16-1105,W04-1013,0,0.0579257,"Sample MRs and corresponding NL utterances from the SF datasets. framework (Chang et al., 2015). Similar to other imitation learning algorithms for structured prediction such as DAGGER (Ross et al., 2011), LOLS reduces structured prediction to classification with greedy inference thus avoiding the enumeration of all outputs, while also ameliorating the issue of error propagation. Unlike other structured prediction frameworks, LOLS is able to learn using non-decomposable loss functions. Thus it is well-suited to learning NLG systems where measures such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) that do not decompose over single words are commonly used for evaluation. Thus we are able to learn the interactions of the word currently being predicted with those to be predicted later in the sentence. We further propose a variant of LOLS using sequence correction and updates (Collins and Roark, 2004), and an exponential decay schedule (Daum´e III et al., 2009) . We compare against the systems by Duˇsek and Jurc´ıcek (2015) and Wen et al. (2015) on their respective datasets (three datasets across two domains) and show that the NLG system proposed achieves comparable results in both automat"
C16-1105,P10-1157,0,0.449163,"Missing"
C16-1105,N16-1086,0,0.143077,"or both of these criteria.4 Table 4, shows the results if we isolate the human judgements for the subset of non-overlapping MRs (see section 4.2); we see that the differences between the systems are much clearer but again we must state that no safe conclusions can be drawn from that small a subset. Fluency Informativeness SF R ESTAURANT Wen et al. (2015) LOLS 3.53 3.23 4.90 5.23 SF H OTEL Wen et al. (2015) 2.38 4.65 LOLS 4.22 4.30 Table 4: Human evaluation on the non-overlapping MRs of the SF datasets. 5 Related work Apart from the works we compared against, only Konstas and Lapata (2013) and Mei et al. (2016) do not need pre-aligned data. Konstas and Lapata (2013)’s approach incorporates the work of Liang et al. (2009) that learns a generative semi-Markov model to calculate the alignments. We note that this alignment model is developed on the datasets considered, and does not generalize equally well to other datasets (Angeli et al., 2010). On the other hand, the naive alignments we infer are much simpler and we improve them by joint learning of word and content action prediction with respect to the sentence-level evaluation via BLEU and ROUGE. Concurrently, Mei et al. (2016) introduced an encoder-"
C16-1105,P02-1040,0,0.104025,"rom the BAGEL dataset. Figure 2: Sample MRs and corresponding NL utterances from the SF datasets. framework (Chang et al., 2015). Similar to other imitation learning algorithms for structured prediction such as DAGGER (Ross et al., 2011), LOLS reduces structured prediction to classification with greedy inference thus avoiding the enumeration of all outputs, while also ameliorating the issue of error propagation. Unlike other structured prediction frameworks, LOLS is able to learn using non-decomposable loss functions. Thus it is well-suited to learning NLG systems where measures such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) that do not decompose over single words are commonly used for evaluation. Thus we are able to learn the interactions of the word currently being predicted with those to be predicted later in the sentence. We further propose a variant of LOLS using sequence correction and updates (Collins and Roark, 2004), and an exponential decay schedule (Daum´e III et al., 2009) . We compare against the systems by Duˇsek and Jurc´ıcek (2015) and Wen et al. (2015) on their respective datasets (three datasets across two domains) and show that the NLG system proposed achieves comparable re"
C16-1105,Q14-1042,1,0.901362,"(Ross et al., 2011) frameworks roll-in using mixtures of the learned and expert policies; however, partially using the expert policy to roll-in limits those algorithms’ capability to learn from the classifiers’ mistakes. For roll-out, SEARN stochastically uses a mixture of the learned and expert policies, similarly to LOLS. DAGGER does not employ roll-outs, but uses the expert policy (also referred to as dynamic oracle) to “teach” the correct action to the classifier; this means that it cannot be used to learn using non-decomposable loss functions. Later, a variant of DAGGER, coined V-DAGGER (Vlachos and Clark, 2014), introduced roll-outs to the framework (again using a stochastic mixture of the two policies). AGGRE VAT E (Ross and Bagnell, 2014) uses a similar roll-in strategy to LOLS, but the expert policy is used exclusively for each roll-out; in the case of NLG, where the expert policy is suboptimal (see section 3.1), this may introduce noise to the classifier. 4 4.1 Data and experiments Implementation details We use the adaptive regularization of weight vectors (AROW) algorithm (Crammer et al., 2013) for costsensitive classification learning. We train separate content and word classifiers for each pr"
C16-1105,D15-1199,0,0.382687,"Missing"
C18-1029,P11-1030,0,0.0824852,"Missing"
C18-1029,E17-2068,0,0.0326572,"nd could. Figure 7: Explanation of individual predictions of Logistic Regression classifier on an IMDb62 document using LIME. The bar chart represent the weight given to the most relevant words which are also highlighted in the text. 6 Extending a Neural Model These findings are further validated by applying them to a continuous n-grams-based authorship attribution model recently proposed by Sari et al. (2017). They represented a document as a bag of n-grams features and learned the continuous representation of each feature jointly with the classifier in a shallow feed-forward neural network (Joulin et al., 2017). Sari et al. conducted experiments with three different feature choices: characters, words and their combination. The character-based model outperformed the state-of-the-art on the CCAT50 and IMDb62 datasets, while producing comparable results on the remaining two. We extend their character-based model by incorporating each feature type (style, content and hybrid) as an auxiliary feature represented in discrete form. Auxiliary features provide additional information related to the dataset characteristics. Given xaux as a normalized auxiliary features frequency vector, V is the weight applied"
C18-1029,N16-3020,0,0.0316082,"Missing"
C18-1029,rose-etal-2002-reuters,1,0.440355,"cteristics in terms of the number of authors, topic/genre and document length (see Table 1). Judgment genre # authors # total documents avg characters per document avg words per document legal judgments 3 1,342 11,957 2367 CCAT10 CCAT50 newswire 10 50 1,000 5,000 3,089 3,058 580 584 IMDb62 movie reviews 62 79,550 1,401 288 Table 1: Dataset statistics. 1 Code to reproduce the experiments is available from https://github.com/yunitata/coling2018 344 Judgment consists of legal judgments from three Australian High Court judges while both CCAT datasets are subsets of Reuters Corpus Volume 1 (RCV1) (Rose et al., 2002). The IMDb62 dataset was collected from movie reviews and message board posts of the Internet Movie database. Train/test partitions are provided for both CCAT datasets by the respective authors. For Judgment and IMDb62 we follow previous work (Seroussi et al., 2013) by using 10-fold cross validation in our experiments. We do not make use of datasets from recent authorship attribution shared task events, e.g. PAN (Juola, 2012), due to their relatively small size and fact that they provide a very small number of documents per author. 4 Dataset Analysis The aim of this analysis is to quantify the"
C18-1029,N15-1010,0,0.561458,"gorization. In some cases, where there is a clear topical distinction between the documents written by different authors, content-related features such as those used in text categorization may be effective. However, style-based features are more likely to be effective for datasets containing a more homogeneous set of topics. Previous work on feature exploration for authorship attribution, focused on the overall effectiveness of features without considering the characteristics of the datasets to which they were applied, e.g. (Grieve, 2007; Guthrie, 2008; Stamatatos, 2009; Brennan et al., 2012; Sapkota et al., 2015). A wide range of features have been applied to the authorship attribution problem and many previous studies concluded that using character n-grams is often effective, e.g. (Peng et al., 2003; Koppel et al., 2011; Schwartz et al., 2013; Sapkota et al., 2015; Sari et al., 2017; Shrestha et al., 2017). Thus, character n-grams have become the go-to features for this task to capture both an author’s topical preferences and writing style. This study explores how the characteristics of a dataset affect the usefulness of different types of features for the authorship attribution task. Experiments are"
C18-1029,E17-2043,1,0.800402,"s containing a more homogeneous set of topics. Previous work on feature exploration for authorship attribution, focused on the overall effectiveness of features without considering the characteristics of the datasets to which they were applied, e.g. (Grieve, 2007; Guthrie, 2008; Stamatatos, 2009; Brennan et al., 2012; Sapkota et al., 2015). A wide range of features have been applied to the authorship attribution problem and many previous studies concluded that using character n-grams is often effective, e.g. (Peng et al., 2003; Koppel et al., 2011; Schwartz et al., 2013; Sapkota et al., 2015; Sari et al., 2017; Shrestha et al., 2017). Thus, character n-grams have become the go-to features for this task to capture both an author’s topical preferences and writing style. This study explores how the characteristics of a dataset affect the usefulness of different types of features for the authorship attribution task. Experiments are carried out using four datasets that have previously been widely used for this task. Three types of features are considered: style, content and hybrid (a mixture of the previous two types). In contrast to previous work, this study finds that character n-grams do not perform"
C18-1029,D13-1193,0,0.609556,"tions. We apply the conclusions from our analysis to an extension of an existing approach to authorship attribution and outperform the prior state-of-the-art on two out of the four datasets used. 1 Introduction Authorship attribution plays an important role in many applications, including plagiarism detection and forensic investigation. Approaches to this problem attempt to identify a document’s author through analysis of individual’s writing style and/or topics they tend to write about. The problem has been extensively studied and a wide range of features has been explored (Stamatatos, 2013; Schwartz et al., 2013; Seroussi et al., 2013; H¨urlimann et al., 2015). However, there has been a lack of analysis of the behavior of features across multiple datasets or using a range of classifiers. Consequently, it is difficult to determine which types of features will be most useful for a particular authorship attribution dataset. Authorship attribution is a unique task which is closely related to both the representation of individuals’ writing style and text categorization. In some cases, where there is a clear topical distinction between the documents written by different authors, content-related features su"
C18-1029,E17-2106,0,0.123968,"homogeneous set of topics. Previous work on feature exploration for authorship attribution, focused on the overall effectiveness of features without considering the characteristics of the datasets to which they were applied, e.g. (Grieve, 2007; Guthrie, 2008; Stamatatos, 2009; Brennan et al., 2012; Sapkota et al., 2015). A wide range of features have been applied to the authorship attribution problem and many previous studies concluded that using character n-grams is often effective, e.g. (Peng et al., 2003; Koppel et al., 2011; Schwartz et al., 2013; Sapkota et al., 2015; Sari et al., 2017; Shrestha et al., 2017). Thus, character n-grams have become the go-to features for this task to capture both an author’s topical preferences and writing style. This study explores how the characteristics of a dataset affect the usefulness of different types of features for the authorship attribution task. Experiments are carried out using four datasets that have previously been widely used for this task. Three types of features are considered: style, content and hybrid (a mixture of the previous two types). In contrast to previous work, this study finds that character n-grams do not perform equally well in all data"
C18-1283,D15-1075,0,0.0205467,"e is likely to appear in a graph by modeling the task as a path ranking problem (Lao et al., 2011). The truth verdict is derived from the cost of traversing a path between the two entities under transitive closure, weighted by the degree of connectedness of the path. Nakashole and Mitchell (2014) combine linguistic features on the subjectivity of the language used with the co-occurrence of a triple with other triples from the same topic, a form of collective classification. Ferreira and Vlachos (2016) modeled fact checking as a form of Recognizing Textual Entailment (RTE) (Dagan et al., 2009; Bowman et al., 2015), predicting whether a premise, typically (part of) a news article, is for, against, or observing a given claim. The same type of model was used by most of the 50 participating teams in the Fake News Challenge (Pomerleau and Rao, 2017), including most of the top entries (Riedel et al., 2017; Hanselowski et al., 2017). The RTE-based models assume that the textual evidence to fact check a claim is given. Thus they are inapplicable in cases where this is not provided (as in HeroX), or it is a rather large textual resource such as Wikipedia (as in FEVER). For the latter, Thorne et al. (2018) devel"
C18-1283,W17-4210,0,0.0889642,"that we can learn from to develop natural language technologies for fact checking. However, because these are conducted in a siloed manner and often without wider awareness of the issue, there is inconsistency in the definitions and lack of communication between disciplines, impeding understanding and further progress. For example, the classification of whether an article title can be supported by content in the body has been interchangeably referred to as both fake news detection (Pomerleau and Rao, 2017), stance classification (Ferreira and Vlachos, 2016) and incongruent headline detection (Chesney et al., 2017). In this survey we aim to unify the definitions presented in related works and identify common concepts, datasets and modelling approaches. In this process, we summarize key results, highlight limitations and propose some open research challenges. In particular, we detail the different types of evidence used by different approaches and how they affect the NLP requirements on the systems developed. For example, many approaches rely on textual entailment/natural language inference (Dagan et al., 2009), while others rely on knowledge base construction techniques (Ji and Grishman, 2011). The stru"
C18-1283,doddington-etal-2004-automatic,0,0.158718,"precisely the source of a quote, its content, and the event at which it supposedly occurred. 1 Note that Mantzarlis and Silverman disagree on whether fact-checking should be considered a subset of verification, or just overlapping with it. While this is in an important question, we do not address it in this survey as it does not affect our analysis. 2 http://www.politifact.com/ 3 http://www.fullfact.org/ 3348 While some of these claims could be represented as triples, they typically require more complex representations; for example, events typically need to be represented with multiple slots (Doddington et al., 2004) to denote their various participants. Regardless of whether textual claims are verified via a subject-predicate-object triple representation or as text, it is often necessary to disambiguate the entities and their properties. For example, the claim ‘Beckham played for Manchester United’ is true for the soccer player ‘David Beckham’, but not true (at the time of writing) for the American football player ‘Odel Beckham Jr’. Correctly identifying, disambiguating and grounding entities is the task of Named Entity Linking (McNamee and Dang, 2009). While this must be performed explicitly if converti"
C18-1283,I13-1039,0,0.0147006,"checking. As mentioned earlier, Long et al. (2017) introduced a notion of credit history which increases the classification accuracy for fake news detection. However, this notion doesn’t account for which topics the originator lies about. Furthermore, the assumption that each source has an overall trustworthiness score to be attached to every claim from there is not a valid one, since inaccurate information may be found even on the most reputable of sources. An alternative is to consider the compatibility of a claim with respect to the originator’s profile. This remains an open research area. Feng and Hirst (2013) perform the inverse of this task for deceptive language detection in product reviews by creating an average profile for the targets (products) and using the distance between a review and the target as a feature. The shortcomings of this method are that number of reviews are required for each target. Considering the tasks of verifying automatically extracted information or fact checking for politics, for a new topic the challenge is that there may be insufficient data to create a profile for it. P´erez-Rosas and Mihalcea (2015) identify author characteristics (such as age and gender) that infl"
C18-1283,P12-2034,0,0.0273733,"psycholinguistic cues to improve classification accuracy. Mihalcea and Strapparava (2009) found that truthful texts were more likely to contain words belonging to the ‘optimistic’ LIWC class such as ‘best’, ‘hope’, and ‘determined’. This is corroborated by the study of sentiment in deceptive texts (Ott et al., 2013) which also identified that texts with negative sentiment were more likely to be deceptive. These 3353 feature classes however may be an artifact of the data generation process as crowd-sourced volunteers were first asked to write an honest text and rewrite it to make it deceptive. Feng et al. (2012) detect deceptive texts and customer-generated reviews through the use of syntactic style rather word-based content. Non-terminal nodes of the constituency parse trees are used as features in conjunction with a lexical model to increase the accuracy over using words alone. Hai et al. (2016) identify deceptive reviews also using lexical features. However, rather than relying on labeled data, the authors induce labels over an unlabeled dataset through a semi-supervised learning approach that exploits a minimal amount labeled data from related tasks in a multi-task learning set up. Even though li"
C18-1283,N16-1138,1,0.681583,"body of related works with different motivations and approaches that we can learn from to develop natural language technologies for fact checking. However, because these are conducted in a siloed manner and often without wider awareness of the issue, there is inconsistency in the definitions and lack of communication between disciplines, impeding understanding and further progress. For example, the classification of whether an article title can be supported by content in the body has been interchangeably referred to as both fake news detection (Pomerleau and Rao, 2017), stance classification (Ferreira and Vlachos, 2016) and incongruent headline detection (Chesney et al., 2017). In this survey we aim to unify the definitions presented in related works and identify common concepts, datasets and modelling approaches. In this process, we summarize key results, highlight limitations and propose some open research challenges. In particular, we detail the different types of evidence used by different approaches and how they affect the NLP requirements on the systems developed. For example, many approaches rely on textual entailment/natural language inference (Dagan et al., 2009), while others rely on knowledge base"
C18-1283,N18-1175,0,0.0243816,"o known-bad nodes rather than the information content. An alternative is Knowledge-based Trustworthiness scoring (Dong et al., 2015), which allows predicting whether the facts extracted from a given document page are likely to be accurate given the method used to extract the facts and the website in which the document is published. Common Sense Reasoning Fact checking requires the ability to reason about arguments with common sense knowledge. This requires developing systems that go beyond recognizing semantic phenomena more complex than those typically considered in textual entailment tasks. Habernal et al. (2018) introduced a new task and dataset for predicting which implicit warrant (the rationale of an argument) is required to support a claim from a given premise. Angeli and Manning (2014) proposed a method of extracting common sense knowledge from WordNet for reasoning about common sense knowledge using Natural Logic and evaluated their approach on a subset of textual entailment problems in the FraCaS test suite (Cooper et al., 1996). It is important to build systems that can reason about both explicit world knowledge and implicit common sense knowledge is an essential step towards automating fact"
C18-1283,D16-1187,0,0.0173562,"texts (Ott et al., 2013) which also identified that texts with negative sentiment were more likely to be deceptive. These 3353 feature classes however may be an artifact of the data generation process as crowd-sourced volunteers were first asked to write an honest text and rewrite it to make it deceptive. Feng et al. (2012) detect deceptive texts and customer-generated reviews through the use of syntactic style rather word-based content. Non-terminal nodes of the constituency parse trees are used as features in conjunction with a lexical model to increase the accuracy over using words alone. Hai et al. (2016) identify deceptive reviews also using lexical features. However, rather than relying on labeled data, the authors induce labels over an unlabeled dataset through a semi-supervised learning approach that exploits a minimal amount labeled data from related tasks in a multi-task learning set up. Even though linguistic content, emotive language and syntax are useful indicators for detecting deceit, the truthfulness of a statement depends also on the context. Without considering these factors these approaches cannot be used to fact check information alone. Rumor Detection Rumor detection (Qazvinia"
C18-1283,P17-2032,0,0.0247258,"g most of the top entries (Riedel et al., 2017; Hanselowski et al., 2017). The RTE-based models assume that the textual evidence to fact check a claim is given. Thus they are inapplicable in cases where this is not provided (as in HeroX), or it is a rather large textual resource such as Wikipedia (as in FEVER). For the latter, Thorne et al. (2018) developed a pipelined approach in which the RTE component is preceded by a document retrieval and a sentence selection component. There is also work focusing exclusively on retrieving sentence-level evidence from related documents for a given claim (Hua and Wang, 2017). Vlachos and Riedel (2015) and Thorne and Vlachos (2017) use distantly supervised relation extraction (Mintz et al., 2009) to identify surface patterns in text which describe relations between two entities in a knowledge graph. Because these fact checking approaches only focus on statistical properties of entities, identification of positive training examples is simplified to searching for sentences containing numbers that are approximately equal to the values stored in the graph. Extending this approach to entity-entity relations would pose different challenges, as a there may be many relati"
C18-1283,P11-1115,0,0.0550976,"detection (Chesney et al., 2017). In this survey we aim to unify the definitions presented in related works and identify common concepts, datasets and modelling approaches. In this process, we summarize key results, highlight limitations and propose some open research challenges. In particular, we detail the different types of evidence used by different approaches and how they affect the NLP requirements on the systems developed. For example, many approaches rely on textual entailment/natural language inference (Dagan et al., 2009), while others rely on knowledge base construction techniques (Ji and Grishman, 2011). The structure of this survey is as follows: we first discuss fact checking in the context of journalism as this provides definitions and distinctions on key terminology that will be used throughout in the remainder. We then proceed to discussing previous research in automated fact checking in terms of what inputs they expect, what outputs they return and the evidence used in this process. Following this, we provide an overview of the most commonly used datasets and the models developed and evaluated on them. Subsequently, we discuss work related to automated fact checking and conclude by pro"
C18-1283,D11-1049,0,0.0255386,"tation of text classification approaches is that fact checking a claim requires additional world knowledge, typically not provided with the claim itself. While language can indicate whether a sentence is factual (Nakashole and Mitchell, 2014), credible sounding sentences may also be inherently false. Text classification on claims alone has been used for the related task of detecting fact-check worthy claims (Hassan et al., 2017a). Ciampaglia et al. (2015) use network analysis to predict whether an unobserved triple is likely to appear in a graph by modeling the task as a path ranking problem (Lao et al., 2011). The truth verdict is derived from the cost of traversing a path between the two entities under transitive closure, weighted by the degree of connectedness of the path. Nakashole and Mitchell (2014) combine linguistic features on the subjectivity of the language used with the co-occurrence of a triple with other triples from the same topic, a form of collective classification. Ferreira and Vlachos (2016) modeled fact checking as a form of Recognizing Textual Entailment (RTE) (Dagan et al., 2009; Bowman et al., 2015), predicting whether a premise, typically (part of) a news article, is for, ag"
C18-1283,P17-1015,0,0.0224678,"ring the strong interest in it. While recently published large scale resources such as FEVER can stimulate progress, they only consider simple short sentences (8 words long on average). Thus, there is scope to fact check compound information or complex sentences, and scale up to fact checking at the document level. Furthermore, in FEVER the justification for the labels is restricted to sentences selected from Wikipedia. This is much unlike the rationales produced by human fact checkers, who synthesize information. The generation of such rationales has attracted attention only recently in NLP (Ling et al., 2017), and automated fact checking could provide an ideal testbed to develop it further. While text is often used to make a claim, often the evidence need for fact checking appears in other modalities, such as images and videos. Given the recent interest in multi-modal NLP, we argue that this would be an important direction for future research, especially when considering that a lot of the verification efforts in journalism are focused on identifying forged images and footage. Finally, it is important to acknowledge that the complexity of the fact checking conducted by journalists is for the moment"
C18-1283,I17-2043,0,0.309354,"elations between the same pair entities that would need to be accurately distinguished for this approach to be used. A popular type of model often employed by fact checking organizations in their process is that of matching a claim with existing, previously fact checked ones. This reduces the task to sentence-level textual similarity as suggested by Vlachos and Riedel (2014) and implemented in ClaimBuster (Hassan et al., 2017b), Truthteller by The Washington Post7 and one of the two modes of Full Fact’s Live platform.8 However, it can only be used to fact check repeated or paraphrased claims. Long et al. (2017) extend the models produced by Wang (2017) and improve accuracy of a simple fact checking system through more extended profiling of the originators of the claims. The most influential feature in this model is the credit history of the originator, a metric describing how often the originator’s claims are classified as false. This feature introduces a bias in the model that fits with the adage “never 7 https://www.knightfoundation.org/articles/debuting-truth-teller-washington-postreal-time-lie-detection-service-your-service-not-quite-yet 8 https://fullfact.org/blog/2017/jun/automated-fact-checki"
C18-1283,P09-2078,0,0.121055,"ing to divisive topics (such as ‘liberals’ or ‘Trump’). The authors apply a similar model to the prediction of claims made by politicians from claims collected from Politifact. The addition of the LIWC lexicon which provides an indication of emotive tone and authenticity marginally improved the classification accuracy for simple lexical models. Deceptive Language Detection There are linguistic cues and features in written text that are useful in identifying deceptive language (Zhou et al., 2004). In the context of detecting deceptive user-generated content - a specific form of disinformation, Mihalcea and Strapparava (2009) use a simple lexical classification model without further feature engineering. Analysis of the model identifies a number of word classes of the LIWC lexicon which pertain only to the deceptive texts. Ott et al. (2011) incorporate the use of psycholinguistic cues to improve classification accuracy. Mihalcea and Strapparava (2009) found that truthful texts were more likely to contain words belonging to the ‘optimistic’ LIWC class such as ‘best’, ‘hope’, and ‘determined’. This is corroborated by the study of sentiment in deceptive texts (Ott et al., 2013) which also identified that texts with ne"
C18-1283,P09-1113,0,0.0145749,"dence to fact check a claim is given. Thus they are inapplicable in cases where this is not provided (as in HeroX), or it is a rather large textual resource such as Wikipedia (as in FEVER). For the latter, Thorne et al. (2018) developed a pipelined approach in which the RTE component is preceded by a document retrieval and a sentence selection component. There is also work focusing exclusively on retrieving sentence-level evidence from related documents for a given claim (Hua and Wang, 2017). Vlachos and Riedel (2015) and Thorne and Vlachos (2017) use distantly supervised relation extraction (Mintz et al., 2009) to identify surface patterns in text which describe relations between two entities in a knowledge graph. Because these fact checking approaches only focus on statistical properties of entities, identification of positive training examples is simplified to searching for sentences containing numbers that are approximately equal to the values stored in the graph. Extending this approach to entity-entity relations would pose different challenges, as a there may be many relations between the same pair entities that would need to be accurately distinguished for this approach to be used. A popular t"
C18-1283,P14-1095,0,0.470928,"ghlight the differences between the task definitions used in previous research in the following axes: input, i.e. what is being fact checked, output, i.e. what kinds of verdicts are expected, and the evidence used in the fact checking process. 3.1 Inputs We first consider the inputs to automated fact checking approaches as their format and content influences the types of evidence used in this process. A frequently considered input to fact checking approaches is subject-predicate-object triples, e.g. (London, capital of, UK), and is popular across different research communities, including NLP (Nakashole and Mitchell, 2014), data mining (Ciampaglia et al., 2015) and Web search (Bast et al., 2015). The popularity of triples as input stems from the fact that they facilitate fact checking against (semi-)structured knowledge bases such Freebase (Bollacker et al., 2008). However, it is important to acknowledge that approaches using triples as input implicitly assume a non-trivial level of processing in order to convert text, speech or other forms of claims into triples, a task falling under the broad definition of natural language understanding (Woods, 1973). A second type of input often considered in automated fact"
C18-1283,P11-1032,0,0.0377351,"cation of emotive tone and authenticity marginally improved the classification accuracy for simple lexical models. Deceptive Language Detection There are linguistic cues and features in written text that are useful in identifying deceptive language (Zhou et al., 2004). In the context of detecting deceptive user-generated content - a specific form of disinformation, Mihalcea and Strapparava (2009) use a simple lexical classification model without further feature engineering. Analysis of the model identifies a number of word classes of the LIWC lexicon which pertain only to the deceptive texts. Ott et al. (2011) incorporate the use of psycholinguistic cues to improve classification accuracy. Mihalcea and Strapparava (2009) found that truthful texts were more likely to contain words belonging to the ‘optimistic’ LIWC class such as ‘best’, ‘hope’, and ‘determined’. This is corroborated by the study of sentiment in deceptive texts (Ott et al., 2013) which also identified that texts with negative sentiment were more likely to be deceptive. These 3353 feature classes however may be an artifact of the data generation process as crowd-sourced volunteers were first asked to write an honest text and rewrite i"
C18-1283,N13-1053,0,0.014805,"ic form of disinformation, Mihalcea and Strapparava (2009) use a simple lexical classification model without further feature engineering. Analysis of the model identifies a number of word classes of the LIWC lexicon which pertain only to the deceptive texts. Ott et al. (2011) incorporate the use of psycholinguistic cues to improve classification accuracy. Mihalcea and Strapparava (2009) found that truthful texts were more likely to contain words belonging to the ‘optimistic’ LIWC class such as ‘best’, ‘hope’, and ‘determined’. This is corroborated by the study of sentiment in deceptive texts (Ott et al., 2013) which also identified that texts with negative sentiment were more likely to be deceptive. These 3353 feature classes however may be an artifact of the data generation process as crowd-sourced volunteers were first asked to write an honest text and rewrite it to make it deceptive. Feng et al. (2012) detect deceptive texts and customer-generated reviews through the use of syntactic style rather word-based content. Non-terminal nodes of the constituency parse trees are used as features in conjunction with a lexical model to increase the accuracy over using words alone. Hai et al. (2016) identif"
C18-1283,D15-1133,0,0.0297274,"Missing"
C18-1283,D11-1147,0,0.0698489,". (2016) identify deceptive reviews also using lexical features. However, rather than relying on labeled data, the authors induce labels over an unlabeled dataset through a semi-supervised learning approach that exploits a minimal amount labeled data from related tasks in a multi-task learning set up. Even though linguistic content, emotive language and syntax are useful indicators for detecting deceit, the truthfulness of a statement depends also on the context. Without considering these factors these approaches cannot be used to fact check information alone. Rumor Detection Rumor detection (Qazvinian et al., 2011) is the task of identifying unverified reports circulating on social media. A prediction is typically based on language subjectivity and growth of readership through a social network. While these are important factors to consider, a sentence can be true or false regardless of whether it is a rumor (Zubiaga et al., 2018). Speaker Profiling Identifying claims that do not fit with the profile of the originator may provide insights as to whether the information is truthful or not. Furthermore, determining which topics the originator is truthful about may allow for generation of a risk-based approa"
C18-1283,D17-1317,0,0.20781,"y performing relation extraction (Vlachos and Riedel, 2015) or through a supervised sentence-level classification (Hassan et al., 2015b). 3.2 Sources of evidence The type of evidence that is used for fact checking influences the model and the types of outputs that the fact checking system can produce. For example, whether the output is a label or whether a justification can be produced depends largely on the information available to the fact checking system. We first consider task formulations that do not use any evidence beyond the claim itself when predicting its veracity such as the one by Rashkin et al. (2017). In these instances, surface-level linguistic features in the claims are associated with the predicted veracity. We contrast this to how journalists work when fact checking, where they must find knowledge relating to the fact and evaluate the claim given the evidence and context when making the decision as to whether a claim is true or false. The predictions made in task formulations that do not consider evidence beyond the claim are based on surface patterns of how the claim is written rather than considering the current state of the world. Wang (2017) incorporate additional metadata in fact"
C18-1283,E17-3010,1,0.91927,"ounding the claim, the additional context can act as a prior to improve the classification accuracy, and can be used as part of the justification of a verdict. Knowledge graphs provide a rich collection of structured canonical information about the world stored in a machine readable format that could support the task of fact checking. We observe two types of formulations using this type of evidence. The first approach is to identify/retrieve the element in the knowledge graph that provides the information supporting or refuting the claim at question. For example, Vlachos and Riedel (2015) and Thorne and Vlachos (2017) identify the subject-predicate-object triples from small knowledge graphs to fact check numerical claims. Once the relevant triple had been found, a truth label is computed through a rule based approach that considers the error between the claimed values and the retrieved values from the graph. The key limitation in using knowledge graphs as evidence in this fashion is that it assumes that the true facts relevant to the claim are present in them. However, it is not feasible to capture and store every conceivable fact in the graph in advance of knowing the claim. The alternative use of a knowl"
C18-1283,N18-1074,1,0.78019,"s, policy documents, verified news and scientific journals contain information that can be used to fact check claims. Ferreira and Vlachos (2016) use article headlines (single sentences) as evidence to predict whether an article is for, against or observing a claim. The Fake News Challenge (Pomerleau and Rao, 2017) also formulated this part of the fact checking process in the same way, but in contrast to (Ferreira and Vlachos, 2016), entire documents are used as evidence, thus allowing for evidence from multiple sentences to be combined. 3349 The Fact Extraction and VERification (FEVER) task (Thorne et al., 2018) requires combining information from multiple documents and sentences for fact checking. Unlike the aforementioned works which use text as evidence, the evidence is not given but must be retrieved from Wikipedia, a large corpus of encyclopedic articles. Scaling up even further, the triple scoring task of the WSDM cup (Bast et al., 2017) required participants to assess knowledge graph triples considering both Wikipedia and a portion of the web-scale ClueWeb dataset (Gabrilovich et al., 2013). A different source of text-based evidence is repositories of previously fact checked claims (Hassan et"
C18-1283,W14-2508,1,0.726456,"lated Work section. 3.3 Output The simplest model for fact checking is to label a claim as true or false as a binary classification task (Nakashole and Mitchell, 2014). However, we must also consider that it is possible in natural language to be purposefully flexible with the degree of truthfulness of the information expressed or express a particular bias using true facts. Journalistic fact checking agencies such as Politifact model the degree of truthfulness on a multi-point scale (ranging from true, mostly-true, half-true, etc). Rather than modeling fact checking as a binary classification, Vlachos and Riedel (2014) suggested modeling this degree of truthfulness as an ordinal classification task. However, the reasoning behind why the manual fact checking agencies have applied these more fine-grained labels is complex, sometimes inconsistent4 and likely to be difficult to capture and express in our models. Wang et al. (2017) and Rashkin et al. (2017) expect as output multiclass labels following the definitions by the journalists over a multi-point scale but ignoring the ordering among them. The triple scoring task of the WSDM cup (Bast et al., 2017) expected as output triples scored within a numerical ran"
C18-1283,D15-1312,1,0.915151,"nd Dang, 2009). While this must be performed explicitly if converting a claim to a subject-predicate-object triple with reference to a knowledge base, it may also be performed implicitly through the retrieval of appropriate textual evidence if fact checking against textual sources. Finally, there have been approaches that consider an entire document as their input. These approaches must first identify the claims and then fact check them. This increases the complexity of the task, as approaches are required to extract the claims, either in the form of triples by performing relation extraction (Vlachos and Riedel, 2015) or through a supervised sentence-level classification (Hassan et al., 2015b). 3.2 Sources of evidence The type of evidence that is used for fact checking influences the model and the types of outputs that the fact checking system can produce. For example, whether the output is a label or whether a justification can be produced depends largely on the information available to the fact checking system. We first consider task formulations that do not use any evidence beyond the claim itself when predicting its veracity such as the one by Rashkin et al. (2017). In these instances, surface-level li"
C18-1283,P17-2067,0,0.389455,"s veracity such as the one by Rashkin et al. (2017). In these instances, surface-level linguistic features in the claims are associated with the predicted veracity. We contrast this to how journalists work when fact checking, where they must find knowledge relating to the fact and evaluate the claim given the evidence and context when making the decision as to whether a claim is true or false. The predictions made in task formulations that do not consider evidence beyond the claim are based on surface patterns of how the claim is written rather than considering the current state of the world. Wang (2017) incorporate additional metadata in fact checking such as the originator of the claim, speaker profile and the media source in which the claim is presented. While these do not provide evidence grounding the claim, the additional context can act as a prior to improve the classification accuracy, and can be used as part of the justification of a verdict. Knowledge graphs provide a rich collection of structured canonical information about the world stored in a machine readable format that could support the task of fact checking. We observe two types of formulations using this type of evidence. Th"
C18-1283,H05-1044,0,0.00729295,"explicit world knowledge and implicit common sense knowledge is an essential step towards automating fact checking. Subjectivity and Emotive Language Rashkin et al. (2017) assess the reliability of entire news articles by predicting whether the document originates from a website classified as Hoax, Satire or Propaganda. This work is an instance of subjective language detection and does not represent evidence-based fact checking. The authors used supervised classifiers augmented with lexicons including the Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2015), a sentiment lexicon (Wilson et al., 2005), a hedging lexicon (Hyland, 2015), and a novel ‘dramatic’ language lexicon to identify emotive, subjective and sensational language in the article bodies. Furthermore, analysis of the lexical features using a logistic regression classifier shows that the highest weighted (most distinguishing) features for the unreliable sources included the use of hedge words (such as ‘reportedly’) or words pertaining to divisive topics (such as ‘liberals’ or ‘Trump’). The authors apply a similar model to the prediction of claims made by politicians from claims collected from Politifact. The addition of the L"
C18-1283,D14-1059,0,\N,Missing
D09-1071,D07-1043,0,0.0448775,"Missing"
D09-1071,N03-1028,0,0.00683801,"Missing"
D09-1071,P07-1035,0,0.0583576,"o obtain a new sample. method) has a reasonable level of performance, it should improve on the performance of a system that does not use PoS tags. Moreover, if the performance is very good indeed, it should get close to the performance of a system that uses real PoS tags, provided either by human annotation or by a good supervised system. Similar extrinsic evaluation was performed by Biemann et al. (2007). It is of interest to compare the results between the clustering evaluation and the extrinsic one. A different approach in evaluating nonparametric Bayesian models for NLP is statesplitting (Finkel et al., 2007; Liang et al., 2007). In this setting, the model is used in order to refine existing annotation of the dataset. While this approach can provide us with some insights and interpretable results, the use of existing annotation influences the output of the model. In this work, we want to verify whether the output of the iHMM (without any supervision) can be used instead of that of a supervised system. 5 First, we present results using clustering evaluation measures which appear in the figures of Table 1. The three runs exhibit different behavior. The number of states reached by the iHMM with fixe"
D09-1071,W00-0726,0,0.111597,"Missing"
D09-1071,D08-1036,0,0.554587,"ing approaches. These learning methods rely on the availability of labeled datasets which are usually produced by expensive manual annotation. For some tasks, we have the choice to use unsupervised learning approaches. While they do not necessarily achieve the same level of performance, they are appealing as unlabeled data is usually abundant. In particular, for the purpose of exploring new domains and languages, obtainining labeled material can be prohibitively expensive and unsupervised learning methods are a very attractive choice. Recent work (Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008) explored the task of part-of-speech tagging 678 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 678–687, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP The rest of the paper is structured as follows: in section 2 we introduce the iHMM as a nonparametric version of the Bayesian HMM used in previous work on unsupervised PoS tagging. Then, in section 3 we describe some details of our implementation of the iHMM. In section 4 we present a variety of evaluation metrics to compare our results with previous work. Finally, in section 5 we report our expe"
D09-1071,W09-0210,1,0.884455,"Missing"
D09-1071,P07-1094,0,0.520461,"tackled using supervised learning approaches. These learning methods rely on the availability of labeled datasets which are usually produced by expensive manual annotation. For some tasks, we have the choice to use unsupervised learning approaches. While they do not necessarily achieve the same level of performance, they are appealing as unlabeled data is usually abundant. In particular, for the purpose of exploring new domains and languages, obtainining labeled material can be prohibitively expensive and unsupervised learning methods are a very attractive choice. Recent work (Johnson, 2007; Goldwater and Griffiths, 2007; Gao and Johnson, 2008) explored the task of part-of-speech tagging 678 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 678–687, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP The rest of the paper is structured as follows: in section 2 we introduce the iHMM as a nonparametric version of the Bayesian HMM used in previous work on unsupervised PoS tagging. Then, in section 3 we describe some details of our implementation of the iHMM. In section 4 we present a variety of evaluation metrics to compare our results with previous work. Finally, in sect"
D09-1071,D07-1031,0,0.533893,"rsity of Cambridge jv249@cam.ac.uk Andreas Vlachos Computer Laboratory University of Cambridge av308@cl.cam.ac.uk Abstract (PoS) using unsupervised Hidden Markov Models (HMMs) with encouraging results. PoS tagging is a standard component in many linguistic processing pipelines, so any improvement on its performance is likely to impact a wide range of tasks. It is important to point out that a completely unsupervised learning method will discover the statistics of a dataset according to a particular model choice but these statistics might not correspond exactly to our intuition about PoS tags. Johnson (2007) and Gao & Johnson (2008) assume that words are generated by a hidden Markov model and find that the resulting states strongly correlate with POS tags. Nonetheless, identifying the HMM states with appropriate POS tags is hard. Because many evaluation methods often require POS tags (rather than HMM states) this identification problem makes unsupervised systems difficult to evaluate. One potential solution is to add a small amount of supervision as in Goldwater & Griffiths (2007) who assume a dictionary of frequent words associated with possible PoS tags extracted from a labeled corpus. Although"
D09-1071,D07-1072,0,0.0612619,"Missing"
D13-1143,D07-1090,0,0.0166611,"te and store the denominators easily during training, so that we do not need to sum over the vocabulary each time we evaluate the estimator. We refer to this model as the order N unlabelled dependency language model. As is the case for n-gram language models, even for low values of N, we will often encounter sequences (A(N −1) (w), w) which were not observed in training. In order to avoid assigning zero probability to the entire sentence, we need to use a smoothing method. We can use any of the smoothing methods used for n-gram language models. For simplicity, we use stupid backoff smoothing (Brants et al., 2007). 3 Labelled Dependency Language Models We assumed above that the words are generated independently from the grammatical relations. However, we are likely to ignore valuable information in doing so. To illustrate this point, consider the following pair of sentences: dobj det nsubj You ate det An an nsubj apple apple dobj ate you The dependency trees of the two sentences are very similar, with only the grammatical relations between ate and its arguments differing. The unlabelled dependency language model will assign the same probability to both of the sentences as it ignores the labels of gramm"
D13-1143,P01-1017,0,0.113314,"ity of the next word on the linear trigram context, as well as some part of the dependency graph information relating to the words on its left. The language models we propose are far simpler to train and compute. A somewhat similar model to our unlabelled dependency language model was proposed in Graham and van Genabith (2010). However they seem to have used different probability estimators which ignore the fact that each node in the dependency tree can have multiple children. Other research on syntactic language modelling has focused on using phrase structure grammars (Pauls and Klein, 2012; Charniak, 2001; Roark, 2001; Hall and Johnson, 2003). The linear complexity of deterministic dependency parsing makes dependency language models such as ours more scalable than these approaches. The most similar task to sentence completion is lexical substitution (McCarthy and Navigli, 2007). The main difference between them is that in the latter the word to be substituted provides a very important clue in choosing the right candidate, while in sentence completion this is not available. Another related task is selectional preference modeling (S´eaghdha, 2010; Ritter et al., 2010), where the aim is to assess"
D13-1143,W10-3815,0,0.048691,"Missing"
D13-1143,C04-1010,0,0.0223743,"Missing"
D13-1143,nivre-etal-2006-maltparser,0,0.0901575,"Missing"
D13-1143,P11-1027,0,0.0510089,"Missing"
D13-1143,P12-1101,0,0.0196624,"nditioning the probability of the next word on the linear trigram context, as well as some part of the dependency graph information relating to the words on its left. The language models we propose are far simpler to train and compute. A somewhat similar model to our unlabelled dependency language model was proposed in Graham and van Genabith (2010). However they seem to have used different probability estimators which ignore the fact that each node in the dependency tree can have multiple children. Other research on syntactic language modelling has focused on using phrase structure grammars (Pauls and Klein, 2012; Charniak, 2001; Roark, 2001; Hall and Johnson, 2003). The linear complexity of deterministic dependency parsing makes dependency language models such as ours more scalable than these approaches. The most similar task to sentence completion is lexical substitution (McCarthy and Navigli, 2007). The main difference between them is that in the latter the word to be substituted provides a very important clue in choosing the right candidate, while in sentence completion this is not available. Another related task is selectional preference modeling (S´eaghdha, 2010; Ritter et al., 2010), where the"
D13-1143,P10-1044,0,0.0239543,"Missing"
D13-1143,J01-2004,0,0.0600162,"word on the linear trigram context, as well as some part of the dependency graph information relating to the words on its left. The language models we propose are far simpler to train and compute. A somewhat similar model to our unlabelled dependency language model was proposed in Graham and van Genabith (2010). However they seem to have used different probability estimators which ignore the fact that each node in the dependency tree can have multiple children. Other research on syntactic language modelling has focused on using phrase structure grammars (Pauls and Klein, 2012; Charniak, 2001; Roark, 2001; Hall and Johnson, 2003). The linear complexity of deterministic dependency parsing makes dependency language models such as ours more scalable than these approaches. The most similar task to sentence completion is lexical substitution (McCarthy and Navigli, 2007). The main difference between them is that in the latter the word to be substituted provides a very important clue in choosing the right candidate, while in sentence completion this is not available. Another related task is selectional preference modeling (S´eaghdha, 2010; Ritter et al., 2010), where the aim is to assess the plausibi"
D13-1143,P10-1045,0,0.0212009,"Missing"
D13-1143,W12-2704,0,0.503188,"easoning sections of standardised tests such as the Scholastic Aptitude Test (SAT) feature problems where a partially complete sentence is given and the candidate must choose the word or phrase from a list of options which completes the sentence in a logically consistent way. Sentence completion is a challenging semantic modelling problem. Systematic approaches for solving such problems require models that can judge the global coherence of sentences. Such measures of global coherence may prove to be useful in various applications, including machine translation and natural language generation (Zweig and Burges, 2012). In this paper we tackle sentence completion using language models based on dependency grammar. These models are similar to standard n-gram language models, but instead of using the linear ordering of the words in the sentence, they generate words along paths in the dependency tree of the sentence. Unlike other approaches incorporating syntax into language models (e.g., Chelba et al., 1997), our models are relatively easy to train and estimate, and can exploit standard smoothing methods. We apply them to the Microsoft Research Sentence Completion Challenge (Zweig and Burges, 2012) and show an"
D13-1143,P12-1063,0,0.250534,"Missing"
D15-1086,P12-2011,0,0.010186,"t al., 2011), improving heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the architecture domain (Vlachos and Clark, 2014) and the Web (Xin et al., 2014; Augenst"
D15-1086,D14-1164,0,0.0148031,"s at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the architecture domain (Vlachos and Clark, 2014) and the Web (Xin et al., 2014; Augenstein et al., 2014; Augenstein et al., 2015). To date, there is very little research on improving NERC for distant supervision to extract relations between non-standard entities such as musical artists and albums. Some research has been done on impr"
D15-1086,C14-1199,0,0.0948878,"Missing"
D15-1086,P14-5010,0,0.00440395,"Missing"
D15-1086,N13-1095,0,0.0189979,"sing specialised Web features, such as appearances of entities in lists and links to other Web pages, improves average precision by 7 points, which other Web search-based relation extraction approaches could also benefit from (Xin et al., 2014; Augenstein et al., 2014). In future work, the proposed approach could be combined with other approaches to solve typical issues arising in the context of distant supervision, such as dealing with overlapping relations (Hoffmann et al., 2011), improving heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries"
D15-1086,P09-1113,0,0.716408,"partment, University College London a.vlachos@cs.ucl.ac.uk Abstract knowledge bases and can then be accessed by an information retrieval system, a commercial example for this being Google’s knowledge vault (Xin et al., 2014). In order to keep knowledge bases up to date should new facts emerge, and to quickly adapt to new domains, there is a need for flexible and accurate information extraction (IE) approaches which do not require manual effort to be developed for new domains. A popular approach for creating IE methods to extract such relations is distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009) which is a method for learning relation extractors using relations stored in a knowledge base combined with raw text to automatically generate training data. An important first step in distant supervision is to identify named entities (NEs) and their types to determine if a pair of NEs is a suitable candidate for the relation. As an example, the album relation has a Musical Artist and an Album as arguments. Existing works use supervised named entity recognisers and classifiers (NERC) with either a small set of types such as the Stanford NER system (Manning et al., 2014), or fine-grained NE ty"
D15-1086,P11-2048,0,0.00989521,"heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the architecture domain (Vlachos and Clark, 2014) and the Web (Xin et al., 2014; Augenstein et al., 2014; Augenstein et al., 2015). To date, there is very little research on improving NERC for distant supervision to extract relations between non-standard entities such as musical artists and albums. Some research has been done on improving distant supervision by using fine-grained named entity classifiers (Ling and Weld, 2012; Liu et al., 2014) and on using named entity linking for distant supervision (Koch et al.,"
D15-1086,D14-1140,0,0.0244847,"Missing"
D15-1086,D13-1152,0,0.0207776,"Missing"
D15-1086,N13-1008,0,0.0402883,"with overlapping relations (Hoffmann et al., 2011), improving heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the architecture domain (Vlachos and Clark, 20"
D15-1086,P11-1055,0,0.392904,"best to integrate multiple NE labels as features could be performed, as shown by Liu et al. (2014). NEC features For the one-stage and imitation learning model, we use the following Web features based on HTML markup, both as local features if the entity mention contains the markup, and as global features if a mention somewhere else in the document with the same lexicalisation contains that markup: is link, is list element, is header or subheader, is bold, is emphasised, is italics, is title, is in title. In addition, the following NEC features are extracted, based on Nadeau et al. (2007) and Hoffmann et al. (2011): Word features (mentfeats): • Object occurrence • Sequence and BOW of occurrence • Sequence and bag of POS of occurrence 751 Musical Artist Relation type NE type album MISC record label ORG track MISC Business Relation type NE type employees PER founders PER Film Relation type NE type director PER producer PER actor PER character MISC River Relation type NE type origin LOC mouth LOC Model RelOnly Stanf FIGER OS IL Politician Relation type NE type birthplace LOC educational institution ORG spouse PER Educational Institution Relation type NE type mascot MISC city LOC Book Relation type NE type"
D15-1086,D14-1203,0,0.0141929,"ts which involved rather standard entity types and they did not compare against using off-the shelf NEC systems. vision as done by existing works often causes errors which can be prevented by instead separating NEC and RE with imitation learning. We also showed that using Web features increases precision for NEC. Finally, it is worth noting that the recall for some of the relations is quite low because they only infrequently occur in text, especially in the same sentence as the subject of the relation. These issues can be overcome by performing coreference resolution (Augenstein et al., 2014; Koch et al., 2014), by retrieving more Web pages or improving the information retrieval component of the approach (West et al., 2014) and by combining extractors operating on sentences with other extractors for semi-structured content on Web pages (Carlson et al., 2010). 7 8 Conclusion and Future Work In this paper, we proposed a method for extracting non-standard relations with distant supervision that learns a NEC jointly with relation extraction using imitation learning. Our proposed imitation learning approach outperforms models with supervised NEC for relations involving nonstandard entities as well as rel"
D15-1086,D13-1003,0,0.00864977,"heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the architecture domain (Vlachos and Clark, 2014) and the Web (Xin et al., 2014; Augenstein et al., 2014; Augen"
D15-1086,D12-1042,0,0.0465296,"text of distant supervision, such as dealing with overlapping relations (Hoffmann et al., 2011), improving heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the"
D15-1086,P12-1076,0,0.0187977,"nes with FIGER and Stanford NE labels. We further demonstrate that using specialised Web features, such as appearances of entities in lists and links to other Web pages, improves average precision by 7 points, which other Web search-based relation extraction approaches could also benefit from (Xin et al., 2014; Augenstein et al., 2014). In future work, the proposed approach could be combined with other approaches to solve typical issues arising in the context of distant supervision, such as dealing with overlapping relations (Hoffmann et al., 2011), improving heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al.,"
D15-1086,W14-4501,1,0.900852,"eir effect using a (possibly non-decomposable) loss function on the complete structure predicted. The dependencies between the actions are learnt via appropriate generation of training examples. The ability to learn by assessing only the final prediction and not the intermediate steps is very useful in the face of missing labels, such as in the case of the labels for the NEC stage. Recall that the action sequence in our case consists of one NEC action and possibly one RE action, dependent on whether the NEC action is true, i.e. the entity is of the appropriate type for the relation. Following Vlachos and Clark (2014), for each training instance, we obtain supervision for the NEC stage by taking both options for this stage, true or false, obtaining the prediction from the RE stage in the former case and then comparing the outcomes against the label obtained from distant supervision. Thus the NEC stage is learned so that it enhances the performance of RE. In parallel, the RE stage is learned using only instances that actually reach this stage. The process is iterated so that the models learned adjust to each other. For more details on this we refer the reader to Vlachos and Clark (2014). 4.2 • Noun phrases:"
D15-1086,W11-0307,1,0.840046,"precision, since precision errors can be dealt with by the NEC stage. For a relation candidate identification stage with higher recall we instead rely on POS-based heuristics for detecting NEs2 and HTML markup. We use the following POS heuristics: Imitation learning1 algorithms such as S EARN (Daum´e III et al., 2009) and DAGGER (Ross et al., 2011) have been applied successfully to a variety of structured prediction tasks due to their flexibility in incorporating features and their ability to learn with non-decomposable loss functions. Sample applications include biomedical event extraction (Vlachos and Craven, 2011), dynamic feature selection (He et al., 2013), and machine translation (Grissom II et al., 2014). Imitation learning algorithms for structured prediction decompose the prediction task into a sequence of actions; these actions are predicted by classifiers which are trained to take into account the effect of their predictions on the whole sequence by assessing their effect using a (possibly non-decomposable) loss function on the complete structure predicted. The dependencies between the actions are learnt via appropriate generation of training examples. The ability to learn by assessing only the"
D15-1086,P13-2117,0,0.0304912,"Missing"
D15-1086,D10-1099,0,0.0167606,"ations (Hoffmann et al., 2011), improving heuristic labelling of sentences (Takamatsu et al., 2012) or dealing with incomplete knowledge bases (Min et al., 2013). Related Work One of the first papers to introduce distant supervision was Mintz et al. (2009), which aims at extracting relations between entities in Wikipedia for the most frequent relations in Freebase. Most distant supervision research focuses on addressing the disadvantages of heuristic labelling, namely reducing false positive training data (Hoffmann et al., 2011; Surdeanu et al., 2012; Riedel et al., 2010; Riedel et al., 2013; Yao et al., 2010; Alfonseca et al., 2012; Roth and Klakow, 2013; Takamatsu et al., 2012; Xu et al., 2013) and dealing with false negatives due to missing entries in the knowledge base (Min et al., 2013), as well as combining distant supervision with active learning (Angeli et al., 2014) Distant supervision has been researched for different domains, including newswire (Riedel et al., 2010; Riedel et al., 2013), Wikipedia (Mintz et al., 2009; Nguyen and Moschitti, 2011), the biomedical domain (Craven and Kumlien, 1999; Roller and Stevenson, 2014), the architecture domain (Vlachos and Clark, 2014) and the Web (X"
D15-1197,R11-1063,0,0.0584259,"gone home and reducing the weight for all subsequent sentences. However, since the improvements due to these rules were negligible, we did not include them in our final system. Nevertheless, these rules were helpful in analyzing problem areas in the datasets, as discussed in Section 6. 5 Results We evaluated our system on MC160 and MC500 test sets and the results are shown in Table 2. Our proposed baseline outperforms the baseline of Richardson et al. (2013) by 4 and 3 points in accuracy on MC160 and MC500 respectively.3 Our system is comparable to the MSR baseline with the RTE system BIUTEE (Stern and Dagan, 2011). If we linearly combine the RTE scores used in the MSR baseline with our method, we achieve 5 and 2.5 accuracy points higher than the best results achieved by Richardson et al. (2013). Concurrently with ours, three other approaches to solving MCTest were developed and subsequently published a few months before our method. Narasimhan and Barzilay (2015) presented a discourse-level approach, which chooses an answer by utilising relations between sentences chosen as important. Despite is simplicity, our method is comparable in performance, suggesting that better lexical matching could help impro"
D15-1197,D14-1059,0,0.0182783,"stems cannot handle well questions involving negation and quantification. Numerical questions, which we found to be particularly challenging, have been the focus of recent work on algebra word problems (Kushman et al., 2014) for which dedicated systems have been developed. MacCartney et al. (2006) demonstrated that a large set of rules can be used to recognize valid textual entailments. These consider phenomena such as polarity and quantification, similar to those we used in our analysis of the MCTest datasets. More complex methods, which attempt deeper modeling of text include Natural Logic (Angeli and Manning, 2014) and Combinatorial Categorial Grammars (Lewis and Steedman, 2013) combined with distributional models. While promising, these approaches have been developed primarily on sentence-level tasks, thus the stories in MCTest are likely to present additional challenges. The recently proposed class of methods called Memory Network (Weston et al., 2014), uses neural networks and external memory to answer a simpler comprehension task. Though quite successful on toy tasks, those methods cannot yet be applied to MCTest as they require much larger training datasets than the ones available for this task. A"
D15-1197,P15-2115,0,0.48726,"Missing"
D15-1197,P99-1042,0,0.42751,"that two or more misleading choices included. Richardson et al. (2013) demonstrate that the MC160 and MC500 have similar ratings for clarity and grammar, and that humans perform equally well on both. However, in many cases MC500 appears to be designed in such a way to confuse lexical algorithms and encourage the use of more sophisticated techniques necessary to deal with phenomena such as elimination questions, negation, and common knowledge not explicitly written in the story. 7 Related work The use of shallow methods for machine comprehension has been explored in previous work, for example Hirschman et al. (1999) used a bag-ofwords to match question-answer pairs to sentences in the text, and choose the best pair with the best matching sentence. As discussed in our analysis, such systems cannot handle well questions involving negation and quantification. Numerical questions, which we found to be particularly challenging, have been the focus of recent work on algebra word problems (Kushman et al., 2014) for which dedicated systems have been developed. MacCartney et al. (2006) demonstrated that a large set of rules can be used to recognize valid textual entailments. These consider phenomena such as polar"
D15-1197,P14-1026,0,0.019927,"nation questions, negation, and common knowledge not explicitly written in the story. 7 Related work The use of shallow methods for machine comprehension has been explored in previous work, for example Hirschman et al. (1999) used a bag-ofwords to match question-answer pairs to sentences in the text, and choose the best pair with the best matching sentence. As discussed in our analysis, such systems cannot handle well questions involving negation and quantification. Numerical questions, which we found to be particularly challenging, have been the focus of recent work on algebra word problems (Kushman et al., 2014) for which dedicated systems have been developed. MacCartney et al. (2006) demonstrated that a large set of rules can be used to recognize valid textual entailments. These consider phenomena such as polarity and quantification, similar to those we used in our analysis of the MCTest datasets. More complex methods, which attempt deeper modeling of text include Natural Logic (Angeli and Manning, 2014) and Combinatorial Categorial Grammars (Lewis and Steedman, 2013) combined with distributional models. While promising, these approaches have been developed primarily on sentence-level tasks, thus th"
D15-1197,Q13-1015,0,0.0164219,"ication. Numerical questions, which we found to be particularly challenging, have been the focus of recent work on algebra word problems (Kushman et al., 2014) for which dedicated systems have been developed. MacCartney et al. (2006) demonstrated that a large set of rules can be used to recognize valid textual entailments. These consider phenomena such as polarity and quantification, similar to those we used in our analysis of the MCTest datasets. More complex methods, which attempt deeper modeling of text include Natural Logic (Angeli and Manning, 2014) and Combinatorial Categorial Grammars (Lewis and Steedman, 2013) combined with distributional models. While promising, these approaches have been developed primarily on sentence-level tasks, thus the stories in MCTest are likely to present additional challenges. The recently proposed class of methods called Memory Network (Weston et al., 2014), uses neural networks and external memory to answer a simpler comprehension task. Though quite successful on toy tasks, those methods cannot yet be applied to MCTest as they require much larger training datasets than the ones available for this task. A recent approach by Hermann et al. (2015) uses attention-based rec"
D15-1197,N06-1006,0,0.0394625,"Missing"
D15-1197,P14-5010,0,0.00835248,"Missing"
D15-1197,P15-1121,0,0.314237,"ts and the results are shown in Table 2. Our proposed baseline outperforms the baseline of Richardson et al. (2013) by 4 and 3 points in accuracy on MC160 and MC500 respectively.3 Our system is comparable to the MSR baseline with the RTE system BIUTEE (Stern and Dagan, 2011). If we linearly combine the RTE scores used in the MSR baseline with our method, we achieve 5 and 2.5 accuracy points higher than the best results achieved by Richardson et al. (2013). Concurrently with ours, three other approaches to solving MCTest were developed and subsequently published a few months before our method. Narasimhan and Barzilay (2015) presented a discourse-level approach, which chooses an answer by utilising relations between sentences chosen as important. Despite is simplicity, our method is comparable in performance, suggesting that better lexical matching could help improve their model. Sachan et al. (2015) treated MCTest as a structured prediction problem, searching for a latent structure connecting the question, answer and the text, dubbed the answer-entailing structure. Their model performs better on MC500 (was 3 We consider the updated MSR algorithms and results, together with partial credit accuracies, provided at"
D15-1197,D13-1020,0,0.438581,"uter Science University College London {e.smith,n.greco,m.bosnjak,a.vlachos}@cs.ucl.ac.uk Abstract Machine comprehension of text is the overarching goal of a great deal of research in natural language processing. The Machine Comprehension Test (Richardson et al., 2013) was recently proposed to assess methods on an open-domain, extensible, and easy-to-evaluate task consisting of two datasets. In this paper we develop a lexical matching method that takes into account multiple context windows, question types and coreference resolution. We show that the proposed method outperforms the baseline of Richardson et al. (2013), and despite its relative simplicity, is comparable to recent work using machine learning. We hope that our approach will inform future work on this task. Furthermore, we argue that MC500 is harder than MC160 due to the way question answer pairs were created. 1 Introduction Machine comprehension of text is the central goal in NLP. The academic community has proposed a variety of tasks, such as information extraction (Sarawagi, 2008), semantic parsing (Mooney, 2007) and textual entailment (Androutsopoulos and Malakasiotis, 2010). However, these tasks assess performance on each task individuall"
D15-1197,P15-1024,0,0.143172,"Missing"
D15-1197,W08-1301,0,\N,Missing
D15-1312,P09-1113,0,0.0657074,"and the value claimed (Lesotho and 2,000,000 respectively). We then proceed to verify the value claimed in text for the property of this entity against the value known in a knowledge base such as Freebase and return a score reflecting the accuracy of the claim (absolute percentage error in the example). Claim identification is essentially an instance of information extraction. While it would be possible to develop supervised models, this would require expensive manual data annotation for each property of interest. Instead, we follow the distant supervision paradigm (Craven and Kumlien, 1999; Mintz et al., 2009) using supervision obtained by combining triples from a knowledge base and raw text. However, statistical properties are more challenging in applying the distant supervision assumption than relations between named entities due to the fact that the numerical values are often approximated in text, as in the example of Figure 1. Consequently, linking the values mentioned in text with those in the knowledge base is not trivial and thus it is not straightforward to generate training instances for the property of interest. 2596 Proceedings of the 2015 Conference on Empirical Methods in Natural Langu"
D15-1312,P14-1095,0,0.0863033,"erty, even inaccurate ones; in information extraction on the other hand, and especially its formulation as knowledge base population, we are interested in the accurate claims only, since extracting inaccurate ones will lead to erroneous information added to the knowledge base. The difference between the two tasks is captured by the verification task. In this paper our main goals are identification and verification, but we train our approach on information extraction, relying on the assumption that most claims made in the texts retrieved via the web search engine are accurate. In related work, Nakashole and Mitchell (2014) 2 Accessed in August 2015. developed an approach to verify subject-verbobject triples against a knowledge base, taking into account the objectivity of the language used in the sources stating the triple. Our approach is agnostic to the syntactic form of the claims, thus it can identify claims expressed in greater linguistic variety. Ciampaglia et al. (2015) fact-checked subject-predicate-object triples against a knowledge graph constructed from DBpedia, but they considered only the paths between the subject and the predicate in their algorithm thus ignoring the predicate itself. Dong et al. ("
D15-1312,W14-2508,1,0.833679,"ples against a knowledge graph constructed from DBpedia, but they considered only the paths between the subject and the predicate in their algorithm thus ignoring the predicate itself. Dong et al. (2015) established the trustworthiness of a web source by comparing the subject-predicate-object triples extracted from it to the Knowledge Vault built by Google, but did not focus on claim identification and verification. Adar et al. (2009) developed an approach to detect inconsistencies between versions of Wikipedia in different languages, but they focused on manually extracted infoboxes. Finally, Vlachos and Riedel (2014) compiled a dataset of claims fact-checked by journalists, but the claims are much more complex than the ones we considered in this paper. Other work that discussed the extraction of statistical properties includes the approaches of Hoffmann et al. (2010) and Intxaurrondo et al. (2015), both employing approximate matching to deal with the approximation of numerical values in text. In order to learn their model, Hoffmann et al. (2010) take advantage of the structure of the articles in Wikipedia developing a classifier that identifies the schema followed by each article, which is not straightfor"
D15-1312,P10-1030,0,0.104234,"Missing"
D15-1312,P14-5010,0,0.00558418,"aving values for 150-175 regions (mostly countries). To collect texts from which the text patterns between entities and numerical values will be extracted we downloaded documents from the web. In particular, for each region combined with each property we formed a query consisting of the two and submitted it to Bing via its Search API. Following this we obtained the top 50 results for each query, downloaded the HTML pages corresponding to each result and extracted their textual content with BoilerPipe (Kohlsch¨utter et al., 2010). We then processed the texts using the Stanford CoreNLP toolkit (Manning et al., 2014) and from each sentence we extracted textual patterns between all the named entities recognized as locations and all the numerical values. Two kinds of patterns were extracted for each location and numerical value: surface patterns (as the ones shown in Table 1) and lexicalized dependency paths. This pattern extraction process resulted in a large set of triples consisting of a region, a pattern and a value. Different sentences might result in triples containing the same region and textual pattern but different value. Such variation can arise due to either the approximations of values in text o"
D15-1312,N15-1066,0,\N,Missing
D16-1084,W15-1516,0,0.0260991,"ork mostly considered target-specific stance prediction in debates (Hasan and Ng, 2013; Walker et al., 2012) or student essays (Faulkner, 2014). The task considered in this paper is more challenging than stance detection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et al., 2015). Related work on rumour stance detection either requires training data from the same rumour (Qazvinian et al., 2011), i.e., target, or is rulebased (Liu et al., 2015) and thus potentially hard to generalise. Finally, the target-dependent stance detection task tackled in this paper is different from that of Ferreira and Vlachos (2016), which while related concerned with the stance of a statement in natural language towards another statement. Conditional Encoding: Conditional encoding has been applie"
D16-1084,S16-1063,1,0.714724,"models are suitable for unseen, as well as seen target stance detection. 7 Related Work Stance Detection: Previous work mostly considered target-specific stance prediction in debates (Hasan and Ng, 2013; Walker et al., 2012) or student essays (Faulkner, 2014). The task considered in this paper is more challenging than stance detection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et al., 2015). Related work on rumour stance detection either requires training data from the same rumour (Qazvinian et al., 2011), i.e., target, or is rulebased (Liu et al., 2015) and thus potentially hard to generalise. Finally, the target-dependent stance detection task tackled in this paper is different from that of Ferreira and Vlachos (2016), which while related concerned with the stance of a"
D16-1084,D15-1075,0,0.0850398,"tweets on targets outperform all baselines on the test set. It is further worth noting that the Bag-of-WordVectors baseline achieves results comparable with TweetOnly, Concat and one of the conditional encoding models, TarCondTweet, on the dev set, even though it achieves significantly lower performance on the test set. This indicates that the pre-trained word embeddings on their own are already very useful for stance detection. This is consistent with findings of other works showing the usefulness of such a Bag-of-Word-Vectors baseline for the related tasks of recognising textual entailment Bowman et al. (2015) and sentiment analysis Eisner et al. (2016). Our best result in the test setup with BiCond is the second highest reported result on the Twitter Stance Detection corpus, however the first, third and fourth best approaches achieved their results by automatically labelling Donald Trump training data. BiCond for the unseen target setting outperforms the third and fourth best approaches by a large margin (5 and 7 points in Macro F1, respectively), as can be seen in Table 7. Results for weakly supervised stance detection are discussed in Section 6. Pre-Training Table 4 shows the effect of unsupervi"
D16-1084,S16-1061,0,0.0879263,"0.5078 TweetOnly FAVOR AGAINST Macro 0.5284 0.5774 0.6284 0.4615 0.5741 0.5130 0.5435 Concat FAVOR AGAINST Macro 0.5506 0.5794 0.5878 0.4883 0.5686 0.5299 0.5493 TarCondTweet FAVOR AGAINST Macro 0.5636 0.5947 0.6284 0.4515 0.5942 0.5133 0.5538 TweetCondTar FAVOR AGAINST Macro 0.5868 0.5915 0.6622 0.4649 0.6222 0.5206 0.5714 BiCond FAVOR AGAINST Macro 0.6268 0.6057 0.6014 0.4983 0.6138 0.5468 0.5803 Table 6: Stance Detection test results for weakly supervised setup, trained on automatically labelled pos+neg+neutral Trump data, and reported on the official test set. Marsh, 2016) and INF-UFRGS (Dias and Becker, 2016) considered a different experimental setup. They automatically annotated training data for the test target Donald Trump, thus converting the task into weakly supervised seen target stance detection. The pkudblab system uses a deep convolutional neural network that learns to make 2-way predictions on automatically labelled positive and negative training data for Donald Trump. The neutral class is predicted according to rules which are applied at test time. Since the best performing systems which participated in the shared task consider a weakly supervised setup, we further compare our proposed"
D16-1084,W16-6208,1,0.849006,"Missing"
D16-1084,N16-1138,1,0.817213,"tioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et al., 2015). Related work on rumour stance detection either requires training data from the same rumour (Qazvinian et al., 2011), i.e., target, or is rulebased (Liu et al., 2015) and thus potentially hard to generalise. Finally, the target-dependent stance detection task tackled in this paper is different from that of Ferreira and Vlachos (2016), which while related concerned with the stance of a statement in natural language towards another statement. Conditional Encoding: Conditional encoding has been applied to the related task of recognising textual entailment (Rockt¨aschel et al., 2016), using a dataset of half a million training examples (Bowman et al., 2015) and numerous different hypotheses. Our experiments here show that conditional encoding is also successful on a relatively small training set and when applied to an unseen testing target. Moreover, we augment conditional encoding with bidirectional encoding and demonstrate"
D16-1084,I13-1191,0,0.15284,"es which also learn representations for the targets (BoWV, Concat). By training conditional encoding models on automatically labelled stance detection data we achieve state-of-the-art results. The best result (F1 of 0.5803) is achieved with the bi-directional conditional encoding model (BiCond). This shows that 3 Method Note that “|” indiates “or”, ( ?) indicates optional space 883 in Dias and Becker (2016) such models are suitable for unseen, as well as seen target stance detection. 7 Related Work Stance Detection: Previous work mostly considered target-specific stance prediction in debates (Hasan and Ng, 2013; Walker et al., 2012) or student essays (Faulkner, 2014). The task considered in this paper is more challenging than stance detection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell"
D16-1084,D13-1171,0,0.0742182,"Missing"
D16-1084,S16-1003,0,0.327289,"Missing"
D16-1084,D11-1147,0,0.135482,"tection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et al., 2015). Related work on rumour stance detection either requires training data from the same rumour (Qazvinian et al., 2011), i.e., target, or is rulebased (Liu et al., 2015) and thus potentially hard to generalise. Finally, the target-dependent stance detection task tackled in this paper is different from that of Ferreira and Vlachos (2016), which while related concerned with the stance of a statement in natural language towards another statement. Conditional Encoding: Conditional encoding has been applied to the related task of recognising textual entailment (Rockt¨aschel et al., 2016), using a dataset of half a million training examples (Bowman et al., 2015) and numerous different hypotheses. Our experiments her"
D16-1084,D13-1170,0,0.00386016,"la and Marsh, 2016) made use of this, thus changing the task to weakly supervised seen target stance detection, instead of an unseen target task. Although the goal of this paper is to present stance detection methods for targets for which no training data is available, we show that they can also be used successfully in a weakly supervised framework and outperform the state-of-the-art on the SemEval 2016 Stance Detection for Twitter dataset. 877 3 Methods A common stance detection approach is to treat it as a sentence-level classification task similar to sentiment analysis (Pang and Lee, 2008; Socher et al., 2013). However, such an approach cannot capture the stance of a tweet with respect to a particular target, unless training data is available for each of the test targets. In such cases, we could learn that a tweet mentioning Donald Trump in a positive manner expresses a negative stance towards Hillary Clinton. Despite this limitation, we use two such baselines, one implemented with a Support Vector Machine (SVM) classifier and one with an LSTM network, in order to assess whether we are successful in incorporating the target in stance prediction. A naive approach to incorporate the target in stance"
D16-1084,N12-1072,0,0.115068,"representations for the targets (BoWV, Concat). By training conditional encoding models on automatically labelled stance detection data we achieve state-of-the-art results. The best result (F1 of 0.5803) is achieved with the bi-directional conditional encoding model (BiCond). This shows that 3 Method Note that “|” indiates “or”, ( ?) indicates optional space 883 in Dias and Becker (2016) such models are suitable for unseen, as well as seen target stance detection. 7 Related Work Stance Detection: Previous work mostly considered target-specific stance prediction in debates (Hasan and Ng, 2013; Walker et al., 2012) or student essays (Faulkner, 2014). The task considered in this paper is more challenging than stance detection in debates because, in addition to irregular language, the Mohammad et al. (2016) dataset is offered without any context, e.g., conversational structure or tweet metadata. The targets are also not always mentioned in the tweets, which is an additional challenge (Augenstein et al., 2016) and distinguishes this task from target-dependent (Vo and Zhang, 2015; Zhang et al., 2016; Alghunaim et al., 2015) and open-domain target-dependent sentiment analysis (Mitchell et al., 2013; Zhang et"
D16-1084,S16-1062,0,0.392335,"data is not provided. Systems need to classify the stance of each tweet as “positive” (FAVOR), “negative” (AGAINST) or “neutral” (NONE) towards the target. The official metric reported for the shared task is F1 macroaveraged over the classes FAVOR and AGAINST. Although the F1 of NONE is not considered, systems still need to predict it to avoid precision errors for the other two classes. Even though participants were not allowed to manually label data for the test target Donald Trump, they were allowed to label data automatically. The two best-performing systems submitted to Task B, pkudblab (Wei et al., 2016) and LitisMind (Zarrella and Marsh, 2016) made use of this, thus changing the task to weakly supervised seen target stance detection, instead of an unseen target task. Although the goal of this paper is to present stance detection methods for targets for which no training data is available, we show that they can also be used successfully in a weakly supervised framework and outperform the state-of-the-art on the SemEval 2016 Stance Detection for Twitter dataset. 877 3 Methods A common stance detection approach is to treat it as a sentence-level classification task similar to sentiment analysis"
D16-1084,S16-1074,0,0.326839,"ed to classify the stance of each tweet as “positive” (FAVOR), “negative” (AGAINST) or “neutral” (NONE) towards the target. The official metric reported for the shared task is F1 macroaveraged over the classes FAVOR and AGAINST. Although the F1 of NONE is not considered, systems still need to predict it to avoid precision errors for the other two classes. Even though participants were not allowed to manually label data for the test target Donald Trump, they were allowed to label data automatically. The two best-performing systems submitted to Task B, pkudblab (Wei et al., 2016) and LitisMind (Zarrella and Marsh, 2016) made use of this, thus changing the task to weakly supervised seen target stance detection, instead of an unseen target task. Although the goal of this paper is to present stance detection methods for targets for which no training data is available, we show that they can also be used successfully in a weakly supervised framework and outperform the state-of-the-art on the SemEval 2016 Stance Detection for Twitter dataset. 877 3 Methods A common stance detection approach is to treat it as a sentence-level classification task similar to sentiment analysis (Pang and Lee, 2008; Socher et al., 2013"
D16-1084,D15-1073,0,0.0469489,"Missing"
D16-1200,P06-1009,0,0.0466079,"also makes possible to make them dependent on the events, e.g. a binary indicator encoding whether two consecutive events with the same stem share the same anchor or not. The full list of local and global features extracted by Φ are presented in Tables 1 and 2. Predicting with the scoring function in Eq.2 amounts to finding the anchoring sequence vector z that maximizes it. To be able to perform exact inference efficiently, we impose a first order Markov assumption and use the Viterbi algorithm (Viterbi, 1967). Similar approaches have been successful in word alignment for machine translation (Blunsom and Cohn, 2006). 4.3 Post-processing During testing, we need to construct the timeline for each target entity using the events that were predicted to be anchored to it and the timestamps of the temporal expressions each event was anchored to. Thus, we need to perform two additional tasks, 1939 Results We evaluate our system using the setup provided by the TimeLine task ensuring that the training and validation are performed only using the development data i.e. the Apple collection. All linear models were trained with the perceptron update rule (Pedregosa et al., 2011). We tuned the number of perceptron itera"
D16-1200,P15-2059,0,0.0181183,"pulation task (Ji et al., 2011). However they focus on learning real-world event ordering constraints (e.g. people go to school before university) instead of how events are reported in text. 6 In this paper we proposed a timeline extraction approach in which we generate noisy training data for anchoring events to entities and temporal expressions using distant supervision. By learning a binary classifier we match the state-of-the-art F1 -score for the Track B of the TimeLine shared task. We further improve this result by 3.2 F1 -score points using joint inference. Related work In recent work, Laparra et al. (2015) also considered anchoring at the document-level in the context of the Track A of the TimeLine shared task, however they developed a rule-based approach. The structure features used in our joint inference approach encode similar intuitions, but we are learning model weights using distant supervision so that we can combine them more flexibly. And even though the noise in the trainng data generated with distant supervision is a concern, manual annotation of temporal relations is known to have low inter-annotator agreement rates1 and thus also likely to be noisy. Prior to the TimeLine shared task"
D16-1200,J13-4004,0,0.021896,"rbus, GM and Stock market respectively. We used the official evaluation which is based on the metric introduced by UzZaman and Allen (2011) which assesses a predicted timeline versus the gold standard one using precision, recall and F-score over binary temporal relations between the events. 3 Distant supervision In order to generate training data for anchoring event mentions to target entities and temporal expressions 1937 via distant supervision, we first need to identify them. For entity recognition we use approximate string matching combined with the Stanford Coreference Resolution System (Lee et al., 2013). For temporal expression identification and resolution to absolute timestamps we use the UWTime temporal parser (Lee et al., 2014). Next we generate labeled instances as follows. For anchoring events to entities, we consider for each event mention the correct entity mention to be the nearest mention of the target entity in the same sentence, and all others to be incorrect. Similarly, for anchoring events to timestamps, we consider for each event mention the correct temporal expression to be the nearest temporal expression that exactly matches the timestamp according to the timeline (but not n"
D16-1200,P14-1135,0,0.0257073,"n (2011) which assesses a predicted timeline versus the gold standard one using precision, recall and F-score over binary temporal relations between the events. 3 Distant supervision In order to generate training data for anchoring event mentions to target entities and temporal expressions 1937 via distant supervision, we first need to identify them. For entity recognition we use approximate string matching combined with the Stanford Coreference Resolution System (Lee et al., 2013). For temporal expression identification and resolution to absolute timestamps we use the UWTime temporal parser (Lee et al., 2014). Next we generate labeled instances as follows. For anchoring events to entities, we consider for each event mention the correct entity mention to be the nearest mention of the target entity in the same sentence, and all others to be incorrect. Similarly, for anchoring events to timestamps, we consider for each event mention the correct temporal expression to be the nearest temporal expression that exactly matches the timestamp according to the timeline (but not necessarily in the same sentence), and all others to be incorrect. The datasets generated will be noisy since correct anchors may be"
D16-1200,S10-1063,0,0.146095,") and show that the latter outperforms the former by a margin of 3.2 points in F-score, achieving a micro F1 -score of 28.58 across the three test corpora, thus confirming the benefits of joint inference. The only corpus in which joint inference did not help was Stock which has on average shorter event chains per document (Minard et al., 2015) and thus renders joint anchoring less likely to be useful. We now compare our approach to the two participants in the TimeLine shared task with two runs each. The best-performing GPLSIUA team (Navarro and Saquete, 2015) used the TIPSem tool developed by Llorens et al. (2010) for temporal relation processing which extracts events and temporal expressions and uses a Conditional Random Field model to anchor them against each other. However, TIPSem only considers anchoring of events to temporal expressions that are in the same sentence. GPLSIUA also used the semantic role labeler from SENNA (Collobert et al., 2011) and OpenNER and anchored entities to events using a rulebased approach. The HeidelToul team (Moulahi et al., 2015) used HeidelTime (Str¨otgen et al., 2013) to identify and resolve temporal expressions and deSystem GPLSIUA 1 GPLSIUA 2 HeidelToul 1 HeidelTou"
D16-1200,P14-5010,0,0.016713,"for each of these tasks. 4.1 Classification Using distant supervision we obtained examples of correct and incorrect anchoring of event mentions to entities and temporal expressions. Thus we learn for each of the two tasks a binary linear classifier of the form: score(x, y, w) = w · φ(x, y) (1) where x is an event mention, y is the anchor (either the target entity or the temporal expression) and w are the parameters to be learned. The features extracted by φ represent various distance measures and syntactic dependencies between the event mention and the anchor obtained using Stanford CoreNLP (Manning et al., 2014). The temporal expression anchoring model also uses a few feature templates that depend on the timestamp of the temporal expression. The full list of features extracted by φ are denoted as local in Tables 1 and 2. 4.2 Alignment The classification approach described is limited to anchoring each event mention to an entity or a temporal expression in isolation. However it would be preferable to infer the decisions for each task jointly at the document level and take into account the dependencies in anchoring different events, e.g. that consecutive events in text are likely to be anchored 1938 Fea"
D16-1200,D12-1080,0,0.0601563,"Missing"
D16-1200,P09-1113,0,0.0105896,"Department of Computer Science University of Sheffield, UK a.vlachos@sheffield.ac.uk set of documents with annotated event mentions (input) and the timelines extracted for a few target entities (output). No training data was provided, thus participating systems used rules combined with temporal linking systems trained on related tasks in order to anchor events to temporal expressions and entities to construct the timelines. We propose a new approach to timeline extraction that uses the development data provided as distant supervision to generate noisy training data (Craven and Kumlien, 1999; Mintz et al., 2009). More specifically, we heuristically align the target entity and the timestamps from the timelines with automatically recognized entities and temporal expressions in the documents. This noisy labeled data set allows us to learn models for the subtasks of anchoring events to temporal expressions and to entities, without requiring training models on additional data. Also, we improve the performance using joint inference for both anchoring subtasks. In our experiments, we show that our distantly supervised approach matches the state-of-the-art performance while joint inference further improves o"
D16-1200,S15-2139,0,0.0297067,"Missing"
D16-1200,S15-2138,0,0.0213083,"tem Binary) against the alignment model (Our System Alignment) and show that the latter outperforms the former by a margin of 3.2 points in F-score, achieving a micro F1 -score of 28.58 across the three test corpora, thus confirming the benefits of joint inference. The only corpus in which joint inference did not help was Stock which has on average shorter event chains per document (Minard et al., 2015) and thus renders joint anchoring less likely to be useful. We now compare our approach to the two participants in the TimeLine shared task with two runs each. The best-performing GPLSIUA team (Navarro and Saquete, 2015) used the TIPSem tool developed by Llorens et al. (2010) for temporal relation processing which extracts events and temporal expressions and uses a Conditional Random Field model to anchor them against each other. However, TIPSem only considers anchoring of events to temporal expressions that are in the same sentence. GPLSIUA also used the semantic role labeler from SENNA (Collobert et al., 2011) and OpenNER and anchored entities to events using a rulebased approach. The HeidelToul team (Moulahi et al., 2015) used HeidelTime (Str¨otgen et al., 2013) to identify and resolve temporal expressions"
D16-1200,S13-2003,0,0.0526734,"Missing"
D16-1200,P11-2061,0,0.0232402,"ared task had two tracks, A and B, the only difference being that in Track B the event mentions are provided in the input. We consider this track in this paper and focus on learning the anchoring of events to temporal expressions and entities. The development data provided in the context of the shared task consisted of documents related to Apple and gold timelines for six target entities. Evaluation was performed by extracting timelines from three document sets, each related to Airbus, GM and Stock market respectively. We used the official evaluation which is based on the metric introduced by UzZaman and Allen (2011) which assesses a predicted timeline versus the gold standard one using precision, recall and F-score over binary temporal relations between the events. 3 Distant supervision In order to generate training data for anchoring event mentions to target entities and temporal expressions 1937 via distant supervision, we first need to identify them. For entity recognition we use approximate string matching combined with the Stanford Coreference Resolution System (Lee et al., 2013). For temporal expression identification and resolution to absolute timestamps we use the UWTime temporal parser (Lee et a"
D16-1200,S13-2001,0,0.231848,"Missing"
D16-1200,S07-1014,0,0.315542,"ered anchoring at the document-level in the context of the Track A of the TimeLine shared task, however they developed a rule-based approach. The structure features used in our joint inference approach encode similar intuitions, but we are learning model weights using distant supervision so that we can combine them more flexibly. And even though the noise in the trainng data generated with distant supervision is a concern, manual annotation of temporal relations is known to have low inter-annotator agreement rates1 and thus also likely to be noisy. Prior to the TimeLine shared task, TempEval (Verhagen et al., 2007) was the original task that focused on categorising the relations between events, temporal expressions and Document Creation Time using the the TimeML annotation language. The task classified only the relations between mentions in the same or consecutive sentences. The two following tasks, TempEval-2 (Verhagen et al., 2010) and TempEval-3 (UzZaman et al., 2013), added tasks for event and temporal expression identifica1 http://www.timeml.org/timebank/ documentation-1.2.html 1940 7 Conclusions Acknowledgments Part of this work was conducted while both authors were at University College London. T"
D16-1200,S10-1010,0,\N,Missing
D18-1086,N16-1012,0,0.0671797,"Missing"
D18-1086,N16-1087,0,0.199941,"rdy2@sheffield.ac.uk Andreas Vlachos The University of Sheffield a.vlachos@sheffield.ac.uk Abstract However, the use of AMR also has its own shortcomings. While AMR is suitable for information aggregation, it ignores aspects of language such as tense, grammatical number, etc., which are important for the natural language generation (NLG) stage that normally occurs in the end of the summarization process. Due to the lack of such information, approaches for NLG from AMR typically infer it from regularities in the training data (Pourdamghani et al., 2016; Konstas et al., 2017; Song et al., 2016; Flanigan et al., 2016), which however is not suitable in the context of summarization. Consequently, the main previous work on AMR-based abstractive summarization (Liu et al., 2015) only generated bag-of-words from the summary AMR graph. Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage"
D18-1086,P17-1014,0,0.154104,"ion Hardy The University of Sheffield hhardy2@sheffield.ac.uk Andreas Vlachos The University of Sheffield a.vlachos@sheffield.ac.uk Abstract However, the use of AMR also has its own shortcomings. While AMR is suitable for information aggregation, it ignores aspects of language such as tense, grammatical number, etc., which are important for the natural language generation (NLG) stage that normally occurs in the end of the summarization process. Due to the lack of such information, approaches for NLG from AMR typically infer it from regularities in the training data (Pourdamghani et al., 2016; Konstas et al., 2017; Song et al., 2016; Flanigan et al., 2016), which however is not suitable in the context of summarization. Consequently, the main previous work on AMR-based abstractive summarization (Liu et al., 2015) only generated bag-of-words from the summary AMR graph. Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AM"
D18-1086,N15-1114,0,0.151948,"is suitable for information aggregation, it ignores aspects of language such as tense, grammatical number, etc., which are important for the natural language generation (NLG) stage that normally occurs in the end of the summarization process. Due to the lack of such information, approaches for NLG from AMR typically infer it from regularities in the training data (Pourdamghani et al., 2016; Konstas et al., 2017; Song et al., 2016; Flanigan et al., 2016), which however is not suitable in the context of summarization. Consequently, the main previous work on AMR-based abstractive summarization (Liu et al., 2015) only generated bag-of-words from the summary AMR graph. Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural language generation stage which we guide using the source document. We demonstrate that this guidance improves summarization results by 7.4 and 10.5 points in ROUGE-2 using gold standar"
D18-1086,D15-1166,0,0.168837,"Missing"
D18-1086,W13-2322,0,0.210584,"bstractive summarization is the task of automatically producing the summary of a source document through the process of paraphrasing, aggregating and/or compressing information. Recent work in abstractive summarization has made progress with neural encoder-decoder architectures (See et al., 2017; Chopra et al., 2016; Rush et al., 2015). However, these models are often challenged when they are required to combine semantic information in order to generate a longer summary (Wiseman et al., 2017). To address this shortcoming, several works have explored the use of Abstract Meaning Representation (Banarescu et al., 2013, AMR). These were motivated by AMR’s capability to capture the predicate-argument structure which can be utilized in information aggregation during summarization. Our approach is evaluated using the Proxy Report section from the AMR dataset (Knight et al., 2017, LDC2017T10) which contains manually annotated document and summary AMR graphs. Using our proposed guided AMR-to-text NLG, we improve summarization results using both gold standard AMR parses and parses obtained using the RIGA (Barzdins and Gosko, 2016) parser by 7.4 and 10.5 ROUGE-2 points respectively. Our model also outperforms a st"
D18-1086,S16-1166,0,0.0477781,"estimated using words in the side information, Pside , in order to score each word given its context during decoding. We estimate Pside as the linear interpolation of 2-gram to 4gram probabilities in the form of Table 1: Results for AMR-to-text of summarization? (3) Does the improvement in AMR-to-Text hold when we use the generator for abstractive summarization using AMR? We answer each of these in the following paragraphs. AMR-to-Text baseline comparison We compare our baseline model (described in §3.2) against previous works in AMR-to-text using the data from the recent SemEval-2016 Task 8 (May, 2016, LDC2015E86). Table 1 reports BLEU scores comparing our model against previous works. Here, we see that our model achieves a BLEU score comparable with the state-of-the-art, and thus we argue that it is sufficient to be used in our subsequent experiments with guidance. j−1 Pside (xj |xj−1 j−3 ) = λ3 PLM (xj |xj−3 ) + λ2 PLM (xj |xj−1 j−2 ) (5) + λ1 PLM (xj |xj−1 ) , where xj is a word occurring in side information document, PLM is an N -gram LM estimated using Maximum Likelihood: j−1 PLM (xj |xj−N −1 ) = count(xj−N −1 . . . xj ) count(xj−N −1 . . . xj−1 ) (6) Guided NLG for AMR-to-Text In thi"
D18-1086,S16-1176,0,0.190485,"s shortcoming, several works have explored the use of Abstract Meaning Representation (Banarescu et al., 2013, AMR). These were motivated by AMR’s capability to capture the predicate-argument structure which can be utilized in information aggregation during summarization. Our approach is evaluated using the Proxy Report section from the AMR dataset (Knight et al., 2017, LDC2017T10) which contains manually annotated document and summary AMR graphs. Using our proposed guided AMR-to-text NLG, we improve summarization results using both gold standard AMR parses and parses obtained using the RIGA (Barzdins and Gosko, 2016) parser by 7.4 and 10.5 ROUGE-2 points respectively. Our model also outperforms a strong baseline seq2seq model (See et al., 2017) for summarization by 2 ROUGE-2 points. 768 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 768–773 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2 Related Work concepts and the relations between pairs of concepts. They then extracted a summary graph, G0 using the following sub-graph prediction: X X ψ |f(e) (1) G0 = arg max θ |f(v) + Abstractive Summarization using AMR"
D18-1086,W16-6603,0,0.0478708,"bstract Meaning Representation Hardy The University of Sheffield hhardy2@sheffield.ac.uk Andreas Vlachos The University of Sheffield a.vlachos@sheffield.ac.uk Abstract However, the use of AMR also has its own shortcomings. While AMR is suitable for information aggregation, it ignores aspects of language such as tense, grammatical number, etc., which are important for the natural language generation (NLG) stage that normally occurs in the end of the summarization process. Due to the lack of such information, approaches for NLG from AMR typically infer it from regularities in the training data (Pourdamghani et al., 2016; Konstas et al., 2017; Song et al., 2016; Flanigan et al., 2016), which however is not suitable in the context of summarization. Consequently, the main previous work on AMR-based abstractive summarization (Liu et al., 2015) only generated bag-of-words from the summary AMR graph. Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Mean"
D18-1086,D15-1044,0,0.025612,"ion missing from AMR but needed for NLG and (2) improve the quality of the summary. We achieve this in a two-stages process: (1) estimating the probability distribution of the side information, and (2) using it to guide a Luong et al. (2015)’s seq2seq model for NLG. Introduction Abstractive summarization is the task of automatically producing the summary of a source document through the process of paraphrasing, aggregating and/or compressing information. Recent work in abstractive summarization has made progress with neural encoder-decoder architectures (See et al., 2017; Chopra et al., 2016; Rush et al., 2015). However, these models are often challenged when they are required to combine semantic information in order to generate a longer summary (Wiseman et al., 2017). To address this shortcoming, several works have explored the use of Abstract Meaning Representation (Banarescu et al., 2013, AMR). These were motivated by AMR’s capability to capture the predicate-argument structure which can be utilized in information aggregation during summarization. Our approach is evaluated using the Proxy Report section from the AMR dataset (Knight et al., 2017, LDC2017T10) which contains manually annotated docum"
D18-1086,P17-1099,0,0.574776,"s twofold: (1) to retrieve the information missing from AMR but needed for NLG and (2) improve the quality of the summary. We achieve this in a two-stages process: (1) estimating the probability distribution of the side information, and (2) using it to guide a Luong et al. (2015)’s seq2seq model for NLG. Introduction Abstractive summarization is the task of automatically producing the summary of a source document through the process of paraphrasing, aggregating and/or compressing information. Recent work in abstractive summarization has made progress with neural encoder-decoder architectures (See et al., 2017; Chopra et al., 2016; Rush et al., 2015). However, these models are often challenged when they are required to combine semantic information in order to generate a longer summary (Wiseman et al., 2017). To address this shortcoming, several works have explored the use of Abstract Meaning Representation (Banarescu et al., 2013, AMR). These were motivated by AMR’s capability to capture the predicate-argument structure which can be utilized in information aggregation during summarization. Our approach is evaluated using the Proxy Report section from the AMR dataset (Knight et al., 2017, LDC2017T10"
D18-1086,D16-1224,0,0.10755,"ty of Sheffield hhardy2@sheffield.ac.uk Andreas Vlachos The University of Sheffield a.vlachos@sheffield.ac.uk Abstract However, the use of AMR also has its own shortcomings. While AMR is suitable for information aggregation, it ignores aspects of language such as tense, grammatical number, etc., which are important for the natural language generation (NLG) stage that normally occurs in the end of the summarization process. Due to the lack of such information, approaches for NLG from AMR typically infer it from regularities in the training data (Pourdamghani et al., 2016; Konstas et al., 2017; Song et al., 2016; Flanigan et al., 2016), which however is not suitable in the context of summarization. Consequently, the main previous work on AMR-based abstractive summarization (Liu et al., 2015) only generated bag-of-words from the summary AMR graph. Recent work on abstractive summarization has made progress with neural encoder-decoder architectures. However, such models are often challenged due to their lack of explicit semantic modeling of the source document and its summary. In this paper, we extend previous work on abstractive summarization using Abstract Meaning Representation (AMR) with a neural la"
D18-1086,D17-1239,0,0.0217528,"istribution of the side information, and (2) using it to guide a Luong et al. (2015)’s seq2seq model for NLG. Introduction Abstractive summarization is the task of automatically producing the summary of a source document through the process of paraphrasing, aggregating and/or compressing information. Recent work in abstractive summarization has made progress with neural encoder-decoder architectures (See et al., 2017; Chopra et al., 2016; Rush et al., 2015). However, these models are often challenged when they are required to combine semantic information in order to generate a longer summary (Wiseman et al., 2017). To address this shortcoming, several works have explored the use of Abstract Meaning Representation (Banarescu et al., 2013, AMR). These were motivated by AMR’s capability to capture the predicate-argument structure which can be utilized in information aggregation during summarization. Our approach is evaluated using the Proxy Report section from the AMR dataset (Knight et al., 2017, LDC2017T10) which contains manually annotated document and summary AMR graphs. Using our proposed guided AMR-to-text NLG, we improve summarization results using both gold standard AMR parses and parses obtained"
D18-1086,N18-1120,0,0.0313123,"line is a standard (unguided) seq2seq model with attention (Luong et al., 2015) which consists of an encoder and a decoder. The encoder computes the hidden representation of the input, {z1 , z2 , . . . , zk }, which is the linearized summary AMR graph, G0 from Liu et al. (2015), following Van Noord and Bos (2017)’s preprocessing steps. Following this, the decoder generates the target words, {y1 , y2 , . . . , ym }, using the conditional probability Ps2s (yj |y<j , z), which is calculated using the equation Seq2seq using Side Information: In Neural Machine Translation (NMT) field, recent work (Zhang et al., 2018) explored modifications to the decoder of seq2seq models to improve translation results. They used a search engine to retrieve sentences and their translation (referred to as translation pieces) that have high similarity with the source sentence. When similar n-grams from a source document were found in the translation pieces, they rewarded the presence of those ngrams during the decoding process through a scoring mechanism calculating the similarity between source sentence and the source side of the translation pieces. Zhang et al. (2018) reported improvements in translation results up to 6 B"
D19-1233,P16-1231,0,0.0436299,"rn parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng and Hirst, 2014a; Joty et al., 2015; Braud et al., 2017) have used locally normalized discriminative models. However, these are known to have worse performance than generative models when there is little training data (Ng and Jordan, 2002; Yogatama et al., 2017). Unlike locally normalised discriminative models, generative models are not susceptible to label bias (Lafferty et al., 2001). The success of generative (Dyer et al., 2016; Charniak et al., 2016) and globally normalised (Andor et al., 2016) syntactic parsers suggests that reducing label bias leads to better performance. We hypothesize that using a generative parser would also lead to improved performance on RST parsing. However, while they are free from label bias, generative parsers require more sophisticated search algorithms for decoding. Fried et al. (2017) presented a word-level beam search algorithm that made it possible to decode directly from neural generative parsers rather than using them as rerankers. In this paper, we present the first generative RST parser1 . Our model is a document-level version of an RNN Grammar ("
D19-1233,D15-1263,0,0.205956,"ting this branching bias and results in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1 points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data. 1 Introduction Understanding a document’s discourse-level organization is important for correctly interpreting it, and discourse analyses have been shown to be helpful for several NLP tasks (Bhatia et al., 2015; Ji and Smith, 2017; Feng and Hirst, 2014b; Ferracane et al., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to R"
D19-1233,E17-1028,0,0.39538,"formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng and Hirst, 2014a; Joty et al., 2015; Braud et al., 2017) have used locally normalized discriminative models. However, these are known to have worse performance than generative models when there is little training data (Ng and Jordan, 2002; Yogatama et al., 2017). Unlike locally normalised discriminative models, generative models are not susceptible to label bias (Lafferty et al., 2001). The success of generative (Dyer et al., 2016; Charniak et al., 2016) and globally normalised (Andor et al., 2016) syntactic parsers suggests that reducing label bias leads to better performance. We hypothesize that using a generative parser would also lead to improv"
D19-1233,C16-1179,0,0.312021,"Missing"
D19-1233,N18-1086,0,0.0200786,"contains a single tree. Table 2 shows an example of a completed computation for our transition system. 2286 3.3 Transition Model In initial experiments we found, as did Kuncoro et al. (2017) for syntactic parsing, that conditioning only on the stack led to better parsing accuracy, so we specify the next action distribution as p(aj |Sj ). To handle the unbounded number of possible EDUs, we parametrize the probabilities of GEN(e) actions using a neural language model. The next action distribution is factorised into a structural action distribution ptrans and a generation distribution pgen as in Buys and Blunsom (2018), so that p(RE(r, n)|S) = ptrans (RE(r, n)|S) and p(GEN(e)|S) = ptrans (GEN|S)·pgen (e|S) where pgen is the neural language model. We parametrize ptrans as a feedforward neural network on an embedding of the stack hS (S). In initial experiments we found, consistent with Morey et al. (2017), that a model with neural embeddings as its only features performed poorly. We therefore compute the representation using both neural embeddings of the discourse units on the stack (Section 3.3.1) and a set of structural features extracted from the stack (Section 3.3.2). 3.3.1 Neural Embeddings To produce th"
D19-1233,P15-1033,0,0.018673,"vely as the nucleus if the nucleus is an EDU, or the nuclear EDU of the nucleus if the nucleus is itself a unit. For multinuclear relations, we take the left-most nucleus. Then, if Unit(r, n) UL UR is a unit and eN is its nuclear EDU, hEDU (eN ) is the embedding of the nuclear EDU, and hR (r, n) is an embedding of the nuclearity-relation pair (r, n) in a lookup table: hU (U ) = TREELSTM([hEDU (eN ); hR (r, n)], hU (UL ), hU (UR )) (5) where hU (UL ) and hU (UR ) are the hidden state and memory cell of the left and right argument of the unit respectively. We embed the stack with a stack LSTM (Dyer et al., 2015). If the stack contents are D1 |· · · |Dm with each Di being a discourse unit, then S S hN S (S) = LSTM (hU (D1:m ), h0 ) (6) 3.3.2 Structural Features We extract additional features from the stack that have been found to be useful in prior work. As in Braud et al. (2017), for each discourse unit, we extract the word embeddings of up to three words whose syntactic head is not in the unit, adding padding if there are fewer than three. We concatenate these features for the top two discourse units on the stack, using a dummy embedding if the stack only contains one discourse unit. We write hhead"
D19-1233,N16-1024,0,0.486855,"k (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng and Hirst, 2014a; Joty et al., 2015; Braud et al., 2017) have used locally normalized discriminative models. However, these are known to have worse performance than generative models when there is little training data (Ng and Jordan, 2002; Yogatama et al., 2017). Unlike locally normalised discriminative models, generative models are not susceptible to label bias (Lafferty et al., 2001). The success of generative (Dyer et al., 2016; Charniak et al., 2016) and globally normalised (Andor et al., 2016) syntactic parsers suggests that reducing label bias leads to better performance. We hypothesize that using a generative parser would also lead to improved performance on RST parsing. However, while they are free from label bias, generative parsers require more sophisticated search algorithms for decoding. Fried et al. (2017) presented a word-level beam search algorithm that made it possible to decode directly from neural generative parsers rather than using them as rerankers. In this paper, we present the first generative RS"
D19-1233,P14-1048,0,0.588331,"bsolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1 points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data. 1 Introduction Understanding a document’s discourse-level organization is important for correctly interpreting it, and discourse analyses have been shown to be helpful for several NLP tasks (Bhatia et al., 2015; Ji and Smith, 2017; Feng and Hirst, 2014b; Ferracane et al., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng"
D19-1233,I17-1059,0,0.0226243,"6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1 points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data. 1 Introduction Understanding a document’s discourse-level organization is important for correctly interpreting it, and discourse analyses have been shown to be helpful for several NLP tasks (Bhatia et al., 2015; Ji and Smith, 2017; Feng and Hirst, 2014b; Ferracane et al., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng and Hirst, 2014a; Joty et"
D19-1233,P17-2025,0,0.479973,"g data (Ng and Jordan, 2002; Yogatama et al., 2017). Unlike locally normalised discriminative models, generative models are not susceptible to label bias (Lafferty et al., 2001). The success of generative (Dyer et al., 2016; Charniak et al., 2016) and globally normalised (Andor et al., 2016) syntactic parsers suggests that reducing label bias leads to better performance. We hypothesize that using a generative parser would also lead to improved performance on RST parsing. However, while they are free from label bias, generative parsers require more sophisticated search algorithms for decoding. Fried et al. (2017) presented a word-level beam search algorithm that made it possible to decode directly from neural generative parsers rather than using them as rerankers. In this paper, we present the first generative RST parser1 . Our model is a document-level version of an RNN Grammar (RNNG, Dyer et al. (2016)) defined through a transition system with both word- and structure-generating actions. It uses distributed representations of discourse units and transition probabilities parametrized by RNNs to model unbounded dependencies in a document. For our discourse parser, we find that Fried et al. (2017)’s wo"
D19-1233,W16-3616,0,0.0541394,"Missing"
D19-1233,D16-1257,0,0.0571551,"Missing"
D19-1233,P14-1002,0,0.567919,"th, 2017; Feng and Hirst, 2014b; Ferracane et al., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng and Hirst, 2014a; Joty et al., 2015; Braud et al., 2017) have used locally normalized discriminative models. However, these are known to have worse performance than generative models when there is little training data (Ng and Jordan, 2002; Yogatama et al., 2017). Unlike locally normalised discriminative models, generative models are not susceptible to label bias (Lafferty et al., 2001). The success of generative (Dyer et al., 2016; Charniak et al., 2016) and globally normalised (Andor et al., 2016) syntactic parsers suggests that reducing label bias leads to better performance. We hypot"
D19-1233,N16-1037,0,0.0524378,"gorithm that made it possible to decode directly from neural generative parsers rather than using them as rerankers. In this paper, we present the first generative RST parser1 . Our model is a document-level version of an RNN Grammar (RNNG, Dyer et al. (2016)) defined through a transition system with both word- and structure-generating actions. It uses distributed representations of discourse units and transition probabilities parametrized by RNNs to model unbounded dependencies in a document. For our discourse parser, we find that Fried et al. (2017)’s word-level beam search algorithm is bi1 Ji et al. (2016) introduced a neural generative discourse parser, but they used the annotation scheme of the Penn Discourse Treebank (Prasad et al., 2008) and Switchboard Dialog Act (Godfrey et al., 1992) corpora, predicting flat discourse representations between adjacent sentences, rather than hierarchical relations among clauses. 2284 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2284–2295, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 3 Rhetorica"
D19-1233,P17-1092,0,0.0506627,"ias and results in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1 points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data. 1 Introduction Understanding a document’s discourse-level organization is important for correctly interpreting it, and discourse analyses have been shown to be helpful for several NLP tasks (Bhatia et al., 2015; Ji and Smith, 2017; Feng and Hirst, 2014b; Ferracane et al., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and E"
D19-1233,J15-3002,0,0.126699,"., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and Eisenstein, 2014; Feng and Hirst, 2014a; Joty et al., 2015; Braud et al., 2017) have used locally normalized discriminative models. However, these are known to have worse performance than generative models when there is little training data (Ng and Jordan, 2002; Yogatama et al., 2017). Unlike locally normalised discriminative models, generative models are not susceptible to label bias (Lafferty et al., 2001). The success of generative (Dyer et al., 2016; Charniak et al., 2016) and globally normalised (Andor et al., 2016) syntactic parsers suggests that reducing label bias leads to better performance. We hypothesize that using a generative parser woul"
D19-1233,D17-1136,0,0.825311,"y the next action distribution as p(aj |Sj ). To handle the unbounded number of possible EDUs, we parametrize the probabilities of GEN(e) actions using a neural language model. The next action distribution is factorised into a structural action distribution ptrans and a generation distribution pgen as in Buys and Blunsom (2018), so that p(RE(r, n)|S) = ptrans (RE(r, n)|S) and p(GEN(e)|S) = ptrans (GEN|S)·pgen (e|S) where pgen is the neural language model. We parametrize ptrans as a feedforward neural network on an embedding of the stack hS (S). In initial experiments we found, consistent with Morey et al. (2017), that a model with neural embeddings as its only features performed poorly. We therefore compute the representation using both neural embeddings of the discourse units on the stack (Section 3.3.1) and a set of structural features extracted from the stack (Section 3.3.2). 3.3.1 Neural Embeddings To produce the stack embedding, we first require embeddings for both EDUs and units. We embed EDUs with bidirectional LSTMs4 . If e is an EDU consisting of the word sequence w1:k , then (→) (w1:k , h→ h→ 0 ) k = LSTM (←) h← (wk:1 , h← 0 ) k = LSTM (2) where wt is the word embedding of wt . The embeddin"
D19-1233,E17-1117,0,0.0210675,"results in absolute improvements of 6.8 and 2.9 on unlabelled and labelled F1 over previous algorithms. Overall, our generative model outperforms a discriminative model with the same features by 2.6 F1 points and achieves performance comparable to the state-of-the-art, outperforming all published parsers from a recent replication study that do not use additional training data. 1 Introduction Understanding a document’s discourse-level organization is important for correctly interpreting it, and discourse analyses have been shown to be helpful for several NLP tasks (Bhatia et al., 2015; Ji and Smith, 2017; Feng and Hirst, 2014b; Ferracane et al., 2017). A popular formalism for discourse analysis is Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) (Fig. 1) which represents a document as a tree of discourse units recursively built by connecting smaller units through rhetorical relations. Learning to predict RST trees is difficult because it depends on pragmatics as well as literal meaning, and the English RST Discourse Treebank (RST-DT) (Carlson et al., 2003) is small by the standards of modern parsing datasets, with 347 training documents. Previous approaches to RST parsing (Ji and E"
D19-1233,P18-1132,1,0.896753,"Missing"
D19-1233,prasad-etal-2008-penn,0,0.0603041,"present the first generative RST parser1 . Our model is a document-level version of an RNN Grammar (RNNG, Dyer et al. (2016)) defined through a transition system with both word- and structure-generating actions. It uses distributed representations of discourse units and transition probabilities parametrized by RNNs to model unbounded dependencies in a document. For our discourse parser, we find that Fried et al. (2017)’s word-level beam search algorithm is bi1 Ji et al. (2016) introduced a neural generative discourse parser, but they used the annotation scheme of the Penn Discourse Treebank (Prasad et al., 2008) and Switchboard Dialog Act (Godfrey et al., 1992) corpora, predicting flat discourse representations between adjacent sentences, rather than hierarchical relations among clauses. 2284 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2284–2295, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics 3 Rhetorical Structure RNNGs J USTIFY In this section, we present a generative model for predicting RST trees given a document segmented into a seque"
D19-1233,N03-1030,0,0.190355,"than three. We concatenate these features for the top two discourse units on the stack, using a dummy embedding if the stack only contains one discourse unit. We write hhead (S) for these features. S We use a categorical feature for whether the top two discourse units are: in the same sentence; in different sentences; or incomparable since one of them spans multiple sentences. We also use an equivalent feature for paragraphs. Feature values are represented by embeddings in a lookup table. We write hcomp (S) for these features. S Finally, we extract features describing the dominance relation (Soricut and Marcu, 2003) between the top two discourse units on the stack. If there is a word in one discourse unit whose syntactic head is in the other, we extract the word embeddings of these two words as well as an embedding of the dependency relation between them, otherwise we use a single dummy embedding. We write hdom S (S) for these features. 2287 The structural feature representation is then the concatenation of these three features: hFS (S) = [hhead (S); hcomp (S); hdom S S (S)] S (7) and the full stack representation is the concatenation of the neural embedding and the feature representation: F hS (S) = [hN"
D19-1233,D17-1178,0,0.0243961,"ring model and the search algorithm. We can isolate bias in search algorithms by studying the trees they return when the scoring model contains no information. Intuitively, if the scoring model has no preference over trees, then any preference shown by the parser is the result of biases in the search algorithm. We tested whether the left-branching bias came from the word-level beam search (the search algorithm of Fried et al. (2017)) by using it to parse sequences of various lengths using a bottom-up RNNG with a uniform scoring model. We broke 6 We used candidate fast-tracking as described in Stern et al. (2017)’s extension to Fried et al. (2017)’s algorithm. 2288 1 Algorithm 1 Word-level Beam Search ties at beam cut-offs by uniform sampling without replacement. We measured branching bias using Sampson (1997)’s production-based measure of left-branching for parse trees which we write as PL (T ) for a tree T . The measure is the fraction of non-terminals whose left child is also a non-terminal, and varies from 0 for a fully right-branching tree to n−2 n−1 → 1 for a fully leftbranching tree, where n is the number of leaves. Figure 2 shows the median value of this measure for 100 trees each for sequence"
D19-1233,D16-1035,0,0.43598,"Missing"
D19-1233,N15-3001,0,0.0640996,"Missing"
D19-1233,P19-1410,0,0.310533,"eraged F1 scores on labelled attachment decisions as calculated by the EDUCE python package8 . We report F1 for predicting span attachments (S), span attachments with nuclearity (N), span attachments with relation labels (R) and span attachments with nuclearity and relation labels (F). We compare our results against the numbers from Morey et al. (2017), since they include several competitive parers under a consistent evaluation scheme.9 As a baseline, we use a discriminative version of 8 https://github.com/irit-melodi/educe 9 We do not compare against Yu et al. (2018), Zhang et al. (2018) and Lin et al. (2019)’s recent neural RST parsers since they do not evaluate labelled attachment decisions so their results are not comparable to ours. Training and Hyperparameters We use 300-dimensional word embeddings initialized to word2vec vectors (Mikolov et al., 2013). We tie the embeddings in the EDU LSTM and the decoder LSTM input and output embeddings. We use a 2-layer bidirectional LSTM with 512-dimensional hidden state for the EDU LSTM. The TreeLSTM composition function also has a 512-dimensional (in total) hidden state with 100-dimensional relation embeddings. The stack LSTM and decoder LSTM also have"
D19-1233,Q17-1012,0,0.0176644,"(Section 3.3.1) and a set of structural features extracted from the stack (Section 3.3.2). 3.3.1 Neural Embeddings To produce the stack embedding, we first require embeddings for both EDUs and units. We embed EDUs with bidirectional LSTMs4 . If e is an EDU consisting of the word sequence w1:k , then (→) (w1:k , h→ h→ 0 ) k = LSTM (←) h← (wk:1 , h← 0 ) k = LSTM (2) where wt is the word embedding of wt . The embedding for e, hEDU (e), is the concatenation of the final forward and backward hidden states: ← hEDU (e) = [h→ k ; hk ] (3) We embed units by composing their arguments with a Tree LSTM5 (Teng and Zhang, 2017). A Tree LSTM recursively composes vectors while using memory cells to track long-term dependencies. We produce a new representation for each EDU e by applying a linear transformation 4 We track memory cells and use them when updating the hidden state in LSTMs and Tree LSTMs, but use only the hidden states for stack embeddings. Initial hidden states and memory cells are learned parameters. 5 Since constituency trees are n-ary branching, RNNGs for constituency parsing have used a bidirectional LSTM composition function (Dyer et al., 2016; Kuncoro et al., 2017, 2018) to compose the variable numb"
D19-1233,C18-1047,0,0.159014,"wing this study we evaluate using micro-averaged F1 scores on labelled attachment decisions as calculated by the EDUCE python package8 . We report F1 for predicting span attachments (S), span attachments with nuclearity (N), span attachments with relation labels (R) and span attachments with nuclearity and relation labels (F). We compare our results against the numbers from Morey et al. (2017), since they include several competitive parers under a consistent evaluation scheme.9 As a baseline, we use a discriminative version of 8 https://github.com/irit-melodi/educe 9 We do not compare against Yu et al. (2018), Zhang et al. (2018) and Lin et al. (2019)’s recent neural RST parsers since they do not evaluate labelled attachment decisions so their results are not comparable to ours. Training and Hyperparameters We use 300-dimensional word embeddings initialized to word2vec vectors (Mikolov et al., 2013). We tie the embeddings in the EDU LSTM and the decoder LSTM input and output embeddings. We use a 2-layer bidirectional LSTM with 512-dimensional hidden state for the EDU LSTM. The TreeLSTM composition function also has a 512-dimensional (in total) hidden state with 100-dimensional relation embeddings."
D19-1233,D08-1059,1,0.674659,"= argmax p(x, y) (17) y∈Y(x) The search space grows exponentially with the input length, so we must perform inexact search as our model conditions on the entire relation structure of every subtree on the stack. Search is generally more difficult for generative models than for discriminative ones, requiring more complex search algorithms. For this reason, Dyer et al. (2016) used RNNGs only to rerank the output of a discriminative parser. Fried et al. (2017) presented the first algorithm for decoding directly from RNNGs to give competitive performance. They found that action-level beam search (Zhang and Clark, 2008) gave poor performance for constituency parsing with RNNGs. The problem was that GEN actions almost always have lower probabilities than structure-generating actions, causing computations where GEN actions come earlier to “fall off the beam” even if the completed computation would have a higher probability than other completed computations. To address this problem, Fried et al. (2017) proposed word-level beam search (Algorithm 1). Briefly, the algorithm keeps an array of beams indexed by the current position in the sequence and the number of structure-generating actions taken since this positi"
D19-1292,D18-1389,0,0.0250835,"her potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines. 1 Evidence: Bullitt is a 1968 American action thriller film directed by Peter Yates and produced by Philip D’Antoni Figure 1: Adversarial instances generated through rulebased transformations of existing claims Introduction Fact verification is the task of predicting whether claims can be supported or refuted by evidence. Advances in this task have been achieved through improved modelling and the availability of resources to train and validate systems (e.g. Wang (2017); Baly et al. (2018b); Thorne et al. (2018)). As this is a task with potentially sensitive applications like propaganda (Baly et al., 2018a) or biased news detection (Potthast et al., 2018), it is critical to understand how systems and models behave when exposed to real-world data and how deficiencies in their training data may contribute to this. It has been observed in related NLP tasks that as models become more complex, it is difficult to fully understand and characterize their behaviour (Samek et al., 2017). And from an NLP perspective, there has been an ongoing discussion as to what extent these models und"
D19-1292,N18-2004,0,0.0507936,"her potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines. 1 Evidence: Bullitt is a 1968 American action thriller film directed by Peter Yates and produced by Philip D’Antoni Figure 1: Adversarial instances generated through rulebased transformations of existing claims Introduction Fact verification is the task of predicting whether claims can be supported or refuted by evidence. Advances in this task have been achieved through improved modelling and the availability of resources to train and validate systems (e.g. Wang (2017); Baly et al. (2018b); Thorne et al. (2018)). As this is a task with potentially sensitive applications like propaganda (Baly et al., 2018a) or biased news detection (Potthast et al., 2018), it is critical to understand how systems and models behave when exposed to real-world data and how deficiencies in their training data may contribute to this. It has been observed in related NLP tasks that as models become more complex, it is difficult to fully understand and characterize their behaviour (Samek et al., 2017). And from an NLP perspective, there has been an ongoing discussion as to what extent these models und"
D19-1292,P18-2006,0,0.026822,"stances are expensive to construct, the attacker would have a high degree of confidence that the instances are correct and therefore correctness is not incorporated into the scoring metrics of any of these works. In FEVER, generating instances is more complex due to the need for annotators to highlight appropriate evidence: scaling up annotation to create new instances from scratch is non-trivial. Noise introduced by character-level perturbations: Character-level attacks highlight the brittleness of systems by making letter swaps or insertions. Belinkov and Bisk (2018), Naik et al. (2018) and Ebrahimi et al. (2018) generate distorted examples which cause misclassifications or translation errors. An evaluation is performed comparing the performance of human crowdworkers for a classification task which only indicated minimal losses in classification accuracy between the orig2945 inal and modified instances. While it is unlikely that a single character can unintentionally change the semantics of a sentence, requiring relabelling, this method is still intentionally introducing typographical errors meaning that by the definition of the FEVER task, the instances would be incorrect. Adding distractor informati"
D19-1292,N18-2017,0,0.035195,"18), it is critical to understand how systems and models behave when exposed to real-world data and how deficiencies in their training data may contribute to this. It has been observed in related NLP tasks that as models become more complex, it is difficult to fully understand and characterize their behaviour (Samek et al., 2017). And from an NLP perspective, there has been an ongoing discussion as to what extent these models understand language (Jia and Liang, 2017) or they are exploiting unintentional biases and cues that are present in the datasets they are trained on (Poliak et al., 2018; Gururangan et al., 2018). One of the diagnostic tools for understanding how models behave is adversarial evaluation, where data that is deliberately designed to induce classification errors is used to expose “blind spots” of a system. While there are many recently proposed techniques for generating adversarial instances for NLP tasks (surveyed in Section 2), they vary in the degree to which newly generated instances are correct, i.e. grammatical and appropriately labelled. In this paper, we introduce two scoring metrics, (adversarial) attack potency and system resilience, that enable comparison of both adversarial in"
D19-1292,W18-5516,0,0.298047,"vely expensive for some users. In our evaluation, we compare the rules generated using model predictions on a sample of 1000 instances from the development set against rules generated from the full development set containing 9999 instances. 5 Experimental setup We evaluate the potency of the adversarial instances generated by the approaches described in the previous section against four state-of-the-art models from the FEVER shared task and two baseline models. The models we evaluate were the Neuro-Semantic Matching Network (NSMN) (Nie et al., 2019), HexaF (Yoneda et al., 2018) Enhanced ESIM (Hanselowski et al., 2018) and the Transformer Model (Malon, 2018) as trained by the authors. For the baseline systems, we adopt the architecture from Thorne et al. (2018), using both a Decomposable Attention model and ESIM+ELMo model for natural language inference from AllenNLP (Gardner et al., 2017) and a Term Frequency - Inverse Document Frequency (TF-IDF) model for document retrieval and sentence selection. Adversarial instances are generated by applying each adversary to existing FEVER instances and making modifications to the claim and label where appropriate. We apply each adversary to the development and test s"
D19-1292,D17-1263,0,0.0685915,"Missing"
D19-1292,N18-1170,0,0.061202,"Missing"
D19-1292,D17-1215,0,0.0194961,"(2018)). As this is a task with potentially sensitive applications like propaganda (Baly et al., 2018a) or biased news detection (Potthast et al., 2018), it is critical to understand how systems and models behave when exposed to real-world data and how deficiencies in their training data may contribute to this. It has been observed in related NLP tasks that as models become more complex, it is difficult to fully understand and characterize their behaviour (Samek et al., 2017). And from an NLP perspective, there has been an ongoing discussion as to what extent these models understand language (Jia and Liang, 2017) or they are exploiting unintentional biases and cues that are present in the datasets they are trained on (Poliak et al., 2018; Gururangan et al., 2018). One of the diagnostic tools for understanding how models behave is adversarial evaluation, where data that is deliberately designed to induce classification errors is used to expose “blind spots” of a system. While there are many recently proposed techniques for generating adversarial instances for NLP tasks (surveyed in Section 2), they vary in the degree to which newly generated instances are correct, i.e. grammatical and appropriately lab"
D19-1292,W17-5405,0,0.0576766,"Missing"
D19-1292,W18-5517,0,0.129406,"compare the rules generated using model predictions on a sample of 1000 instances from the development set against rules generated from the full development set containing 9999 instances. 5 Experimental setup We evaluate the potency of the adversarial instances generated by the approaches described in the previous section against four state-of-the-art models from the FEVER shared task and two baseline models. The models we evaluate were the Neuro-Semantic Matching Network (NSMN) (Nie et al., 2019), HexaF (Yoneda et al., 2018) Enhanced ESIM (Hanselowski et al., 2018) and the Transformer Model (Malon, 2018) as trained by the authors. For the baseline systems, we adopt the architecture from Thorne et al. (2018), using both a Decomposable Attention model and ESIM+ELMo model for natural language inference from AllenNLP (Gardner et al., 2017) and a Term Frequency - Inverse Document Frequency (TF-IDF) model for document retrieval and sentence selection. Adversarial instances are generated by applying each adversary to existing FEVER instances and making modifications to the claim and label where appropriate. We apply each adversary to the development and test split of the dataset of Thorne et al. (20"
D19-1292,W13-3819,0,0.0747882,"Missing"
D19-1292,S18-2023,0,0.0965161,"Missing"
D19-1292,P18-1079,0,0.302965,"claims are S UPPORTED or R EFUTED by evidence from Wikipedia. Systems must return not only the correct label but also the sentences providing the evidence for it. In the case where there is not enough evidence in Wikipedia for either label, the label N OT ENOUGH I NFO (NEI) is applied and no evidence needs to be returned. We evaluate three adversarial attacks against four state-of-the-art systems and two baselines and apply our proposed scoring metrics that incorporate instance correctness. The first attack, informed by model behaviour, uses Semantically Equivalent Adversarial Rules (SEARs) (Ribeiro et al., 2018), a state-of-the-art method for generating rules that perform meaning-preserving transformations to instances that induce classification errors. The second attack is informed by dataset biases: we identify common patterns and constructions in the claims of the FEVER dataset and exploit them with hand-crafted rules to generate a number of new dataset instances. The final attack is a lexically-informed approach which makes use of a paraphrase model to generate new instances. Our findings indicate that the instances generated by hand-crafted dataset-informed rules reduced all systems’ classificat"
D19-1292,W17-5410,0,0.0590853,"Missing"
D19-1292,N18-1074,1,0.899511,"Missing"
D19-1292,P17-2067,0,0.107449,"acks have higher potency, and that while the rankings among the top systems changed, they exhibited higher resilience than the baselines. 1 Evidence: Bullitt is a 1968 American action thriller film directed by Peter Yates and produced by Philip D’Antoni Figure 1: Adversarial instances generated through rulebased transformations of existing claims Introduction Fact verification is the task of predicting whether claims can be supported or refuted by evidence. Advances in this task have been achieved through improved modelling and the availability of resources to train and validate systems (e.g. Wang (2017); Baly et al. (2018b); Thorne et al. (2018)). As this is a task with potentially sensitive applications like propaganda (Baly et al., 2018a) or biased news detection (Potthast et al., 2018), it is critical to understand how systems and models behave when exposed to real-world data and how deficiencies in their training data may contribute to this. It has been observed in related NLP tasks that as models become more complex, it is difficult to fully understand and characterize their behaviour (Samek et al., 2017). And from an NLP perspective, there has been an ongoing discussion as to what exte"
D19-1292,W18-5515,0,0.0375952,"ng the full dataset may be prohibitively expensive for some users. In our evaluation, we compare the rules generated using model predictions on a sample of 1000 instances from the development set against rules generated from the full development set containing 9999 instances. 5 Experimental setup We evaluate the potency of the adversarial instances generated by the approaches described in the previous section against four state-of-the-art models from the FEVER shared task and two baseline models. The models we evaluate were the Neuro-Semantic Matching Network (NSMN) (Nie et al., 2019), HexaF (Yoneda et al., 2018) Enhanced ESIM (Hanselowski et al., 2018) and the Transformer Model (Malon, 2018) as trained by the authors. For the baseline systems, we adopt the architecture from Thorne et al. (2018), using both a Decomposable Attention model and ESIM+ELMo model for natural language inference from AllenNLP (Gardner et al., 2017) and a Term Frequency - Inverse Document Frequency (TF-IDF) model for document retrieval and sentence selection. Adversarial instances are generated by applying each adversary to existing FEVER instances and making modifications to the claim and label where appropriate. We apply eac"
D19-1292,D18-1316,0,0.0432228,"Missing"
D19-1292,P18-1022,0,0.079985,"ction thriller film directed by Peter Yates and produced by Philip D’Antoni Figure 1: Adversarial instances generated through rulebased transformations of existing claims Introduction Fact verification is the task of predicting whether claims can be supported or refuted by evidence. Advances in this task have been achieved through improved modelling and the availability of resources to train and validate systems (e.g. Wang (2017); Baly et al. (2018b); Thorne et al. (2018)). As this is a task with potentially sensitive applications like propaganda (Baly et al., 2018a) or biased news detection (Potthast et al., 2018), it is critical to understand how systems and models behave when exposed to real-world data and how deficiencies in their training data may contribute to this. It has been observed in related NLP tasks that as models become more complex, it is difficult to fully understand and characterize their behaviour (Samek et al., 2017). And from an NLP perspective, there has been an ongoing discussion as to what extent these models understand language (Jia and Liang, 2017) or they are exploiting unintentional biases and cues that are present in the datasets they are trained on (Poliak et al., 2018; Gur"
D19-1292,D16-1264,0,0.109407,"Missing"
D19-1665,P19-1267,0,0.0569191,"Missing"
D19-1665,I13-1191,0,0.0156265,"illaryclinton spends millions on msm to discourage #americans voting #sanders Stances: Clinton: AGAINST, Sanders: FAVOR Moral Foundations Twitter (Dehghani et al., 2019) Utterance: blatant racism in #colorado, #blacklivesmatter http://fb.me/1ibyxmswm Stances: cheating, harm Figure 1: Examples from each of the datasets. Introduction Stance detection is an established task in the computational linguistics community, and is typically concerned with whether an utterance (e.g. a tweet) expresses an attitude (often positive, negative or neutral) against a target such as an entity e.g. a politician (Hasan and Ng, 2013; Mohammad et al., 2016), or another utterance, e.g. a previous tweet in a thread (Zubiaga et al., 2016). Thus stance detection is an important task for analyzing discourse in online forums and social media platforms and is a component in assessing the veracity of claims (Kochkina et al., 2018). When the stances are mutually exclusive as in the aforementioned cases, multiclass classification is an appropriate formulation for the task. Often, however, a text may express multiple stances simultaneously. Such cases need to be formulated as multilabel classification (Sorower, 2010), where an insta"
D19-1665,E17-2068,0,0.0491624,"labels). It is less harsh than accuracy (Exact Match Ratio) (Sorower, 2010), which requires the entire label combination to be predicted correctly. For the ETC dataset, where each 1 The original dataset contained 1,682 utterances, but we removed duplicates occurring in the training and test sets. 2 Originally seven but we dropped one domain after consultation with the authors. 5 Results In our experiments, we consider models that capture label dependencies explicitly as well as baselines that do not capture these. As our baselines, we consider binary relevance using FastText (FT) classifiers (Joulin et al., 2017) for each stance label in BBC/MFTC and politician in ETC, as well as a multi-task learning (MTL) approach (Ruder, 2017) where each of the classifiers becomes a task and they all operate on a shared hidden layer (hard parameter sharing). As models capturing dependencies, we considered three options: the combination of the cross label dependency loss with MTL (MTL-XLD), and the combinations of label powerset with FT and MTL (FT-LP and MTL-LP respectively). For the latter, each label combination becomes a task learned jointly with the rest. Further details on all models and parameter tuning are i"
D19-1665,C18-1288,0,0.0299028,"es from each of the datasets. Introduction Stance detection is an established task in the computational linguistics community, and is typically concerned with whether an utterance (e.g. a tweet) expresses an attitude (often positive, negative or neutral) against a target such as an entity e.g. a politician (Hasan and Ng, 2013; Mohammad et al., 2016), or another utterance, e.g. a previous tweet in a thread (Zubiaga et al., 2016). Thus stance detection is an important task for analyzing discourse in online forums and social media platforms and is a component in assessing the veracity of claims (Kochkina et al., 2018). When the stances are mutually exclusive as in the aforementioned cases, multiclass classification is an appropriate formulation for the task. Often, however, a text may express multiple stances simultaneously. Such cases need to be formulated as multilabel classification (Sorower, 2010), where an instance can receive multiple, non-mutually exclusive labels. The most commonly used approaches to multiclass classification treat the task by learning models for each label. However, such approaches do not model dependencies between the labels explicitly, i.e. that the presence of one label results"
D19-1665,S16-1003,0,0.0865818,"Missing"
D19-1665,N18-1100,0,0.144728,"Missing"
D19-1665,C16-1230,0,0.0183627,"ST, Sanders: FAVOR Moral Foundations Twitter (Dehghani et al., 2019) Utterance: blatant racism in #colorado, #blacklivesmatter http://fb.me/1ibyxmswm Stances: cheating, harm Figure 1: Examples from each of the datasets. Introduction Stance detection is an established task in the computational linguistics community, and is typically concerned with whether an utterance (e.g. a tweet) expresses an attitude (often positive, negative or neutral) against a target such as an entity e.g. a politician (Hasan and Ng, 2013; Mohammad et al., 2016), or another utterance, e.g. a previous tweet in a thread (Zubiaga et al., 2016). Thus stance detection is an important task for analyzing discourse in online forums and social media platforms and is a component in assessing the veracity of claims (Kochkina et al., 2018). When the stances are mutually exclusive as in the aforementioned cases, multiclass classification is an appropriate formulation for the task. Often, however, a text may express multiple stances simultaneously. Such cases need to be formulated as multilabel classification (Sorower, 2010), where an instance can receive multiple, non-mutually exclusive labels. The most commonly used approaches to multiclass"
D19-6601,D18-1316,0,0.0201437,"P × f (Yˆs,a , Ya ) P a∈A ca a∈A ca 3.1 For the ‘build-it’ phase, we report both FEVER score of the system over the FEVER shared task test set (Thorne et al., 2018a) and the resilience of the system over the FEVER2.0 test set that comprises adversarial instances submitted by the breakers. For the ‘break-it’ phase, we report the potency of attack over all systems and the correctness rate. For the ‘fix-it’ phase, we report the score delta compared to the system submitted in the ‘build-it’ phase. 3 Team DOMLIN (Stammbach and Neumann, 2019) used the document retrieval module of Hanselowski et al. (2018) and a BERT model for two-staged sentence selection based on the work by (Nie et al., 2019). They also use a BERT-based model for the NLI stage. The CUNLP team (Hidey et al.) used a combination of Google search and TF-IDF for document retrieval and a pointer network using features from BERT and trained with reinforcement learning. Finally, team GPLSI (Alonso-Reina et al., 2019) kept Hanselowski et al. (2018)’s document retrieval and NLI modules. For the sentence selection they converted both the claims and candidate evidence sentence into OpenIE-style triples using the extractor from Estevez-V"
D19-6601,D15-1075,0,0.0584501,"from the first shared task and under adversarial evaluation, all systems exhibited losses in FEVER score. There was a great variety in adversarial attack types as well as the techniques used to generate the attacks, In this paper, we present the results of the shared task and a summary of the systems, highlighting commonalities and innovations among participating systems. 1 Introduction Significant progress for a large number of natural language processing tasks has been made through the development of new deep neural models. Higher scores for shared tasks such as Natural Language Inference (Bowman et al., 2015) and Question Answering (Rajpurkar et al., 2016) have been achieved through models which are becoming increasingly complex. This complexity raises new challenges: as models become more complex, it becomes difficult to fully understand and characterize their behaviour. From an NLP perspective, 1 https://builditbreakit.org 1 Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 1–6 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics phase) that were hosted by the shared task organisers. Participants could experiment with attacks by submit"
D19-6601,W17-5401,0,0.0304124,"he first Fact Extraction and VERification (FEVER) shared task (Thorne et al., 2018b) focused on building systems that predict whether a textual claim is S UPPORTED or R EFUTED given evidence (see (Thorne et al., 2018a) for a task description), or N OT E NOUGH I NFORMATION in case Wikipedia does not have appropriate evidence to verify it. As automated systems for fact checking have potentially sensitive applications it is important to study the vulnerabilities of these systems, as well as the deficiencies of the datasets they are trained on. Such vulnerabilities were also the motivation behind Ettinger et al. (2017)’s NLP shared task that was inspired by the Build It, Break It, Fix It competition1 . The second Fact Extraction and VERification (FEVER2.0) shared task is building on the dataset of the first FEVER shared task, but adopted the setup of build-it, break-it, fix-it where builders submitted systems based on the original FEVER dataset and task definition; breakers generated adversarial examples targeting the systems built in the first stage; and finally, fixers implemented solutions to remedy the attacks from the second We present the results of the second Fact Extraction and VERification (FEVER2."
D19-6601,N18-2017,0,0.0216076,"ond Fact Extraction and VERification (FEVER2.0) Shared Task James Thorne University of Cambridge jt719@cam.ac.uk Andreas Vlachos University of Cambridge av308@cam.ac.uk Oana Cocarascu Imperial College London oana.cocarascu11@imperial.ac.uk Christos Christodoulopoulos Amazon chrchrs@amazon.co.uk Arpit Mittal Amazon mitarpit@amazon.co.uk Abstract there has been an ongoing discussion as to what extent these models understand language (Jia and Liang, 2017) or to what extent they are exploiting unintentional biases and cues that are present in the datasets they are trained on (Poliak et al., 2018; Gururangan et al., 2018). When a model is evaluated on data outside of the distribution defined (implicitly) by its training dataset, its behaviour is likely to be unpredictable; such “blind spots” can be exposed through adversarial evaluation (Szegedy et al., 2014). The first Fact Extraction and VERification (FEVER) shared task (Thorne et al., 2018b) focused on building systems that predict whether a textual claim is S UPPORTED or R EFUTED given evidence (see (Thorne et al., 2018a) for a task description), or N OT E NOUGH I NFORMATION in case Wikipedia does not have appropriate evidence to verify it. As automated sy"
D19-6601,W18-5516,0,0.123857,"effectiveness of breakers’ adversarial instances (a) on a builder’s system (s) through the average reduction in FEVER score (from a perfect system) on the set of predictions made by the system Yˆs,a . The score is weighted by the correctness rate ca of the adversarial instances. Instances are correct if they are grammatical, appropriately labeled and meet the annotation guidelines requirements described by Thorne et al. (2018a). The top 4 submission from the first shared task were submitted as baseline systems for this shared task: UNC (Nie et al., 2019), UCLMR (Yoneda et al., 2018), Athene (Hanselowski et al., 2018) and Papelo (Malon, 2018). Break-It In the second phase, “breakers”, were tasked with generating adversarial examples that induce classification errors for the existing systems. Breakers submitted a dataset of up to 1000 instances with equal number of instances for each of the three classes (S UPPORT, R EFUTE and N OT E NOUGH I N FORMATION ); half of which were released to fixers and half of which were retained as a blind test set. We considered only novel claims (i.e. not contained in the original FEVER dataset) as valid entries to the shared task. All of the claims in this submission were an"
D19-6601,W18-5517,0,0.0461722,"l instances (a) on a builder’s system (s) through the average reduction in FEVER score (from a perfect system) on the set of predictions made by the system Yˆs,a . The score is weighted by the correctness rate ca of the adversarial instances. Instances are correct if they are grammatical, appropriately labeled and meet the annotation guidelines requirements described by Thorne et al. (2018a). The top 4 submission from the first shared task were submitted as baseline systems for this shared task: UNC (Nie et al., 2019), UCLMR (Yoneda et al., 2018), Athene (Hanselowski et al., 2018) and Papelo (Malon, 2018). Break-It In the second phase, “breakers”, were tasked with generating adversarial examples that induce classification errors for the existing systems. Breakers submitted a dataset of up to 1000 instances with equal number of instances for each of the three classes (S UPPORT, R EFUTE and N OT E NOUGH I N FORMATION ); half of which were released to fixers and half of which were retained as a blind test set. We considered only novel claims (i.e. not contained in the original FEVER dataset) as valid entries to the shared task. All of the claims in this submission were annotated were annotated by"
D19-6601,D19-6617,0,0.147869,"Missing"
D19-6601,D19-6604,0,0.135246,"Missing"
D19-6601,S18-2023,0,0.0439041,"Missing"
D19-6601,D16-1264,0,0.0154588,"rial evaluation, all systems exhibited losses in FEVER score. There was a great variety in adversarial attack types as well as the techniques used to generate the attacks, In this paper, we present the results of the shared task and a summary of the systems, highlighting commonalities and innovations among participating systems. 1 Introduction Significant progress for a large number of natural language processing tasks has been made through the development of new deep neural models. Higher scores for shared tasks such as Natural Language Inference (Bowman et al., 2015) and Question Answering (Rajpurkar et al., 2016) have been achieved through models which are becoming increasingly complex. This complexity raises new challenges: as models become more complex, it becomes difficult to fully understand and characterize their behaviour. From an NLP perspective, 1 https://builditbreakit.org 1 Proceedings of the Second Workshop on Fact Extraction and VERification (FEVER), pages 1–6 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics phase) that were hosted by the shared task organisers. Participants could experiment with attacks by submitting small samples of 50 instances for scoring t"
D19-6601,D19-6616,0,0.0144841,"l., 2018a) and 3 new qualifying submissions from the ‘Build-It‘ def Resilience(s) = 2 P × f (Yˆs,a , Ya ) P a∈A ca a∈A ca 3.1 For the ‘build-it’ phase, we report both FEVER score of the system over the FEVER shared task test set (Thorne et al., 2018a) and the resilience of the system over the FEVER2.0 test set that comprises adversarial instances submitted by the breakers. For the ‘break-it’ phase, we report the potency of attack over all systems and the correctness rate. For the ‘fix-it’ phase, we report the score delta compared to the system submitted in the ‘build-it’ phase. 3 Team DOMLIN (Stammbach and Neumann, 2019) used the document retrieval module of Hanselowski et al. (2018) and a BERT model for two-staged sentence selection based on the work by (Nie et al., 2019). They also use a BERT-based model for the NLI stage. The CUNLP team (Hidey et al.) used a combination of Google search and TF-IDF for document retrieval and a pointer network using features from BERT and trained with reinforcement learning. Finally, team GPLSI (Alonso-Reina et al., 2019) kept Hanselowski et al. (2018)’s document retrieval and NLI modules. For the sentence selection they converted both the claims and candidate evidence sente"
D19-6601,N18-1074,1,0.938356,"co.uk Abstract there has been an ongoing discussion as to what extent these models understand language (Jia and Liang, 2017) or to what extent they are exploiting unintentional biases and cues that are present in the datasets they are trained on (Poliak et al., 2018; Gururangan et al., 2018). When a model is evaluated on data outside of the distribution defined (implicitly) by its training dataset, its behaviour is likely to be unpredictable; such “blind spots” can be exposed through adversarial evaluation (Szegedy et al., 2014). The first Fact Extraction and VERification (FEVER) shared task (Thorne et al., 2018b) focused on building systems that predict whether a textual claim is S UPPORTED or R EFUTED given evidence (see (Thorne et al., 2018a) for a task description), or N OT E NOUGH I NFORMATION in case Wikipedia does not have appropriate evidence to verify it. As automated systems for fact checking have potentially sensitive applications it is important to study the vulnerabilities of these systems, as well as the deficiencies of the datasets they are trained on. Such vulnerabilities were also the motivation behind Ettinger et al. (2017)’s NLP shared task that was inspired by the Build It, Break"
D19-6601,D19-1292,1,0.835647,"resilience to adversarial attack. Task Phases In what follows we describe the three phases of FEVER2.0 in more detail: Build-It In the first phase of the shared task, “builders” constructed fact verification systems that were trained using the FEVER dataset released in Thorne et al. (2018a). Participants were required to submit docker images of systems which implemented a common web API that would facilitate interactive development of attacks through a sandbox which was hosted for the duration of the shared task. 2.2 Scoring Method The submissions were scored using ‘potency’ and ‘resilience’ (Thorne et al., 2019) that compute a weighted average of FEVER scores: accounting for the correctness of adversarial instances. Potency Intuitively, better adversarial instances induce more classification errors, resulting in a lower FEVER score of the systems they are evaluated on. We measure the effectiveness of breakers’ adversarial instances (a) on a builder’s system (s) through the average reduction in FEVER score (from a perfect system) on the set of predictions made by the system Yˆs,a . The score is weighted by the correctness rate ca of the adversarial instances. Instances are correct if they are grammati"
D19-6601,W18-5501,1,0.827898,"Missing"
D19-6601,W18-5515,0,0.0272972,"e evaluated on. We measure the effectiveness of breakers’ adversarial instances (a) on a builder’s system (s) through the average reduction in FEVER score (from a perfect system) on the set of predictions made by the system Yˆs,a . The score is weighted by the correctness rate ca of the adversarial instances. Instances are correct if they are grammatical, appropriately labeled and meet the annotation guidelines requirements described by Thorne et al. (2018a). The top 4 submission from the first shared task were submitted as baseline systems for this shared task: UNC (Nie et al., 2019), UCLMR (Yoneda et al., 2018), Athene (Hanselowski et al., 2018) and Papelo (Malon, 2018). Break-It In the second phase, “breakers”, were tasked with generating adversarial examples that induce classification errors for the existing systems. Breakers submitted a dataset of up to 1000 instances with equal number of instances for each of the three classes (S UPPORT, R EFUTE and N OT E NOUGH I N FORMATION ); half of which were released to fixers and half of which were retained as a blind test set. We considered only novel claims (i.e. not contained in the original FEVER dataset) as valid entries to the shared task. All of th"
E17-2043,bogdanova-lazaridou-2014-cross,0,0.0377882,"Missing"
E17-2043,P11-1030,0,0.335794,"Missing"
E17-2043,D14-1181,0,0.00396687,"ains. 3 p(y|x) = sof tmax(BAx) (1) where x is the frequency vector of features for the document, the weight matrix A is a dictionary containing the embeddings learned for each feature, and B is a weight matrix that is learned to predict the label correctly using the learned representations (essentially averaged feature embeddings). Since the documents in this model are represented as bags of discrete features, sequence information is lost. To recover some of this information we will consider feature n-grams, similar to the way convolutional neural network architectures incorporate word order (Kim, 2014) but with a simpler architecture. The proposed model ignores long-range dependencies that could conceivably be captured using alternative architectures, such as recurrent neural networks (RNN) (Mikolov et al., 2010; Luong et al., 2013). However, topical and stylistic information is contained in shorter word and character sequences for which the shallow neural network architectures with n-gram feature representations are likely to be sufficient, while having the advantage of being much faster to run. This is particularly important for authorship attribution tasks which normally involves documen"
E17-2043,rose-etal-2002-reuters,1,0.427592,"Missing"
E17-2043,W13-3512,0,0.020701,"to predict the label correctly using the learned representations (essentially averaged feature embeddings). Since the documents in this model are represented as bags of discrete features, sequence information is lost. To recover some of this information we will consider feature n-grams, similar to the way convolutional neural network architectures incorporate word order (Kim, 2014) but with a simpler architecture. The proposed model ignores long-range dependencies that could conceivably be captured using alternative architectures, such as recurrent neural networks (RNN) (Mikolov et al., 2010; Luong et al., 2013). However, topical and stylistic information is contained in shorter word and character sequences for which the shallow neural network architectures with n-gram feature representations are likely to be sufficient, while having the advantage of being much faster to run. This is particularly important for authorship attribution tasks which normally involves documents that are much longer than the single sentences which RNNs typically model. Continuous n-grams Representations 4 This work focuses on learning continuous n-gram representations for authorship attribution tasks. Continuous representat"
E17-2043,W06-1657,0,0.0337857,"Missing"
E17-2043,N15-1010,0,0.15788,"chos and Mark Stevenson Department of Computer Science, University of Sheffield, UK {y.sari, a.vlachos, mark.stevenson}@sheffield.ac.uk Abstract Previous studies have found that word and character-level n-grams are the most effective features for identifying authors (Peng et al., 2003; Stamatatos, 2013; Schwartz et al., 2013). Word n-grams can represent local structure of texts and document topic (Coyotl-Morales et al., 2006; Wang and Manning, 2012). On the other hand, character n-grams have been shown to be effective for capturing stylistic and morphological information (Koppel et al., 2011; Sapkota et al., 2015). However, previous work relied on discrete feature representations which suffer from data sparsity and do not consider the semantic relatedness between features. To address this problem we propose the use of continuous n-gram representations learned jointly with the classifier as a feedforward neural network. Continuous n-grams representations combine the advantages of n-grams features and continuous representations. The proposed method outperforms the prior state-of-theart approaches on two out of four datasets while producing comparable results for the remaining two. This paper presents wor"
E17-2043,D13-1193,0,0.0301519,"rd-based approaches on both datasets. In addition, this results also support the superiority of character n-grams that have been reported in the previous work (Peng et al., 2003; Stamatatos, Figure 1: Accuracy on IMDb62 data subset with varying number of authors 5.1 Domain Influence Word vs Character Table 2 demonstrates that the character models are superior to the word models. In particular, we found that models which employ character level n-grams appear to be more suitable for datasets with a large number of authors, i.e. CCAT50 and IMDb62. To explore this further, we ran an addi270 2013; Schwartz et al., 2013). Acknowledgments 5.3 We thank the anonymous reviewers for their comments. The first author would like to acknowledge Indonesia Endowment Fund for Education (LPDP) for support in the form of a doctoral studentship. Feature Contributions An ablation study was performed to further explore the influence of different types of features by removing a single class of n-grams. For this experiment the character model was used on the two CCAT datasets. Three feature types are defined including: References Shlomo Argamon, Casey Whitelaw, Paul Chase, Sobhan Raj Hota, Navendu Garg, and Shlomo Levitan. 2007"
E17-2043,J14-2003,0,\N,Missing
E17-3010,P14-5010,0,0.00304107,"he table and apply a set of simple rules to extract subject, predicate, object tuples, and for each named entity and numeric value we generate a query containing the entity name and the predicate. For example, the entry (Germany,Population:2015,81413145) is converted to the query “Germany” Population 2015. The queries are then executed on the Bing search engine and the top 50 web-page results are retained. We extract the text from the webpages using a script built around the BeautifulSoup package.4 This text is parsed and annotated with co-reference chains using the Stanford CoreNLP pipeline (Manning et al., 2014). Each sentence containing a mention of an entity and a number is used to generate a training example. The examples are labeled as follows. If the absolute percentage error between the value in the KB and the number extracted from text is below a threshold (an adjustable hyperparameter), the training instance is marked as a positive instance. Sentences which contain a number outside of this threshold are marked as negative instances. We make an exception for numbers tagged as dates where an exact match is required. For each claim, the feature generation function, φ(r, c), outputs lexical and s"
E17-3010,P09-1113,0,0.0905227,"sing claims taken from Vlachos and Riedel (2015). We make the source code publicly available to the community. 2 Design Considerations We developed our fact-checking approach in the context of the HeroX challenge2 – a competition organised by the fact checking organization FullFact3 . The types of claims the system presented can fact check was restricted to those which require looking up a value in a KB, similar to the one in Figure 1. To learn a model to perform the KB look up (essentially a semantic parsing task), we extend the work of Vlachos and Riedel (2015) who used distant supervision (Mintz et al., 2009) to generate training data, obviating the need for manual labeling. In particular, we extend it to handle simple temporal expressions in order to fact check time-dependent claims appropriately, i. e. population in 2015. While the recently proposed semantic parser of Pasupat and Liang (2015) is also able to handle temporal expressions, it makes the assumption that the table against which the claim needs to be interpreted is known, which is unrealistic in the context of fact checking. Furthermore, the system we propose can predict relations from the KB on which the semantic parser has not been t"
E17-3010,P15-1142,0,0.0308527,"The types of claims the system presented can fact check was restricted to those which require looking up a value in a KB, similar to the one in Figure 1. To learn a model to perform the KB look up (essentially a semantic parsing task), we extend the work of Vlachos and Riedel (2015) who used distant supervision (Mintz et al., 2009) to generate training data, obviating the need for manual labeling. In particular, we extend it to handle simple temporal expressions in order to fact check time-dependent claims appropriately, i. e. population in 2015. While the recently proposed semantic parser of Pasupat and Liang (2015) is also able to handle temporal expressions, it makes the assumption that the table against which the claim needs to be interpreted is known, which is unrealistic in the context of fact checking. Furthermore, the system we propose can predict relations from the KB on which the semantic parser has not been trained, a paradigm referred to as zero-shot learning (Larochelle et al., 2008). We achieve this by learning a binary classifier that assesses how well the claim “matches” each relation in the KB. Finally, another consideration in our design is algorithmic accountability (Diakopoulos, 2016)"
E17-3010,D11-1147,0,0.0892765,"onstrate the extensible nature of our system by evaluating it on relations used in previous work. We make our system publicly available so that it can be used and extended by the community.1 1 Figure 1: Fact checking a claim by matching it to an entry in the knowldge base. a verdict. For example, in the claim of Figure 1 a system needs to recognize the named entity (Germany), the statistical property (population) and the year, link them to appropriate elements in a KB, and deduce the truthfulness of the claim using the absolute percentage error. We contrast this task against rumour detection (Qazvinian et al., 2011) – a similar prediction task based on language subjectivity and growth of readership through a social network. While these are important factors to consider, a sentence can be true or false regardless of whether it is a rumour (Lukasik et al., 2016). Existing fact checking systems are capable of detecting fact-check-worthy claims in text (Hassan et al., 2015b), returning semantically similar textual claims (Walenz et al., 2014); and scoring the truth of triples on a knowledge graph through semantic distance (Ciampaglia et al., 2015). However, neither of these are suitable for fact checking a c"
E17-3010,W14-2508,1,0.842039,"describe its architecture and design decisions, evaluate its accuracy and discuss future work. We Introduction Fact checking is the task of assessing the truthfulness in spoken or written language. We are motivated by calls to provide tools to support journalists with resources to verify content at source (Cohen et al., 2011) or upon distribution. Manual verification can be too slow to verify information given the speed at which claims travel on social networks (Hassan et al., 2015a). In the context of natural language processing research, the task of automated fact checking was discussed by Vlachos and Riedel (2014). Given a claim, a system for this task must determine what information is needed to support or refute the claim, retrieve the information from a knowledge base (KB) and then compute a deduction to assign 1 https://github.com/sheffieldnlp/ numerical-fact-checking-eacl2017 37 Proceedings of the EACL 2017 Software Demonstrations, Valencia, Spain, April 3-7 2017, pages 37–40 c 2017 Association for Computational Linguistics highlight the ease of incorporating new information sources to fact check, which may be unavailable during training. To validate the extensibility of the system, we complete an"
E17-3029,P09-1039,0,0.0167833,"Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in n"
E17-3029,N13-1008,1,0.77865,"Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in news https://github.com/andre-martins/ TurboParser 118 Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. 2016. Is neural machine translation ready for deployment? A case study on 30 translation directions. CoRR, abs/1610.01108. documents are connected. 2.9 Storyline Construction and Summarization Storylines are co"
E17-3029,E17-3017,1,0.751559,"m a multilingual corpus of nearly 600k documents in 8 of the 9 SUMMA languages (all except Latvian), which were manually annotated by journalists at Deutsche Welle. The document model is a hierarchical attention network with attention at each level of the hierarchy, inspired by Yang et al. (2016), followed by a sigmoid classification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is"
E17-3029,P13-1020,0,0.025126,"Missing"
E17-3029,E17-1051,1,0.815701,"lassification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 K"
E17-5003,P15-1136,0,0.0132507,"on learning is applied to semantic parsing (Goodman et al., 2016), and how it can benefit natural language generation (Lampouras and Vlachos, 2016), where the search space is all English sentences. In this process we will explain techniques that can enhance imitation learning and solve problems that arise in each practice, such as focused costing, noise reduction, targeted exploration, sequence correction, and change propagation. Finally, we will briefly present applications on biomedical event extraction (Vlachos and Craven, 2011), feature selection (He et al., 2013), coreference resolution (Clark and Manning, 2015), autonomous driving learning (Zhang and Cho, 2016), and pruning policies for syntactic parsing (Vieira and Eisner, 2016). 4 Structure Part I: Imitation Learning Algorithms (90 minutes) • Introduction, basic concepts and intuition: Structured prediction basics, per-action and end-to-end supervision, and decomposability. • Detailed algorithm descriptions: Exact imitation, DAgger, roll outs and non-decomposable losses, latent variables, and cost-sensitive classification. • Advanced topics: Imitation learning by coaching, imitation learning for recurrent neural network training, bandit learning,"
E17-5003,Q13-1033,0,0.0152028,"a comparative overview of the various algorithms presented that will expose their differences and their practical implications. We will conclude by discussing the relation of imitation learning to recurrent neural networks, bandit learning, adversarial learning, and reinforcement learning. 3 Part II: Applications in NLP In the second part, we will discuss recent work applying imitation learning methods in the context of NLP. We will begin by reviewing the application of imitation learning to syntactic dependency parsing and discuss how to create expert policies, also known as dynamic oracles (Goldberg and Nivre, 2013). Furthermore, we will review how imitation learning is applied to semantic parsing (Goodman et al., 2016), and how it can benefit natural language generation (Lampouras and Vlachos, 2016), where the search space is all English sentences. In this process we will explain techniques that can enhance imitation learning and solve problems that arise in each practice, such as focused costing, noise reduction, targeted exploration, sequence correction, and change propagation. Finally, we will briefly present applications on biomedical event extraction (Vlachos and Craven, 2011), feature selection (H"
E17-5003,P16-1001,1,0.827231,"cal implications. We will conclude by discussing the relation of imitation learning to recurrent neural networks, bandit learning, adversarial learning, and reinforcement learning. 3 Part II: Applications in NLP In the second part, we will discuss recent work applying imitation learning methods in the context of NLP. We will begin by reviewing the application of imitation learning to syntactic dependency parsing and discuss how to create expert policies, also known as dynamic oracles (Goldberg and Nivre, 2013). Furthermore, we will review how imitation learning is applied to semantic parsing (Goodman et al., 2016), and how it can benefit natural language generation (Lampouras and Vlachos, 2016), where the search space is all English sentences. In this process we will explain techniques that can enhance imitation learning and solve problems that arise in each practice, such as focused costing, noise reduction, targeted exploration, sequence correction, and change propagation. Finally, we will briefly present applications on biomedical event extraction (Vlachos and Craven, 2011), feature selection (He et al., 2013), coreference resolution (Clark and Manning, 2015), autonomous driving learning (Zhang and"
E17-5003,D13-1152,0,0.0696143,"Missing"
E17-5003,C16-1105,1,0.838573,"learning to recurrent neural networks, bandit learning, adversarial learning, and reinforcement learning. 3 Part II: Applications in NLP In the second part, we will discuss recent work applying imitation learning methods in the context of NLP. We will begin by reviewing the application of imitation learning to syntactic dependency parsing and discuss how to create expert policies, also known as dynamic oracles (Goldberg and Nivre, 2013). Furthermore, we will review how imitation learning is applied to semantic parsing (Goodman et al., 2016), and how it can benefit natural language generation (Lampouras and Vlachos, 2016), where the search space is all English sentences. In this process we will explain techniques that can enhance imitation learning and solve problems that arise in each practice, such as focused costing, noise reduction, targeted exploration, sequence correction, and change propagation. Finally, we will briefly present applications on biomedical event extraction (Vlachos and Craven, 2011), feature selection (He et al., 2013), coreference resolution (Clark and Manning, 2015), autonomous driving learning (Zhang and Cho, 2016), and pruning policies for syntactic parsing (Vieira and Eisner, 2016)."
E17-5003,Q14-1042,1,0.855431,"n, and show how they can be applied to a variety of NLP tasks. All material associated with the tutorial will be made available through https://sheffieldnlp.github.io/ImitationLearningTutorialEACL2017/. 2 Part I: Imitation Learning In the first part, we will give a unified presentation of imitation learning for structured prediction focusing on the intuition behind the framework. We will then delve into the details of the different algorithms that have been proposed so 1 far under the imitation learning paradigm, including Searn (Daum´e III et al., 2009), DAgger (Ross et al., 2011), v-DAgger (Vlachos and Clark, 2014), and lols (Chang et al., 2015). Furthermore, we will give a comparative overview of the various algorithms presented that will expose their differences and their practical implications. We will conclude by discussing the relation of imitation learning to recurrent neural networks, bandit learning, adversarial learning, and reinforcement learning. 3 Part II: Applications in NLP In the second part, we will discuss recent work applying imitation learning methods in the context of NLP. We will begin by reviewing the application of imitation learning to syntactic dependency parsing and discuss how"
E17-5003,W11-0307,1,0.801814,"n as dynamic oracles (Goldberg and Nivre, 2013). Furthermore, we will review how imitation learning is applied to semantic parsing (Goodman et al., 2016), and how it can benefit natural language generation (Lampouras and Vlachos, 2016), where the search space is all English sentences. In this process we will explain techniques that can enhance imitation learning and solve problems that arise in each practice, such as focused costing, noise reduction, targeted exploration, sequence correction, and change propagation. Finally, we will briefly present applications on biomedical event extraction (Vlachos and Craven, 2011), feature selection (He et al., 2013), coreference resolution (Clark and Manning, 2015), autonomous driving learning (Zhang and Cho, 2016), and pruning policies for syntactic parsing (Vieira and Eisner, 2016). 4 Structure Part I: Imitation Learning Algorithms (90 minutes) • Introduction, basic concepts and intuition: Structured prediction basics, per-action and end-to-end supervision, and decomposability. • Detailed algorithm descriptions: Exact imitation, DAgger, roll outs and non-decomposable losses, latent variables, and cost-sensitive classification. • Advanced topics: Imitation learning b"
N16-1138,D15-1075,0,0.0121331,"o in that correct prediction requires considering entailment relation between the claim and the headline. It also differs from work on target-specific stance prediction in debates (Walker et al., 2012; Hasan and Ng, 2013), since the targets considered there are topic labels such as abortion, instead of event claims as in this work. Emergent, being derived from the workflow of journalists is more realistic than data-sets designed for textual entailment such as FraCas (Cooper et al., 1996) and SICK (Marelli et al., 2014) that are constructed artificially. Compared to the crowdsourced dataset of Bowman et al. (2015), it is smaller but of a different nature, since the former assumes that all sentences are visual representations, while news tend be more varied. Stance detection in the context of Emergent is one component in the process of fact-checking claims appearing in the news which are usually more complex than the entity-relation-entity or entity-property-number triples considered in previous work (Nakashole and Mitchell, 2014; Vlachos and Riedel, 2015). The choice of claims to factcheck is a task in its own right, as shown by Hassan et al. (2015). Finally, the only other use of data from the Emergen"
N16-1138,I13-1191,0,0.120224,"help, especially when compared to PPDB, can be partly attributed to the inability of methods relying solely on contexts to learn antonymy. 5 Related work The task defined by the Emergent dataset differs from recent work in stance classification (Qazvinian et al., 2011; Lukasik et al., 2015; Zhao et al., 2015) not only in the number of claims from which the article headlines are derived, but also in that correct prediction requires considering entailment relation between the claim and the headline. It also differs from work on target-specific stance prediction in debates (Walker et al., 2012; Hasan and Ng, 2013), since the targets considered there are topic labels such as abortion, instead of event claims as in this work. Emergent, being derived from the workflow of journalists is more realistic than data-sets designed for textual entailment such as FraCas (Cooper et al., 1996) and SICK (Marelli et al., 2014) that are constructed artificially. Compared to the crowdsourced dataset of Bowman et al. (2015), it is smaller but of a different nature, since the former assumes that all sentences are visual representations, while news tend be more varied. Stance detection in the context of Emergent is one com"
N16-1138,D15-1311,0,0.0245888,"tance ALL observing for, observing for for, against for, observing against Table 2: Ablation results: each row represents the drop in accuracy caused by removing the corresponding feature(s). The last column shows for which stance label(s) the feature(s) had non-zero weight(s). the PPDB. Finally, the fact that -word2vec did not help, especially when compared to PPDB, can be partly attributed to the inability of methods relying solely on contexts to learn antonymy. 5 Related work The task defined by the Emergent dataset differs from recent work in stance classification (Qazvinian et al., 2011; Lukasik et al., 2015; Zhao et al., 2015) not only in the number of claims from which the article headlines are derived, but also in that correct prediction requires considering entailment relation between the claim and the headline. It also differs from work on target-specific stance prediction in debates (Walker et al., 2012; Hasan and Ng, 2013), since the targets considered there are topic labels such as abortion, instead of event claims as in this work. Emergent, being derived from the workflow of journalists is more realistic than data-sets designed for textual entailment such as FraCas (Cooper et al., 1996)"
N16-1138,P14-5008,0,0.0208144,"Missing"
N16-1138,P14-5010,0,0.00297184,"Headline features The features extracted from the headline are the commonly used bag of words representation (BoW) and whether it ends in a question mark (Q). In addition, we added two features representing the minimum distance from the root of the sentence to common refuting (e. g. deny) and hedging/reporting (e.g. claim, presumably) words (RootDist). As an example of the RootDist feature, consider the dependency parse in Figure 2. The minimum number of edges from the root to a hedging/refuting word (“not” in the example) is three. The dependency parses were obtained using Stanford CoreNLP (Manning et al., 2014) and the word lists were compiled using online resources. Claim-headline features While the article headline often provides adequate features to classify its stance, we also need to take into account its entailment relation with the claim. Therefore, based on the work by Rus and Lintean (2012) we compute an alignment using the Paraphrase Database (PPDB) (Pavlick et al., 2015) and the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) as follows. For each word pairing between the claim and the headline an edge is created and assigned a score by the following scheme: • if the stems of the words"
N16-1138,marelli-etal-2014-sick,0,0.0060572,"al., 2015) not only in the number of claims from which the article headlines are derived, but also in that correct prediction requires considering entailment relation between the claim and the headline. It also differs from work on target-specific stance prediction in debates (Walker et al., 2012; Hasan and Ng, 2013), since the targets considered there are topic labels such as abortion, instead of event claims as in this work. Emergent, being derived from the workflow of journalists is more realistic than data-sets designed for textual entailment such as FraCas (Cooper et al., 1996) and SICK (Marelli et al., 2014) that are constructed artificially. Compared to the crowdsourced dataset of Bowman et al. (2015), it is smaller but of a different nature, since the former assumes that all sentences are visual representations, while news tend be more varied. Stance detection in the context of Emergent is one component in the process of fact-checking claims appearing in the news which are usually more complex than the entity-relation-entity or entity-property-number triples considered in previous work (Nakashole and Mitchell, 2014; Vlachos and Riedel, 2015). The choice of claims to factcheck is a task in its o"
N16-1138,P14-1095,0,0.0862625,"Missing"
N16-1138,P15-2070,0,0.017548,"Missing"
N16-1138,D11-1147,0,0.670479,"Missing"
N16-1138,W12-2018,0,0.0149235,"d hedging/reporting (e.g. claim, presumably) words (RootDist). As an example of the RootDist feature, consider the dependency parse in Figure 2. The minimum number of edges from the root to a hedging/refuting word (“not” in the example) is three. The dependency parses were obtained using Stanford CoreNLP (Manning et al., 2014) and the word lists were compiled using online resources. Claim-headline features While the article headline often provides adequate features to classify its stance, we also need to take into account its entailment relation with the claim. Therefore, based on the work by Rus and Lintean (2012) we compute an alignment using the Paraphrase Database (PPDB) (Pavlick et al., 2015) and the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) as follows. For each word pairing between the claim and the headline an edge is created and assigned a score by the following scheme: • if the stems of the words are identical, assign maxScore • else, if the words are paraphrases according to PPDB, assign their maximum paraphrase score • else, assign minScore 3 Specifically, we used the sklearn LogisticRegression classifier with the default parameters, and L1 penalization. 1165 maxScore and minScore we"
N16-1138,W14-2508,1,0.417248,"regators lift stories from social media and re-publish them without fact-checking. This issue could be helped by developing methods for automated fact-checking of news stories, part Andreas Vlachos Department of Computer Science University of Sheffield, UK of the reporter’s black box envisioned in Cohen et al. (2011) and one of the main objectives in computational journalism. While this task is related to a variety of natural language processing tasks such as textual entailment and machine comprehension, it poses additional challenges due to its opendomain, real-world nature. Previous work by Vlachos and Riedel (2014) proposed using data from fact-checking websites such as Politifact1 , but the labelling provided by the journalists is only the degree of truthfulness of the claims, without any machinereadable verdicts to supervise the various steps in deciding it. Thus, the task defined by the dataset proposed remains too challenging for the NLP methods currently available. In this paper we propose to use data from the Emergent Project (Silverman, 2015), a rumour debunking project carried out in collaboration with the Tow Center for Digital Journalism at Columbia Journalism School2 . Consisting of 300 claim"
N16-1138,D15-1312,1,0.853622,"entailment such as FraCas (Cooper et al., 1996) and SICK (Marelli et al., 2014) that are constructed artificially. Compared to the crowdsourced dataset of Bowman et al. (2015), it is smaller but of a different nature, since the former assumes that all sentences are visual representations, while news tend be more varied. Stance detection in the context of Emergent is one component in the process of fact-checking claims appearing in the news which are usually more complex than the entity-relation-entity or entity-property-number triples considered in previous work (Nakashole and Mitchell, 2014; Vlachos and Riedel, 2015). The choice of claims to factcheck is a task in its own right, as shown by Hassan et al. (2015). Finally, the only other use of data from the Emergent project is by Liu et al. (2015); however their focus was not on the NLP aspects of the task but on using Twitter data to assess the veracity of the claim, ignoring the articles and their stances curated by the journalists. 6 Conclusions - Future work In this paper we proposed Emergent, a new realworld dataset derived from the digital journalism project Emergent which can be used for a variety of NLP tasks in the context of fact-checking. We foc"
N16-1138,N12-1072,0,0.190669,"hat -word2vec did not help, especially when compared to PPDB, can be partly attributed to the inability of methods relying solely on contexts to learn antonymy. 5 Related work The task defined by the Emergent dataset differs from recent work in stance classification (Qazvinian et al., 2011; Lukasik et al., 2015; Zhao et al., 2015) not only in the number of claims from which the article headlines are derived, but also in that correct prediction requires considering entailment relation between the claim and the headline. It also differs from work on target-specific stance prediction in debates (Walker et al., 2012; Hasan and Ng, 2013), since the targets considered there are topic labels such as abortion, instead of event claims as in this work. Emergent, being derived from the workflow of journalists is more realistic than data-sets designed for textual entailment such as FraCas (Cooper et al., 1996) and SICK (Marelli et al., 2014) that are constructed artificially. Compared to the crowdsourced dataset of Bowman et al. (2015), it is smaller but of a different nature, since the former assumes that all sentences are visual representations, while news tend be more varied. Stance detection in the context o"
N18-1074,W02-0109,0,0.268437,"Missing"
N18-1074,D14-1059,0,0.507624,"0K labelled claim-article pairs, combining 300 claims with 2,582 articles. The claims and the articles were curated and labeled by journalists in the context of the Emergent Project (Silverman, 2015), and the dataset was first proposed by Ferreira and Vlachos (2016), who only classified the claim w.r.t. the article headline instead of the whole article. Similar to recognizing textual entailment (RTE) (Dagan et al., 2009), the systems were provided with the sources to verify against, instead of having to retrieve them. A differently motivated but closely related dataset is the one developed by Angeli and Manning (2014) to evaluate natural logic inference for common sense reasoning, as it evaluated simRelated Works Vlachos and Riedel (2014) constructed a dataset for claim verification consisting of 106 claims, selecting data from fact-checking websites such as PolitiFact, taking advantage of the labelled claims available there. However, in order to develop claim verification components we typically require the justification for each verdict, including the sources used. While this information is usually available in justifications provided by the journalists, they are not in a machine-readable form. Thus, als"
N18-1074,D15-1075,0,0.798033,"e evidence. Another related task is question answering (QA), for which approaches have recently been extended to handle large-scale resources such as Wikipedia (Chen et al., 2017). However, questions typically provide the information needed to identify the answer, while information missing from a claim can often be crucial in retrieving refuting evidence. For example, a claim stating “Fiji’s largest island is Kauai.” can be refuted by retrieving “Kauai is the oldest Hawaiian Island.” as evidence. Progress on the aforementioned tasks has benefited from the availability of large-scale datasets (Bowman et al., 2015; Rajpurkar et al., 2016). However, despite the rising interest in verification and fact checking among researchers, the datasets currently used for this task are limited to a few hundred claims. Indicatively, the recently conducted Fake News Challenge (Pomerleau and Rao, 2017) with 50 participating teams used a dataset consisting of 300 claims verified against 2,595 associated news articles which is orders of magnitude smaller than those used for TE and QA. In this paper we present a new dataset for claim verification, FEVER: Fact Extraction and VERification. It consists of 185,445 claims man"
N18-1074,P14-5010,0,0.00395303,"thesis length of 8.3 tokens in Bowman et al. (2015). Fact extraction and verification dataset The dataset was constructed in two stages4 : Claim Generation Extracting information from Wikipedia and generating claims from it. Claim Labeling Classifying whether a claim is supported or refuted by Wikipedia and selecting the evidence for it, or deciding there’s not enough information to make a decision. 3.1 Task 1 - Claim Generation The objective of this task was to generate claims from information extracted from Wikipedia. We used the June 2017 Wikipedia dump, processed it with Stanford CoreNLP (Manning et al., 2014), and sampled sentences from the introductory sections of approximately 50,000 popular pages.5 The annotators were given a sentence from the sample chosen at random, and were asked to generate a set of claims containing a single piece of information, focusing on the entity that its original Wikipedia page was about. We asked the annotators to generate claims about a single fact which could be arbitrarily complex and allowed for a variety of expressions for the entities. 3.2 Task 2 - Claim Labeling The annotators were asked to label each individual claim generated during Task 1 as S UPPORTED, R"
N18-1074,P17-1171,0,0.442116,"oulopoulos2 , and Arpit Mittal2 1 Department of Computer Science, University of Sheffield 2 Amazon Research Cambridge {j.thorne, a.vlachos}@sheffield.ac.uk {chrchrs, mitarpit}@amazon.co.uk Abstract the key difference is that in these tasks the passage to verify each claim is given, and in recent years it typically consists a single sentence, while in verification systems it is retrieved from a large set of documents in order to form the evidence. Another related task is question answering (QA), for which approaches have recently been extended to handle large-scale resources such as Wikipedia (Chen et al., 2017). However, questions typically provide the information needed to identify the answer, while information missing from a claim can often be crucial in retrieving refuting evidence. For example, a claim stating “Fiji’s largest island is Kauai.” can be refuted by retrieving “Kauai is the oldest Hawaiian Island.” as evidence. Progress on the aforementioned tasks has benefited from the availability of large-scale datasets (Bowman et al., 2015; Rajpurkar et al., 2016). However, despite the rising interest in verification and fact checking among researchers, the datasets currently used for this task a"
N18-1074,D12-1048,0,0.0449155,"im verification components we typically require the justification for each verdict, including the sources used. While this information is usually available in justifications provided by the journalists, they are not in a machine-readable form. Thus, also considering the small number of claims, the task defined by the dataset proposed 1 http://fever.ai https://github.com/awslabs/fever 3 https://github.com/sheffieldnlp/ fever-baselines 2 810 ple claims such as “not all birds can fly” against textual sources — including Wikipedia — which were processed with an Open Information Extraction system (Mausam et al., 2012). However, the claims were small in number (1,378) and limited in variety as they were derived from eight binary ConceptNet relations (Tandon et al., 2011). Claim verification is also related to the multilingual Answer Validation Exercise (Rodrigo et al., 2009) conducted in the context of the TREC shared tasks. Apart from the difference in dataset size (1,000 instances per language), the key difference is that the claims being validated were answers returned to questions by QA systems. The questions and the QA systems themselves provide additional context to the claim, while in our task defini"
N18-1074,D16-1244,0,0.286775,"Missing"
N18-1074,D16-1264,0,0.13422,"Missing"
N18-1074,N16-1138,1,0.869646,"w for verification against any sources and no evidence needs to be returned to justify the verdicts. The Fake News challenge (Pomerleau and Rao, 2017) modelled verification as stance classification: given a claim and an article, predict whether the article supports, refutes, observes (neutrally states the claim) or is irrelevant to the claim. It consists of 50K labelled claim-article pairs, combining 300 claims with 2,582 articles. The claims and the articles were curated and labeled by journalists in the context of the Emergent Project (Silverman, 2015), and the dataset was first proposed by Ferreira and Vlachos (2016), who only classified the claim w.r.t. the article headline instead of the whole article. Similar to recognizing textual entailment (RTE) (Dagan et al., 2009), the systems were provided with the sources to verify against, instead of having to retrieve them. A differently motivated but closely related dataset is the one developed by Angeli and Manning (2014) to evaluate natural logic inference for common sense reasoning, as it evaluated simRelated Works Vlachos and Riedel (2014) constructed a dataset for claim verification consisting of 106 claims, selecting data from fact-checking websites suc"
N18-1074,N10-1086,0,0.0274237,"evious section is one possible approach to the task proposed in our dataset, but we envisage different ones to be equally valid and possibly better performing. For instance, it would be interesting to test how approaches similar to natural logic inference (Angeli and Manning, 2014) can be applied, where a knowledge base/graph is constructed by reading the textual sources and then a reasoning process over the claim is applied, possibly using recent advances in neural theorem proving (Rockt¨aschel and Riedel, 2017). A different approach could be to consider a combination of question generation (Heilman and Smith, 2010) followed by a question answering model such as BiDAF (Seo et al., 2016), possibly requiring modification as they are designed to select a single span of text from a document rather than return one or more sentences as per our scoring criteria. The sentence-level evidence annotation in our dataset Acknowledgments The work reported was partly conducted while James Thorne was at Amazon Research Cambridge. Andreas Vlachos is supported by the EU H2020 SUMMA project (grant agreement number 688139). The authors would like to thank the team of annotators involved in preparing this dataset. 817 Refere"
N18-1074,I17-1097,0,0.0297478,"ety as they were derived from eight binary ConceptNet relations (Tandon et al., 2011). Claim verification is also related to the multilingual Answer Validation Exercise (Rodrigo et al., 2009) conducted in the context of the TREC shared tasks. Apart from the difference in dataset size (1,000 instances per language), the key difference is that the claims being validated were answers returned to questions by QA systems. The questions and the QA systems themselves provide additional context to the claim, while in our task definition the claims are outside any particular context. In the same vein, Kobayashi et al. (2017) collected a dataset of 412 statements in context from high-school student exams that were validated against Wikipedia and history textbooks. 3 If only the source sentences were used to generate claims then this would result in trivially verifiable claims, as the new claims would in essence be simplifications and paraphrases. At the other extreme, if we allowed world knowledge to be freely incorporated it would result in claims that would be hard to verify on Wikipedia alone. We address this issue by introducing a dictionary: a list of terms that were (hyper-)linked in the original sentence, a"
N18-1074,W14-2508,1,0.686522,"eled by journalists in the context of the Emergent Project (Silverman, 2015), and the dataset was first proposed by Ferreira and Vlachos (2016), who only classified the claim w.r.t. the article headline instead of the whole article. Similar to recognizing textual entailment (RTE) (Dagan et al., 2009), the systems were provided with the sources to verify against, instead of having to retrieve them. A differently motivated but closely related dataset is the one developed by Angeli and Manning (2014) to evaluate natural logic inference for common sense reasoning, as it evaluated simRelated Works Vlachos and Riedel (2014) constructed a dataset for claim verification consisting of 106 claims, selecting data from fact-checking websites such as PolitiFact, taking advantage of the labelled claims available there. However, in order to develop claim verification components we typically require the justification for each verdict, including the sources used. While this information is usually available in justifications provided by the journalists, they are not in a machine-readable form. Thus, also considering the small number of claims, the task defined by the dataset proposed 1 http://fever.ai https://github.com/aws"
N18-1074,P17-2067,0,0.224947,"Claim: The Rodney King riots took place in the most populous county in the USA. [wiki/Los Angeles Riots] The 1992 Los Angeles riots, also known as the Rodney King riots were a series of riots, lootings, arsons, and civil disturbances that occurred in Los Angeles County, California in April and May 1992. [wiki/Los Angeles County] Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA. Verdict: Supported Figure 1: Manually verified claim requiring evidence from multiple Wikipedia pages. remains too challenging for the ML/NLP methods currently available. Wang (2017) extended this approach by including all 12.8K claims available by Politifact via its API, however the justification and the evidence contained in it was ignored in the experiments as it was not machine-readable. Instead, the claims were classified considering only the text and the metadata related to the person making the claim. While this rendered the task amenable to current NLP/ML methods, it does not allow for verification against any sources and no evidence needs to be returned to justify the verdicts. The Fake News challenge (Pomerleau and Rao, 2017) modelled verification as stance clas"
N18-1074,W18-2501,0,\N,Missing
N19-1101,D16-1216,0,0.0259052,"sis and asked to highlight words considered essential to explain the label. Kim, 2017). It has been studied in natural language processing through both black-box analysis, and through modifications to the models under investigation; we refer to the latter approaches as white-box. Common black-box techniques generate explanations of predictions through training meta-models by perturbing input tokens (Ribeiro et al., 2016; Nguyen, 2018; Ribeiro et al., 2018) or through interpretation of model sensitivity to input tokens (Li et al., 2016; Feng et al., 2018). Whitebox methods induce new features (Aubakirova and Bansal, 2016), augment models to generate explanations accompanying their predictions (Lei et al., 2016; Camburu et al., 2018), or expose model internals such as magnitude of hidden states (Linzen et al., 2016), gradients (as a proxy for model sensitivity to input tokens (Li et al., 2016)) or attention (Bahdanau et al., 2014; Xu et al., 2015). Model explanations typically comprise a list of features (such as tokens) that contributed to the prediction and can serve two distinct purposes: acting either as a diagnostic during model development or to allow for a rationale to be generated for a system user. Whi"
N19-1101,D15-1075,0,0.0399433,"es (such as tokens) that contributed to the prediction and can serve two distinct purposes: acting either as a diagnostic during model development or to allow for a rationale to be generated for a system user. While methods for explaining predictions may output what was salient to the model, there is no guarantee these will correspond to the features that users deem important. In this paper we introduce a white-box method that thresholds the attention matrix of a neural entailment model to induce token-level explanations. Introduction Large-scale datasets for Natural Language Inference (NLI) (Bowman et al., 2015; Williams et al., 2018) have enabled the development of many deep-learning models (Rockt¨aschel et al., 2016; Peters et al., 2018; Radford et al., 2018). The task is modeled as 3-way classification of the entailment relation between a pair of sentences. Model performance is assessed through accuracy on a held-out test set. While state-of-the-art models achieve high accuracy, their complexity makes it difficult to interpret their behavior. Explaining the predictions made by classifiers has been of increasing concern (Doshi-Velez and 963 Proceedings of NAACL-HLT 2019, pages 963–969 c Minneapoli"
N19-1101,N18-1097,0,0.0351349,"a The kids are frowning Contradiction Figure 1: Example of token-level highlights from the eSNLI dataset (Camburu et al., 2018). Annotators were provided a premise and hypothesis and asked to highlight words considered essential to explain the label. Kim, 2017). It has been studied in natural language processing through both black-box analysis, and through modifications to the models under investigation; we refer to the latter approaches as white-box. Common black-box techniques generate explanations of predictions through training meta-models by perturbing input tokens (Ribeiro et al., 2016; Nguyen, 2018; Ribeiro et al., 2018) or through interpretation of model sensitivity to input tokens (Li et al., 2016; Feng et al., 2018). Whitebox methods induce new features (Aubakirova and Bansal, 2016), augment models to generate explanations accompanying their predictions (Lei et al., 2016; Camburu et al., 2018), or expose model internals such as magnitude of hidden states (Linzen et al., 2016), gradients (as a proxy for model sensitivity to input tokens (Li et al., 2016)) or attention (Bahdanau et al., 2014; Xu et al., 2015). Model explanations typically comprise a list of features (such as tokens) th"
N19-1101,D16-1244,0,0.0829442,"Missing"
N19-1101,D14-1162,0,0.0808882,"Missing"
N19-1101,N18-2017,0,0.0389669,"Missing"
N19-1101,N18-1202,0,0.0142289,"odel development or to allow for a rationale to be generated for a system user. While methods for explaining predictions may output what was salient to the model, there is no guarantee these will correspond to the features that users deem important. In this paper we introduce a white-box method that thresholds the attention matrix of a neural entailment model to induce token-level explanations. Introduction Large-scale datasets for Natural Language Inference (NLI) (Bowman et al., 2015; Williams et al., 2018) have enabled the development of many deep-learning models (Rockt¨aschel et al., 2016; Peters et al., 2018; Radford et al., 2018). The task is modeled as 3-way classification of the entailment relation between a pair of sentences. Model performance is assessed through accuracy on a held-out test set. While state-of-the-art models achieve high accuracy, their complexity makes it difficult to interpret their behavior. Explaining the predictions made by classifiers has been of increasing concern (Doshi-Velez and 963 Proceedings of NAACL-HLT 2019, pages 963–969 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Word Embeddings We use pretrained GloVe embedd"
N19-1101,S18-2023,0,0.0462924,"Missing"
N19-1101,N19-1357,0,0.0773971,"Missing"
N19-1101,N18-1027,0,0.199175,"s provided of the form {(xpk , xhk , yk )}K k=1 . For each instance, the model must generate an explanation e defined as a subset of zero or more tokens from both the premise and hypothesis sentences: ep ∈ P(xp ), eh ∈ P(xh ). We generate token-level explanations by thresholding token attention weights. Concretely, we select all tokens, x, with a weight greater than NLI Model The model we use for both white- and black-box experiments is based on an architecture widely adopted for sentence-pair classification (Lan and Xu, 2018). It comprises the following: 964 R3 : This term, also adapted from Rei and Søgaard (2018), encodes the assumption that not all tokens must be selected in the explanation. This is achieved by penalizing the smallest nonzero attention weight, which has the effect of encouraging at least one weight to be close to zero. a threshold. While similar to Rei and Søgaard (2018), we incorporate a re-scaling using the tanh function: ep = {xpi |˜ api ∈ A˜∗,n ∧ tanh(˜ api ) ≥ τ } and likewise for the hypothesis. 3.1 Multiple Instance Learning Thresholding the attention distributions from our model will give an indication of which tokens the model is weighting strongly for the entailment task. H"
N19-1101,N16-1030,0,0.0306697,"thors of the respective works setting any hyperparameters to the default values or those suggested in the papers. Outputs The model is jointly trained with two output objectives: a labeling objective and a tagging objective. During training, the losses for both tasks are equally weighted. The first output objective is the three-way SNLI classification over the pair of sentences. This is the same component as the model presented in Section 2. The second objective is a binary tagging objective over the highlighted token-level explanations. We use a jointly-trained LSTM-CRF decoder architecture (Lample et al., 2016) which operates a CRF over encoded representations for each token. In our model, we independently decode the premise and hypothesis sentences. The inputs to our CRF are the attended premise and hypothesis: ap hp and ah hh respectively (where is the point-wise multiplication between the attention vector and the encoded tokens). 5 Results Our experimental results (Table 1) indicate that the LIME black-box explanation technique over the model described in Section 2 provides token-level explanations that are more similar to human judgments than thresholding the attention distributions. We show tha"
N19-1101,N16-3020,0,0.85164,"and waving at a camera The kids are frowning Contradiction Figure 1: Example of token-level highlights from the eSNLI dataset (Camburu et al., 2018). Annotators were provided a premise and hypothesis and asked to highlight words considered essential to explain the label. Kim, 2017). It has been studied in natural language processing through both black-box analysis, and through modifications to the models under investigation; we refer to the latter approaches as white-box. Common black-box techniques generate explanations of predictions through training meta-models by perturbing input tokens (Ribeiro et al., 2016; Nguyen, 2018; Ribeiro et al., 2018) or through interpretation of model sensitivity to input tokens (Li et al., 2016; Feng et al., 2018). Whitebox methods induce new features (Aubakirova and Bansal, 2016), augment models to generate explanations accompanying their predictions (Lei et al., 2016; Camburu et al., 2018), or expose model internals such as magnitude of hidden states (Linzen et al., 2016), gradients (as a proxy for model sensitivity to input tokens (Li et al., 2016)) or attention (Bahdanau et al., 2014; Xu et al., 2015). Model explanations typically comprise a list of features (such"
N19-1101,C18-1328,0,0.0204137,"between xp and xh where y ∈ {entails, contradicts, neutral}. Labeled training data is provided of the form {(xpk , xhk , yk )}K k=1 . For each instance, the model must generate an explanation e defined as a subset of zero or more tokens from both the premise and hypothesis sentences: ep ∈ P(xp ), eh ∈ P(xh ). We generate token-level explanations by thresholding token attention weights. Concretely, we select all tokens, x, with a weight greater than NLI Model The model we use for both white- and black-box experiments is based on an architecture widely adopted for sentence-pair classification (Lan and Xu, 2018). It comprises the following: 964 R3 : This term, also adapted from Rei and Søgaard (2018), encodes the assumption that not all tokens must be selected in the explanation. This is achieved by penalizing the smallest nonzero attention weight, which has the effect of encouraging at least one weight to be close to zero. a threshold. While similar to Rei and Søgaard (2018), we incorporate a re-scaling using the tanh function: ep = {xpi |˜ api ∈ A˜∗,n ∧ tanh(˜ api ) ≥ τ } and likewise for the hypothesis. 3.1 Multiple Instance Learning Thresholding the attention distributions from our model will giv"
N19-1101,D16-1011,0,0.0729092,"Missing"
N19-1101,N18-1101,0,0.0257762,"hat contributed to the prediction and can serve two distinct purposes: acting either as a diagnostic during model development or to allow for a rationale to be generated for a system user. While methods for explaining predictions may output what was salient to the model, there is no guarantee these will correspond to the features that users deem important. In this paper we introduce a white-box method that thresholds the attention matrix of a neural entailment model to induce token-level explanations. Introduction Large-scale datasets for Natural Language Inference (NLI) (Bowman et al., 2015; Williams et al., 2018) have enabled the development of many deep-learning models (Rockt¨aschel et al., 2016; Peters et al., 2018; Radford et al., 2018). The task is modeled as 3-way classification of the entailment relation between a pair of sentences. Model performance is assessed through accuracy on a held-out test set. While state-of-the-art models achieve high accuracy, their complexity makes it difficult to interpret their behavior. Explaining the predictions made by classifiers has been of increasing concern (Doshi-Velez and 963 Proceedings of NAACL-HLT 2019, pages 963–969 c Minneapolis, Minnesota, June 2 - J"
N19-1101,Q16-1037,0,0.0342603,"models under investigation; we refer to the latter approaches as white-box. Common black-box techniques generate explanations of predictions through training meta-models by perturbing input tokens (Ribeiro et al., 2016; Nguyen, 2018; Ribeiro et al., 2018) or through interpretation of model sensitivity to input tokens (Li et al., 2016; Feng et al., 2018). Whitebox methods induce new features (Aubakirova and Bansal, 2016), augment models to generate explanations accompanying their predictions (Lei et al., 2016; Camburu et al., 2018), or expose model internals such as magnitude of hidden states (Linzen et al., 2016), gradients (as a proxy for model sensitivity to input tokens (Li et al., 2016)) or attention (Bahdanau et al., 2014; Xu et al., 2015). Model explanations typically comprise a list of features (such as tokens) that contributed to the prediction and can serve two distinct purposes: acting either as a diagnostic during model development or to allow for a rationale to be generated for a system user. While methods for explaining predictions may output what was salient to the model, there is no guarantee these will correspond to the features that users deem important. In this paper we introduce a w"
N19-1102,W18-0503,0,0.0271936,"Missing"
N19-1102,yimam-etal-2017-multilingual,0,0.0762093,"me of the results obtained. 1 Sentence Target word/MWE Complex? Both China and the Philippines flexed their muscles on Wednesday. flexed flexed their muscles muscles Yes Yes No Table 1: An annotated sentence in the English dataset of the Second CWI Shared Task. data. In this paper, we are interested in both monolingual and cross-lingual CWI; in the latter, we build models to make predictions for languages not seen during training. While monolingual CWI has been studied extensively (see a survey in Paetzold and Specia (2017)), the cross-lingual setup of the task was introduced only recently by Yimam et al. (2017b), who collected human annotations from native and non-native speakers of Spanish and German, and integrated them with similar data previously produced for three English domains (Yimam et al., 2017a): News, WikiNews and Wikipedia. For the Second CWI Shared Task (Yimam et al., 2018), participants built monolingual models using the datasets previously described, and also tested their cross-lingual capabilities on newly collected French data. In the monolingual track, the best systems for English (Gooding and Kochmar, 2018) differed significantly in terms of feature set size and the model’s comp"
N19-1102,W18-0518,0,0.0731674,"in a sentence is given, and our goal is to determine if it is complex or not (an example is shown in Table 1). Under this setting, CWI is normally treated using supervised learning and feature engineering to build monolingual models (Paetzold and Specia, 2016; Yimam et al., 2018). Unfortunately, this approach is infeasible for languages with scarce resources of annotated 1 We consider n-grams with n ≥ 2 as MWEs, while Yimam et al. (2018) used n ≥ 3. 970 Proceedings of NAACL-HLT 2019, pages 970–977 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Bingel and Bjerva (2018) who implemented an ensemble of Random Forests and feed-forward neural networks in a multi-task learning architecture. Our approach to CWI differs from previous work in that we begin by building competitive monolingual models, but using the same set of features and learning algorithm across languages. This reduces the possibility of getting high scores due to modelling annotation artifacts present in the dataset of one language. Our monolingual models achieve better scores for Spanish and German than the best systems in the Second CWI Shared Task. After that, we focus on language-independent f"
N19-1102,W17-5910,0,0.0501409,"Missing"
N19-1102,W18-0520,0,0.0732682,"cia (2017)), the cross-lingual setup of the task was introduced only recently by Yimam et al. (2017b), who collected human annotations from native and non-native speakers of Spanish and German, and integrated them with similar data previously produced for three English domains (Yimam et al., 2017a): News, WikiNews and Wikipedia. For the Second CWI Shared Task (Yimam et al., 2018), participants built monolingual models using the datasets previously described, and also tested their cross-lingual capabilities on newly collected French data. In the monolingual track, the best systems for English (Gooding and Kochmar, 2018) differed significantly in terms of feature set size and the model’s complexity, to the best systems for German and Spanish (Kajiwara and Komachi, 2018). The latter used Random Forests with eight features, whilst the former used AdaBoost with 5000 estimators or ensemble voting combining AdaBoost and Random Forest classifiers, with about 20 features. In the cross-lingual track, only two teams achieved better scores than the baseline: Kajiwara and Komachi (2018) who used length and frequency based features with Random Forests, and Introduction Complex Word Identification (CWI) consists of decidi"
N19-1102,W18-0521,0,0.0658622,"non-native speakers of Spanish and German, and integrated them with similar data previously produced for three English domains (Yimam et al., 2017a): News, WikiNews and Wikipedia. For the Second CWI Shared Task (Yimam et al., 2018), participants built monolingual models using the datasets previously described, and also tested their cross-lingual capabilities on newly collected French data. In the monolingual track, the best systems for English (Gooding and Kochmar, 2018) differed significantly in terms of feature set size and the model’s complexity, to the best systems for German and Spanish (Kajiwara and Komachi, 2018). The latter used Random Forests with eight features, whilst the former used AdaBoost with 5000 estimators or ensemble voting combining AdaBoost and Random Forest classifiers, with about 20 features. In the cross-lingual track, only two teams achieved better scores than the baseline: Kajiwara and Komachi (2018) who used length and frequency based features with Random Forests, and Introduction Complex Word Identification (CWI) consists of deciding which words (or phrases) in a text could be difficult to understand by a specific type of reader. In this work, we follow the CWI Shared Tasks (Paetz"
N19-1102,W02-0109,0,0.245788,"e annotation of the datasets and find some inconsistencies that could explain some of our results. Code for all our models can be found at: German, Spanish and French). They can be divided into three broad categories: features based on the target word/MWE, sub-word level features, and sentence-level features to capture information from the target’s context. As we intended that our features be applicable across languages, we drew on features found to be useful in previous work on CWI (Yimam et al., 2017b, 2018). We made use of the python libraries spaCy2 (Honnibal and Montani, 2017) and NLTK3 (Loper and Bird, 2002). Details on the resources used for extracting each feature can be found in Appendix A. At the target word/MWE level, we experimented with features such as Named Entity (NE) type, part-of-speech, hypernym counts, number of tokens in the target, language-normalised number of characters in each word, and simple unigram probabilities. These features are linguistically motivated. The perceived complexity of a MWE may be higher than that of a single word, as each component word can be complex, or simple component words can be synthesised into a complex whole. Similarly, infrequent words are less fa"
N19-1102,W18-0507,0,0.247406,"we are interested in both monolingual and cross-lingual CWI; in the latter, we build models to make predictions for languages not seen during training. While monolingual CWI has been studied extensively (see a survey in Paetzold and Specia (2017)), the cross-lingual setup of the task was introduced only recently by Yimam et al. (2017b), who collected human annotations from native and non-native speakers of Spanish and German, and integrated them with similar data previously produced for three English domains (Yimam et al., 2017a): News, WikiNews and Wikipedia. For the Second CWI Shared Task (Yimam et al., 2018), participants built monolingual models using the datasets previously described, and also tested their cross-lingual capabilities on newly collected French data. In the monolingual track, the best systems for English (Gooding and Kochmar, 2018) differed significantly in terms of feature set size and the model’s complexity, to the best systems for German and Spanish (Kajiwara and Komachi, 2018). The latter used Random Forests with eight features, whilst the former used AdaBoost with 5000 estimators or ensemble voting combining AdaBoost and Random Forest classifiers, with about 20 features. In t"
N19-1102,I17-2068,0,0.1211,"me of the results obtained. 1 Sentence Target word/MWE Complex? Both China and the Philippines flexed their muscles on Wednesday. flexed flexed their muscles muscles Yes Yes No Table 1: An annotated sentence in the English dataset of the Second CWI Shared Task. data. In this paper, we are interested in both monolingual and cross-lingual CWI; in the latter, we build models to make predictions for languages not seen during training. While monolingual CWI has been studied extensively (see a survey in Paetzold and Specia (2017)), the cross-lingual setup of the task was introduced only recently by Yimam et al. (2017b), who collected human annotations from native and non-native speakers of Spanish and German, and integrated them with similar data previously produced for three English domains (Yimam et al., 2017a): News, WikiNews and Wikipedia. For the Second CWI Shared Task (Yimam et al., 2018), participants built monolingual models using the datasets previously described, and also tested their cross-lingual capabilities on newly collected French data. In the monolingual track, the best systems for English (Gooding and Kochmar, 2018) differed significantly in terms of feature set size and the model’s comp"
P13-2009,J93-2003,0,0.0286326,"ecoding: given any sequence of decorated MRL tokens, we can always reconstruct the corresponding tree structure (if one exists). Arity labeling additionally allows functions with variable numbers of arguments (e.g. cityid, which in some training examples is unary) to align with different natural language strings depending on context. LINEARIZE state border texa state1 next to1 state1 stateid1 texas0 ⇓ ALIGN state border texa state1 next to1 state1 stateid1 texas0 Alignment Following the linearization of the MRs, we find alignments between the MR tokens and the NL tokens using the IBM Model 4 (Brown et al., 1993). Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al., 2005). ⇓ EXTRACT ( PHRASE ) h state , state1 i h state border , state1 border1 i h texa , state1 stateid1 texas0 i .. . ⇓ EXTRACT ( HIER ) [X] → hstate , state1 i Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. We consider a phrase-based translation model (Koehn et al., 2003) and a hierarchical translati"
P13-2009,P11-1149,0,0.0308621,"he use of machine translation methods as an informative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation. 1 Stephen Clark Computer Laboratory University of Cambridge sc609@cam.ac.uk Introduction Semantic parsing (SP) is the problem of transforming a natural language (NL) utterance into a machine-interpretable meaning representation (MR). It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. 2 MT-based semantic"
P13-2009,P12-1051,0,0.666025,"opescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. 2 MT-based semantic parsing The input is a corpus of NL utterances paired with MRs. In order to learn a semantic parser using MT we linearize the MRs, learn alignments between the MRL and the NL, extract translation rules, and learn a language model for the MRL. We also specify a decoding procedure that will return structured MRs for an utterance during prediction. 47 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 47–52, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguist"
P13-2009,N03-1017,0,0.0364768,"okens using the IBM Model 4 (Brown et al., 1993). Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al., 2005). ⇓ EXTRACT ( PHRASE ) h state , state1 i h state border , state1 border1 i h texa , state1 stateid1 texas0 i .. . ⇓ EXTRACT ( HIER ) [X] → hstate , state1 i Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. We consider a phrase-based translation model (Koehn et al., 2003) and a hierarchical translation model (Chiang, 2005). Rules for the phrase-based model consist of pairs of aligned source and target sequences, while hierarchical rules are SCFG productions containing at most two instances of a single nonterminal symbol. Note that both extraction algorithms can learn rules which a traditional tree-transducer-based approach cannot—for example the right hand side [X] → hstate [X] texa , state1 [X] state1 stateid1 texas0 i .. . Figure 1: Illustration of preprocessing and rule extraction. Linearization We assume that the MRL is variable-free (that is, the meaning"
P13-2009,2005.iwslt-1.8,0,0.0119875,"ents (e.g. cityid, which in some training examples is unary) to align with different natural language strings depending on context. LINEARIZE state border texa state1 next to1 state1 stateid1 texas0 ⇓ ALIGN state border texa state1 next to1 state1 stateid1 texas0 Alignment Following the linearization of the MRs, we find alignments between the MR tokens and the NL tokens using the IBM Model 4 (Brown et al., 1993). Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al., 2005). ⇓ EXTRACT ( PHRASE ) h state , state1 i h state border , state1 border1 i h texa , state1 stateid1 texas0 i .. . ⇓ EXTRACT ( HIER ) [X] → hstate , state1 i Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. We consider a phrase-based translation model (Koehn et al., 2003) and a hierarchical translation model (Chiang, 2005). Rules for the phrase-based model consist of pairs of aligned source and target sequences, while hierarchical rules are SCFG productions containing at most two instances of a singl"
P13-2009,P07-2045,0,0.00547291,"tely discard malformed MRs; for the experiments in this paper we simply filter the regular n-best list until we find a well-formed MR. This filtering can be done with time linear in the length of the example by exploiting the argument label numbers introduced during linearization. Finally, we insert the brackets according to the tree structure specified by the argument number labels. 3 Implementation In all experiments, we use the IBM Model 4 implementation from the GIZA++ toolkit (Och and Ney, 2000) for alignment, and the phrase-based and hierarchical models implemented in the Moses toolkit (Koehn et al., 2007) for rule extraction. The best symmetrization algorithm, translation and language model weights for each language are selected using cross-validation on the development set. In the case of English and German, we also found that stemming (Bird et al., 2009; Porter, 1980) was hepful in reducing data sparsity. 4 Results We first compare the results for the two translation rule extraction models, phrase-based and hierarchical (“MT-phrase” and “MT-hier” respectively in Table 1). We find that the hierarchical model performs better in all languages apart from Greek, indicating that the long-range reo"
P13-2009,D10-1119,0,0.0752731,"wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. 2 MT-based semantic parsing The input is a corpus of NL utterances paired with MRs. In order to learn a semantic parser using MT we linearize the MRs, learn alignments between the MRL and the NL, extract translation rules, and learn a language model for the MRL. We also specify a decoding procedure that will return structured MRs for an utterance during prediction. 47 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 47–52, c Sofia,"
P13-2009,P11-1060,0,0.0670688,"formative baseline in semantic parsing evaluations, and suggest that research in semantic parsing could benefit from advances in machine translation. 1 Stephen Clark Computer Laboratory University of Cambridge sc609@cam.ac.uk Introduction Semantic parsing (SP) is the problem of transforming a natural language (NL) utterance into a machine-interpretable meaning representation (MR). It is well-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. 2 MT-based semantic parsing The input is a corpus of NL utte"
P13-2009,D11-1149,0,0.0250669,"scribes a generative model over derivations of MRL trees. The remaining system discussed in this paper, UBL (Kwiatkowski et al., 2010), leverages the fact that the MRL does not simply encode trees, but rather λ-calculus expressions. It employs resolution procedures specific to the λ-calculus such as splitting and unification in order to generate rule templates. Like other systems described, it uses GIZA alignments for initialization. Other work which generalizes from variable-free meaning representations to λ-calculus expressions includes the natural language generation procedure described by Lu and Ng (2011). UBL , like an MT system (and unlike most of the other systems discussed in this section), extracts rules at multiple levels of granularity by means of this splitting and unification procedure. hybridtree similarly benefits from the introduction of We also compare the MT-based semantic parsers to several recently published ones: WASP (Wong and Mooney, 2006), which like the hierarchical model described here learns a SCFG to translate between NL and MRL; tsVB (Jones et al., 2012), which uses variational Bayesian inference to learn weights for a tree transducer; UBL (Kwiatkowski et al., 2010), w"
P13-2009,D08-1082,0,0.0842745,"ctic MT to extract rules. tsVB also uses a piece of standard MT machinery, specifically tree transducers, which have been profitably employed for syntax-based machine translation (Maletti, 2010). In that work, however, the usual MT parameter-estimation technique of simply counting the number of rule occurrences does not improve scores, and the authors instead resort to a variational inference procedure to acquire rule weights. The present work is also the first we are aware of which uses phrasebased rather than tree-based machine translation techniques to learn a semantic parser. hybrid-tree (Lu et al., 2008) similarly describes a generative model over derivations of MRL trees. The remaining system discussed in this paper, UBL (Kwiatkowski et al., 2010), leverages the fact that the MRL does not simply encode trees, but rather λ-calculus expressions. It employs resolution procedures specific to the λ-calculus such as splitting and unification in order to generate rule templates. Like other systems described, it uses GIZA alignments for initialization. Other work which generalizes from variable-free meaning representations to λ-calculus expressions includes the natural language generation procedure"
P13-2009,P00-1056,0,0.0603827,"le numbers of arguments (e.g. cityid, which in some training examples is unary) to align with different natural language strings depending on context. LINEARIZE state border texa state1 next to1 state1 stateid1 texas0 ⇓ ALIGN state border texa state1 next to1 state1 stateid1 texas0 Alignment Following the linearization of the MRs, we find alignments between the MR tokens and the NL tokens using the IBM Model 4 (Brown et al., 1993). Once the alignment algorithm is run in both directions (NL to MRL, MRL to NL), we symmetrize the resulting alignments to obtain a consensus many-to-many alignment (Och and Ney, 2000; Koehn et al., 2005). ⇓ EXTRACT ( PHRASE ) h state , state1 i h state border , state1 border1 i h texa , state1 stateid1 texas0 i .. . ⇓ EXTRACT ( HIER ) [X] → hstate , state1 i Rule extraction From the many-to-many alignment we need to extract a translation rule table, consisting of corresponding phrases in NL and MRL. We consider a phrase-based translation model (Koehn et al., 2003) and a hierarchical translation model (Chiang, 2005). Rules for the phrase-based model consist of pairs of aligned source and target sequences, while hierarchical rules are SCFG productions containing at most two"
P13-2009,N06-1056,0,0.940236,"l-studied in NLP, and a wide variety of methods have been proposed to tackle it, e.g. rule-based (Popescu et al., 2003), supervised (Zelle, 1995), unsupervised (Goldwasser et al., 2011), and response-based (Liang et al., 2011). At least superficially, SP is simply a machine translation (MT) task: we transform an NL utterance in one language into a statement of another (un-natural) meaning representation language (MRL). Indeed, successful semantic parsers often resemble MT systems in several important respects, including the use of word alignment models as a starting point for rule extraction (Wong and Mooney, 2006; Kwiatkowski et al., 2010) and the use of automata such as tree transducers (Jones et al., 2012) to encode the relationship between NL and MRL. 2 MT-based semantic parsing The input is a corpus of NL utterances paired with MRs. In order to learn a semantic parser using MT we linearize the MRs, learn alignments between the MRL and the NL, extract translation rules, and learn a language model for the MRL. We also specify a decoding procedure that will return structured MRs for an utterance during prediction. 47 Proceedings of the 51st Annual Meeting of the Association for Computational Linguist"
P13-2009,P05-1033,0,\N,Missing
P15-2084,D13-1143,1,0.738498,"models (Zweig et al., 2012; Gubbins and Vlachos, 2013; Mnih and Kavukcuoglu, 2013; Mikolov et al., 2013). Most neural language models consider the tokens in a sentence in the order they appear, and the hidden state representation of the network is typically reset at the beginning of each sentence. In this work we propose a novel neural language model that learns a recurrent neural network (RNN) (Mikolov et al., 2010) on top of the syntactic dependency parse of a sentence. Syntactic dependencies bring relevant contexts closer to the word being predicted, thus enhancing performance as shown by Gubbins and Vlachos (2013) for count-based language models. Our Dependency RNN model is published simultaneously with another model, introduced in Tai et al. (2015), who extend the Long-Short Term Memory (LSTM) architecture to tree-structured network topologies and evaluate it at sentence-level sentiment classification and semantic relatedness tasks, but not as a language model. Adapting the RNN to use the syntactic dependency structure required to reset and run the network on all the paths in the dependency parse tree of a given sentence, while maintaining a count of how often each token appears in those paths. Furthe"
P15-2084,P14-2050,0,0.0691736,"Missing"
P15-2084,P14-5010,0,0.00696697,"e the distribution 2 513 http://research.microsoft.com/en-us/projects/rnn/ Architecture RNN (dev) RNN (test) RNN+2g (dev) RNN+2g (test) RNN+3g (dev) RNN+3g (test) RNN+4g (dev) RNN+4g (test) enhanced the RNN library by replacing some large matrix multiplication routines by calls to the CBLAS library, thus yielding a two- to three-fold speed-up in the test and training time.3 The training corpus consists of 522 19th century novels from Project Gutenberg (Zweig and Burges, 2012). All processing (sentence-splitting, PoS tagging, syntactic parsing) was performed using the Stanford CoreNLP toolkit (Manning et al., 2014). The test set contains 1040 sentences to be completed. Each sentence consists of one ground truth and 4 impostor sentences where a specific word has been replaced with a syntactically correct but semantically incorrect impostor word. Dependency trees are generated for each sentence candidate. We split that set into two, using the first 520 sentences in the validation (development) set and the latter 520 sentences in the test set. During training, we start annealing the learning rate λ with decay factor 0.66 as soon as the classification error on the validation set starts to increase. 5 50h 29"
P15-2084,P11-1027,0,0.0446339,"Missing"
P15-2084,W15-1002,0,0.0533538,"Missing"
P15-2084,P12-1101,0,0.0584017,"Missing"
P15-2084,P15-1150,0,0.188498,"tokens in a sentence in the order they appear, and the hidden state representation of the network is typically reset at the beginning of each sentence. In this work we propose a novel neural language model that learns a recurrent neural network (RNN) (Mikolov et al., 2010) on top of the syntactic dependency parse of a sentence. Syntactic dependencies bring relevant contexts closer to the word being predicted, thus enhancing performance as shown by Gubbins and Vlachos (2013) for count-based language models. Our Dependency RNN model is published simultaneously with another model, introduced in Tai et al. (2015), who extend the Long-Short Term Memory (LSTM) architecture to tree-structured network topologies and evaluate it at sentence-level sentiment classification and semantic relatedness tasks, but not as a language model. Adapting the RNN to use the syntactic dependency structure required to reset and run the network on all the paths in the dependency parse tree of a given sentence, while maintaining a count of how often each token appears in those paths. Furthermore, we explain how we can incorporate the dependency labels as features. Our results show that the dependency RNN language model propos"
P15-2084,W12-2704,0,0.312272,"ction Language Models (LM) are commonly used to score a sequence of tokens according to its probability of occurring in natural language. They are an essential building block in a variety of applications such as machine translation, speech recognition and grammatical error correction. The standard way of evaluating a language model has been to calculate its perplexity on a large corpus. However, this evaluation assumes the output of the language model to be probabilistic and it has been observed that perplexity does not always correlate with the downstream task performance. For these reasons, Zweig and Burges (2012) proposed the Sentence Completion Challenge, in which the task is to pick the correct word to complete a sentence out of five candidates. Performance is evaluated by accuracy (how many sentences were completed correctly), thus both probabilistic and non-probabilistic models (e.g. Roark 511 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 511–517, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Dependency Recurrent Neural Net"
P15-2084,W12-2700,0,0.181907,"dels (LM) are commonly used to score a sequence of tokens according to its probability of occurring in natural language. They are an essential building block in a variety of applications such as machine translation, speech recognition and grammatical error correction. The standard way of evaluating a language model has been to calculate its perplexity on a large corpus. However, this evaluation assumes the output of the language model to be probabilistic and it has been observed that perplexity does not always correlate with the downstream task performance. For these reasons, Zweig and Burges (2012) proposed the Sentence Completion Challenge, in which the task is to pick the correct word to complete a sentence out of five candidates. Performance is evaluated by accuracy (how many sentences were completed correctly), thus both probabilistic and non-probabilistic models (e.g. Roark 511 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 511–517, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Dependency Recurrent Neural Net"
P15-2084,P12-1063,0,0.368697,"Missing"
P15-2084,P15-1082,0,\N,Missing
P15-5005,D12-1087,0,0.0831903,"Missing"
P15-5005,P14-1130,0,0.0534323,"Missing"
P15-5005,D14-1162,0,0.0934629,"Missing"
P15-5005,N13-1008,1,0.708077,"Missing"
P15-5005,N15-1118,1,0.854412,"Missing"
P16-1001,P14-1134,0,0.415271,"r-01 prep defenses will poss NATO against pobj attacks nn possessive cyber ’s ARG0 ARG1 center defend-01 ARG1 military name name op1 prep-against attack-01 mod cyber NATO Figure 1: Dependency (left) and AMR graph (right) for: “The center will bolster NATO’s defenses against cyber-attacks.’ Liang et al., 2013). Such machine-interpretable representations enable many applications relying on natural language understanding. The ambition of Abstract Meaning Representation (AMR) is that it is domain-independent and useful in a variety of applications (Banarescu et al., 2013). The first AMR parser by Flanigan et al. (2014) used graph-based inference to find a highestscoring maximum spanning connected acyclic graph. Later work by Wang et al. (2015b) was inspired by the similarity between the dependency parse of a sentence and its semantic AMR graph (Figure 1). Wang et al. (2015b) start from the dependency parse and learn a transition-based parser that converts it incrementally into an AMR graph using greedy decoding. An advantage of this approach is that the initial stage of dependency parsing is well-studied and trained using larger corpora than that for which AMR annotations exist. Greedy decoding, where the p"
P16-1001,D15-1198,0,0.194549,"Missing"
P16-1001,N10-1115,0,0.0178762,"re, at the cost of 80-100x more computation. In problems with a less good expert, the gain from exploration could be much greater. Similarly, if designing an expert for a task is time-consuming, then it may be a better investment to rely on exploration with a poor expert to achieve the same result. 6 VRelated Work Other strategies have been used to mitigate the error propagation problem in transition-based parsing. A common approach is to use beam search through state-space for each action choice to find a better approximation of the long-term score of the action, e.g. Zhang and Clark (2008). Goldberg and Elhadad (2010) remove the determinism of the sequence of actions to create easy-first parsers, which postpone uncertain, error-prone decisions until more information is available. This contrasts with working inflexibly left-to-right along a sentence, or bottom-to-top up a tree. Goldberg and Nivre (2012) introduce dynamic experts that are complete in that they will respond from any state, not just those on the perfect trajectory assuming no earlier mistakes; any expert used with an imitation learning algorithm needs to be complete in this sense. Their algorithm takes exploratory steps off the expert trajecto"
P16-1001,W13-2322,0,0.183855,"Computer Science Department, University College London james@janigo.co.uk, jason.narad@gmail.com † Department of Computer Science, University of Sheffield a.vlachos@sheffield.ac.uk Abstract bolster nsubj Semantic parsers map natural language statements into meaning representations, and must abstract over syntactic phenomena, resolve anaphora, and identify word senses to eliminate ambiguous interpretations. Abstract meaning representation (AMR) is a recent example of one such semantic formalism which, similar to a dependency parse, utilizes a graph to represent relationships between concepts (Banarescu et al., 2013). As with dependency parsing, transition-based approaches are a common approach to this problem. However, when trained in the traditional manner these systems are susceptible to the accumulation of errors when they find undesirable states during greedy decoding. Imitation learning algorithms have been shown to help these systems recover from such errors. To effectively use these methods for AMR parsing we find it highly beneficial to introduce two novel extensions: noise reduction and targeted exploration. The former mitigates the noise in the feature representation, a result of the complexity"
P16-1001,C12-1059,0,0.0658743,"result. 6 VRelated Work Other strategies have been used to mitigate the error propagation problem in transition-based parsing. A common approach is to use beam search through state-space for each action choice to find a better approximation of the long-term score of the action, e.g. Zhang and Clark (2008). Goldberg and Elhadad (2010) remove the determinism of the sequence of actions to create easy-first parsers, which postpone uncertain, error-prone decisions until more information is available. This contrasts with working inflexibly left-to-right along a sentence, or bottom-to-top up a tree. Goldberg and Nivre (2012) introduce dynamic experts that are complete in that they will respond from any state, not just those on the perfect trajectory assuming no earlier mistakes; any expert used with an imitation learning algorithm needs to be complete in this sense. Their algorithm takes exploratory steps off the expert trajectory to augment the training data collected in a fashion very similar to DAGGER. Honnibal et al. (2013) use a non-monotonic parser that allows actions that are inconsistent with previous actions. When such an action is taken it amends the results of previous actions to ensure post-hoc consis"
P16-1001,P13-2131,0,0.299232,"f β0 . Other children of σ0 become children of β0 . Insert β0 at the head of σ and re-initialise β. Pop β0 and delete edge (σ0 , β0 ). Attach β0 as a child of κ. If κ has already been popped from σ then re-insert it as σ1 . Pop σ0 and delete it from the graph. Insert a new node δ with AMR concept lc as the parent of σ0 , and insert δ into σ. Insert a new node δ with AMR concept lc as a child of σ0 . Table 1: Action Space for the transition-based graph parsing algorithm in each stack, σ0 and β0 . We reach a terminal state when σ is empty. The objective function to maximise is the Smatch score (Cai and Knight, 2013), which calculates an F1 -Score between the predicted and gold-target AMR graphs. Table 1 summarises the actions in A. NextNode and NextEdge form the core action set, labelling nodes and edges respectively without changing the graph structure. Swap, Reattach and ReplaceHead change graph structure, keeping it a tree. We permit a Reattach action to use parameter κ equal to any node within six edges from σ0 , excluding any that would disconnect the graph or create a cycle. The Insert/InsertBelow actions insert a new node as a parent/child of σ0 . These actions are not used in Wang et al. (2015b),"
P16-1001,Q13-1033,0,0.0394942,"DAGGER and the targeted V-DAGGER we propose here. One way to ameliorate this problem is to employ imitation learning algorithms for structured prediction. Algorithms such as SEARN (Daum´e III et al., 2009), DAGGER (Ross et al., 2011), and LOLS (Chang et al., 2015) address the problem of error propagation by iteratively adjusting the training data to increasingly expose the model to training instances it is likely to encounter during test. Such algorithms have been shown to improve performance in a variety of tasks including information extraction(Vlachos and Craven, 2011), dependency parsing (Goldberg and Nivre, 2013), and feature selection (He et al., 2013). In this work we build on the transition-based parsing approach of Wang et al. (2015b) and explore the applicability of different imitation algorithms to AMR parsing, which has a more complex output space than those considered previously. The complexity of AMR parsing affects transition-based methods that rely on features to represent structure, since these often cannot capture the information necessary to predict the correct transition according to the gold standard. In other words, the features defined are not sufficient to “explain” why different ac"
P16-1001,S16-1180,1,0.834203,"n of 2 points from a semantic role labeller. Table 4 lists previous AMR work on the same dataset. Dataset proxy dfa bolt xinhua lpp Validation F-Score EI D V-D 0.670 0.686 0.704 0.495 0.532 0.546 0.456 0.468 0.524 0.598 0.623 0.683 0.540 0.546 0.564 Test F-Score V-D Rao et al 0.70 0.61 0.50 0.44 0.52 0.46 0.62 0.52 0.55 0.52 Table 5: Comparison of Exact Imitation (EI), DAGGER (D), V-DAGGER (V-D) on all components of the LDC2014T12 corpus. Using DAGGER with this system we obtained an F-Score of 0.60 in the Semeval 2016 task on AMR parsing, one standard deviation above the mean of all entries. (Goodman et al., 2016) Finally we test on all components of the LDC2014T12 corpus as shown in Table 5, which include both newswire and weblog data, as well as the freely available AMRs for The Little Prince, (lpp)3 . For each we use exact imitation, DAG GER , and V-DAGGER on the train/validation/splits specified in the corpus. In all cases, imitation learning without RollOuts (DAGGER) improves on exact imitation, and incorporating RollOuts (VDAGGER) provides an additional benefit. Rao et al. (2015) use SEARN on the same datasets, but with a very different transition system. We show their results for comparison. Our"
P16-1001,P05-1022,0,0.0137021,"2.9 points from 0.682 to 0.711. In all the settings tried, focused costing improves the results, and requires progressive removal of the expert to achieve the best score. We use the classifier from the Focused Costing 5/5 run to achieve an F-Score on the held-out test set of 0.70, equal to the best published result so far (Wang et al., 2015a). Our gain of 4.7 points from imitation learning over standard transition-based parsing is orthogonal to that of Wang et al. (2015a) using exact imitation with additional trained analysers; they experience a gain of 2 points from using a Charniak parser (Charniak and Johnson, 2005) trained on the full OntoNotes corpus instead of the Stanford parser used here and in Wang et al. (2015b), and a further gain of 2 points from a semantic role labeller. Table 4 lists previous AMR work on the same dataset. Dataset proxy dfa bolt xinhua lpp Validation F-Score EI D V-D 0.670 0.686 0.704 0.495 0.532 0.546 0.456 0.468 0.524 0.598 0.623 0.683 0.540 0.546 0.564 Test F-Score V-D Rao et al 0.70 0.61 0.50 0.44 0.52 0.46 0.62 0.52 0.55 0.52 Table 5: Comparison of Exact Imitation (EI), DAGGER (D), V-DAGGER (V-D) on all components of the LDC2014T12 corpus. Using DAGGER with this system we"
P16-1001,D13-1152,0,0.0297612,"Missing"
P16-1001,W13-3518,0,0.012685,", which postpone uncertain, error-prone decisions until more information is available. This contrasts with working inflexibly left-to-right along a sentence, or bottom-to-top up a tree. Goldberg and Nivre (2012) introduce dynamic experts that are complete in that they will respond from any state, not just those on the perfect trajectory assuming no earlier mistakes; any expert used with an imitation learning algorithm needs to be complete in this sense. Their algorithm takes exploratory steps off the expert trajectory to augment the training data collected in a fashion very similar to DAGGER. Honnibal et al. (2013) use a non-monotonic parser that allows actions that are inconsistent with previous actions. When such an action is taken it amends the results of previous actions to ensure post-hoc consistency. Our parser is nonmonotonic, and we have the same problem encountered by Honnibal et al. (2013) with many different actions from a state si able to reach the target sT , following different “paths up the mountain”. This leads to poor learning. To resolve 7 Conclusions Imitation learning provides a total benefit of 4.5 points with our AMR transition-based parser over exact imitation. This is a more comp"
P16-1001,W02-1001,0,0.0489256,"08 as the test set. The data split is the same as that used by Flanigan et al. (2014) and Wang et al. (2015b). 1 We first assess the impact of noise reduction using the alpha bound, and report these experiments without Rollouts (i.e. using DAGGER) to isolate the effect of noise reduction. Table 3 summarises results using exact imitation and DAGGER with the α-bound set to discard a training instance after one misclassification. This is the most extreme setting, and the one that gave best results. We try AROW (Crammer et al., 2013), PassiveAggressive (PA) (Crammer et al., 2006), and perceptron (Collins, 2002) classifiers, with averaging in all cases. We see a benefit from the α-bound for exact imitation only with AROW, which is more noise-sensitive than PA or the simple perceptron. With DAGGER there is a benefit for all classifiers. In all cases the α-bound and DAGGER are synergistic; without the α-bound imitation learning works less well, if at all. α=1 was the optimal setting, with lesser benefit observed for larger values. We now turn our attention to targeted exploration and focused costing, for which we use VDAGGER as explained in section 4. For all VNa¨ıve Smatch as Loss Function Smatch (Cai"
P16-1001,D07-1013,0,0.0234595,"spired by the similarity between the dependency parse of a sentence and its semantic AMR graph (Figure 1). Wang et al. (2015b) start from the dependency parse and learn a transition-based parser that converts it incrementally into an AMR graph using greedy decoding. An advantage of this approach is that the initial stage of dependency parsing is well-studied and trained using larger corpora than that for which AMR annotations exist. Greedy decoding, where the parser builds the parse while maintaining only the best hypothesis at each step, has a well-documented disadvantage: error propagation (McDonald and Nivre, 2007). When the parser encounters states during parsing that are unlike those found during training, it is more likely to make mistakes, leading to states which are increasingly more foreign and causing errors to accumulate. Introduction Meaning representation languages and systems have been devised for specific domains, such as ATIS for air-travel bookings (Dahl et al., 1994) and database queries (Zelle and Mooney, 1996; 1 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1–11, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Lin"
P16-1001,K15-1004,0,0.259438,"Missing"
P16-1001,D15-1136,0,0.155254,"Missing"
P16-1001,P15-1095,0,0.0795576,"Missing"
P16-1001,D08-1059,0,0.0138467,"gain 2.7 points in F-Score, at the cost of 80-100x more computation. In problems with a less good expert, the gain from exploration could be much greater. Similarly, if designing an expert for a task is time-consuming, then it may be a better investment to rely on exploration with a poor expert to achieve the same result. 6 VRelated Work Other strategies have been used to mitigate the error propagation problem in transition-based parsing. A common approach is to use beam search through state-space for each action choice to find a better approximation of the long-term score of the action, e.g. Zhang and Clark (2008). Goldberg and Elhadad (2010) remove the determinism of the sequence of actions to create easy-first parsers, which postpone uncertain, error-prone decisions until more information is available. This contrasts with working inflexibly left-to-right along a sentence, or bottom-to-top up a tree. Goldberg and Nivre (2012) introduce dynamic experts that are complete in that they will respond from any state, not just those on the perfect trajectory assuming no earlier mistakes; any expert used with an imitation learning algorithm needs to be complete in this sense. Their algorithm takes exploratory"
P16-1001,Q14-1042,1,0.817886,"om si , with π ˆ (s) = arg maxa∈A wa · Φ(s), assuming a linear classifier and a feature function Φ(s). We require an expert, π ∗ , that can indicate what actions should be taken on each si to reach the target (gold) end state. In problems like POStagging these are directly inferable from gold, as the number of actions (T ) equals the number of DAGGER relies on an externally specified expert (oracle) to define the correct action in each state; this defines a simple 0-1 loss function for each action. Other imitation learning algorithms (such as LOLS, SEARN) and the variant of DAGGER proposed by Vlachos and Clark (2014) (henceforth V-DAGGER) can leverage a task level loss function that does not decompose over the actions taken to construct the AMR graph. However these require extra computations to roll-out to an end-state AMR graph for each possible action not taken. The large action-space of our transition system makes these algorithms computationally infeasible, and roll-outs to an end-state for many of 2 Action Name NextEdge NextNode Swap Param. lr lc ReplaceHead β non-empty Reattach κ DeleteNode Insert lc InsertBelow Pre-conditions β non-empty β empty β non-empty β non-empty β empty; leaf σ0 Outcome of a"
P16-1001,W11-0307,1,0.704386,"th the application of imitation learning using DAGGER and the targeted V-DAGGER we propose here. One way to ameliorate this problem is to employ imitation learning algorithms for structured prediction. Algorithms such as SEARN (Daum´e III et al., 2009), DAGGER (Ross et al., 2011), and LOLS (Chang et al., 2015) address the problem of error propagation by iteratively adjusting the training data to increasingly expose the model to training instances it is likely to encounter during test. Such algorithms have been shown to improve performance in a variety of tasks including information extraction(Vlachos and Craven, 2011), dependency parsing (Goldberg and Nivre, 2013), and feature selection (He et al., 2013). In this work we build on the transition-based parsing approach of Wang et al. (2015b) and explore the applicability of different imitation algorithms to AMR parsing, which has a more complex output space than those considered previously. The complexity of AMR parsing affects transition-based methods that rely on features to represent structure, since these often cannot capture the information necessary to predict the correct transition according to the gold standard. In other words, the features defined a"
P16-1001,P15-2141,0,0.105602,"1 prep-against attack-01 mod cyber NATO Figure 1: Dependency (left) and AMR graph (right) for: “The center will bolster NATO’s defenses against cyber-attacks.’ Liang et al., 2013). Such machine-interpretable representations enable many applications relying on natural language understanding. The ambition of Abstract Meaning Representation (AMR) is that it is domain-independent and useful in a variety of applications (Banarescu et al., 2013). The first AMR parser by Flanigan et al. (2014) used graph-based inference to find a highestscoring maximum spanning connected acyclic graph. Later work by Wang et al. (2015b) was inspired by the similarity between the dependency parse of a sentence and its semantic AMR graph (Figure 1). Wang et al. (2015b) start from the dependency parse and learn a transition-based parser that converts it incrementally into an AMR graph using greedy decoding. An advantage of this approach is that the initial stage of dependency parsing is well-studied and trained using larger corpora than that for which AMR annotations exist. Greedy decoding, where the parser builds the parse while maintaining only the best hypothesis at each step, has a well-documented disadvantage: error prop"
P16-1001,H94-1010,0,\N,Missing
P16-1001,P14-5010,0,\N,Missing
P16-1001,P11-1060,0,\N,Missing
P16-1001,N15-1040,0,\N,Missing
P19-1330,N18-1064,0,0.0407251,"Missing"
P19-1330,W11-2101,0,0.0860651,"Missing"
P19-1330,P18-1015,0,0.0342298,", 2005) must be measured separately from “fluency”, as judgments for them had low correlation. Finally, we make the highlighted XS UM dataset, codebase to replicate the crowd-sourcing experiments and all other materials produced in our study publicly available. 2 Literature Review In recent years, summarization literature has investigated different means of conducting manual evaluation. We study a sample of 26 recent papers from major ACL conferences and outline the trends of manual evaluation in summarization in Table 1. From 26 papers, 11 papers (e.g., See et al., 2017; Kedzie et al., 2018; Cao et al., 2018) did not conduct any manual evaluation. Following the Document Understanding Conference (DUC, Dang, 2005), a majority of work has focused on evaluating the content and the linguistic quality of summaries (Nenkova, 2005). However, there seems to be a lack of consensus on how a summary should be evaluated: (i) Should it be evaluated relative to other summaries or standalone in absolute terms? and (ii) What would be a good source of comparison: the input document or the reference summary? The disagreements on these issues result in authors evaluating their summaries often (11 out of 26 papers) us"
P19-1330,N18-1150,0,0.337783,"ning multiple ones increases dataset creation cost, thus evaluation against them is likely to exhibit reference bias (Louis and Nenkova, 2013; Fomicheva and Specia, 2016), penalizing summaries containing salient content different from the reference. For the above reasons manual evaluation is considered necessary for measuring progress in summarization. However, the intrinsic difficulty of the task has led to research without manual evaluation or only fluency being assessed manually. Those that conduct manual assessment of the content, typically use a single reference summary, either directly (Celikyilmaz et al., 2018; Tan et al., 2017) or through questions (Narayan et al., 2018b,c) and thus are also likely to exhibit reference bias. In this paper we propose a novel approach for manual evaluation, H IGHlight-based Referenceless Evaluation of document Summarization (H IGH RES), in which a summary is assessed against the source document via manually highlighted salient content in the latter (see Figure 1 for an example). Our approach avoids reference bias, as the multiple highlights obtained help consider more content than what is contained in a single reference. The highlights are not dependent on the summa"
P19-1330,N18-2097,0,0.0682798,"Missing"
P19-1330,P16-2013,0,0.172488,"g of the Association for Computational Linguistics, pages 3381–3392 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics automatic measures are unlikely to be sufficient to measure performance in summarization (Schluter, 2017), also known for other tasks in which the goal is to generate natural language (Novikova et al., 2017). Furthermore, the datasets typically considered have a single reference summary, as obtaining multiple ones increases dataset creation cost, thus evaluation against them is likely to exhibit reference bias (Louis and Nenkova, 2013; Fomicheva and Specia, 2016), penalizing summaries containing salient content different from the reference. For the above reasons manual evaluation is considered necessary for measuring progress in summarization. However, the intrinsic difficulty of the task has led to research without manual evaluation or only fluency being assessed manually. Those that conduct manual assessment of the content, typically use a single reference summary, either directly (Celikyilmaz et al., 2018; Tan et al., 2017) or through questions (Narayan et al., 2018b,c) and thus are also likely to exhibit reference bias. In this paper we propose a"
P19-1330,N18-1065,0,0.0767735,"Missing"
P19-1330,P18-1064,0,0.0146091,"s annotators. Finally, a small number of work evaluates the ”Correctness” (Chen and Bansal, 2018; Li et al., 2018b; Chen and Bansal, 2018) of the summary, similar to fact checking (Vlachos and Riedel, 2014), which can be a challenging task in its own right. The linguistic quality of a summary encompasses many different qualities such as fluency, grammatically, readability, formatting, naturalness and coherence. Most recent work uses a single human judgment to capture all linguistic qualities of the summary (Hsu et al., 2018; Kry´sci´nski et al., 2018; Narayan et al., 2018b; Song et al., 2018; Guo et al., 2018); we group them under “Fluency” in Table 1 with an exception of “Clarity” which 3383 was evaluated in the DUC evaluation campaigns (Dang, 2005). The “Clarity” metric puts emphasis in easy identification of noun and pronoun phrases in the summary which is a different dimension than “Fluency”, as a summary may be fluent but difficult to be understood due to poor clarity. et al., 2018; Narayan et al., 2018a; Hsu et al., 2018; Kry´sci´nski et al., 2018), asking judges to assess the summary after reading the source document. However this requires more effort and is known to lead to low inter-annota"
P19-1330,D18-1086,1,0.833109,"In relative assessment of summarization, annotators are shown two or more summaries and are asked to rank them according to the dimension at question (Yang et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Guo et al., 2018; Krishna and Srinivasan, 2018). The relative assessment is often done using the paired comparison (Thurstone, 1994) or the best-worst scaling (Woodworth and G, 1991; Louviere et al., 2015), to improve inter-annotator agreement. On the other hand, absolute assessment of summarization (Li et al., 2018b; Song et al., 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Hardy and Vlachos, 2018) is often done using the Likert rating scale (Likert, 1932) where a summary is assessed on a numerical scale. Absolute assessment was also employed in combination with the question answering approach for content evaluation (Narayan et al., 2018b; Mendes et al., 2019). Both approaches, relative ranking and absolute assessment, have been investigated extensively in Machine Translation (Bojar et al., 2016, 2017). Absolute assessment correlates highly with the relative assessment without the bias introduced by having a simultaneous assessment of several models (Bojar et al., 2011). 3 Choice of Ref"
P19-1330,P18-1063,0,0.395164,"nd (ii) What would be a good source of comparison: the input document or the reference summary? The disagreements on these issues result in authors evaluating their summaries often (11 out of 26 papers) using automatic measures such as ROUGE (Lin, 2004) despite of its limitations (Schluter, 2017). In what follows, we discuss previously proposed approaches along three axes: evaluation metrics, relative vs. absolute, and the choice of reference. Evaluation Metrics Despite differences in the exact definitions, the majority (e.g., Hsu et al., 2018; Celikyilmaz et al., 2018; Narayan et al., 2018b; Chen and Bansal, 2018; Peyrard and Gurevych, 2018) agree on both or either one of two broad quality definitions: coverage determines how much of the salient content of the source document is captured in the summary, and informativeness, how much of the content captured in the summary is salient with regards to the original document. These measures correspond to “recall” and “precision” metrics respectively in Table 1, notions that are commonly used 3382 X X X X X X X X X With Ref. & Doc. Relative X With Document Absolute X X X With Reference Precision X X Recall Clarity Fluency Correctness QA Pyramid No Manual Eva"
P19-1330,P18-1013,0,0.384447,"ated relative to other summaries or standalone in absolute terms? and (ii) What would be a good source of comparison: the input document or the reference summary? The disagreements on these issues result in authors evaluating their summaries often (11 out of 26 papers) using automatic measures such as ROUGE (Lin, 2004) despite of its limitations (Schluter, 2017). In what follows, we discuss previously proposed approaches along three axes: evaluation metrics, relative vs. absolute, and the choice of reference. Evaluation Metrics Despite differences in the exact definitions, the majority (e.g., Hsu et al., 2018; Celikyilmaz et al., 2018; Narayan et al., 2018b; Chen and Bansal, 2018; Peyrard and Gurevych, 2018) agree on both or either one of two broad quality definitions: coverage determines how much of the salient content of the source document is captured in the summary, and informativeness, how much of the content captured in the summary is salient with regards to the original document. These measures correspond to “recall” and “precision” metrics respectively in Table 1, notions that are commonly used 3382 X X X X X X X X X With Ref. & Doc. Relative X With Document Absolute X X X With Reference P"
P19-1330,J10-3005,0,0.114096,"presents papers that do not report on human evaluation; the second column identifies matrices used for evaluating content (“Pyramid”, “QA”, “Correctness”, “Recall” and “Precision”) and quality (“Clarity”, “Fluency”) of summaries; the third column focuses if the system ranking reported by humans on content evaluation were “Absolute” or “Relative”; and finally, the fourth column evaluates if summaries were evaluated against the input document (“With Document”), the reference summary (“With Reference”) or both (“With Ref. & Doc.”). in information retrieval and information extraction literature. Clarke and Lapata (2010) proposed a question-answering based approach to improve the agreement among human evaluations for the quality of summary content, which was recently employed by Narayan et al. (2018b) and Narayan et al. (2018c) (QA in Table 1). In this approach, questions were created first from the reference summary and then the system summaries were judged with regards to whether they enabled humans to answer those questions correctly. ShafieiBavani et al. (2018), on the other hand, used the “Pyramid” method (Nenkova and Passonneau, 2004) which requires summaries to be annotated by experts for salient infor"
P19-1330,P18-1014,0,0.0550599,"Missing"
P19-1330,D18-1208,0,0.130462,"Missing"
P19-1330,N18-1153,0,0.0428051,"Missing"
P19-1330,D18-1207,0,0.0443902,"Missing"
P19-1330,N18-2009,0,0.139967,"ani et al. (2018), on the other hand, used the “Pyramid” method (Nenkova and Passonneau, 2004) which requires summaries to be annotated by experts for salient information. A similar evaluation approach is the factoids analysis by Teufel and Van Halteren (2004) which evaluates the system summary against factoids, a representation based on atomic units of information, that are extracted from multiple gold summaries. However, as in the case of the “Pyramid” method, extracting factoids requires experts annotators. Finally, a small number of work evaluates the ”Correctness” (Chen and Bansal, 2018; Li et al., 2018b; Chen and Bansal, 2018) of the summary, similar to fact checking (Vlachos and Riedel, 2014), which can be a challenging task in its own right. The linguistic quality of a summary encompasses many different qualities such as fluency, grammatically, readability, formatting, naturalness and coherence. Most recent work uses a single human judgment to capture all linguistic qualities of the summary (Hsu et al., 2018; Kry´sci´nski et al., 2018; Narayan et al., 2018b; Song et al., 2018; Guo et al., 2018); we group them under “Fluency” in Table 1 with an exception of “Clarity” which 3383 was evaluat"
P19-1330,C18-1121,0,0.145098,"ani et al. (2018), on the other hand, used the “Pyramid” method (Nenkova and Passonneau, 2004) which requires summaries to be annotated by experts for salient information. A similar evaluation approach is the factoids analysis by Teufel and Van Halteren (2004) which evaluates the system summary against factoids, a representation based on atomic units of information, that are extracted from multiple gold summaries. However, as in the case of the “Pyramid” method, extracting factoids requires experts annotators. Finally, a small number of work evaluates the ”Correctness” (Chen and Bansal, 2018; Li et al., 2018b; Chen and Bansal, 2018) of the summary, similar to fact checking (Vlachos and Riedel, 2014), which can be a challenging task in its own right. The linguistic quality of a summary encompasses many different qualities such as fluency, grammatically, readability, formatting, naturalness and coherence. Most recent work uses a single human judgment to capture all linguistic qualities of the summary (Hsu et al., 2018; Kry´sci´nski et al., 2018; Narayan et al., 2018b; Song et al., 2018; Guo et al., 2018); we group them under “Fluency” in Table 1 with an exception of “Clarity” which 3383 was evaluat"
P19-1330,C18-1101,0,0.0506976,"Missing"
P19-1330,W04-1013,0,0.581662,"as input a source document consisting of multiple sentences and methods need to generate a shorter text that expresses the salient information of the source fluently and succinctly. Thus there can be multiple equally good summaries for the same source document as not all salient information can fit in a given summary length, while even extractive methods that select complete sentences are not guaranteed to produce a coherent summary overall. The most consistently used evaluation approach is comparison of the summaries produces against reference summaries via automatic measures such as ROUGE (Lin, 2004) and its variants. However, 3381 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3381–3392 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics automatic measures are unlikely to be sufficient to measure performance in summarization (Schluter, 2017), also known for other tasks in which the goal is to generate natural language (Novikova et al., 2017). Furthermore, the datasets typically considered have a single reference summary, as obtaining multiple ones increases dataset creation cost, thus evaluation again"
P19-1330,P18-2027,0,0.0675982,"Missing"
P19-1330,J13-2002,0,0.179231,"of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3381–3392 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics automatic measures are unlikely to be sufficient to measure performance in summarization (Schluter, 2017), also known for other tasks in which the goal is to generate natural language (Novikova et al., 2017). Furthermore, the datasets typically considered have a single reference summary, as obtaining multiple ones increases dataset creation cost, thus evaluation against them is likely to exhibit reference bias (Louis and Nenkova, 2013; Fomicheva and Specia, 2016), penalizing summaries containing salient content different from the reference. For the above reasons manual evaluation is considered necessary for measuring progress in summarization. However, the intrinsic difficulty of the task has led to research without manual evaluation or only fluency being assessed manually. Those that conduct manual assessment of the content, typically use a single reference summary, either directly (Celikyilmaz et al., 2018; Tan et al., 2017) or through questions (Narayan et al., 2018b,c) and thus are also likely to exhibit reference bias"
P19-1330,N19-1397,1,0.811344,"lative assessment is often done using the paired comparison (Thurstone, 1994) or the best-worst scaling (Woodworth and G, 1991; Louviere et al., 2015), to improve inter-annotator agreement. On the other hand, absolute assessment of summarization (Li et al., 2018b; Song et al., 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Hardy and Vlachos, 2018) is often done using the Likert rating scale (Likert, 1932) where a summary is assessed on a numerical scale. Absolute assessment was also employed in combination with the question answering approach for content evaluation (Narayan et al., 2018b; Mendes et al., 2019). Both approaches, relative ranking and absolute assessment, have been investigated extensively in Machine Translation (Bojar et al., 2016, 2017). Absolute assessment correlates highly with the relative assessment without the bias introduced by having a simultaneous assessment of several models (Bojar et al., 2011). 3 Choice of Reference. The most convenient way to evaluate a system summary is to assess it against the reference summary (Celikyilmaz et al., 2018; Yang et al., 2017; Peyrard and Gurevych, 2018), as this typically requires less effort than reading the source document. The question"
P19-1330,P18-1188,1,0.0544469,"hasize differences among systems that would be ignored under other evaluation approaches.1 1 Figure 1: Highlight-based evaluation of a summary. Annotators to evaluate a summary (bottom) against the highlighted source document (top) presented with a heat map marking the salient content in the document; the darker the colour, the more annotators deemed the highlighted text salient. Introduction Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets (Sandhaus, 2008; Hermann et al., 2015; Narayan et al., 2018b) which has enabled the development of novel methods, many of them employing recent advances in neural networks (See et al., 2017; Narayan et al., 2018c; Pasunuru and Bansal, 2018, inter alia). ∗ The work was primarily done while Shashi was still at School of Informatics, University of Edinburgh. 1 Our dataset and code are available at https:// github.com/sheffieldnlp/highres Measuring progress in summarization is difficult, as the task has as input a source document consisting of multiple sentences and methods need to generate a shorter text that expresses the salient information of the sour"
P19-1330,D18-1206,1,0.0465765,"hasize differences among systems that would be ignored under other evaluation approaches.1 1 Figure 1: Highlight-based evaluation of a summary. Annotators to evaluate a summary (bottom) against the highlighted source document (top) presented with a heat map marking the salient content in the document; the darker the colour, the more annotators deemed the highlighted text salient. Introduction Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets (Sandhaus, 2008; Hermann et al., 2015; Narayan et al., 2018b) which has enabled the development of novel methods, many of them employing recent advances in neural networks (See et al., 2017; Narayan et al., 2018c; Pasunuru and Bansal, 2018, inter alia). ∗ The work was primarily done while Shashi was still at School of Informatics, University of Edinburgh. 1 Our dataset and code are available at https:// github.com/sheffieldnlp/highres Measuring progress in summarization is difficult, as the task has as input a source document consisting of multiple sentences and methods need to generate a shorter text that expresses the salient information of the sour"
P19-1330,N18-1158,1,0.0535864,"hasize differences among systems that would be ignored under other evaluation approaches.1 1 Figure 1: Highlight-based evaluation of a summary. Annotators to evaluate a summary (bottom) against the highlighted source document (top) presented with a heat map marking the salient content in the document; the darker the colour, the more annotators deemed the highlighted text salient. Introduction Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets (Sandhaus, 2008; Hermann et al., 2015; Narayan et al., 2018b) which has enabled the development of novel methods, many of them employing recent advances in neural networks (See et al., 2017; Narayan et al., 2018c; Pasunuru and Bansal, 2018, inter alia). ∗ The work was primarily done while Shashi was still at School of Informatics, University of Edinburgh. 1 Our dataset and code are available at https:// github.com/sheffieldnlp/highres Measuring progress in summarization is difficult, as the task has as input a source document consisting of multiple sentences and methods need to generate a shorter text that expresses the salient information of the sour"
P19-1330,N04-1019,0,0.879652,"& Doc.”). in information retrieval and information extraction literature. Clarke and Lapata (2010) proposed a question-answering based approach to improve the agreement among human evaluations for the quality of summary content, which was recently employed by Narayan et al. (2018b) and Narayan et al. (2018c) (QA in Table 1). In this approach, questions were created first from the reference summary and then the system summaries were judged with regards to whether they enabled humans to answer those questions correctly. ShafieiBavani et al. (2018), on the other hand, used the “Pyramid” method (Nenkova and Passonneau, 2004) which requires summaries to be annotated by experts for salient information. A similar evaluation approach is the factoids analysis by Teufel and Van Halteren (2004) which evaluates the system summary against factoids, a representation based on atomic units of information, that are extracted from multiple gold summaries. However, as in the case of the “Pyramid” method, extracting factoids requires experts annotators. Finally, a small number of work evaluates the ”Correctness” (Chen and Bansal, 2018; Li et al., 2018b; Chen and Bansal, 2018) of the summary, similar to fact checking (Vlachos and"
P19-1330,D17-1238,0,0.0658811,"Missing"
P19-1330,N18-2102,0,0.0458397,"y (bottom) against the highlighted source document (top) presented with a heat map marking the salient content in the document; the darker the colour, the more annotators deemed the highlighted text salient. Introduction Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets (Sandhaus, 2008; Hermann et al., 2015; Narayan et al., 2018b) which has enabled the development of novel methods, many of them employing recent advances in neural networks (See et al., 2017; Narayan et al., 2018c; Pasunuru and Bansal, 2018, inter alia). ∗ The work was primarily done while Shashi was still at School of Informatics, University of Edinburgh. 1 Our dataset and code are available at https:// github.com/sheffieldnlp/highres Measuring progress in summarization is difficult, as the task has as input a source document consisting of multiple sentences and methods need to generate a shorter text that expresses the salient information of the source fluently and succinctly. Thus there can be multiple equally good summaries for the same source document as not all salient information can fit in a given summary length, while e"
P19-1330,N18-2103,0,0.0928385,"good source of comparison: the input document or the reference summary? The disagreements on these issues result in authors evaluating their summaries often (11 out of 26 papers) using automatic measures such as ROUGE (Lin, 2004) despite of its limitations (Schluter, 2017). In what follows, we discuss previously proposed approaches along three axes: evaluation metrics, relative vs. absolute, and the choice of reference. Evaluation Metrics Despite differences in the exact definitions, the majority (e.g., Hsu et al., 2018; Celikyilmaz et al., 2018; Narayan et al., 2018b; Chen and Bansal, 2018; Peyrard and Gurevych, 2018) agree on both or either one of two broad quality definitions: coverage determines how much of the salient content of the source document is captured in the summary, and informativeness, how much of the content captured in the summary is salient with regards to the original document. These measures correspond to “recall” and “precision” metrics respectively in Table 1, notions that are commonly used 3382 X X X X X X X X X With Ref. & Doc. Relative X With Document Absolute X X X With Reference Precision X X Recall Clarity Fluency Correctness QA Pyramid No Manual Eval Systems See et al. (2017) L"
P19-1330,N18-1157,0,0.0605052,"Missing"
P19-1330,W14-2508,1,0.832103,"neau, 2004) which requires summaries to be annotated by experts for salient information. A similar evaluation approach is the factoids analysis by Teufel and Van Halteren (2004) which evaluates the system summary against factoids, a representation based on atomic units of information, that are extracted from multiple gold summaries. However, as in the case of the “Pyramid” method, extracting factoids requires experts annotators. Finally, a small number of work evaluates the ”Correctness” (Chen and Bansal, 2018; Li et al., 2018b; Chen and Bansal, 2018) of the summary, similar to fact checking (Vlachos and Riedel, 2014), which can be a challenging task in its own right. The linguistic quality of a summary encompasses many different qualities such as fluency, grammatically, readability, formatting, naturalness and coherence. Most recent work uses a single human judgment to capture all linguistic qualities of the summary (Hsu et al., 2018; Kry´sci´nski et al., 2018; Narayan et al., 2018b; Song et al., 2018; Guo et al., 2018); we group them under “Fluency” in Table 1 with an exception of “Clarity” which 3383 was evaluated in the DUC evaluation campaigns (Dang, 2005). The “Clarity” metric puts emphasis in easy i"
P19-1330,E17-2112,0,0.0192472,"he summary which is a different dimension than “Fluency”, as a summary may be fluent but difficult to be understood due to poor clarity. et al., 2018; Narayan et al., 2018a; Hsu et al., 2018; Kry´sci´nski et al., 2018), asking judges to assess the summary after reading the source document. However this requires more effort and is known to lead to low inter-annotator agreement (Nenkova and Passonneau, 2004). Absolute vs Relative Summary Ranking. In relative assessment of summarization, annotators are shown two or more summaries and are asked to rank them according to the dimension at question (Yang et al., 2017; Chen and Bansal, 2018; Narayan et al., 2018a; Guo et al., 2018; Krishna and Srinivasan, 2018). The relative assessment is often done using the paired comparison (Thurstone, 1994) or the best-worst scaling (Woodworth and G, 1991; Louviere et al., 2015), to improve inter-annotator agreement. On the other hand, absolute assessment of summarization (Li et al., 2018b; Song et al., 2018; Kry´sci´nski et al., 2018; Hsu et al., 2018; Hardy and Vlachos, 2018) is often done using the Likert rating scale (Likert, 1932) where a summary is assessed on a numerical scale. Absolute assessment was also emplo"
P19-1330,E17-2007,0,0.283044,"ile even extractive methods that select complete sentences are not guaranteed to produce a coherent summary overall. The most consistently used evaluation approach is comparison of the summaries produces against reference summaries via automatic measures such as ROUGE (Lin, 2004) and its variants. However, 3381 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3381–3392 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics automatic measures are unlikely to be sufficient to measure performance in summarization (Schluter, 2017), also known for other tasks in which the goal is to generate natural language (Novikova et al., 2017). Furthermore, the datasets typically considered have a single reference summary, as obtaining multiple ones increases dataset creation cost, thus evaluation against them is likely to exhibit reference bias (Louis and Nenkova, 2013; Fomicheva and Specia, 2016), penalizing summaries containing salient content different from the reference. For the above reasons manual evaluation is considered necessary for measuring progress in summarization. However, the intrinsic difficulty of the task has led"
P19-1330,P17-1099,0,0.779884,"summary. Annotators to evaluate a summary (bottom) against the highlighted source document (top) presented with a heat map marking the salient content in the document; the darker the colour, the more annotators deemed the highlighted text salient. Introduction Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets (Sandhaus, 2008; Hermann et al., 2015; Narayan et al., 2018b) which has enabled the development of novel methods, many of them employing recent advances in neural networks (See et al., 2017; Narayan et al., 2018c; Pasunuru and Bansal, 2018, inter alia). ∗ The work was primarily done while Shashi was still at School of Informatics, University of Edinburgh. 1 Our dataset and code are available at https:// github.com/sheffieldnlp/highres Measuring progress in summarization is difficult, as the task has as input a source document consisting of multiple sentences and methods need to generate a shorter text that expresses the salient information of the source fluently and succinctly. Thus there can be multiple equally good summaries for the same source document as not all salient info"
P19-1330,C18-1077,0,0.251682,"eference Precision X X Recall Clarity Fluency Correctness QA Pyramid No Manual Eval Systems See et al. (2017) Lin et al. (2018) Cohan et al. (2018) Liao et al. (2018) Kedzie et al. (2018) Amplayo et al. (2018) Jadhav and Rajan (2018) Li et al. (2018a) Pasunuru and Bansal (2018) Cao et al. (2018) Sakaue et al. (2018) Celikyilmaz et al. (2018) Chen and Bansal (2018) Guo et al. (2018) Hardy and Vlachos (2018) Hsu et al. (2018) Krishna and Srinivasan (2018) Kry´sci´nski et al. (2018) Li et al. (2018b) Narayan et al. (2018a) Narayan et al. (2018b) Narayan et al. (2018c) Peyrard and Gurevych (2018) ShafieiBavani et al. (2018) Song et al. (2018) Yang et al. (2017) H IGH RES (ours) X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X Table 1: Overview of manual evaluations conducted in recent summarization systems. We categorize them in four dimensions: the first columns presents papers that do not report on human evaluation; the second column identifies matrices used for evaluating content (“Pyramid”, “QA”, “Correctness”, “Recall” and “Precision”) and quality (“Clarity”, “Fluency”) of summaries; the third column focuses if the system ranking r"
P19-1330,C18-1146,0,0.050614,"Missing"
P19-1330,P17-1108,0,0.0299993,"es dataset creation cost, thus evaluation against them is likely to exhibit reference bias (Louis and Nenkova, 2013; Fomicheva and Specia, 2016), penalizing summaries containing salient content different from the reference. For the above reasons manual evaluation is considered necessary for measuring progress in summarization. However, the intrinsic difficulty of the task has led to research without manual evaluation or only fluency being assessed manually. Those that conduct manual assessment of the content, typically use a single reference summary, either directly (Celikyilmaz et al., 2018; Tan et al., 2017) or through questions (Narayan et al., 2018b,c) and thus are also likely to exhibit reference bias. In this paper we propose a novel approach for manual evaluation, H IGHlight-based Referenceless Evaluation of document Summarization (H IGH RES), in which a summary is assessed against the source document via manually highlighted salient content in the latter (see Figure 1 for an example). Our approach avoids reference bias, as the multiple highlights obtained help consider more content than what is contained in a single reference. The highlights are not dependent on the summaries being evaluate"
P19-1330,W04-3254,0,0.548402,"Missing"
P19-1330,W16-2301,0,\N,Missing
P19-1330,W17-4717,0,\N,Missing
P19-1585,C18-1139,0,0.187687,"d comparably to BiLSTM-CRFs (Huang et al., 2015) (which have dominated the named entity literature for the last few years) on a flat NER task. Given the larger size of the OntoNotes dataset, we report results from a single iteration, as opposed to the average of 5 runs as in the case of ACE05. Model F1 BiLSTM-CRF (Chiu and Nichols, 2016) ID-CNN (Strubell et al., 2017) BiLSTM-CRF (Strubell et al., 2017) Merge and Label 86.28 86.84 86.99 87.59 LM embeddings or extra data BiLSTM-CRF lex (Ghaddar and Langlais, 2018) BiLSTM-CRF with CVT (Clark et al., 2018) Merge and Label [BERT] BiLSTM-CRF Flair (Akbik et al., 2018) 87.95 88.81 89.20 89.71 5 To better understand the results, we conducted a small ablation study. The affect of including the Static Layer in the architecture is consistent across both datasets, yielding an improvement of around 2 F1 points; the updating of the token embeddings based on context seems to allow better merge decisions for each pair of tokens. Next, we look at the method used to update entity embeddings prior to combination into larger entities in the Structure Layer. In the described architecture, we use the Embed Update mechanism (see Figure 7), allowing embeddings to be changed"
P19-1585,Q16-1026,0,0.131442,"Missing"
P19-1585,D18-1217,0,0.0592818,"Missing"
P19-1585,D15-1102,0,0.159494,"trating that its ability to predict nested structures does not impact performance in simpler cases.1 1 Introduction The task of nested named entity recognition (NER) focuses on recognizing and classifying entities that can be nested within each other, such as “United Kingdom” and “The Prime Minister of the United Kingdom” in Figure 1. Such entity structures, while very commonly occurring, cannot be handled by the predominant variant of NER models (McCallum and Li, 2003; Lample et al., 2016), which can only tag non-overlapping entities. A number of approaches have been proposed for nested NER. Lu and Roth (2015) introduced a hypergraph representation which can represent 1 Code available at fishjh2/merge_label https://github.com/ overlapping mentions, which was further improved by Muis and Lu (2017), by assigning tags between each pair of consecutive words, preventing the model from learning spurious structures (overlapping entity structures which are gramatically impossible). More recently, Katiyar and Cardie (2018) built on this approach, adapting an LSTM (Hochreiter and Schmidhuber, 1997) to learn the hypergraph directly, and Wang and Lu (2018) introduced a segmental hypergraph approach, which is a"
P19-1585,N19-1308,0,0.129494,"Missing"
P19-1585,W03-0430,0,0.0119302,"previous approaches trained on the same data. Additionally we compare it against BiLSTM-CRFs, the dominant approach for flat NER structures, demonstrating that its ability to predict nested structures does not impact performance in simpler cases.1 1 Introduction The task of nested named entity recognition (NER) focuses on recognizing and classifying entities that can be nested within each other, such as “United Kingdom” and “The Prime Minister of the United Kingdom” in Figure 1. Such entity structures, while very commonly occurring, cannot be handled by the predominant variant of NER models (McCallum and Li, 2003; Lample et al., 2016), which can only tag non-overlapping entities. A number of approaches have been proposed for nested NER. Lu and Roth (2015) introduced a hypergraph representation which can represent 1 Code available at fishjh2/merge_label https://github.com/ overlapping mentions, which was further improved by Muis and Lu (2017), by assigning tags between each pair of consecutive words, preventing the model from learning spurious structures (overlapping entity structures which are gramatically impossible). More recently, Katiyar and Cardie (2018) built on this approach, adapting an LSTM ("
P19-1585,C18-1161,0,0.0384484,"Missing"
P19-1585,D17-1276,0,0.123117,"nd classifying entities that can be nested within each other, such as “United Kingdom” and “The Prime Minister of the United Kingdom” in Figure 1. Such entity structures, while very commonly occurring, cannot be handled by the predominant variant of NER models (McCallum and Li, 2003; Lample et al., 2016), which can only tag non-overlapping entities. A number of approaches have been proposed for nested NER. Lu and Roth (2015) introduced a hypergraph representation which can represent 1 Code available at fishjh2/merge_label https://github.com/ overlapping mentions, which was further improved by Muis and Lu (2017), by assigning tags between each pair of consecutive words, preventing the model from learning spurious structures (overlapping entity structures which are gramatically impossible). More recently, Katiyar and Cardie (2018) built on this approach, adapting an LSTM (Hochreiter and Schmidhuber, 1997) to learn the hypergraph directly, and Wang and Lu (2018) introduced a segmental hypergraph approach, which is able to incorporate a larger number of span based features, by encoding each span with an LSTM. Our approach decomposes nested NER into two stages. First tokens are merged into entities (Leve"
P19-1585,D14-1162,0,0.0864475,"and u to 3. For full hyperparameter details see the supplementary materials. The number of levels, L, is set to 3, with a kernel size k of 10 on the first level, 20 on the second, and 30 on the third (we increase the kernel size gradually for computational efficiency as first level entities are extremely unlikely to be composed of more than 10 tokens, whereas higher level nested entities may be larger). Training took around 10 hours for OntoNotes, and around 6 hours for ACE 2005, on an Nvidia 1080 Ti. For experiments without language model (LM) embeddings, we used pretrained Glove embeddings (Pennington et al., 2014) of dimension 300. ACE 2005 Model Pr. Rec. F1 Multigraph + MS (Muis and Lu, 2017) RNN + hyp (Katiyar and Cardie, 2018) BiLSTM-CRF stacked (Ju et al., 2018) LSTM + forest [POS] (Wang et al., 2018) Segm. hyp [POS] (Wang and Lu, 2018) Merge and Label 69.1 70.6 74.2 74.5 76.8 75.1 58.1 70.4 70.3 71.5 72.3 74.1 63.1 70.5 72.2 73.0 74.5 74.6 LM embeddings Merge and Label [ELMO] Merge and Label [BERT] 79.7 78.0 78.9 82.7 82.1 82.4 LM + OntoNotes DyGIE (Luan et al., 2019) 82.9 Table 1: ACE 2005 Given the recent success on many tasks using contextual word embeddings, we also evaluate performance using"
P19-1585,N18-1202,0,0.0310098,"Muis and Lu, 2017) RNN + hyp (Katiyar and Cardie, 2018) BiLSTM-CRF stacked (Ju et al., 2018) LSTM + forest [POS] (Wang et al., 2018) Segm. hyp [POS] (Wang and Lu, 2018) Merge and Label 69.1 70.6 74.2 74.5 76.8 75.1 58.1 70.4 70.3 71.5 72.3 74.1 63.1 70.5 72.2 73.0 74.5 74.6 LM embeddings Merge and Label [ELMO] Merge and Label [BERT] 79.7 78.0 78.9 82.7 82.1 82.4 LM + OntoNotes DyGIE (Luan et al., 2019) 82.9 Table 1: ACE 2005 Given the recent success on many tasks using contextual word embeddings, we also evaluate performance using the output of pre-trained BERT (Devlin et al., 2018) and ELMO (Peters et al., 2018) models as input embeddings. This leads to a significant jump in performance to 78.9 with ELMO, and 82.4 with BERT (both avg. over 5846 6 https://github.com/zalandoresearch/flair/ 5 runs with 0.4 and 0.3 std. dev. respectively), an overall increase of 8 F1 points from the previous state-of-the-art. Finally, we report the concurrently published result of Luan et al. (2019), in which they use ELMO embeddings, and additional labelled data (used to train the coreference part of their model and the entity boundaries) from the larger OntoNotes dataset. A secondary advantage of our architecture relat"
P19-1585,N18-1131,0,0.0804942,", and if anything, disadvantages our model, as it not only has to predict the correct entity, but do so on the correct level. That said, the NP labels provide additional information during training, which may give our model an advantage over flat NER models, which do not have access to these labels. 3.4 On the ACE 2005 corpus, we begin our analysis of our model’s performance by comparing to models which do not use the POS tags as additional features, and which use non-contextual word embeddings. These are shown in the top section of Table 1. The previous state-of-the-art F1 of 72.2 was set by Ju et al. (2018), using a series of stacked BiLSTM layers, with CRF decoders on top of each of them. Our model improves this result with an F1 of 74.6 (avg. over 5 runs with std. dev. of 0.4). This also brings the performance into line with Wang et al. (2018) and Wang and Lu (2018), which concatenate embeddings of POS tags with word embeddings as an additional input feature. Training and HyperParameters We performed a small amount of hyperparameter tuning across dropout, learning rate, distance embedding size d, and number of update layers u. We set dropout at 0.1, the learning rate to 0.0005, d to 200, and u"
P19-1585,N18-1079,0,0.290058,"dled by the predominant variant of NER models (McCallum and Li, 2003; Lample et al., 2016), which can only tag non-overlapping entities. A number of approaches have been proposed for nested NER. Lu and Roth (2015) introduced a hypergraph representation which can represent 1 Code available at fishjh2/merge_label https://github.com/ overlapping mentions, which was further improved by Muis and Lu (2017), by assigning tags between each pair of consecutive words, preventing the model from learning spurious structures (overlapping entity structures which are gramatically impossible). More recently, Katiyar and Cardie (2018) built on this approach, adapting an LSTM (Hochreiter and Schmidhuber, 1997) to learn the hypergraph directly, and Wang and Lu (2018) introduced a segmental hypergraph approach, which is able to incorporate a larger number of span based features, by encoding each span with an LSTM. Our approach decomposes nested NER into two stages. First tokens are merged into entities (Level 1 in Figure 1), which are merged with other tokens or entities in higher levels. These merges are encoded as real-valued decisions, which enables a parameterized combination of word embeddings into entity embeddings at d"
P19-1585,D14-1181,0,0.0048042,"is in, as well as neighbouring entities, as opposed to just using information about neighbouring tokens. 5841 2.2 Preliminaries Before analysing each of the main layers of the network, we introduce two building blocks, which are used multiple times throughout the architecture. The first one is the Unfold operators. Given that we process whole news articles in one batch (often giving a sequence length (s) of 500 or greater) we do not allow each token in the sequence to consider every other token. Instead, we define a kernel of size k around each token, similar to convolutional neural networks (Kim, 2014), allowing it to consider the k/2 prior tokens and the k/2 following tokens. Figure 3: Unfold Operators for the passage “... yesterday. The President of France met with ...”. Each row in the matrices corresponds to the words “The”, “President”, ”of” and “France” (top to bottom). The unfold operators create kernels transforming tensors holding the word embeddings of shape [b, s, e] to shape [b, s, k, e]. unfold[from] simply tiles the embedding x of each token k times, and unfold[to] generates the k/2 token embeddings either side, as shown in Figure 3, for a kernel size k of 4. The first row of"
P19-1585,D18-1019,0,0.528721,"umber of approaches have been proposed for nested NER. Lu and Roth (2015) introduced a hypergraph representation which can represent 1 Code available at fishjh2/merge_label https://github.com/ overlapping mentions, which was further improved by Muis and Lu (2017), by assigning tags between each pair of consecutive words, preventing the model from learning spurious structures (overlapping entity structures which are gramatically impossible). More recently, Katiyar and Cardie (2018) built on this approach, adapting an LSTM (Hochreiter and Schmidhuber, 1997) to learn the hypergraph directly, and Wang and Lu (2018) introduced a segmental hypergraph approach, which is able to incorporate a larger number of span based features, by encoding each span with an LSTM. Our approach decomposes nested NER into two stages. First tokens are merged into entities (Level 1 in Figure 1), which are merged with other tokens or entities in higher levels. These merges are encoded as real-valued decisions, which enables a parameterized combination of word embeddings into entity embeddings at different levels. These entity embeddings are used to label the entities identified. The model itself consists of feedforward neural n"
P19-1585,D18-1124,0,0.159794,"flat NER models, which do not have access to these labels. 3.4 On the ACE 2005 corpus, we begin our analysis of our model’s performance by comparing to models which do not use the POS tags as additional features, and which use non-contextual word embeddings. These are shown in the top section of Table 1. The previous state-of-the-art F1 of 72.2 was set by Ju et al. (2018), using a series of stacked BiLSTM layers, with CRF decoders on top of each of them. Our model improves this result with an F1 of 74.6 (avg. over 5 runs with std. dev. of 0.4). This also brings the performance into line with Wang et al. (2018) and Wang and Lu (2018), which concatenate embeddings of POS tags with word embeddings as an additional input feature. Training and HyperParameters We performed a small amount of hyperparameter tuning across dropout, learning rate, distance embedding size d, and number of update layers u. We set dropout at 0.1, the learning rate to 0.0005, d to 200, and u to 3. For full hyperparameter details see the supplementary materials. The number of levels, L, is set to 3, with a kernel size k of 10 on the first level, 20 on the second, and 30 on the third (we increase the kernel size gradually for compu"
P19-1585,N16-1030,0,0.110889,"ained on the same data. Additionally we compare it against BiLSTM-CRFs, the dominant approach for flat NER structures, demonstrating that its ability to predict nested structures does not impact performance in simpler cases.1 1 Introduction The task of nested named entity recognition (NER) focuses on recognizing and classifying entities that can be nested within each other, such as “United Kingdom” and “The Prime Minister of the United Kingdom” in Figure 1. Such entity structures, while very commonly occurring, cannot be handled by the predominant variant of NER models (McCallum and Li, 2003; Lample et al., 2016), which can only tag non-overlapping entities. A number of approaches have been proposed for nested NER. Lu and Roth (2015) introduced a hypergraph representation which can represent 1 Code available at fishjh2/merge_label https://github.com/ overlapping mentions, which was further improved by Muis and Lu (2017), by assigning tags between each pair of consecutive words, preventing the model from learning spurious structures (overlapping entity structures which are gramatically impossible). More recently, Katiyar and Cardie (2018) built on this approach, adapting an LSTM (Hochreiter and Schmidh"
P19-1585,N19-1423,0,\N,Missing
P19-1589,H05-1091,0,0.32331,"Missing"
P19-1589,D18-1514,0,0.0299407,"t al. (2017) proposed to address the problem by formulating it as a reading comprehension challenge, while Obamuyide and Vlachos (2018) proposed to address it as a textual entailment challenge. In this work we address the case where a limited number of supervision instances is available for all relations. In previous work, Obamuyide and Vlachos (2017) explored the use of a Factorization Machine (Rendle, 2010) framework for extracting relations with limited supervision instances. Here we instead propose an approach which is generally applicable to gradient-optimized relation extraction models. Han et al. (2018) proposed a dataset and evaluation setup for few-shot relation classification which assumes access to full supervision for training relations (specifically 700 instances per relation). In contrast, we address a different setting in which only limited supervision is available for all relations. In addition, the setup in Han et al. (2018) requires a model architecture specific to few-shot learning based on distance metric learning. On the other hand, our approach has the advantage that it applies to any gradient-optimized relation classification model. 3 Model-Agnostic Meta-Learning for Relation"
P19-1589,D14-1162,0,0.0853549,"model on (a) SemEval, and (b) TACRED datasets challenging TACRED dataset (Zhang et al., 2017) (TACRED). The SemEval dataset has a total of 8000 training and 2717 testing instances respectively. For experiments the training set is split into two, and we use 7500 instances for training and 500 instances for development. For TACRED, we use the standard training, development and testing splits as provided by Zhang et al. (2017). 4.4 parameters as in Zhang et al. (2017) and Zhang et al. (2018) respectively. Experimental Details and Hyperparameters We initialize word embeddings with Glove vectors (Pennington et al., 2014) and did not fine-tune them during training. Model training and parameter tuning are carried out on the training and development splits of each dataset, and final results reported on the test set. We ensure all models have access to the same data. For model MLRC, for each fraction, we train for 150 meta-learning iterations on TACRED dataset and 1000 meta-iterations on the SemEval dataset using that fraction of data. We then finetune with standard supervised learning using exactly the same data as was used during metalearning. For both relation classification models, that is TACRED-PA and C-GCN"
P19-1589,D13-1137,0,0.0340468,"Missing"
P19-1589,W09-2415,0,0.128115,"Missing"
P19-1589,K17-1034,0,0.0198135,"across tasks using SGD, by making updates to θ: X θ ← θ − ∇θ LTi (fθi0 ) (3) Ti ∼p(T ) where  is the meta step size parameter. Intuitively, the meta-objective explicitly encourages the model to learn model parameters that can be quickly adapted to achieve optimum predictive performance across all tasks with as few gradient descent steps as possible. A number of approaches have been proposed for extracting relations with zero or few supervision instances. For the problem of zero-shot extraction of relations, Rockt¨aschel et al. (2015); Demeester et al. (2016) proposed the use of logic rules, Levy et al. (2017) proposed to address the problem by formulating it as a reading comprehension challenge, while Obamuyide and Vlachos (2018) proposed to address it as a textual entailment challenge. In this work we address the case where a limited number of supervision instances is available for all relations. In previous work, Obamuyide and Vlachos (2017) explored the use of a Factorization Machine (Rendle, 2010) framework for extracting relations with limited supervision instances. Here we instead propose an approach which is generally applicable to gradient-optimized relation extraction models. Han et al. ("
P19-1589,P09-1113,0,0.179412,"Missing"
P19-1589,W15-1506,0,0.0530063,"Missing"
P19-1589,W18-5511,1,0.849575,"size parameter. Intuitively, the meta-objective explicitly encourages the model to learn model parameters that can be quickly adapted to achieve optimum predictive performance across all tasks with as few gradient descent steps as possible. A number of approaches have been proposed for extracting relations with zero or few supervision instances. For the problem of zero-shot extraction of relations, Rockt¨aschel et al. (2015); Demeester et al. (2016) proposed the use of logic rules, Levy et al. (2017) proposed to address the problem by formulating it as a reading comprehension challenge, while Obamuyide and Vlachos (2018) proposed to address it as a textual entailment challenge. In this work we address the case where a limited number of supervision instances is available for all relations. In previous work, Obamuyide and Vlachos (2017) explored the use of a Factorization Machine (Rendle, 2010) framework for extracting relations with limited supervision instances. Here we instead propose an approach which is generally applicable to gradient-optimized relation extraction models. Han et al. (2018) proposed a dataset and evaluation setup for few-shot relation classification which assumes access to full supervision"
P19-1589,N13-1008,0,0.103615,"Missing"
P19-1589,D12-1110,0,0.140971,"Missing"
P19-1589,D12-1042,0,0.0994619,"Missing"
P19-1589,N16-1065,0,0.0442099,"Missing"
P19-1589,D15-1206,0,0.0480589,"Missing"
P19-1589,C14-1220,0,0.0677509,"Missing"
P19-1589,D18-1244,0,0.0482656,"initialize the model parameters with that learned during meta-training. We then proceed to fine-tune the model parameters with standard supervised learning by taking a number of gradient descent steps using the same randomly sampled batches of supervision instances from the relations’ training set as was used during meta-learning (line 11). 4 4.1 Experiments Relation Classification Models We adopt as the learner model (fθ ) two recent supervised relation classification models, the position-aware model of Zhang et al. (2017) (TACRED-PA) and the contextual graph convolution networks proposed in Zhang et al. (2018) (CGCN), both of which are multi-class models with parameters optimized via stochastic gradient descent. 4.2 i=1 10: end while 11: Fine-tune fθ with standard supervised learning. Subsequently we refer to our overall training procedure as summarized in Algorithm 1 as Metalearning Relation Classification (MLRC). We assume access to fθ (learner model), which is a relation classification model parameterized by θ and a distribution over relations p(R). The algorithm consists of the meta-learning phase (lines 1-10), followed by the supervised learning phase (line Setup We conduct experiments in a li"
P19-1589,D17-1004,0,0.229165,"tream applications, including question answering, knowledge base population and web search. A variety of supervised methods have been proposed in the literature for this task (Zelenko et al., 2003; Bunescu and Mooney, 2005; Mintz et al., 2009; Surdeanu et al., 2012; Riedel et al., 2013). Current approaches are predominantly supervised models based on neural networks, for instance recursive neural networks (Socher et al., 2012; Hashimoto et al., 2013), convolutional neural networks (Zeng et al., 2014; Nguyen and Grishman, 2015), recurrent neural networks (Zhang and Wang, 2015; Xu et al., 2015; Zhang et al., 2017) or a combination of recurrent and convolutional neural networks (Vu et al., 2016). The performance of these approaches relies mostly on the quantity of their training data. However, labelled training data can be expensive In this work we propose a model-agnostic protocol for training supervised relation classification systems to achieve higher predictive performance in limited supervision settings, motivated by the observation that meta-learning leads to learning a better parameter initialization for new tasks than ad hoc multi-task learning across all tasks (Finn et al., 2017). We show that"
Q14-1042,P13-2009,1,0.592847,"Missing"
Q14-1042,D11-1039,0,0.0147709,"as training signal (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014). However such an approach would not be straightforward to implement in our application, since the response from the system is not always the result of a database query but, e.g., a navigation instruction that is context-dependent and thus difficult to assess its correctness. Furthermore, it would require the development of a user simulator (Keizer et al., 2012), a non-trivial task which is beyond the scope of this work. A different approach is to use dialogs between a system and its users as proposed by Artzi and Zettlemoyer (2011) using the DARPA communicator corpus (Walker et al., 2002). However, in that work utterances were selected to be shorter than 6 words and to include one noun phrase present in the lexicon used during learning while ignoring short but common phrases such as “yes” and “no”; 557 thus it is unclear whether it would be applicable to our dataset. Finally, dialog context is only taken into account in predicting the dialog act for each utterance. Even though our corpus contains coreference information, we did not attempt this task as it is difficult to evaluate and our performance on node prediction o"
Q14-1042,W13-2322,0,0.0150392,"s. Finally, our approach to annotating coreference avoids repeating the MR of previous utterances, thus resulting in shorter expressions that are closer to the semantics of the NL utterances. The datasets developed in the recent dialog state tracking challenge (Henderson et al., 2014) also consist of dialogs between a user and a tourism information system. However the task is easier since only three entity types are considered (restaurant, coffeeshop and pub), a slot-filling MRL is used and the argument slots take values from fixed lists. The abstract meaning representation (AMR) described by Banarescu et al. (2013) was developed to provide a semantic interpretation layer to improve machine translation (MT) systems. It has similar predicate argument structure to the MRL proposed here, including a lack of cover for temporal relations and scoping. However, due to the different application domains (MT vs. tourism-related activities), there are some differences. Since MT systems operate at the sentence-level, each sentence is interpreted in isolation in AMR, whilst our proposed MRL takes context into account. Also, AMR tries to account for all the words in a sentence, whilst our MRL only tries to capture the"
Q14-1042,P14-1133,0,0.0329311,"to avoid error propagation, but also how to infer latent variables. The main bottleneck is training data sparsity. Some node types appear only a few times in relatively long utterances, and thus it is difficult to infer appropriate alignments for them. Unlike machine translation between natural languages, it is unrealistic to expect large quantities of utterances to be annotated with MR expressions. An appealing alternative would be to use response-based learning, i.e. use the response from the system instead of MR expressions as training signal (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014). However such an approach would not be straightforward to implement in our application, since the response from the system is not always the result of a database query but, e.g., a navigation instruction that is context-dependent and thus difficult to assess its correctness. Furthermore, it would require the development of a user simulator (Keizer et al., 2012), a non-trivial task which is beyond the scope of this work. A different approach is to use dialogs between a system and its users as proposed by Artzi and Zettlemoyer (2011) using the DARPA communicator corpus (Walker et al., 2002). Ho"
Q14-1042,P13-1042,0,0.124549,"ned classifiers by 9.0 and 4.8 points in F-score on the development and test sets respectively. 1 Introduction Semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation (MR). Progress in semantic parsing has been facilitated by the existence of corpora containing utterances annotated with MRs, the most commonly used being ATIS (Dahl et al., 1994) and GeoQuery (Zelle, 1995). As these corpora cover rather narrow application domains, recent work has developed corpora to support natural language interfaces to the Freebase database (Cai and Yates, 2013), as well as the development of MT systems (Banarescu et al., 2013). However, these existing corpora have some important limitations. The MRs accompanying the utterances are typically restricted to some form of database query. Furthermore, in most cases each utterance is interpreted in isolation; thus utterances that use coreference or whose semantics are contextdependent are typically ignored. In this paper we present a new corpus for context-dependent semantic parsing to support the development of an interactive navigation and exploration system for tourismrelated activities. The new corpus"
Q14-1042,H94-1010,0,0.770624,"on system. We develop a semantic parser for this corpus by adapting the imitation learning algorithm DAGGER without requiring alignment information during training. DAGGER improves upon independently trained classifiers by 9.0 and 4.8 points in F-score on the development and test sets respectively. 1 Introduction Semantic parsing is the task of translating natural language utterances into a machine-interpretable meaning representation (MR). Progress in semantic parsing has been facilitated by the existence of corpora containing utterances annotated with MRs, the most commonly used being ATIS (Dahl et al., 1994) and GeoQuery (Zelle, 1995). As these corpora cover rather narrow application domains, recent work has developed corpora to support natural language interfaces to the Freebase database (Cai and Yates, 2013), as well as the development of MT systems (Banarescu et al., 2013). However, these existing corpora have some important limitations. The MRs accompanying the utterances are typically restricted to some form of database query. Furthermore, in most cases each utterance is interpreted in isolation; thus utterances that use coreference or whose semantics are contextdependent are typically ignor"
Q14-1042,P14-1134,0,0.0277687,"testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al., 2011) models with beam search. In terms of direct comparisons with existing work, the goal of this paper is to introduce the new corpus and provide a competitive first attempt at the new semantic parsing task. However, we believe"
Q14-1042,Q13-1033,0,0.0358589,"Missing"
Q14-1042,D13-1152,0,0.046212,"Missing"
Q14-1042,P12-1051,0,0.0115051,"ogs are being used for training and testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al., 2011) models with beam search. In terms of direct comparisons with existing work, the goal of this paper is to introduce the new corpus and provide a competitive first attempt at the new sema"
Q14-1042,D11-1140,0,0.0179634,"sting on the dialogs from the first scenario and training on the dialogs from the second, the overall performance using Imit+align was 36.9 points in F-score, while in the reverse experiment it was 41.7. Note that direct comparisons against the performances in Tbl. 3 are 556 not meaningful since fewer dialogs are being used for training and testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks"
Q14-1042,D13-1161,0,0.0254847,"GGER to learn not only how to avoid error propagation, but also how to infer latent variables. The main bottleneck is training data sparsity. Some node types appear only a few times in relatively long utterances, and thus it is difficult to infer appropriate alignments for them. Unlike machine translation between natural languages, it is unrealistic to expect large quantities of utterances to be annotated with MR expressions. An appealing alternative would be to use response-based learning, i.e. use the response from the system instead of MR expressions as training signal (Liang et al., 2011; Kwiatkowski et al., 2013; Berant and Liang, 2014). However such an approach would not be straightforward to implement in our application, since the response from the system is not always the result of a database query but, e.g., a navigation instruction that is context-dependent and thus difficult to assess its correctness. Furthermore, it would require the development of a user simulator (Keizer et al., 2012), a non-trivial task which is beyond the scope of this work. A different approach is to use dialogs between a system and its users as proposed by Artzi and Zettlemoyer (2011) using the DARPA communicator corpus"
Q14-1042,P11-1060,0,0.0773233,"nd Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al., 2011) models with beam search. In terms of direct comparisons with existing work, the goal of this paper is to introduce the new corpus and provide a competitive first attempt at the new semantic parsing task. However, we believe it is non-trivial to apply existing approaches to the new task, since, assuming a decomposition similar to that of Sec. 5.1, exhaustive search would be too expensive, and applying vanilla beam search would be difficult since different predictions result in beams of (sometimes radically) different lengths that are not comparable. We have attempted applying the MT-based sema"
Q14-1042,P14-5010,0,0.00320201,"Missing"
Q14-1042,P00-1056,0,0.0211856,"ing Imit+align was 36.9 points in F-score, while in the reverse experiment it was 41.7. Note that direct comparisons against the performances in Tbl. 3 are 556 not meaningful since fewer dialogs are being used for training and testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al.,"
Q14-1042,J00-3003,0,0.0235402,"Missing"
Q14-1042,D07-1071,0,0.0153974,"tionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic parser without requiring a dictionary. In terms of structured prediction frameworks, most previous work uses hidden variable linear (Zettlemoyer and Collins, 2007) or log-linear (Liang et al., 2011) models with beam search. In terms of direct comparisons with existing work, the goal of this paper is to introduce the new corpus and provide a competitive first attempt at the new semantic parsing task. However, we believe it is non-trivial to apply existing approaches to the new task, since, assuming a decomposition similar to that of Sec. 5.1, exhaustive search would be too expensive, and applying vanilla beam search would be difficult since different predictions result in beams of (sometimes radically) different lengths that are not comparable. We have a"
Q14-1042,P09-1110,0,0.0196491,"er evaluated on the other (still respecting the train/test split from before). When testing on the dialogs from the first scenario and training on the dialogs from the second, the overall performance using Imit+align was 36.9 points in F-score, while in the reverse experiment it was 41.7. Note that direct comparisons against the performances in Tbl. 3 are 556 not meaningful since fewer dialogs are being used for training and testing in the cross-scenario setup. 8 Comparison with Related Work Previous work on semantic parsing handled the lack of alignments during training in a variety of ways. Zettlemoyer and Collins (2009) manually engineered a CCG lexicon for the ATIS corpus. Kwiatkowski et al. (2011) used a dedicated algorithm to infer a similar dictionary and used alignments from Giza++ (Och and Ney, 2000) to initialize the relevant features. Most recent work on GeoQuery uses an alignment dictionary that includes for each geographical entity all noun phrases referring to it (Jones et al., 2012). More recently, Flanigan et al. (2014) developed a dedicated alignment model on top of which they learned a semantic parser for the AMR formalism. In our approach, we learn the alignments together with the semantic pa"
Q14-1042,bunt-etal-2012-iso,0,\N,Missing
Q14-1042,P13-1163,0,\N,Missing
S16-1063,D15-1075,0,0.0530468,"Missing"
S16-1063,N16-1138,1,0.893114,"Missing"
S16-1063,I13-1191,0,0.00781955,"Missing"
S16-1063,L16-1729,1,0.859973,"Missing"
S16-1063,P15-1107,0,0.00921714,"000. Each index i in input dim[i] corresponds to a word in the vocabulary, input dim[i] is 1 if the tweet contains the corresponding word in the vocabulary and 0 otherwise. During autoencoder training, an encoder, i.e. embedding function is learned which maps input of size input dim to an embedding of size output dim, as well as a decoder which reconstructs the input. We apply the encoder to the training and test data to obtain features of size output dim for supervised learning and disregard the decoder. While it would be possible to train an encoder which preserves word order, i.e. an LSTM (Li et al., 2015), we opt for a simpler bag-of-word autoencoder here, following Glorot et al. (2011). The architecture of the autoencoder is as follows: input dim is 50000, it has one hidden layer of di1 http://scikit-learn.org/stable/ modules/generated/sklearn.linear_ model.LogisticRegression.html 2 http://github.com/sheffieldnlp/ stance-semeval2016 mensionality 100, and output dim is of size 100. A dropout of 0.1 is added to the hidden layer (Srivastava et al., 2014). The autoencoder is trained with Adam (Kingma and Ba, 2014), using the learning rate 0.1, for 2600 iterations. In each iteration, 500 training"
S16-1063,D13-1171,0,0.0158216,"Missing"
S16-1063,S16-1003,0,0.0869902,"Missing"
S16-1063,strapparava-valitutti-2004-wordnet,0,0.0546914,"e concatenated with the tweet features • Aut-twe*tar: the autoencoder is applied to the tweet and the target, and the outer product of the tweet and target features is used • InTwe: A boolean “targetInTweet” feature We evaluate the impact of traditional sentiment analysis gazetteer features, extracted by assessing appearance of each word of the tweet in the gazetteers: • Emo: emoticon recognition7 One gazetteer/binary feature for each of: happy, sad, happy+sad, not available • Aff: WordNet Affect gazetteer features, one binary feature for each of: anger, disgust, fear, joy, sadness, surprise (Strapparava and Valitutti, 2004)8 6 https://radimrehurek.com/gensim/models/ phrases.html 7 https://github.com/brendano/tweetmotif/ blob/master/emoticons.py 8 http://wndomains.fbk.eu/wnaffect.html 391 Aut-twe+inTwe+Aff Stance FAVOR AGAINST Macro FAVOR AGAINST Macro FAVOR AGAINST Macro P 0.1587 0.5544 R 0.1709 0.4020 0.2278 0.5545 0.1538 0.5700 0.2647 0.5179 0.0769 0.2570 FAVOR AGAINST Macro FAVOR AGAINST Macro FAVOR AGAINST Macro FAVOR AGAINST Macro FAVOR AGAINST Macro 0.1538 0.5680 0.1538 0.7328 0.1652 0.5503 0.1624 0.6539 0.0000 0.5712 0.0000 1.0000 0.2388 0.5709 0.1368 0.7684 0.1731 0.5487 0.0769 0.7888 FAVOR AGAINST Macro"
S16-1063,N12-1072,0,0.00890815,"Missing"
S16-1063,D15-1073,0,0.0194744,"Missing"
S16-1180,W13-2322,0,0.351379,"Missing"
S16-1180,Q15-1039,0,0.036168,"nd also use the DAGGER imitation learning algorithm (Ross et al., 2011) to generalise better to unseen data. The central idea of DAGGER is that the distribution of states encountered by the expert policy during training may not be a good approximation to those seen in testing by the trained policy. Previous work by Rao et al. (2015) used S EARN, a similar imitation learning algorithm, on the AMR problem, with an algorithm that constructs the AMR graph directly from the sentence tokens. Imitation learning has also been used successfully in other semantic parsing tasks (Vlachos and Clark, 2014; Berant and Liang, 2015). In imitation learning approaches such as DAG the previous actions become features for classification learning. However the partial graphs in AMR parsing are rather complex to represent in this way, and combined with the finite amount of training data different actions can be chosen by the expert even though the feature representations for them can be very similar. These decisions appear as noisy outliers in classification learning. To control noise we experiment with the α-bound discussed by Khardon and Wachman (2007), which excludes a training example from future training once it has been m"
S16-1180,P14-1134,0,0.148416,"he Expert Policy The expert policy used in training applies heuristic rules to determine the next action from a given state. It uses the training alignments to construct a mapping between nodes in the dependency tree, and nodes in the target AMR. Any unmapped nodes in the dependency tree will be deleted by the expert, and any unmapped nodes in the AMR graph will be 1 Code available at https://github.com/ hopshackle/dagger-AMR. SemevalSubmission tag bookmarks the version used. 1168 inserted. All our experiments use node alignments from the system of Pourdamghani et al. (2014). 2.2 Action Space Flanigan et al. (2014) and Wang et al. (2015b), both use AMR fragments as their smallest unit, which may consist of more than one AMR concept. Instead, we always work with the individual AMR nodes, and rely on Insert actions to learn how to build common fragments, such as country names. The main adaptations to the actions, summarised in Table 1, stem from this. NextNode and NextEdge form the core action set, labelling nodes and edges respectively without changing the graph structure. Swap, Reattach and ReplaceHead change this structure, but always retain a tree structure. ReplaceHead covers two distinct actions in"
S16-1180,N10-1115,0,0.0327041,"o use a similar set of actions in a concept identification phase. These options improve performance (by 0.5 to 1.0 points on a validation set) by generalising to unseen tokens in test data, which otherwise would have no mapped AMR concepts. For the lc parameters on Insert (InsertBelow) actions, we use all AMR concepts that the expert inserted above (below) any node in the training set with the same lemma as σ0 . 2.3 Additional action constraints Transition-based parsing algorithms have classically relied on a fixed length of trajectory T for guarantees on performance, or at least a bounded T (Goldberg and Elhadad, 2010; Honnibal et al., 2013; Sartorio et al., 2013; McDonald and Nivre, 2007). In our approach T is theoretically unbounded and the algorithm could Insert, or Reattach ad infinitum. We impose constraints to prevent these situations. A Swap action cannot be applied to a previously Swapped edge; once a node has been moved by Reattach, then it cannot be Reattached again; an Insert action is only permissible if no previous Insert action has been used with that node as σ0 ; an Insert action is not permissible if it would insert an AMR concept already in use as any of the parent, children, grand-parents"
S16-1180,W13-3518,0,0.0354728,"ns in a concept identification phase. These options improve performance (by 0.5 to 1.0 points on a validation set) by generalising to unseen tokens in test data, which otherwise would have no mapped AMR concepts. For the lc parameters on Insert (InsertBelow) actions, we use all AMR concepts that the expert inserted above (below) any node in the training set with the same lemma as σ0 . 2.3 Additional action constraints Transition-based parsing algorithms have classically relied on a fixed length of trajectory T for guarantees on performance, or at least a bounded T (Goldberg and Elhadad, 2010; Honnibal et al., 2013; Sartorio et al., 2013; McDonald and Nivre, 2007). In our approach T is theoretically unbounded and the algorithm could Insert, or Reattach ad infinitum. We impose constraints to prevent these situations. A Swap action cannot be applied to a previously Swapped edge; once a node has been moved by Reattach, then it cannot be Reattached again; an Insert action is only permissible if no previous Insert action has been used with that node as σ0 ; an Insert action is not permissible if it would insert an AMR concept already in use as any of the parent, children, grand-parents or grand-children of σ"
S16-1180,P14-5010,0,0.0044165,"ed ner, POS, lemma, brown, label label, path, lemma-path-lemma, POSpath, inserted-inserted, lemma-POS, POS-lemma, dl-lemma, lemma-dl, lemma-label, label-lemma, ner-ner, distance path, lemma-path-lemma, NERpath, POSpath, distance, lemma-POS, dl-lemma, ner-ner distance, lemma-path-lemma, brown-brown, NERpath, POSpath, lemma-dl, lemma-label label, POS-lemma, dl-lemma, ner-ner lemma-lemma-lemma POS-lemma, lemma-POS, dl-lemma, ner-ner Table 2: Features used by context. σ0P is the parent of σ0 , σ0P P the parent of σ0P , and σ0C a child of σ0 . pendency Parser v3.3.1 to construct a dependency tree (Manning et al., 2014); remove punctuation tokens; “/” characters are treated as token separators, but hyphenated words are kept as single tokens; simple regex expressions are applied to find common date formats, and convert these to numeric sequences (e.g. 03-Jan-72 to 3 1 1972); similar regex conversions of common numeric expressions e.g. “two thousand” becomes “2000”. The parser was then able to learn to construct date-entity, temporal-quantity and similar AMR moieties. 3 Table 3: F-Score results on validation set (α = 1). Parameter settings Baseline DAGGER DAGGER & Inc DAGGER & Red DAGGER & Inc & Red NoR 0.642"
S16-1180,D07-1013,0,0.0561978,"tions improve performance (by 0.5 to 1.0 points on a validation set) by generalising to unseen tokens in test data, which otherwise would have no mapped AMR concepts. For the lc parameters on Insert (InsertBelow) actions, we use all AMR concepts that the expert inserted above (below) any node in the training set with the same lemma as σ0 . 2.3 Additional action constraints Transition-based parsing algorithms have classically relied on a fixed length of trajectory T for guarantees on performance, or at least a bounded T (Goldberg and Elhadad, 2010; Honnibal et al., 2013; Sartorio et al., 2013; McDonald and Nivre, 2007). In our approach T is theoretically unbounded and the algorithm could Insert, or Reattach ad infinitum. We impose constraints to prevent these situations. A Swap action cannot be applied to a previously Swapped edge; once a node has been moved by Reattach, then it cannot be Reattached again; an Insert action is only permissible if no previous Insert action has been used with that node as σ0 ; an Insert action is not permissible if it would insert an AMR concept already in use as any of the parent, children, grand-parents or grand-children of σ0 . Any action that would create a cycle is prohib"
S16-1180,D14-1048,0,0.251934,"ach a terminal state when σ is empty. 1 2.1 The Expert Policy The expert policy used in training applies heuristic rules to determine the next action from a given state. It uses the training alignments to construct a mapping between nodes in the dependency tree, and nodes in the target AMR. Any unmapped nodes in the dependency tree will be deleted by the expert, and any unmapped nodes in the AMR graph will be 1 Code available at https://github.com/ hopshackle/dagger-AMR. SemevalSubmission tag bookmarks the version used. 1168 inserted. All our experiments use node alignments from the system of Pourdamghani et al. (2014). 2.2 Action Space Flanigan et al. (2014) and Wang et al. (2015b), both use AMR fragments as their smallest unit, which may consist of more than one AMR concept. Instead, we always work with the individual AMR nodes, and rely on Insert actions to learn how to build common fragments, such as country names. The main adaptations to the actions, summarised in Table 1, stem from this. NextNode and NextEdge form the core action set, labelling nodes and edges respectively without changing the graph structure. Swap, Reattach and ReplaceHead change this structure, but always retain a tree structure. Re"
S16-1180,P13-1014,0,0.0173106,"ication phase. These options improve performance (by 0.5 to 1.0 points on a validation set) by generalising to unseen tokens in test data, which otherwise would have no mapped AMR concepts. For the lc parameters on Insert (InsertBelow) actions, we use all AMR concepts that the expert inserted above (below) any node in the training set with the same lemma as σ0 . 2.3 Additional action constraints Transition-based parsing algorithms have classically relied on a fixed length of trajectory T for guarantees on performance, or at least a bounded T (Goldberg and Elhadad, 2010; Honnibal et al., 2013; Sartorio et al., 2013; McDonald and Nivre, 2007). In our approach T is theoretically unbounded and the algorithm could Insert, or Reattach ad infinitum. We impose constraints to prevent these situations. A Swap action cannot be applied to a previously Swapped edge; once a node has been moved by Reattach, then it cannot be Reattached again; an Insert action is only permissible if no previous Insert action has been used with that node as σ0 ; an Insert action is not permissible if it would insert an AMR concept already in use as any of the parent, children, grand-parents or grand-children of σ0 . Any action that wou"
S16-1180,Q14-1042,1,0.88644,"the parsing algorithm, and also use the DAGGER imitation learning algorithm (Ross et al., 2011) to generalise better to unseen data. The central idea of DAGGER is that the distribution of states encountered by the expert policy during training may not be a good approximation to those seen in testing by the trained policy. Previous work by Rao et al. (2015) used S EARN, a similar imitation learning algorithm, on the AMR problem, with an algorithm that constructs the AMR graph directly from the sentence tokens. Imitation learning has also been used successfully in other semantic parsing tasks (Vlachos and Clark, 2014; Berant and Liang, 2015). In imitation learning approaches such as DAG the previous actions become features for classification learning. However the partial graphs in AMR parsing are rather complex to represent in this way, and combined with the finite amount of training data different actions can be chosen by the expert even though the feature representations for them can be very similar. These decisions appear as noisy outliers in classification learning. To control noise we experiment with the α-bound discussed by Khardon and Wachman (2007), which excludes a training example from future tr"
S16-1180,P15-2141,0,0.401375,"ints of Fscore. The α-bound improved performance by up to 1.8 points. 1 Introduction In abstract meaning representation parsing (Banarescu et al., 2013), the goal is to parse natural language in a domain-independent graph-based meaning representation (AMR). In the first AMR parsing work, Flanigan et al. (2014) split the task into two sub-tasks; concept identification and graph creation. The sub-tasks are learned independently, and exact inference is used to find highest-scoring maximum spanning connected acyclic graph that contains all the concepts identified in the first stage. Later work by Wang et al. (2015b) adopted a different strategy based on the similarity between the dependency parse of a sentence and the semantic AMR graph. They start from the dependency parse and learn a transition-based parser that converts it into an AMR graph. To learn the parser, Wang et al. (2015b) define an algorithm that for each instance in the training data infers the action sequence that convert the In our submission to SemEval Task 8 on AMR parsing, we follow the transition-based paradigm of Wang et al. (2015b) with modifications to the parsing algorithm, and also use the DAGGER imitation learning algorithm (R"
S16-1180,P15-1095,0,0.246259,"d and tail of each expert-assigned AMR relation, and compile possible lr from these. There is no direct generalisation between different concepts and relations; so ARG0 and ARG0-of are independently learned relations for example, although they represent the same semantic relationship. AMR concepts/relations will never be considered during test if they were not aligned to that lemma in the training data. To relax this restriction we allow lc to take the values WORD, LEMMA, VERB, which respectively use the word, lemma, or the lemma concatenated with ‘-01’ as the AMR concept. This is inspired by Werling et al. (2015), who use a similar set of actions in a concept identification phase. These options improve performance (by 0.5 to 1.0 points on a validation set) by generalising to unseen tokens in test data, which otherwise would have no mapped AMR concepts. For the lc parameters on Insert (InsertBelow) actions, we use all AMR concepts that the expert inserted above (below) any node in the training set with the same lemma as σ0 . 2.3 Additional action constraints Transition-based parsing algorithms have classically relied on a fixed length of trajectory T for guarantees on performance, or at least a bounded"
S17-2096,S16-1180,1,0.938653,"artment of Computer Science Department of Computer Science University of Sheffield, UK University of Sheffield, UK g.lampouras@sheffield.ac.uk a.vlachos@sheffield.ac.uk Abstract cast the problem of ordering these subphrases as a travelling salesman problem. Pourdamghani et al. (2016) suggested linearizing the AMR graph using a maximum entropy classifier. The linearization is then used as input to a phrase-based machine translation system, to produce the final sentence. Our submission to SemEval task 9 on AMR-toEnglish Generation is based on inverting previous work on transition-based parsers (Goodman et al., 2016a,b), which was in turn based on the previous work of Wang et al. (2015). Beyond inverting the transition from AMR graph to dependency tree, our system also separates the transition in three passes. Briefly, during the first pass we convert the AMR concepts into content words, during the second pass the structure of the tree is modified (e.g. by inserting, deleting, and moving nodes and edges), while in the third pass missing function words are inserted, and existing words realized in their final form. To form a natural language sentence, the dependency tree needs only to be linearized; we not"
S17-2096,W13-2322,0,0.0883432,"to a dependency tree, with a focus on content rather than function words. An added benefit to this approach is the greater amount of data we can take advantage of to train the parseto-text linearizer. Our submitted run on the test data achieved a BLEU score of 3.32 and a Trueskill score of -2.204 on automatic and human evaluation respectively. 1 Introduction Abstract meaning representation (AMR) is a formalism representing the meaning of a sentence (or multiple sentences) as a directed, acyclic graph, where each node represents a concept, and each edge represents a relation between concepts (Banarescu et al., 2013). Natural language generation (NLG) from AMRs introduces challenges, as AMR abstracts away from syntactic structure, function words, or inflections. Flanigan et al. (2016) were the first work to perform NLG from AMR; they used a weighted combination of a tree-to-string transducer and a language model to transform the AMR graph into English. Later work by Song et al. (2016) proposed segmenting the AMR graph into fragments and generating subphrases from them, using a set of subgraph-to-string rules. They then 2 2.1 System description Pre-processing During pre-processing the graph structure of th"
S17-2096,P09-1091,0,0.0232684,"s per phase, for transition-based transformation of AMR graphs to parse trees. the label, can properly inflect the word (e.g. modify “make VB” with “-s” to construct “makes”). Stage h of Figure 1 shows the outcome of phase 3. 2.5 2.6 Post-processing In post-processing, the dependency tree constructed by the transition needs to be linearized into a sentence. Tree linearization has most commonly been addressed by overgenerating word sequences and ranking (e.g. according to a trigram language model); however there has been a lot of recent research studying this topic (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2011; Zhang, 2013; Futrell and Gibson, 2015). Our approach in this paper is to simply order the nodes in each subtree using a classifier, in effect creating ordered subphrases of the tree. The subtrees are thus incrementally ordered, in a bottom-up approach, and subsequently formed into a natural language sentence. Any date occurrences or numerical expressions that were normalized during pre-processing, are restored to their original form. It is also important to note again, that the structure of this approach allows it to take Expert policy During training,"
S17-2096,W11-2832,0,0.0147219,"transition-based transformation of AMR graphs to parse trees. the label, can properly inflect the word (e.g. modify “make VB” with “-s” to construct “makes”). Stage h of Figure 1 shows the outcome of phase 3. 2.5 2.6 Post-processing In post-processing, the dependency tree constructed by the transition needs to be linearized into a sentence. Tree linearization has most commonly been addressed by overgenerating word sequences and ranking (e.g. according to a trigram language model); however there has been a lot of recent research studying this topic (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2011; Zhang, 2013; Futrell and Gibson, 2015). Our approach in this paper is to simply order the nodes in each subtree using a classifier, in effect creating ordered subphrases of the tree. The subtrees are thus incrementally ordered, in a bottom-up approach, and subsequently formed into a natural language sentence. Any date occurrences or numerical expressions that were normalized during pre-processing, are restored to their original form. It is also important to note again, that the structure of this approach allows it to take Expert policy During training, an expert policy ("
S17-2096,D15-1162,0,0.0303548,"n in the transition between stage a and b in Figure 1). These duplicate nodes are inserted as leaves in the structure, and maintain no edges to the n’s children. The system randomly determines which of the incoming edges will remain connected with n, and lets the transition system remove duplicate nodes, or move any 586 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 586–591, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics of n’s descendants as required. During training, we employ the SpaCy dependency parser (Honnibal and Johnson, 2015) to construct the dependency tree of the training sentence and obtain part-of-speech tags; the dataset’s sentences are already split into tokens. Heuristics are used to normalize all date occurrences and numeric expressions in both the sentence and dependency tree, to help our system handle temporal and numerical AMR concepts and structures. Additionally, we construct a simplified version of the dependency tree where articles, auxiliary words, and punctuation, are removed. This simplified tree is useful for the first and second phases of the transition where the focus is on content words. 2.2"
S17-2096,W11-2835,0,0.0310127,"ansformation of AMR graphs to parse trees. the label, can properly inflect the word (e.g. modify “make VB” with “-s” to construct “makes”). Stage h of Figure 1 shows the outcome of phase 3. 2.5 2.6 Post-processing In post-processing, the dependency tree constructed by the transition needs to be linearized into a sentence. Tree linearization has most commonly been addressed by overgenerating word sequences and ranking (e.g. according to a trigram language model); however there has been a lot of recent research studying this topic (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2011; Zhang, 2013; Futrell and Gibson, 2015). Our approach in this paper is to simply order the nodes in each subtree using a classifier, in effect creating ordered subphrases of the tree. The subtrees are thus incrementally ordered, in a bottom-up approach, and subsequently formed into a natural language sentence. Any date occurrences or numerical expressions that were normalized during pre-processing, are restored to their original form. It is also important to note again, that the structure of this approach allows it to take Expert policy During training, an expert policy (also known as oracle)"
S17-2096,P02-1040,0,0.0976193,"tehe DARPA DEFT program (LDC2016E25); we hope to augment the linearization’s training with other dependency parse datasets in future work. To provide further speed improvement in testing time, we filter actions (conditioned on specific parameters) that appear infrequently in the training set. Table 2 shows the ablation results of our system on the test set of the task. For Phase 1 we calculate the precision of the labels in the output tree compared to the labels of the dependency parse, while on phases 2 and 3 we calculate the unlabeled and labeled attachment scores. We also include the BLEU (Papineni et al., 2002) and Trueskill (Sakaguchi et al., 2014) scores achieved by our submitted run on the task’s test data. In future work, we would like to examine the effect of error propagation from phase to phase. b) After preprocessing manner tremble-01 one mo d one ity AR G1 G R A lar it fear-01 0 po AR G0 make-02 ARG1 even - c) After Phase 1 trembl VB manner AR G1 one one even AV ity G R A lar it fear NN 0 mo d ARG1 po AR G0 make VB without d) After DeleteLeaf (’fear NN’, ’one’) ARG1 manner mo d one even AV ity it fear NN lar AR G1 trembl VB po AR G0 make VB without e) After Reattach ’even AV’ under ’without"
S17-2096,D14-1048,0,0.0368425,"ction should be performed given a particular state. By consulting the alignments between the concepts of each AMR graph and the words of the corresponding sentence in the training data, the expert policy detects any unaligned AMR concepts to be deleted by appropriate actions, as well as any unaligned words in the dependency tree to be inserted; other actions can be similarly inferred. During phases 1 and 2 we consider the simplified dependency tree, where function words have been removed, and in phase 3 we consider the full tree. The alignments were provided in the dataset using the system of Pourdamghani et al. (2014). 588 advantage of additional parse-tree datasets to augment the training of the post-precessing step. a) Original graph G0 AR 3 ity one lar AR G1 fear-01 po it manner tremble-01 mo d ARG1 AR G0 make-02 even We use the adaptive regularization of weight vectors (AROW) algorithm (Crammer et al., 2013) for all aforementioned classifiers. All the features we use are boolean indicators and similar to those proposed by Goodman et al. (2016a,b) and Wang et al. (2015). All classifiers were trained on the same corpus of AMRs released by LDC, and created as part of tehe DARPA DEFT program (LDC2016E25);"
S17-2096,N09-2057,0,0.0359497,"β. Table 1: Available actions per phase, for transition-based transformation of AMR graphs to parse trees. the label, can properly inflect the word (e.g. modify “make VB” with “-s” to construct “makes”). Stage h of Figure 1 shows the outcome of phase 3. 2.5 2.6 Post-processing In post-processing, the dependency tree constructed by the transition needs to be linearized into a sentence. Tree linearization has most commonly been addressed by overgenerating word sequences and ranking (e.g. according to a trigram language model); however there has been a lot of recent research studying this topic (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2011; Zhang, 2013; Futrell and Gibson, 2015). Our approach in this paper is to simply order the nodes in each subtree using a classifier, in effect creating ordered subphrases of the tree. The subtrees are thus incrementally ordered, in a bottom-up approach, and subsequently formed into a natural language sentence. Any date occurrences or numerical expressions that were normalized during pre-processing, are restored to their original form. It is also important to note again, that the structure of this approach allows it to take Expert policy"
S17-2096,W16-6603,0,0.189562,"Missing"
S17-2096,N16-1087,0,0.150713,"the parseto-text linearizer. Our submitted run on the test data achieved a BLEU score of 3.32 and a Trueskill score of -2.204 on automatic and human evaluation respectively. 1 Introduction Abstract meaning representation (AMR) is a formalism representing the meaning of a sentence (or multiple sentences) as a directed, acyclic graph, where each node represents a concept, and each edge represents a relation between concepts (Banarescu et al., 2013). Natural language generation (NLG) from AMRs introduces challenges, as AMR abstracts away from syntactic structure, function words, or inflections. Flanigan et al. (2016) were the first work to perform NLG from AMR; they used a weighted combination of a tree-to-string transducer and a language model to transform the AMR graph into English. Later work by Song et al. (2016) proposed segmenting the AMR graph into fragments and generating subphrases from them, using a set of subgraph-to-string rules. They then 2 2.1 System description Pre-processing During pre-processing the graph structure of the AMR is converted to a tree by identifying each node n with multiple incoming edges in the graph. Each additional incoming edge is redirected to a duplicate node n0 (as s"
S17-2096,W14-3301,0,0.0446707,"Missing"
S17-2096,D15-1231,0,0.0118455,"e trees. the label, can properly inflect the word (e.g. modify “make VB” with “-s” to construct “makes”). Stage h of Figure 1 shows the outcome of phase 3. 2.5 2.6 Post-processing In post-processing, the dependency tree constructed by the transition needs to be linearized into a sentence. Tree linearization has most commonly been addressed by overgenerating word sequences and ranking (e.g. according to a trigram language model); however there has been a lot of recent research studying this topic (Filippova and Strube, 2009; He et al., 2009; Belz et al., 2011; Bohnet et al., 2011; Zhang, 2013; Futrell and Gibson, 2015). Our approach in this paper is to simply order the nodes in each subtree using a classifier, in effect creating ordered subphrases of the tree. The subtrees are thus incrementally ordered, in a bottom-up approach, and subsequently formed into a natural language sentence. Any date occurrences or numerical expressions that were normalized during pre-processing, are restored to their original form. It is also important to note again, that the structure of this approach allows it to take Expert policy During training, an expert policy (also known as oracle) is constructed to determine which actio"
S17-2096,D16-1224,0,0.0845694,"epresentation (AMR) is a formalism representing the meaning of a sentence (or multiple sentences) as a directed, acyclic graph, where each node represents a concept, and each edge represents a relation between concepts (Banarescu et al., 2013). Natural language generation (NLG) from AMRs introduces challenges, as AMR abstracts away from syntactic structure, function words, or inflections. Flanigan et al. (2016) were the first work to perform NLG from AMR; they used a weighted combination of a tree-to-string transducer and a language model to transform the AMR graph into English. Later work by Song et al. (2016) proposed segmenting the AMR graph into fragments and generating subphrases from them, using a set of subgraph-to-string rules. They then 2 2.1 System description Pre-processing During pre-processing the graph structure of the AMR is converted to a tree by identifying each node n with multiple incoming edges in the graph. Each additional incoming edge is redirected to a duplicate node n0 (as shown in the transition between stage a and b in Figure 1). These duplicate nodes are inserted as leaves in the structure, and maintain no edges to the n’s children. The system randomly determines which of"
S17-2096,P16-1001,1,0.740824,"artment of Computer Science Department of Computer Science University of Sheffield, UK University of Sheffield, UK g.lampouras@sheffield.ac.uk a.vlachos@sheffield.ac.uk Abstract cast the problem of ordering these subphrases as a travelling salesman problem. Pourdamghani et al. (2016) suggested linearizing the AMR graph using a maximum entropy classifier. The linearization is then used as input to a phrase-based machine translation system, to produce the final sentence. Our submission to SemEval task 9 on AMR-toEnglish Generation is based on inverting previous work on transition-based parsers (Goodman et al., 2016a,b), which was in turn based on the previous work of Wang et al. (2015). Beyond inverting the transition from AMR graph to dependency tree, our system also separates the transition in three passes. Briefly, during the first pass we convert the AMR concepts into content words, during the second pass the structure of the tree is modified (e.g. by inserting, deleting, and moving nodes and edges), while in the third pass missing function words are inserted, and existing words realized in their final form. To form a natural language sentence, the dependency tree needs only to be linearized; we not"
W06-2209,W99-0613,0,0.0596281,"nce for a fixed number of checked instances. Following the active learning 66 paradigm, a baseline for active annotation is random selection of instances to be checked. There are though some notable differences. During initialization, an unsupervised method u is required to provide an initial tagging on the data D. This is an important restriction which is imposed by the lack of any annotated data. Even under this restriction, there are some options available, especially for tasks which have compiled resources. One option is to use an unsupervised learning algorithm, such the one presented by Collins & Singer (1999), where a seed set of rules is used to bootstrap a rulebased named entity recognizer. A different approach could be the use of a dictionary-based tagger, as in Morgan et al. (2003). It must be noted that the unsupervised method used to provide the initial tagging does not need to generalize to any data (a common problem for such methods), it only needs to perform well on the data used during active annotation. Generalization on unseen data is an attribute we hope that the supervised learning method s will have after training on the annotated material created with active annotation. The query m"
W06-2209,N04-4028,0,0.0211011,") Intuitively, the margin M is the difference between the two highest scored predictions that disagree. The lower the margin, the higher the uncertainty of the HMM on the token at question. A drawback of this method is that it doesn’t take into account the distribution of the previous label. It is possible that the two highest scored predictions are obtained for two different previous labels. It may also be the case that a highly scored label can be obtained given a very improbable previous label. Finally, an alternative that we did not explore in this work is the Field Confidence Estimation (Culotta and McCallum, 2004), which allows the estimation of confidence over sequences of tokens, instead of singleton tokens only. However, in this work confidence estimation over singleton tokens is sufficient. logP (X = x|Y = y) x (4) In our case, X is l[n] and Y is l[n − 1]. Function 4 can be interpreted as the weighted sum of the entropies of P (l[n]|l[n − 1]) for each value of l[n − 1], in our case the weighted sum of entropies of the distribution of the current label for each possible previous label. The probabilities for each tag (needed for P (l[n − 1])) are not calculated directly from the model. P (l[n]) corre"
W06-2209,E03-1068,0,0.0490425,"Missing"
W06-2209,W03-1301,0,0.0293486,"gh some notable differences. During initialization, an unsupervised method u is required to provide an initial tagging on the data D. This is an important restriction which is imposed by the lack of any annotated data. Even under this restriction, there are some options available, especially for tasks which have compiled resources. One option is to use an unsupervised learning algorithm, such the one presented by Collins & Singer (1999), where a seed set of rules is used to bootstrap a rulebased named entity recognizer. A different approach could be the use of a dictionary-based tagger, as in Morgan et al. (2003). It must be noted that the unsupervised method used to provide the initial tagging does not need to generalize to any data (a common problem for such methods), it only needs to perform well on the data used during active annotation. Generalization on unseen data is an attribute we hope that the supervised learning method s will have after training on the annotated material created with active annotation. The query module q is also different from the corresponding module in active learning. Instead of selecting unlabeled informative instances to be annotated and added to the training data, its"
W06-2209,C02-1101,0,0.0275526,"active annotation. Generalization on unseen data is an attribute we hope that the supervised learning method s will have after training on the annotated material created with active annotation. The query module q is also different from the corresponding module in active learning. Instead of selecting unlabeled informative instances to be annotated and added to the training data, its purpose is to identify likely errors in the imperfectly labelled training data, so that they are checked and corrected by the human annotator. In order to perform error-detection, we chose to adapt the approach of Nakagawa and Matsumoto (2002) which resembles uncertainty based sampling for active learning. According to their paradigm, likely errors in the training data are instances that are “hard” for the classifier and inconsistent with the rest of the data. In our case, we used the uncertainty of the classifier as the measure of the “hardness” of an instance. As an indication of inconsistency, we used the disagreement of the label assigned by the classifier with the current label of the instance. Intuitively, if the classifier disagrees with the label of an instance used in its training, it indicates that there have been other s"
W06-2209,W01-0501,0,0.0188962,"rted artificial errors and trained a classifier to recognize them. Dickinson and Meuers (2003) proposed methods based on n-grams occurring with different labellings in the corpus. Therefore, while it is reasonable to expect some correlation between the selections of active annotation and active learning (hard instances are likely to be erroneously annotated by the unsupervised tagger), the task of selecting hard instances is quite different from detecting errors. The use of the disagreement between taggers for selecting candidates for manual correction is reminiscent of corrected co-training (Pierce and Cardie, 2001). However, the main difference is corrected co-training results in a manually annotated corpus, while active annotation allows automatically annotated instances to be kept. 5 HMM uncertainty estimation In order to perform error detection according to the previous section we need to obtain uncertainty estimations over each token from the named entity recognition module of Lingpipe. For each token t and possible label l, Lingpipe estimates the following Hidden Markov Model from the training data: P (t[n], l[n]|l[n − 1], t[n − 1], t[n − 2]) (1) When annotating a certain text passage, the tokens a"
W06-2209,P04-1075,0,0.0727945,"Missing"
W06-2209,W04-3202,0,0.0460118,"illiam Gates Building Computer Laboratory University of Cambridge av308@cl.cam.ac.uk Abstract attracted the attention of the NLP community relatively recently (Kim et al., 2004). Even though there are plenty of biomedical texts, very little of it is annotated, such as the GENIA corpus (Kim et al., 2003). A very popular and well investigated framework in order to cope with the lack of training material is the active learning framework (Cohn et al., 1995; Seung et al., 1992). It has been applied to various NLP/IE tasks, including named entity recognition (Shen et al., 2004) and parse selection (Baldridge and Osborne, 2004) with rather impressive results in reducing the amount of annotated training data. However, some criticism of active learning has been expressed recently, concerning the reusability of the data (Baldridge and Osborne, 2004). This paper presents a framework in order to deal with the lack of training data for NLP tasks. The intuition behind it is that annotated training data is produced by applying an (imperfect) unsupervised method, and then the errors inserted in the annotation are detected automatically and reannotated by a human annotator. The main difference compared to active learning is t"
W06-3328,briscoe-carroll-2002-robust,0,0.0266075,"protein</om&gt; However, in many cases the noun phrase itself is not sufficient to classify the mention, especially when the mention consists of just the gene name, because it is quite common in the biomedical literature to use a gene name to refer to a protein or to other gene products. In order to classify such cases, the annotators need to take into account the context in which the mention appears. In the following examples, the word of the context that enables us to make 139 The test set produced consists of the abstracts from 82 articles curated by FlyBase1 . We used the tokenizer of RASP2 (Briscoe and Carroll, 2002) to process the text, resulting in 15703 tokens. The size and the characteristics of the dataset is comparable with that of Morgan et al (2004) as it can be observed from the statistics of Table 1, except for the number of non-unique gene-names. Apart from the different guidelines, another difference is that we used the original text of the abstracts, without any postprocessing apart from the tokenization. The dataset from Morgan et al. (2004) had been stripped from all punctuation characters, e.g. periods and commas. Keeping the text intact renders this new dataset more realistic and most imp"
W06-3328,J96-2004,0,0.14703,"Missing"
W06-3328,W99-0613,0,0.0546049,"2003) domain and more recently the biomedical domain (Blaschke et al., 2004; Kim et al., 2004). These shared tasks aimed at evaluating fully supervised trainable systems. However, the limited availability of annotated material in most domains, including the biomedical, restricts the application of such methods. In order to circumvent this obstacle several approaches have been presented, among them active learning (Shen et al., 2004) and rule-based systems encoding domain specific knowledge (Gaizauskas et al., 2003). In this work we build on the idea of bootstrapping, which has been applied by Collins & Singer (1999) in the newsire domain and by Morgan et al. (2004) in the biomedical domain. This approach is based on creating training material automatically using existing domain resources, which in turn is used to train a supervised named entity recognizer. The structure of this paper is the following. Section 2 describes the construction of a new test set to evaluate named entity recognition for Drosophila fly genes. Section 3 compares bootstrapping to the use of manually annotated material for training a supervised method. An extension to the evaluation of NER appear in Section 4. Based on this evaluati"
W06-3328,E03-1071,0,0.012882,"ow it to generalize to unseen named entities. Important role in this aspect of the performance play the features that are dependent on the context and on observations on the tokens. The ability to generalize to unseen named entities is very significant because it is unlikely that training material can cover all possible names and moreover, in most domains, new names appear regularly. A common way to assess these two aspects is to measure the performance on seen and unseen data separately. It is straightforward to apply this in tasks with token-based evaluation, such as part-of-speech tagging (Curran and Clark, 2003). However, in the case of NER, this is not entirely appropriate due to the existence of multi-token entities. For example, consider the case of the gene-name “head inhibition defective”, which consists of three common words that are very likely to occur independently of each other in a training set. If this gene name appears in the test set but not in the training set, with 141 a token-based evaluation its identification (or not) would count towards the performance on seen tokens if the tokens appeared independently. Moreover, a system would be rewarded or penalized for each of the tokens. One"
W06-3328,P04-1075,0,0.161015,"Missing"
W06-3328,W03-0419,0,\N,Missing
W07-1031,P06-4020,0,0.227209,"and previous two tokens xt−1 and xt−2 : P (xt , yt |yt−1 , xt−1 , xt−2 ) (1) Tokens unseen in the training data are passed to a morphological rule-based classifier which assigns them to predefined classes according to their capitalization and whether they contain digits or punctuation. In order to use these classes along with the ordinary tokens, during training a second pass over the training data is performed in which tokens that appear fewer times than a given threshold are replaced by their respective classes. In our experiments, this threshold was set experimentally to 8. Vlachos et al. (2006) employed this system and achieved good results on bootstrapping biomedical named entity recognition. They also note though that due to its reliance on seen tokens and the restricted way in which unseen tokens are handled its performance is not as good on unseen data. 1 http://www.alias-i.com/lingpipe. The version used in the experiments was 2.1. 200 2.2 Conditional Random Fields with Syntactic Parsing The second NER system we used in our experiments was the system of Vlachos (2007) that participated in the BioCreative2 Gene Mention task (Krallinger and Hirschman, 2007). Its main components ar"
W07-1031,W03-0425,0,0.0844054,"Missing"
W07-1031,W06-3316,0,0.259833,"lved some flavour of NER using manually annotated training material and fully supervised machine learning methods. In parallel, there have been successful efforts in bootstrapping NER systems using automatically generated training material using domain resources (Morgan et al., 2004; Vlachos et al., 2006). These approaches have a significant appeal, since they don’t require manual annotation of training material which is an expensive and lengthy process. Named entity recognition is an important task because it is a prerequisite to other more complex ones. Examples include anaphora resolution (Gasperin, 2006) and gene normalization (Hirschman et al., 2005). An important point is that until now NER systems have been evaluated on abstracts, or on sentences selected from abstracts. However, NER systems will be applied to full papers, either on their own or in order to support more complex tasks. Full papers though are expected to present additional challenges to the systems than the abstracts, so it is important to evaluate on the former as well in order to obtain a clearer picture of the systems and the task (Ananiadou and McNaught, 2006). In this paper, we compare two NER systems in a variety of se"
W07-1031,W04-1221,0,0.103632,"Missing"
W07-1031,P04-1075,0,0.0294567,"Missing"
W07-1031,N06-2038,0,0.0125196,"therefore feature extraction on its output is likely to introduce some noise. The RASP syntactic parser is domain independent but 201 it has been developed using data from general English corpora mainly, so it is likely not to perform as well in the biomedical domain. Nevertheless, the results of the system in the BioCreative2 Gene Mention task suggest that the use of syntactic parsing features improve performance. Also, despite the lack of domain-specific features, the system is competitive with other systems, having performance in the second quartile of the task. Finally, the BIOEW scheme (Siefkes, 2006) was used to tag the tokenized corpora, under which the first token of a multitoken mention is tagged as B, the last token as E, the inner ones as I, single token mentions as W and tokens outside an entity as O. 3 Corpora In our experiments we used two corpora consisting of abstracts and one consisting of full papers. One of the abstracts corpora was automatically generated while the other two were manually annotated. All three were created using resources from FlyBase4 and they are publicly available5 . The automatically generated corpus was created in order to bootstrap a gene name recognize"
W07-1031,W06-3328,1,0.918595,"used to tag the tokenized corpora, under which the first token of a multitoken mention is tagged as B, the last token as E, the inner ones as I, single token mentions as W and tokens outside an entity as O. 3 Corpora In our experiments we used two corpora consisting of abstracts and one consisting of full papers. One of the abstracts corpora was automatically generated while the other two were manually annotated. All three were created using resources from FlyBase4 and they are publicly available5 . The automatically generated corpus was created in order to bootstrap a gene name recognizer in Vlachos & Gasperin (2006). The approach used was introduced by Morgan et al (2004). In brief, the abstracts of 16,609 articles curated by FlyBase were retrieved and tokenized by RASP (Briscoe et al., 2006). For each article, the gene names and their synonyms that were recorded by the curators were annotated automatically in its abstract using longestextent pattern matching. The pattern matching is flexible in order to accommodate capitalization and punctuation variations. This process resulted in a large but noisy dataset, consisting of 2,923,199 tokens and containing 117,279 gene names, 16,944 of which are unique. Th"
W07-1031,E99-1001,0,0.0486901,"the CRF. The good performance of bootstrapping gene name recognizers using automatically created training data suggests that it is a realistic alternative to fully supervised systems. The latter have benefited from a series of shared tasks that, by providing a testbed for evaluation, helped assessing and improving their performance. Given the variety of methods that are available for generating training data efficiently automatically using extant domain resources (Morgan et al., 2004) or semi-automatically (active learning approaches like Shen et al. (2004) or systems using seed rules such as Mikheev et al. (1999)), it would be of interest to have a shared task in which the participants would have access to evaluation data only and they would be invited to use such methods to develop their systems. The aim of this paper is not about deciding on which of the two models is better but about how the datasets used affect the evaluation and how to combine the strengths of the models based on the analysis performed. In this spirit, we didn’t attempt any of the improvements discussed by Vlachos & Gasperin (2006) because they were based on observations on the behavior of the HMM-based system. From the analysis"
W07-1031,W03-0419,0,\N,Missing
W09-0210,W02-1016,0,0.352058,"Missing"
W09-0210,briscoe-carroll-2002-robust,0,0.143976,"Missing"
W09-0210,D07-1043,0,0.692738,"umber of target clusters in advance, renders them promising for the many NLP tasks where clustering is used for learning purposes. While the results of Vlachos et al. (2008) are promising, the use of a clustering approach which discovers the number of clusters in data presents a new challenge to existing evaluation measures. In this work, we investigate optimal evaluation for such approaches, using the dataset and the basic method of Vlachos et al. as a starting point. We review the applicability of existing evaluation measures and propose a modified version of the newly introduced V-measure (Rosenberg and Hirschberg, 2007). We complement the quantitative evaluation with thorough qualitative assessment, for which we introduce a method to summarize samples obtained from a clustering algorithm. In preliminary work by Vlachos et al. (2008), a constrained version of DPMMs which takes advantage of must-link and cannot-link pairwise constraints was introduced. It was demonstrated how such constraines can guide the clustering solution towards some prior intuition or considerations relevant to the specific NLP application in mind. We explain the inference algorithm for the constrained DPMM in greater detail and evaluate"
W09-0210,P07-1107,0,0.031621,"Department of Engineering University of Cambridge Cambridge CB2 1PZ, UK zoubin@eng.cam.ac.uk Introduction Bayesian non-parametric models have received a lot of attention in the machine learning community. These models have the attractive property that the number of components used to model the data is not fixed in advance but is actually determined by the model and the data. This property is particularly interesting for NLP where many tasks are aimed at discovering novel, previously unknown information in corpora. Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. Recently, Vlachos et al. (2008) applied the basic models of this class, Dirichlet Process Mixture Models (DPMMs) (Neal, 2000), to a typical learning task in NLP: lexical-semantic verb clustering. The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.). Such classes can provide important support for other NLP tasks, such as word sense disambiguation, parsing and semantic role labeling (Dang, 2004; Swier and St"
W09-0210,P06-1124,0,0.0600125,"ac.uk Introduction Bayesian non-parametric models have received a lot of attention in the machine learning community. These models have the attractive property that the number of components used to model the data is not fixed in advance but is actually determined by the model and the data. This property is particularly interesting for NLP where many tasks are aimed at discovering novel, previously unknown information in corpora. Recent work has applied Bayesian non-parametric models to anaphora resolution (Haghighi and Klein, 2007), lexical acquisition (Goldwater, 2007) and language modeling (Teh, 2006) with good results. Recently, Vlachos et al. (2008) applied the basic models of this class, Dirichlet Process Mixture Models (DPMMs) (Neal, 2000), to a typical learning task in NLP: lexical-semantic verb clustering. The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.). Such classes can provide important support for other NLP tasks, such as word sense disambiguation, parsing and semantic role labeling (Dang, 2004; Swier and Stevenson, 2004). Proceedings of the EACL 2009 Workshop on GEMS: GEometical"
W09-0210,korhonen-etal-2006-large,1,0.879281,"ot discovered like by the DPMM. Secondly, PC uses the similarities between the instances to perform the clustering, while the DPMM attempts to find the parameters of the process that generated the data, which is a different and typically a harder task. In addition, the DPMM has two clear advantages which we illustrate in the following sections: it can be used to discover novel information and it can be modified to incorporate intuitive human supervision. Table 1: Clustering performances. syntactic context in which the verb occurs. SCFs were extracted from the publicly available VALEX lexicon (Korhonen et al., 2006a). VALEX was acquired automatically using a domain-independent statistical parsing toolkit, RASP (Briscoe and Carroll, 2002), and a classifier which identifies verbal SCFs. As a consequence, it includes some noise due to standard text processing and parsing errors and due to the subtlety of argument-adjunct distinction. In our experiments, we used the SCFs obtained from VALEX1, parameterized for the prepositional frame, which had the best performance in the experiments of Sun et al. (2008). The feature sets based on verbal SCFs are very sparse and the counts vary over a large range of values."
W09-0210,P06-1044,1,0.889629,"ot discovered like by the DPMM. Secondly, PC uses the similarities between the instances to perform the clustering, while the DPMM attempts to find the parameters of the process that generated the data, which is a different and typically a harder task. In addition, the DPMM has two clear advantages which we illustrate in the following sections: it can be used to discover novel information and it can be modified to incorporate intuitive human supervision. Table 1: Clustering performances. syntactic context in which the verb occurs. SCFs were extracted from the publicly available VALEX lexicon (Korhonen et al., 2006a). VALEX was acquired automatically using a domain-independent statistical parsing toolkit, RASP (Briscoe and Carroll, 2002), and a classifier which identifies verbal SCFs. As a consequence, it includes some noise due to standard text processing and parsing errors and due to the subtlety of argument-adjunct distinction. In our experiments, we used the SCFs obtained from VALEX1, parameterized for the prepositional frame, which had the best performance in the experiments of Sun et al. (2008). The feature sets based on verbal SCFs are very sparse and the counts vary over a large range of values."
W09-1405,P06-4020,1,0.857071,"inally, the triggers connected with appropriate arguments are postprocessed to generate the final set of events. Each of these stages are described in detail in subsequent sections, followed by experiments and discussion. 2 Trigger identification We perform trigger identification using the assumption that events are triggered in text either by verbal or nominal prdicates (Cohen et al., 2008). To build a dictionary of verbs and their associated event classes we use the triggers annotated in the training data. We lemmatize and stem the triggers with the morphology component of the RASP toolkit (Briscoe et al., 2006)1 and the Porter stemmer2 respectively. We sort the trigger stem - event class pairs found according to their frequency in the training data and we keep only those pairs that appear at least 10 times. The trigger stems are then mapped to verbs. This excludes some relatively common triggers, which will reduce recall, but, given that we rely exclusively on the parser for 1 2 http://www.cogs.susx.ac.uk/lab/nlp/rasp/ http://www.tartarus.org/˜martin/PorterStemmer Proceedings of the Workshop on BioNLP: Shared Task, pages 37–40, c Boulder, Colorado, June 2009. 2009 Association for Computational Lingu"
W09-1405,W05-0623,0,0.0684239,"ndependent unlexicalized RASP parser, which generates parses over the part-of-speech (PoS) tags of the tokens generated by an HMM-based tagger trained on balanced English text. While we expect that a parser adapted to the biomedical domain may perform better, we want to preserve the domain-independence of the system and explore its potential. The only adjustment we make is to change the PoS tags of tokens that are part of a protein name to proper names tags. We consider such an adjustment domain-independent given that NER is available in many domains (Lewin, 2007). Following 38 Haghighi et al (2005), in order to ameliorate parsing errors, we use the top-10 parses and return a set of bilexical head-dependent grammatical relations (GRs) weighted according to the proportion and probability of the top parses supporting that GR. The GRs produced by the parser define directed graphs between tokens in the sentence, and a partial event is formed when a path that connects a trigger with an appropriate argument is identified. GR paths that are likely to generate events are selected using the development data, which does not contradict the goals of our approach because we do not require annotated t"
W09-1405,C08-1057,0,\N,Missing
W09-1405,W07-1022,0,\N,Missing
W10-1901,P08-1006,0,0.0268078,"ch can be either a protein or another event. Every event has a trigger which is a contiguous textual string that can span over one or more tokens, as well as a part of a token. Triggers and arguments can be shared across events and 1 Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 1–9, c Uppsala, Sweden, 15 July 2010. 2010 Association for Computational Linguistics ID E1 E2 E3 type Neg reg Pos reg Gene exp trigger suppressed induced production Theme E2 E3 IL-10 Cause • The GDep dependency parser trained for the biomedical domain in the experiments of Miyao et al. (2008). This parser was trained for the biomedical domain using the GENIA treebank. gp41 Table 1: Shared task example annotation. The native Penn TreeBank output of Bikel’s and McClosky’s parser was converted to the Stanford Dependency (SD) collapsed dependency format (de Marneffe and Manning, 2008). The output of the CCG parser was also converted to the same dependency format, while the output of GDep was provided in a different dependency format used for the dependency parsing CoNLL 2007 shared task. From the description above, it is clear that the various parsers have different levels of adaptati"
W10-1901,W09-1402,0,0.0930342,"Missing"
W10-1901,N10-1123,0,0.296727,"Missing"
W10-1901,P06-4020,0,0.0865676,"Missing"
W10-1901,W09-1403,0,0.0212675,"he events and evaluate the dependencies between them directly. Furthermore, given the importance of syntactic parsing via syntactic dependencies to event extraction, it would be interesting to see how performing these tasks jointly would help improve the performance. A dependencybased representation would also allow for noncontiguous event components, as well as more complex phenomena such as the light triggers discussed earlier. Discussion Our error analysis on the output of the best system on the development data discouraged us from pursuing further improvements. Echoing the observations of Buyko et al. (2009), we found that annotation inconsistency was affecting our results significantly. In many cases the event triggers annotated in the development data were rather misleading, e.g. “negative” as a Gene expression event trigger (abstract 8622883), “increase the stability” as a Positive regulation event trigger (abstract 8626752), “disappearance” as a Binding event trigger (abstract 10455128). Finally, some events were ignored by the annotation, such as “regulation of thymidine kinase” (abstract 8622883). An additional complication is that events that are annotated due to anaphoric linking can have"
W10-1901,W09-1406,0,0.0994159,"Missing"
W10-1901,W08-1301,0,0.050711,"Missing"
W10-1901,W06-3312,0,0.0627464,"Missing"
W10-1901,W09-1418,0,0.0700828,"Rule-based systems need annotated data for tuning, but unlike their supervised machine learning-based counterparts they do not learn parameters from it, thus requiring less annotated data. We consider this to be the main advantage of rule-based systems and to demonstrate this point we explicitly avoid using the training data provided. The rules define syntactic dependency paths that connect tokens containing triggers (trigger-tokens) with tokens containing their arguments (arg-tokens). For multitoken protein names, it is sufficient that a path reaches any of its tokens. For Regulation event 1 Kilicoglu and Bergler (2009) made similar observations on the lemma “activity” without formalizing them. 3 class triggers we consider as arg-tokens not only tokens containing (parts of) protein names but also the trigger-tokens found in the same sentence. The rules defined are the following: ment marked as Theme. This approach is expected to deal adequately with all event types except for Binding, which can have multiple themes. We generate Regulation events for trigger-argument pairs whose argument is a protein name or a trigger that has an already formed event. Since Regulation events can have other Regulation events a"
W10-1901,W08-2121,0,0.0613239,"Missing"
W10-1901,W09-1401,0,0.288943,"Missing"
W10-1901,W09-1405,1,0.861497,"Missing"
W10-1901,J04-4004,0,\N,Missing
W10-1901,P08-2026,0,\N,Missing
W10-2809,briscoe-carroll-2002-robust,1,0.73689,"formative link as that on which the model is most uncertain, more ∗ that maxiformally the link between instances lij mizes the following entropy: ∗ lij = arg max H(zi = zj ) i,j 4 Datasets and Evaluation In our experiments we used two verb clustering datasets, one from general English (Sun et al., 2008) and one from the biomedical domain (Korhonen et al., 2006). In both datasets the features for each verb are its subcategorization frames (SCFs) which capture the syntactic context in which it occurs. They were acquired automatically using a domain-independent statistical parsing toolkit, RASP (Briscoe and Carroll, 2002), and a classifier which identifies verbal SCFs. As a consequence, they include some noise due to standard text processing and parsing errors and due to the subtlety of the argument-adjunct distinction. The general English dataset contains 204 verbs (2) If we consider clustering as binary classification of links into must-links and cannot-links, it is equivalent to selecting the pair with the highest label entropy. During the sampling process used for parameter inference, component assignments vary 58 sampling. The performances were averaged across the collected samples. Random selection was r"
W10-2809,N09-1062,0,0.0294167,"n employing uncertaintybased sampling. We achieve substantial improvements over random selection on two datasets. 1 Ted Briscoe Computer Laboratory University of Cambridge ejb@cl.cam.ac.uk Introduction Bayesian non-parametric mixture models have the attractive property that the number of components used to model the data is not fixed in advance but is determined by the model and the data. This property is particularly interesting for NLP where many tasks are aimed at discovering novel information. Recent work has applied such models to various tasks with promising results, e.g. Teh (2006) and Cohn et al. (2009). Vlachos et al. (2009) applied the basic model of this class, the Dirichlet Process Mixture Model (DPMM), to lexical-semantic verb clustering with encouraging results. The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.). Such classes can provide important support for other tasks, such as word sense disambiguation, parsing and semantic role labeling. (Dang, 2004; Swier and Stevenson, 2004) Although some fixed classifications are available these are not comprehensive and are inadequate for specifi"
W10-2809,P06-1124,0,0.0421635,"traint selection employing uncertaintybased sampling. We achieve substantial improvements over random selection on two datasets. 1 Ted Briscoe Computer Laboratory University of Cambridge ejb@cl.cam.ac.uk Introduction Bayesian non-parametric mixture models have the attractive property that the number of components used to model the data is not fixed in advance but is determined by the model and the data. This property is particularly interesting for NLP where many tasks are aimed at discovering novel information. Recent work has applied such models to various tasks with promising results, e.g. Teh (2006) and Cohn et al. (2009). Vlachos et al. (2009) applied the basic model of this class, the Dirichlet Process Mixture Model (DPMM), to lexical-semantic verb clustering with encouraging results. The task involves discovering classes of verbs similar in terms of their syntactic-semantic properties (e.g. MOTION class for travel, walk, run, etc.). Such classes can provide important support for other tasks, such as word sense disambiguation, parsing and semantic role labeling. (Dang, 2004; Swier and Stevenson, 2004) Although some fixed classifications are available these are not comprehensive and are"
W10-2809,W09-0210,1,0.674898,"Missing"
W10-2809,P06-1044,0,0.0158003,"d by a human expert. In the context of the DPMMs, the model chooses a pair of instances for which a must-link or a cannot-link must be provided. To select the pair, we employ the simple but effective idea of uncertainty based sampling. We consider the most informative link as that on which the model is most uncertain, more ∗ that maxiformally the link between instances lij mizes the following entropy: ∗ lij = arg max H(zi = zj ) i,j 4 Datasets and Evaluation In our experiments we used two verb clustering datasets, one from general English (Sun et al., 2008) and one from the biomedical domain (Korhonen et al., 2006). In both datasets the features for each verb are its subcategorization frames (SCFs) which capture the syntactic context in which it occurs. They were acquired automatically using a domain-independent statistical parsing toolkit, RASP (Briscoe and Carroll, 2002), and a classifier which identifies verbal SCFs. As a consequence, they include some noise due to standard text processing and parsing errors and due to the subtlety of the argument-adjunct distinction. The general English dataset contains 204 verbs (2) If we consider clustering as binary classification of links into must-links and can"
W10-2809,D07-1043,0,0.0161767,"om 3 biomedical journals. A team of linguists and biologists created a three-level gold standard with 16, 34 and 50 classes. Both datasets were pre-processed using non-negative matrix factorization (Lin, 2007) which decomposes a large sparse matrix into two dense matrices (of lower dimensionality) with non-negative values. In all experiments 35 dimensions were kept. Preliminary experiments with different number of dimensions kept did not affect the performance substantially. We evaluate our results using three information theoretic measures: Variation of Information (Meil˘a, 2007), V-measure (Rosenberg and Hirschberg, 2007) and V-beta (Vlachos et al., 2009). All three assess the two desirable properties that a clustering should have with respect to a gold standard, homogeneity and completeness. Homogeneity reflects the degree to which each cluster contains instances from a single class and is defined as the conditional entropy of the class distribution of the gold standard given the clustering. Completeness reflects the degree to which each class is contained in a single cluster and is defined as the conditional entropy of clustering given the class distribution in the gold standard. V-beta balances these proper"
W10-3003,W08-1301,0,0.0312856,"Missing"
W10-3003,W10-3001,0,0.0843736,"Missing"
W10-3003,W09-1401,0,0.0202199,"esting aspect of this three-stage parsing approach is that, if the parse selection module fails to construct a parse tree for the sentence (a common issue when syntactic parsers are ported to new domains), the lexical categories obtained by the supertagger preserve some of the syntactic information that would not be found in PoS tags. The adaptation to the biomedical domain by Rimell and Clark (2009) involved re-training the PoS tagger and the CCG supertagger using indomain resources, while the parse selection component was left intact. As recent work in the BioNLP 2009 shared task has shown (Kim et al., 2009), domain-adapted parsing benefits information extraction systems. The native output of the C&C parser is converted into the Stanford Dependency (SD) collapsed dependency format (de Marneffe and Manning, 2008). These dependencies define binary relations between tokens and the labels of these relations are obtained from a hierarchy. While the conversion is unlikely to be perfect given that the native C&C output follows a different formalism, we made this choice because it allows for the use of different parsers with minimal adaptation. Finally, an important pre-processing step we take is tokeniz"
W10-3003,W04-3103,0,0.348959,"Missing"
W10-3003,P07-1125,0,0.0958695,"Missing"
W10-3003,W08-0607,0,\N,Missing
W11-0307,W09-1402,0,0.150558,"re pipelines that decompose event extraction into a set of simpler classification tasks. Classifiers for these tasks are typically learned independently, thereby ignoring event structure during training. Typically in such systems, the relationships among these tasks are taken into account by incorporating post-processing rules that enforce certain constraints when combining their predictions, and by tuning classification thresholds to improve the accuracy of joint predictions. Pipelines are appealing as they are relatively easy to implement and they often achieve state-of-the-art performance (Bjorne et al., 2009; Miwa et al., 2010). Because of the nature of the output space, the task is not amenable to sequential or grammar-based approaches (e.g. linear CRFs, HMMs, PCFGs) which employ dynamic programming in order to do efficient inference. The only joint inference framework that has been applied to BioNLP09ST to date is Markov Logic Networks (MLNs) (Riedel et al., 2009; Poon and Vanderwende, 2010). However, MLNs require task-dependent approximate inference and substantial computational resources in order to achieve state-of-the-art performance. In this work we explore an alternative joint inference a"
W11-0307,J02-3001,0,0.00603025,"gure 1 describes the event extraction decomposition that we use throughout the paper. We assume that the sentences to be processed are parsed into syntactic dependencies and lemmatized. Each stage has its own module, which is either a learned classifier (trigger recognition, Theme/Cause assignment) or a rule-based component (event construction). 3.1 BioNLP09ST focused on the extraction of events involving proteins whose names are annotated in advance. Each event has two types of arguments, Theme and Cause, which correspond respectively to the Agent and Patient roles in semantic role labeling (Gildea and Jurafsky, 2002). Nine event types are defined which can be broadly classified in three categories, namely Simple, Binding and Regulation. Simple events include Gene expression, Transcription, Protein catabolism, Phosphorylation, and Localization events. These have only one Theme argument which is a protein. Binding events have one or more protein Themes. Finally, Regulation events, which include Positive regulation, Negative regulation and Regulation, have one obligatory Theme and one optional Cause, each of which can be either a protein or another event. Each event has a trigger which is a contiguous string"
W11-0307,W09-1401,0,0.046862,"imple yet strong pipeline by 8.6 points in F-score on the BioNLP 2009 shared task, while achieving the best reported performance by a joint inference method. Additionally, we consider the issue of cost estimation during learning and present an approach called focused costing that improves improves efficiency and predictive accuracy. 1 Introduction The term biomedical event extraction is used to refer to the task of extracting descriptions of actions and relations involving one or more entities from the biomedical literature. The recent BioNLP 2009 shared task (BioNLP09ST) on event extraction (Kim et al., 2009) focused on event types of varying complexity. Each event consists of a trigger and one or more arguments, the latter being proteins or other events. Any token in a sentence can be a trigger for one of the nine event types and, depending on their associated event types, triggers are assigned appropriate arguments. Thus, the task can be viewed as a structured prediction problem in which the output for a given instance is a (possibly disconnected) directed acyclic graph (not necessarily a tree) in which vertices correspond to triggers or protein arguments, and edges represent relations between t"
W11-0307,P08-2026,0,0.058244,"Missing"
W11-0307,P11-1163,0,0.0294829,"Missing"
W11-0307,N10-1123,0,0.374168,"edictions, and by tuning classification thresholds to improve the accuracy of joint predictions. Pipelines are appealing as they are relatively easy to implement and they often achieve state-of-the-art performance (Bjorne et al., 2009; Miwa et al., 2010). Because of the nature of the output space, the task is not amenable to sequential or grammar-based approaches (e.g. linear CRFs, HMMs, PCFGs) which employ dynamic programming in order to do efficient inference. The only joint inference framework that has been applied to BioNLP09ST to date is Markov Logic Networks (MLNs) (Riedel et al., 2009; Poon and Vanderwende, 2010). However, MLNs require task-dependent approximate inference and substantial computational resources in order to achieve state-of-the-art performance. In this work we explore an alternative joint inference approach to biomedical event extraction using a search-based structured prediction framework, SEARN (Daum´e III et al., 2009). SEARN is an algorithm that converts the problem of learning a model for structured prediction into learning a set of models for cost-sensitive classification (CSC). CSC is a task in which each training instance has a vector of misclassification costs associated with"
W11-0307,W09-1406,0,0.0564269,"en combining their predictions, and by tuning classification thresholds to improve the accuracy of joint predictions. Pipelines are appealing as they are relatively easy to implement and they often achieve state-of-the-art performance (Bjorne et al., 2009; Miwa et al., 2010). Because of the nature of the output space, the task is not amenable to sequential or grammar-based approaches (e.g. linear CRFs, HMMs, PCFGs) which employ dynamic programming in order to do efficient inference. The only joint inference framework that has been applied to BioNLP09ST to date is Markov Logic Networks (MLNs) (Riedel et al., 2009; Poon and Vanderwende, 2010). However, MLNs require task-dependent approximate inference and substantial computational resources in order to achieve state-of-the-art performance. In this work we explore an alternative joint inference approach to biomedical event extraction using a search-based structured prediction framework, SEARN (Daum´e III et al., 2009). SEARN is an algorithm that converts the problem of learning a model for structured prediction into learning a set of models for cost-sensitive classification (CSC). CSC is a task in which each training instance has a vector of misclassifi"
W11-0307,C10-1088,0,\N,Missing
W11-1805,P05-1022,0,0.0259552,"Missing"
W11-1805,P07-1033,0,0.0796471,"Missing"
W11-1805,W09-1401,0,0.117359,"ter able to handle the domain shift from abstracts to full papers. In addition, we report on experiments using a simple domain adaptation method. 1 Introduction The term biomedical event extraction is used to refer to the task of extracting descriptions of actions and relations among one or more entities from the biomedical literature. The BioNLP 2011 shared task GENIA Task1 (BioNLP11ST-GE1) (Kim et al., 2011) focuses on extracting events from abstracts and full papers. The inclusion of full papers in the datasets is the only difference from Task1 of the BioNLP 2009 shared task (BioNLP09ST1) (Kim et al., 2009), which used the same task definition and abstracts dataset. Each event consists of a trigger and one or more arguments, the latter being proteins or other events. The protein names are annotated in advance and any token in a sentence can be a trigger for one of the nine event types. In an example demonstrating the complexity of the task, given the passage “. . . SQ 22536 suppressed gp41-induced IL-10 production in monocytes”, systems should extract the three nested events shown in Fig. 1d. In our submission, we use the event extraction system of Vlachos and Craven (2011) which employs the sea"
W11-1805,W11-1802,0,0.157882,"abstracts and full papers. We employ a joint inference system developed using the search-based structured prediction framework and show that it improves on a pipeline using the same features and it is better able to handle the domain shift from abstracts to full papers. In addition, we report on experiments using a simple domain adaptation method. 1 Introduction The term biomedical event extraction is used to refer to the task of extracting descriptions of actions and relations among one or more entities from the biomedical literature. The BioNLP 2011 shared task GENIA Task1 (BioNLP11ST-GE1) (Kim et al., 2011) focuses on extracting events from abstracts and full papers. The inclusion of full papers in the datasets is the only difference from Task1 of the BioNLP 2009 shared task (BioNLP09ST1) (Kim et al., 2009), which used the same task definition and abstracts dataset. Each event consists of a trigger and one or more arguments, the latter being proteins or other events. The protein names are annotated in advance and any token in a sentence can be a trigger for one of the nine event types. In an example demonstrating the complexity of the task, given the passage “. . . SQ 22536 suppressed gp41-induc"
W11-1805,N10-1004,0,0.0342463,"Missing"
W11-1805,N10-1123,0,0.0154583,"ts shown in Fig. 1d. In our submission, we use the event extraction system of Vlachos and Craven (2011) which employs the search-based structured prediction framework (SEARN) (Daum´e III et al., 2009). SEARN converts the problem of learning a model for structured prediction into learning a set of models for cost-sensitive classification (CSC). In CSC, each training instance has a vector of misclassification costs associated with it, thus rendering some mistakes in some instances to be more expensive than others. Compared to other structured prediction frameworks such as Markov Logic Networks (Poon and Vanderwende, 2010), SEARN provides high modeling flexibility but it does not requiring taskdependent approximate inference. In this work, we show that SEARN is more accurate than a pipeline using the same features and it is better able to handle the domain shift from abstracts to full papers. Furthermore, we report on experiments with the simple domain adaptation method proposed by Daum´e III (2007), which creates a version of each feature for each domain. While the results were mixed, this method improves our performance on full papers of the test set, for which little training data is available. 2 Event extra"
W11-1805,W11-1816,0,0.176917,"Missing"
W11-1805,W11-0307,1,0.743573,"09 shared task (BioNLP09ST1) (Kim et al., 2009), which used the same task definition and abstracts dataset. Each event consists of a trigger and one or more arguments, the latter being proteins or other events. The protein names are annotated in advance and any token in a sentence can be a trigger for one of the nine event types. In an example demonstrating the complexity of the task, given the passage “. . . SQ 22536 suppressed gp41-induced IL-10 production in monocytes”, systems should extract the three nested events shown in Fig. 1d. In our submission, we use the event extraction system of Vlachos and Craven (2011) which employs the search-based structured prediction framework (SEARN) (Daum´e III et al., 2009). SEARN converts the problem of learning a model for structured prediction into learning a set of models for cost-sensitive classification (CSC). In CSC, each training instance has a vector of misclassification costs associated with it, thus rendering some mistakes in some instances to be more expensive than others. Compared to other structured prediction frameworks such as Markov Logic Networks (Poon and Vanderwende, 2010), SEARN provides high modeling flexibility but it does not requiring taskdep"
W11-1805,W11-1807,0,\N,Missing
W11-1805,W11-1808,0,\N,Missing
W11-1805,W02-1001,0,\N,Missing
W11-1805,W09-1400,0,\N,Missing
W11-1805,W11-1801,0,\N,Missing
W11-2205,P10-1132,0,0.0338147,"Missing"
W11-2205,P06-3002,0,0.352384,"Missing"
W11-2205,D10-1056,0,0.0584722,"oved shallow parsing performance. However, they observed that the clustering evaluation scores did not correlate with the re37 sults of this extrinsic evaluation. In other words, better clustering evaluation scores did not always result in better features for shallow parsing. Van Gael et al. noted that homogeneity correlated better with shallow parsing performance, hypothesizing it is probably worse to assign the same state identifier to tokens that belong to different PoS tags, e.g. verb and adverbs, rather than to generate more than one state identifier for the same PoS. In the same spirit, Christodoulopoulos et al. (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006). Like Van Gael et al., they also found that better clustering evaluation scores did not result in better seeds. Given these results, as well as remembering that unsupervised learning methods do not use any label information in model learning, one is entitled to question whether it is reasonable to expect their output to match a particular labeled gold standard. Why not assume that the state identifiers obtained correlate with named entity recognition tag"
W11-2205,E03-1009,0,0.0532627,"Missing"
W11-2205,P11-1061,0,0.180724,"e. In this work we focus on methods that use only unlabeled data to learn a model and do not involve any form of supervision at any stage. Thus we exclude methods that use seeds such as the dictionaries of PoS tags used by Ravi and Knight (2009) and rules for producing labeled output, e.g. those proposed by Teichert and Daum´e III (2009). We also exclude methods for which the data used to learn a model does not contain any of the labels we are learning to predict, but it does contain other information that we use in the learning process. For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 35–42, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics induced, but it assumes access to a labeled dataset of a different language. We begin by surveying recent work on unsupervised PoS tagging, focusing on the issue of evaluation (Section 2). While PoS tagging is not the only task for which unsupervised learning methods are popular, its relative simplicity and the variety of evaluation"
W11-2205,D08-1036,0,0.0130858,"mpleteness the degree to which each gold standard class is contained in a single cluster. Note that there tends to be a trade-off between these two properties since, increasing the number of clusters is likely to improve homogeneity but worsen completeness and vice-versa. Therefore, clustering evaluation measures need to balance appropriately between them. Some authors proposed clustering evaluation techniques that first induce the mapping from state identifiers to gold standard tags automatically and then use supervised measures to compare the mapped output to the gold standard. For example, Gao and Johnson (2008) proposed to induce a manyto-one mapping of state identifiers to PoS tags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy. However, such techniques evaluate the clustering together with the induced mapping, thus the quality of the latter influences the results obtained. This can be misleading as unsupervised learning methods for PoS tagging induce the clustering, but not the mapping on which they are eventually evaluated. In order to avoid the mapping induction step, the use of information theoretic measures was proposed instead. T"
W11-2205,P07-1094,0,0.0670495,"Missing"
W11-2205,N06-1041,0,0.612121,"ic evaluation. In other words, better clustering evaluation scores did not always result in better features for shallow parsing. Van Gael et al. noted that homogeneity correlated better with shallow parsing performance, hypothesizing it is probably worse to assign the same state identifier to tokens that belong to different PoS tags, e.g. verb and adverbs, rather than to generate more than one state identifier for the same PoS. In the same spirit, Christodoulopoulos et al. (2010) used the output of a number of unsupervised PoS tagging methods to extract seeds for the prototype-driven model of Haghighi and Klein (2006). Like Van Gael et al., they also found that better clustering evaluation scores did not result in better seeds. Given these results, as well as remembering that unsupervised learning methods do not use any label information in model learning, one is entitled to question whether it is reasonable to expect their output to match a particular labeled gold standard. Why not assume that the state identifiers obtained correlate with named entity recognition tags or categorial grammar tags instead of PoS tags, tasks for which sequential models are very common? Even if the state identifiers induced co"
W11-2205,P08-1068,0,0.0436546,"main strength, which is that they can use as much data as possible. Using the pre-processing paradigm, clusteringbased word representations induced from a large unlabeled dataset would be evaluated according to whether they improve the performance of the downstream task they are evaluated with, whose evaluation is likely to be on a different dataset. This use of clustering-based word representation is sometimes referred to as semi-supervised learning and has been shown to be effective in a variety of tasks, including named entity recognition, shallow parsing and syntactic dependency parsing (Koo et al., 2008; Turian et al., 2010). The use of large datasets would also help assess the scalability of the unsupervised methods proposed, as the amount of data that can be handled efficiently by an unsupervised method can be as important as the range of linguistic intuitions it can capture. To examine this trade-off, it would be informative to show performance curves with different amounts of data, which should be straightforward to produce under the pre-processing evaluation paradigm. An added benefit is that, as discussed by Ben-Hur et al. (2002), assessing clustering stability using multiple runs and"
W11-2205,P10-1045,0,0.02974,"Missing"
W11-2205,D09-1026,0,0.0211603,"ate identifiers which are semantically void. However, obtaining meaningful labels such as those found in a gold standard is a useful and important goal in many NLP tasks. How40 ever, this purpose is better served by injecting appropriate supervision to the model, instead of trying to achieve it as an afterthought. Such approaches include the use of PoS dictionaries by sequential tagging models (Haghighi and Klein, 2006; Ravi and Knight, 2009), the use of labeled data from different languages (Snyder et al., 2008; Das and Petrov, 2011) or the (possibly indirect) assignment of labels to topics (Ramage et al., 2009; Zhu et al., 2009). Research in unsupervised learning methods is likely to benefit these partially supervised ones, as they both seek to take advantage of unlabeled data. As the output of such methods uses the same labels as those found in the gold standard, they can be evaluated against a labeled gold standard. 5 Conclusions In this position paper, we discussed the issue of evaluation of unsupervised learning methods for NLP tasks. Using PoS tagging as our case study, we examined recent attempts of evaluating unsupervised approaches and showed that a lot of confusion is caused due to evaluat"
W11-2205,P09-1057,0,0.298287,"08) can help reduce this cost, in tasks for which specialist knowledge is required, such as part-of-speech (PoS) tagging or syntactic parsing, labeling datasets in this fashion can be substantially harder. Before we proceed, it is important to characterize the unsupervised learning methods we are considering, as the term unsupervised is used in multiple ways in the literature. In this work we focus on methods that use only unlabeled data to learn a model and do not involve any form of supervision at any stage. Thus we exclude methods that use seeds such as the dictionaries of PoS tags used by Ravi and Knight (2009) and rules for producing labeled output, e.g. those proposed by Teichert and Daum´e III (2009). We also exclude methods for which the data used to learn a model does not contain any of the labels we are learning to predict, but it does contain other information that we use in the learning process. For example, the multilingual PoS induction approach of Das and Petrov (2011) assumes no supervision for the language whose PoS tags are being 35 Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 35–42, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 A"
W11-2205,W09-1121,0,0.0176971,"ch is referred to as cross-validation accuracy. However, such techniques evaluate the clustering together with the induced mapping, thus the quality of the latter influences the results obtained. This can be misleading as unsupervised learning methods for PoS tagging induce the clustering, but not the mapping on which they are eventually evaluated. In order to avoid the mapping induction step, the use of information theoretic measures was proposed instead. These include Variation of Information (VI) (Meil˘a, 2007), V-measure (Rosenberg and Hirschberg, 2007), and their respective variants NVI (Reichart and Rappoport, 2009) and V-beta (Vlachos et al., 2009). Each of these measures exhibits 1 2 3 4 1 5 EX VBP CD NNS RB . There are 70 children there . There are 70 children there . (a) Unsupervised PoS tagger output (b) Gold standard EX VBP CD NNS RB . 1 1 0 0 0 1 0 2 0 1 0 0 0 0 3 0 0 0 1 0 0 4 0 0 1 0 0 0 5 0 0 0 0 0 1 (c) Confusion matrix Figure 1: Unsupervised PoS tagging evaluation pipeline. some kind of bias towards certain solutions though, e.g. V-measure favors clusterings with large number of clusters, while VI exhibits the opposite behavior. While these biases might follow some reasonable intuitions, unsu"
W11-2205,P10-1044,0,0.0459909,"Missing"
W11-2205,D07-1043,0,0.0466456,"ags from one half of the corpus and evaluate on the second half, which is referred to as cross-validation accuracy. However, such techniques evaluate the clustering together with the induced mapping, thus the quality of the latter influences the results obtained. This can be misleading as unsupervised learning methods for PoS tagging induce the clustering, but not the mapping on which they are eventually evaluated. In order to avoid the mapping induction step, the use of information theoretic measures was proposed instead. These include Variation of Information (VI) (Meil˘a, 2007), V-measure (Rosenberg and Hirschberg, 2007), and their respective variants NVI (Reichart and Rappoport, 2009) and V-beta (Vlachos et al., 2009). Each of these measures exhibits 1 2 3 4 1 5 EX VBP CD NNS RB . There are 70 children there . There are 70 children there . (a) Unsupervised PoS tagger output (b) Gold standard EX VBP CD NNS RB . 1 1 0 0 0 1 0 2 0 1 0 0 0 0 3 0 0 0 1 0 0 4 0 0 1 0 0 0 5 0 0 0 0 0 1 (c) Confusion matrix Figure 1: Unsupervised PoS tagging evaluation pipeline. some kind of bias towards certain solutions though, e.g. V-measure favors clusterings with large number of clusters, while VI exhibits the opposite behavior"
W11-2205,P11-1067,0,0.0283692,"ed are semi-automatically processed in order to produce seeds which were then used by the prototype-driven model of Haghighi and Klein (2006).2 In-context evaluation can be used to assess the performance of unsupervised learning methods for tasks other than clustering-based word representation approaches. For example, topic modeling (Blei et al., 2003) has recently been used and evaluated in approaches to learning models of selectional preferences (Ritter et ´ S´eaghdha, 2010). al., 2010; O The issues affecting the evaluation of unsupervised learning methods are not restricted to PoS tagging. Schwartz et al. (2011) discussed similar issues in the context of unsupervised dependency parsing. Note that some of them arise due to the fact unsupervised dependency parsing produces unlabeled directed edges which are interpreted as denoting headdependent relations. However, there are linguistic phenomena where unless the edges are labeled with a specific interpretation, both directions could be considered correct, e.g. the relation between modal verb and main verb. Even though evaluation against a syntactic parsing gold standard is useful, we argue that in-context evaluation of the output of unsupervised depende"
W11-2205,D08-1027,0,0.104932,"Missing"
W11-2205,D08-1109,0,0.0240992,"ill likely to be informative. Finally, in this paper we considered methods whose output consists of state identifiers which are semantically void. However, obtaining meaningful labels such as those found in a gold standard is a useful and important goal in many NLP tasks. How40 ever, this purpose is better served by injecting appropriate supervision to the model, instead of trying to achieve it as an afterthought. Such approaches include the use of PoS dictionaries by sequential tagging models (Haghighi and Klein, 2006; Ravi and Knight, 2009), the use of labeled data from different languages (Snyder et al., 2008; Das and Petrov, 2011) or the (possibly indirect) assignment of labels to topics (Ramage et al., 2009; Zhu et al., 2009). Research in unsupervised learning methods is likely to benefit these partially supervised ones, as they both seek to take advantage of unlabeled data. As the output of such methods uses the same labels as those found in the gold standard, they can be evaluated against a labeled gold standard. 5 Conclusions In this position paper, we discussed the issue of evaluation of unsupervised learning methods for NLP tasks. Using PoS tagging as our case study, we examined recent atte"
W11-2205,P10-1040,0,0.38129,"oS tagging, focusing on the issue of evaluation (Section 2). While PoS tagging is not the only task for which unsupervised learning methods are popular, its relative simplicity and the variety of evaluation paradigms employed make it a useful case study. Based on this survey, we show that evaluation against a PoS tagging gold standard is not only difficult, but it can be misleading as well. The reason for this is that the unsupervised learning methods used, while they produce output that correlates with PoS tags, perform a different task, namely clustering-based word representation induction (Turian et al., 2010). Instead, we argue that in-context evaluation is more appropriate and more informative, as it takes into account the application context in which these methods are intended to be used (Section 3). Finally, bearing the issue of evaluation in mind, we propose some directions for future work in unsupervised learning for NLP (Section 4). 2 The case of unsupervised part-of-speech tagging PoS tagging is the task of assigning lexical categories such as noun or verb to tokens in a sentence. It is commonly used either as an end-goal or as intermediate processing stage for a downstream task such as syn"
W11-2205,D09-1071,1,0.893457,"Missing"
W11-2205,W09-0210,1,0.773128,"racy. However, such techniques evaluate the clustering together with the induced mapping, thus the quality of the latter influences the results obtained. This can be misleading as unsupervised learning methods for PoS tagging induce the clustering, but not the mapping on which they are eventually evaluated. In order to avoid the mapping induction step, the use of information theoretic measures was proposed instead. These include Variation of Information (VI) (Meil˘a, 2007), V-measure (Rosenberg and Hirschberg, 2007), and their respective variants NVI (Reichart and Rappoport, 2009) and V-beta (Vlachos et al., 2009). Each of these measures exhibits 1 2 3 4 1 5 EX VBP CD NNS RB . There are 70 children there . There are 70 children there . (a) Unsupervised PoS tagger output (b) Gold standard EX VBP CD NNS RB . 1 1 0 0 0 1 0 2 0 1 0 0 0 0 3 0 0 0 1 0 0 4 0 0 1 0 0 0 5 0 0 0 0 0 1 (c) Confusion matrix Figure 1: Unsupervised PoS tagging evaluation pipeline. some kind of bias towards certain solutions though, e.g. V-measure favors clusterings with large number of clusters, while VI exhibits the opposite behavior. While these biases might follow some reasonable intuitions, unsurprisingly none is universally acc"
W14-2508,S13-1004,0,0.0118238,"tackle it as a supervised classification task using algorithms that learn from statements annotated with the verdict labels. However this is unlikely to be successful, since statements such as the ones verified by journalists do not contain the world knowledge and the temporal and spatial context needed for this purpose. A different approach would be to match statements to ones already fact-checked by journalists and return the label in a K-nearest neighbour fashion.9 Thus the task is reduced to assessing the semantic similarity between statements, which was explored in a recent shared task (Agirre et al., 2013). An obvious shortcoming of this approach is that it cannot be applied to new claims that have not been fact-checked, thus it can only be used to detect repetitions and paraphrases of false claims. A possible mechanism to extend the coverage of such an approach to novel statements is to assume that some large text collection is the source of all true statements. For example, Wikipedia is likely • assessing causal relations, e.g. whether a statistic should be attributed to a particular law • concerning the future, e.g. speculations involving oil prices • not concerning facts, e.g. whether a pol"
W14-2508,D13-1020,0,0.032377,"rather uncommon task, it is likely to require human supervision, which could possibly be obtained via crowdsourcing. Furthermore, the open-domain nature of fact checking places greater demands on the established tasks of information extraction and semantic parsing. Thus, fact-checking is likely to stimulate research in these tasks on methods that do not require domain-specific supervision (Riedel et al., 2013) and are able to adapt to new information requests (Kwiatkowski et al., 2013). Fact-checking is related to the tasks of textual entailment (Dagan et al., 2006) and machine comprehension (Richardson et al., 2013), with the difference that the text which should be used to predict the entailment of the hypothesis or the correct answer respectively is not provided in the input. Instead, systems need to locate the sources needed to predict the verdict label as part of the task. Furthermore, by defining the task in the context of real-world journalism we are able to obtain labeled statements at no annotation cost, apart from the assessment of their suitability for the task. to contain a statement that would match the second claim in Figure 1. However, it would still be unable to tackle the other claims men"
W14-2508,H05-1079,0,0.0363533,"ng approach Cohen et al. (2011). Some7 E.g. part of the analysis of the first claim in Figure 1 reads: “the full-time figure has the handy effect of stripping out the very lowest earners and bumping up the average”. 8 https://sites.google.com/site/ andreasvlachos/resources 9 The Truth-Teller by Washington Post (http:// truthteller.washingtonpost.com/) follows this approach. 16444 5 http://blogs.channel4.com/factcheck/ 6 http://www.politifact.com/ truth-o-meter/statements/ 20 Finally, the compilation of the answers into a verdict could be considered as a form of logic-based textual entailment (Bos and Markert, 2005). However, the fact-checking stages described include a novel task, namely question construction for a given statement. This task is likely to rely on semantic parsing of the statement followed by restructuring of the logical form generated. Since question construction is a rather uncommon task, it is likely to require human supervision, which could possibly be obtained via crowdsourcing. Furthermore, the open-domain nature of fact checking places greater demands on the established tasks of information extraction and semantic parsing. Thus, fact-checking is likely to stimulate research in thes"
W14-2508,N13-1008,1,0.385391,"namely question construction for a given statement. This task is likely to rely on semantic parsing of the statement followed by restructuring of the logical form generated. Since question construction is a rather uncommon task, it is likely to require human supervision, which could possibly be obtained via crowdsourcing. Furthermore, the open-domain nature of fact checking places greater demands on the established tasks of information extraction and semantic parsing. Thus, fact-checking is likely to stimulate research in these tasks on methods that do not require domain-specific supervision (Riedel et al., 2013) and are able to adapt to new information requests (Kwiatkowski et al., 2013). Fact-checking is related to the tasks of textual entailment (Dagan et al., 2006) and machine comprehension (Richardson et al., 2013), with the difference that the text which should be used to predict the entailment of the hypothesis or the correct answer respectively is not provided in the input. Instead, systems need to locate the sources needed to predict the verdict label as part of the task. Furthermore, by defining the task in the context of real-world journalism we are able to obtain labeled statements at no a"
W14-2508,J12-2003,0,0.0159657,"Missing"
W14-2508,W10-3001,0,\N,Missing
W14-2508,D13-1161,0,\N,Missing
W14-4501,P07-1073,0,0.0340759,"s who were the correct answers did not always have a dedicated Wikipedia page. Even though combining a search engine with distant supervision results in a highly imbalanced learning task, it increases the potential coverage of our system. In this process we rely on the keywords used in the queries in order to find pages containing the entities intended rather than synonymous ones, e.g. the keyword “building” helps avoid extracting sentences mentioning saints instead of churches. Nevertheless, building names such as churches named after saints were often ambiguous resulting in false positives. Bunescu and Mooney (2007) also used a small seed set and a search engine, but they collected sentences via queries containing both the question and the answer entities, thus (unreallistically) assuming knowledge of all the correct answers. Instead we rely on simple heuristics to identify candidate answers. These heuristics are relation-dependent and different types of answers can be easily accommodated, e.g. in completed year relation they are single-token numbers. Finally, the entity filters learned jointly with relation extraction in our approach, while they perform a role similar to NER, they are learned so that th"
W14-4501,D13-1152,0,0.128656,"Missing"
W14-4501,P11-1055,0,0.103475,"tailored NER system. This is reasonable for the example, since such a list is relatively easy to acquire. In order to create training data, queries containing words from the seeds are sent to a search engine. Sentences from the returned pages are then processed to find examples which contain mentions of both a building and the corresponding architect. Applying the distant supervision hypothesis, we assume that such sentences are indeed expressing the desired relation, and these are positive examples. While such data contains noise, it has been shown to be useful in practice (Yao et al., 2010; Hoffmann et al., 2011). At test time the input is the name of a historical building. Now the web is searched to find example sentences containing this name, and the classifier is applied to each sentence, returning either the name of the architect, or none. Note that different sentences could provide evidence for different architects; hence assuming only one architect for each building, a procedure is required to decide between the possible answers (see Sec. 5). 3 Entity Filtering for Relation Extraction Each relation extraction instance consists of a sentence containing a question entity (e.g. Bute House) and a ca"
W14-4501,P09-1113,0,0.269373,"eal-world application. The application is a dialog-based city tour guide, based in Edinburgh. One of the features of the system is its pro-active nature, offering information which may be of interest to the user. In order to be pro-active in this way, as well as answer users’ questions, the system requires a large amount of knowledge about the city. Part of that knowledge is stored in a database, which is time-consuming and difficult to populate manually. Hence, we have explored the use of an automatic knowledge base population technique based on distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009). The attraction of this approach is that the only input required is a list of seed instances of the relation in question and a corpus of sentences expressing new instances of that relation. However, existing studies typically assume a large seed set, whereas in our application such sets are often not readily available, e.g. Mintz et al. (2009) reported using 7K-140K seed instances per relation as input. In this paper, the two relations that we evaluate on are architect name and completion year of buildings. These were chosen because they are highly relevant to our application, but also somewh"
W14-4501,D10-1099,0,0.335865,"ings instead of a tailored NER system. This is reasonable for the example, since such a list is relatively easy to acquire. In order to create training data, queries containing words from the seeds are sent to a search engine. Sentences from the returned pages are then processed to find examples which contain mentions of both a building and the corresponding architect. Applying the distant supervision hypothesis, we assume that such sentences are indeed expressing the desired relation, and these are positive examples. While such data contains noise, it has been shown to be useful in practice (Yao et al., 2010; Hoffmann et al., 2011). At test time the input is the name of a historical building. Now the web is searched to find example sentences containing this name, and the classifier is applied to each sentence, returning either the name of the architect, or none. Note that different sentences could provide evidence for different architects; hence assuming only one architect for each building, a procedure is required to decide between the possible answers (see Sec. 5). 3 Entity Filtering for Relation Extraction Each relation extraction instance consists of a sentence containing a question entity (e"
W14-4501,P13-2141,0,0.101059,"me and completion year of buildings. These were chosen because they are highly relevant to our application, but also somewhat non-standard compared to the existing literature; and crucially they do not come with a readily-available set of seed instances. Furthermore, previous approaches typically assume named entity recognition (NER) as a preprocessing step in order to construct the training and testing instances. However, since these tools are not tailored to the relations of interest, they introduce spurious entity matches that are harmful to performance as shown by Ling and Weld (2012) and Zhang et al. (2013). These authors ameliorated this issue by learning fine-grained entity recognizers and filters using supervised learning. The labeled data used was extracted from the anchor text of entity mentions annotated in Wikipedia, however this is not possible for entities not annotated in this resource. In this work, instead of relying on labeled data to construct entity filters, we learn them jointly with the relation extraction component. For this purpose we use the imitation learning algorithm DAGGER (Ross et al., 2011), which can handle the dependencies between actions taken in a sequence, and use"
W15-3814,W13-2001,0,0.0348375,"embedding is a collective name for a set of language modelling and feature learning techniques, by which words in a vocabulary 2 2.1 Methods and results BioNLP GENIA task A series of efforts has been initiated to evaluate the available solutions and investigate potentials in event extraction technologies. Among them, the 121 Proceedings of the 2015 Workshop on Biomedical Natural Language Processing (BioNLP 2015), pages 121–126, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics periments, we use LibSVM as the implementation of SVM. BioNLP Shared Tasks (BioNLP-ST) [7] have been consistently conducted since 2009 and attracted community-wide support. BioNLP-ST GENIA task is a core task and had the third edition in 2013. The task gradually increased its difficulties and complexities, for example, by upgrading from abstract-only text to full-text articles and subsuming co-reference tasks. In the latest GENIA 2013 task, EVEX achieves the best performance (F-score: 50.97; recall: 45.44; precision: 58.03) [8]. Our system achieves a comparable result with a higher precision (Fscore 47.33; recall: 37.14; precision: 65.21). 2.2 2.3 Word embedding for trigger and arg"
W15-3814,W11-1805,1,\N,Missing
W15-3814,W13-2002,0,\N,Missing
W16-2381,2009.eamt-1.5,1,0.8201,"f actions, which in this case are tag predictions. This setting allows us to incorporate structural information in the classifier by using features based on previous tag predictions. For instance, let us assume that we are trying to predict the tag ti for word wi . A simple classifier can use features derived from wi and also any other words in the sentence. By framing this as a sequence, it can also use features extracted from the previously predicted tags t{1:i−1} . Introduction Quality estimation (QE) models aim at predicting the quality of machine translated (MT) text (Blatz et al., 2004; Specia et al., 2009). This prediction can be at several levels, including word-, sentenceand document-level. In this paper we focus on our submission to the word-level QE WMT 2016 shared task, where the goal is to assign quality labels to each word of the output of an MT system. Word-level QE is traditionally treated as a structured prediction problem, similar to part-of-speech (POS) tagging. The baseline model used in the shared task employs a Conditional Random Field (CRF) (Lafferty et al., 2001) with a set of baseline features. Our system uses a linear classification model trained with imitation learning (Daum"
W16-2381,C04-1046,0,0.0316158,"perform a sequence of actions, which in this case are tag predictions. This setting allows us to incorporate structural information in the classifier by using features based on previous tag predictions. For instance, let us assume that we are trying to predict the tag ti for word wi . A simple classifier can use features derived from wi and also any other words in the sentence. By framing this as a sequence, it can also use features extracted from the previously predicted tags t{1:i−1} . Introduction Quality estimation (QE) models aim at predicting the quality of machine translated (MT) text (Blatz et al., 2004; Specia et al., 2009). This prediction can be at several levels, including word-, sentenceand document-level. In this paper we focus on our submission to the word-level QE WMT 2016 shared task, where the goal is to assign quality labels to each word of the output of an MT system. Word-level QE is traditionally treated as a structured prediction problem, similar to part-of-speech (POS) tagging. The baseline model used in the shared task employs a Conditional Random Field (CRF) (Lafferty et al., 2001) with a set of baseline features. Our system uses a linear classification model trained with im"
W17-4214,N16-1138,1,0.141377,"Missing"
W17-4214,W11-1808,0,0.04479,"the context of sentiment analysis (e.g. Mohammad et al. (2016)) and . Stance classification serves as a first step in compiling lists of articles that corroborate or refute claims made on social media, allowing end-users to make a better informed judgment. In this paper, we discuss our entry to the fake news challenge: an ensemble comprising five individual systems developed by students in the context of their natural language processing module at The University of Sheffield. We used stacking (Wolpert, 1992) as our ensembling technique as it has been applied successfully in other tasks (e.g. Riedel et al. (2011)) and show that it increases the ensemble score above the performance of any of Fake news has become a hotly debated topic in journalism. In this paper, we present our entry to the 2017 Fake News Challenge which models the detection of fake news as a stance classification task that finished in 11th place on the leader board. Our entry is an ensemble system of classifiers developed by students in the context of their coursework. We show how we used the stacking ensemble method for this purpose and obtained improvements in classification accuracy exceeding each of the individual models’ performa"
W18-5501,P17-1171,0,0.0423227,"claim. The systems participating in the FEVER shared task were required to label claims with the correct class and also return the sentence(s) forming the necessary evidence for the assigned label. Performing well at this task requires both identifying relevant evidence and reasoning correctly with respect to the claim. A key difference between this task and other textual entailment and natural language inference tasks (Dagan et al., 2009; Bowman et al., 2015) is the need to identify the evidence from a large textual corpus. Furthermore, in comparison to large-scale question answering tasks (Chen et al., 2017), systems must reason about information that is not present in the claim. We hope that research in these fields will be stimulated by the challenges present in FEVER. One of the limitations of using human annotators to identify correct evidence when constructing the dataset was the trade-off between annotation velocity and evidence recall (Thorne et al., 2018). Evidence selected by annotators was often incomplete. As part of the FEVER shared task, any evidence retrieved by participating systems that was not contained in the original dataset was annotated and used to augment the evidence in the"
W18-5501,D17-1070,0,0.016436,"on; UCL Machine Reading Group classify each evidenceclaim pair individually and aggregate the results using a simple multilayer perceptron (MLP); Columbia NLP perform majority voting; and finally, Athene-UKP TU Darmstadt encode each evidence-claim pair individually using an Enhanced LSTM, pool the resulting vectors and use an MLP for classification. Sentence Representation: University of Arizona explore using non-lexical features for predicting entailment, considering the proportion of negated verbs, presence of antonyms and noun overlap. Columbia NLP learn universal sentence representations (Conneau et al., 2017). UNC-NLP include an additional token-level feature the sentence similarity score from the sentence selection module. Both Ohio State and UNC-NLP report alternative token encodings: UNC-NLP report using ELMo (Peters et al., 2018) and WordNet (Miller, 1995) and Ohio State report using vector representations of named entities. FujiXerox report representing sentences using D E I S T E (Yin et al., 2018). Training: BUPT-NLPer and SWEEPer model the evidence selection and claim verification using a multi-task learning model under the hypothesis that information from each task supplements the other."
W18-5501,D16-1244,0,0.065618,"Missing"
W18-5501,N18-1202,0,0.0112822,"ch evidence-claim pair individually using an Enhanced LSTM, pool the resulting vectors and use an MLP for classification. Sentence Representation: University of Arizona explore using non-lexical features for predicting entailment, considering the proportion of negated verbs, presence of antonyms and noun overlap. Columbia NLP learn universal sentence representations (Conneau et al., 2017). UNC-NLP include an additional token-level feature the sentence similarity score from the sentence selection module. Both Ohio State and UNC-NLP report alternative token encodings: UNC-NLP report using ELMo (Peters et al., 2018) and WordNet (Miller, 1995) and Ohio State report using vector representations of named entities. FujiXerox report representing sentences using D E I S T E (Yin et al., 2018). Training: BUPT-NLPer and SWEEPer model the evidence selection and claim verification using a multi-task learning model under the hypothesis that information from each task supplements the other. SWEEPer also report parameter tuning using reinforcement learning. 5 Additional Annotation As mentioned in the introduction, to increase the evidence coverage in the test set, the evidence submitted by participating systems was a"
W18-5511,N16-1097,0,0.0357433,"Missing"
W18-5511,D15-1075,0,0.0590623,"thus not reliant on crowd-sourcing. Our approach also takes inspiration from var3 Model Our approach takes as input two pieces of text, a sentence containing the subject and object entities of a candidate relation, and the relation’s description, and returns as output a binary response indicating whether the meaning of the description can be inferred between the two entities in the sentence. See Table 1 for some examples. The problem of determining whether the meaning of a text fragment can be inferred from another is that of natural language inference/textual entailment (Dagan et al., 2005; Bowman et al., 2015). We take as our base model the Enhanced Sequential Inference Model (ESIM) introduced by Chen et al. (2017), one of the commonly used models for text pair tasks (?). ESIM utilizes Bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) (BiLSTM) units as a building block and accepts two sequences of text as input. It then passes the two sequences through three model stages - input encoding, local inference modelling and inference composition, and returns the class c with the highest classification score, where c in textual entailment is one of entai"
W18-5511,D14-1162,0,0.0818591,"DS only supervision. ting. We find that CIM pre-trained on only textual entailment data is already able to make predictions for unseen test relations, while using a combination of distant supervision and textual entailment data achieved improved F1 scores across both datasets, demonstrating the validity of our approach in this setting. We also note that using TE+DS data performs worse than DS data alone in the case of the UW-RE dataset, unlike in the case of LMU-RC. We hypothesize that this is because DS data performs much better for the former. We initialize word embeddings with 300D Glove (Pennington et al., 2014) vectors. We found a few epochs of training (generally less than 5) to be sufficient for convergence. We apply Dropout with a keep probability of 0.9 to all layers. The result reported for each experiment is the average taken over five runs with independent random initializations. In order to prevent overfitting to specific entities, we mask out the subject and object entities with the tokens SUBJECT ENTITY and OBJECT ENTITY respectively. 5.1 5.2 Few-shot Relation Learning For the experiments in the limited-supervision setting, we randomly partition the dataset along relations into a train/dev"
W18-5511,P17-1152,0,0.0297198,"s input two pieces of text, a sentence containing the subject and object entities of a candidate relation, and the relation’s description, and returns as output a binary response indicating whether the meaning of the description can be inferred between the two entities in the sentence. See Table 1 for some examples. The problem of determining whether the meaning of a text fragment can be inferred from another is that of natural language inference/textual entailment (Dagan et al., 2005; Bowman et al., 2015). We take as our base model the Enhanced Sequential Inference Model (ESIM) introduced by Chen et al. (2017), one of the commonly used models for text pair tasks (?). ESIM utilizes Bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) (BiLSTM) units as a building block and accepts two sequences of text as input. It then passes the two sequences through three model stages - input encoding, local inference modelling and inference composition, and returns the class c with the highest classification score, where c in textual entailment is one of entailment, contradiction or neutral. In our experiments, for each (sentence, relation description) pair we retu"
W18-5511,N18-1202,0,0.0373235,"ovember 24, 1775) was an Italian Jesuit, elected the 18th Superior General of the Y. X is a 1956 Danish romantic comedy written and directed by Y. X is a computer game for the PC, developed by Y and published by Sierra Entertainment. Description (Hypothesis) X was a member of the group Y The director of X is Y Y is the designer of X Table 1: Examples of relations, entities, sample text instances, and relation descriptions. 2 Related Work ious approaches for leveraging knowledge from a set of source tasks to target tasks, such as recent transfer learning methods in natural language processing (Peters et al., 2018; McCann et al., 2017). Closest to our work is that of Conneau et al. (2017), who showed that representations learned from natural language inference data can enhance performance when transferred to a number of other natural language tasks. In this work, we consider the task of zero-shot relation classification by utilizing relation descriptions. Most recent work, including Adel et al. (2016) and Zhang et al. (2017), proposed models that assume the availability of supervised data for the task of relation classification. Rockt¨aschel et al. (2015) and Demeester et al. (2016) inject prior knowle"
W18-5511,N13-1008,0,0.408778,"on classification. Introduction The task of determining the relation between various entities from text is an important one for many natural language understanding systems, including question answering, knowledge base construction and web search. Relation classification is an essential part of many high-performing relation extraction systems in the NIST-organised TAC Knowledge Base Population (TAC-KBP) track (Ji and Grishman, 2011; Adel et al., 2016). As a result of its wide application, many approaches and systems have been proposed for this task (Zelenko et al., 2003; Surdeanu et al., 2012; Riedel et al., 2013; Zhang et al., 2017). A shortcoming common to previous proposed approaches, however, is that they identify only relations observed at training time, and are unable to generalize to new (unobserved) relations at test time. To address this challenge, we propose to formulate relation classification as follows: Given a unit of text T which mentions a subject X and a candidate object Y of a knowledge base relation R(X, Y ), and a natural language description d of R, we wish to evaluate whether T expresses In experiments on two datasets, we assess the performance of our approach in two supervision"
W18-5511,D17-1070,0,0.0253089,"of the Y. X is a 1956 Danish romantic comedy written and directed by Y. X is a computer game for the PC, developed by Y and published by Sierra Entertainment. Description (Hypothesis) X was a member of the group Y The director of X is Y Y is the designer of X Table 1: Examples of relations, entities, sample text instances, and relation descriptions. 2 Related Work ious approaches for leveraging knowledge from a set of source tasks to target tasks, such as recent transfer learning methods in natural language processing (Peters et al., 2018; McCann et al., 2017). Closest to our work is that of Conneau et al. (2017), who showed that representations learned from natural language inference data can enhance performance when transferred to a number of other natural language tasks. In this work, we consider the task of zero-shot relation classification by utilizing relation descriptions. Most recent work, including Adel et al. (2016) and Zhang et al. (2017), proposed models that assume the availability of supervised data for the task of relation classification. Rockt¨aschel et al. (2015) and Demeester et al. (2016) inject prior knowledge in the form of propositional logic rules to improve relation extraction"
W18-5511,N15-1118,0,0.104307,"Missing"
W18-5511,D16-1146,0,0.0458699,"Missing"
W18-5511,D12-1042,0,0.149103,"Missing"
W18-5511,N18-1101,0,0.030598,"tance consisting of a subject entity, a knowledge base relation, a question template for the relation, and a sentence retrieved from the subject entity’s Wikipedia page. We wrote descriptions for each of the 120 relations in the dataset, with each relation’s question templates serving as a guide. Thus all instances in the dataset (30 million positive and 2 million negative ones) now include the corresponding relation description, making them suitable for relation classification using our approach. In addition to the two datasets, we also utilize the MultiNLI natural language inference corpus (Williams et al., 2018) in our experiments as a source of supervision. We map its entailment and contradiction class instances to positive and negative relation instances respectively. (1) (2) − − where → c p,← c p ∈ Rd are respectively the last cell states in the forward and reverse directions of the − − BiLSTM that reads the premise. → c h,← c h ∈ Rd are similarly defined for the hypothesis. 3.1 Conditional encoding for ESIM When used for zero-shot relation classification, ESIM encodes the sentence independently of the relation description. Given a new target relation’s description, it is desirable for representat"
W18-5511,P11-1115,0,0.0996036,"Missing"
W18-5511,K17-1034,0,0.451594,"edge in the form of propositional logic rules to improve relation extraction for new relations with zero and few training labels, in the context of the universal schema approach (Riedel et al., 2013). They considered the use of propositional logic rules, which for instance, can be mined from external knowledge bases (such as Freebase (Bollacker et al., 2008)) or obtained from ontologies such as WordNet (Miller, 1995). However, the use of propositional logic rules assumes prior knowledge of the possible relations between entities, and is thus of limited application in extracting new relations. Levy et al. (2017) showed that a related and complementary task, that of entity/attribute relation extraction, can be reduced to a question answering problem. The task we address in this work is that of zero-shot relation classification, which determines if a given relation exists between two given entities in text. As a result the output of our approach is a binary classification decision indicating whether a given relation exists between two given entities in text. The task performed by Levy et al. (2017) is that of zero-shot entity/attribute relation extraction, since their approach returns the span correspo"
W18-5511,D17-1004,0,0.0312644,"ptions. 2 Related Work ious approaches for leveraging knowledge from a set of source tasks to target tasks, such as recent transfer learning methods in natural language processing (Peters et al., 2018; McCann et al., 2017). Closest to our work is that of Conneau et al. (2017), who showed that representations learned from natural language inference data can enhance performance when transferred to a number of other natural language tasks. In this work, we consider the task of zero-shot relation classification by utilizing relation descriptions. Most recent work, including Adel et al. (2016) and Zhang et al. (2017), proposed models that assume the availability of supervised data for the task of relation classification. Rockt¨aschel et al. (2015) and Demeester et al. (2016) inject prior knowledge in the form of propositional logic rules to improve relation extraction for new relations with zero and few training labels, in the context of the universal schema approach (Riedel et al., 2013). They considered the use of propositional logic rules, which for instance, can be mined from external knowledge bases (such as Freebase (Bollacker et al., 2008)) or obtained from ontologies such as WordNet (Miller, 1995)"
W19-4326,D18-1514,0,0.19219,"ifelong relation extraction as a metalearning challenge, to which the machinery of current optimization-based meta-learning algorithms can be applied. Unlike the use of a separate alignment model as proposed in Wang et al. (2019), the proposed approach does not introduce additional parameters. In addition, the proposed approach is more data efficient since it explicitly optimizes for the transfer of knowledge from past relations, while avoiding the catastrophic forgetting of previously learned relations. Empirically, we evaluate on lifelong versions of the datasets by Bordes et al. (2015) and Han et al. (2018) and demonstrate conMost existing relation extraction models assume a fixed set of relations and are unable to adapt to exploit newly available supervision data to extract new relations. In order to alleviate such problems, there is the need to develop approaches that make relation extraction models capable of continuous adaptation and learning. We investigate and present results for such an approach, based on a combination of ideas from lifelong learning and optimizationbased meta-learning. We evaluate the proposed approach on two recent lifelong relation extraction benchmarks, and demonstrat"
W19-4326,K17-1034,0,0.0196337,"relation extraction benchmarks, and demonstrate that it markedly outperforms current state-of-the-art approaches. 1 Introduction The majority of existing supervised relation extraction models can only extract a fixed set of relations which has been specified at training time. They are unable to detect an evolving set of novel relations observed after training without substantial retraining, which can be computationally expensive and may lead to catastrophic forgetting of previously learned relations. Zero-shot relation extraction approaches (Rockt¨aschel et al., 2015; Demeester et al., 2016; Levy et al., 2017; Obamuyide and Vlachos, 2018) can extract unseen relations, but at lower performance levels, and are unable to continually exploit newly available supervision to improve performance without considerable retraining. These limitations also extend to approaches to extracting relations in other limited supervision settings, for instance in the oneshot setting (Obamuyide and Vlachos, 2017). It is therefore desirable for relation extraction models to have the capability to learn continuously without catastrophic forgetting of previously learned relations. This would enable them exploit newly 224 Pr"
W19-4326,N19-1086,0,0.526918,"romising results, a key weakness of the approach is that the use of an alignment model introduces additional parameters to already overparameterized relation extraction models, which may in turn lead to an increase in the quantity of supervision required for training. In addition, the approach can only align embeddings between observed relations, and does not have any explicit objective that encourages the model to transfer and exploit knowledge gathered from previously observed relations to facilitate the efficient learning of yet to be observed relations. In this work, we extend the work of Wang et al. (2019) by exploiting ideas from both lifelong learning and meta-learning. We propose to consider lifelong relation extraction as a metalearning challenge, to which the machinery of current optimization-based meta-learning algorithms can be applied. Unlike the use of a separate alignment model as proposed in Wang et al. (2019), the proposed approach does not introduce additional parameters. In addition, the proposed approach is more data efficient since it explicitly optimizes for the transfer of knowledge from past relations, while avoiding the catastrophic forgetting of previously learned relations"
W19-4326,W18-5511,1,0.828485,"n benchmarks, and demonstrate that it markedly outperforms current state-of-the-art approaches. 1 Introduction The majority of existing supervised relation extraction models can only extract a fixed set of relations which has been specified at training time. They are unable to detect an evolving set of novel relations observed after training without substantial retraining, which can be computationally expensive and may lead to catastrophic forgetting of previously learned relations. Zero-shot relation extraction approaches (Rockt¨aschel et al., 2015; Demeester et al., 2016; Levy et al., 2017; Obamuyide and Vlachos, 2018) can extract unseen relations, but at lower performance levels, and are unable to continually exploit newly available supervision to improve performance without considerable retraining. These limitations also extend to approaches to extracting relations in other limited supervision settings, for instance in the oneshot setting (Obamuyide and Vlachos, 2017). It is therefore desirable for relation extraction models to have the capability to learn continuously without catastrophic forgetting of previously learned relations. This would enable them exploit newly 224 Proceedings of the 4th Workshop"
W19-4326,P17-1053,0,0.0243986,"EMR MLLRE 0.189 0.492 0.361 0.271 0.566 0.564 0.526 0.510 0.602 0.208 0.598 0.425 0.302 0.673 0.674 0.632 0.620 0.741 0.632 0.841 0.776 0.672 0.878 0.857 0.869 0.852 0.880 0.569 0.796 0.722 0.590 0.824 0.812 0.820 0.808 0.842 Table 1: Accuracy on the test set of all tasks ACCwhole (denoted ACCw. ) and average accuracy on the test set of only observed tasks ACCavg (denoted ACCa. ) on the Lifelong FewRel and Lifelong SimpleQuestions datasets. Best results are in bold. Except for MLLRE, results for other models are obtained from Wang et al. (2019). arachical Residual BiLSTM (HR-BiLSTM) model of Yu et al. (2017), which is the same model used by Wang et al. (2019) for their experiments. The HR-BILSTM is a relation classifier which accepts as input a sentence and a candidate relation, then utilizes two Bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) (BiLSTM) units with shared parameters to process the Glove (Pennington et al., 2014) embeddings of words in the sentence and relation names, then selects the relation with the maximum cosine similarity to the sentence as its response. Algorithm 1 Meta-Learning for Lifelong Relation Extraction (MLLRE) Req"
W19-4326,D14-1162,0,0.0834402,"oted ACCa. ) on the Lifelong FewRel and Lifelong SimpleQuestions datasets. Best results are in bold. Except for MLLRE, results for other models are obtained from Wang et al. (2019). arachical Residual BiLSTM (HR-BiLSTM) model of Yu et al. (2017), which is the same model used by Wang et al. (2019) for their experiments. The HR-BILSTM is a relation classifier which accepts as input a sentence and a candidate relation, then utilizes two Bidirectional Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves and Schmidhuber, 2005) (BiLSTM) units with shared parameters to process the Glove (Pennington et al., 2014) embeddings of words in the sentence and relation names, then selects the relation with the maximum cosine similarity to the sentence as its response. Algorithm 1 Meta-Learning for Lifelong Relation Extraction (MLLRE) Require: Stream of incoming tasks T1 , T2 , T3 , ... Require: Relation extraction function fθ Require: Optimization algorithm (e.g. SGD) Require: Step size , learning rate α Require: Buffer memory B 1: Randomly initialize θ 2: while there are still tasks do 3: Retrieve next task Tt from stream 4: Initialize θt ← θ 5: repeat 6: if B is not empty then 7: Retrieve exemplars E of ra"
