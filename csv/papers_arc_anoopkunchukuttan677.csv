2021.wat-1.1,Overview of the 8th Workshop on {A}sian Translation,2021,-1,-1,9,0,283,toshiaki nakazawa,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"This paper presents the results of the shared tasks from the 8th workshop on Asian translation (WAT2021). For the WAT2021, 28 teams participated in the shared tasks and 24 teams submitted their translation results for the human evaluation. We also accepted 5 research papers. About 2,100 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated."
2021.wat-1.22,Itihasa: A large-scale corpus for {S}anskrit to {E}nglish translation,2021,-1,-1,3,0,371,rahul aralikatte,Proceedings of the 8th Workshop on Asian Translation (WAT2021),0,"This work introduces Itihasa, a large-scale translation dataset containing 93,000 pairs of Sanskrit shlokas and their English translations. The shlokas are extracted from two Indian epics viz., The Ramayana and The Mahabharata. We first describe the motivation behind the curation of such a dataset and follow up with empirical analysis to bring out its nuances. We then benchmark the performance of standard translation models on this corpus and show that even state-of-the-art transformer architectures perform poorly, emphasizing the complexity of the dataset."
2021.eacl-main.303,A Large-scale Evaluation of Neural Machine Transliteration for Indic Languages,2021,-1,-1,1,1,290,anoop kunchukuttan,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"We take up the task of large-scale evaluation of neural machine transliteration between English and Indic languages, with a focus on multilingual transliteration to utilize orthographic similarity between Indian languages. We create a corpus of 600K word pairs mined from parallel translation corpora and monolingual corpora, which is the largest transliteration corpora for Indian languages mined from public sources. We perform a detailed analysis of multilingual transliteration and propose an improved multilingual training recipe for Indic languages. We analyze various factors affecting transliteration quality like language family, transliteration direction and word origin."
2020.wmt-1.19,Contact Relatedness can help improve multilingual {NMT}: {M}icrosoft {STCI}-{MT} @ {WMT}20,2020,-1,-1,2,0,13798,vikrant goyal,Proceedings of the Fifth Conference on Machine Translation,0,"We describe our submission for the EnglishâTamil and TamilâEnglish news translation shared task. In this submission, we focus on exploring if a low-resource language (Tamil) can benefit from a high-resource language (Hindi) with which it shares contact relatedness. We show utilizing contact relatedness via multilingual NMT can significantly improve translation quality for English-Tamil translation."
2020.wat-1.1,Overview of the 7th Workshop on {A}sian Translation,2020,-1,-1,9,0,283,toshiaki nakazawa,Proceedings of the 7th Workshop on Asian Translation,0,"This paper presents the results of the shared tasks from the 7th workshop on Asian translation (WAT2020). For the WAT2020, 20 teams participated in the shared tasks and 14 teams submitted their translation results for the human evaluation. We also received 12 research paper submissions out of which 7 were accepted. About 500 translation results were submitted to the automatic evaluation server, and selected submissions were manually evaluated."
2020.repl4nlp-1.6,Learning Geometric Word Meta-Embeddings,2020,23,0,3,0.833333,15637,pratik jawanpuria,Proceedings of the 5th Workshop on Representation Learning for NLP,0,"We propose a geometric framework for learning meta-embeddings of words from different embedding sources. Our framework transforms the embeddings into a common latent space, where, for example, simple averaging or concatenation of different embeddings (of a given word) is more amenable. The proposed latent space arises from two particular geometric transformations - source embedding specific orthogonal rotations and a common Mahalanobis metric scaling. Empirical results on several word similarity and word analogy benchmarks illustrate the efficacy of the proposed framework."
2020.findings-emnlp.445,"{I}ndic{NLPS}uite: Monolingual Corpora, Evaluation Benchmarks and Pre-trained Multilingual Language Models for {I}ndian Languages",2020,-1,-1,2,0,19984,divyanshu kakwani,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"In this paper, we introduce NLP resources for 11 major Indian languages from two major language families. These resources include: (a) large-scale sentence-level monolingual corpora, (b) pre-trained word embeddings, (c) pre-trained language models, and (d) multiple NLU evaluation datasets (\textit{IndicGLUE} benchmark). The monolingual corpora contains a total of 8.8 billion tokens across all 11 languages and Indian English, primarily sourced from news crawls. The word embeddings are based on \textit{FastText}, hence suitable for handling morphological complexity of Indian languages. The pre-trained language models are based on the compact ALBERT model. Lastly, we compile the (\textit{IndicGLUE} benchmark for Indian language NLU. To this end, we create datasets for the following tasks: Article Genre Classification, Headline Prediction, Wikipedia Section-Title Prediction, Cloze-style Multiple choice QA, Winograd NLI and COPA. We also include publicly available datasets for some Indic languages for tasks like Named Entity Recognition, Cross-lingual Sentence Retrieval, Paraphrase detection, \textit{etc.} Our embeddings are competitive or better than existing pre-trained embeddings on multiple tasks. We hope that the availability of the dataset will accelerate Indic NLP research which has the potential to impact more than a billion people. It can also help the community in evaluating advances in NLP over a more diverse pool of languages. The data and models are available at \url{https://indicnlp.ai4bharat.org}."
2020.coling-tutorials.3,Multilingual Neural Machine Translation,2020,0,28,3,0.168849,286,raj dabre,Proceedings of the 28th International Conference on Computational Linguistics: Tutorial Abstracts,0,"The advent of neural machine translation (NMT) has opened up exciting research in building multilingual translation systems i.e. translation models that can handle more than one language pair. Many advances have been made which have enabled (1) improving translation for low-resource languages via transfer learning from high resource languages; and (2) building compact translation models spanning multiple languages. In this tutorial, we will cover the latest advances in NMT approaches that leverage multilingualism, especially to enhance low-resource translation. In particular, we will focus on the following topics: modeling parameter sharing for multi-way models, massively multilingual models, training protocols, language divergence, transfer learning, zero-shot/zero-resource learning, pivoting, multilingual pre-training and multi-source translation."
Q19-1007,Learning Multilingual Word Embeddings in Latent Metric Space: A Geometric Approach,2019,13,15,3,0.833333,15637,pratik jawanpuria,Transactions of the Association for Computational Linguistics,0,"We propose a novel geometric approach for learning bilingual mappings given monolingual embeddings and a bilingual dictionary. Our approach decouples the source-to-target language transformation into (a) language-specific rotations on the original embeddings to align them in a common, latent space, and (b) a language-independent similarity metric in this common space to better model the similarity between the embeddings. Overall, we pose the bilingual mapping problem as a classification problem on smooth Riemannian manifolds. Empirically, our approach outperforms previous approaches on the bilingual lexicon induction and cross-lingual word similarity tasks. We next generalize our framework to represent multiple languages in a common latent space. Language-specific rotations for all the languages and a common similarity metric in the latent space are learned jointly from bilingual dictionaries for multiple language pairs. We illustrate the effectiveness of joint learning for multiple languages in an indirect word translation setting."
N19-1387,Addressing word-order Divergence in Multilingual Neural Machine Translation for extremely Low Resource Languages,2019,0,6,2,0.952381,5018,rudra murthy,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Transfer learning approaches for Neural Machine Translation (NMT) train a NMT model on an assisting language-target language pair (parent model) which is later fine-tuned for the source language-target language pair of interest (child model), with the target language being the same. In many cases, the assisting language has a different word order from the source language. We show that divergent word order adversely limits the benefits from transfer learning when little to no parallel corpus between the source and target language is available. To bridge this divergence, we propose to pre-order the assisting language sentences to match the word order of the source language and train the parent model. Our experiments on many language pairs show that bridging the word order gap leads to significant improvement in the translation quality in extremely low-resource scenarios."
D19-5201,Overview of the 6th Workshop on {A}sian Translation,2019,0,1,9,0,283,toshiaki nakazawa,Proceedings of the 6th Workshop on Asian Translation,0,"This paper presents the results of the shared tasks from the 6th workshop on Asian translation (WAT2019) including JaâEn, JaâZh scientific paper translation subtasks, JaâEn, JaâKo, JaâEn patent translation subtasks, HiâEn, MyâEn, KmâEn, TaâEn mixed domain subtasks and RuâJa news commentary translation task. For the WAT2019, 25 teams participated in the shared tasks. We also received 10 research paper submissions out of which 61 were accepted. About 400 translation results were submitted to the automatic evaluation server, and selected submis- sions were manually evaluated."
Y18-3001,Overview of the 5th Workshop on {A}sian Translation,2018,0,5,9,0,283,toshiaki nakazawa,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
Y18-3003,{NICT}{'}s Participation in {WAT} 2018: Approaches Using Multilingualism and Recurrently Stacked Layers,2018,0,3,2,0.168849,286,raj dabre,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
Y18-3013,Multilingual {I}ndian Language Translation System at {WAT} 2018: Many-to-one Phrase-based {SMT},2018,0,6,2,0,5017,tamali banerjee,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 5th Workshop on Asian Translation: 5th Workshop on Asian Translation",0,None
Q18-1022,Leveraging Orthographic Similarity for Multilingual Neural Transliteration,2018,0,1,1,1,290,anoop kunchukuttan,Transactions of the Association for Computational Linguistics,0,"We address the task of joint training of transliteration models for multiple language pairs (multilingual transliteration). This is an instance of multitask learning, where individual tasks (language pairs) benefit from sharing knowledge with related tasks. We focus on transliteration involving related tasks i.e., languages sharing writing systems and phonetic properties (orthographically similar languages). We propose a modified neural encoder-decoder model that maximizes parameter sharing across language pairs in order to effectively leverage orthographic similarity. We show that multilingual transliteration significantly outperforms bilingual transliteration in different scenarios (average increase of 58{\%} across a variety of languages we experimented with). We also show that multilingual transliteration models can generalize well to languages/language pairs not encountered during training and hence perform well on the zeroshot transliteration task. We show that further improvements can be achieved by using phonetic feature input."
P18-2064,Judicious Selection of Training Data in Assisting Language for Multilingual Neural {NER},2018,0,3,2,0.952381,5018,rudra murthy,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Multilingual learning for Neural Named Entity Recognition (NNER) involves jointly training a neural network for multiple languages. Typically, the goal is improving the NER performance of one of the languages (the primary language) using the other assisting languages. We show that the divergence in the tag distributions of the common named entities between the primary and assisting languages can reduce the effectiveness of multilingual learning. To alleviate this problem, we propose a metric based on symmetric KL divergence to filter out the highly divergent training instances in the assisting language. We empirically show that our data selection strategy improves NER performance in many languages, including those with very limited training data."
L18-1548,The {IIT} {B}ombay {E}nglish-{H}indi Parallel Corpus,2018,-1,-1,1,1,290,anoop kunchukuttan,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-5717,Comparing Recurrent and Convolutional Architectures for {E}nglish-{H}indi Neural Machine Translation,2017,7,1,3,0.869565,31053,sandhya singh,Proceedings of the 4th Workshop on {A}sian Translation ({WAT}2017),0,"In this paper, we empirically compare the two encoder-decoder neural machine translation architectures: convolutional sequence to sequence model (ConvS2S) and recurrent sequence to sequence model (RNNS2S) for English-Hindi language pair as part of IIT Bombay{'}s submission to WAT2017 shared task. We report the results for both English-Hindi and Hindi-English direction of language pair."
W17-4102,Learning variable length units for {SMT} between related languages via Byte Pair Encoding,2017,0,7,1,1,290,anoop kunchukuttan,Proceedings of the First Workshop on Subword and Character Level Models in {NLP},0,"We explore the use of segments learnt using Byte Pair Encoding (referred to as BPE units) as basic units for statistical machine translation between related languages and compare it with orthographic syllables, which are currently the best performing basic units for this translation task. BPE identifies the most frequent character sequences as basic units, while orthographic syllables are linguistically motivated pseudo-syllables. We show that BPE units modestly outperform orthographic syllables as units of translation, showing up to 11{\%} increase in BLEU score. While orthographic syllables can be used only for languages whose writing systems use vowel representations, BPE is writing system independent and we show that BPE outperforms other units for non-vowel writing systems too. Our results are supported by extensive experimentation spanning multiple language families and writing systems."
I17-2048,"Utilizing Lexical Similarity between Related, Low-resource Languages for Pivot-based {SMT}",2017,16,1,1,1,290,anoop kunchukuttan,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"We investigate pivot-based translation between related languages in a low resource, phrase-based SMT setting. We show that a subword-level pivot-based SMT model using a related pivot language is substantially better than word and morpheme-level pivot models. It is also highly competitive with the best direct translation model, which is encouraging as no direct source-target training corpus is used. We also show that combining multiple related language pivot models can rival a direct translation model. Thus, the use of subwords as translation units coupled with multiple related pivot languages can compensate for the lack of a direct parallel corpus."
W16-4811,Faster Decoding for Subword Level Phrase-based {SMT} between Related Languages,2016,18,1,1,1,290,anoop kunchukuttan,"Proceedings of the Third Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial3)",0,"A common and effective way to train translation systems between related languages is to consider sub-word level basic units. However, this increases the length of the sentences resulting in increased decoding time. The increase in length is also impacted by the specific choice of data format for representing the sentences as subwords. In a phrase-based SMT framework, we investigate different choices of decoder parameters as well as data format and their impact on decoding time and translation accuracy. We suggest best options for these settings that significantly improve decoding time with little impact on the translation accuracy."
W16-4604,{IIT} {B}ombay{'}s {E}nglish-{I}ndonesian submission at {WAT}: Integrating Neural Language Models with {SMT},2016,4,0,2,0.869565,31053,sandhya singh,Proceedings of the 3rd Workshop on {A}sian Translation ({WAT}2016),0,"This paper describes the IIT Bombay{'}s submission as a part of the shared task in WAT 2016 for English{--}Indonesian language pair. The results reported here are for both the direction of the language pair. Among the various approaches experimented, Operation Sequence Model (OSM) and Neural Language Model have been submitted for WAT. The OSM approach integrates translation and reordering process resulting in relatively improved translation. Similarly the neural experiment integrates Neural Language Model with Statistical Machine Translation (SMT) as a feature for translation. The Neural Probabilistic Language Model (NPLM) gave relatively high BLEU points for Indonesian to English translation system while the Neural Network Joint Model (NNJM) performed better for English to Indonesian direction of translation system. The results indicate improvement over the baseline Phrase-based SMT by 0.61 BLEU points for English-Indonesian system and 0.55 BLEU points for Indonesian-English translation system."
N16-4006,Statistical Machine Translation between Related Languages,2016,5,4,3,0,382,pushpak bhattacharyya,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Tutorial Abstracts,0,None
K16-1027,Substring-based unsupervised transliteration with phonetic and contextual knowledge,2016,0,0,1,1,290,anoop kunchukuttan,Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning,0,None
D16-1196,Orthographic Syllable as basic unit for {SMT} between Related Languages,2016,0,8,1,1,290,anoop kunchukuttan,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
W15-5902,Addressing Class Imbalance in Grammatical Error Detection with Evaluation Metric Optimization,2015,24,0,1,1,290,anoop kunchukuttan,Proceedings of the 12th International Conference on Natural Language Processing,0,"We address the problem of class imbalance in supervised grammatical error detection (GED) for non-native speaker text, which is the result of the low proportion of erroneous examples compared to a large number of error-free examples. Most learning algorithms maximize accuracy which is not a suitable objective for such imbalanced data. For GED, most systems address this issue by tuning hyperparameters to maximize metrics like Fxcexb2 . Instead, we show that learning classifiers that directly learn model parameters by optimizing evaluation metrics like F1 and F2 score deliver better performance on these metrics as compared to traditional sampling and cost-sensitive learning solutions for addressing class imbalance. Optimizing these metrics is useful in recall-oriented grammar error detection scenarios. We also show that there are inherent difficulties in optimizing precision-oriented evaluation metrics like F0.5. We establish this through a systematic evaluation on multiple datasets and different GED tasks."
W15-5944,Augmenting Pivot based {SMT} with word segmentation,2015,0,2,2,0,36405,rohit more,Proceedings of the 12th International Conference on Natural Language Processing,0,None
W15-5950,Investigating the potential of post-ordering {SMT} output to improve translation quality,2015,21,0,2,0,26981,pratik mehta,Proceedings of the 12th International Conference on Natural Language Processing,0,"Post-ordering of Statistical Machine Translation (SMT) output to correct word order errors could be a promising area of research to overcome structural divergence between language pairs. This is especially true when it is difficult to incorporate rich linguistic features into the baseline decoder. In this paper, we propose an algorithm for generating oracle reorderings of MT output. We use the oracle reorderings to empirically quantify an upper bound on improvement in translation quality through post-ordering techniques. In our study encompassing multiple language pairs, we show that significant improvement in translation quality can be obtained by applying reordering transformations on the output of the SMT system. This presents a strong case for investing effort in exploring the post-ordering problem."
W15-3912,Data representation methods and use of mined corpora for {I}ndian language transliteration,2015,14,7,1,1,290,anoop kunchukuttan,Proceedings of the Fifth Named Entity Workshop,0,"Our NEWS 2015 shared task submission is a PBSMT based transliteration system with the following corpus preprocessing enhancements: (i) addition of wordboundary markers, and (ii) languageindependent, overlapping character segmentation. We show that the addition of word-boundary markers improves transliteration accuracy substantially, whereas our overlapping segmentation shows promise in our preliminary analysis. We also compare transliteration systems trained using manually created corpora with the ones mined from parallel translation corpus for English to Indian language pairs. We identify the major errors in English to Indian language transliterations by analyzing heat maps of confusion matrices."
N15-3017,Brahmi-Net: A transliteration and script conversion system for languages of the {I}ndian subcontinent,2015,5,11,1,1,290,anoop kunchukuttan,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Demonstrations,0,"We present Brahmi-Net - an online system for transliteration and script conversion for all major Indian language pairs (306 pairs). The system covers 13 Indo-Aryan languages, 4 Dravidian languages and English. For training the transliteration systems, we mined parallel transliteration corpora from parallel translation corpora using an unsupervised method and trained statistical transliteration systems using the mined corpora. Languages which do not have parallel corpora are supported by transliteration through a bridge language. Our script conversion system supports conversion between all Brahmi-derived scripts as well as ITRANS romanization scheme. For this, we leverage co-ordinated Unicode ranges between Indic scripts and use an extended ITRANS encoding for transliterating between English and Indic scripts. The system also provides top-k transliterations and simultaneous transliteration into multiple output languages. We provide a Python as well as REST API to access these services. The API and the mined transliteration corpus are made available for research use under an open source license."
W14-5105,Supertag Based Pre-ordering in Machine Translation,2014,16,1,2,0,13898,rajen chatterjee,Proceedings of the 11th International Conference on Natural Language Processing,0,None
W14-3308,The {IIT} {B}ombay {H}indi-{E}nglish Translation System at {WMT} 2014,2014,17,9,4,0,36370,piyush dungarwal,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"In this paper, we describe our EnglishHindi and Hindi-English statistical systems submitted to the WMT14 shared task. The core components of our translation systems are phrase based (Hindi-English) and factored (English-Hindi) SMT systems. We show that the use of number, case and Tree Adjoining Grammar information as factors helps to improve English-Hindi translation, primarily by generating morphological inflections correctly. We show improvements to the translation systems using pre-procesing and post-processing components. To overcome the structural divergence between English and Hindi, we preorder the source side sentence to conform to the target language word order. Since parallel corpus is limited, many words are not translated. We translate out-of-vocabulary words and transliterate named entities in a post-processing stage. We also investigate ranking of translations from multiple systems to select the best translation."
W14-1708,Tuning a Grammar Correction System for Increased Precision,2014,14,6,1,1,290,anoop kunchukuttan,Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task,0,"In this paper, we propose two enhancements to a statistical machine translation based approach to grammar correction for correcting all error categories. First, we propose tuning the SMT systems to optimize a metric more suited to the grammar correction task (F- score) rather than the traditional BLEU metric used for tuning language translation tasks. Since the F- score favours higher precision, tuning to this score can potentially improve precision. While the results do not indicate improvement due to tuning with the new metric, we believe this could be due to the small number of grammatical errors in the tuning corpus and further investigation is required to answer the question conclusively. We also explore the combination of custom-engineered grammar correction techniques, which are targeted to specific error categories, with the SMT based method. Our simple ensemble methods yield improvements in recall but decrease the precision. Tuning the custom-built techniques can help in increasing the overall accuracy also."
kunchukuttan-etal-2014-shata,Shata-Anuvadak: Tackling Multiway Translation of {I}ndian Languages,2014,11,15,1,1,290,anoop kunchukuttan,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present a compendium of 110 Statistical Machine Translation systems built from parallel corpora of 11 Indian languages belonging to both Indo-Aryan and Dravidian families. We analyze the relationship between translation accuracy and the language families involved. We feel that insights obtained from this analysis will provide guidelines for creating machine translation systems of specific Indian language pairs. We build phrase based systems and some extensions. Across multiple languages, we show improvements on the baseline phrase based systems using these extensions: (1) source side reordering for English-Indian language translation, and (2) transliteration of untranslated words for Indian language-Indian language translation. These enhancements harness shared characteristics of Indian languages. To stimulate similar innovation widely in the NLP community, we have made the trained models for these language pairs publicly available."
khapra-etal-2014-transliteration,"When Transliteration Met Crowdsourcing : An Empirical Study of Transliteration via Crowdsourcing using Efficient, Non-redundant and Fair Quality Control",2014,20,4,3,0.312016,3162,mitesh khapra,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Sufficient parallel transliteration pairs are needed for training state of the art transliteration engines. Given the cost involved, it is often infeasible to collect such data using experts. Crowdsourcing could be a cheaper alternative, provided that a good quality control (QC) mechanism can be devised for this task. Most QC mechanisms employed in crowdsourcing are aggressive (unfair to workers) and expensive (unfair to requesters). In contrast, we propose a low-cost QC mechanism which is fair to both workers and requesters. At the heart of our approach, lies a rule based Transliteration Equivalence approach which takes as input a list of vowels in the two languages and a mapping of the consonants in the two languages. We empirically show that our approach outperforms other popular QC mechanisms ({\textbackslash}textit{viz.}, consensus and sampling) on two vital parameters : (i) fairness to requesters (lower cost per correct transliteration) and (ii) fairness to workers (lower rate of rejecting correct answers). Further, as an extrinsic evaluation we use the standard NEWS 2010 test set and show that such quality controlled crowdsourced data compares well to expert data when used for training a transliteration engine."
W13-3611,{IITB} System for {C}o{NLL} 2013 Shared Task: A Hybrid Approach to Grammatical Error Correction,2013,7,5,1,1,290,anoop kunchukuttan,Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task,0,"We describe our grammar correction system for the CoNLL-2013 shared task. Our system corrects three of the five error types specified for the shared task noun-number, determiner and subject-verb agreement errors. For noun-number and determiner correction, we apply a classification approach using rich lexical and syntactic features. For subject-verb agreement correction, we propose a new rulebased system which utilizes dependency parse information and a set of conditional rules to ensure agreement of the verb group with its subject. Our system obtained an F-score of 11.03 on the official test set using the M 2 evaluation method (the official evaluation method)."
P13-4030,{T}rans{D}oop: A Map-Reduce based Crowdsourced Translation for Complex Domain,2013,11,2,1,1,290,anoop kunchukuttan,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"Large amount of parallel corpora is required for building Statistical Machine Translation (SMT) systems. We describe the TransDoop system for gathering translations to create parallel corpora from online crowd workforce who have familiarity with multiple languages but are not expert translators. Our system uses a Map-Reduce-like approach to translation crowdsourcing where sentence translation is decomposed into the following smaller tasks: (a) translation of constituent phrases of the sentence; (b) validation of quality of the phrase translations; and (c) composition of complete sentence translations from phrase translations. TransDoop incorporates quality control mechanisms and easy-to-use worker user interfaces designed to address issues with translation crowdsourcing. We have evaluated the crowdxe2x80x99s output using the METEOR metric. For a complex domain like judicial proceedings, the higher scores obtained by the map-reduce based approach compared to complete sentence translation establishes the efficacy of our work."
W12-5906,Partially modelling word reordering as a sequence labelling problem,2012,10,3,1,1,290,anoop kunchukuttan,Proceedings of the Workshop on Reordering for Statistical Machine Translation,0,"Source side reordering has been shown to improve the performance of phrase based machine translation systems. In this work, we explore the learning of source side reordering given a training corpus of word aligned data. Given the large number of re-orderings this problem is NP-hard. We explore the possibility of representing the problem as a reordering of word sequences, instead of words. To this end, we propose a sequence labelling framework to identify work sequences. We also model the reversal of word sequences as a sequence labelling problem. These transformations reduce the problem to a phrase reordering problem, which has a smaller search space."
kunchukuttan-etal-2012-experiences,Experiences in Resource Generation for Machine Translation through Crowdsourcing,2012,21,9,1,1,290,anoop kunchukuttan,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The logistics of collecting resources for Machine Translation (MT) has always been a cause of concern for some of the resource deprived languages of the world. The recent advent of crowdsourcing platforms provides an opportunity to explore the large scale generation of resources for MT. However, before venturing into this mode of resource collection, it is important to understand the various factors such as, task design, crowd motivation, quality control, etc. which can influence the success of such a crowd sourcing venture. In this paper, we present our experiences based on a series of experiments performed. This is an attempt to provide a holistic view of the different facets of translation crowd sourcing and identifying key challenges which need to be addressed for building a practical crowdsourcing solution for MT."
