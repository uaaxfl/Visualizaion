2004.jeptalnrecital-long.32,J93-2004,0,0.0234603,"Missing"
2004.jeptalnrecital-long.32,C96-2120,0,0.0690649,"Missing"
2007.jeptalnrecital-poster.24,abeille-etal-2000-building,0,0.129185,"Missing"
2007.jeptalnrecital-poster.24,H91-1060,0,0.0349069,"Missing"
2007.jeptalnrecital-poster.24,E03-1085,1,0.899581,"Missing"
2007.jeptalnrecital-poster.24,J93-2004,0,0.0276908,"Missing"
2007.jeptalnrecital-poster.24,C96-2120,0,0.100692,"Missing"
2007.jeptalnrecital-poster.24,paroubek-etal-2006-data,1,0.859225,"Missing"
2007.jeptalnrecital-poster.24,roark-etal-2006-sparseval,0,0.0289417,"Missing"
2007.jeptalnrecital-poster.24,vilnat-etal-2004-ongoing,1,0.89069,"Missing"
2010.jeptalnrecital-court.26,P05-2008,0,0.203777,"Missing"
2011.jeptalnrecital-long.29,W10-0216,0,0.0314078,"Missing"
2011.jeptalnrecital-long.29,S07-1094,0,0.0783176,"Missing"
2011.jeptalnrecital-long.29,N10-1120,0,0.0577944,"Missing"
2011.jeptalnrecital-long.29,pak-paroubek-2010-twitter,1,0.29659,"Missing"
2011.jeptalnrecital-long.29,P10-1141,0,0.0325042,"Missing"
2011.jeptalnrecital-long.29,P04-1035,0,0.0503339,"Missing"
2011.jeptalnrecital-long.29,W02-1011,0,0.0194824,"Missing"
2011.jeptalnrecital-long.29,paroubek-etal-2010-annotations,1,0.84204,"Missing"
2015.jeptalnrecital-long.24,J90-1003,0,0.540887,"Missing"
2015.jeptalnrecital-long.24,fraisse-paroubek-2014-toward,1,0.8913,"Missing"
2015.jeptalnrecital-long.24,S12-1033,0,0.0606554,"Missing"
2015.jeptalnrecital-long.24,2010.jeptalnrecital-court.26,1,0.855712,"Missing"
2015.jeptalnrecital-long.24,paroubek-etal-2010-annotations,1,0.886119,"Missing"
2015.jeptalnrecital-long.24,W13-1602,0,0.0356488,"Missing"
2015.jeptalnrecital-long.24,P05-2008,0,0.164364,"Missing"
2015.jeptalnrecital-long.24,roberts-etal-2012-empatweet,0,0.0471187,"Missing"
2018.jeptalnrecital-deft.1,2018.jeptalnrecital-deft.12,0,0.102356,"(negative, neutral, positive, mixed), to identify clues of sentiment and target, and to annotate each tweet in terms of source and target concerning all expressed sentiments. Twelve teams participated, mainly on the two first tasks. On the identification of tweets about public transportation, micro F-measure values range from 0.827 to 0.908. On the identification of the global polarity, micro F-measure values range from 0.381 to 0.823. M OTS - CLÉS : Classification automatique, Analyse de sentiments, Fouille de texte. K EYWORDS: Automatic Classification, Sentiment Analysis, Text Mining. c ATALA 2018 219 1 Introduction Dans la continuité de l’édition 2015 (Hamon et al., 2015), la treizième édition du DÉfi Fouille de Textes (DEFT) porte sur l’extraction d’information et l’analyse de sentiments dans des tweets rédigés en français sur la thématique des transports. La campagne s’est déroulée sur une période limitée avec une ouverture des inscriptions le 31 janvier, la diffusion des données d’entraînement à partir du 19 février, et le déroulement de la phase de test entre le 28 mars et le 5 avril, sur une durée de trois jours fixée par chacun des participants. Quinze équipes se sont inscrites,"
2020.bionlp-1.5,I17-2052,0,0.0212824,"outcomes (Koroleva, 2019a). We compared a rule-based system and several machine learning algorithms for primary and reported outcome extraction. Details about the annotated datasets and the methods that we tested can be found in Koroleva et al. (EasyChair, 2020). We selected the best performing approach to be included in our tool For primary outcomes extraction, the best performance was demonstrated by the BioBERT model Identification of sections in the abstract For identifying sections within the abstract (in particular, Results and Conclusions), we used the PubMed 200k dataset introduced in Dernoncourt and Lee (2017). This dataset contains approximately 200,000 abstracts of RCTs with 2.3 million sentences. Each sentence is annotated with one of the following classes, corresponding to the sections of the abstract: background, objective, method, result, or conclusion. We used the train-dev-test split provided by the developers of the dataset. 55 6 fine-tuned for named entity recognition task on our corpus of 2,000 sentences annotated for primary outcomes. For reported outcomes extraction, the best performance was achieved by the SciBERT model fine-tuned for named entity recognition task on our corpus of 1,9"
2020.bionlp-1.5,W19-5038,1,0.827395,"ing markers of statistical significance from the corpus annotated with reported outcomes. We annotated the pairs of outcomes and significance levels with a binary label (“positive”: the significance level is related to the outcome; “negative”: the significance level is not related to the outcome). The final corpus contains 663 sentences with 2,552 annotated relations (Koroleva, 2019c). We tested several machine learning algorithms and the BERT, SciBERT and BioBERT model finetuned for the relation extraction task on the annotated corpus. The details on the corpus and the method can be found in Koroleva and Paroubek (2019). The best result for this task was achieved by the fine-tuned BioBERT model. 56 Figure 1: Example of a processed text types), application (promoting the tool among the target audience; encouraging users to submit their manually annotated data, to be used to improve the algorithms), and optimization (parallel processing of multiple input text files). Our system can be easily incorporated into other text processing tools. Another interesting yet challenging direction for the future work is detecting spin/distorted reporting in texts belonging to scientific domains other than biomedicine. First"
2020.bionlp-1.5,P17-4002,0,0.0655193,"Missing"
2020.eamt-1.57,N19-1423,0,0.00800429,"of the project, a connection to eTranslation,8 an online machine translation service provided by the European Commission, will be established to foster the provision of multilingual datasets by public administrations that may in turn improve the coverage and quality of machine translation systems. 2 Approach At its core, the MAPA anonymisation toolkit will rely on Named Entity Recognition and Classification (NERC) techniques using neural networks and deep learning techniques. The latest deep learning architectures and the availability of pre-trained multilingual language models, such as BERT (Devlin et al., 2019) have pushed the state of the art in NERC to new levels of performance. In addition, thanks to the transfer learning capabilities shown by this type of deep learning models, new systems can be trained using smaller datasets of manually labelled data, and the knowledge acquired for a given domain or language can be reused in a cross-domain or cross-language setting (Garc´ıa-Pablos et al., 2020). MAPA will leverage the most innovative technology to provide robust models for the 24 official European languages, trained to detect named entities that involve sensitive information, depending on the a"
2020.eamt-1.57,2020.lrec-1.552,1,0.874506,"Missing"
2020.lrec-1.275,S17-2089,0,0.044971,"Missing"
2020.lrec-1.275,N19-1423,0,0.0176955,"be as modular as possible in order to allow for maximum reuse in different tasks pertaining to Economics, Finance and Regulation. After presenting existing resources, we relate the construction of the DoRe corpus and the rationale behind our choices, concluding on the spectrum of possible uses of this new resource for NLP applications. Keywords: Corpus, French, Finance, Annual Reports 1. Introduction Since the emergence of context insensitive neural wordembeddings (Mikolov et al., 2013) and the revolution of context sensitive pre-trained language models like ELMO (Peters et al., 2018), BERT (Devlin et al., 2019) and XLNET (Yang et al., 2019), Natural Language Processing (NLP) is gaining more and more popularity as the number of profitable use-cases in various industries identified by both research & private sectors is increasing. Because of the intrinsic language complexity due to its high combinatorics, the reason behind the progress of State-Of-The-Art pretrained models (Shoeybi et al., 2019) is often the availability of more training data and the corresponding computing power. Those models are also often trained with accessible general Internet Data as Wikipedia, offering wide vocabularies and top"
2020.lrec-1.275,D14-1148,0,0.0145512,"al., 2013; Devlin et al., 2019; Peters et al., 2018; Yang et al., 2019) or specialized (Lee et al., 2019; Beltagy et al., 2019) corpora lowers the barrier-to-entry required to reach a minimal performance for many semantic tasks (Devlin et al., 2019). Marketing, Financial, Economic and Regulatory sectors are no strangers to this NLP research interest trend, opening a whole new research area (Lou, 2019; Hiew et al., 2019). Different domain-specific corpora were built those last years, such as Corporate Annual Report Corpora (Kogan et al., 2009; H¨andschke et al., 2018), Financial News Corpora (Ding et al., 2014), Financial Twitter based Corpora (Malo et al., 2013; Cortis et al., 2017). The vast majority being in English, there is a lack of domain specific corpora in other languages, including French. Multilingual models and Zero-shot learning still lacking the accuracy of monolingual language models, there is a Financial domain research interest in building domain-specific corpora for different languages; or even better, parallel corpora, which would then enable gauging the impact the characteristics of a given language have on task performance. As in the JOCo corpus (H¨andschke et al., 2018) and the"
2020.lrec-1.275,P18-1031,0,0.0146946,"boratory from CNRS associated with Paris-Saclay University. Baseline Language Model Annual Reporting of companies being highly regulated and at the responsibility of these companies, their distribution is limited to themselves and the responsible Regulation Authority by Intellectual Property Rights (IPR). We then can’t openly distribute the DoRe Corpus except for Research and Educational Purposes3 . As a substitute, we release a Language Model of Financial and Economics jargon. PDF to TXT conversion being subject to mistakes, we chose to train word embeddings using ULMFit from Fastai toolkit (Howard and Ruder, 2018) due to its intrinsic capability to handle out-of-vocabulary tokens and then to be less sensible to typos in plain text. Based on AWD-LSTM model architecture (Stephen et al. 2017), our model ran for 5 epochs with batch size of 128 on the clean DoRe distribution. We obtain a model perplexity of 21.6753. To briefly present our model, we show some examples from a sentence completion task for general risk sentences. Next words predictions are made using Beam Search algorithm with a 20 beam width parameter. Table 2. shows 3 Table 2: Sentence completion examples. Please contact us at {corentin.masso"
2020.lrec-1.275,N18-5017,0,0.32018,"Missing"
2020.lrec-1.275,N18-1202,0,0.0171829,". This corpus is designed to be as modular as possible in order to allow for maximum reuse in different tasks pertaining to Economics, Finance and Regulation. After presenting existing resources, we relate the construction of the DoRe corpus and the rationale behind our choices, concluding on the spectrum of possible uses of this new resource for NLP applications. Keywords: Corpus, French, Finance, Annual Reports 1. Introduction Since the emergence of context insensitive neural wordembeddings (Mikolov et al., 2013) and the revolution of context sensitive pre-trained language models like ELMO (Peters et al., 2018), BERT (Devlin et al., 2019) and XLNET (Yang et al., 2019), Natural Language Processing (NLP) is gaining more and more popularity as the number of profitable use-cases in various industries identified by both research & private sectors is increasing. Because of the intrinsic language complexity due to its high combinatorics, the reason behind the progress of State-Of-The-Art pretrained models (Shoeybi et al., 2019) is often the availability of more training data and the corresponding computing power. Those models are also often trained with accessible general Internet Data as Wikipedia, offeri"
2020.lrec-1.275,N09-1031,0,0.676093,"of word embeddings pre-trained on huge amounts of generic (Mikolov et al., 2013; Devlin et al., 2019; Peters et al., 2018; Yang et al., 2019) or specialized (Lee et al., 2019; Beltagy et al., 2019) corpora lowers the barrier-to-entry required to reach a minimal performance for many semantic tasks (Devlin et al., 2019). Marketing, Financial, Economic and Regulatory sectors are no strangers to this NLP research interest trend, opening a whole new research area (Lou, 2019; Hiew et al., 2019). Different domain-specific corpora were built those last years, such as Corporate Annual Report Corpora (Kogan et al., 2009; H¨andschke et al., 2018), Financial News Corpora (Ding et al., 2014), Financial Twitter based Corpora (Malo et al., 2013; Cortis et al., 2017). The vast majority being in English, there is a lack of domain specific corpora in other languages, including French. Multilingual models and Zero-shot learning still lacking the accuracy of monolingual language models, there is a Financial domain research interest in building domain-specific corpora for different languages; or even better, parallel corpora, which would then enable gauging the impact the characteristics of a given language have on tas"
2020.lrec-1.275,J93-2004,0,0.0704624,"ARs. Metadata was collected on Euronext using ISIN codes as IDs for each company, allowing merges with others Datasets about these Corpus Exploration Corpus Analysis Based on the cleaned text version of the corpus, we used NLTK.org tools to count tokens and sentences for all the reports, grouped by Indexes. As depicted in Table 1. our corpus currently sums up to 2350 corporate annual reports with a vast majority being from France because of a bigger Financial market. Our corpus contains more than 257M tokens (5.7M sentences), compared to 4,5M, 30M, 282M and 46B for respectively Penn Treebank (Marcus et al., 1993), TRC2-financial (Thomson Reuters Text Research Collection), JOCO Corpus (H¨andschke et al., 2018) and OSCAR’s French subset (Su´arez et al., 2019). As shown in Table 1 there is a difference between lengths of ARs between countries in the DoRe corpus coming from different regulation policies. All European Financial Authorities follows the European Securities Markets Authority (ESMA) guidelines and apply them to their own country with their own interpretation & freedom. French Financial 2263 Index Tokens Sent. CAC40 CAC60 CAC90 BEL20 BELMid BELSmall Total 78 044 159 60 550 457 88 552 491 11 599"
2021.econlp-1.1,P11-1016,0,0.130872,"Missing"
2021.econlp-1.1,C18-2002,0,0.0608202,"Missing"
2021.econlp-1.1,S17-2089,0,0.0509352,"Missing"
2021.econlp-1.1,D09-1019,0,0.0637386,"ssion order implies that the latter, i.e., new information, is more salient. Therefore we only extract opinions expressed inside the effect part of a causal relation. A low degree of belief is, in our standpoint, explicit speculation about the future. People are less inclined to make decisions based upon opinions with a low degree of conviction; thus, these opinions are less likely to be the main driver of market dynamics. Conditional Opinions 4 We also distinguish conditional opinions. In a conditional sentence, opinion expressions can be challenging to determine due to the condition clause (Narayanan et al., 2009). For example, in the sentence, ""If rents fail to keep pace with inflation, the requirement for higher yields will drive down real assets prices."", the author is pessimistic about ""real asset prices"" under the stated hypothetical scenario, but does not express an opinion on ""rents"". However, if we remove ""if"" in this sentence, the first clause becomes negative. It is crucial not to misinterpret the author’s intent. To do so, we want to enable our algorithm to recognize conditional sentences. Therefore, in addition to annotating opinions inside the main clause, we label the span of words corres"
2021.econlp-1.1,N19-1423,0,0.00493271,"the Reuters Corpus11 , which is composed of Markets & Finance news of 90 categories12 . 14,035 sentences were used for training, 3,250 for development and 3,453 for evaluation. We have experimented with four NER systems, first three open-source libraries: SpaCy13 , Stanza (Qi et al., 2020)14 , and FLERT (Schweter and Akbik, 2020). In the FLERT model, firstly a transformer is fine-tuned on the NER task, and then the resulting features are provided to a BiLSTM-CRF (Huang et al., 2015) sequence labeling architecture. We also modeled a transformer with self-attention heads using BERT embeddings (Devlin et al., 2019) to predict named entities. Model SpaCy Stanza BERT FLERT [LOC] 0.71 0.92 0.93 0.98 [ORG] 0.23 0.74 0.87 0.88 [PER] 0.67 0.89 0.97 0.97 [MISC] / 0.85 0.95 0.92 Time (min) 0.19 7.55 8.07 19.81 Table 2 F1-scores and computation time of the test set of CoNLL 2003 dataset From Table 2, we conclude that classification precision is proportionate with computation cost. FLERT performs the best across all classes, its improvement on the class [LOC] with respect to BERT (the second-best model) are significant. Nonetheless, its computation costs is much higher than our BERT model. For this reason, we use"
2021.econlp-1.1,S14-2004,0,0.606328,"ask 1 is the first corpus that labels opinion polarities together with its targets from sentences of microblog and headlines. Nevertheless, its size is relatively small (1,313 samples), and it contains only relatively short sentences2 . In the texts in which we are interested, sentences written by financial experts are likely to be more complex; as detailed in Table 12, the average sentence length from different texts sources is about 30 tokens, and the most extended sentence is 258. As of the time of writing, among the analysis tasks, usually known as Aspect-Based Sentiment Analysis (ABSA) ((Pontiki et al., 2014), (Nazir et al., 2020)), there is only one economic news article related study on Target-Based Opinion Analysis (TBOA) (Barbaglia et al., 2020), which focuses on six macroeconomic aggregates. However, the dataset is not publicly available, and details regarding the statistics of this corpus are not specified. We have created our corpus to respond to the need for TBOA corpus in economy and finance by incorporating argumentative and conditional opinions and opinions expressed inside a consequence or explicit speculation about the future. We are interested in combining two subtasks of ABSA (Ponti"
2021.econlp-1.1,2020.acl-demos.14,0,0.0201695,"pus. • Tweets10 : Social media messages of 19 domain experts. 4.2 of-the-art models’ performance for this task (see section 4.3). In additional to [LOC], [ORG], [PER] classes, [MISC] (miscellaneous) is the category of words derived from location (e.g., Italian), organization, person. Sentences of this dataset are taken from the Reuters Corpus11 , which is composed of Markets & Finance news of 90 categories12 . 14,035 sentences were used for training, 3,250 for development and 3,453 for evaluation. We have experimented with four NER systems, first three open-source libraries: SpaCy13 , Stanza (Qi et al., 2020)14 , and FLERT (Schweter and Akbik, 2020). In the FLERT model, firstly a transformer is fine-tuned on the NER task, and then the resulting features are provided to a BiLSTM-CRF (Huang et al., 2015) sequence labeling architecture. We also modeled a transformer with self-attention heads using BERT embeddings (Devlin et al., 2019) to predict named entities. Model SpaCy Stanza BERT FLERT [LOC] 0.71 0.92 0.93 0.98 [ORG] 0.23 0.74 0.87 0.88 [PER] 0.67 0.89 0.97 0.97 [MISC] / 0.85 0.95 0.92 Time (min) 0.19 7.55 8.07 19.81 Table 2 F1-scores and computation time of the test set of CoNLL 2003 dataset Fr"
2021.eval4nlp-1.1,Q17-1010,0,0.00626055,"token is associated to one true label and named entities are encoded according to the BIO (begin, inside, outside) scheme. In the present work we deal with tokens rather than entities, so that we can apply the presented method directly. We consider that ‘O’ labels are negatives and that all other labels are positives. A true positive system prediction is an association between an input token and a non-‘O’ label that is the gold-standard label for this token. We are comparing entity detection systems that rely on word embeddings based upon CharacterBert (El Boukkouri et al., 2020) or fastText (Bojanowski et al., 2017), pre-trained on different corpora, either as-is or concatenated with knowledge embeddings learned using node2vec (Grover and Leskovec, 2016) on two biomedical vocabularies (the Medical Suject Headings (MeSH), and SNOMED CT). Moreover, we also consider a variant of CharacterBert where the node2vec embeddings are injected within the model architecture. The fastText embeddings are either randomly initialized, which we note “fastTextRandom”; pre-trained on a newswire corpus (Gigaword (Graff et al., 2007)), which we note “fastTextGigaword”; or on medical corpora (PubMed Central3 and MIMIC-III (Joh"
2021.eval4nlp-1.1,2020.coling-main.609,1,0.733787,"Missing"
2021.eval4nlp-1.1,2020.emnlp-main.393,0,0.03534,"sign choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrated by Peyrard et al. (2021) who discuss the use of the average to aggregate evaluation scores. They show that very different system behaviors can yield similar scores when using the average and suggest an alternative aggregation mechanism. Some researchers also call for going beyond performance scores: Ethayarajh and Jurafsky (2020) suggest that performance-based evaluation (as promoted by leaderboards) overlooks aspects such as utility, prediction cost, and robustness of models. They recommend considering the point of view of the user of models rather than just performance scores to estimate their relevance. Trying to provide a finer understanding of the issues raised by the input text and of the limitations of the evaluated systems, we propose a new qualitative analysis method that takes into account the observed relative difficulty of predicting gold labels for each input. This difficulty is assessed pragmatically bas"
2021.eval4nlp-1.1,C96-1079,0,0.860475,"ipating in a multi-label text classification task (CLEF eHealth 2018 ICD-10 coding), and a comparison of neural models trained for biomedical entity detection (BioCreative V chemical-disease relations dataset). 1 Introduction The analysis of NLP system results has mainly focused on evaluation scores meant to rank systems and feed leaderboards. In tasks such as information extraction, text classification, etc., evaluation generally relies on the comparison of a hypothesis (typically a system output) with a gold standard, generally produced through manual annotation. Since the MUC-6 conference (Grishman and Sundheim, 1996), the metrics used were created for information retrieval (Cleverdon, 1960): recall (true positive rate), precision (positive predictive value) and their harmonic (possibly weighted) mean, the F1score. Evaluation scripts are widely available nowadays, for instance those of the CoNLL shared tasks (Tjong Kim Sang and De Meulder, 2003). These scripts rely on an annotation scheme based on the 1 Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP 2021), pages 1–10 c November 10, 2021. 2021 Association for Computational Linguistics Figure 1: Example input file for a"
2021.eval4nlp-1.1,W16-2703,0,0.017816,"r outside of an annotation span, making it a de facto standard for NER evaluation (Nadeau and Sekine, 2007). Many other NLP tasks have developed or used their own metrics, such as accuracy for classification, BLEU (Papineni et al., 2002) for machine translation, ROUGE for machine translation and text summarization (Lin, 2004), word error rate for automatic speech recognition, etc. While evaluation is the key step in shared tasks, developers also need to evaluate the performance of their systems for feature selection or architecture design choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrated by Peyrard et al. (2021) who discuss the use of the average to aggregate evaluation scores. They show that very different system behaviors can yield similar scores when using the average and suggest an alternative aggregation mechanism. Some researchers also call for going beyond performance scores: Ethayarajh and Jurafsky (2020) suggest that performance-based evaluation (as pr"
2021.eval4nlp-1.1,W04-1013,0,0.0384163,"o Processing Lucie Gianola, Hicham El Boukkouri, Cyril Grouin, Thomas Lavergne, Patrick Paroubek, Pierre Zweigenbaum Université Paris-Saclay, CNRS, LISN, 91405, Orsay, France firstname.lastname@lisn.fr Abstract BIO prefix used to specify whether a token is at the beginning, inside or outside of an annotation span, making it a de facto standard for NER evaluation (Nadeau and Sekine, 2007). Many other NLP tasks have developed or used their own metrics, such as accuracy for classification, BLEU (Papineni et al., 2002) for machine translation, ROUGE for machine translation and text summarization (Lin, 2004), word error rate for automatic speech recognition, etc. While evaluation is the key step in shared tasks, developers also need to evaluate the performance of their systems for feature selection or architecture design choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrated by Peyrard et al. (2021) who discuss the use of the average to aggregate evaluation scor"
2021.eval4nlp-1.1,P02-1040,0,0.112402,"on: a Qualitative Analysis of Natural Language Processing System Behavior Based Upon Data Resistance to Processing Lucie Gianola, Hicham El Boukkouri, Cyril Grouin, Thomas Lavergne, Patrick Paroubek, Pierre Zweigenbaum Université Paris-Saclay, CNRS, LISN, 91405, Orsay, France firstname.lastname@lisn.fr Abstract BIO prefix used to specify whether a token is at the beginning, inside or outside of an annotation span, making it a de facto standard for NER evaluation (Nadeau and Sekine, 2007). Many other NLP tasks have developed or used their own metrics, such as accuracy for classification, BLEU (Papineni et al., 2002) for machine translation, ROUGE for machine translation and text summarization (Lin, 2004), word error rate for automatic speech recognition, etc. While evaluation is the key step in shared tasks, developers also need to evaluate the performance of their systems for feature selection or architecture design choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrate"
2021.eval4nlp-1.1,2021.acl-long.179,0,0.0278283,"machine translation, ROUGE for machine translation and text summarization (Lin, 2004), word error rate for automatic speech recognition, etc. While evaluation is the key step in shared tasks, developers also need to evaluate the performance of their systems for feature selection or architecture design choices, especially when several systems are combined (Jiang et al., 2016). However, scores only are insufficient to capture the behavior of systems and to provide a finergrained analysis of their pros and cons. Indeed, though widely used, scores are not free of imperfections, as demonstrated by Peyrard et al. (2021) who discuss the use of the average to aggregate evaluation scores. They show that very different system behaviors can yield similar scores when using the average and suggest an alternative aggregation mechanism. Some researchers also call for going beyond performance scores: Ethayarajh and Jurafsky (2020) suggest that performance-based evaluation (as promoted by leaderboards) overlooks aspects such as utility, prediction cost, and robustness of models. They recommend considering the point of view of the user of models rather than just performance scores to estimate their relevance. Trying to"
2021.fnp-1.11,2020.acl-demos.14,0,0.0277217,"Missing"
2021.fnp-1.11,S17-2089,0,0.0232449,"ce §Paris-Saclay University, France §LISN, Bât 507, Rue du Belvedère, 91400 Orsay, France *jiahui.hu@student-cs.fr †pap@limsi.fr Abstract 1 Introduction The processing of information is crucial in determining financial assets’ prices. Thus, market participants’ opinions can be an essential driver of price dynamics. Recent progress of NLP technologies and access to digitized texts have facilitated automatic sentiment analysis of financial narratives. Most existing corpora for opinion analysis applied to economy and finance focus on the sentence level polarity (Malo et al., 2013) or text level (Cortis et al., 2017), leaving aside opinion targets. (Barbaglia et al., 2020) created a corpus focusing on the polarity of six macroeconomic aggregates, but it is not publicly available as of the time of writing. The corpus of FiQA task 1 1 for fine-grained opinion analysis of news headlines and tweets is relatively small (1,313 samples) for training supervised learning models, and it contains relatively short sentences. In the texts we will analyze, the sentences are generally longer. Therefore, we introduce a corpus consisting of labels annotated at the intra-sentential level by humans and algorithms to fill th"
2021.fnp-1.11,2020.coling-main.69,0,0.0260259,"ructure and word classes of OEEs can be valuable clues for determining where their corresponding target can be found. For example, for a unigram OEE whose word class is adjective, its target is likely to be the noun that follows because adjectives precede the noun they modify in English. Following this idea, we manually examine 30 sentence of our corpus whose OEEs are in the form of verb+adp and find that most TOI precedes this type of OEEs, but some exceptions can be found in OEEs with the adp ""to"". Similarly, targets are very likely to be announced before OEEs ""remain adj"". Recent studies ((Huang et al., 2020),(Zhao et al., 2021)) have proposed integrating syntax-related information in graph neural networks (GNN) or using GNN for sequence labelling by propagating the labelling information from known to unknown rules (which can be any rules, including syntactic ones). In the future, we want to study how these mechanism can be exploited to analyze our corpus. 3.5 Figure 3: Dependency tree of ""This fragmentation increases cost and reduces the possibility of economies of scale."" When it comes to dependency relations on the top of OEEs (posterior_OEE), we can find noun subjects, they can be the receive"
2021.fnp-1.11,C18-2002,0,0.0658693,"Missing"
2021.fnp-1.2,W17-3518,0,0.0190298,"that a handful of such intentions are sufficient to describe a data-driven narrative in a speciality domain, such as Finance. For instance, an intention DescribeValue is a sentence stating the value of a financial indicator at a precise time and an intention DescribeVariation is a sentence describing the variation in time of a financial indicator’s value. We use the prefix Merge to define a sentence composed of two or more intentions. In order to identify and extract these intentions in our corpus, a Ruta grammar (KLUEGL et al., 2016) was created to automate the triples extraction, imitating Gardent et al. (2017) data modeling. This grammar first uses dictionaries as well as POS-tag patterns to identify financial key elements related to these intentions and characterize them into one of the following categories: • financial indicator • reference (time and geographical element) • measure • predicate Indicators, measures and dimensions are generic elements that can be found in all intentions. Predicates, on the other hand, vary according to the intention. To create a dictionary that take into account this specificity and can later be used for intention detection, we conducted a manual analysis of the ma"
2021.fnp-1.2,2020.inlg-1.9,0,0.461309,"Dominique Mariko * dmariko @yseop.com Danxin Cui * danxin.cui @sorbonne-nouvelle.fr * equal contribution Estelle Labidurie elabidurie @yseop.com Hugues de Mazancourt hdemazancourt @yseop.com Abstract In this paper we present experiments to evaluate how a T5 model behaves with regard to input data fidelity. The rationale behind these experiments is to evaluate if a sequence to sequence transformer can be constrained into generating the specifics of a financial report, and more generally whether it can trustfully reproduce a semantic logic, and to what extent. 1 Introduction T5 by Raffel et al. (2020) recently demonstrated strong constrained and data to text generation capabilities. Experiments have been lead on AQG tasks (Grover et al. (2021)) and on the WebNLG dataset as to explore the data to text capabilities of a T5 model. In particular, Kale and Rastogi (2020) demonstrates T5 model shows interesting capacities in generalization to new domains and relations, and Kasner and Dusek (2020) proposes significant text generation experiment even without any in-domain examples. Text generation in Finance can be very demanding as to the level of constraint a neural model should comply with. The"
2021.jeptalnrecital-taln.13,P19-1258,0,0.0304597,"Missing"
2021.jeptalnrecital-taln.13,N19-1423,0,0.00896013,"Missing"
2021.jeptalnrecital-taln.13,W19-3646,0,0.0619439,"Missing"
2021.jeptalnrecital-taln.13,W19-5932,0,0.0622926,"Missing"
2021.jeptalnrecital-taln.13,2020.acl-main.54,0,0.0861342,"Missing"
2021.jeptalnrecital-taln.13,P19-1358,0,0.0627263,"Missing"
2021.jeptalnrecital-taln.13,W14-4337,0,0.0876161,"Missing"
2021.jeptalnrecital-taln.13,2020.acl-main.428,0,0.0719233,"Missing"
2021.jeptalnrecital-taln.13,J06-3004,0,0.208014,"Missing"
2021.jeptalnrecital-taln.13,P19-1289,0,0.0616478,"Missing"
2021.jeptalnrecital-taln.13,P18-1136,0,0.050132,"Missing"
2021.jeptalnrecital-taln.13,2020.coling-main.40,0,0.0419678,"Missing"
2021.jeptalnrecital-taln.13,P19-1363,0,0.0619111,"Missing"
2021.jeptalnrecital-taln.13,2020.emnlp-main.66,0,0.0392676,"Missing"
2021.jeptalnrecital-taln.13,P19-1426,0,0.0499665,"Missing"
A92-1030,W90-0203,0,0.0424511,"Missing"
A92-1030,C88-2121,1,0.931373,"Missing"
A92-1030,1991.iwpt-1.4,1,0.808348,"Missing"
A92-1030,W90-0102,1,0.901398,"Missing"
A92-1030,C90-3045,1,0.900667,"Missing"
A92-1030,C88-2147,0,0.0976216,"Missing"
A92-1030,C90-3001,1,\N,Missing
A92-1030,C92-3145,1,\N,Missing
adda-decker-etal-2008-annotation,J96-2004,0,\N,Missing
asadullah-etal-2014-bidirectionnal,candito-etal-2010-statistical,0,\N,Missing
asadullah-etal-2014-bidirectionnal,C10-2013,0,\N,Missing
asadullah-etal-2014-bidirectionnal,villemonte-de-la-clergerie-etal-2008-passage,1,\N,Missing
asadullah-etal-2014-bidirectionnal,vilnat-etal-2010-passage,1,\N,Missing
asadullah-etal-2014-bidirectionnal,W06-2920,0,\N,Missing
asadullah-etal-2014-bidirectionnal,W08-1301,0,\N,Missing
asadullah-etal-2014-bidirectionnal,abeille-barrier-2004-enriching,0,\N,Missing
asadullah-etal-2014-bidirectionnal,paroubek-etal-2006-data,1,\N,Missing
devillers-etal-2004-french,H92-1003,0,\N,Missing
devillers-etal-2004-french,P01-1066,0,\N,Missing
devillers-etal-2004-french,antoine-etal-2002-predictive,1,\N,Missing
devillers-etal-2004-french,antoine-etal-2000-obtaining,1,\N,Missing
E03-1085,abeille-etal-2000-building,0,0.648194,"ny segmentation chosen by a parser to be converted into our formalism. For the same reason, the information that is not expressed in the constituents is expressed through a large number of functional relations: twelve in all. Such formalism is closer to a dependency-based formalism than to a constituent based formalism (Sleator and Temperley, 1991). It neither prevents the ""deep"" parsers to be evaluated, nor disadvantages them, but the transcription of their parses could be more complex. The six types of chunks and twelve functional relations are given in table 1. They were mainly inspired by Abeille et al. (2000), and have been adapted while annotating corpus excerpts. Chunks Functional relations NV — verbal subject-verb GN — nominal auxiliary-verb GR — adverbial argument-verb GA — adjectival modifier-verb GP — prepositional introducing a nominal phrase modifier-noun PV — prepositional introducing a verbal phrase modifier-adverb modifier-adjective attribute-subject/object Coordination Apposition Complementer 96 No clausal or sentential segmentation is identified, because as in a dependency-based formalism, the complex structure of the sentence is obtained through the whole chain of relations. The foll"
E03-1085,gendner-etal-2002-protocol,1,0.794438,"Missing"
F13-2011,galibert-etal-2012-extended,0,0.0275754,"Missing"
F13-2011,W11-0411,0,0.0247796,"Missing"
F13-2011,P11-1015,0,0.0272691,"Missing"
F13-2011,P10-1141,0,0.0544817,"Missing"
F13-2011,paroubek-etal-2010-annotations,1,0.869652,"Missing"
F13-2022,abeille-etal-2000-building,0,0.25588,"Missing"
F13-2022,W06-2920,0,0.0567215,"Missing"
F13-2022,C10-2013,0,0.0544182,"Missing"
F13-2022,F12-2024,0,0.0225171,"Missing"
F13-2022,N04-1013,0,0.0893033,"Missing"
F13-2022,villemonte-de-la-clergerie-etal-2008-passage,1,0.754008,"Missing"
F13-2022,W08-1301,0,0.12586,"Missing"
F13-2022,W03-2401,0,0.0846126,"Missing"
F13-2022,paroubek-etal-2006-data,1,0.872343,"Missing"
F13-2022,vilnat-etal-2010-passage,1,0.863273,"Missing"
fraisse-paroubek-2014-toward,paroubek-etal-2010-annotations,1,\N,Missing
gendner-etal-2002-protocol,paroubek-2000-language,1,\N,Missing
gendner-etal-2002-protocol,mengel-lezius-2000-xml,0,\N,Missing
gendner-etal-2002-protocol,J93-2004,0,\N,Missing
gendner-etal-2002-protocol,A97-1012,0,\N,Missing
gendner-etal-2002-protocol,C92-3129,0,\N,Missing
gendner-etal-2002-protocol,C96-2120,0,\N,Missing
gendner-etal-2002-protocol,H94-1020,0,\N,Missing
gendner-etal-2002-protocol,P01-1040,0,\N,Missing
gendner-etal-2002-protocol,lenci-etal-2000-opposites,0,\N,Missing
L16-1052,drouin-2004-detection,0,0.112328,"Missing"
L16-1052,mariani-etal-2014-rediscovering,1,0.715644,"Missing"
L16-1052,W12-3208,0,\N,Missing
L16-1298,clough-etal-2002-building,0,0.163903,"Missing"
L16-1298,barron-cedeno-etal-2010-corpus,0,0.0398857,"Missing"
L16-1298,D14-1153,0,0.0465381,"Missing"
L16-1298,R11-1102,0,0.0314,"Missing"
L16-1298,P02-1020,0,0.124436,"Missing"
L16-1298,J13-4005,0,0.0296008,"Missing"
L16-1298,R09-1011,0,0.0731076,"Missing"
L16-1298,W12-3208,0,0.0470348,"Missing"
L16-1298,C10-1048,0,0.051923,"Missing"
L16-1298,W01-0515,0,0.224865,"Missing"
L16-1298,vilnat-etal-2010-passage,1,0.858327,"Missing"
L16-1298,C10-2115,0,\N,Missing
L16-1298,S12-1008,0,\N,Missing
L18-1080,P14-1095,0,0.0132843,"econdary result) In the rest of this paper, we present our linguistic model of spin (section 2), the annotation scheme (section 3), the the annotation guidelines (section 4), conclusions and plans for future work (section 5). 2. Model of spin To the best of our knowledge, this is the first attempt at addressing the analysis of spin in the biomedical literature from a Natural Language Processing point of view. Spin detection overlaps partially with previous works in NLP, in particular objectivity/subjectivity identification (Wiebe 504 et al. 2005), sentiment analysis (Pak 2012), fact checking (Nakashole & Mitchell 2014) or deception detection (Hancock et al. 2010; Litvinova et al. 2017); a point to note is that these works address texts of general domain while we deal with spin in biomedical texts. We regard spin detection as a task most closely related to deception detection. Deception is defined as a deliberate act of communicating information that the speaker/author believes to be false, with the intention to induce listeners/readers to believe a distorted presentation of the topic. Strictly speaking, spin is not necessarily a form of deception, as the intention is difficult to establish most of the time,"
L18-1297,drouin-2004-detection,0,0.142057,"Missing"
L18-1297,R09-1061,0,0.0597835,"Missing"
mariani-etal-2014-facing,councill-etal-2008-parscit,0,\N,Missing
mariani-etal-2014-facing,calzolari-etal-2012-lre,1,\N,Missing
pak-paroubek-2010-twitter,H05-1044,0,\N,Missing
pak-paroubek-2010-twitter,P05-2008,0,\N,Missing
pak-paroubek-2010-twitter,W02-1011,0,\N,Missing
pak-paroubek-2010-twitter,A00-2009,0,\N,Missing
paroubek-2000-language,C94-1097,0,\N,Missing
paroubek-2000-language,P98-2164,0,\N,Missing
paroubek-2000-language,C98-2159,0,\N,Missing
paroubek-etal-2006-data,J93-2004,0,\N,Missing
paroubek-etal-2006-data,W04-2703,0,\N,Missing
paroubek-etal-2006-data,J05-1004,0,\N,Missing
paroubek-etal-2006-data,vilnat-etal-2004-ongoing,1,\N,Missing
paroubek-etal-2006-data,H91-1060,0,\N,Missing
paroubek-etal-2008-easy,paroubek-2000-language,1,\N,Missing
paroubek-etal-2008-easy,J93-2004,0,\N,Missing
paroubek-etal-2008-easy,vilnat-etal-2004-ongoing,1,\N,Missing
paroubek-etal-2008-easy,chaudiron-mariani-2006-techno,0,\N,Missing
paroubek-etal-2008-easy,paroubek-etal-2006-data,1,\N,Missing
paroubek-etal-2010-annotations,kamps-etal-2004-using,0,\N,Missing
paroubek-etal-2010-annotations,N06-1026,0,\N,Missing
paroubek-etal-2010-annotations,N06-4006,0,\N,Missing
paroubek-etal-2010-annotations,W08-1204,1,\N,Missing
paroubek-etal-2010-annotations,W03-1017,0,\N,Missing
paroubek-etal-2010-annotations,W06-1652,0,\N,Missing
paroubek-etal-2010-annotations,P94-1019,0,\N,Missing
paroubek-etal-2010-annotations,P02-1053,0,\N,Missing
paroubek-etal-2010-annotations,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
paroubek-tannier-2012-rough,chiao-etal-2006-evaluation,0,\N,Missing
paroubek-tannier-2012-rough,villemonte-de-la-clergerie-etal-2008-passage,1,\N,Missing
paroubek-tannier-2012-rough,J93-2004,0,\N,Missing
paroubek-tannier-2012-rough,W08-2319,0,\N,Missing
paroubek-tannier-2012-rough,vilnat-etal-2010-passage,1,\N,Missing
paroubek-tannier-2012-rough,M95-1005,0,\N,Missing
paroubek-tannier-2012-rough,magnini-etal-2008-evaluation,0,\N,Missing
paroubek-tannier-2012-rough,paroubek-etal-2006-data,1,\N,Missing
paroubek-tannier-2012-rough,pak-paroubek-2010-twitter,1,\N,Missing
paroubek-tannier-2012-rough,W10-4233,0,\N,Missing
paroubek-tannier-2012-rough,W05-1517,0,\N,Missing
S10-1097,W02-1011,0,0.0176197,"ish evenly split between negative and positive classes. The collected texts were processed as follows to obtain a set of n-grams: P (s|M ) = 1. Filtering – we remove URL links (e.g. http://example.com), Twitter user names (e.g. 5 P (s) · P (M |s) P (M ) P (M |s) P (M ) (2) 6 An abbreviation for retweet, which means citation or reposting of a message http://en.wikipedia.org/wiki/Emoticon#Asian style 437 P (s|M ) ∼ P (M |s) (3) micro accuracy We train Bayes classifiers which use a presence of an n-grams as a binary feature. We have experimented with unigrams, bigrams, and trigrams. Pang et al. (Pang et al., 2002) reported that unigrams outperform bigrams when doing sentiment classification of movie reviews, but Dave et al. (Dave et al., 2003) have obtained contrary results: bigrams and trigrams worked better for the product-review polarity classification. We tried to determine the best settings for our microblogging data. On the one hand high-order n-grams, such as trigrams, should capture patterns of sentiments expressions better. On the other hand, unigrams should provide a good coverage of the data. Therefore we combine three classifiers that are based on different n-gram orders (unigrams, bigrams"
S10-1097,H05-1044,0,0.085847,"a bag of words. For English, we kept short forms as a single word: “don’t”, “I’ll”, “she’d”. 3. Stopwords removal – in English, texts we removed articles (“a”, “an”, “the”) from the bag of words. 4. N-grams construction – we make a set of ngrams out of consecutive words. A negation particle is attached to a word which precedes it and follows it. For example, a sentence “I do not like fish” will form three bigrams: “I do+not”, “do+not like”, “not+like fish”. Such a procedure improves the accuracy of the classification since the negation plays a special role in opinion and sentiment expression (Wilson et al., 2005). In English, we used negative particles ’no’ and ’not’. In Chinese, we used the following particles: 1. 不 – is not + noun 2. 未 – does not + verb, will not + verb 3. 莫 (別) – do not (imperative) 4. 無 (沒有) – does not have 3 Our method 3.1 3.2 Classifier Corpus collection We build a sentiment classifier using the multinomial Na¨ıve Bayes classifier which is based on Bayes’ theorem. Using Twitter API we collected a corpus of text posts and formed a dataset of two classes: positive sentiments and negative sentiments. We queried Twitter for two types of emoticons considering eastern and western type"
S10-1097,S10-1014,0,0.0195649,"classifies the meaning of adjectives into positive or negative sentiment polarity according to the given context. Our approach is fully automatic. It does not require any additional hand-built language resources and it is language independent. 1. In the first one, we collected Chinese texts from Twitter and used them to train a classifier to annotate the test dataset. 1 Introduction 2. In the second one, we used machine translator to translate the dataset from Chinese to English and annotated it using collected English texts from Twitter as the training data. The dataset of the SemEval task (Wu and Jin, 2010) consists of short texts in Chinese containing target adjectives whose sentiments need to be disambiguated in the given contexts. Those adjectives are: 大 big, 小 small, 多 many, 少 few, 高 high, 低 low, 厚 thick, 薄 thin, 深 deep, shallow, 重 heavy, light, 巨大 huge, 重大 grave. Disambiguating sentiment ambiguous adjectives is a challenging task for NLP. Previous studies were mostly focused on word sense disambiguation rather than sentiment disambiguation. Although both problems look similar, the latter is more challenging in our opinion because impregnated with more subjectivity. In order to solve the tas"
S10-1097,P05-2008,0,\N,Missing
S10-1097,pak-paroubek-2010-twitter,1,\N,Missing
villemonte-de-la-clergerie-etal-2008-passage,paroubek-2000-language,1,\N,Missing
villemonte-de-la-clergerie-etal-2008-passage,paroubek-etal-2008-easy,1,\N,Missing
villemonte-de-la-clergerie-etal-2008-passage,2006.iwslt-papers.1,0,\N,Missing
villemonte-de-la-clergerie-etal-2008-passage,vilnat-etal-2004-ongoing,1,\N,Missing
villemonte-de-la-clergerie-etal-2008-passage,vanrullen-etal-2006-constraint,0,\N,Missing
villemonte-de-la-clergerie-etal-2008-passage,paroubek-etal-2006-data,1,\N,Missing
villemonte-de-la-clergerie-etal-2008-passage,galliano-etal-2006-corpus,0,\N,Missing
vilnat-etal-2004-ongoing,kingsbury-palmer-2002-treebank,0,\N,Missing
vilnat-etal-2004-ongoing,J93-2004,0,\N,Missing
vilnat-etal-2010-passage,W03-2401,0,\N,Missing
vilnat-etal-2010-passage,villemonte-de-la-clergerie-etal-2008-passage,1,\N,Missing
vilnat-etal-2010-passage,W08-1301,0,\N,Missing
vilnat-etal-2010-passage,P04-1041,0,\N,Missing
vilnat-etal-2010-passage,J07-3004,0,\N,Missing
vilnat-etal-2010-passage,bosco-lombardo-2006-comparing,0,\N,Missing
vilnat-etal-2010-passage,declerck-2006-synaf,0,\N,Missing
vilnat-etal-2010-passage,paroubek-etal-2006-data,1,\N,Missing
W03-2802,antoine-etal-2002-predictive,1,0.891043,"Missing"
W03-2802,bonneau-maynard-etal-2000-predictive,0,0.0581338,"Missing"
W03-2802,J96-2004,0,0.137079,"Missing"
W03-2802,devillers-etal-2002-annotations,1,0.918558,"re of dialog. Consequently to these shortcomings, researchers are often unable to provide principled design and system capabilities for technology transfer. In other research areas, such as speech recognition and information retrieval, common reference tasks have been highly effective in sharing research costs and efforts. A similar development is highly needed in the dialog community. In this contribution which addresses only a part of the SLDS evaluation problem, a paradigm for evaluating the context-sensitive understanding capability of any spoken language dialog system is proposed. PEACE (Devillers et al., 2002a) described in section 3, is based on test sets extracted from real corpora, and has three main aspects: it is generic, contextual and it offers diagnostic capabilities. Here genericity is envisaged in a context of information dialogs access. The diagnostic aspect is important in order to determine the different qualities of the systems under test. The contextual aspect of evaluation is a crucial point since dialog is dynamic by nature. We propose to simulate/synthesize the contextual information. The PEACE paradigm will be tested in the French Technolangue MEDIA project and will serve as bas"
W03-2802,geoffrois-etal-2000-transcribing,0,0.0141776,"xtracted from real corpus. For dialog system diagnosis, it is also crucial to build test sets labeled with the linguistic phenomena and dialogic functions. Thus, the capabilities of system’s contextual understanding can be assessed for the main linguistic and dialogic difficulties such as, for instance, anaphora or ellipsis resolution. 3.1.4 A data structuring method Two types of units, one for literal understanding (LU), the other for contextual understanding (CU) are defined. The format of the annotated data will be adapted to language resource standard annotations implemented in XML, e.g. (Geoffrois et al., 2000), (Ide and Romary, 2002). Each unit is extracted from a real dialog corpus. LU units are composed of the user query, the corresponding audio signal, an automatic transcription obtained with a recognition system, and finally the literal semantic representation of the utterance (see Figure 1). CU units are composed of Context paraphrase (LU) AVR User query (LU) AVR (CU) AVR je voudrais un hˆotel 4 e´ toiles dans le neuvi`eme I would like a 4 category hotel in the ninth (+, argument, hotel) (+, district, 9) (+, category, 4) la mˆeme cat´egorie dans un autre arrondissement the same category in ano"
W03-2802,H92-1003,0,0.0449057,"Missing"
W03-2802,H94-1022,0,0.0374573,"test set) and finally how to restrict and organize the language phenomena used in the test set. 3 The PEACE paradigm We first describe the paradigm and relate preliminary experiments with PEACE. This paradigm which is as basement for the MEDIA project will be refined by all the partners and use for an evaluation campaign between seven systems of industrial and academic sites. 3.1 Description The PEACE paradigm relies on the idea that for database querying tasks, it is possible to define a common semantic representation, onto which all the systems are able to convert their own representation (Moore, 1994). The paradigm based on data extracted from real corpus, includes both literal and contextual understanding test sets. More precisely, it provides:  the definition of a semantic representation (see 3.1.1),  the definition of a model for dialogic contexts (see 3.1.2),  the definition and typology of linguistic phenomena and dialogic functions used to selectively diagnoze the system language capabilities (anaphora resolution, constraints relaxation, etc.) (see 3.1.3),  a data structuring method. The format of the annotated data will be adapted to language resource standard annotations implem"
W03-2802,P01-1066,0,\N,Missing
W03-2802,J98-3008,0,\N,Missing
W08-1204,2005.iwslt-1.1,0,0.014556,"nder the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. 1 See http://deft.limsi.fr/ for a presentation in French. 2 http://trec.nist.gov http://www.clef-campaign.org 4 http://www.technolangue.net 3 17 Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 17–23 Manchester, August 2008 ISLE6 projects. For cost-efficiency reasons, automatic evaluation is relevant, and its results have sometimes been compared to results from human judges. For instance, Eck and Hori (2005) compare results of evaluation measurements used in automatic translation with human judgments on the same corpus. In (Burstein and Wolska, 2003), the authors describe an experiment in the evaluation of writing style and find a better agreement between the automatic evaluation system and one human judge, than between two human judges. Returning to the DEFT campaign, once the task is chosen, the corpora are collected, and evaluation measurements are defined, there can remain some necessity of adjusting parameters, according to the expected difficulty of the task. This could be, for instance, th"
W08-1204,W01-1614,0,0.032139,"ree judges who took part in the test, the first and third one agree well with themselves, while for the second one, the agreement is only moderate. • 5 values for film and book reviews8 (a mark between 0 and 4, from bad to excellent) ; • 20 values for video game reviews9 (a global mark calculated from a set of advices about various aspects of the game: graphics, playability, life span, sound track and scenario). In order to, first, assess the feasibility of the task, and to, secondly, define the scale of values to be used in the evaluation campaign, we submitted human judges to several tests (Paek, 2001): they were instructed to assign a mark on two kinds of scale, a wide one with the original values, and a restricted one with 2 or 3 values, depending on the corpus it was applying to. The results from various judges were evaluated in terms of precision and recall, and matched to each other by way of the Kappa coefficient (Carletta, 1996) (Cohen, 1960). We present hereunder the values of the κ coefficient between pairs of human judges, and with the reference, on the video game corpus. The wide scale (Table 1) uses the original values (marks between 0 and 20), while the restricted scale (Table"
W08-1204,J96-2004,0,0.241768,"pects of the game: graphics, playability, life span, sound track and scenario). In order to, first, assess the feasibility of the task, and to, secondly, define the scale of values to be used in the evaluation campaign, we submitted human judges to several tests (Paek, 2001): they were instructed to assign a mark on two kinds of scale, a wide one with the original values, and a restricted one with 2 or 3 values, depending on the corpus it was applying to. The results from various judges were evaluated in terms of precision and recall, and matched to each other by way of the Kappa coefficient (Carletta, 1996) (Cohen, 1960). We present hereunder the values of the κ coefficient between pairs of human judges, and with the reference, on the video game corpus. The wide scale (Table 1) uses the original values (marks between 0 and 20), while the restricted scale (Table 2) relies upon 3 values with following definitions: class 0 for original marks between 0 and 10, class 1 for marks between 11 and 14, and class 2 for marks between 15 and 20. Judge 1 2 3 1 0.74 2 3 0.46 0.70 Table 3: Video game corpus: agreement of each judge with himself when scales change. 7 http://www.assemblee-nationale.fr/12/ debats/"
W08-1204,W01-0902,0,\N,Missing
W08-1306,P04-1041,0,0.0689989,"Missing"
W08-1306,P02-1022,0,0.0996386,"Missing"
W08-1306,declerck-2006-synaf,0,0.0295328,"s for French, whether such annotations come from human annotators or parsers. The representation format is intended to be used both in the evaluation of different parsers, so the parses’ representations should be easily comparable, and in the construction of a large scale annotation treebank which requires that all French constructions can be represented with enough details. The format is based on three distinct specifications and requirements: 1. MAF (ISO 24611)4 and SynAF (ISO 24615)5 which are the ISO TC37 specifications for morpho-syntactic and syntactic annotation (Ide and Romary, 2002) (Declerck, 2006) (Francopoulo, 2008). Let us note that these specifications cannot be called ”standards” because they are work in progress and these documents do not yet have the status Published Standard. Currently, their official status is only Committee Draft. 1. Parsing creates syntactic annotations; 2. Syntactic annotations create or enrich linguistic resources such as lexicons, grammars or annotated corpora; 2. The format used during the previous TECHNOLANGUE/EASY evaluation campaign in order to minimize porting effort for the existing tools and corpora. 3. Linguistic resources created or enriched on th"
W08-1306,paroubek-2000-language,1,0.855159,"Missing"
W08-1306,E03-1085,1,0.757565,"l ) includes a verb, the clitic pronouns and possible particles attached to it. Verb kernels may have different forms: conjugated tense, present or past participle, or infinitive. When the conjugation produces compound forms, distinct NVs are identified; element gathers tokens, word forms, groups, relations and marks and all sentences are included inside a “Document” element. 3 PASSAGE Syntactic Annotation Specification 3.1 Introduction The annotation formalism used in PASSAGE7 is based on the EASY one(Vilnat et al., 2004) which whose first version was crafted in an experimental project PEAS (Gendner et al., 2003), with inspiration taken from the propositions of (Carroll et al., 2002). The definition has been completed with the input of all the actors involved in the EASY evaluation campaign (both parsers’ developers and corpus providers) and refined with the input of PASSAGE participants. This formalism aims at making possible the comparison of all kinds of syntactic annotation (shallow or deep parsing, complete or partial analysis), without giving any advantage to any particular approach. It has six kinds of syntactic “chunks”, we call constituents and 14 kinds of relations The annotation formalism a"
W08-1306,J07-3004,0,0.0539813,"Missing"
W08-1306,vilnat-etal-2004-ongoing,1,0.694807,"Data Category Registry, inist.fr 38 see http://syntax. • the verb kernel (NV for noyau verbal ) includes a verb, the clitic pronouns and possible particles attached to it. Verb kernels may have different forms: conjugated tense, present or past participle, or infinitive. When the conjugation produces compound forms, distinct NVs are identified; element gathers tokens, word forms, groups, relations and marks and all sentences are included inside a “Document” element. 3 PASSAGE Syntactic Annotation Specification 3.1 Introduction The annotation formalism used in PASSAGE7 is based on the EASY one(Vilnat et al., 2004) which whose first version was crafted in an experimental project PEAS (Gendner et al., 2003), with inspiration taken from the propositions of (Carroll et al., 2002). The definition has been completed with the input of all the actors involved in the EASY evaluation campaign (both parsers’ developers and corpus providers) and refined with the input of PASSAGE participants. This formalism aims at making possible the comparison of all kinds of syntactic annotation (shallow or deep parsing, complete or partial analysis), without giving any advantage to any particular approach. It has six kinds of"
W08-1306,2006.iwslt-papers.1,0,0.0438108,"Missing"
W08-1306,villemonte-de-la-clergerie-etal-2008-passage,1,0.766148,"Missing"
W08-1306,paroubek-etal-2008-easy,1,\N,Missing
W08-1306,paroubek-etal-2006-data,1,\N,Missing
W12-1101,S10-1004,0,0.173659,"Missing"
W12-1101,W12-1102,0,0.0186244,"Missing"
W14-6301,W14-6306,0,0.0610287,"Missing"
W14-6301,W14-6304,0,0.0536407,"Missing"
W14-6301,W14-6308,0,0.035181,"Missing"
W16-1509,barron-cedeno-etal-2010-corpus,0,0.0546065,"Missing"
W16-1509,J13-4005,0,0.0362311,"Missing"
W16-1509,D14-1153,0,0.043852,"Missing"
W16-1509,R09-1011,0,0.0756892,"Missing"
W16-1509,R11-1102,0,0.0447661,"Missing"
W16-1509,clough-etal-2002-building,0,0.16174,"Missing"
W16-1509,councill-etal-2008-parscit,0,0.113433,"Missing"
W16-1509,W12-3208,0,0.0468919,"Missing"
W16-1509,C10-1048,0,0.0351019,"Missing"
W16-1509,W01-0515,0,0.219072,"Missing"
W16-1509,C10-2115,0,0.0761366,"Missing"
W16-1509,vilnat-etal-2010-passage,1,0.852446,"Missing"
W16-4711,ahtaridis-etal-2012-ldc,0,0.0194146,"us recorded in the ACL Anthology (Bird et al., 2008). Various studies, based on the same corpus followed, for instance (Bordea et al., 2014) on trend analysis and resulted in systems such as Saffron or the Michigan University web site. Other studies were conducted specifically on the speech-related ISCA archive (Mariani et al., 2013), and on the LREC archives (Mariani et al., 2016). More focused on resource usage is the study conducted by the Linguistic Data Consortium (LDC) team whose goal was, and still is, to build a language resource (LR) database documenting the use of the LDC resources (Ahtaridis et al., 2012). 3 Corpus The corpus NLP4NLP2 is made of the largest possible selection of NLP papers from conferences and journals, covering written, speech and for a limited part, sign language processing sub-domains; reaching out to a limited number of sub-corpora for which Information Retrieval and NLP activities intersect, reflecting the fact that we use NLP methods to process NLP content. It currently contains 65,003 documents coming from various conferences and journals. This is a large part of the existing published articles in our field, apart from workshop proceedings and published books. Despite t"
W16-4711,councill-etal-2008-parscit,0,0.0520858,"Missing"
W16-4711,drouin-2004-detection,0,0.0702855,"Missing"
W19-5038,W17-2302,0,0.0219105,"backend (Abadi et al., 2016). Parallel convolutional layers process the input, using sequence windows centered around the candidate entity, relation or event. Vector space embeddings are built for input tokens, including features such as word vectors, POS, entity features, relative position, etc. The system was tested on several tasks and showed improved performance and good generalizability. plex relations, such as biomolecular events. A common feature of the works in this domain, noted by (Zhou et al., 2014; Lever and Jones, 2017) and still relevant for recent works e.g. (Peng and Lu, 2017; Asada et al., 2017), consists in assuming that entities of interest are already extracted and provided to the relation extraction system as input. Thus, the relation extraction is assessed separately, without taking into account the performance of entity extraction. We adopt this approach for relation extraction evaluation in our work, but we provide separate assessment for our algorithms of entity extraction. One of the general frameworks for relation extraction in the biomedical domain is proposed by (Zhou et al., 2014). The authors suggest using trigger words to determine the type of a relation, noting that f"
W19-5038,D15-1162,0,0.0136274,"es and significance levels to detect those related to each other. As significance levels are not characterized by high variability, we follow the previous research in using rules (regular expressions and sequential rules using information from pos-tagging) to extract significance levels. We present our methods and results for outcome extraction in detail elsewhere, here we provide a brief summary. We tested several approaches: a baseline approach using sequential rules using information from pos-tagging; an approach using rules based on syntactic structure provided by spaCy dependency parser (Honnibal and Johnson, 2015); a combination of bi-LSTM, CNN and CRF using GloVe(Pennington et al., 2014) word embeddings and character-level representations (Ma and Hovy, 2016); and a fine-tuned bi-LSTM using BERT (Devlin et al., 2018) vector word representations. BERT (Bidirectional Encoder Representations from Transformers) is a recently introduced approach to pre-training language representations, using a masked language model (MLM) which randomly masks some input tokens, allowing to pre-train a deep bidirectional Transformer using both left and right context. The pre-trained BERT models can be fine-tuned for supervis"
W19-5038,W18-2311,0,0.0427508,"Missing"
W19-5038,W17-2322,0,0.0264765,"r event and relation extraction, using Keras (Chollet et al., 2015) with Tensorflow backend (Abadi et al., 2016). Parallel convolutional layers process the input, using sequence windows centered around the candidate entity, relation or event. Vector space embeddings are built for input tokens, including features such as word vectors, POS, entity features, relative position, etc. The system was tested on several tasks and showed improved performance and good generalizability. plex relations, such as biomolecular events. A common feature of the works in this domain, noted by (Zhou et al., 2014; Lever and Jones, 2017) and still relevant for recent works e.g. (Peng and Lu, 2017; Asada et al., 2017), consists in assuming that entities of interest are already extracted and provided to the relation extraction system as input. Thus, the relation extraction is assessed separately, without taking into account the performance of entity extraction. We adopt this approach for relation extraction evaluation in our work, but we provide separate assessment for our algorithms of entity extraction. One of the general frameworks for relation extraction in the biomedical domain is proposed by (Zhou et al., 2014). The autho"
W19-5038,P16-1101,0,0.0193282,"search in using rules (regular expressions and sequential rules using information from pos-tagging) to extract significance levels. We present our methods and results for outcome extraction in detail elsewhere, here we provide a brief summary. We tested several approaches: a baseline approach using sequential rules using information from pos-tagging; an approach using rules based on syntactic structure provided by spaCy dependency parser (Honnibal and Johnson, 2015); a combination of bi-LSTM, CNN and CRF using GloVe(Pennington et al., 2014) word embeddings and character-level representations (Ma and Hovy, 2016); and a fine-tuned bi-LSTM using BERT (Devlin et al., 2018) vector word representations. BERT (Bidirectional Encoder Representations from Transformers) is a recently introduced approach to pre-training language representations, using a masked language model (MLM) which randomly masks some input tokens, allowing to pre-train a deep bidirectional Transformer using both left and right context. The pre-trained BERT models can be fine-tuned for supervised downstream tasks by adding one output layer. BERT was trained on a dataset of 3.3B words combining English Wikipedia and BooksCorpus. Two domain-"
W19-5038,P16-1105,0,0.0631025,"Missing"
W19-5038,E17-1077,0,0.0386702,"Missing"
W19-5038,W17-2304,0,0.0165553,"5) with Tensorflow backend (Abadi et al., 2016). Parallel convolutional layers process the input, using sequence windows centered around the candidate entity, relation or event. Vector space embeddings are built for input tokens, including features such as word vectors, POS, entity features, relative position, etc. The system was tested on several tasks and showed improved performance and good generalizability. plex relations, such as biomolecular events. A common feature of the works in this domain, noted by (Zhou et al., 2014; Lever and Jones, 2017) and still relevant for recent works e.g. (Peng and Lu, 2017; Asada et al., 2017), consists in assuming that entities of interest are already extracted and provided to the relation extraction system as input. Thus, the relation extraction is assessed separately, without taking into account the performance of entity extraction. We adopt this approach for relation extraction evaluation in our work, but we provide separate assessment for our algorithms of entity extraction. One of the general frameworks for relation extraction in the biomedical domain is proposed by (Zhou et al., 2014). The authors suggest using trigger words to determine the type of a re"
W19-5038,D14-1162,0,0.0835521,"ce levels are not characterized by high variability, we follow the previous research in using rules (regular expressions and sequential rules using information from pos-tagging) to extract significance levels. We present our methods and results for outcome extraction in detail elsewhere, here we provide a brief summary. We tested several approaches: a baseline approach using sequential rules using information from pos-tagging; an approach using rules based on syntactic structure provided by spaCy dependency parser (Honnibal and Johnson, 2015); a combination of bi-LSTM, CNN and CRF using GloVe(Pennington et al., 2014) word embeddings and character-level representations (Ma and Hovy, 2016); and a fine-tuned bi-LSTM using BERT (Devlin et al., 2018) vector word representations. BERT (Bidirectional Encoder Representations from Transformers) is a recently introduced approach to pre-training language representations, using a masked language model (MLM) which randomly masks some input tokens, allowing to pre-train a deep bidirectional Transformer using both left and right context. The pre-trained BERT models can be fine-tuned for supervised downstream tasks by adding one output layer. BERT was trained on a datase"
