2020.acl-main.109,D17-1209,0,0.0188618,"d). The En preposition by is not considered a content word and is not aligned with the Ko verb gigohan. See § 3 for further details. Introduction The assumption that the syntactic structure of a sentence is predictably related to the syntactic structure of its translation has deep roots in NLP, notably in cross-lingual transfer methods, such as annotation projection and multi-lingual parsing (Hwa et al., 2005; McDonald et al., 2011; Kozhevnikov and Titov, 2013; Rasooli and Collins, 2017, inter alia), as well as in syntaxaware machine translation (MT; Birch et al., 2008; Williams et al., 2016; Bastings et al., 2017). Relatedly, typological parameters that provide information on the dimensions of similarity between grammars of different languages were found useful for a variety of NLP applications (Ponti et al., 2019). For example, neural MT in low-resource settings has been shown to benefit from bridg∗ Work mostly done while at the Hebrew University of Jerusalem. ing morphosyntactic differences in parallel training data by different types of preprocessing, such as reordering (Zhou et al., 2019) and hand-coded syntactic manipulations (Ponti et al., 2018). Nevertheless, little empirical work has been done"
2020.acl-main.109,W18-6008,0,0.0291681,"Missing"
2020.acl-main.109,D08-1078,0,0.0201662,"elcl+nsubj path (paths are given in bold red). The En preposition by is not considered a content word and is not aligned with the Ko verb gigohan. See § 3 for further details. Introduction The assumption that the syntactic structure of a sentence is predictably related to the syntactic structure of its translation has deep roots in NLP, notably in cross-lingual transfer methods, such as annotation projection and multi-lingual parsing (Hwa et al., 2005; McDonald et al., 2011; Kozhevnikov and Titov, 2013; Rasooli and Collins, 2017, inter alia), as well as in syntaxaware machine translation (MT; Birch et al., 2008; Williams et al., 2016; Bastings et al., 2017). Relatedly, typological parameters that provide information on the dimensions of similarity between grammars of different languages were found useful for a variety of NLP applications (Ponti et al., 2019). For example, neural MT in low-resource settings has been shown to benefit from bridg∗ Work mostly done while at the Hebrew University of Jerusalem. ing morphosyntactic differences in parallel training data by different types of preprocessing, such as reordering (Zhou et al., 2019) and hand-coded syntactic manipulations (Ponti et al., 2018). Nev"
2020.acl-main.109,W16-1715,0,0.0155263,"screpancy is due to the fact that Ko nearly 7 Cf. also 23 En nsubj edges mapped to Ru nsubj+obl. Inspection of these sentences reveals that the CLMD can be ascribed to metaphorical usage (e.g., the sense of read employed in the post reads has no direct correspondent in Ru). Some such cases can be disambiguated using existing annotation schemes. obligatorily adds contextually-predictable predicates to oblique relations such as actions [taken] in Crimea or people [being] without children. The Korean PUD does not demote these verbs to functional-word status (such an approach is advocated for in [Gerdes and Kahane, 2016]) but turns them into clause-heading verbs, thus yielding an acl+X divergence.8 Language ru 25 Thematic 78 Promotional 0 10 Demotional Structural 83 Conflational 10 nsubj+obj 8 Categorical nsubj+(i)obj/obl 51 #Sentences 995 Full nsubj to obj/obl fr 4 57 0 2 67 5 12 34 999 zh 8 43 0 4 17 5 23 25 999 ko 5 25 0 19 0 6 11 11 884 jp 5 53 0 1 35 2 4 13 999 Table 1: Prevalence in absolute counts of translation divergences as defined in Dorr (1994). Row headings are explained in § 6. 6 Revisiting Dorr’s Divergences We turn to show that the types of the seminal classification of divergences from Dorr"
2020.acl-main.109,W17-3209,0,0.109509,"h as adding a case ending. 2 The resource can be found at https://github. com/macleginn/exploring-clmd-divergences Fine-grained Classification of CLMD In this section, we present a novel cross-linguistic dataset that provides a high-resolution overview of morphosyntactic differences between pairs of languages and a formal definition of morphosyntactic divergences formulated based on it. Divergences in the syntax of sentences and their translations can stem from a number of reasons. Setting aside semantic divergences, which are differences in the content expressed by the source and the target (Carpuat et al., 2017; Vyas et al., 2018), the remaining varieties of divergences are essentially different ways to express the same content (Fisiak, 1984; Boas, 2010), which we call CLMD. We define CLMD empirically to be recurrent divergence patterns in the syntactic structures of sentences and their translations. While content 3 Their alignment process involved bottom-up and a topdown passes, sometimes yielding contradictory results. 1160 differences may account for some of the observed syntactic divergences, by aiming for recurring patterns we expect to filter out most such cases, as they are subject to fewer g"
2020.acl-main.109,N18-1104,0,0.0590334,"d propose a methodology that automatically derives fine-grained CLMD from aligned annotated corpora and enables straightforward computation of their type statistics. 2 3 Related Work Comparing syntactic and semantic structures over parallel corpora is the subject of much previous work. Dorr et al. (2010) compiled a multiplyparallel corpus and annotated it with increasingly refined categories in an attempt to abstract away from syntactic detail but did not report any systematic measurement of the distribution of divergences. Šindlerová et al. (2013), Xue et al. (2014), Sulem et al. (2015), and Damonte and Cohen (2018) studied divergences over semantic graphs and argument-structure phenomena, while a related line of work examined divergences in discourse phenomena (Šoštari´c et al., 2018). Other works studied the ability of a given grammar formalism to capture CLMD in a parallel corpus (e.g., object construction to an oblique construction, often involve morphological processes, such as adding a case ending. 2 The resource can be found at https://github. com/macleginn/exploring-clmd-divergences Fine-grained Classification of CLMD In this section, we present a novel cross-linguistic dataset that provides a hi"
2020.acl-main.109,J17-3002,0,0.294374,"Missing"
2020.acl-main.109,N19-1423,0,0.020749,"pplicability of our method for analyzing the performance of a downstream cross-lingual transfer task. We consider zero-shot cross-lingual parsing (Ammar et al., 2016; Schuster et al., 2019) as a test case and investigate to what extent the performance of a zeroshot parser on a given dependency label can be predicted from its stability in translation. As test sets we use the test sets of GSD UD corpora for the five languages (Ru, Fr, Zh, Ko, and Jp), as well as the corresponding PUD corpora. We train a parser following the setup of Mulcaire et al. (2019) and use a pretrained multilingual BERT (Devlin et al., 2019), feeding its output embeddings into a biaffine-attention neural UD parser (Dozat and Manning, 2017) trained on the English EWT corpus. We evaluate the parser’s ability to predict relation types by computing F-scores for each de9 http://www.statmt.org/wmt14/ These were defined here as edges with the following labels: root, nsubj, amod, nmod, advmod, nummod, acl, advcl, xcomp, compound, flat, obj, obl. 1165 10 pendency label (save for labels corresponding to function words that were generally not aligned). Appendix E gives full implementation details. We start by computing Spearman correlations"
2020.acl-main.109,W17-7622,0,0.090255,"Missing"
2020.acl-main.109,P11-1132,0,0.0234518,"the observed syntactic divergences, by aiming for recurring patterns we expect to filter out most such cases, as they are subject to fewer grammatical constraints and should thus not yield systematic patterns of morphosyntactic divergence. It is harder to distinguish between translation artifacts and CLMD in translated sentences that are due to the genuine differences between grammar and usage. However, translated texts are usually characterized by a higher degree of morphosyntactic transfer and rarely portray the target language as more different from the source language than it needs to be (Koppel and Ordan, 2011; Volansky et al., 2015). Therefore, we do not expect to find spurious recurrent morphosyntacticdivergence patterns introduced by the process of translation. 3.1 The Manually Aligned PUD Resource Universal Dependencies (UD) is a framework for treebank annotation, whose objectives include satisfactory analyses of individual languages, providing a suitable basis for bringing out crosslinguistic parallelism, suitability for rapid consistent annotation and accurate automatic parsing, ease of comprehension by non-linguists, and effective support for downstream tasks. See Appendix A for a glossary o"
2020.acl-main.109,P13-1117,0,0.0163583,"ompson Figure 1: An example En-Ko sentence pair exhibiting a divergence, where an En nmod path corresponds to a Ko acl:relcl+nsubj path (paths are given in bold red). The En preposition by is not considered a content word and is not aligned with the Ko verb gigohan. See § 3 for further details. Introduction The assumption that the syntactic structure of a sentence is predictably related to the syntactic structure of its translation has deep roots in NLP, notably in cross-lingual transfer methods, such as annotation projection and multi-lingual parsing (Hwa et al., 2005; McDonald et al., 2011; Kozhevnikov and Titov, 2013; Rasooli and Collins, 2017, inter alia), as well as in syntaxaware machine translation (MT; Birch et al., 2008; Williams et al., 2016; Bastings et al., 2017). Relatedly, typological parameters that provide information on the dimensions of similarity between grammars of different languages were found useful for a variety of NLP applications (Ponti et al., 2019). For example, neural MT in low-resource settings has been shown to benefit from bridg∗ Work mostly done while at the Hebrew University of Jerusalem. ing morphosyntactic differences in parallel training data by different types of preproc"
2020.acl-main.109,J94-4004,0,0.694389,"resource settings has been shown to benefit from bridg∗ Work mostly done while at the Hebrew University of Jerusalem. ing morphosyntactic differences in parallel training data by different types of preprocessing, such as reordering (Zhou et al., 2019) and hand-coded syntactic manipulations (Ponti et al., 2018). Nevertheless, little empirical work has been done on systematically quantifying the type and prevalence of syntactic divergences across languages. Moreover, previous work generally classified divergences into a small set of divergence classes, often based on theoretical considerations (Dorr, 1994) or on categorical (“hard”) typological features selected in an ad-hoc manner, and left basic questions, such as how often POS tags are preserved in translation and what syntactic structures are likely correspondents of different syntactic relations, largely unaddressed. See § 2. We propose a language-neutral, fine-grained definition of cross-linguistic morphosyntactic divergences (CLMD) that allows for their extraction using a syntactically annotated, content-wordaligned parallel corpus. Concretely, we classify CLMD based on the edge labels on the dependency paths between corresponding pairs"
2020.acl-main.109,D11-1006,0,0.110631,"Missing"
2020.acl-main.109,K19-1029,0,0.0218708,"pplicability for Zero-Shot Parsing We come to demonstrate the applicability of our method for analyzing the performance of a downstream cross-lingual transfer task. We consider zero-shot cross-lingual parsing (Ammar et al., 2016; Schuster et al., 2019) as a test case and investigate to what extent the performance of a zeroshot parser on a given dependency label can be predicted from its stability in translation. As test sets we use the test sets of GSD UD corpora for the five languages (Ru, Fr, Zh, Ko, and Jp), as well as the corresponding PUD corpora. We train a parser following the setup of Mulcaire et al. (2019) and use a pretrained multilingual BERT (Devlin et al., 2019), feeding its output embeddings into a biaffine-attention neural UD parser (Dozat and Manning, 2017) trained on the English EWT corpus. We evaluate the parser’s ability to predict relation types by computing F-scores for each de9 http://www.statmt.org/wmt14/ These were defined here as edges with the following labels: root, nsubj, amod, nmod, advmod, nummod, acl, advcl, xcomp, compound, flat, obj, obl. 1165 10 pendency label (save for labels corresponding to function words that were generally not aligned). Appendix E gives full implem"
2020.acl-main.109,L16-1262,0,0.0983045,"Missing"
2020.acl-main.109,J19-3005,0,0.0391105,"Missing"
2020.acl-main.109,P18-1142,0,0.0607311,"Missing"
2020.acl-main.109,W09-3805,0,0.0361287,"slate sequences of events, expressed in English using coordinating conjunctions, with subordinate clauses). See § 5. Further experiments demonstrate the methodology’s applicative potential. First, we show that the proposed methodology can be straightforwardly automated by replacing manual parses and alignments with automatically induced ones (§ 7). We present a study done on a larger En-Zh corpus, which yields results similar to those obtained manually. Secondly, we show that the reported distribution over divergence types is predictive of the performance patterns of a zero-shot parser (§ 8). Søgaard and Wu, 2009). However, none of these works defined a general methodology for extracting and classifying CLMD. The only previous work we are aware of to use UD for identifying CLMD is (Wong et al., 2017), which addresses Mandarin-Cantonese divergences by comparing the marginal distribution of syntactic categories on both sides (without alignment). Relatedly, Deng and Xue (DX17; 2017) aligned phrase structure trees over an EnZh parallel corpus. Notwithstanding the similarity in the general approach, we differ from DX17 in (i) specifically targeting content words, (ii) relying on UD, which is standardized cr"
2020.acl-main.109,W18-6305,0,0.101203,"Missing"
2020.acl-main.109,W15-3502,1,0.903352,"Missing"
2020.acl-main.109,K18-2016,0,0.019491,"ady made for En-Zh in DX17. It seems then that “hand-crafted” translation divergences, however insightful they may be, receive attention disproportionate to their empirical frequency. 7 Perspectives for Automation One of the strengths of our approach is that it only relies on UD parses and alignments, for which automatic tools exist in many languages. To demonstrate the feasibility of an automated protocol, we conducted an analysis of the WMT14 En-Zh News Commentary corpus.9 We used TsinghuaAligner (Liu and Sun, 2015) and pretrained English and Chinese UD parsers from the StanfordNLP toolkit (Qi et al., 2018). To verify that the aligner we are using is adequate for the task, we aligned the En-Zh PUD corpus pair and checked the resulting precision and recall of the edges corresponding to content words.10 The results (P=0.86, R=0.32) indicate that the automated approach is able to recover around a third of the information obtained through manual alignment with reasonable precision. Importantly, we find recall to be nearly uniform for all source edge types, which suggests that the low recall can be mitigated by using a larger corpus without biasing the results. The POS and edge-type confusion matrice"
2020.acl-main.109,N18-1136,0,0.167844,"Missing"
2020.acl-main.109,Q17-1020,0,0.0278913,"n-Ko sentence pair exhibiting a divergence, where an En nmod path corresponds to a Ko acl:relcl+nsubj path (paths are given in bold red). The En preposition by is not considered a content word and is not aligned with the Ko verb gigohan. See § 3 for further details. Introduction The assumption that the syntactic structure of a sentence is predictably related to the syntactic structure of its translation has deep roots in NLP, notably in cross-lingual transfer methods, such as annotation projection and multi-lingual parsing (Hwa et al., 2005; McDonald et al., 2011; Kozhevnikov and Titov, 2013; Rasooli and Collins, 2017, inter alia), as well as in syntaxaware machine translation (MT; Birch et al., 2008; Williams et al., 2016; Bastings et al., 2017). Relatedly, typological parameters that provide information on the dimensions of similarity between grammars of different languages were found useful for a variety of NLP applications (Ponti et al., 2019). For example, neural MT in low-resource settings has been shown to benefit from bridg∗ Work mostly done while at the Hebrew University of Jerusalem. ing morphosyntactic differences in parallel training data by different types of preprocessing, such as reordering"
2020.acl-main.109,W17-6530,0,0.0202277,"st, we show that the proposed methodology can be straightforwardly automated by replacing manual parses and alignments with automatically induced ones (§ 7). We present a study done on a larger En-Zh corpus, which yields results similar to those obtained manually. Secondly, we show that the reported distribution over divergence types is predictive of the performance patterns of a zero-shot parser (§ 8). Søgaard and Wu, 2009). However, none of these works defined a general methodology for extracting and classifying CLMD. The only previous work we are aware of to use UD for identifying CLMD is (Wong et al., 2017), which addresses Mandarin-Cantonese divergences by comparing the marginal distribution of syntactic categories on both sides (without alignment). Relatedly, Deng and Xue (DX17; 2017) aligned phrase structure trees over an EnZh parallel corpus. Notwithstanding the similarity in the general approach, we differ from DX17 in (i) specifically targeting content words, (ii) relying on UD, which is standardized cross-linguistically and allows to simplify the alignment process by focusing on the level of words,3 and (iii) addressing multiple language pairs. It should be noted that the classification o"
2020.acl-main.109,N19-1162,0,0.0172371,"gests that the low recall can be mitigated by using a larger corpus without biasing the results. The POS and edge-type confusion matrices built from this experiment are very similar to the ones reported in this paper (save for compound, which is not produced by the Stanford Zh parser), and are not reproduced here (they can be found in the Supplementary Materials). 8 Applicability for Zero-Shot Parsing We come to demonstrate the applicability of our method for analyzing the performance of a downstream cross-lingual transfer task. We consider zero-shot cross-lingual parsing (Ammar et al., 2016; Schuster et al., 2019) as a test case and investigate to what extent the performance of a zeroshot parser on a given dependency label can be predicted from its stability in translation. As test sets we use the test sets of GSD UD corpora for the five languages (Ru, Fr, Zh, Ko, and Jp), as well as the corresponding PUD corpora. We train a parser following the setup of Mulcaire et al. (2019) and use a pretrained multilingual BERT (Devlin et al., 2019), feeding its output embeddings into a biaffine-attention neural UD parser (Dozat and Manning, 2017) trained on the English EWT corpus. We evaluate the parser’s ability"
2020.acl-main.109,xue-etal-2014-interlingua,0,0.254079,"Missing"
2020.acl-main.109,D19-1143,0,0.0302258,"inter alia), as well as in syntaxaware machine translation (MT; Birch et al., 2008; Williams et al., 2016; Bastings et al., 2017). Relatedly, typological parameters that provide information on the dimensions of similarity between grammars of different languages were found useful for a variety of NLP applications (Ponti et al., 2019). For example, neural MT in low-resource settings has been shown to benefit from bridg∗ Work mostly done while at the Hebrew University of Jerusalem. ing morphosyntactic differences in parallel training data by different types of preprocessing, such as reordering (Zhou et al., 2019) and hand-coded syntactic manipulations (Ponti et al., 2018). Nevertheless, little empirical work has been done on systematically quantifying the type and prevalence of syntactic divergences across languages. Moreover, previous work generally classified divergences into a small set of divergence classes, often based on theoretical considerations (Dorr, 1994) or on categorical (“hard”) typological features selected in an ad-hoc manner, and left basic questions, such as how often POS tags are preserved in translation and what syntactic structures are likely correspondents of different syntactic"
2020.acl-main.559,N19-1358,0,0.0271589,"al., 2020). As in similar projects exploring embodied understanding (Pustejovsky and Krishnaswamy, 2016; Baldridge et al., 2018), new simulator frameworks must be developed. While full embodiment calls for multiple modalities, the degree to which it is required remains an important open question (Lupyan and Lewis, 2019). Accordingly, and for immediate applicability to purely textual NLU problems we propose also focusing on the simpler setting of interactive text (Nelson, 2005). Recent research on text-based games shows how agents can learn to “program” in such languages (Cˆot´e et al., 2019; Ammanabrolu and Riedl, 2019), and how real language understanding problems can be framed as executable semantic parsing using configurable text-based simulators (Tamari et al., 2019). In this setting, learning compiled knowledge is closely related to automated knowledge base construction (Winn et al., 2019) or frame induction from text (QasemiZadeh et al., 2019). Our proposed paradigm suggests enriching classic symbolic knowledge representations (Speer et al., 2017) to executable form (Tamari et al., 2020). Preliminary steps in this direction are seen in inferential knowledge bases such as ATOMIC (Sap et al., 2019), whic"
2020.acl-main.559,P17-1022,0,0.0238196,"signal, research shows that most of the relevant information required for understanding a linguistic message is not present in the words (Stolk et al., 2016; David et al., 2016). Accordingly, the ECL view suggests shifting the focus to the mental models that communicators use, and the neural mechanisms used to construct them, e.g., mental simulation. What follows here adapts a relevant cognitiveinspired framework from general AI to the present NLU setting (§5.1), and discusses computational challenges (§5.2). Note that similar insights have been applied to multi-agent communication problems (Andreas et al., 2017), but their application to general NLU has been limited. 5.1 Formal Framework The recently introduced Consciousness Prior (CP; Bengio, 2017) is a framework to represent the mental model of a single agent, through the notion of abstract state representations.6 Here, an abstract state corresponds with s˜ (§4), a low-dimensional, structured, interpretable state encoding, useful for planning, communication, and predicting upcoming observations (Franc¸ois-Lavet et al., 2019). One example is a dynamic knowledge graph embedding to represent a scene (Kipf et al., 2020). 5 We use Liu et al. (2019b) wit"
2020.acl-main.559,W18-1406,0,0.0310228,"tic parsing from relatively narrow domains to handle more general literal language on-the-fly, similarly to zero-shot semantic parsing (Givoli and Reichart, 2019). For the example in §2.1, the parser could be expected to infer the types (boxes as containers, fruits as objects) either by context (Yao et al. (2018) explore a preliminary schema-based approach) or 6275 explicit declarative language, using them to configure the emulator to handle the specific required problem setting (Tamari et al., 2020). As in similar projects exploring embodied understanding (Pustejovsky and Krishnaswamy, 2016; Baldridge et al., 2018), new simulator frameworks must be developed. While full embodiment calls for multiple modalities, the degree to which it is required remains an important open question (Lupyan and Lewis, 2019). Accordingly, and for immediate applicability to purely textual NLU problems we propose also focusing on the simpler setting of interactive text (Nelson, 2005). Recent research on text-based games shows how agents can learn to “program” in such languages (Cˆot´e et al., 2019; Ammanabrolu and Riedl, 2019), and how real language understanding problems can be framed as executable semantic parsing using con"
2020.acl-main.559,P16-2017,0,0.0174092,"osely related to automated knowledge base construction (Winn et al., 2019) or frame induction from text (QasemiZadeh et al., 2019). Our proposed paradigm suggests enriching classic symbolic knowledge representations (Speer et al., 2017) to executable form (Tamari et al., 2020). Preliminary steps in this direction are seen in inferential knowledge bases such as ATOMIC (Sap et al., 2019), which provides limited execution logic using edges typed with if-then relations. Alongside FrameNet and MetaNet, others have collected schema and metaphor mappings, by learning them from large corpora (Beigman Klebanov et al., 2016; Gao et al., 2018). Pastra et al. (2011) built a database of concepts directly groundable to sensorimotor representations, primarily for robotics applications. 8 7.2 This phase assumes that the compiled knowledge is given (hard-coded), and the parsing and emulator modules are neural (learned). A hard-coded emulator will likely be needed to train a learned emulator. The learned event execution of Narayanan (1997) provides a useful starting point towards computational models capable of such inference. In general, learned simulation is relatively unexplored in the context of natural language, th"
2020.acl-main.559,P19-1470,0,0.0171148,"rrent NLU systems (McClelland et al., 2019; Bisk et al., 2020), here we take a more focused look at mental models; challenges arising due to their embodied nature, their importance in general NLU, and how we might begin integrating them into current approaches. 6268 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6268–6281 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Mainstream NLU work, be it entirely distributional, such as BERT (Devlin et al., 2019), or also involving symbolic knowledge representation (Liu et al., 2019a; Bosselut et al., 2019), seldom addresses mental models directly. Crucially, such approaches lack the interactive worlds within which mental models1 are learned jointly through language and embodied action. The most closely related lines of work to the present proposal are grounded approaches, which feature worlds in the form of interactive environments, and address mapping text to programs (executable semantic parses) (e.g., Gauthier and Mordatch, 2016; Liang, 2016; Kiela et al., 2016; Chevalier-Boisvert et al., 2019). However, while well-aligned with a model-building paradigm, typically such approaches have been l"
2020.acl-main.559,C02-1156,1,0.588193,"applications (Lakoff and Narayanan, 2010). These works also focused on a particular type of simulation (sensorimotor), understood as only one mechanism of many used in language understanding (Stolk et al., 2016). FrameNet (Ruppenhofer et al., 2016) and MetaNet (David and Dodge, 2014) are closely related projects in that each provides an extensive collection of schemata used in everyday and metaphoric language comprehension, respectively, via the concept of a semantic frame (Fillmore, 1985). However, neither incorporates simulation semantics, as needed for a full realization of the ECL vision (Chang et al., 2002). 4 Linking ECL to NLU and Embodied AI Research We propose a unifying view of ECL, bringing it closer to contemporary cognitive science and deep learning approaches. This section presents notations and motivating intuitions, further developing the computational framework in §5,§6. The proposal centers around the view of natural language as a kind of neural programming language (Lupyan and Bergen, 2016), or higher-level cognitive control system for systematically querying and induc6271 Concept Symbolic ECL Primitives Basic data structures, operators, variables... Knowledge Organization a) Compo"
2020.acl-main.559,N19-1423,0,0.00614674,"sition papers highlighting significant differences between human language understanding and current NLU systems (McClelland et al., 2019; Bisk et al., 2020), here we take a more focused look at mental models; challenges arising due to their embodied nature, their importance in general NLU, and how we might begin integrating them into current approaches. 6268 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6268–6281 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Mainstream NLU work, be it entirely distributional, such as BERT (Devlin et al., 2019), or also involving symbolic knowledge representation (Liu et al., 2019a; Bosselut et al., 2019), seldom addresses mental models directly. Crucially, such approaches lack the interactive worlds within which mental models1 are learned jointly through language and embodied action. The most closely related lines of work to the present proposal are grounded approaches, which feature worlds in the form of interactive environments, and address mapping text to programs (executable semantic parses) (e.g., Gauthier and Mordatch, 2016; Liang, 2016; Kiela et al., 2016; Chevalier-Boisvert et al., 2019). H"
2020.acl-main.559,N19-1246,0,0.0214633,"Missing"
2020.acl-main.559,P19-1188,0,0.0292947,"tly groundable to sensorimotor representations, primarily for robotics applications. 8 7.2 This phase assumes that the compiled knowledge is given (hard-coded), and the parsing and emulator modules are neural (learned). A hard-coded emulator will likely be needed to train a learned emulator. The learned event execution of Narayanan (1997) provides a useful starting point towards computational models capable of such inference. In general, learned simulation is relatively unexplored in the context of natural language, though recent work has explored it in generated instruction following setups (Gaddy and Klein, 2019; Adhikari et al., 2020). Outside of NLU, learning structured world models is a long-studied, fast-growing field in embodied AI research (Schmidhuber, 1990; Ha and Schmidhuber, 2018; Hamrick, 2019; Kipf et al., 2020), and recently also in learned executors for neural programming (Kant, 2018). We expect much useful cross fertilization with these fields. 7.3 Conclusions Sub-goal 2: learning to simulate Sub-goal 3: learning compiled knowledge This phase focuses on the component seemingly hardest to learn – compiled knowledge. Out of scope here is fully neural setting where all components are join"
2020.acl-main.559,D18-1060,0,0.0153168,"ted knowledge base construction (Winn et al., 2019) or frame induction from text (QasemiZadeh et al., 2019). Our proposed paradigm suggests enriching classic symbolic knowledge representations (Speer et al., 2017) to executable form (Tamari et al., 2020). Preliminary steps in this direction are seen in inferential knowledge bases such as ATOMIC (Sap et al., 2019), which provides limited execution logic using edges typed with if-then relations. Alongside FrameNet and MetaNet, others have collected schema and metaphor mappings, by learning them from large corpora (Beigman Klebanov et al., 2016; Gao et al., 2018). Pastra et al. (2011) built a database of concepts directly groundable to sensorimotor representations, primarily for robotics applications. 8 7.2 This phase assumes that the compiled knowledge is given (hard-coded), and the parsing and emulator modules are neural (learned). A hard-coded emulator will likely be needed to train a learned emulator. The learned event execution of Narayanan (1997) provides a useful starting point towards computational models capable of such inference. In general, learned simulation is relatively unexplored in the context of natural language, though recent work ha"
2020.acl-main.559,D19-5815,0,0.164209,"rich mental representations that people utilize for language understanding. Indeed, despite the tremendous progress in NLU, recent work shows that today’s state-ofthe-art (SOTA) systems differ from human-like language understanding in crucial ways, in particular in their generalization, grounding, reasoning, and explainability capabilities (Glockner et al., 2018; McCoy et al., 2019a,b; Nie et al., 2019; Yogatama et al., 2019; Lake et al., 2019). Question-answering (QA) is currently one of the predominant methods of training deep-learning models for general, open-domain language understanding (Gardner et al., 2019b). While QA is a versatile, broadly-applicable framework, recent studies have shown it to be fraught with pitfalls (Gardner et al., 2019a; Mudrakarta et al., 2018). A recent workshop on QA for reading comprehension suggested that “There is growing realization that the traditional supervised learning paradigm is broken [...] – we’re fitting artifacts” (Gardner, 2019). In many respects, the problems of NLU mirror those of artificial intelligence (AI) research in general. Lake et al.’s (2017a) seminal work identified a significant common factor at the root of problems in general AI. The current"
2020.acl-main.559,C18-1014,0,0.017392,"s the challenges presented in §5.2. Fig. 4 shows the proposed architecture. For simplicity, the focus is on a static reading comprehension setting, but the architecture supports richer environments as well. 6.1 Environment The environment provides an “interaction API” to the agent, as well as the reward signal. The supported interaction may vary considerably depending on the task; for reading comprehension, it allows structured access to the text while supporting flexible reading strategies (Yuan et al., 2019). The flexibility is important for long documents, where navigation may be required (Geva and Berant, 2018). For executable semantic parsing, there might be external systems to interact with besides the text, such as a database (Liang et al., 2016). 6.2 Agent The agent architecture approximates the important ECL functions outlined in §4, and consists of four main modules: Memory. We distinguish between two forms of memory, the first an episodic, short-term mental model – the system’s current abstract state representation (˜ st ). The symbolic programming analog is the execution trace of a program, containing the states of relevant working variables at each execution step. Fig. 4 displays the update"
2020.acl-main.559,P19-1438,0,0.0135675,"ey are hard-coded in a symbolic programming language), and which must be learned. 7.1 Sub-goal 1: learning open-domain simulation Observing that literal language is close to the embodied primitives level, its interpretation is simpler (than that of non-literal language, see §4). Therefore, in this phase, the emulator and compiled knowledge are hard-coded; here the focus is learning the parser. In other words, this sub-goal focuses on extending executable semantic parsing from relatively narrow domains to handle more general literal language on-the-fly, similarly to zero-shot semantic parsing (Givoli and Reichart, 2019). For the example in §2.1, the parser could be expected to infer the types (boxes as containers, fruits as objects) either by context (Yao et al. (2018) explore a preliminary schema-based approach) or 6275 explicit declarative language, using them to configure the emulator to handle the specific required problem setting (Tamari et al., 2020). As in similar projects exploring embodied understanding (Pustejovsky and Krishnaswamy, 2016; Baldridge et al., 2018), new simulator frameworks must be developed. While full embodiment calls for multiple modalities, the degree to which it is required remai"
2020.acl-main.559,P18-2103,0,0.0146599,"Introduction “Not those speaking the same language, but those sharing the same feeling understand each other.” – Jalal ad-Din Rumi While current NLU systems “speak” human language by learning strong statistical models, they do not possess anything like the rich mental representations that people utilize for language understanding. Indeed, despite the tremendous progress in NLU, recent work shows that today’s state-ofthe-art (SOTA) systems differ from human-like language understanding in crucial ways, in particular in their generalization, grounding, reasoning, and explainability capabilities (Glockner et al., 2018; McCoy et al., 2019a,b; Nie et al., 2019; Yogatama et al., 2019; Lake et al., 2019). Question-answering (QA) is currently one of the predominant methods of training deep-learning models for general, open-domain language understanding (Gardner et al., 2019b). While QA is a versatile, broadly-applicable framework, recent studies have shown it to be fraught with pitfalls (Gardner et al., 2019a; Mudrakarta et al., 2018). A recent workshop on QA for reading comprehension suggested that “There is growing realization that the traditional supervised learning paradigm is broken [...] – we’re fitting a"
2020.acl-main.559,P16-1138,0,0.0195763,"anguage Simulation Fig. 1 includes a short story about a world with crates, boxes, and objects inside them. It is a short and simple narrative, far from capturing the fullblown complexity of natural language. Following Gardner et al. (2019a), we assume that a system understands the story if it can correctly answer arbitrary questions about it. To do so requires basic commonsense and mathematical reasoning, referent grounding, tracking events, handling declarative knowledge, and more. The task is similar to narrative comprehension tasks in datasets such as bAbI (Bordes et al., 2015) and SCONE (Long et al., 2016), and could be solved given large amounts of annotated training data. But, the goal here is different, specifically, to develop models that, like humans, can understand such language on-the-fly (like zero-shot learning). QA approaches. Current QA systems, used in an off-the-shelf manner, do not generalize well to tasks on which they have not been trained; NLU models are known to be brittle even to slight 6269 changes in style and vocabulary (Gardner et al., 2020; Keysers et al., 2020). The closest QA setting is the DROP challenge (Dua et al., 2019), requiring reading comprehension and basic nu"
2020.acl-main.559,P19-1334,0,0.0284701,"speaking the same language, but those sharing the same feeling understand each other.” – Jalal ad-Din Rumi While current NLU systems “speak” human language by learning strong statistical models, they do not possess anything like the rich mental representations that people utilize for language understanding. Indeed, despite the tremendous progress in NLU, recent work shows that today’s state-ofthe-art (SOTA) systems differ from human-like language understanding in crucial ways, in particular in their generalization, grounding, reasoning, and explainability capabilities (Glockner et al., 2018; McCoy et al., 2019a,b; Nie et al., 2019; Yogatama et al., 2019; Lake et al., 2019). Question-answering (QA) is currently one of the predominant methods of training deep-learning models for general, open-domain language understanding (Gardner et al., 2019b). While QA is a versatile, broadly-applicable framework, recent studies have shown it to be fraught with pitfalls (Gardner et al., 2019a; Mudrakarta et al., 2018). A recent workshop on QA for reading comprehension suggested that “There is growing realization that the traditional supervised learning paradigm is broken [...] – we’re fitting artifacts” (Gardner,"
2020.acl-main.559,P18-1176,0,0.0179284,"-ofthe-art (SOTA) systems differ from human-like language understanding in crucial ways, in particular in their generalization, grounding, reasoning, and explainability capabilities (Glockner et al., 2018; McCoy et al., 2019a,b; Nie et al., 2019; Yogatama et al., 2019; Lake et al., 2019). Question-answering (QA) is currently one of the predominant methods of training deep-learning models for general, open-domain language understanding (Gardner et al., 2019b). While QA is a versatile, broadly-applicable framework, recent studies have shown it to be fraught with pitfalls (Gardner et al., 2019a; Mudrakarta et al., 2018). A recent workshop on QA for reading comprehension suggested that “There is growing realization that the traditional supervised learning paradigm is broken [...] – we’re fitting artifacts” (Gardner, 2019). In many respects, the problems of NLU mirror those of artificial intelligence (AI) research in general. Lake et al.’s (2017a) seminal work identified a significant common factor at the root of problems in general AI. The current deep-learning paradigm is a statistical pattern-recognition approach predominantly applied to relatively narrow task-specific prediction. In contrast, human cogniti"
2020.acl-main.559,L16-1730,0,0.0246472,"uld be the sequence of desired states, and each sentence corresponds to an utterance (u1 =“The world contains 2 crates.”,...). 5.2 Computational challenges of embodiment We can now more precisely characterize the challenges that the recipient faces. At the root of the problem is the embodiment principle (Lawrence, 2017): human internal representations and computation capacity, as represented by s˜ and T˜, respectively, are many orders of magnitude larger than their linguistic communication “bandwidth”. We note that though s˜t is only a subspace of the full mental state, following Stolk et al. (2016); Bengio (2017) we assume that it still holds that dim (˜ st )  dim (ut ).The embodiment principle dictates extreme economy in language use (Grice et al., 1975), and results in three major challenges: Common ground (prior world knowledge). Meaning cannot be spelled out in words but rather must be evoked in the listener (Rumelhart, 1981) by assuming and exploiting common ground (Clark and Schaefer, 1989; Tomasello, 2008), i.e., shared structures of mental representations. In other words, to achieve some aligned goal state g ∗ , the communicators must rely heavily on pre-existing similarities i"
2020.acl-main.559,S19-2003,1,0.850855,"Missing"
2020.acl-main.559,W06-3510,0,0.117387,"Missing"
2020.acl-main.559,W19-2609,1,0.832514,"e developed. While full embodiment calls for multiple modalities, the degree to which it is required remains an important open question (Lupyan and Lewis, 2019). Accordingly, and for immediate applicability to purely textual NLU problems we propose also focusing on the simpler setting of interactive text (Nelson, 2005). Recent research on text-based games shows how agents can learn to “program” in such languages (Cˆot´e et al., 2019; Ammanabrolu and Riedl, 2019), and how real language understanding problems can be framed as executable semantic parsing using configurable text-based simulators (Tamari et al., 2019). In this setting, learning compiled knowledge is closely related to automated knowledge base construction (Winn et al., 2019) or frame induction from text (QasemiZadeh et al., 2019). Our proposed paradigm suggests enriching classic symbolic knowledge representations (Speer et al., 2017) to executable form (Tamari et al., 2020). Preliminary steps in this direction are seen in inferential knowledge bases such as ATOMIC (Sap et al., 2019), which provides limited execution logic using edges typed with if-then relations. Alongside FrameNet and MetaNet, others have collected schema and metaphor map"
2020.acl-main.559,2020.emnlp-main.703,0,\N,Missing
2020.coling-main.264,E17-2039,0,0.0668035,"Missing"
2020.coling-main.264,P98-1013,0,0.62291,"s. Enriching a supervised UCCA parser with preposition supersense features from STREUSLE—and, even more so, training a parser to predict supersenses jointly with UCCA—improved parsing performance, revealing the two frameworks to be overlapping but complementary. 2.4 Related Representations The above annotation schemes define finite inventories of coarse-grained categories to avoid depending on language-specific lexical resources, and thus can in principle be applied to any language. This fact distinguishes UCCA and STREUSLE from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998; Fillmore and Baker, 2009) and the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005). The Prague Dependency Treebank tectogrammatical layer (Böhmová et al., 2003) uses few lexicon-free roles, but its semantics is determined by a valency lexicon. The Parallel Meaning Bank (Abzianidze et al., 2017) uses lexicon-free5 VerbNet (Schuler, 2005) semantic roles. The STREUSLE tagset for preposition supersenses generalizes VerbNet’s role set to cover non-core arguments/adjuncts of verbs, as well as prepositional complements of nouns and adjectives."
2020.coling-main.264,D16-1134,1,0.684487,"ntactic paraphrases in a typologically-motivated, cross-linguistic fashion (Sulem et al., 2015), building on Basic Linguistic Theory (Dixon, 2010, 2012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. Beyond syntactic paraphrases, UCCA encodes lexical semantic properties such as the aspectual distinction between states and processes (whether an event evolves in time or not). UCCA has been applied to text simplification (Sulem et al., 2018b) and evaluation of text-to-text generation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). UCCA corpora are available for English, French and German, and pilot studies have been conducted on additional languages. 2948 Here we summarize the principles and main distinctions in UCCA.1 In UCCA, an analysis of a text passage is a DAG (directed acyclic graph) over semantic elements called units. A unit corresponds to (is anchored by) one or more tokens, labeled with one or more semantic categories in relation to a parent unit.2 The principal kind of unit is a scene denoting a situation mentioned in the sentence, typically involving a scene-"
2020.coling-main.264,K19-2007,0,0.0143559,"d for data-driven analysis and complement the rules. TUPA. This UCCA parser (Hershcovich et al., 2017) is based on a transition-based algorithm with a neural network transition classifier, using a BiLSTM for encoding input representation, with word, lemma, and syntactic features embedded as real-valued features. We add the supersense and lexcat from STREUSLE as embedding inputs to the TUPA BiLSTM (concatenated with existing inputs). For prepositions, we add both the scene role and function (see §2).9 HIT-SCIR Parser. This is a transition-based parser for several MR frameworks, including UCCA (Che et al., 2019). It achieved the highest average score in the CoNLL 2019 shared task (Oepen et al., 2019). While Che et al. (2019) fine-tuned BERT (Devlin et al., 2019) for contextualized word representation, our delexicalized version replaces it with UD and STREUSLE features: POS tag, dependency relation, supersenses (scene role and function; see §2), the lexical category of the word or the MWE that the word is part of, and the BIO tag. These are concatenated to form word representations.10 5 Experiments Data. We use the Reviews section from UD 2.6 English_EWT (Zeman et al., 2020), with lexical semantic ann"
2020.coling-main.264,N18-2020,1,0.848296,"in a typologically-motivated, cross-linguistic fashion (Sulem et al., 2015), building on Basic Linguistic Theory (Dixon, 2010, 2012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. Beyond syntactic paraphrases, UCCA encodes lexical semantic properties such as the aspectual distinction between states and processes (whether an event evolves in time or not). UCCA has been applied to text simplification (Sulem et al., 2018b) and evaluation of text-to-text generation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). UCCA corpora are available for English, French and German, and pilot studies have been conducted on additional languages. 2948 Here we summarize the principles and main distinctions in UCCA.1 In UCCA, an analysis of a text passage is a DAG (directed acyclic graph) over semantic elements called units. A unit corresponds to (is anchored by) one or more tokens, labeled with one or more semantic categories in relation to a parent unit.2 The principal kind of unit is a scene denoting a situation mentioned in the sentence, typically involving a scene-evoking predicate, partic"
2020.coling-main.264,2020.dmr-1.5,1,0.734608,"bels represent the lexical contribution of the preposition in itself. The two labels are drawn from the same supersense inventory and are identical for many tokens. The lexcat annotations (syntactic category of lexical unit) is a slight extension to the Universal POS tagset, adding categories for certain MWE subtypes, such as light verb constructions, following Walsh 1 For further details, see the extensive UCCA annotation manual: https://github.com/UniversalConceptualCognitiveAnnotation/docs/blob/master/guidelines.pdf 2 UCCA also supports implicit units which do not correspond to any tokens (Cui and Hershcovich, 2020), but these are excluded from parsing evaluation and we ignore them for purposes of this paper. 3 STREUSLE distinguishes strong MWEs, which are opaque (noncompositional) or idiosyncratic in meaning, and weak MWEs, which represent looser collocations that are nevertheless semantically compositional, like “highly recommended”. 2949 et al. (2018) and idiomatic PPs; it also distinguishes possessive pronouns, the possessive clitic ’s, and discourse expressions.4 Figure 1 illustrates the MWE, lexcat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations"
2020.coling-main.264,N19-1423,0,0.0113546,"neural network transition classifier, using a BiLSTM for encoding input representation, with word, lemma, and syntactic features embedded as real-valued features. We add the supersense and lexcat from STREUSLE as embedding inputs to the TUPA BiLSTM (concatenated with existing inputs). For prepositions, we add both the scene role and function (see §2).9 HIT-SCIR Parser. This is a transition-based parser for several MR frameworks, including UCCA (Che et al., 2019). It achieved the highest average score in the CoNLL 2019 shared task (Oepen et al., 2019). While Che et al. (2019) fine-tuned BERT (Devlin et al., 2019) for contextualized word representation, our delexicalized version replaces it with UD and STREUSLE features: POS tag, dependency relation, supersenses (scene role and function; see §2), the lexical category of the word or the MWE that the word is part of, and the BIO tag. These are concatenated to form word representations.10 5 Experiments Data. We use the Reviews section from UD 2.6 English_EWT (Zeman et al., 2020), with lexical semantic annotations from STREUSLE 4.4 (Schneider and Smith, 2015; Schneider et al., 2018),11 and with UCCA graphs from UCCA_English-EWT v1.0.1 (Hershcovich et al.,"
2020.coling-main.264,W17-6811,0,0.0125142,"s, which represent looser collocations that are nevertheless semantically compositional, like “highly recommended”. 2949 et al. (2018) and idiomatic PPs; it also distinguishes possessive pronouns, the possessive clitic ’s, and discourse expressions.4 Figure 1 illustrates the MWE, lexcat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations have been applied to other languages: verbal multi-word expressions (Ramisch et al., 2018), noun and verb supersenses (Picca et al., 2008; Qiu et al., 2011; Schneider et al., 2013; Martínez Alonso et al., 2015; Hellwig, 2017), and preposition supersenses (Hwang et al., 2017; Peng et al., 2020; Hwang et al., 2020). Liu et al. (2020) presented a comprehensive lexical semantic tagger for STREUSLE, which predicts the comprehensive lexical semantic analysis from text, and is freely available. Prange et al. (2019) proposed several procedures for integrating STREUSLE supersenses directly into UCCA, refining its coarsegrained categories with preposition supersenses. Enriching a supervised UCCA parser with preposition supersense features from STREUSLE—and, even more so, training a parser to predict supersenses jointly with"
2020.coling-main.264,P17-1104,1,0.842047,"as the C. 4 Second Conversion Approach: Delexicalized Supervised UCCA Parsing Previous work tackled the UCCA parsing task using supervised learning. In order to complement and validate the analysis of the rule-based converter, we compare its findings to a delexicalized supervised parser, that can be seen as inducing a converter from data. By removing all word and lemma features from these parsers, and instead adding features based on gold UD and STREUSLE annotations, we obtain supervised “converters”, which can be used for data-driven analysis and complement the rules. TUPA. This UCCA parser (Hershcovich et al., 2017) is based on a transition-based algorithm with a neural network transition classifier, using a BiLSTM for encoding input representation, with word, lemma, and syntactic features embedded as real-valued features. We add the supersense and lexcat from STREUSLE as embedding inputs to the TUPA BiLSTM (concatenated with existing inputs). For prepositions, we add both the scene role and function (see §2).9 HIT-SCIR Parser. This is a transition-based parser for several MR frameworks, including UCCA (Che et al., 2019). It achieved the highest average score in the CoNLL 2019 shared task (Oepen et al.,"
2020.coling-main.264,P18-1035,1,0.846519,"rties into account? What does this suggest about the potential for exploiting simpler or better-resourced linguistic representations for improved MR parsing? Intuitively, we ask whether: ? sentence-level MR = syntax + lexical semantics To address this question, we examine UCCA, a document-level MR often used for sentence-level semantics (see §2.1). Hershcovich et al. (2019) began to examine the relation of UCCA to syntax, contributing a corpus with gold standard UD and UCCA parses, heuristically aligning them, and quantifying the correlations between syntactic and semantic labels. Conversely, Hershcovich et al. (2018) provided some initial evidence that other MRs can be brought to bear on the UCCA parsing task via multitask learning, but left the details of the relationship between representations to latent (and opaque) parameters of neural models. In this paper, we aim to close the gap between the two previous investigations by (1) building an interpretable rule-based system to convert from shallower representations (syntax and lexical semantic units/tags) into UCCA, forcing us to be linguistically precise about what UCCA captures and how it “decomposes”; and (2) training top-performing supervised parsers"
2020.coling-main.264,N19-1047,1,0.900698,"rm linguistic understanding of MRs. In particular: are they merely a coarsening and rearranging of syntactic information, such as is encoded in Universal Dependencies (UD; Nivre et al., 2016, 2020)? To what extent do they take lexical semantic properties into account? What does this suggest about the potential for exploiting simpler or better-resourced linguistic representations for improved MR parsing? Intuitively, we ask whether: ? sentence-level MR = syntax + lexical semantics To address this question, we examine UCCA, a document-level MR often used for sentence-level semantics (see §2.1). Hershcovich et al. (2019) began to examine the relation of UCCA to syntax, contributing a corpus with gold standard UD and UCCA parses, heuristically aligning them, and quantifying the correlations between syntactic and semantic labels. Conversely, Hershcovich et al. (2018) provided some initial evidence that other MRs can be brought to bear on the UCCA parsing task via multitask learning, but left the details of the relationship between representations to latent (and opaque) parameters of neural models. In this paper, we aim to close the gap between the two previous investigations by (1) building an interpretable rul"
2020.coling-main.264,S17-1022,1,0.90147,"Missing"
2020.coling-main.264,2020.dmr-1.6,1,0.720944,"nal, like “highly recommended”. 2949 et al. (2018) and idiomatic PPs; it also distinguishes possessive pronouns, the possessive clitic ’s, and discourse expressions.4 Figure 1 illustrates the MWE, lexcat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations have been applied to other languages: verbal multi-word expressions (Ramisch et al., 2018), noun and verb supersenses (Picca et al., 2008; Qiu et al., 2011; Schneider et al., 2013; Martínez Alonso et al., 2015; Hellwig, 2017), and preposition supersenses (Hwang et al., 2017; Peng et al., 2020; Hwang et al., 2020). Liu et al. (2020) presented a comprehensive lexical semantic tagger for STREUSLE, which predicts the comprehensive lexical semantic analysis from text, and is freely available. Prange et al. (2019) proposed several procedures for integrating STREUSLE supersenses directly into UCCA, refining its coarsegrained categories with preposition supersenses. Enriching a supervised UCCA parser with preposition supersense features from STREUSLE—and, even more so, training a parser to predict supersenses jointly with UCCA—improved parsing performance, revealing the two frameworks to be overlapping but co"
2020.coling-main.264,W04-2705,0,0.331073,"Missing"
2020.coling-main.264,L18-1537,0,0.0446629,"Missing"
2020.coling-main.264,2020.lrec-1.497,0,0.0283756,"Missing"
2020.coling-main.264,L16-1262,0,0.0457846,"Missing"
2020.coling-main.264,K19-2001,1,0.895831,"Missing"
2020.coling-main.264,J05-1004,0,0.278998,"upersenses jointly with UCCA—improved parsing performance, revealing the two frameworks to be overlapping but complementary. 2.4 Related Representations The above annotation schemes define finite inventories of coarse-grained categories to avoid depending on language-specific lexical resources, and thus can in principle be applied to any language. This fact distinguishes UCCA and STREUSLE from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998; Fillmore and Baker, 2009) and the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005). The Prague Dependency Treebank tectogrammatical layer (Böhmová et al., 2003) uses few lexicon-free roles, but its semantics is determined by a valency lexicon. The Parallel Meaning Bank (Abzianidze et al., 2017) uses lexicon-free5 VerbNet (Schuler, 2005) semantic roles. The STREUSLE tagset for preposition supersenses generalizes VerbNet’s role set to cover non-core arguments/adjuncts of verbs, as well as prepositional complements of nouns and adjectives. Universal Decompositional Semantics (DeComp) defines semantic roles as bundles of lexicon-free features. Cross-linguistic applicability in"
2020.coling-main.264,2020.lrec-1.733,1,0.827363,"ntically compositional, like “highly recommended”. 2949 et al. (2018) and idiomatic PPs; it also distinguishes possessive pronouns, the possessive clitic ’s, and discourse expressions.4 Figure 1 illustrates the MWE, lexcat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations have been applied to other languages: verbal multi-word expressions (Ramisch et al., 2018), noun and verb supersenses (Picca et al., 2008; Qiu et al., 2011; Schneider et al., 2013; Martínez Alonso et al., 2015; Hellwig, 2017), and preposition supersenses (Hwang et al., 2017; Peng et al., 2020; Hwang et al., 2020). Liu et al. (2020) presented a comprehensive lexical semantic tagger for STREUSLE, which predicts the comprehensive lexical semantic analysis from text, and is freely available. Prange et al. (2019) proposed several procedures for integrating STREUSLE supersenses directly into UCCA, refining its coarsegrained categories with preposition supersenses. Enriching a supervised UCCA parser with preposition supersense features from STREUSLE—and, even more so, training a parser to predict supersenses jointly with UCCA—improved parsing performance, revealing the two frameworks to"
2020.coling-main.264,D14-1162,0,0.0858457,"UD+STREUSLE (middle), and supervised parsers with word features (bottom). Evaluation. We use standard UCCA parsing evaluation, matching edges by the terminal yields of their endpoint units.13 Labeled precision, recall and F1-score consider the edge categories when matching edges. Where an edge has multiple categories, each of them is considered separately. 6 Results Table 2 shows the EWT Reviews dev scores. For comparison with parsers that have access to words, we also show the TUPA dev results from Hershcovich et al. (2019), who used syntactic features from the gold UD annotation and GloVe (Pennington et al., 2014);14 and the HIT-SCIR parser with BERT/GloVe, and with UD+STREUSLE features. Rules with gold UD and STREUSLE close the gap between the syntax-based converter and parsers with word information, reaching the same primary labeled F1 as TUPA with word features. This is surprising (since supervised parsers are known to usually outperform rule-based ones), and suggests that the training data (see table 1) was insufficient for the parser to learn a mapping as accurate as the complex conversion rules (described in §3). Enhancing GloVe-based HIT-SCIR with UD and STREUSLE yields similar results. However,"
2020.coling-main.264,picca-etal-2008-supersense,0,0.0245028,"s strong MWEs, which are opaque (noncompositional) or idiosyncratic in meaning, and weak MWEs, which represent looser collocations that are nevertheless semantically compositional, like “highly recommended”. 2949 et al. (2018) and idiomatic PPs; it also distinguishes possessive pronouns, the possessive clitic ’s, and discourse expressions.4 Figure 1 illustrates the MWE, lexcat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations have been applied to other languages: verbal multi-word expressions (Ramisch et al., 2018), noun and verb supersenses (Picca et al., 2008; Qiu et al., 2011; Schneider et al., 2013; Martínez Alonso et al., 2015; Hellwig, 2017), and preposition supersenses (Hwang et al., 2017; Peng et al., 2020; Hwang et al., 2020). Liu et al. (2020) presented a comprehensive lexical semantic tagger for STREUSLE, which predicts the comprehensive lexical semantic analysis from text, and is freely available. Prange et al. (2019) proposed several procedures for integrating STREUSLE supersenses directly into UCCA, refining its coarsegrained categories with preposition supersenses. Enriching a supervised UCCA parser with preposition supersense feature"
2020.coling-main.264,K19-1017,1,0.839748,"excat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations have been applied to other languages: verbal multi-word expressions (Ramisch et al., 2018), noun and verb supersenses (Picca et al., 2008; Qiu et al., 2011; Schneider et al., 2013; Martínez Alonso et al., 2015; Hellwig, 2017), and preposition supersenses (Hwang et al., 2017; Peng et al., 2020; Hwang et al., 2020). Liu et al. (2020) presented a comprehensive lexical semantic tagger for STREUSLE, which predicts the comprehensive lexical semantic analysis from text, and is freely available. Prange et al. (2019) proposed several procedures for integrating STREUSLE supersenses directly into UCCA, refining its coarsegrained categories with preposition supersenses. Enriching a supervised UCCA parser with preposition supersense features from STREUSLE—and, even more so, training a parser to predict supersenses jointly with UCCA—improved parsing performance, revealing the two frameworks to be overlapping but complementary. 2.4 Related Representations The above annotation schemes define finite inventories of coarse-grained categories to avoid depending on language-specific lexical resources, and thus can in"
2020.coling-main.264,P18-1018,1,0.932403,"es, such as the dotted edge from the possession scene unit to vehicle. 2.2 Universal Dependencies UD is a syntactic dependency scheme used in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bi-lexical trees, with edge labels representing syntactic relations. An example UD tree appears at the bottom of figure 1. 2.3 STREUSLE STREUSLE (Supersense-Tagged Repository of English with a Unified Semantics for Lexical Expressions) is a corpus annotated comprehensively for several forms of lexical semantics (Schneider and Smith, 2015; Schneider et al., 2018). All kinds of multi-word expressions (MWEs) are annotated, giving each sentence a lexical semantic segmentation.3 Syntactic and semantic tags are then applied to individual units (single- and multi-word). The semantic tags are supersenses for noun, verb, and prepositional/possessive units. Preposition supersenses include two tiers of annotation: scene role labels represent the semantic role of the prepositional phrase marked by the preposition, and function labels represent the lexical contribution of the preposition in itself. The two labels are drawn from the same supersense inventory and a"
2020.coling-main.264,N13-1076,1,0.760943,"mpositional) or idiosyncratic in meaning, and weak MWEs, which represent looser collocations that are nevertheless semantically compositional, like “highly recommended”. 2949 et al. (2018) and idiomatic PPs; it also distinguishes possessive pronouns, the possessive clitic ’s, and discourse expressions.4 Figure 1 illustrates the MWE, lexcat, and supersense layers. STREUSLE itself is limited to English, but many of its component annotations have been applied to other languages: verbal multi-word expressions (Ramisch et al., 2018), noun and verb supersenses (Picca et al., 2008; Qiu et al., 2011; Schneider et al., 2013; Martínez Alonso et al., 2015; Hellwig, 2017), and preposition supersenses (Hwang et al., 2017; Peng et al., 2020; Hwang et al., 2020). Liu et al. (2020) presented a comprehensive lexical semantic tagger for STREUSLE, which predicts the comprehensive lexical semantic analysis from text, and is freely available. Prange et al. (2019) proposed several procedures for integrating STREUSLE supersenses directly into UCCA, refining its coarsegrained categories with preposition supersenses. Enriching a supervised UCCA parser with preposition supersense features from STREUSLE—and, even more so, trainin"
2020.coling-main.264,N15-1177,1,0.875791,"s, which express reentrancies, such as the dotted edge from the possession scene unit to vehicle. 2.2 Universal Dependencies UD is a syntactic dependency scheme used in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bi-lexical trees, with edge labels representing syntactic relations. An example UD tree appears at the bottom of figure 1. 2.3 STREUSLE STREUSLE (Supersense-Tagged Repository of English with a Unified Semantics for Lexical Expressions) is a corpus annotated comprehensively for several forms of lexical semantics (Schneider and Smith, 2015; Schneider et al., 2018). All kinds of multi-word expressions (MWEs) are annotated, giving each sentence a lexical semantic segmentation.3 Syntactic and semantic tags are then applied to individual units (single- and multi-word). The semantic tags are supersenses for noun, verb, and prepositional/possessive units. Preposition supersenses include two tiers of annotation: scene role labels represent the semantic role of the prepositional phrase marked by the preposition, and function labels represent the lexical contribution of the preposition in itself. The two labels are drawn from the same s"
2020.coling-main.264,Q19-1027,0,0.0155495,"Time (T) and Quantifier (Q) expressions frequently coincide with certain syntactic categories such as adverbs and prepositions, and can typically be identified from corresponding supersenses, if available. The converter tends to err on the conservative side, falling back to Adverbials (D) and Elaborators (E) when it cannot find sufficient explicit semantic evidence. 7.3 Low Match—Divergences or Insufficient Information Noun compound interpretation. Lexical composition in noun compounds evokes various forms of event structures, which are underspecified by the meaning of the constituent words (Shwartz and Dagan, 2019). While often compounding is used for Elaboration, as in [E tap] [C water], it is not necessarily always the case. For example, in [C sea] [C bottom] both “sea"" and “bottom"" are Center, since they reflect part-whole relations. The modifier may also be a Participant in the scene evoked by the head, as in [A road] [P construction]. This is partially encoded in STREUSLE, as the fact that the MWE “road construction” has the N . EVENT supersense indicates that it is scene-evoking, but it still does not reveal the relationship between the constituent words. Adverbs and linkage. While many syntactic"
2020.coling-main.264,W15-3502,1,0.772929,"besides STREUSLE, UD also serves as the backbone of the DeComp scheme (White et al., 2016), and so information as to its semantic content is important there as well. Argument structural phenomena are at the heart of many MRs, which provide further motivation for empirical studies to the extent lexical semantics and syntax can encode them. 2.1 Universal Conceptual Cognitive Annotation Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013) targets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologically-motivated, cross-linguistic fashion (Sulem et al., 2015), building on Basic Linguistic Theory (Dixon, 2010, 2012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. Beyond syntactic paraphrases, UCCA encodes lexical semantic properties such as the aspectual distinction between states and processes (whether an event evolves in time or not). UCCA has been applied to text simplification (Sulem et al., 2018b) and evaluation of text-to-text generation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). UCCA corpora are available f"
2020.coling-main.264,P18-1016,1,0.84781,"gets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologically-motivated, cross-linguistic fashion (Sulem et al., 2015), building on Basic Linguistic Theory (Dixon, 2010, 2012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. Beyond syntactic paraphrases, UCCA encodes lexical semantic properties such as the aspectual distinction between states and processes (whether an event evolves in time or not). UCCA has been applied to text simplification (Sulem et al., 2018b) and evaluation of text-to-text generation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). UCCA corpora are available for English, French and German, and pilot studies have been conducted on additional languages. 2948 Here we summarize the principles and main distinctions in UCCA.1 In UCCA, an analysis of a text passage is a DAG (directed acyclic graph) over semantic elements called units. A unit corresponds to (is anchored by) one or more tokens, labeled with one or more semantic categories in relation to a parent unit.2 The principal kind of unit is a scene denoting a s"
2020.coling-main.264,tsvetkov-etal-2014-augmenting-english,1,0.741527,"gular A or C. Scene-evoking adjectives. Inspecting the high-frequency confusions, adjectives stand out as persistent error inducers. Different classes of adjectives are handled differently in UCCA: e.g., while most adjectives are scene-evoking, pertainyms (academic), inherent-composition modifiers (sugary), and quantity modifiers (many) are not. Some adjectives are ambiguous: a legal practice may refer to a behavior that is legal as opposed to illegal, in which case it should be scene-evoking, or to a law office, in which case it should not. Enriching STREUSLE with supersenses for adjectives (Tsvetkov et al., 2014) might be fruitful for such distinctions. Even with lexical disambiguation, the scene attachment of the adjective may be ambiguous: e.g. a good chef probably means a chef who cooks well, so good should be an Adverbial in the scene evoked by chef—in contrast with a tall chef, where tall is not part of the cooking scene and instead should evoke a State. Predicative adjectives, and adjective modifiers in predicative NPs, are another source of difficulty, especially when they occur in fragments: sometimes the adjective is annotated as evoking the main scene, and sometimes not. Determining this req"
2020.coling-main.264,W18-4921,1,0.864317,"Missing"
2020.coling-main.264,D16-1177,0,0.0622494,"Missing"
2020.coling-main.264,D18-1194,0,0.0488351,"Missing"
2020.coling-tutorials.1,P13-1023,1,0.776661,"nd supports rapid annotation. The tutorial will provide a detailed introduction to the UCCA annotation guidelines, design philosophy and the available resources; and a comparison to other meaning representations. It will also survey the existing parsing work, including the findings of three recent shared tasks, in SemEval and CoNLL, that addressed UCCA parsing. Finally, the tutorial will present recent applications and extensions to the scheme, demonstrating its value for natural language processing in a range of languages and domains. 1 Introduction Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013), abbreviated as “UCCA”, is a symbolic meaning representation (MR) that supports human annotation of text with broad coverage. While several meaning representation schemes share this goal (Abend and Rappoport, 2017), UCCA targets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion, building on Basic Linguistic Theory (Dixon, 20102012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. UCCA has been annotate"
2020.coling-tutorials.1,P17-1008,1,0.85136,"will also survey the existing parsing work, including the findings of three recent shared tasks, in SemEval and CoNLL, that addressed UCCA parsing. Finally, the tutorial will present recent applications and extensions to the scheme, demonstrating its value for natural language processing in a range of languages and domains. 1 Introduction Universal Conceptual Cognitive Annotation (Abend and Rappoport, 2013), abbreviated as “UCCA”, is a symbolic meaning representation (MR) that supports human annotation of text with broad coverage. While several meaning representation schemes share this goal (Abend and Rappoport, 2017), UCCA targets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion, building on Basic Linguistic Theory (Dixon, 20102012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. UCCA has been annotated on several corpora of different genres and languages,1 as summarized in table 1. Pilot studies have been conducted in additional languages. A web-based annotation system is available (Abend et al., 2017). In UCCA,"
2020.coling-tutorials.1,P17-4019,1,0.922155,"l (Abend and Rappoport, 2017), UCCA targets a level of semantic granularity that abstracts away from syntactic paraphrases in a typologicallymotivated, cross-linguistic fashion, building on Basic Linguistic Theory (Dixon, 20102012), an influential framework for linguistic description. The scheme does not rely on language-specific resources, and sets a low threshold for annotator training. UCCA has been annotated on several corpora of different genres and languages,1 as summarized in table 1. Pilot studies have been conducted in additional languages. A web-based annotation system is available (Abend et al., 2017). In UCCA, an analysis of a text passage is a directed acyclic graph over semantic elements called units. The principal kind of unit is a scene, which describes an action, movement or state, and is similar to FrameNet’s notion of a frame. Figure 1 contains three scenes, evoked, respectively, by the verb took, the noun phrase a repair, and the possessive our. Several elements are exemplified, including participants, secondary relations, and scene linkage. The graph is anchored in the text tokens (the leaves generally correspond to one or more tokens), and relations between units are indicated b"
2020.coling-tutorials.1,D19-3009,0,0.0678743,"abbreviations: H = parallel scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda"
2020.coling-tutorials.1,2020.conll-shared.7,1,0.892951,"gners understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested in UCCA parsing, and more generally graph parsing, deepen their understanding of the framework, and what properties make it unique. The tutorial will i"
2020.coling-tutorials.1,K19-2008,0,0.0203831,"ulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow resea"
2020.coling-tutorials.1,P98-1013,0,0.725327,"t al., 2019; Oepen et al., 2020) show that multi-task meaning representation parsing is difficult. The tutorial will compare and contrast UCCA and other meaning representations, and will thereby inform participants of the potential advantages and difficulties in employing multi-task learning across semantic schemes. UCCA defines a small inventory of coarse-grained categories so as not to rely on language-specific lexical resources, and can thus in principle be applied to a great variety of languages. This distinguishes UCCA from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998), the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005), and Universal Decompositional Semantics (White et al., 2016). For example, FrameNet requires a different ontology for each new language addressed (Ohara et al., 2003; You and Liu, 2005; Borin et al., 2013; Park et al., 2014; Hayoun and Elhadad, 2016; Djemaa et al., 2016), and AMR underwent significant customization to be applicable to Chinese (Li et al., 2016). Decomp takes a different approach to multilinguality, where the parser is required to parse sentences in other languages to"
2020.coling-tutorials.1,W13-2322,1,0.6552,"ing representation parsing is difficult. The tutorial will compare and contrast UCCA and other meaning representations, and will thereby inform participants of the potential advantages and difficulties in employing multi-task learning across semantic schemes. UCCA defines a small inventory of coarse-grained categories so as not to rely on language-specific lexical resources, and can thus in principle be applied to a great variety of languages. This distinguishes UCCA from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998), the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005), and Universal Decompositional Semantics (White et al., 2016). For example, FrameNet requires a different ontology for each new language addressed (Ohara et al., 2003; You and Liu, 2005; Borin et al., 2013; Park et al., 2014; Hayoun and Elhadad, 2016; Djemaa et al., 2016), and AMR underwent significant customization to be applicable to Chinese (Li et al., 2016). Decomp takes a different approach to multilinguality, where the parser is required to parse sentences in other languages to their corresponding English semantic forms (Zhang et al., 2018"
2020.coling-tutorials.1,D16-1134,1,0.883248,"e 1: Example sentence from EWT (reviews-086839-0003), with its UCCA annotation. Category abbreviations: H = parallel scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al"
2020.coling-tutorials.1,L16-1601,0,0.0227815,"s not to rely on language-specific lexical resources, and can thus in principle be applied to a great variety of languages. This distinguishes UCCA from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998), the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005), and Universal Decompositional Semantics (White et al., 2016). For example, FrameNet requires a different ontology for each new language addressed (Ohara et al., 2003; You and Liu, 2005; Borin et al., 2013; Park et al., 2014; Hayoun and Elhadad, 2016; Djemaa et al., 2016), and AMR underwent significant customization to be applicable to Chinese (Li et al., 2016). Decomp takes a different approach to multilinguality, where the parser is required to parse sentences in other languages to their corresponding English semantic forms (Zhang et al., 2018). The tutorial will address contemporary issues in the field, such as the question of how to represent semantic structure multilingually with broad coverage, which is actively being explored from many angles. While UCCA structures and categories are intentionally coarse, the scheme has a multi-layered architecture, whi"
2020.coling-tutorials.1,P18-1035,1,0.942057,"aning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested in UCCA parsing, and more generally graph parsing, deepen their understanding of the framework, and what properties make it unique. The tutorial will include a brief survey of the various approaches taken by existing parsers, and prepare attendees to work on UCCA parsing themselves. Furthermore, UCCA parsing has been shown to benefit from multi-task learning (Caruana, 1997) with 2 other meaning representations (Hershcovich et al., 2018), although results from the CoNLL 2019 and CoNLL 2020 shared tasks (Oepen et al., 2019; Oepen et al., 2020) show that multi-task meaning representation parsing is difficult. The tutorial will compare and contrast UCCA and other meaning representations, and will thereby inform participants of the potential advantages and difficulties in employing multi-task learning across semantic schemes. UCCA defines a small inventory of coarse-grained categories so as not to rely on language-specific lexical resources, and can thus in principle be applied to a great variety of languages. This distinguishes"
2020.coling-tutorials.1,N19-1047,1,0.913183,"Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested in UCCA parsing, and more generally graph parsing, deepen their understanding of the framework, and what properties make it unique. The tutorial will include a brief survey of the various approaches taken by existing parsers, and prepare attendees to work on UCCA parsing themselves. Furth"
2020.coling-tutorials.1,Q16-1023,0,0.0330143,"d typology. The necessary background will be provided as part of the tutorial. However, participants are expected to know about basic data structures such as trees and graphs. For the parsing section, prior knowledge is assumed about common machine learning techniques, including supervised learning and neural networks. 3.2 Reading list The following are recommended to read before the tutorial, as they provide background and frame the context in which the tutorial materials lie: 1. Chapter 3 of Dixon (2005) contains an introduction to some basic concepts in semantics on which UCCA is based. 2. Kiperwasser and Goldberg (2016) present a transition-based parser using an architecture on which TUPA, the first UCCA parser, is based (Hershcovich et al., 2017). 3. Peng et al. (2017) performed multi-task learning for meaning representation parsing, inspiring work on cross-framework parsing for UCCA (Hershcovich et al., 2018). 4. Abend and Rappoport (2017) compare and constrast several meaning representations according to various aspects. 5. Deng and Xue (2017) investigate translation divergences using a hierarchical alignment, and discuss bridging them with cross-lingual semantic representations. 6. Croft et al. (2017) li"
2020.coling-tutorials.1,P19-4002,0,0.0205062,", basic explanation of categories, simple examples. 2. Annotation guidelines (35m). Linguistic details, interesting constructions in several languages. 3. Data and annotation (10m). Overview of annotated data (see §1) and the annotation process and software (Abend et al., 2017). 4. Extensions to UCCA and integration with other schemes (15m). Semantic roles (Prange et al., 2019a), coreference (Prange et al., 2019b), and fine-grained implicit arguments (Cui and Hershcovich, 2020). 5. Relation to other representations (15m). Comparison to other meaning representations (Abend and Rappoport, 2017; Koller et al., 2019; Hershcovich et al., 2020) and to UD (Hershcovich et al., 2019a). 6. Parsing (25m). TUPA (Hershcovich et al., 2017; Hershcovich et al., 2018; Hershcovich and Arviv, 2019; Arviv et al., 2020), SemEval 2019 Task 1 (Hershcovich et al., 2019b; Jiang et al., 2019), CoNLL 2019 and CoNLL 2020 Shared Tasks (Oepen et al., 2019; Oepen et al., 2020), and more recent parsers (Zhang et al., 2019a). 7. Monolingual tasks and evaluation (20m). Sentence simplification (Sulem et al., 2018b), evaluation of sentence simplification (Sulem et al., 2018a; Alva-Manchego et al., 2019) and grammatical error correction"
2020.coling-tutorials.1,K19-2011,0,0.0153953,"l., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested in UCCA parsing, and m"
2020.coling-tutorials.1,K19-2010,0,0.0187105,"Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested"
2020.coling-tutorials.1,W16-1702,0,0.0189137,"great variety of languages. This distinguishes UCCA from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998), the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005), and Universal Decompositional Semantics (White et al., 2016). For example, FrameNet requires a different ontology for each new language addressed (Ohara et al., 2003; You and Liu, 2005; Borin et al., 2013; Park et al., 2014; Hayoun and Elhadad, 2016; Djemaa et al., 2016), and AMR underwent significant customization to be applicable to Chinese (Li et al., 2016). Decomp takes a different approach to multilinguality, where the parser is required to parse sentences in other languages to their corresponding English semantic forms (Zhang et al., 2018). The tutorial will address contemporary issues in the field, such as the question of how to represent semantic structure multilingually with broad coverage, which is actively being explored from many angles. While UCCA structures and categories are intentionally coarse, the scheme has a multi-layered architecture, which allows for refinement using additional layers, which serve as “modules” of semantic dist"
2020.coling-tutorials.1,K19-2004,0,0.017117,"l., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively"
2020.coling-tutorials.1,S19-2012,0,0.0169137,"eserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as w"
2020.coling-tutorials.1,W17-4769,0,0.0403872,"Missing"
2020.coling-tutorials.1,S19-2015,0,0.0117123,"LP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-ling"
2020.coling-tutorials.1,2020.lrec-1.497,0,0.0612167,"Missing"
2020.coling-tutorials.1,K19-2001,1,0.891885,"Missing"
2020.coling-tutorials.1,2020.conll-shared.1,1,0.842632,"Missing"
2020.coling-tutorials.1,J05-1004,0,0.324702,"al will compare and contrast UCCA and other meaning representations, and will thereby inform participants of the potential advantages and difficulties in employing multi-task learning across semantic schemes. UCCA defines a small inventory of coarse-grained categories so as not to rely on language-specific lexical resources, and can thus in principle be applied to a great variety of languages. This distinguishes UCCA from finer-grained sentence-structural representations like FrameNet (Baker et al., 1998), the Abstract Meaning Representation (Banarescu et al., 2013), which relies on PropBank (Palmer et al., 2005), and Universal Decompositional Semantics (White et al., 2016). For example, FrameNet requires a different ontology for each new language addressed (Ohara et al., 2003; You and Liu, 2005; Borin et al., 2013; Park et al., 2014; Hayoun and Elhadad, 2016; Djemaa et al., 2016), and AMR underwent significant customization to be applicable to Chinese (Li et al., 2016). Decomp takes a different approach to multilinguality, where the parser is required to parse sentences in other languages to their corresponding English semantic forms (Zhang et al., 2018). The tutorial will address contemporary issues"
2020.coling-tutorials.1,P17-1186,0,0.0196926,"d graphs. For the parsing section, prior knowledge is assumed about common machine learning techniques, including supervised learning and neural networks. 3.2 Reading list The following are recommended to read before the tutorial, as they provide background and frame the context in which the tutorial materials lie: 1. Chapter 3 of Dixon (2005) contains an introduction to some basic concepts in semantics on which UCCA is based. 2. Kiperwasser and Goldberg (2016) present a transition-based parser using an architecture on which TUPA, the first UCCA parser, is based (Hershcovich et al., 2017). 3. Peng et al. (2017) performed multi-task learning for meaning representation parsing, inspiring work on cross-framework parsing for UCCA (Hershcovich et al., 2018). 4. Abend and Rappoport (2017) compare and constrast several meaning representations according to various aspects. 5. Deng and Xue (2017) investigate translation divergences using a hierarchical alignment, and discuss bridging them with cross-lingual semantic representations. 6. Croft et al. (2017) list typologically-informed design criteria for Universal Dependencies (Nivre et al., 2020), which are also relevant for other structural representations i"
2020.coling-tutorials.1,K19-1017,1,0.843837,"heir corresponding English semantic forms (Zhang et al., 2018). The tutorial will address contemporary issues in the field, such as the question of how to represent semantic structure multilingually with broad coverage, which is actively being explored from many angles. While UCCA structures and categories are intentionally coarse, the scheme has a multi-layered architecture, which allows for refinement using additional layers, which serve as “modules” of semantic distinctions. We will give an overview of the recently proposed extensions (to support coreference) and joint parsing experiments (Prange et al., 2019a; Prange et al., 2019b). 3 Agenda The planned division of time is as follows: 1. Bird’s eye view (45m). Design philosophy, notion of scenes, basic explanation of categories, simple examples. 2. Annotation guidelines (35m). Linguistic details, interesting constructions in several languages. 3. Data and annotation (10m). Overview of annotated data (see §1) and the annotation process and software (Abend et al., 2017). 4. Extensions to UCCA and integration with other schemes (15m). Semantic roles (Prange et al., 2019a), coreference (Prange et al., 2019b), and fine-grained implicit arguments (Cui"
2020.coling-tutorials.1,W19-3319,1,0.835934,"heir corresponding English semantic forms (Zhang et al., 2018). The tutorial will address contemporary issues in the field, such as the question of how to represent semantic structure multilingually with broad coverage, which is actively being explored from many angles. While UCCA structures and categories are intentionally coarse, the scheme has a multi-layered architecture, which allows for refinement using additional layers, which serve as “modules” of semantic distinctions. We will give an overview of the recently proposed extensions (to support coreference) and joint parsing experiments (Prange et al., 2019a; Prange et al., 2019b). 3 Agenda The planned division of time is as follows: 1. Bird’s eye view (45m). Design philosophy, notion of scenes, basic explanation of categories, simple examples. 2. Annotation guidelines (35m). Linguistic details, interesting constructions in several languages. 3. Data and annotation (10m). Overview of annotated data (see §1) and the annotation process and software (Abend et al., 2017). 4. Extensions to UCCA and integration with other schemes (15m). Semantic roles (Prange et al., 2019a), coreference (Prange et al., 2019b), and fine-grained implicit arguments (Cui"
2020.coling-tutorials.1,S19-2016,0,0.0133934,"(Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representati"
2020.coling-tutorials.1,2020.conll-shared.5,0,0.0188687,"t abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested in UCCA parsing, and more generally graph parsing, deepen their understanding of the framework, and what properties make it unique. The tutorial will include a brief survey of"
2020.coling-tutorials.1,2020.conll-shared.0,0,0.254101,"Missing"
2020.coling-tutorials.1,N15-4003,1,0.751125,"Missing"
2020.coling-tutorials.1,K19-2012,0,0.0140622,"020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et al., 2020), where 12 and 4 teams, respectively, submitted parsed UCCA graphs. This tutorial will allow researchers interested in UCCA parsing, and more generally graph parsing"
2020.coling-tutorials.1,W15-3502,1,0.92171,"Table 1: Data statistics for existing UCCA corpora. H A A D P took our A P L E C S|A We H C A F C R F vehicle in for a repair to the air conditioning Figure 1: Example sentence from EWT (reviews-086839-0003), with its UCCA annotation. Category abbreviations: H = parallel scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019"
2020.coling-tutorials.1,N18-1063,1,0.89855,"P L E C S|A We H C A F C R F vehicle in for a repair to the air conditioning Figure 1: Example sentence from EWT (reviews-086839-0003), with its UCCA annotation. Category abbreviations: H = parallel scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 20"
2020.coling-tutorials.1,P18-1016,1,0.908942,"P L E C S|A We H C A F C R F vehicle in for a repair to the air conditioning Figure 1: Example sentence from EWT (reviews-086839-0003), with its UCCA annotation. Category abbreviations: H = parallel scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 20"
2020.coling-tutorials.1,2020.starsem-1.6,1,0.83547,"F C R F vehicle in for a repair to the air conditioning Figure 1: Example sentence from EWT (reviews-086839-0003), with its UCCA annotation. Category abbreviations: H = parallel scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019"
2020.coling-tutorials.1,S19-2014,0,0.0231412,"UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross"
2020.coling-tutorials.1,S19-2013,0,0.0142518,"(Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL"
2020.coling-tutorials.1,D16-1177,0,0.0645357,"Missing"
2020.coling-tutorials.1,2020.wmt-1.104,0,0.0835123,"scene, L = scene linker, P = process (dynamic event), S = state, A = scene participant, D = scene adverbial, E = non-scene elaborator, C = center (non-scene head), R = relator, F = functional element. It is also cross-linguistically stable, and reflects a level of semantic structure that is usually preserved in translations (Sulem et al., 2015). UCCA has been applied in NLP to text simplification (Sulem et al., 2018b; Sulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Stra"
2020.coling-tutorials.1,S19-2017,0,0.0155621,"ulem et al., 2020), and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et"
2020.coling-tutorials.1,D18-1194,0,0.0382982,"Missing"
2020.coling-tutorials.1,D19-1392,0,0.0259621,"Missing"
2020.coling-tutorials.1,K19-2014,0,0.10178,"and text-to-text generation evaluation (Birch et al., 2016; Mareˇcek et al., 2017; Choshen and Abend, 2018; Sulem et al., 2018b; Alva-Manchego et al., 2019; Xu et al., 2020). The tutorial will describe the guidelines and rationale behind UCCA, helping potential application designers understand what abstractions it makes. Significant effort has been devoted to building UCCA parsers (Hershcovich et al., 2017; Hershcovich 2 et al., 2018; Jiang et al., 2019; Lyu et al., 2019; Tuan Nguyen and Tran, 2019; Taslimipoor et al., 2019; Marzinotto et al., 2019; Pütz and Glocker, 2019; Yu and Sagae, 2019; Zhang et al., 2019a; Hershcovich and Arviv, 2019; Li et al., 2019; Donatelli et al., 2019; Che et al., 2019; Bai and Zhao, 2019; Lai et al., 2019; Koreeda et al., 2019; Straka and Straková, 2019; Cao et al., 2019; Zhang et al., 2019b; Droganova et al., 2019; Chen et al., 2019; Arviv et al., 2020; Samuel and Straka, 2020; Dou et al., 2020), including a SemEval 2019 shared task on cross-lingual UCCA parsing (Hershcovich et al., 2019b), which had 8 participating teams, as well as CoNLL 2019 and CoNLL 2020 shared tasks on cross-framework and cross-lingual meaning representation parsing (Oepen et al., 2019; Oepen et"
2020.conll-1.7,P16-1070,1,0.850178,"Missing"
2020.conll-1.7,W18-6111,0,0.0194061,"focus on POS-based SEs. ERRANT (Bryant et al., 2017) is essentially the only classifier in use today, and is therefore a natural point of comparison. ERRANT taxonomy is coarse-grained. It assumes for the most part that POS tags are not altered in corrections, classifying many errors by their POS tag (e.g. adverb error). Consequently, ERRANT covers mostly spelling and word-form errors. We note three important differences between SE R C L and ERRANT. First, being based on UD, SE R C L is applicable across languages (see §4.2), while ERRANT requires new rules or other modifications per language (Boyd, 2018). Second, relying Comparing to Manually Typed Edits Unlike many NLP tasks, this work does not aim to mimic human behavior. Still, there is sense in comparing SE R C L to a manually annotated taxonomy. We compare NUCLE annotated train errors and SE R C L’s (confusion matrix in appendix Table 17). We ignore relocation errors as edits lack the necessary information to discern relocation from deletion. 4 A number of works designed parsers with learner language specifically in mind. However, as such parsers exist only for learner English, we use UDPipe for uniformity. 99 on an established framework"
2020.conll-1.7,W19-4406,0,0.165388,"rs not classified by ERRANT are classified by SE R C L. We demonstrate SE R C L’s unique features, notably cross-linguistic applicability, by analyzing SE distributions in available corpora for learner English (§4.1) and learner Russian (§4.2). Finally, we find in GEC systems (1) certain SEs are harder to correct (2) SEs are harder than nonSEs (c.f. 5) (3) the granular types can help devising rules to improve products (e.g. Grammarly, §5.2). 2 Notes TLE (Berzak et al., 2016) Manual parses NUCLE (Dahlmeier et al., 2013) Standard GEC benchmark Lang8 (Mizumoto et al., 2012) No error classes W&I (Bryant et al., 2019) Varied proficiency levels ERRANT classes RULEC (Rozovskaya and Roth, 2019) Learner Russian Table 1: Datasets used in this work. to the root.2 The rationale for this decision is that UD treats grammatical markers as dependents of content words. Therefore, in most cases the semantic and syntactic heads correspond to one another, even if lexical items are changed. For example, in wentwas walking, the semantic and syntactic head of the target has the lemma walk and not be.3 We define an SE as an edit where the two representative’s labels do not match. The SE type is defined as the ordered pair o"
2020.conll-1.7,P17-1074,0,0.0688396,"types contain much of the information conveyed by NUCLE types. Qualitatively, SE R C L has more categories and splits NUCLE types to meaningful sub-types. It is thus usually more informative. For example, the ”article or determiner” NUCLE type is split to insertions and deletions of determiners in addition to other SEs (mostly from or to determiner). 3.3 Comparing to the Automatic ERRANT This section studies the relation between SE R C L’s predictions and those of ERRANT. For comparability, we apply SE R C L to the edit spans produced by ERRANT. For brevity, we focus on POS-based SEs. ERRANT (Bryant et al., 2017) is essentially the only classifier in use today, and is therefore a natural point of comparison. ERRANT taxonomy is coarse-grained. It assumes for the most part that POS tags are not altered in corrections, classifying many errors by their POS tag (e.g. adverb error). Consequently, ERRANT covers mostly spelling and word-form errors. We note three important differences between SE R C L and ERRANT. First, being based on UD, SE R C L is applicable across languages (see §4.2), while ERRANT requires new rules or other modifications per language (Boyd, 2018). Second, relying Comparing to Manually T"
2020.conll-1.7,W19-4423,0,0.0127087,"or patterns. 5 Analyzing GEC System Outputs AIP-TOHOKU UEDIN-MS GOLD 18 37 21 76 17 3 0 21 46 33 87 21 2 3 55 105 59 142 38 5 1 Table 5: AIP-TOHOKU, UEDIN-MS and Gold annotation changes (correct or not) on selected replacement types of SEs in absolute counts. The non-uniform behaviour of the systems over types indicates that SE R C L produces meaningful results. 5.1 Experiments with Leading GEC Systems We use the outputs of several systems that participated in the BEA2019 shared task (Bryant et al., 2019), namely: the winning system UEDINMS (Grundkiewicz et al., 2019), as well as KAKAO&BRAIN (Choe et al., 2019), SHUYAO (Xu et al., 2019), CAMB-CUED (Stahlberg and Byrne, 2019), and AIP-TOHOKU (Asano et al., 2019), that were ranked second, fifth, eighth and ninth respectively.5 We extract matrices for the system outputs using the same method as in §3.1. Recall is bounded by the amount of predicted SEs, divided by their number in the gold standard. The full matrices are given in Appendix §3.2. Our results in Table 5 show that the top-ranking UEDIN-MS makes consistently more changes in general and per source SE than AIP-TOHOKU ranked 9th, but less than CAMB-CUED, ranked 8th (found in appendix §3.2). Howe"
2020.conll-1.7,P18-1127,1,0.929333,". When using end-to-end systems, balancing the distribution of errors in the train and test sets has been shown to improve results (Junczys-Dowmunt et al., 2018). Ensembling black-box systems relying on per-type performance has been shown superior to each system’s performance and over average ensembling (Kantor et al., 2019). Augmenting the training data with synthetic errors of a particular type is effective for improving performance on that type (Belinkov and Bisk, 2018). The classification of grammatical error types is also used to analyze system performance (e.g., Lichtarge et al., 2019). Choshen and Abend (2018a,b) showed that current systems and evaluation measures essentially ignore some error types, suggesting that targeted evaluation of these types may be needed. To date, several error taxonomies have been proposed and applied for annotating errors in major English learner-language corpora (Bryant et al., 2019; Dahlmeier et al., 2013; Nicholls, 2003, inter alia). There has been interest lately in other languages, for which different datasets and taxonomies were created (Rozovskaya and Roth, 2019; Rao et al., 2018; Zaghouani et al., 2014). However, different taxonomies are used by different corpo"
2020.conll-1.7,N18-2020,1,0.922913,". When using end-to-end systems, balancing the distribution of errors in the train and test sets has been shown to improve results (Junczys-Dowmunt et al., 2018). Ensembling black-box systems relying on per-type performance has been shown superior to each system’s performance and over average ensembling (Kantor et al., 2019). Augmenting the training data with synthetic errors of a particular type is effective for improving performance on that type (Belinkov and Bisk, 2018). The classification of grammatical error types is also used to analyze system performance (e.g., Lichtarge et al., 2019). Choshen and Abend (2018a,b) showed that current systems and evaluation measures essentially ignore some error types, suggesting that targeted evaluation of these types may be needed. To date, several error taxonomies have been proposed and applied for annotating errors in major English learner-language corpora (Bryant et al., 2019; Dahlmeier et al., 2013; Nicholls, 2003, inter alia). There has been interest lately in other languages, for which different datasets and taxonomies were created (Rozovskaya and Roth, 2019; Rao et al., 2018; Zaghouani et al., 2014). However, different taxonomies are used by different corpo"
2020.conll-1.7,P13-1023,1,0.720099,"of the correction and not of the source sentence. Moreover, relying solely on POS tags yields difficulties in classifying constructions that involve more than a single word. For such cases, it defines specialized error types, such as Incorrect Argument Structure, which serves as a residual category for argument structure errors that cannot be accounted for by adposition or agreement errors. However, unlike SE R C L, it does not provide any information as to what particular incorrect argument structure was used or how it should be corrected. Choshen and Abend (2018c) used a semantic annotation(Abend and Rappoport, 2013) to show semantics, unlike syntax is kept upon changes. UD was previously used in GEC in the TLE corpus and in a learner language parser (e.g., Sakaguchi et al., 2017) (we do not apply their parser, as it is made specifically for English, and might alter the origin parse). 7 Conclusion We presented SE R C L, a novel method for classifying SEs based on UD parses of learner text and its correction. We show that SE R C L provides a detailed picture of the prevalence of different SEs in two languages, and can be straightforwardly automated. We further show that the method manages to classify about"
2020.conll-1.7,W18-3706,0,0.0258222,"s is also used to analyze system performance (e.g., Lichtarge et al., 2019). Choshen and Abend (2018a,b) showed that current systems and evaluation measures essentially ignore some error types, suggesting that targeted evaluation of these types may be needed. To date, several error taxonomies have been proposed and applied for annotating errors in major English learner-language corpora (Bryant et al., 2019; Dahlmeier et al., 2013; Nicholls, 2003, inter alia). There has been interest lately in other languages, for which different datasets and taxonomies were created (Rozovskaya and Roth, 2019; Rao et al., 2018; Zaghouani et al., 2014). However, different taxonomies are used by different corpora, based on commonly observed error types in the target domain and language, which impedes direct comparison across corpora. Moreover, these taxonomies are not formulated based on a specific theory or annotation scheme for morphosyntactic representation, which may promote accessibility to nonexperts but often leads to non-uniform terminology and difficulty in leveraging available NLP tools. 104 Another automatic type classification was suggested apart from ERRANT. Swanson and Yamangil (2012) trained a log-line"
2020.conll-1.7,J94-4004,0,0.11182,"Missing"
2020.conll-1.7,Q19-1001,0,0.105472,"e SE R C L’s unique features, notably cross-linguistic applicability, by analyzing SE distributions in available corpora for learner English (§4.1) and learner Russian (§4.2). Finally, we find in GEC systems (1) certain SEs are harder to correct (2) SEs are harder than nonSEs (c.f. 5) (3) the granular types can help devising rules to improve products (e.g. Grammarly, §5.2). 2 Notes TLE (Berzak et al., 2016) Manual parses NUCLE (Dahlmeier et al., 2013) Standard GEC benchmark Lang8 (Mizumoto et al., 2012) No error classes W&I (Bryant et al., 2019) Varied proficiency levels ERRANT classes RULEC (Rozovskaya and Roth, 2019) Learner Russian Table 1: Datasets used in this work. to the root.2 The rationale for this decision is that UD treats grammatical markers as dependents of content words. Therefore, in most cases the semantic and syntactic heads correspond to one another, even if lexical items are changed. For example, in wentwas walking, the semantic and syntactic head of the target has the lemma walk and not be.3 We define an SE as an edit where the two representative’s labels do not match. The SE type is defined as the ordered pair of labels with the source label going first. Special cases of SEs are additi"
2020.conll-1.7,P14-2027,0,0.0253154,"iners. Indeed, in many of the cases, only a small portion of the SEs was detected. While it is possible that Grammarly tends to overlook such cases because of the dominance of punctuation, spelling, and determiner errors in learner language, some of the types here involve only a handful of lexemes, suggesting that targeted treatment or data augmentation may be effective. 6 Related Work Error types are often used to improve performance and evaluation in GEC. Taxonomies have been used to construct classifiers and rule-based engines to correct specific error types (e.g., Rozovskaya et al., 2014; Farra et al., 2014; Zheng et al., 2018). When using end-to-end systems, balancing the distribution of errors in the train and test sets has been shown to improve results (Junczys-Dowmunt et al., 2018). Ensembling black-box systems relying on per-type performance has been shown superior to each system’s performance and over average ensembling (Kantor et al., 2019). Augmenting the training data with synthetic errors of a particular type is effective for improving performance on that type (Belinkov and Bisk, 2018). The classification of grammatical error types is also used to analyze system performance (e.g., Lich"
2020.conll-1.7,W19-4427,0,0.011888,"n and development, in addressing these recurring error patterns. 5 Analyzing GEC System Outputs AIP-TOHOKU UEDIN-MS GOLD 18 37 21 76 17 3 0 21 46 33 87 21 2 3 55 105 59 142 38 5 1 Table 5: AIP-TOHOKU, UEDIN-MS and Gold annotation changes (correct or not) on selected replacement types of SEs in absolute counts. The non-uniform behaviour of the systems over types indicates that SE R C L produces meaningful results. 5.1 Experiments with Leading GEC Systems We use the outputs of several systems that participated in the BEA2019 shared task (Bryant et al., 2019), namely: the winning system UEDINMS (Grundkiewicz et al., 2019), as well as KAKAO&BRAIN (Choe et al., 2019), SHUYAO (Xu et al., 2019), CAMB-CUED (Stahlberg and Byrne, 2019), and AIP-TOHOKU (Asano et al., 2019), that were ranked second, fifth, eighth and ninth respectively.5 We extract matrices for the system outputs using the same method as in §3.1. Recall is bounded by the amount of predicted SEs, divided by their number in the gold standard. The full matrices are given in Appendix §3.2. Our results in Table 5 show that the top-ranking UEDIN-MS makes consistently more changes in general and per source SE than AIP-TOHOKU ranked 9th, but less than CAMB-CUE"
2020.conll-1.7,E14-1038,0,0.0272076,"ion of superfluous determiners. Indeed, in many of the cases, only a small portion of the SEs was detected. While it is possible that Grammarly tends to overlook such cases because of the dominance of punctuation, spelling, and determiner errors in learner language, some of the types here involve only a handful of lexemes, suggesting that targeted treatment or data augmentation may be effective. 6 Related Work Error types are often used to improve performance and evaluation in GEC. Taxonomies have been used to construct classifiers and rule-based engines to correct specific error types (e.g., Rozovskaya et al., 2014; Farra et al., 2014; Zheng et al., 2018). When using end-to-end systems, balancing the distribution of errors in the train and test sets has been shown to improve results (Junczys-Dowmunt et al., 2018). Ensembling black-box systems relying on per-type performance has been shown superior to each system’s performance and over average ensembling (Kantor et al., 2019). Augmenting the training data with synthetic errors of a particular type is effective for improving performance on that type (Belinkov and Bisk, 2018). The classification of grammatical error types is also used to analyze system per"
2020.conll-1.7,P17-2030,0,0.0491536,"Missing"
2020.conll-1.7,N18-1055,0,0.0127248,"e of punctuation, spelling, and determiner errors in learner language, some of the types here involve only a handful of lexemes, suggesting that targeted treatment or data augmentation may be effective. 6 Related Work Error types are often used to improve performance and evaluation in GEC. Taxonomies have been used to construct classifiers and rule-based engines to correct specific error types (e.g., Rozovskaya et al., 2014; Farra et al., 2014; Zheng et al., 2018). When using end-to-end systems, balancing the distribution of errors in the train and test sets has been shown to improve results (Junczys-Dowmunt et al., 2018). Ensembling black-box systems relying on per-type performance has been shown superior to each system’s performance and over average ensembling (Kantor et al., 2019). Augmenting the training data with synthetic errors of a particular type is effective for improving performance on that type (Belinkov and Bisk, 2018). The classification of grammatical error types is also used to analyze system performance (e.g., Lichtarge et al., 2019). Choshen and Abend (2018a,b) showed that current systems and evaluation measures essentially ignore some error types, suggesting that targeted evaluation of these"
2020.conll-1.7,W19-4417,0,0.0120441,"DIN-MS GOLD 18 37 21 76 17 3 0 21 46 33 87 21 2 3 55 105 59 142 38 5 1 Table 5: AIP-TOHOKU, UEDIN-MS and Gold annotation changes (correct or not) on selected replacement types of SEs in absolute counts. The non-uniform behaviour of the systems over types indicates that SE R C L produces meaningful results. 5.1 Experiments with Leading GEC Systems We use the outputs of several systems that participated in the BEA2019 shared task (Bryant et al., 2019), namely: the winning system UEDINMS (Grundkiewicz et al., 2019), as well as KAKAO&BRAIN (Choe et al., 2019), SHUYAO (Xu et al., 2019), CAMB-CUED (Stahlberg and Byrne, 2019), and AIP-TOHOKU (Asano et al., 2019), that were ranked second, fifth, eighth and ninth respectively.5 We extract matrices for the system outputs using the same method as in §3.1. Recall is bounded by the amount of predicted SEs, divided by their number in the gold standard. The full matrices are given in Appendix §3.2. Our results in Table 5 show that the top-ranking UEDIN-MS makes consistently more changes in general and per source SE than AIP-TOHOKU ranked 9th, but less than CAMB-CUED, ranked 8th (found in appendix §3.2). However, there is no correlation between the number of SE changes in"
2020.conll-1.7,W19-4414,1,0.838352,"gmentation may be effective. 6 Related Work Error types are often used to improve performance and evaluation in GEC. Taxonomies have been used to construct classifiers and rule-based engines to correct specific error types (e.g., Rozovskaya et al., 2014; Farra et al., 2014; Zheng et al., 2018). When using end-to-end systems, balancing the distribution of errors in the train and test sets has been shown to improve results (Junczys-Dowmunt et al., 2018). Ensembling black-box systems relying on per-type performance has been shown superior to each system’s performance and over average ensembling (Kantor et al., 2019). Augmenting the training data with synthetic errors of a particular type is effective for improving performance on that type (Belinkov and Bisk, 2018). The classification of grammatical error types is also used to analyze system performance (e.g., Lichtarge et al., 2019). Choshen and Abend (2018a,b) showed that current systems and evaluation measures essentially ignore some error types, suggesting that targeted evaluation of these types may be needed. To date, several error taxonomies have been proposed and applied for annotating errors in major English learner-language corpora (Bryant et al."
2020.conll-1.7,L16-1680,0,0.0270957,"Missing"
2020.conll-1.7,N19-1333,0,0.012005,"2014; Zheng et al., 2018). When using end-to-end systems, balancing the distribution of errors in the train and test sets has been shown to improve results (Junczys-Dowmunt et al., 2018). Ensembling black-box systems relying on per-type performance has been shown superior to each system’s performance and over average ensembling (Kantor et al., 2019). Augmenting the training data with synthetic errors of a particular type is effective for improving performance on that type (Belinkov and Bisk, 2018). The classification of grammatical error types is also used to analyze system performance (e.g., Lichtarge et al., 2019). Choshen and Abend (2018a,b) showed that current systems and evaluation measures essentially ignore some error types, suggesting that targeted evaluation of these types may be needed. To date, several error taxonomies have been proposed and applied for annotating errors in major English learner-language corpora (Bryant et al., 2019; Dahlmeier et al., 2013; Nicholls, 2003, inter alia). There has been interest lately in other languages, for which different datasets and taxonomies were created (Rozovskaya and Roth, 2019; Rao et al., 2018; Zaghouani et al., 2014). However, different taxonomies ar"
2020.conll-1.7,N12-1037,0,0.0338105,"d (Rozovskaya and Roth, 2019; Rao et al., 2018; Zaghouani et al., 2014). However, different taxonomies are used by different corpora, based on commonly observed error types in the target domain and language, which impedes direct comparison across corpora. Moreover, these taxonomies are not formulated based on a specific theory or annotation scheme for morphosyntactic representation, which may promote accessibility to nonexperts but often leads to non-uniform terminology and difficulty in leveraging available NLP tools. 104 Another automatic type classification was suggested apart from ERRANT. Swanson and Yamangil (2012) trained a log-linear model to predict types defined by Nicholls (2003). This taxonomy resembles ours in that it uses grammatical categories (POS tags), but differs in that it only distinguishes types based on the POS tag of the correction and not of the source sentence. Moreover, relying solely on POS tags yields difficulties in classifying constructions that involve more than a single word. For such cases, it defines specialized error types, such as Incorrect Argument Structure, which serves as a residual category for argument structure errors that cannot be accounted for by adposition or ag"
2020.conll-1.7,C12-2084,0,0.0773807,"Missing"
2020.conll-1.7,2020.acl-main.109,1,0.858724,"Missing"
2020.conll-1.7,W19-4415,0,0.0145719,"System Outputs AIP-TOHOKU UEDIN-MS GOLD 18 37 21 76 17 3 0 21 46 33 87 21 2 3 55 105 59 142 38 5 1 Table 5: AIP-TOHOKU, UEDIN-MS and Gold annotation changes (correct or not) on selected replacement types of SEs in absolute counts. The non-uniform behaviour of the systems over types indicates that SE R C L produces meaningful results. 5.1 Experiments with Leading GEC Systems We use the outputs of several systems that participated in the BEA2019 shared task (Bryant et al., 2019), namely: the winning system UEDINMS (Grundkiewicz et al., 2019), as well as KAKAO&BRAIN (Choe et al., 2019), SHUYAO (Xu et al., 2019), CAMB-CUED (Stahlberg and Byrne, 2019), and AIP-TOHOKU (Asano et al., 2019), that were ranked second, fifth, eighth and ninth respectively.5 We extract matrices for the system outputs using the same method as in §3.1. Recall is bounded by the amount of predicted SEs, divided by their number in the gold standard. The full matrices are given in Appendix §3.2. Our results in Table 5 show that the top-ranking UEDIN-MS makes consistently more changes in general and per source SE than AIP-TOHOKU ranked 9th, but less than CAMB-CUED, ranked 8th (found in appendix §3.2). However, there is no correlati"
2020.conll-1.7,zaghouani-etal-2014-large,0,0.0312397,"analyze system performance (e.g., Lichtarge et al., 2019). Choshen and Abend (2018a,b) showed that current systems and evaluation measures essentially ignore some error types, suggesting that targeted evaluation of these types may be needed. To date, several error taxonomies have been proposed and applied for annotating errors in major English learner-language corpora (Bryant et al., 2019; Dahlmeier et al., 2013; Nicholls, 2003, inter alia). There has been interest lately in other languages, for which different datasets and taxonomies were created (Rozovskaya and Roth, 2019; Rao et al., 2018; Zaghouani et al., 2014). However, different taxonomies are used by different corpora, based on commonly observed error types in the target domain and language, which impedes direct comparison across corpora. Moreover, these taxonomies are not formulated based on a specific theory or annotation scheme for morphosyntactic representation, which may promote accessibility to nonexperts but often leads to non-uniform terminology and difficulty in leveraging available NLP tools. 104 Another automatic type classification was suggested apart from ERRANT. Swanson and Yamangil (2012) trained a log-linear model to predict types"
2020.conll-1.7,K18-2001,0,0.0576309,"Missing"
2020.conll-1.7,W18-6105,0,0.0224245,"ny of the cases, only a small portion of the SEs was detected. While it is possible that Grammarly tends to overlook such cases because of the dominance of punctuation, spelling, and determiner errors in learner language, some of the types here involve only a handful of lexemes, suggesting that targeted treatment or data augmentation may be effective. 6 Related Work Error types are often used to improve performance and evaluation in GEC. Taxonomies have been used to construct classifiers and rule-based engines to correct specific error types (e.g., Rozovskaya et al., 2014; Farra et al., 2014; Zheng et al., 2018). When using end-to-end systems, balancing the distribution of errors in the train and test sets has been shown to improve results (Junczys-Dowmunt et al., 2018). Ensembling black-box systems relying on per-type performance has been shown superior to each system’s performance and over average ensembling (Kantor et al., 2019). Augmenting the training data with synthetic errors of a particular type is effective for improving performance on that type (Belinkov and Bisk, 2018). The classification of grammatical error types is also used to analyze system performance (e.g., Lichtarge et al., 2019)."
2020.conll-shared.1,W13-2322,0,0.190653,"owards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although most AMR parsing research presupposes a pre-processing step that mod (domain) mod (domain) other (ARG1)-of exemplify-01 ARG0 and op1 cotton soybean op2 op3 rice op4 et-cetera Figure 4: Abstract Meaning Representation (AMR) for the running example A simila"
2020.conll-shared.1,W15-0128,1,0.796384,"des). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). Elementary Dependency Structures The EDS graphs (Oepen and Lønning, 2006) originally derive from the underspecified logical forms computed by the English Resource Grammar (Flickinger et al., 2017; Copestake et al., 2005). These logical forms are not in and of themselves semantic graphs (in the sense of §2 above) and are often refered to as English Resource Semantics (ERS; Bender et al., 2015).3 Elementary Dependency Structures (EDS; Oepen and Lønning, 2006) encode English Resource Semantics in a variablefree semantic dependency graph—not limited to bi-lexical dependencies—where graph nodes correspond to logical predications and edges to labeled argument positions. The EDS conversion from underspecified logical forms to directed graphs discards partial information on semantic scope from the full ERS, which makes these graphs abstractly— if not linguistically—similar to Abstract Meaning Representation (see below). Nodes in EDS are in principle independent of surface lexical units, b"
2020.conll-shared.1,D16-1134,1,0.827452,"(UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient), ADDR(essee), ORIG(in), and EFF(ect) indicate ‘participant’ positions in an underlying valency fr"
2020.conll-shared.1,W13-0101,1,0.816811,"imilar 〈2:9〉 sempos adj.denot ACT #Benef sempos x EXT almost 〈23:29〉 sempos adv.denot.grad.neg coref.gram #Gen sempos x RSTR other 〈53:58〉 sempos adj.denot Figure 2: Semantic dependency graphs for the running example A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice: Prague Tectogrammatical Graphs (PTG). In addition to node properties, visualized similarly to the EDS in Figure 1, boolean edge attributes are abbreviated below edge labels, for true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al.,"
2020.conll-shared.1,E17-2039,1,0.896998,"Missing"
2020.conll-shared.1,2020.emnlp-main.195,0,0.0866269,"with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general availability of accurate data-driven parsers for a broad range of different frameworks, with performance levels ranging between 0.76 MRP F1 (English UCCA) and 0.94 F1 (English EDS). Parsing accuracies in the cross-lingual track present comparable levels of performance, despite"
2020.conll-shared.1,2020.conll-shared.2,1,0.940067,"Structure (DRS), the meaning representations at the core of Discourse Representation Theory (DRT; Kamp and Reyle, 1993; Van der Sandt, 1992; Asher, 1993). DRSs can model many challenging semantic phenomena including quantifiers, negation, scope, pronoun resolution, presupposition accommodation, and discourse structure. Moreover, they are directly translatable into first-order logic formulas to account for logical inference. DRG used in the shared task represents a type of graph encoding of DRS that makes the graphs structurally as close as possible to the structures found in other frameworks; Abzianidze et al. (2020) provide more details on the design choices in the DRG encoding. The source DRS annotations are taken from data release 3.0.0 of the Parallel Meaning Bank (PMB; Abzianidze et al., 2017; Bos et al., 2017).6 Although the annotations in the PMB are compositionally derived from lexical semantics, anchoring information is not explicit in its DRSs; thus, (like AMR) the DRG framework formally instantiates Flavor (2) of meaning representations. The DRG of the running example is given in Figure 5. The concepts (vissualized as oval shapes) are represented by WordNet 3.0 senses and semantic roles (in dia"
2020.conll-shared.1,W19-1201,1,0.906261,"Missing"
2020.conll-shared.1,2020.lrec-1.234,1,0.763502,"s (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also more linguistic, qualitative comparison. General availability of parallel gold-standard annotations over the same text samples—drawing from the WSJ and LPPS corpora—enables side-by-side comparison of linguistic design choices in the different frameworks. This is an area of investigation that we hope will see increased interest in the aftermath of the MRP task series, to go well beyond the impressionistic observations from §3 and ideally lead to contrastive refinement across linguistic sc"
2020.conll-shared.1,2020.conll-shared.7,1,0.75652,"Missing"
2020.conll-shared.1,D19-1393,0,0.0309981,"nt alignment parser using stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR"
2020.conll-shared.1,2020.findings-emnlp.89,0,0.016475,"ebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (20"
2020.conll-shared.1,2020.acl-main.119,0,0.179058,"s similar to the use of Factored Concept Labels in Wang and Xue (2017). Another innovation of the PERIN system is that it is trained with a permutation-invariant loss function that returns the same value independently of how the nodes in the graph are ordered. This captures the unordered nature of nodes in (most of the MRP 2020) meaning representation graphs and prevents situations in which the model is penalized for generating the correct nodes in an order that is different from that in the training data. The HIT-SCIR and JBNU systems adopt the iterative inference framework first proposed by Cai and Lam (2020) for Flavor (2) meaning representation graphs that do not enforce strict correspondences between tokens in the input sentence and the concepts in meaning representation graphs. The iterative inference framework is also based on an encoder–decoder architecture. The encoder takes the sentence as input and computes contextualized token embeddings that are used as text memory by a decoder that iteratively predicts the next node given the text memory and a predicted parent node in the partially constructed graph memory at the previous time step, and then identifies the parent node for the newly pre"
2020.conll-shared.1,2020.conll-shared.6,0,0.0921787,"guage per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu 1 Background and Motivation The 2020 Conference on Computational Language Learning (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning Representation Parsing (MRP 2020), which is a revised and extended re-run of a similar CoNLL shared task in the preceding year. The goal of these tasks is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, the MRP task series combines formally and linguistically different approaches to meaning rep1 To reduce the threshold to participation, two of the target frameworks represented in MRP 2019 are not in focus this year, viz. the purely bi-lexical DELPH-IN MRS Bi-Lexical Dependencies and Prague Semantic Dependencies (PSD). These graphs largely overlap with the correspon"
2020.conll-shared.1,P13-2131,0,0.187673,"Missing"
2020.conll-shared.1,W11-2927,1,0.757531,"was not formally enforced. 5 Evaluation Following the previous edition of the shared task, the official MRP metric for the task is the microaverage F1 score across frameworks over all tuple types that encode ‘atoms’ of information in MRP graphs. The cross-framework metric uniformly evaluates graphs of different flavors, regardless of a specific framework exhibiting (a) labeled or unlabeled nodes or edges, (b) nodes with or without anchors, and (c) nodes and edges with optional properties and attributes, respectively (see Table 4). The MRP metric generalizes earlier frameworkspecific metrics (Dridan and Oepen, 2011; Cai and Knight, 2013; Hershcovich et al., 2019a) in terms of decomposing each graph into sets of typed tuples, as indicated in Figure 6. To quantify graph similarity in terms of tuple overlap, a correspondence relation between the nodes of the goldstandard and system graphs must be determined. Adapting a search procedure for the NP-hard maximum common edge subgraph (MCES) isomorphism problem, the MRP scorer will search for the node-to-node correspondence that maximizes the intersection of tuples between two graphs, where node identifiers (m and n in Figure 6) act like variables that can be e"
2020.conll-shared.1,K19-2007,0,0.0722366,"ground and Motivation The 2020 Conference on Computational Language Learning (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning Representation Parsing (MRP 2020), which is a revised and extended re-run of a similar CoNLL shared task in the preceding year. The goal of these tasks is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, the MRP task series combines formally and linguistically different approaches to meaning rep1 To reduce the threshold to participation, two of the target frameworks represented in MRP 2019 are not in focus this year, viz. the purely bi-lexical DELPH-IN MRS Bi-Lexical Dependencies and Prague Semantic Dependencies (PSD). These graphs largely overlap with the corresponding (but richer) frameworks in 2020, EDS and PTG, respectively, and the original bi-lexical semantic dependency graphs remain independently available (Oepen et al., 2015). 1 Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing, pages 1–22 c Online, Nov. 19-20, 2020. 2020 Association for Computational Linguistics (a) a unifying formal model over different semantic graph banks (§2)"
2020.conll-shared.1,1997.iwpt-1.10,0,0.772333,"Missing"
2020.conll-shared.1,W19-1202,0,0.0200921,"ntation format for DRS that was specially designed for MRP 2020 to make it structurally as close as possible to other frameworks (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also more linguistic, qualitative comparison. General availability of parallel gold-standard annotations over the same text samples—drawing from the WSJ and LPPS corpora—enables side-by-side comparison of linguistic design choices in the different frameworks. This is an area of investigation that we hope will see increased interest in the aftermath of the MRP"
2020.conll-shared.1,K19-2016,0,0.0872997,"UCCA, and AMR. This allows a comparison on nearly equal grounds: as Table 9 shows, in terms of LPPS F1 , the state-of-the-art has substantially improved for EDS and AMR parsing, but stayed the same for UCCA. However, as mentioned in §6, remote edge detection for UCCA improved substantially, though it carries only a small weight in terms of overall scores due to the scarcity of remote edges. For EDS, the strongest results were obtained in the MRP 2019 official competition by SUDA– Alibaba (Zhang et al., 2019c). However, in the post-evaluation stage, they were outperformed by the Peking system (Chen et al., 2019). Both used factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard"
2020.conll-shared.1,D19-1278,0,0.0153051,"Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRGs. Similarly to English DRG, German DRG has not been used for semantic parsing prior to the shared task due to the new DRG format. Moreover, semantic parsing with German DRG is novel in the sense that its DRS counterpart is also new. In German DRG, concepts are grounded in English WordNet 3.0 (Fellbaum, 2012) senses assuming that synsets"
2020.conll-shared.1,N18-2020,1,0.822488,"ed on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient), ADDR(essee), ORIG(in), and EFF(ect) indicate ‘participant’ positions in an underlying valency frame and, thus, correspond more closely to the n"
2020.conll-shared.1,P19-4007,0,0.0514516,"Missing"
2020.conll-shared.1,2020.acl-main.629,0,0.125349,"Missing"
2020.conll-shared.1,N18-1104,0,0.0512423,"tudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general availability of accurate data-driven parsers for a broad range of different frameworks, with performance levels ranging between 0.76 MRP F1 (English UCCA) and 0.94 F1 (English EDS). Parsing accuracies in the cross-lingual track present comparable levels of performance, despite limited training data in the case of UCCA and DRG. Furthermore, the evaluation sets for most of the frameworks comprise different text types and subject matters—offering some hope of robustness to"
2020.conll-shared.1,N18-1000,0,0.197513,"Missing"
2020.conll-shared.1,K19-2006,0,0.10434,"been included in the CoNLL 2009 Shared Task on Semantic Role Labeling (Hajiˇc et al., 2009), but the differences in task design are and conversion make empirical comparison impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019),"
2020.conll-shared.1,S19-2002,0,0.0495413,"Missing"
2020.conll-shared.1,hajic-etal-2012-announcing,0,0.357504,"Missing"
2020.conll-shared.1,kingsbury-palmer-2002-treebank,0,0.452155,"o distinguish different types of the underlying DRS elements. logy broadly comparable to EDS, with some notable differences. Similar to the UCCA example graph (and unlike EDS), the AMR representation of the coordinate structure is flat. Although most lemmas are linked to derivationally related forms in the sense lexicon, this is not universal, as seen by the nodes corresponding to similar and such as, which are labeled as resemble-01 and exemplify-01, respectively. These sense distinctions (primarily for verbal predicates) are grounded in the inventory of predicates from the PropBank lexicon (Kingsbury and Palmer, 2002; Hovy et al., 2006). Role labels in AMR encode semantic argument positions, with the particular roles defined according to each PropBank sense, though the counting in AMR is zero-based such that the ARG1 and ARG2 roles in Figure 4 often correspond to ARG2 and ARG3, respectively, in the EDS of Figure 1. PropBank distinguishes such numbered arguments from non-core roles labeled from a general semantic inventory, such as frequency, duration, or domain. Figure 4 also shows the use of inverted edges in AMR, for example ARG1-of and mod. These serve to allow annotators (and in principle also parsing"
2020.conll-shared.1,J16-4009,1,0.877647,"d (d) increased crossfertilization of parsing approaches (§7). 2 tute ordered graphs. A natural way to visualize a bi-lexical dependency graph is to draw its edges as semicircles in the halfplane above the sentence. An ordered graph is called noncrossing if in such a drawing, the semicircles intersect only at their endpoints (this property is a natural generalization of projectivity as it is known from dependency trees). A natural generalization of the noncrossing property, where one is allowed to also use the halfplane below the sentence for drawing edges is a property called pagenumber two. Kuhlmann and Oepen (2016) provide additional definitions and a quantitative summary of various formal graph properties across frameworks. Definitions: Graphs and Flavors Reflecting different traditions and communities, there is wide variation in how individual meaning representation frameworks think (and talk) about semantic graphs, down to the level of visual conventions used in rendering graph structures. Increased terminological uniformity and guidance in how to navigate this rich and diverse landscape are among the desirable side-effects of the MRP task series. The following paragraphs provide semi-formal definiti"
2020.conll-shared.1,P17-1104,1,0.878575,".78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRG"
2020.conll-shared.1,P19-1232,0,0.0175512,"Chen et al., 2019). Both used factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Compara"
2020.conll-shared.1,P18-1035,1,0.856893,"e invited to develop parsing systems that support five distinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a"
2020.conll-shared.1,W19-6202,0,0.0118155,"sed factorization-based parsing with pre-trained contextualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Comparability is limited by"
2020.conll-shared.1,S19-2001,1,0.83448,"Missing"
2020.conll-shared.1,2020.findings-emnlp.288,0,0.0119332,"with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the approach of Zhang et al. (2019b), using cross-lingual transfer learning, outperforming the transition-based cross-lingual AMR parser of Damonte and Cohen (2018) on German, Spanish, Italian, and Chinese. Reflections and Outlook The MRP series of shared tasks has contributed to general"
2020.conll-shared.1,P19-1450,0,0.182109,"istinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four"
2020.conll-shared.1,P18-1040,0,0.0323045,", summarization, or text generation. Maybe equally importantly, the MRP task design capitalizes on uniformity of representations and evaluation, enabling resource creators and parser developers to more closely (inter)relate representations and parsing approaches across a diverse range of semantic graph frameworks. This facilitates DRG is a novel graph representation format for DRS that was specially designed for MRP 2020 to make it structurally as close as possible to other frameworks (Abzianidze et al., 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al., 2019). The shared task gave rise to several DRS parsers: Evang (2019); Liu et al. (2019); van Noord (2019); 16 both quantitative contrastive studies (e.g. the ‘postmortem’ analysis by Buljan et al. (2020), which observes that top-performing MRP 2019 parsers have complementary strengths and weaknesses) but also m"
2020.conll-shared.1,W19-1203,0,0.0982514,"he UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1 calculated based on the DRS clausal forms, which is not comparable to MRP F1 over DRGs. Similarly to English DRG, German DRG has not been used for semantic parsing prior to the shared task due to the new DRG format. Moreover, semantic parsing with German DRG is novel in the sense that its DRS counterpart is also new. In German DRG, concepts are grounded in English WordNet 3.0 (Fellbaum, 2012) senses assuming that synsets are language-neutral. The mismatch between German tokens and English lemmas of senses must be expected to add additional complexity to"
2020.conll-shared.1,2020.acl-main.607,0,0.0482936,"Missing"
2020.conll-shared.1,K19-2001,1,0.563644,"Missing"
2020.conll-shared.1,P18-1037,0,0.0412388,"tokens and English lemmas of senses must be expected to add additional complexity to German DRG parsing. Direct comparison to non-MRP results is impossible: we are using a new version of AMRbank. Gold-standard tokenization is not provided for any of the frameworks. We use the MRP scorer. However, general trends appear consistent with recent developments. Pretrained embeddings and crosslingual transfer help; but multi-task learning less so. There is yet progress to be made in sharing information between parsers for different frameworks and making better use of their overlap. Prior to MRP 2019, Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. Lyu et al. (2020) recently improved the latent alignment parser using stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on al"
2020.conll-shared.1,K19-2003,1,0.845577,"on marks in the left or right periphery of a normalized anchor. Assuming the string Oh no! as a hypothetical parser input, the following anchorings will all be considered equivalent: {h0 : 6i}, {h0 : 2i, h3 : 6i}, {h0 : 1i, h1 : 6i}, and {h0 : 5i}. 6 Six teams submitted parser outputs to the shared task within the official evaluation period. In addition, we received two submissions after the submission deadline, which we mark as ‘unofficial’. We further include results from an additional ‘reference’ system by one of the task co-organizers, namely EDS outputs from the grammar-based ERG parser (Oepen and Flickinger, 2019). Table 5 presents an overview of the participating systems and the tracks and frameworks they submitted results for. All official systems submitted results for the cross-framework track (across all frameworks), and additionally five of them submitted results to the cross-lingual track as well (where TJU-BLCU did not submit UCCA parser outputs in the cross-lingual track). We note that the shared task explicitly allowed partial submissions, in order to lower the bar for participation (which is no doubt substantial). Two of the teams—ISCAS and TJUBLCU—declined the invitation to submit a system d"
2020.conll-shared.1,P14-5010,0,0.0027367,"ces of ‘raw’ sentence strings and (b) in pre-tokenized, partof-speech–tagged, lemmatized, and syntactically parsed form. For the latter, premium-quality morpho-syntactic dependency analyses were provided to participants, called the MRP 2020 companion parses. These parses were obtained using a prerelease of the ‘future’ UDPipe architecture (Straka, 2018; Straka and Strakov´a, 2020), trained on available gold-standard UD 2.x treebanks, for English augmented with conversions from PTB-style annotations in the WSJ and OntoNotes corpora (Hovy et al., 2006), using the UD-style CoreNLP 4.0 tokenizer (Manning et al., 2014) and jack-knifing where appropriate (to avoid overlap with the texts underlying the MRP semantic graphs). Table 4: Different tuple types per framework. on-line CodaLab infrastructure. Teams were allowed to make repeated submissions, but only the most recent successful upload to CodaLab within the evaluation period was considered for the official, primary ranking of submissions. Task participants were encouraged to process all inputs using the same general parsing system, but—owing to inevitable fuzziness about what constitutes ‘one’ parser—this constraint was not formally enforced. 5 Evaluatio"
2020.conll-shared.1,S15-2153,1,0.842966,"Missing"
2020.conll-shared.1,J93-2004,0,0.0699436,", as fully lexically anchored and wholly unanchored, respectively, leading to the categorization of mixed forms of anchoring as Flavor (1), and allow for the presence of ordered graphs, in principle at least, at all levels of the hierarchy.2 Meaning Representation Frameworks The shared task combines five distinct frameworks for graph-based meaning representation, each with its specific formal and linguistic assumptions. This section reviews the frameworks and presents English example graphs for sentence #20209013 from the venerable Wall Street Journal (WSJ) Corpus from the Penn Treebank (PTB; Marcus et al., 1993): (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice. The example exhibits some interesting linguistic complexity, including what is called a tough adjective (impossible), a scopal adverb (almost), a tripartite coordinate structure, and apposition. The example graphs in Figures 1 through 4 are prewhere unanchored nodes for unexpressed material beyond the surface string can be postulated (Schuster and Manning, 2016). Whether or not these nodes occupy a well-defined position in the otherwise total order of basic UD nodes remains an open questi"
2020.conll-shared.1,S16-1166,0,0.0329712,"context of the shared task. ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchoring is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the su"
2020.conll-shared.1,S14-2008,1,0.896688,"Missing"
2020.conll-shared.1,S17-2090,0,0.0310415,"the shared task. ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchoring is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 4 has a topopossible-01 polarity ARG1 apply-02 ARG1 technique almost ARG2 crop (ARG1)-of resemble-01 Abstract Meaning Representation The shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although mo"
2020.conll-shared.1,2020.conll-shared.4,0,0.136232,"(Hajiˇc et al., 2009), but the differences in task design are and conversion make empirical comparison impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word"
2020.conll-shared.1,2020.conll-shared.8,0,0.415434,"Missing"
2020.conll-shared.1,P17-1186,0,0.154444,"Missing"
2020.conll-shared.1,2020.lrec-1.497,1,0.867008,"Missing"
2020.conll-shared.1,W19-1204,0,0.0450391,"Missing"
2020.conll-shared.1,W16-6401,0,0.0666192,"Missing"
2020.conll-shared.1,Q18-1043,1,0.868093,"Missing"
2020.conll-shared.1,2020.conll-shared.5,0,0.310999,"impossible. AMR P R F P R F P R F .92 .97 .93 .97 .93 .97 .84 .86 .82 .80 .83 .83 .74 .78 .72 .79 .73 .79 Table 9: Per-framework cross-task comparison of top MRP metric scores on LPPS between the 2019 and 2020 editions of the MRP task, on the three frameworks represented in both year, for English. The top systems in MRP 2019 for EDS, UCCA, and AMR were Peking (Chen et al., 2019), HIT-SCIR (Che et al., 2019), and Saarland (Donatelli et al., 2019), respectively; in MRP 2020 the Hitachi system (Ozaki et al., 2020) was at the top for all three frameworks, sharing the UCCA first ´ rank with UFAL (Samuel and Straka, 2020). UCCA parsing has been dominated by transitionbased methods (Hershcovich et al., 2017, 2018; Che et al., 2019). However, both English and German UCCA parsing featured in a SemEval shared task (Hershcovich et al., 2019b), where the best system, a composition-based parser (Jiang et al., 2019), treated the task as constituency tree parsing with the recovery of remote edges as a postprocess15 ing task. Fancellu et al. (2019), among which the best results (F1 = 0.85) were achieved by the word-level sequence-to-sequence model with Tranformer (Liu et al., 2019). Note that the DRS shared task used F1"
2020.conll-shared.1,N18-2040,1,0.887249,"Missing"
2020.conll-shared.1,L16-1376,0,0.0817331,"Missing"
2020.conll-shared.1,D17-1129,1,0.843784,"contextualized token embeddings with XLM-R (Conneau et al., 2019) on the encoder side, and then on the decoder side, uses separate attention heads to predict the node labels, identify anchors for nodes, and predict edges between nodes, as well as edge labels. Because the label set for nodes is typically very large, rather than predicting the node labels directly, the PERIN system reduces the search space by predicting ‘relative rules’ that can be used to map surface token strings to node labels in meaning representation graphs, an idea that is similar to the use of Factored Concept Labels in Wang and Xue (2017). Another innovation of the PERIN system is that it is trained with a permutation-invariant loss function that returns the same value independently of how the nodes in the graph are ordered. This captures the unordered nature of nodes in (most of the MRP 2020) meaning representation graphs and prevents situations in which the model is penalized for generating the correct nodes in an order that is different from that in the training data. The HIT-SCIR and JBNU systems adopt the iterative inference framework first proposed by Cai and Lam (2020) for Flavor (2) meaning representation graphs that d"
2020.conll-shared.1,P19-1454,0,0.0201296,"extualized language model embeddings (which has consistently proved to be very effective for other frameworks too). These parsers even approached the performance of the carefully designed grammarbased ERG parser (Oepen and Flickinger, 2019). English PTG has not been comprehensively addressed by parsers prior to MRP 2020, but a bilexical framework called PSD is a subset of PTG. It was included in the SDP shared tasks (Oepen et al., 2014, 2015) as well as in MRP 2019, and has been addressed by numerous parsers since (Kurita and Søgaard, 2019; Kurtz et al., 2019; Jia et al., 2020, among others). Wang et al. (2019) established the state of the art in supervised PSD using a second-order factorization-based parser, and Fern´andez-Gonz´alez and G´omez-Rodr´ıguez (2020) matched it using a stack-pointer parser. On the State of the Art MRP 2019 (Oepen et al., 2019) yielded parsers for five frameworks in a uniform format, of which EDS, UCCA, and AMR are represented in MRP 2020 again. Submissions included transition-, factorization-, and composition-based systems, and gold-standard target structures in 2019 were solely for English. Comparability is limited by the fact that two of the 2020 frameworks (PTG and DR"
2020.conll-shared.1,D18-1263,0,0.0141923,"ng systems that support five distinct semantic graph frameworks in four languages (see §3 below)— all encoding core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; Stanovsky and Dagan, 2018; or Lindemann et al., 2019). The task design aims to reduce frameworkspecific ‘balkanization’ in the field of meaning representation parsing. Its contributions include The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction"
2020.conll-shared.1,2020.emnlp-main.196,0,0.450632,"stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the ap"
2020.conll-shared.1,2020.wmt-1.104,0,0.540185,"stochastic softmax. Lindemann et al. (2019) trained a composition-based parser on five frameworks including AMR and EDS, using the Apply–Modify algebra, on which the third-ranked Saarland submission to MRP 2019 was based (Donatelli et al., 2019). They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. Since then, a new state-of-the-art has been established for English AMR, using sequenceto-sequence transduction (Zhang et al., 2019a,b) and iterative inference with graph encoding (Cai and Lam, 2019, 2020). Xu et al. (2020a) improved sequence-to-sequence parsing for AMR by using pre-trained encoders, reaching similar performance to Cai and Lam (2020). Astudillo et al. (2020) introduced a stack-transformer to enhance transitionbased AMR parsing (Ballesteros and Al-Onaizan, 2017), and Lee et al. (2020) improved it further, using a trained parser for mining oracle actions and combining it with AMR-to-text generation to outperform the state of the-art. 9 Wang et al. (2018) parsed Chinese AMR with a transition-based system. For cross-lingual AMR parsing, Blloshmi et al. (2020) trained an AMR parser similar to the ap"
2020.conll-shared.1,2020.lt4hala-1.20,0,0.0828616,"Missing"
2020.conll-shared.1,2020.conll-shared.3,1,0.64593,"Missing"
2020.conll-shared.1,W15-3502,1,0.865834,"arly to the EDS in Figure 1, boolean edge attributes are abbreviated below edge labels, for true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges fr"
2020.conll-shared.1,P19-1009,0,0.173641,"Missing"
2020.conll-shared.1,D19-1392,0,0.119567,"Missing"
2020.conll-shared.1,N18-1063,1,0.899522,"Missing"
2020.conll-shared.1,K19-2014,0,0.104365,"Missing"
2020.conll-shared.1,P18-1016,1,0.879706,"true values. Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018c), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). the deep object of apply can be argued to not have a semantic contribution of their own. The ADDR argument relation to the apply predicate has been recursively propagated to both elements of the apposition and to all members of the coordinate structure. Accordingly, edge labels in PTG are not always functional, in the sense of allowing multiple outgoing edges from one node with the same label. In FGD, role labels (called functors) ACT(or), PAT(ient),"
2020.starsem-1.6,W11-2123,0,0.0516606,"odel with the highest accuracy (where perplexity was used in cases of ties). The system was evaluated on the development data every 10K steps. For comparison, we also implement our system in the case where the Transformer is replaced by another NMT system, namely a two-layers LSTM model and the Moses phrase-based machine translation system (Koehn et al., 2007). The neural model, also implemented with OpenNMT-py, is trained and validated in the same way as the Transformer. For Moses, the default model is used in a single setting (LessTrain) with MGIZA word alignment,5 and KenLM language model (Heafield, 2011) using the monolingual data provided in WMT 2014, and MERT tuning on the development set. Here too we compare the combined systems to baseline systems which do not perform decomposition. 4 4.1 S −→ Sc1 |Sc2 |· · · |Scn Experimental Setup Evaluation Using Crowdsourcing In addition to the limitations of BLEU evaluation (Papineni et al., 2002) in the context of MT (Callison-Burch et al., 2006, and much subsequent work), BLEU may correlate negatively with output quality in cases that involve sentence splitting (Sulem et al., 2018a). We therefore evaluate using crowdsourcing, and follow the protoco"
2020.starsem-1.6,P13-1023,1,0.826921,"Scenes can provide additional information about an established entity (Elaborator scenes), commonly participles or relative clauses. For example, “(child) who went home” is an Elaborator scene in “The child who went home is John”. A scene may also be a Participant in another scene. For example, “John went home” in the sentence: “He said John went home”. In other cases, scenes are annotated as parallel scenes (H), which are flat structures and may include a Linker (L), as in: “WhenL [he arrives]H , [he will call them]H ”. Semantic Decomposition UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a; Langacker, 2008). It aims to represent the main semantic phe51 For UCCA parsing, we use TUPA, a transitionbased parser (Hershcovich et al., 2017) (specifically, the TUPABiLST M model). We build on the DSS rule-based semantic splitting method (Sulem et al., 2018b), and use Rule #1 which targets parallel scenes. We further explore the use of the additional kinds of scenes in Section 7 for less conservative sentence splitting. In Rule #1, parallel scenes of a given sentence are extracted, split"
2020.starsem-1.6,P17-1104,1,0.804448,"e a Participant in another scene. For example, “John went home” in the sentence: “He said John went home”. In other cases, scenes are annotated as parallel scenes (H), which are flat structures and may include a Linker (L), as in: “WhenL [he arrives]H , [he will call them]H ”. Semantic Decomposition UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a; Langacker, 2008). It aims to represent the main semantic phe51 For UCCA parsing, we use TUPA, a transitionbased parser (Hershcovich et al., 2017) (specifically, the TUPABiLST M model). We build on the DSS rule-based semantic splitting method (Sulem et al., 2018b), and use Rule #1 which targets parallel scenes. We further explore the use of the additional kinds of scenes in Section 7 for less conservative sentence splitting. In Rule #1, parallel scenes of a given sentence are extracted, split into different sentences and concatenated according to the order of appearance. More formally, given a decomposition of a sentence S into parallel scenes Sc1 , Sc2 , · · · Scn (indexed by the order of the first token), we obtain the following rule,"
2020.starsem-1.6,P17-4019,1,0.838075,"Missing"
2020.starsem-1.6,W18-2703,0,0.0277102,"er hand, where splitting is used as preprocessing, adequacy scores decrease. In particular, LessTrain Transformer Baseline significantly outperforms the SemSplit counterpart (47.5 vs. 39.8, p &lt; 10−4 ). For sentences longer than 30, SemSplit Transformer in the LessTrain setting significantly outperforms the baseline in terms of fluency (52.1 vs. 39.6, p = 0.02), with only a non-significant (small) degradation in adequacy (41.7 vs. 40.1, p = 0.46). 6 7 Additional Experiments We first explore the performance of the proposed system in low-resource machine translation, by following the approach of Hoang et al. (2018) and randomly select 1M and 100K sentence pairs from the entire English-French training set, defining the 1MTrain and 100KTrain settings respectively. Tuning and testing remain as before. The resulted raw scores for the 1MTrain and 100KTrain settings are presented in Appendix D, Table 4 . We observe that while in 1MTrain, the SemSplit models obtain low results compared to the respective baselines, the SemSplit models obtain higher fluency in 100KTrain, though not significantly. Second, to further explore the sentence splitting component, we replicate our model, separating both parallel and emb"
2020.starsem-1.6,P18-2114,0,0.0634266,"rease in fluency on sentences longer than 30 words on the newstest2014 test corpus for English-to-French translation, with a training corpus of 5M sentence pairs, without degrading adequacy. Considering all sentence lengths, we observe a tradeoff between fluency and adequacy. We explore it using a manual analysis, suggesting that the decrease in adequacy is partly due to the loss of cohesion resulting from the splitting (§6). We then proceed to investigate the case of simulated low-resource settings as well as the effect of other sentence splitting methods, including Splitand-Rephrase models (Aharoni and Goldberg, 2018; Botha et al., 2018) (§7). The latter yield considerably lower scores than the use of simple semantic rules, supporting the case for corpusindependent simplification rules. Building on recent advances in semantic parsing and text simplification, we investigate the use of semantic splitting of the source sentence as preprocessing for machine translation. We experiment with a Transformer model and evaluate using large-scale crowd-sourcing experiments. Results show a significant increase in fluency on long sentences on an English-toFrench setting with a training corpus of 5M sentence pairs, whil"
2020.starsem-1.6,P07-2045,0,0.00711325,"ansformer Baseline, where no splitting is performed. The pipeline architecture is summarized in Figure 1. The Transformer is trained for 200K training steps, both in the FullTrain and the LessTrain settings. The development data was used for selecting the model with the highest accuracy (where perplexity was used in cases of ties). The system was evaluated on the development data every 10K steps. For comparison, we also implement our system in the case where the Transformer is replaced by another NMT system, namely a two-layers LSTM model and the Moses phrase-based machine translation system (Koehn et al., 2007). The neural model, also implemented with OpenNMT-py, is trained and validated in the same way as the Transformer. For Moses, the default model is used in a single setting (LessTrain) with MGIZA word alignment,5 and KenLM language model (Heafield, 2011) using the monolingual data provided in WMT 2014, and MERT tuning on the development set. Here too we compare the combined systems to baseline systems which do not perform decomposition. 4 4.1 S −→ Sc1 |Sc2 |· · · |Scn Experimental Setup Evaluation Using Crowdsourcing In addition to the limitations of BLEU evaluation (Papineni et al., 2002) in t"
2020.starsem-1.6,D18-1080,0,0.113829,"s longer than 30 words on the newstest2014 test corpus for English-to-French translation, with a training corpus of 5M sentence pairs, without degrading adequacy. Considering all sentence lengths, we observe a tradeoff between fluency and adequacy. We explore it using a manual analysis, suggesting that the decrease in adequacy is partly due to the loss of cohesion resulting from the splitting (§6). We then proceed to investigate the case of simulated low-resource settings as well as the effect of other sentence splitting methods, including Splitand-Rephrase models (Aharoni and Goldberg, 2018; Botha et al., 2018) (§7). The latter yield considerably lower scores than the use of simple semantic rules, supporting the case for corpusindependent simplification rules. Building on recent advances in semantic parsing and text simplification, we investigate the use of semantic splitting of the source sentence as preprocessing for machine translation. We experiment with a Transformer model and evaluate using large-scale crowd-sourcing experiments. Results show a significant increase in fluency on long sentences on an English-toFrench setting with a training corpus of 5M sentence pairs, while retaining comparabl"
2020.starsem-1.6,E06-1032,0,0.0500455,", also implemented with OpenNMT-py, is trained and validated in the same way as the Transformer. For Moses, the default model is used in a single setting (LessTrain) with MGIZA word alignment,5 and KenLM language model (Heafield, 2011) using the monolingual data provided in WMT 2014, and MERT tuning on the development set. Here too we compare the combined systems to baseline systems which do not perform decomposition. 4 4.1 S −→ Sc1 |Sc2 |· · · |Scn Experimental Setup Evaluation Using Crowdsourcing In addition to the limitations of BLEU evaluation (Papineni et al., 2002) in the context of MT (Callison-Burch et al., 2006, and much subsequent work), BLEU may correlate negatively with output quality in cases that involve sentence splitting (Sulem et al., 2018a). We therefore evaluate using crowdsourcing, and follow the protocol proposed by Graham et al. (2016). Evaluation was carried out using Amazon Mechanical Turk.6 See Appendix A for a detailed description. Corpora We experiment on the full EnglishFrench training data provided in the WMT setting (Bojar et al., 2014), which corresponds to about 39M sentence pairs after cleaning.2 We refer to this setting as the FullTrain Setting. We also experiment on the Les"
2020.starsem-1.6,W10-1762,0,0.0247513,"ork was done when being affiliated to the Hebrew University of Jerusalem 1 The code and the evaluation data are available at https://github.com/eliorsulem/ Semantic-Structural-Decomposition-for-NMT This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. Related Work Sentence segmentation for MT. Segmenting sentences into sub-units, based on punctuation and syntactic structures, and recombining their output has been explored by a number of statistical MT works (Xiong et al., 2009; Goh and Sumita, 2011; Sudoh et al., 2010). In NMT, Pouget-Abadie et al. (2014) segmented the source using ILP, tackling English-to-French neural translation. They con50 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 50–57 Barcelona, Spain (Online), December 12–13, 2020 cluded that segmentation improves overall translation quality but quality may decrease if the segmented fragments are not well-formed. The concatenation may sometimes degrade fluency and result in errors in punctuation and capitalization. Kuang and Xiong (2016) attempted to find split positions such that no reordering wil"
2020.starsem-1.6,D18-1081,1,0.93696,"scenes are annotated as parallel scenes (H), which are flat structures and may include a Linker (L), as in: “WhenL [he arrives]H , [he will call them]H ”. Semantic Decomposition UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a; Langacker, 2008). It aims to represent the main semantic phe51 For UCCA parsing, we use TUPA, a transitionbased parser (Hershcovich et al., 2017) (specifically, the TUPABiLST M model). We build on the DSS rule-based semantic splitting method (Sulem et al., 2018b), and use Rule #1 which targets parallel scenes. We further explore the use of the additional kinds of scenes in Section 7 for less conservative sentence splitting. In Rule #1, parallel scenes of a given sentence are extracted, split into different sentences and concatenated according to the order of appearance. More formally, given a decomposition of a sentence S into parallel scenes Sc1 , Sc2 , · · · Scn (indexed by the order of the first token), we obtain the following rule, where “|” is the sentence delimiter: As UCCA allows argument sharing between scenes, the rule may duplicate the sam"
2020.starsem-1.6,P18-1016,1,0.910005,"scenes are annotated as parallel scenes (H), which are flat structures and may include a Linker (L), as in: “WhenL [he arrives]H , [he will call them]H ”. Semantic Decomposition UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a; Langacker, 2008). It aims to represent the main semantic phe51 For UCCA parsing, we use TUPA, a transitionbased parser (Hershcovich et al., 2017) (specifically, the TUPABiLST M model). We build on the DSS rule-based semantic splitting method (Sulem et al., 2018b), and use Rule #1 which targets parallel scenes. We further explore the use of the additional kinds of scenes in Section 7 for less conservative sentence splitting. In Rule #1, parallel scenes of a given sentence are extracted, split into different sentences and concatenated according to the order of appearance. More formally, given a decomposition of a sentence S into parallel scenes Sc1 , Sc2 , · · · Scn (indexed by the order of the first token), we obtain the following rule, where “|” is the sentence delimiter: As UCCA allows argument sharing between scenes, the rule may duplicate the sam"
2020.starsem-1.6,P09-2035,0,0.0374661,"semantic units, namely scenes, 2 ∗ This work was done when being affiliated to the Hebrew University of Jerusalem 1 The code and the evaluation data are available at https://github.com/eliorsulem/ Semantic-Structural-Decomposition-for-NMT This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http: //creativecommons.org/licenses/by/4.0/. Related Work Sentence segmentation for MT. Segmenting sentences into sub-units, based on punctuation and syntactic structures, and recombining their output has been explored by a number of statistical MT works (Xiong et al., 2009; Goh and Sumita, 2011; Sudoh et al., 2010). In NMT, Pouget-Abadie et al. (2014) segmented the source using ILP, tackling English-to-French neural translation. They con50 Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics (*SEM), pages 50–57 Barcelona, Spain (Online), December 12–13, 2020 cluded that segmentation improves overall translation quality but quality may decrease if the segmented fragments are not well-formed. The concatenation may sometimes degrade fluency and result in errors in punctuation and capitalization. Kuang and Xiong (2016) attempted to find"
2021.emnlp-main.394,Q16-1031,0,0.0700581,"Missing"
2021.emnlp-main.394,2020.acl-main.147,0,0.0115734,"ro-shot parsing performance. amod case amod Japanese company sharikat yabania company Japanese Nihon no kaisha Japan of company Figure 1: A UD parse of the English phrase Japanese company (left) and of its translations to Arabic (middle) and Japanese (right). Word order aside, the UD tree of the Arabic translation is identical to the English one while that of the Japanese translation is different. The amod edge in this phrase is stable in translation to Arabic but unstable in translation to Japanese. is through the use of cross-lingual symbolic representation schemes (Chen et al., 2017, 2018; Bugliarello and Okazaki, 2020). Many advances have been made in this area in recent years, most notably the development and quick adoption of Universal Dependencies (UD; Nivre et al., 2016), a cross-lingually applicable scheme that has become the de facto standard for syntactic annotation. While these schemes abstract away from many syntactic differences, there is still considerable variability in the strategies employed to express the 1 Introduction same basic meanings across languages. In this Recent progress in cross-lingual transfer methods, work, we are mainly interested in the flip side of such as multi-lingual embed"
2021.emnlp-main.394,P17-1177,0,0.0182908,"-lingual stability and zero-shot parsing performance. amod case amod Japanese company sharikat yabania company Japanese Nihon no kaisha Japan of company Figure 1: A UD parse of the English phrase Japanese company (left) and of its translations to Arabic (middle) and Japanese (right). Word order aside, the UD tree of the Arabic translation is identical to the English one while that of the Japanese translation is different. The amod edge in this phrase is stable in translation to Arabic but unstable in translation to Japanese. is through the use of cross-lingual symbolic representation schemes (Chen et al., 2017, 2018; Bugliarello and Okazaki, 2020). Many advances have been made in this area in recent years, most notably the development and quick adoption of Universal Dependencies (UD; Nivre et al., 2016), a cross-lingually applicable scheme that has become the de facto standard for syntactic annotation. While these schemes abstract away from many syntactic differences, there is still considerable variability in the strategies employed to express the 1 Introduction same basic meanings across languages. In this Recent progress in cross-lingual transfer methods, work, we are mainly interested in the fl"
2021.emnlp-main.394,2020.findings-emnlp.265,0,0.0879469,"Missing"
2021.emnlp-main.394,D17-1004,0,0.0853949,"each targeting an area of syntax that is known to give rise to cross-lingual divergences in UD terms, and thus create three slightly modified versions of UD. We then apply these transformations to the training set in order to check if these versions of UD lead to improved ZS performance. As attachment scores across different schemes are not comparable, we opt for extrinsic evaluation through ZS cross-lingual relation extraction. Since there are no multilingual RE datasets that include data from non-Western-European languages, we translated a 500-sentence subset of the English TACRED dataset (Zhang et al., 2017) into Korean and Russian and annotated it with the relations from respective English source sentences. Our results show that modified versions of UD give rise to improved results. This indicates that UD can be made more cross-lingually stable without sacrificing its usefulness for downstream tasks. §5. Related work is summarised in §6. Section 7 concludes the paper. 2 Edge Stability and ZS Parsability This section evaluates the relation between the extent to which an edge in a translated sentence corresponds to an edge in the original sentence (which we operationalize as several stability cate"
2021.emnlp-main.619,D15-1075,0,0.036895,"(Rajpurkar et al., 2018).4 This model can also determine that no answer can be found in the paragraph. This is important in Q2 , since question qij generated for a completely hallucinated content ari should have no answer in k. Answer Similarity and Final Scores. The last step in Q2 assesses the similarity between answers ari and akij . To be robust to lexical variability between the response and the knowledge, e.g. “US” vs. “United States” or “a book series” vs. “a set of novels”, we measure the answer span similarity using an NLI model. We use RoBERTa (Liu et al., 2019) fine-tuned on SNLI (Bowman et al., 2015) as implemented in AllenNLP (Gardner et al., 2017). For span pairs ari and akij that match perfectly at the token-level, we assign a score of 1. For each 2 https://spacy.io/ https://huggingface.co/mrm8488/ t5-base-finetuned-question-generation-ap 4 https://huggingface.co/ktrapeznikov/ albert-xlarge-v2-squad-v2 3 span pair ari and akij that do not match perfectly at the token-level, we run the NLI model with akij as the premise and ari as the hypothesis. To add context for the NLI model, each answer is concatenated after the question qij . For example, for the question “Where were the Red Hot C"
2021.emnlp-main.619,P18-1082,0,0.0179822,"(Rajpurkar et al., 2016) as the QG model.3 As suggested by Wang et al. (2020), we use beam search decoding, taking the top-n generated questions for ari . We set n = 5 and test two variants of generating multiple questions. In the first, we use all n questions for ari . In the second variant, we only take the top-ranked question that passed the filtering stage for ari (see “Question Filtering” below). We observed similar trends for both variants, and therefore only report the results of the second variant. To increase the diversity of the generated questions, we tried sampling-based methods (Fan et al., 2018; Holtzman et al., 2020), but obtained inferior results that are not reported in this paper. Question Answering. To mark the answer span akij in the knowledge k for question qij , we use the Albert-Xlarge model (Lan et al., 2020) finetuned on SQuAD2.0 (Rajpurkar et al., 2018).4 This model can also determine that no answer can be found in the paragraph. This is important in Q2 , since question qij generated for a completely hallucinated content ari should have no answer in k. Answer Similarity and Final Scores. The last step in Q2 assesses the similarity between answers ari and akij . To be rob"
2021.emnlp-main.619,N16-1001,0,0.0607014,"Missing"
2021.emnlp-main.619,W04-1013,0,0.0542644,"d.” Even though the response used the knowledge faithfully, one out of two valid generated questions for it was “What is purple?”, for which the response-based answer is “my favorite color”, while the knowledge-based answer is, of course, different. 6 Related Work Automatic Evaluation of Dialogue Systems. Automatically evaluating natural language generation is a notoriously difficult problem, especially when considering open-ended tasks such as dialogue. Standard token-matching metrics, such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005) in machine translation, or ROUGE (Lin, 2004) in summarization, were shown to have weak or no correlation with human judgements for dialogue (Liu et al., 2016; Lowe et al., 2017). Supervised assessment methods learn to predict human-like evaluation scores (Lowe et al., 2017), but they require a significant annotation effort for achieving training data. Recently, Mehri and Eskenazi (2020) and Pang et al. (2020) suggested to use large pretrained language models (Liu et al., 2019; Radford et al., 2019) to develop reference-response-free metrics for dialogue evaluation. Such LMs are also the backbone of the QG, QA and NLI models employed in"
2021.emnlp-main.619,D16-1230,0,0.0796999,"Missing"
2021.emnlp-main.619,2021.ccl-1.108,0,0.0798974,"Missing"
2021.emnlp-main.619,P17-1103,0,0.0232409,"le?”, for which the response-based answer is “my favorite color”, while the knowledge-based answer is, of course, different. 6 Related Work Automatic Evaluation of Dialogue Systems. Automatically evaluating natural language generation is a notoriously difficult problem, especially when considering open-ended tasks such as dialogue. Standard token-matching metrics, such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005) in machine translation, or ROUGE (Lin, 2004) in summarization, were shown to have weak or no correlation with human judgements for dialogue (Liu et al., 2016; Lowe et al., 2017). Supervised assessment methods learn to predict human-like evaluation scores (Lowe et al., 2017), but they require a significant annotation effort for achieving training data. Recently, Mehri and Eskenazi (2020) and Pang et al. (2020) suggested to use large pretrained language models (Liu et al., 2019; Radford et al., 2019) to develop reference-response-free metrics for dialogue evaluation. Such LMs are also the backbone of the QG, QA and NLI models employed in Q2 . Qualitative Analysis. To get a better impression of Q2 ’s operation, we give examples of how it operates in its various stages."
2021.emnlp-main.619,2020.acl-main.173,0,0.0290262,"Missing"
2021.emnlp-main.619,2020.acl-main.64,0,0.206657,"system, and the last two are outputs of MemNet. Each response should be grounded on a sentence from Wikipedia that is relevant to the conversation topic. Since this dataset does not contain explicit annotations for factual consistency of dialog responses, we construct a new dataset with such annotations for dialogues based on the W OW dataset as detailed in Section 4. 3.2 Topical-Chat Topical-Chat (Gopalakrishnan et al., 2019) is a human-human knowledge-grounded conversation dataset. Each dialogue is accompanied by relevant Wikipedia pages, Washington Post articles and fun-facts from Reddit. Mehri and Eskenazi (2020) introduced USR, an evaluation metric that measures different aspects required from dialogue systems. To test USR, they collected human annotations on four different system responses and two human-generated responses for 60 dialog contexts from Topical-Chat. Each response was scored on a “Uses Knowledge” category, among others. Since a model that properly uses the knowledge is expected to use it in a factually consistent manner, we find it interesting to measure Q2 ’s correlation with the human judgements for this category. 3.3 Dialogue NLI or the previous dialogue history. 4 Dataset Creation"
2021.emnlp-main.619,D17-2014,0,0.0587393,"Missing"
2021.emnlp-main.619,2020.amta-research.14,0,0.015208,"e summarization systems (Cao et al., 2018) span, and generated the question: “What are they and in evaluating the factual consistency of generreliant on?”. The answer to this question using the ated summaries (Goodrich et al., 2019; Kry´sci´nski knowledge was “conservation”, which resulted in et al., 2019; Xu et al., 2020). Factual inconsistency assigning this question a score of 0. has been observed in neural machine translation These examples also demonstrate a major ad- (Lee et al., 2019) mainly when considering out7863 of-domain scenarios (Koehn and Knowles, 2017; Wang and Sennrich, 2020; Müller et al., 2020). Concurrently with our work, Dziri et al. (2021) introduced the Benchmark for Evaluation of Grounded INteraction (BEGIN). BEGIN consists of W OW-based dialogue turns annotated for factual consistency with respect to the grounding knowledge. BEGIN models the task of evaluating groundedness as an NLI task and examples are annotated with five labels: entailment, contradiction, hallucination, off-topic and generic, where the last three are all considered to be neutral from an NLI perspective. Also relevant to our work, Rashkin et al. (2021) showed that faithfulness in knowledgegrounded dialogues"
2021.emnlp-main.619,2021.acl-long.536,0,0.0313201,"set, as well as on the Topical-Chat and DialogueNLI datasets, present strong results for Q2 against various baselines. In future work we would like to map parts of a response to different types like chit-chat, persona and factual, in order to evaluate each against its appropriate source of truth. Other directions for future research are to apply Q2 in additional tasks where factual consistency is essential, such as automated fact-checking (Thorne and Vlachos, 2018), and to use its evaluation signal to improve the factual consistency of generation models as proposed by Rashkin et al. (2021) or Nan et al. (2021). Evaluation via Question Answering and Question Generation. QA-based evaluation metrics have been proposed as a means for measuring content coverage in text generation tasks. For example, Eyal et al. (2019) used QA models for abstractive summarization both as an evaluation metric and as an optimization criterion that improved the downstream ROUGE scores by manually constructing questions around entities in the source document. These metrics aim at assessing whether key information from the input documents is expressed in the summaries (Recall-oriented). Durmus et al. (2020) and Wang et al. (2"
2021.emnlp-main.619,2020.acl-main.333,0,0.366796,"with akij as the premise and ari as the hypothesis. To add context for the NLI model, each answer is concatenated after the question qij . For example, for the question “Where were the Red Hot Chili Peppers formed?”, the response answer “LA”, and the knowledge answer “Los Angeles”, we run the NLI model with: “Where were the Red Hot Chili Peppers formed? Los Angeles” as the premise, and with “Where were the Red Hot Chili Peppers formed? LA” as the hypothesis. Our use of NLI differs from prior use of NLI in dialogue evaluation, where it was applied in an end-to-end manner (Welleck et al., 2019; Pang et al., 2020). We set qij ’s score to be 1 for the case of entailment and 0 for contradiction or for cases where the QA model produced no answer. In the neutral case, we take the answers token-level F1 score, as in Wang et al. (2020). Finally, the match scores for all answer pairs are averaged to yield a response-level score, and the response-level scores are averaged to yield a system-level Q2 score. Question Filtering. To alleviate errors made by the automatic QG and QA models, we follow the validation step in Wang et al. (2020); We run the QA model to answer qij with the response r as the input paragrap"
2021.emnlp-main.619,P02-1040,0,0.111027,"en red and blue.”, when the knowledge was: “Purple is a color intermediate between blue and red.” Even though the response used the knowledge faithfully, one out of two valid generated questions for it was “What is purple?”, for which the response-based answer is “my favorite color”, while the knowledge-based answer is, of course, different. 6 Related Work Automatic Evaluation of Dialogue Systems. Automatically evaluating natural language generation is a notoriously difficult problem, especially when considering open-ended tasks such as dialogue. Standard token-matching metrics, such as BLEU (Papineni et al., 2002) or METEOR (Banerjee and Lavie, 2005) in machine translation, or ROUGE (Lin, 2004) in summarization, were shown to have weak or no correlation with human judgements for dialogue (Liu et al., 2016; Lowe et al., 2017). Supervised assessment methods learn to predict human-like evaluation scores (Lowe et al., 2017), but they require a significant annotation effort for achieving training data. Recently, Mehri and Eskenazi (2020) and Pang et al. (2020) suggested to use large pretrained language models (Liu et al., 2019; Radford et al., 2019) to develop reference-response-free metrics for dialogue ev"
2021.emnlp-main.619,P18-2124,0,0.0279548,". In the second variant, we only take the top-ranked question that passed the filtering stage for ari (see “Question Filtering” below). We observed similar trends for both variants, and therefore only report the results of the second variant. To increase the diversity of the generated questions, we tried sampling-based methods (Fan et al., 2018; Holtzman et al., 2020), but obtained inferior results that are not reported in this paper. Question Answering. To mark the answer span akij in the knowledge k for question qij , we use the Albert-Xlarge model (Lan et al., 2020) finetuned on SQuAD2.0 (Rajpurkar et al., 2018).4 This model can also determine that no answer can be found in the paragraph. This is important in Q2 , since question qij generated for a completely hallucinated content ari should have no answer in k. Answer Similarity and Final Scores. The last step in Q2 assesses the similarity between answers ari and akij . To be robust to lexical variability between the response and the knowledge, e.g. “US” vs. “United States” or “a book series” vs. “a set of novels”, we measure the answer span similarity using an NLI model. We use RoBERTa (Liu et al., 2019) fine-tuned on SNLI (Bowman et al., 2015) as i"
2021.emnlp-main.619,D16-1264,0,0.0267906,"we mark informative spans in the response r to serve as target answer spans for the QG system. To this end, we mark all named entities and noun phrases in r using spaCy.2 For example, in “coffee is very acidic” we mark ‘coffee’ as an informative span. Then, a QG system takes each informative span ari and the response r as input and generates the corresponding questions qij for which ari should be the answer. In our example, a generated question for the informative span ‘coffee’ and the response in Figure 2 is “What is very acidic?”. We use T5-base (Raffel et al., 2020) fine-tuned on SQuAD1.1 (Rajpurkar et al., 2016) as the QG model.3 As suggested by Wang et al. (2020), we use beam search decoding, taking the top-n generated questions for ari . We set n = 5 and test two variants of generating multiple questions. In the first, we use all n questions for ari . In the second variant, we only take the top-ranked question that passed the filtering stage for ari (see “Question Filtering” below). We observed similar trends for both variants, and therefore only report the results of the second variant. To increase the diversity of the generated questions, we tried sampling-based methods (Fan et al., 2018; Holtzma"
2021.emnlp-main.619,2021.acl-long.58,0,0.137105,"narios (Koehn and Knowles, 2017; Wang and Sennrich, 2020; Müller et al., 2020). Concurrently with our work, Dziri et al. (2021) introduced the Benchmark for Evaluation of Grounded INteraction (BEGIN). BEGIN consists of W OW-based dialogue turns annotated for factual consistency with respect to the grounding knowledge. BEGIN models the task of evaluating groundedness as an NLI task and examples are annotated with five labels: entailment, contradiction, hallucination, off-topic and generic, where the last three are all considered to be neutral from an NLI perspective. Also relevant to our work, Rashkin et al. (2021) showed that faithfulness in knowledgegrounded dialogues can be improved by using controllable features based on NLI model predictions. based answer comparison allows lexical variability in the Precision-based component as well. Comparing to other automatic evaluation methods of abstractive summaries, the QG-QA based methods showed higher correlations with human judgments of factual consistency. To the best of our knowledge, our work is the first to apply a QG-QA approach for evaluating dialogue generation. 7 Conclusion and Future Work We presented Q2 , an automatic evaluation method for factu"
2021.emnlp-main.619,2021.emnlp-main.529,0,0.0599996,"Missing"
2021.emnlp-main.619,D19-1320,0,0.0360777,"Missing"
2021.emnlp-main.619,2020.acl-main.704,0,0.0161063,"token-based matching as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of Q2 against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements. 1 Figure 1: An example from our dataset. Human messages are in Blue, the generated response is in Orange and the grounding knowledge is in Black at the bottom. The factual inconsistency is marked in Red. Introduction (Sellam et al., 2020; Xu et al., 2020; Goodrich et al., 2019). Yet, evaluating grounded dialogues poses Generative conversational agents show remarkable additional challenges, since dialogue outputs may progress lately (Shuster et al., 2020; Adiwardana refer to the dialogue history and include personal et al., 2020). Yet, generative dialogue models opinions, questions to the user, and general “chitthat are grounded by external knowledge sources still struggle to be consistent with that knowl- chat”, whose consistency with external knowledge is mostly irrelevant. Additionally, many of those edge. Their output is o"
2021.emnlp-main.619,2020.acl-main.4,0,0.0405589,"Missing"
2021.emnlp-main.619,2020.acl-main.222,0,0.195118,"rm a thorough meta-evaluation of Q2 against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements. 1 Figure 1: An example from our dataset. Human messages are in Blue, the generated response is in Orange and the grounding knowledge is in Black at the bottom. The factual inconsistency is marked in Red. Introduction (Sellam et al., 2020; Xu et al., 2020; Goodrich et al., 2019). Yet, evaluating grounded dialogues poses Generative conversational agents show remarkable additional challenges, since dialogue outputs may progress lately (Shuster et al., 2020; Adiwardana refer to the dialogue history and include personal et al., 2020). Yet, generative dialogue models opinions, questions to the user, and general “chitthat are grounded by external knowledge sources still struggle to be consistent with that knowl- chat”, whose consistency with external knowledge is mostly irrelevant. Additionally, many of those edge. Their output is often incompatible with the given knowledge or even completely “hallucinated” metrics require gold-label human-constructed ref(Roller et al., 2020). Figure 1 depicts such incon- erences, while dialogue is an open-ended ta"
2021.emnlp-main.619,C18-1283,0,0.0145209,"et of dialogue responses from two systems on the Wizard of Wikipedia dataset, which we annotated for factual consistency. Extensive experiments on this dataset, as well as on the Topical-Chat and DialogueNLI datasets, present strong results for Q2 against various baselines. In future work we would like to map parts of a response to different types like chit-chat, persona and factual, in order to evaluate each against its appropriate source of truth. Other directions for future research are to apply Q2 in additional tasks where factual consistency is essential, such as automated fact-checking (Thorne and Vlachos, 2018), and to use its evaluation signal to improve the factual consistency of generation models as proposed by Rashkin et al. (2021) or Nan et al. (2021). Evaluation via Question Answering and Question Generation. QA-based evaluation metrics have been proposed as a means for measuring content coverage in text generation tasks. For example, Eyal et al. (2019) used QA models for abstractive summarization both as an evaluation metric and as an optimization criterion that improved the downstream ROUGE scores by manually constructing questions around entities in the source document. These metrics aim at"
2021.emnlp-main.619,2020.acl-main.450,0,0.156863,"2 , pairs automatic evaluation methods for text generation (Celikyil- question generation (QG) and question answering maz et al., 2020). Evaluation approaches that ad- (QA) for dialogue generation evaluation, inspired dress this gap were recently proposed for tasks like by recent work on factual consistency evaluation machine translation and abstractive summarization in abstractive summarization (Durmus et al., 2020; 7856 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7856–7870 c November 7–11, 2021. 2021 Association for Computational Linguistics Wang et al., 2020). Q2 first takes a given generated response as input, and generates questions whose answers are informative spans in the response, using a QG system. It then employs a QA system to find corresponding answer spans in the knowledge that the response should be grounded in. The evaluation score reflects the similarity between each informative response span and its corresponding answer span from the knowledge, for each generated question. Unlike previous QG/QA approaches, which used token-based matching to compare answer spans, we propose a novel comparison method using natural language inference m"
2021.emnlp-main.619,2020.acl-main.326,0,0.0139981,"s an informative stractive summarization systems (Cao et al., 2018) span, and generated the question: “What are they and in evaluating the factual consistency of generreliant on?”. The answer to this question using the ated summaries (Goodrich et al., 2019; Kry´sci´nski knowledge was “conservation”, which resulted in et al., 2019; Xu et al., 2020). Factual inconsistency assigning this question a score of 0. has been observed in neural machine translation These examples also demonstrate a major ad- (Lee et al., 2019) mainly when considering out7863 of-domain scenarios (Koehn and Knowles, 2017; Wang and Sennrich, 2020; Müller et al., 2020). Concurrently with our work, Dziri et al. (2021) introduced the Benchmark for Evaluation of Grounded INteraction (BEGIN). BEGIN consists of W OW-based dialogue turns annotated for factual consistency with respect to the grounding knowledge. BEGIN models the task of evaluating groundedness as an NLI task and examples are annotated with five labels: entailment, contradiction, hallucination, off-topic and generic, where the last three are all considered to be neutral from an NLI perspective. Also relevant to our work, Rashkin et al. (2021) showed that faithfulness in knowle"
2021.emnlp-main.619,P19-1363,0,0.219777,"rization evaluation, our work is the first to apply them to knowledgegrounded dialogues, which hold distinct properties compared to other grounded generation tasks; Mixing different types of utterances such as knowledge, personal statements and chit-chat in a single response is unique to dialogue and is well addressed by our metric given its modular nature and robustness to lexical variability. We assess Q2 against other reference-responsefree metrics on three dialogue benchmarks: Wizard of Wikipedia (W OW; Dinan et al., 2019), TopicalChat (Gopalakrishnan et al., 2019) and Dialogue NLI (DNLI; Welleck et al., 2019). To foster proper evaluation, we curate a new dataset of dialogue system responses using the W OW dataset, manually annotated for factual consistency. Q2 reaches significantly higher correlations with human judgments on all datasets compared to the other metrics, demonstrating its potential as an evaluation framework for grounded dialogue generation. To summarize, our contributions in this work are three-fold: (1) We develop a novel framework for evaluating the factual consistency of knowledge– grounded, open-domain dialogue systems, incorporating question generation, question answering and N"
2021.emnlp-main.619,2020.acl-main.455,0,0.173476,"as done in previous work. To foster proper evaluation, we curate a novel dataset of dialogue system outputs for the Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We perform a thorough meta-evaluation of Q2 against other metrics using this dataset and two others, where it consistently shows higher correlation with human judgements. 1 Figure 1: An example from our dataset. Human messages are in Blue, the generated response is in Orange and the grounding knowledge is in Black at the bottom. The factual inconsistency is marked in Red. Introduction (Sellam et al., 2020; Xu et al., 2020; Goodrich et al., 2019). Yet, evaluating grounded dialogues poses Generative conversational agents show remarkable additional challenges, since dialogue outputs may progress lately (Shuster et al., 2020; Adiwardana refer to the dialogue history and include personal et al., 2020). Yet, generative dialogue models opinions, questions to the user, and general “chitthat are grounded by external knowledge sources still struggle to be consistent with that knowl- chat”, whose consistency with external knowledge is mostly irrelevant. Additionally, many of those edge. Their output is often incompatible"
2021.emnlp-main.619,P18-1205,0,0.0513647,"Missing"
2021.emnlp-main.806,2020.acl-main.385,0,0.0328306,"nd to distinct concepts. Our experiments indicate a substantial regularity in the BERT-space. We see regions in the space that correspond to distinct senses. These regions can be recovered using our technique; for example, by sampling points around a pseudoword and looking at the points in the BERT-space which decode to it. Moreover, we see that between sense-regions there are often “voids” in the space that do not correspond to any intelligible sense. 2 Analyzing Contextual Representations resentations is directly analyzing the attention weights and activation patterns (Brunner et al., 2020; Abnar and Zuidema, 2020). Criticism against some instances of this approach is found in Jain and Wallace (2019), who claimed that attention weights are less transparent than is often stipulated. The shortcomings of probing. Some recent work has taken a more critical view regarding probing techniques (Belinkov, 2021). Elazar et al. (2021) argue that while probing methods might show that certain linguistic properties exist in a representation, they do not reveal how and if this information is being used by the probing model. This could be due to the disconnect between the representation itself and the probing model. Re"
2021.emnlp-main.806,P17-1080,0,0.0652387,"Missing"
2021.emnlp-main.806,D09-1046,0,0.041057,"e the pseudoword-space for its own sake—pseudowords are a tool to shed light on the geometry and behavior of the BERT-space—our experiments with pseudowords and artificially perturbed pseudowords reveal that the pseudowordspace contains regions that are semantically coherent as inputs to BERT. Prospects for the MaPP technique. Our dataset is manually curated to control for specific linguistic phenomena. We expect that pseudoword may be less semantically targeted if learned with larger contexts that create more opportunities for confounds. We note also that senses are not necessarily discrete (Erk and McCarthy, 2009), and it would be worthwhile to explore how graded semantic distinctions are represented, as well as underspecified meanings. We are also interested in exploring how BERT represents tokens in sentences that permit multiple plausible interpretations. The MaPP technique can be applied to investigate the properties of other CR models as well, as it requires only that the model be a differentiable function from input token embeddings to contextualized embeddings. 7 Conclusion there is substantial regularity in the BERT-space, with regions that correspond to distinct senses. Moreover, we found evid"
2021.emnlp-main.806,D19-1006,0,0.0180079,"extent to which the lexical semantic classes of nouns are disam- sentences so it remains to be seen whether pseudowords can be induced to capture word senses “in biguated by CRs (Zhao et al., 2020), showing that the wild”. BERT fares well in this respect. Beyond classifier-based probes, other ap- The geometry of BERT. Understanding the geproaches have also been explored, such as informa- ometry of the BERT-space is not easy. Some attion theoretic probing (Pimentel et al., 2020; Voita tempts in this direction have been made (Coenen and Titov, 2020), and structural probing (Hewitt et al., 2019; Ethayarajh, 2019; Michael et al., 2020; and Manning, 2019), which evaluates whether syn- Mickus et al., 2020; Xypolopoulos et al., 2021; tax trees are embedded in a linear transformation Garí Soler and Apidianaki, 2020), but a more thorof a CR’s word representation space. ough investigation is lacking. As opposed to predicAn alternative approach to probing learned rep- tive methods such as probing, descriptive methods 10301 that rely on geometric features of the space analyze the information in CRs directly. This paper takes a different approach that views BERT as a function that is defined over a continuous"
2021.emnlp-main.806,2021.acl-long.540,0,0.0409096,"ng from a predefined probing classifiers to see how well the CRs may serve as features in predicting specific properties. sense inventory (Bevilacqua et al., 2021). Disambiguation can also be defined indirectly, through The intuition is that if the CR can be used to predict a specific property, then knowledge about it is en- minimal pairs that contrast two senses of a word (Trott and Bergen, 2021) or through another word coded in the representation. Recent classifier-based probes have focused on various linguistic proper- in the text that determines the semantic class of the word in question (Jiang and Riloff, 2021). Our ties such as morphology, parts of speech, sentence work bears on this line of work as well: we are length, and syntactic and semantic relations (Liu et al., 2019; Conneau et al., 2018; Belinkov et al., using MaPP to test whether the masked prediction indicates that the pseudoword encodes the expected 2017; Adi et al., 2016, inter alia). Closely related sense. However, we are using carefully controlled to ours is work that studied the extent to which the lexical semantic classes of nouns are disam- sentences so it remains to be seen whether pseudowords can be induced to capture word sense"
2021.emnlp-main.806,S19-1026,0,0.0545581,"Missing"
2021.emnlp-main.806,N19-1112,0,0.173523,"ts (§5.2). In other experiments, the pseudoword is perturbed prior to step 3. Introduction Vector spaces defined over static word vectors are somewhat interpretable, as the points are limited to the vocabulary. Contextualized representations (CRs), by contrast, are mysterious because of the unbounded number of distinct contextualized embeddings, and no obvious way to discover the word and context that would correspond to an arbitrary point in the space. Attempts have been made to characterize the information captured in contextualized representations (Rogers et al., 2020; Tenney et al., 2019; Liu et al., 2019), but some of the techniques used (e.g., probing classifiers) have been subject to criticism for their indirectness. We propose a new technique called Masked Pseudoword Probing (MaPP) that allows controlled exploration of the space of a contextualized masked LMs (specifically, English BERT; Devlin et al., 2019). MaPP takes advantage of the static embedding at the first layer of BERT and “hallucinates” new embeddings into this space to correspond to tokens’ contextualized representations. By extending BERT’s vocabulary with these pseudowords, we can use them as inputs for masked prediction of w"
2021.emnlp-main.806,D19-1007,0,0.0629295,"Missing"
2021.emnlp-main.806,2020.emnlp-main.552,0,0.0385908,"Missing"
2021.emnlp-main.806,2020.scil-1.35,0,0.0838615,"Missing"
2021.emnlp-main.806,2020.acl-main.420,0,0.0125787,"ted 2017; Adi et al., 2016, inter alia). Closely related sense. However, we are using carefully controlled to ours is work that studied the extent to which the lexical semantic classes of nouns are disam- sentences so it remains to be seen whether pseudowords can be induced to capture word senses “in biguated by CRs (Zhao et al., 2020), showing that the wild”. BERT fares well in this respect. Beyond classifier-based probes, other ap- The geometry of BERT. Understanding the geproaches have also been explored, such as informa- ometry of the BERT-space is not easy. Some attion theoretic probing (Pimentel et al., 2020; Voita tempts in this direction have been made (Coenen and Titov, 2020), and structural probing (Hewitt et al., 2019; Ethayarajh, 2019; Michael et al., 2020; and Manning, 2019), which evaluates whether syn- Mickus et al., 2020; Xypolopoulos et al., 2021; tax trees are embedded in a linear transformation Garí Soler and Apidianaki, 2020), but a more thorof a CR’s word representation space. ough investigation is lacking. As opposed to predicAn alternative approach to probing learned rep- tive methods such as probing, descriptive methods 10301 that rely on geometric features of the space analyze"
2021.emnlp-main.806,2020.tacl-1.54,0,0.0446319,"od as used in the specialization experiments (§5.2). In other experiments, the pseudoword is perturbed prior to step 3. Introduction Vector spaces defined over static word vectors are somewhat interpretable, as the points are limited to the vocabulary. Contextualized representations (CRs), by contrast, are mysterious because of the unbounded number of distinct contextualized embeddings, and no obvious way to discover the word and context that would correspond to an arbitrary point in the space. Attempts have been made to characterize the information captured in contextualized representations (Rogers et al., 2020; Tenney et al., 2019; Liu et al., 2019), but some of the techniques used (e.g., probing classifiers) have been subject to criticism for their indirectness. We propose a new technique called Masked Pseudoword Probing (MaPP) that allows controlled exploration of the space of a contextualized masked LMs (specifically, English BERT; Devlin et al., 2019). MaPP takes advantage of the static embedding at the first layer of BERT and “hallucinates” new embeddings into this space to correspond to tokens’ contextualized representations. By extending BERT’s vocabulary with these pseudowords, we can use t"
2021.emnlp-main.806,P18-1018,1,0.796855,"he ambiguous word “for” has a PURPOSE sense, strongly signaled by “reading”. All sentences were reviewed by a linguist to maximize naturalness and minimize ambiguity. The dataset consists of 3 portions, each used in different experiments. We describe each portion adjacent to the relevant experiment. Relational words as a test case. We chose to focus our analysis on the ambiguity of relational words in English, specifically prepositions and verbs. Relational words present an interesting test case: many are highly ambiguous and encode basic semantic distinctions, such as space, time and manner (Schneider et al., 2018). We do not attempt to cover all possible senses of the selected words; instead, we have constructed our dataset to illustrate just a few clear contrasts (see further discussion in appendix A.4). 5 Experiments Query The dinner is on Monday. Top 5 predictions z fire 7 offer 7 sale 7 Friday 3 hold 7 z∗ Sunday 3 Saturday 3 Thursday 3 Tuesday 3 Friday 3 The clip is z minute 7 year 7 second 7 day 7 week 7 about a queen. z∗ woman 3 girl 3 man 3 child 3 boy 3 Table 2: Specialization examples where the pseudoword z∗ learned from the query sentence corresponds to a different sense from BERT’s static wo"
2021.emnlp-main.806,N19-1162,0,0.0216687,"present MaPP to study this hypothesis, and a continuous function also allows us to invert it, in doing so introduce the concept of pseudowords. and obtain a point in the inverse image z∗ of BERT This concept opens additional research questions. by solving an optimization problem. We note that ∗ d viewing the BERT space as a continuous space, e.g., Specialization. Let z ∈ R be a pseudoword obfor purposes of mapping between it and other con- tained by solving eq. (1) for a sentence s with a focus token t and cue token at position j, holding tinuous spaces, is an increasingly common practice ∗ (Schuster et al., 2019; Gauthier and Levy, 2019); a sense η. Does z yield a sense distribution (determined by its slot fillers in the jth position) that see further discussion in appendix A.4. concentrates on η? That is, does a pseudoword In our experiments (§5), the pseudowords will help us explore the geometry of the BERT-space, decode to a specific sense of the focus token? by traveling across it in a “continuous” way— Generalization. Is it possible to transplant a pseudoword into a sentence where the context something that is not possible to do with the BERT around the focus token is different, and still obtain"
2021.emnlp-main.806,J98-1004,0,0.882729,"Missing"
2021.emnlp-main.806,2021.naacl-main.8,1,0.826036,"Missing"
2021.emnlp-main.806,2021.acl-long.550,0,0.0392035,"ns ambiguation (WSD) aims at making explicit the like BERT is widely investigated in recent NLP semantics of a word in context, typically by identiresearch. Probing methods use CRs as inputs to fying the most suitable meaning from a predefined probing classifiers to see how well the CRs may serve as features in predicting specific properties. sense inventory (Bevilacqua et al., 2021). Disambiguation can also be defined indirectly, through The intuition is that if the CR can be used to predict a specific property, then knowledge about it is en- minimal pairs that contrast two senses of a word (Trott and Bergen, 2021) or through another word coded in the representation. Recent classifier-based probes have focused on various linguistic proper- in the text that determines the semantic class of the word in question (Jiang and Riloff, 2021). Our ties such as morphology, parts of speech, sentence work bears on this line of work as well: we are length, and syntactic and semantic relations (Liu et al., 2019; Conneau et al., 2018; Belinkov et al., using MaPP to test whether the masked prediction indicates that the pseudoword encodes the expected 2017; Adi et al., 2016, inter alia). Closely related sense. However,"
2021.emnlp-main.806,2020.emnlp-main.14,0,0.0239961,"Missing"
2021.emnlp-main.806,2020.acl-main.383,0,0.0254198,"recent work has taken a more critical view regarding probing techniques (Belinkov, 2021). Elazar et al. (2021) argue that while probing methods might show that certain linguistic properties exist in a representation, they do not reveal how and if this information is being used by the probing model. This could be due to the disconnect between the representation itself and the probing model. Relying on classifiers to interpret representations might be problematic; they add additional confounds to the interpretability of the results, and different representations may need different classifiers (Wu et al., 2020; Zhou and Srikumar, 2021). Another critique concerns the difference between correlation and causation (Feder et al., 2021): classifier-based probes may rely on shallow correlations in the training set, thus reflecting data artifacts that are irrelevant to the studied distinction. Probing representations. Deciphering the inforWord Sense Disambiguation. Word Sense Dismation encoded in contextualized representations ambiguation (WSD) aims at making explicit the like BERT is widely investigated in recent NLP semantics of a word in context, typically by identiresearch. Probing methods use CRs as i"
2021.emnlp-main.806,2021.eacl-main.297,0,0.0142132,"eudowords can be induced to capture word senses “in biguated by CRs (Zhao et al., 2020), showing that the wild”. BERT fares well in this respect. Beyond classifier-based probes, other ap- The geometry of BERT. Understanding the geproaches have also been explored, such as informa- ometry of the BERT-space is not easy. Some attion theoretic probing (Pimentel et al., 2020; Voita tempts in this direction have been made (Coenen and Titov, 2020), and structural probing (Hewitt et al., 2019; Ethayarajh, 2019; Michael et al., 2020; and Manning, 2019), which evaluates whether syn- Mickus et al., 2020; Xypolopoulos et al., 2021; tax trees are embedded in a linear transformation Garí Soler and Apidianaki, 2020), but a more thorof a CR’s word representation space. ough investigation is lacking. As opposed to predicAn alternative approach to probing learned rep- tive methods such as probing, descriptive methods 10301 that rely on geometric features of the space analyze the information in CRs directly. This paper takes a different approach that views BERT as a function that is defined over a continuous space. Our proposed methodology thus allows for a more direct inspection of “gaps” between embedded tokens, that does n"
2021.findings-acl.231,P13-1023,1,0.902853,"zed Graph Convolution Network (C-GCN; Zhang et al., 2018) represents a method for exposing structure representation to the machinery of a deep neural network. C-GCN uses a sentence’s UD structure as explicit input to the neural network, resulting in a model whose results are both competitive and interpretable. In this paper we explore whether we can adopt broad coverage semantic structure to the same effect, and whether we are able to observe improved performance in comparison to the baseline model, based on syntactic structure. We focus on the Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) framework as a test case, but note that our method can be easily extended to other semantic representations. Our results demonstrate that broad coverage semantic structures, including those that, like UCCA, require representation by directed acyclic graphs (DAGs), can be integrated effectively in neural networks for relation extraction. 2614 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 2614–2626 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Background We introduce the representation approaches and datasets we employ in our study. 2.1"
2021.findings-acl.231,D15-1056,0,0.0235179,"fication problem, rather than the multi label classification problem posed by the 41 relations of the TACRED ontology, it nonetheless highlights the potential for leveraging trigger words and sentence graph representations (syntactic as in UD v1 basic and semantic as in UCCA) for construction of relation matching systems. Indeed, comprehensive trigger word selection per relation is not scalable if done manually, however we could utilize word embedding techniques, so that trigger word matching could be performed using a vector distance threshold, which would capture similar trigger word terms (Batista et al., 2015). In the next sections, we describe more sophisticated machinery, which realizes a softer notion of matching between the paths of the training data, and those of the test set. 4 C-GCN for Semantic Representation We propose a supervised deep-neural-network model that explicitly utilizes sentence graph representations, so that we may compare the utility of UCCA and UD paths (and their combination). The model receives a sentence and two entity spans (subject and object) as input and gives preference to the word representations corresponding to the syntactic or semantic path between subject and ob"
2021.findings-acl.231,2020.acl-tutorials.1,0,0.0303769,"d 1 Code can be found on GitHub at https: //github.com/yyellin/gcn-over-semanticrepresentations. In supervised RE, a multi-class classifier is trained to determine whether a relation between entities is evoked by a text. With the increased predominance of end-to-end neural network architectures in NLP practice, it is not surprising that recent work on supervised relation extraction has focused on adapting end-to-end neural systems for the task (Peters et al., 2019; Yamada et al., 2020). However, end-to-end neural models pose interpretability and customization challenges (Conneau et al., 2018; Belinkov et al., 2020), motivating the study of hybrid models, in which the neural architecture is fed explicit structure representations. The Contextualized Graph Convolution Network (C-GCN; Zhang et al., 2018) represents a method for exposing structure representation to the machinery of a deep neural network. C-GCN uses a sentence’s UD structure as explicit input to the neural network, resulting in a model whose results are both competitive and interpretable. In this paper we explore whether we can adopt broad coverage semantic structure to the same effect, and whether we are able to observe improved performance"
2021.findings-acl.231,2020.conll-shared.1,1,0.830938,"Missing"
2021.findings-acl.231,L16-1376,0,0.0547368,"Missing"
2021.findings-acl.231,2020.acl-demos.7,0,0.0125279,"ctures.1 1 Introduction Early work on RE focused on pattern-based rules for capturing the structure of relation-evoking words and phrases. These rules are applied over text to identify entity relations in much the same way a regular expression would be applied to discover matching text. The pattern machinery spans from simple, regular-expression like, surface patterns (Brin, 1999; Agichtein and Gravano, 2000), through systems that integrate both lexical features and syntactic dependencies into the pattern construct (Mintz et al., 2009). The PredPatt and pyBART frameworks (Zhang et al., 2017a; Tiktinsky et al., 2020) are examples of syntactic dependency based systems that leverage a set of rules defined 1 Code can be found on GitHub at https: //github.com/yyellin/gcn-over-semanticrepresentations. In supervised RE, a multi-class classifier is trained to determine whether a relation between entities is evoked by a text. With the increased predominance of end-to-end neural network architectures in NLP practice, it is not surprising that recent work on supervised relation extraction has focused on adapting end-to-end neural systems for the task (Peters et al., 2019; Yamada et al., 2020). However, end-to-end n"
2021.findings-acl.231,2020.emnlp-main.523,0,0.0153361,"hang et al., 2017a; Tiktinsky et al., 2020) are examples of syntactic dependency based systems that leverage a set of rules defined 1 Code can be found on GitHub at https: //github.com/yyellin/gcn-over-semanticrepresentations. In supervised RE, a multi-class classifier is trained to determine whether a relation between entities is evoked by a text. With the increased predominance of end-to-end neural network architectures in NLP practice, it is not surprising that recent work on supervised relation extraction has focused on adapting end-to-end neural systems for the task (Peters et al., 2019; Yamada et al., 2020). However, end-to-end neural models pose interpretability and customization challenges (Conneau et al., 2018; Belinkov et al., 2020), motivating the study of hybrid models, in which the neural architecture is fed explicit structure representations. The Contextualized Graph Convolution Network (C-GCN; Zhang et al., 2018) represents a method for exposing structure representation to the machinery of a deep neural network. C-GCN uses a sentence’s UD structure as explicit input to the neural network, resulting in a model whose results are both competitive and interpretable. In this paper we explore"
2021.findings-acl.231,W17-6944,0,0.0606792,"Missing"
2021.findings-acl.231,D18-1244,0,0.35032,"ntities is evoked by a text. With the increased predominance of end-to-end neural network architectures in NLP practice, it is not surprising that recent work on supervised relation extraction has focused on adapting end-to-end neural systems for the task (Peters et al., 2019; Yamada et al., 2020). However, end-to-end neural models pose interpretability and customization challenges (Conneau et al., 2018; Belinkov et al., 2020), motivating the study of hybrid models, in which the neural architecture is fed explicit structure representations. The Contextualized Graph Convolution Network (C-GCN; Zhang et al., 2018) represents a method for exposing structure representation to the machinery of a deep neural network. C-GCN uses a sentence’s UD structure as explicit input to the neural network, resulting in a model whose results are both competitive and interpretable. In this paper we explore whether we can adopt broad coverage semantic structure to the same effect, and whether we are able to observe improved performance in comparison to the baseline model, based on syntactic structure. We focus on the Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) framework as a test case, but"
2021.findings-acl.231,D17-1004,0,0.400304,"phbased semantic structures.1 1 Introduction Early work on RE focused on pattern-based rules for capturing the structure of relation-evoking words and phrases. These rules are applied over text to identify entity relations in much the same way a regular expression would be applied to discover matching text. The pattern machinery spans from simple, regular-expression like, surface patterns (Brin, 1999; Agichtein and Gravano, 2000), through systems that integrate both lexical features and syntactic dependencies into the pattern construct (Mintz et al., 2009). The PredPatt and pyBART frameworks (Zhang et al., 2017a; Tiktinsky et al., 2020) are examples of syntactic dependency based systems that leverage a set of rules defined 1 Code can be found on GitHub at https: //github.com/yyellin/gcn-over-semanticrepresentations. In supervised RE, a multi-class classifier is trained to determine whether a relation between entities is evoked by a text. With the increased predominance of end-to-end neural network architectures in NLP practice, it is not surprising that recent work on supervised relation extraction has focused on adapting end-to-end neural systems for the task (Peters et al., 2019; Yamada et al., 20"
2021.findings-acl.231,D19-1005,0,0.0502719,"Missing"
2021.naacl-main.8,silveira-etal-2014-gold,0,0.0362777,"Missing"
2021.naacl-main.8,W09-2415,0,0.0159445,"ed average (i.e., Elayer ) of one task is higher than another, whereas (1) 87 in each sub-set (by a given context length) it is lower. This may occur when the weight of the sub-sets differs between the two aggregations. 3 Experiments We hypothesize that the context length is a mediating factor in the Elayer of a task. In order to test this hypothesis, we run the following experiments, aiming at isolating the context length. We use the SPR1 dataset (Reisinger et al., 2015) to probe SPR, the English Web Treebank for the Dep. task (Silveira et al., 2014), the SemEval 2010 Task 8 for the RC task (Hendrickx et al., 2009), and the OntoNotes 5.0 dataset (Weischedel et al., 2013) for the other tasks. Configurations follow the defaults in the Jiant toolkit implementation (Wang et al., 2019). In addition, we work with the BERTbase model. 3.1 Figure 2: Elayer as a function of a threshold on the context length. For each such threshold thr (x-axis), Elayer (y-axis) is computed based only on the examples with context length no longer than thr. 3.1.1 Manipulating the Context Length Distribution: An Extreme Case. We begin by examining two specific tasks: Dep. and NER, and their Elayer for each context length’s range. We"
2021.naacl-main.8,P19-1452,0,0.0269222,"Missing"
2021.naacl-main.8,N16-1082,0,0.0206741,"ad to contradictory conclusions as to the localization patterns of the network, depending on the distribution of the probing dataset. Indeed, when probing BERT with seven tasks, we find that it is possible to get 196 different rankings between them when manipulating the distribution of context lengths in the probing dataset. We conclude by presenting best practices for conducting such comparisons in the future.1 1 Introduction The strong performance of end-to-end models and the difficulty in understanding their inner workings has led to extensive research aimed at interpreting their behavior (Li et al., 2016; Yosinski et al., 2015; Karpathy et al., 2015). This notion has led researchers to investigate the behavioral traits of networks in general (Li et al., 2015; Hacohen et al., 2020) and representative architectures in particular (Schlichtkrull et al., 2020). Within NLP, Transformer-based pretrained embeddings are the basis for many tasks, which underscores the importance in interpreting their behavior (Belinkov et al., 2020), and especially the behavior of BERT (Devlin et al., 2019; Rogers et al., 2020), perhaps the most widely used of Transformer-based models. In this work, we analyze the comm"
2021.naacl-main.8,D18-1179,0,0.0639342,"Missing"
2021.naacl-main.8,Q15-1034,0,0.0345791,"Missing"
2021.naacl-main.8,N09-1028,0,0.00980362,"the context length, which we define as the number of tokens whose processing is minimally required to perform the prediction. We operationalize this notion by defining it as the maximal distance between any two tokens for which a label is predicted. This amounts to the span length in tasks that involve a single span (e.g., NER), and to the dependency length in tasks that address the relation between two spans. See §2. Our motivation for considering context length as a mediator is grounded in previous work that presented the difficulty posed by long-distance dependencies in various NLP tasks (Xu et al., 2009; Sennrich, 2017), Probing neural models for the ability to perform downstream tasks using their activation patterns is often used to localize what parts of the network specialize in performing what tasks. However, little work addressed potential mediating factors in such comparisons. As a test-case mediating factor, we consider the prediction’s context length, namely the length of the span whose processing is minimally required to perform the prediction. We show that not controlling for context length may lead to contradictory conclusions as to the localization patterns of the network, depend"
2021.naacl-main.8,2020.tacl-1.54,0,0.041451,"standing their inner workings has led to extensive research aimed at interpreting their behavior (Li et al., 2016; Yosinski et al., 2015; Karpathy et al., 2015). This notion has led researchers to investigate the behavioral traits of networks in general (Li et al., 2015; Hacohen et al., 2020) and representative architectures in particular (Schlichtkrull et al., 2020). Within NLP, Transformer-based pretrained embeddings are the basis for many tasks, which underscores the importance in interpreting their behavior (Belinkov et al., 2020), and especially the behavior of BERT (Devlin et al., 2019; Rogers et al., 2020), perhaps the most widely used of Transformer-based models. In this work, we analyze the common approach of probing (§2), used to localize where “knowledge” 1 The code is available at https://github.com/ lovodkin93/BERT-context-distance. 86 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 86–93 June 6–11, 2021. ©2021 Association for Computational Linguistics Once all the {∆(l) }12 l=1 are computed, we may compute Elayer : and particularly in previous work that indicated the Transformers’ diffic"
2021.naacl-main.8,E17-2060,0,0.0172486,"th, which we define as the number of tokens whose processing is minimally required to perform the prediction. We operationalize this notion by defining it as the maximal distance between any two tokens for which a label is predicted. This amounts to the span length in tasks that involve a single span (e.g., NER), and to the dependency length in tasks that address the relation between two spans. See §2. Our motivation for considering context length as a mediator is grounded in previous work that presented the difficulty posed by long-distance dependencies in various NLP tasks (Xu et al., 2009; Sennrich, 2017), Probing neural models for the ability to perform downstream tasks using their activation patterns is often used to localize what parts of the network specialize in performing what tasks. However, little work addressed potential mediating factors in such comparisons. As a test-case mediating factor, we consider the prediction’s context length, namely the length of the span whose processing is minimally required to perform the prediction. We show that not controlling for context length may lead to contradictory conclusions as to the localization patterns of the network, depending on the distri"
C08-1002,P98-1013,0,0.153692,"Missing"
C08-1002,P98-1046,0,0.0308457,"where most instances were monosemous. For completeness, we compared our method to theirs2 , achieving similar results. We review related work in Section 2, and discuss the task in Section 3. Section 4 introduces the model, Section 5 describes the experimental setup, and Section 6 presents our results. 2 Related Work VerbNet. VN is a major electronic English verb lexicon. It is organized in a hierarchical structure of classes and sub-classes, each sub-class inheriting the full characterization of its super-class. VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically. VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004). VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type). Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class). VN’s significant coverage of the English verb lexicon is demonstrated by the 3 Propbank (Palmer et al., 2005) is a corpus annotation of the WSJ section"
C08-1002,C96-1055,0,0.370452,"Missing"
C08-1002,W01-0502,0,0.0298959,"1 >, < x2 , C2 , c2 >, ..., < xn , Cn , cn > } ⊆ (X × 2S × S)n , where n is the size of the training set. Let < xn+1 , Cn+1 >∈ (X × 2S ) be a new instance. Our task is to select which of the labels in Cn+1 is its correct label cn+1 (xn+1 does not have to be a previously observed lemma, but its lemma must appear in a VN class). The structure of the task lets us apply a learning algorithm that is especially appropriate for it. What we need is an algorithm that allows us to restrict the possible labels of each instance, both in training and in testing. The sequential model algorithm presented by Even-Zohar and Roth (2001) directly supports this requirement. We use the SNOW learning architecture for multi-class classification (Roth, 1998), which contains an implementation of that algorithm 9 . 5 Experimental Setup We used SemLink VN annotations and parse trees on sections 02-21 of the WSJ Penn Treebank for training, and section 00 as a development set, as is common in the parsing community. We performed two parallel sets of experiments, one using manually created gold standard parse trees and one using parse trees created by a state-of-the-art 9 Experiments on development data revealed that for verbs for which"
C08-1002,J01-3003,0,0.115307,"Missing"
C08-1002,P05-3014,0,0.0532961,"Missing"
C08-1002,J05-1004,0,0.151397,"in classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically. VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004). VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type). Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class). VN’s significant coverage of the English verb lexicon is demonstrated by the 3 Propbank (Palmer et al., 2005) is a corpus annotation of the WSJ sections of the Penn Treebank with semantic roles of each verbal proposition. 4 Semlink was not available then. 1 Our annotations will be made available to the community. 2 Using the same sentences and instances, obtained from the authors. 10 number of classes relative to the number of types6 . A classifier may gather valuable information for all members of a certain VN class, without seeing all of its members in the training data. From this perspective the task resembles POS tagging. In both tasks there are many dozens (or more) of possible labels, while eac"
C08-1002,C00-2108,0,0.214619,"Missing"
C08-1002,kipper-etal-2006-extending,0,0.0365947,"ed work in Section 2, and discuss the task in Section 3. Section 4 introduces the model, Section 5 describes the experimental setup, and Section 6 presents our results. 2 Related Work VerbNet. VN is a major electronic English verb lexicon. It is organized in a hierarchical structure of classes and sub-classes, each sub-class inheriting the full characterization of its super-class. VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically. VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004). VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type). Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class). VN’s significant coverage of the English verb lexicon is demonstrated by the 3 Propbank (Palmer et al., 2005) is a corpus annotation of the WSJ sections of the Penn Treebank with semantic roles of each verbal proposition. 4 Semlink was not available then. 1 Our annotations will be made"
C08-1002,H05-1111,0,0.166298,"Missing"
C08-1002,P98-1112,0,0.0739319,"Missing"
C08-1002,N07-1069,0,0.045338,"Missing"
C08-1002,W04-2606,0,0.0702308,"ntroduces the model, Section 5 describes the experimental setup, and Section 6 presents our results. 2 Related Work VerbNet. VN is a major electronic English verb lexicon. It is organized in a hierarchical structure of classes and sub-classes, each sub-class inheriting the full characterization of its super-class. VN is built on a refinement of the Levin classes, the intersective Levin classes (Dang et al., 1998), aimed at achieving more coherent classes both semantically and syntactically. VN was also substantially extended (Kipper et al., 2006) using the Levin classes extension proposed in (Korhonen and Briscoe, 2004). VN today contains 3626 verb lemmas (forms), organized in 237 main classes having 4991 verb types (we refer to a lemma with an ascribed class as a type). Of the 3626 lemmas, 912 are polysemous (i.e., appear in more than a single class). VN’s significant coverage of the English verb lexicon is demonstrated by the 3 Propbank (Palmer et al., 2005) is a corpus annotation of the WSJ sections of the Penn Treebank with semantic roles of each verbal proposition. 4 Semlink was not available then. 1 Our annotations will be made available to the community. 2 Using the same sentences and instances, obtai"
C08-1002,J03-4003,0,\N,Missing
C08-1002,C98-1046,0,\N,Missing
C08-1002,C98-1108,0,\N,Missing
C08-1002,C98-1013,0,\N,Missing
C08-1002,P05-1022,0,\N,Missing
C08-1002,J04-1003,0,\N,Missing
C08-1002,C96-2102,0,\N,Missing
C12-1147,P11-2086,0,0.0132508,"s no correlation, and −1 indicates anticorrelation. We also compute a significance p-value, which is the probability for obtaining a given correlation at random (Abdi 2007). Results show that the relative orderings obtained in the different settings are very much in concordance. The obtained Kendall τ correlation coefficients range between (0.46, 0.88). Interestingly, when excluding DMV, results are even more significant (correlation in (0.64, 0.88)). This corresponds to p-values smaller than 10−7 and smaller than 10−13 if we exclude DMV. 9 This is a commonly used measure in NLP (Lapata 2006; Brody and Kantor 2011). 2414 Relation between Learnability Measures. In order to explore the relations between the two learnability measures, we focused on pairs of orderings that use the same parser, but different learnability measures (|P |= 5 pairs). The Kendall τ values in this case range between (0.75, 0.82), which corresponds to p-values < 10−18 . Despite the high correlation between the measures, the biases discovered under the AccuracyLearnability measure are stronger than the ones discovered under the Rate-Learnability measure. This demonstrates the somewhat different perspectives obtained by using differe"
C12-1147,N09-1009,0,0.0254999,"ps Figure 3: The VSS’s with which we experiment. The possible annotations for each structure are marked using solid and dashed lines. alternatives2 . 4 Experimental Setup 4.1 The Parsers In this work we experiment with five parsers of different types. We briefly describe them. Dependency Model with Valence (DMV) (Klein and Manning 2004) is a generative parser that defines a probabilistic grammar for unlabeled dependency structures. This parser is widely used in the field of unsupervised dependency parsing, where the great majority of recent works are in fact elaborations of this model (e.g., (Cohen and Smith 2009; Headden III et al. 2009)). In our experiments we use a supervised version of this parser, by training it using maximum likelihood estimation (MLE). This approach was used in various previous works as an upper bound for the unsupervised model (Blunsom and Cohn 2010; Spitkovsky et al. 2011). Decoding is performed using the Viterbi algorithm3. MST Parser (McDonald et al. 2005)4 formulates dependency parsing as a search for a maximum spanning tree (MST). It uses online training and extends the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer 2003) to learning with structured outputs."
C12-1147,de-marneffe-etal-2006-generating,0,0.0102724,"Missing"
C12-1147,N10-1115,0,0.0778492,"ing with structured outputs. Clear Parser (Choi and Nicolov 2009)5 is a fast transition-based parser that uses the robust risk minimization technique (Zhang et al. 2002). k-best ranking is used to prune the next state in decoding. Su Parser (Nivre 2009)6 is a transition-based parser and an extension of the MALT parser (Nivre et al. 2006). The parser starts by constructing arcs between adjacent words and then swaps the order of input words in order to learn more complex structures. It uses the stackeager algorithm, and is trained using various linear classifiers (including SVM). NonDir Parser (Goldberg and Elhadad 2010)7 is a non-directional, easy-first parser, which is greedy and deterministic. It first attempts to induce a non-directional version of the easiest arcs in 2 Some definitions of verb groups also include auxiliaries. We choose to exclude them from our definition since we use the PTB POS set, which distinguishes modals, but not auxiliaries, from other verbs. 3 http://www.cs.columbia.edu/~scohen/parser.html http://www.seas.upenn.edu/~strctlrn/MSTParser/MSTParser.html http://code.google.com/p/clearparser/ 6 http://maltparser.org/ 7 http://www.cs.bgu.ac.il/~yoavg/software/easyfirst/ 4 5 2411 a depen"
C12-1147,P11-1067,1,0.473969,"al. 2012) and is described in detail in Section 3. While these examples are all taken from English, variation is found in any language for which sufficient resources are available (Zeman et al. 2012). Many previous works addressed the difficulties imposed by the lack of established standards for syntactic representation. Jiang and Liu (2009) adapted statistical tools trained with one annotation standard to another. Other works proposed to normalize the different representations into a standard scheme (Ide and Bunt 2010; Zeman et al. 2012). Parsing evaluation is also highly affected by VSS’s. Schwartz et al. (2011) suggested Neutral Edge Direction (NED), an evaluation measure for unsupervised dependency parsing that accepts more than one plausible annotation for dependency VSS’s. Tsarfaty et al. (2011) suggested a new evaluation measure for supervised dependency parsing to address representational variation. The measure is based on tree edit distance. Tsarfaty et al. (2012) extended this measure for comparing between annotations from different formalisms. The emphasis of all the above works was mainly to overcome the problems incurred by the lack of standard, and not to select the most advantageous anno"
C12-1147,W03-3023,0,0.0564775,", pages 2405–2422, COLING 2012, Mumbai, December 2012. 2405 1 Introduction The formal manner in which syntactic relations are represented is at the core of the study of grammar. Numerous representations have been proposed over the years for expressing similar syntactic relations. This diversity of representations is expressed in a variety of syntactic annotation schemes currently in use in NLP. Examples include, for constituency annotation, schemes by (Marcus et al. 1993; Sampson 1995; Nelson et al. 2002, inter alia) and for dependency annotation, schemes by (Collins 1999; Rambow et al. 2002; Yamada and Matsumoto 2003; Johansson and Nugues 2007, inter alia). Variation within the same formalism is expressed in structures that have several alternative annotations (henceforth Varying Syntactic Structure or VSS). In this work we focus on dependency structures, where some of the most basic structures are VSS’s. One example is prepositional phrases, which consist of a preposition followed by a noun phrase (e.g., “about everyone”). While some schemes select the preposition to head the NP (Collins 1999), others select the NP as the head of the preposition (Johansson and Nugues 2007) (see Figure 1). Other prominent"
C12-1147,zeman-etal-2012-hamledt,0,0.00944774,"Missing"
C12-1147,W12-3602,0,\N,Missing
C12-1147,nivre-etal-2006-maltparser,0,\N,Missing
C12-1147,E12-1006,0,\N,Missing
C12-1147,W10-1840,0,\N,Missing
C12-1147,J93-2004,0,\N,Missing
C12-1147,W11-0303,0,\N,Missing
C12-1147,W09-3803,0,\N,Missing
C12-1147,W04-2609,0,\N,Missing
C12-1147,N01-1021,0,\N,Missing
C12-1147,N09-1012,0,\N,Missing
C12-1147,J03-4003,0,\N,Missing
C12-1147,D10-1117,0,\N,Missing
C12-1147,P06-1033,0,\N,Missing
C12-1147,H05-1066,0,\N,Missing
C12-1147,J06-4002,0,\N,Missing
C12-1147,P09-1040,0,\N,Missing
C12-1147,P06-3004,0,\N,Missing
C12-1147,D11-1036,0,\N,Missing
C12-1147,D07-1112,0,\N,Missing
C12-1147,W07-2416,0,\N,Missing
C12-1147,P04-1061,0,\N,Missing
C12-1147,W04-1501,0,\N,Missing
C12-1147,rambow-etal-2002-dependency,0,\N,Missing
D16-1134,W13-2322,0,0.335895,"Missing"
D16-1134,W05-0909,0,0.252262,"Missing"
D16-1134,W13-2203,1,0.896683,"Missing"
D16-1134,W12-4204,1,0.908186,"Missing"
D16-1134,W11-2101,1,0.914645,"Missing"
D16-1134,W14-4005,0,0.0250442,"Missing"
D16-1134,P16-2013,0,0.0371161,"Missing"
D16-1134,W07-0738,0,0.0835328,"Missing"
D16-1134,N15-1124,0,0.125815,"Missing"
D16-1134,P15-1174,0,0.0311734,"Missing"
D16-1134,P07-2045,1,0.0114599,"Missing"
D16-1134,W05-0904,0,0.093592,"Missing"
D16-1134,W11-1002,0,0.0373567,"Missing"
D16-1134,lo-wu-2014-reliability,0,0.0405899,"Missing"
D16-1134,oepen-lonning-2006-discriminant,0,0.0919776,"Missing"
D16-1134,2006.amta-papers.25,0,0.300368,"Missing"
D16-1134,W15-3502,1,0.478779,"Missing"
D16-1134,P02-1040,0,\N,Missing
D18-1081,P18-2114,0,0.100626,"Missing"
D18-1081,E06-1032,0,0.141524,"s. In Set 1, annotators are required to split the original as much as possible, while preserving the sentence’s gramn=1 where BP is the brevity penalty term, pn are the modified precisions, and wn are the corresponding weights, which are usually uniform in practice. The experiments of Papineni et al. (2002) showed that BLEU correlates with human judgments in the ranking of five English-to-Chinese MT systems and that it can distinguish human and machine translations. Although BLEU is widely used in MT, several works have pointed out its shortcomings (e.g., Koehn and Monz, 2006). In particular, Callison-Burch et al. (2006) showed that BLEU may not correlate in some cases with human judgments since a huge number of potential translations have the same BLEU score, and that correlation decreases when translation quality is low. Some of the reported shortcomings are relevant to monolingual translation, such as the impossibility to capture synonyms and paraphrases that are not in the reference set, or the uniform weighting of words. 3 https://github.com/cocoxu/ simplification includes the corpus, the SARI metric and the SBMT-SARI system. The corpus comprises 359 sentences. tural operations are involved (Nisioi et al"
D18-1081,W14-5603,0,0.122844,"Missing"
D18-1081,P14-1041,0,0.466995,"Missing"
D18-1081,D15-1013,0,0.0213823,"ple structural paraphrases, and performed a correlation analysis with human judgments.1 We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences. 1 Introduction BLEU (Papineni et al., 2002) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011), summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; ˇ Stajner et al., 2015; Xu et al., 2016), i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014), BLEU became the main automatic metric for TS, despite its deficiencies (see §2). Indeed, focusing on lexical simplification, Xu et al. (2016) argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their"
D18-1081,W16-6620,0,0.153104,"Missing"
D18-1081,D17-1064,0,0.056638,"erences is used to address cross-reference variation. To address changes in word order, BLEU uses n-gram precision, modified to eliminate repetitions across the references. A brevity term penalizes overly short sentences. Formally: BLEU = BP × exp( N X 3 wn log(pn )) Gold-Standard Splitting Corpus In order to investigate the effect of correctly splitting sentences on the automatic metric scores, we build a parallel corpus, where each sentence is modified by 4 annotators, according to specific sentence splitting guidelines. We use the complex side of the test corpus of Xu et al. (2016).3 While Narayan et al. (2017) recently proposed the semi-automatically compiled WEBSPLIT dataset for training automatic sentence splitting systems, here we generate a completely manual corpus, without a-priori splitting points nor do we pre-suppose that all sentences should be split. This corpus enriches the set of references focused on lexical operations that were collected by Xu et al. (2016) for the same source sentences and can also be used as an out-of-domain test set for Split-and-Rephrase (Narayan et al., 2017). We use two sets of guidelines. In Set 1, annotators are required to split the original as much as possib"
D18-1081,P07-2045,0,0.00848781,"Missing"
D18-1081,P17-2014,0,0.578835,"ves high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, 1 The corpus can be found in https://github.com/ eliorsulem/HSplit-corpus 2 Nevertheless, they are also used in contexts where struc738 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 738–744 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics BLEU in TS. While BLEU is standardly used for TS evaluation (e.g., Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Ma and Sun, 2017), only few works tested its correlation with human judgments. Using 20 source sentences from the PWKP test corpus (Zhu et al., 2010) with 5 simplified sentences for each of them, Wubben et al. (2012) reported positive correlation of BLEU with simplicity ratings, but no correlation with adˇ equacy. T-BLEU (Stajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source. It was found to have moderate positive correlation for meaning pr"
D18-1081,P02-1040,0,0.113538,"lexical and structural aspects. In this paper we show that BLEU is not suitable for the evaluation of sentence splitting, the major structural simplification operation. We manually compiled a sentence splitting gold standard corpus containing multiple structural paraphrases, and performed a correlation analysis with human judgments.1 We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences. 1 Introduction BLEU (Papineni et al., 2002) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011), summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; ˇ Stajner et al., 2015; Xu et al., 2016), i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014), BLEU became the main automatic metric for TS, despit"
D18-1081,W06-3114,0,0.0406323,"., 2017). We use two sets of guidelines. In Set 1, annotators are required to split the original as much as possible, while preserving the sentence’s gramn=1 where BP is the brevity penalty term, pn are the modified precisions, and wn are the corresponding weights, which are usually uniform in practice. The experiments of Papineni et al. (2002) showed that BLEU correlates with human judgments in the ranking of five English-to-Chinese MT systems and that it can distinguish human and machine translations. Although BLEU is widely used in MT, several works have pointed out its shortcomings (e.g., Koehn and Monz, 2006). In particular, Callison-Burch et al. (2006) showed that BLEU may not correlate in some cases with human judgments since a huge number of potential translations have the same BLEU score, and that correlation decreases when translation quality is low. Some of the reported shortcomings are relevant to monolingual translation, such as the impossibility to capture synonyms and paraphrases that are not in the reference set, or the uniform weighting of words. 3 https://github.com/cocoxu/ simplification includes the corpus, the SARI metric and the SBMT-SARI system. The corpus comprises 359 sentences"
D18-1081,P11-1094,0,0.0127649,"gold standard corpus containing multiple structural paraphrases, and performed a correlation analysis with human judgments.1 We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences. 1 Introduction BLEU (Papineni et al., 2002) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011), summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; ˇ Stajner et al., 2015; Xu et al., 2016), i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014), BLEU became the main automatic metric for TS, despite its deficiencies (see §2). Indeed, focusing on lexical simplification, Xu et al. (2016) argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple"
D18-1081,D15-1148,0,0.0283395,"Missing"
D18-1081,Q16-1005,0,0.066965,"Missing"
D18-1081,W02-0109,0,0.0130657,"ion. We use the evaluation benchmark provided by Sulem et al. (2018b),11 including system outputs and human evaluation scores corresponding to the first 70 sentences of 4 Examples are taken from Siddharthan (2006). Examples are not provided in the case of Set 2 so as not to give an a-priori notion of simplicity. The complete guidelines are found in the supplementary material. 6 Wilicoxon’s signed rank test, p = 1.6 · 10−5 for #Sents and p = 0.002 for SplitSents. 7 System-level BLEU scores are computed using the multi-bleu Moses support tool. Sentence-level BLEU scores are computed using NLTK (Loper and Bird, 2002). 5 8 We thus computed the correlation in §4.2 for -FK. LDSC is computed using NLTK. 10 Taking the fourth hypothesis rather than the first has been found to yield considerably less conservative TS systems. 11 https://github.com/eliorsulem/ simplification-acl2018 9 740 BLEU-1ref BLEU-8ref iBLEU-1ref iBLEU-8ref -FK SARI-8ref G 0.43 (0.2) 0.61 (0.07) 0.21 (0.3) 0.61 (0.07) -0.21 (0.3) -0.64 (0.06) -LDSC 0.29 (0.3) Systems/Corpora without Splits M S 1.00 (0) -0.81 (0.01) 0.89 (0.003) -0.59 (0.08) 0.93 (0.001) -0.85 (0.008) 0.89 (0.003) -0.59 (0.08) -0.57 (0.09) 0.67 (0.05) -0.86 (0.007) 0.52 (0.1)"
D18-1081,P15-2135,0,0.122451,"h human judgments.1 We find low or no correlation between BLEU and the grammaticality and meaning preservation parameters where sentence splitting is involved. Moreover, BLEU often negatively correlates with simplicity, essentially penalizing simpler sentences. 1 Introduction BLEU (Papineni et al., 2002) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011), summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; ˇ Stajner et al., 2015; Xu et al., 2016), i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014), BLEU became the main automatic metric for TS, despite its deficiencies (see §2). Indeed, focusing on lexical simplification, Xu et al. (2016) argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, 1 The corpus can be found i"
D18-1081,C10-1152,0,0.541581,"ively correlates with simplicity, essentially penalizing simpler sentences. 1 Introduction BLEU (Papineni et al., 2002) is an n-grambased evaluation metric, widely used for Machine Translation (MT) evaluation. BLEU has also been applied to monolingual translation tasks, such as grammatical error correction (Park and Levy, 2011), summarization (Graham, 2015) and text simplification (Narayan and Gardent, 2014; ˇ Stajner et al., 2015; Xu et al., 2016), i.e. the rewriting of a sentence as one or more simpler sentences. Along with the application of parallel corpora and MT techniques for TS (e.g., Zhu et al., 2010; Wubben et al., 2012; Narayan and Gardent, 2014), BLEU became the main automatic metric for TS, despite its deficiencies (see §2). Indeed, focusing on lexical simplification, Xu et al. (2016) argued that BLEU gives high scores to sentences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, 1 The corpus can be found in https://github.com/ eliorsulem/HSplit-corpus 2 Nevertheless, they are also used in contexts where struc738 Proceedings of the 2018 Conference on Empirical Methods in Natural Langua"
D18-1081,W14-1201,0,0.257452,"s in Natural Language Processing, pages 738–744 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics BLEU in TS. While BLEU is standardly used for TS evaluation (e.g., Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Ma and Sun, 2017), only few works tested its correlation with human judgments. Using 20 source sentences from the PWKP test corpus (Zhu et al., 2010) with 5 simplified sentences for each of them, Wubben et al. (2012) reported positive correlation of BLEU with simplicity ratings, but no correlation with adˇ equacy. T-BLEU (Stajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source. It was found to have moderate positive correlation for meaning preservation, and positive but low correlation for grammaticality. Correlation with simplicity was not considered in this experiment. Xu et al. (2016) focused on lexical simplification, finding that BLEU obtains reasonable correlation for grammaticality and meaning preservation but fails to capture simplicity, even when multiple references are used. To our knowledge, no"
D18-1081,N18-1063,1,0.869674,"hat BLEU may not correlate in some cases with human judgments since a huge number of potential translations have the same BLEU score, and that correlation decreases when translation quality is low. Some of the reported shortcomings are relevant to monolingual translation, such as the impossibility to capture synonyms and paraphrases that are not in the reference set, or the uniform weighting of words. 3 https://github.com/cocoxu/ simplification includes the corpus, the SARI metric and the SBMT-SARI system. The corpus comprises 359 sentences. tural operations are involved (Nisioi et al., 2017; Sulem et al., 2018b). 739 readability;8 (3) SARI (Xu et al., 2016), which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. For completeness, we also experiment with the negative Levenshtein distance to the source (-LDSC ), which serves as a measure of conservatism.9 We explore two settings. In one (“Standard Reference Setting”, §4.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by X"
D18-1081,P18-1016,1,0.918364,"hat BLEU may not correlate in some cases with human judgments since a huge number of potential translations have the same BLEU score, and that correlation decreases when translation quality is low. Some of the reported shortcomings are relevant to monolingual translation, such as the impossibility to capture synonyms and paraphrases that are not in the reference set, or the uniform weighting of words. 3 https://github.com/cocoxu/ simplification includes the corpus, the SARI metric and the SBMT-SARI system. The corpus comprises 359 sentences. tural operations are involved (Nisioi et al., 2017; Sulem et al., 2018b). 739 readability;8 (3) SARI (Xu et al., 2016), which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. For completeness, we also experiment with the negative Levenshtein distance to the source (-LDSC ), which serves as a measure of conservatism.9 We explore two settings. In one (“Standard Reference Setting”, §4.2), we use two sets of references: the Simple Wikipedia reference (yielding BLEU-1ref and iBLEU-1ref), and 8 references obtained by crowdsourcing by X"
D18-1081,P12-2008,0,0.102861,"Missing"
D18-1081,W03-2317,0,0.253965,"Missing"
D18-1081,D11-1038,0,0.501083,"Missing"
D18-1081,P12-1107,0,0.703788,"Missing"
D18-1081,Q16-1029,0,0.276632,"Zhu et al. (2010), respectively (Narayan and Gardent, 2016). Sentence splitting is also the focus of the recently proposed Split-and Rephrase sub-task (Narayan et al., 2017; Aharoni and Goldberg, 2018), in which the automatic metric used is BLEU. For exploring the effect of sentence splitting on BLEU scores, we compile a human-generated gold standard sentence splitting corpus – HSplit, which will also be useful for future studies of splitting in TS, and perform correlation analyses with human judgments. We consider two reference sets. First, we experiment with the most common set, proposed by Xu et al. (2016), evaluating a variety of system outputs, as well as HSplit. The references in this setting explicitly emphasize lexical operations, and do not contain splitting or content deletion.2 Second, we experiment with HSplit as BLEU is widely considered to be an informative metric for text-to-text generation, including Text Simplification (TS). TS includes both lexical and structural aspects. In this paper we show that BLEU is not suitable for the evaluation of sentence splitting, the major structural simplification operation. We manually compiled a sentence splitting gold standard corpus containing"
D18-1081,D17-1062,0,0.230702,"ntences that are close or even identical to the input, especially when multiple references are used. In their experiments, BLEU failed to predict simplicity, 1 The corpus can be found in https://github.com/ eliorsulem/HSplit-corpus 2 Nevertheless, they are also used in contexts where struc738 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 738–744 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics BLEU in TS. While BLEU is standardly used for TS evaluation (e.g., Xu et al., 2016; Nisioi et al., 2017; Zhang and Lapata, 2017; Ma and Sun, 2017), only few works tested its correlation with human judgments. Using 20 source sentences from the PWKP test corpus (Zhu et al., 2010) with 5 simplified sentences for each of them, Wubben et al. (2012) reported positive correlation of BLEU with simplicity ratings, but no correlation with adˇ equacy. T-BLEU (Stajner et al., 2014), a variant of BLEU which uses lower n-grams when no overlapping 4-grams are found, was tested on outputs that applied only structural modifications to the source. It was found to have moderate positive correlation for meaning preservation, and positive"
K18-2010,P13-1023,1,0.849468,"corpora (realized as reentrancy by remote edges; see §2), and TUPA supports them as a standard feature. Introduction In this paper, we present the HUJI submission to the CoNLL 2018 shared task on Universal Dependency parsing (Zeman et al., 2018). We focus only on parsing, using the baseline system, UDPipe 1.2 (Straka et al., 2016; Straka and Straková, 2017) for tokenization, sentence splitting, part-of-speech tagging and morphological tagging. Our system is based on TUPA (Hershcovich et al., 2017, 2018, see §3), a transition-based UCCA parser. UCCA (Universal Conceptual Cognitive Annotation; Abend and Rappoport, 2013) is a cross-linguistic semantic annotation scheme, representing events, participants, attributes and relations in a directed acyclic graph (DAG) structure. UCCA allows reentrancy to support argument sharing, discontinuity (corresponding to non-projectivity in dependency formalisms) and non-terminal nodes (as opposed to dependencies, As their annotation in UD is not yet widespread and standardized, enhanced dependencies are not included in the evaluation metrics for UD parsing, and so TUPA’s ability to parse them is not reflected in the official shared task scores. However, we believe these enh"
K18-2010,Q17-1010,0,0.0156329,"6= root,  y 6;G x i(x) < i(y) Figure 6: The transition set of TUPA. We write the stack with its top to the right and the buffer with its head to the left. (·, ·)X denotes a primary X-labeled edge, and (·, ·)∗X a remote X-labeled edge. i(x) is the swap index (see §3.3). In addition to the specified conditions, the prospective child in an E DGE transition must not already have a primary parent. Parser state S made B G to feel very wel... nsubj We initialized randomly, except for the word embeddings, which are initialized with the 250K most frequent word vectors from fastText for each language (Bojanowski et al., 2017),5 pre-trained over Wikipedia and updated during training. We do not use word embeddings for languages without pre-trained fastText vectors (Ancient Greek, North Sami and Old French). To the feature embeddings, we concatenate numeric features representing the node height, number of (remote) parents and children, and the ratio between the number of terminals to total number of nodes in the graph G. Table 1 lists all feature used for the classifier. Numeric features are taken as they are, whereas categorical features are mapped to real-valued embedding vectors. For each non-terminal node, we sel"
K18-2010,P17-1104,1,0.852228,"ly a few UD treebanks contain any enhanced dependencies, similar structures are an integral part of UCCA and its annotated corpora (realized as reentrancy by remote edges; see §2), and TUPA supports them as a standard feature. Introduction In this paper, we present the HUJI submission to the CoNLL 2018 shared task on Universal Dependency parsing (Zeman et al., 2018). We focus only on parsing, using the baseline system, UDPipe 1.2 (Straka et al., 2016; Straka and Straková, 2017) for tokenization, sentence splitting, part-of-speech tagging and morphological tagging. Our system is based on TUPA (Hershcovich et al., 2017, 2018, see §3), a transition-based UCCA parser. UCCA (Universal Conceptual Cognitive Annotation; Abend and Rappoport, 2013) is a cross-linguistic semantic annotation scheme, representing events, participants, attributes and relations in a directed acyclic graph (DAG) structure. UCCA allows reentrancy to support argument sharing, discontinuity (corresponding to non-projectivity in dependency formalisms) and non-terminal nodes (as opposed to dependencies, As their annotation in UD is not yet widespread and standardized, enhanced dependencies are not included in the evaluation metrics for UD par"
K18-2010,D17-1009,0,0.0246567,"d nonterminal nodes. By converting UD trees and graphs to a UCCA-like DAG format, we train TUPA almost without modification on the UD parsing task. The generic nature of our approach lends itself naturally to multitask learning. Our code is available at https://github. com/CoNLL-UD-2018/HUJI. 1 Enhanced dependencies. Our method treats enhanced dependencies1 as part of the dependency graph, providing the first approach, to our knowledge, for supervised learning of enhanced UD parsing. Due to the scarcity of enhanced dependencies in UD treebanks, previous approaches (Schuster and Manning, 2016; Reddy et al., 2017) have attempted to recover them using languagespecific rules. Our approach attempts to learn them from data: while only a few UD treebanks contain any enhanced dependencies, similar structures are an integral part of UCCA and its annotated corpora (realized as reentrancy by remote edges; see §2), and TUPA supports them as a standard feature. Introduction In this paper, we present the HUJI submission to the CoNLL 2018 shared task on Universal Dependency parsing (Zeman et al., 2018). We focus only on parsing, using the baseline system, UDPipe 1.2 (Straka et al., 2016; Straka and Straková, 2017)"
K18-2010,P18-1035,1,0.881214,". Figure 2 demonstrates this format. Note that if the basic dependency is repeated in the enhanced graph (3:nsubj:pass in the example), we do not treat it as an enhanced dependency, so that the converted graph will only contain each edge once. In addition to the UD relations defined in the basic representations, enhanced dependencies may contain the relation ref, used for relative clauses. In addition, they may contain more specific relation subtypes, and optionally also case information. Unified DAG Format To apply TUPA to UD parsing, we convert UD trees and graphs into a unified DAG format (Hershcovich et al., 2018). The format consists of a rooted DAG, where the tokens are the terminal nodes.2 Edges are labeled (but not nodes), and are divided into primary and remote edges, where the primary edges form a tree (all nodes have at most one primary parent, and the root has none). Remote edges (denoted as dashed edges in Figure 1) Language-specific extensions and case information. Dependencies may contain language2 Our conversion code supports full conversion between UCCA and UD, among other representation schemes, and is publicly available at http://github.com/danielhers/semstr/ tree/master/semstr/conversio"
K18-2010,L16-1376,0,0.0585668,"reentrancy, discontinuity and nonterminal nodes. By converting UD trees and graphs to a UCCA-like DAG format, we train TUPA almost without modification on the UD parsing task. The generic nature of our approach lends itself naturally to multitask learning. Our code is available at https://github. com/CoNLL-UD-2018/HUJI. 1 Enhanced dependencies. Our method treats enhanced dependencies1 as part of the dependency graph, providing the first approach, to our knowledge, for supervised learning of enhanced UD parsing. Due to the scarcity of enhanced dependencies in UD treebanks, previous approaches (Schuster and Manning, 2016; Reddy et al., 2017) have attempted to recover them using languagespecific rules. Our approach attempts to learn them from data: while only a few UD treebanks contain any enhanced dependencies, similar structures are an integral part of UCCA and its annotated corpora (realized as reentrancy by remote edges; see §2), and TUPA supports them as a standard feature. Introduction In this paper, we present the HUJI submission to the CoNLL 2018 shared task on Universal Dependency parsing (Zeman et al., 2018). We focus only on parsing, using the baseline system, UDPipe 1.2 (Straka et al., 2016; Straka"
K18-2010,silveira-etal-2014-gold,0,0.0608387,"Missing"
K18-2010,Q16-1023,0,0.0551994,"ich et al., 2017). Table 2: Hyperparameter settings. 4.1 Training details The model is implemented using DyNet v2.0.3 (Neubig et al., 2017).6 Unless otherwise noted, we use the default values provided by the package. We use the same hyperparameters as used in previous experiments on UCCA parsing (Hershcovich et al., 2018), without any hyperparameter 6 Hyperparameters We use dropout (Srivastava et al., 2014) between MLP layers, and recurrent dropout (Gal and Ghahramani, 2016) between BiLSTM layers, both with p = 0.4. We also use word, lemma, coarseand fine-grained POS tag dropout with α = 0.2 (Kiperwasser and Goldberg, 2016): in training, the embedding for a feature value w is replaced α with a zero vector with a probability of #(w)+α , where #(w) is the number of occurrences of w observed. In addition, we use node dropout (Hershcovich et al., 2018): with a probability of 0.1 at each step, all features associated with a single node in the parser state are replaced with zero vectors. For optimization we use a minibatch size of 100, decaying all weights by 10−5 at each update, and train with stochastic gradient descent for 50 epochs with a learning rate of 0.1, followed by AMSGrad (Sashank J. Reddi, 2018) for 250 e"
K18-2010,W16-0906,0,0.335427,"only the root node and terminals exist. We assign the root a swap index of 0, and for each terminal, its position in the text (starting at 1). Whenever a node is created as a result Features. We use vector embeddings representing the words, lemmas, coarse (universal) POS tags and fine-grained POS tags, provided by UDPipe 1.2 during test. For training, we use the gold-annotated lemmas and POS tags. In addition, we use one-character prefix, three-character suffix, shape (capturing orthographic features, e.g., “Xxxx”) and named entity type, provided by spaCy;4 punctuation and gap type features (Maier and Lichte, 2016), and previously predicted edge labels and parser actions. These embeddings are 4 Constraints 5 http://spacy.io 107 http://fasttext.cc Nodes s0 s1 s2 s3 b0 b1 , b 2 , b 3 s0 l, s0 r, s1 l, s1 r, s0 ll, s0 lr, s0 rl, s0 rr, s1 ll, s1 lr, s1 rl, s1 rr s0 L, s0 R, s1 L, s1 R, b0 L, b0 R Edges s0 → s1 , s0 → b0 , s1 → s0 , b0 → s0 s0 → b0 , b0 → s0 Past actions a0 , a1 Global tuning on UD treebanks. wmtuepT#ˆ$xhqyPCIEMN wmtueT#ˆ$xhyN wmtueT#ˆ$xhy wmtueT#ˆ$xhyN wmtuT#ˆ$hPCIEMN wmtuT#ˆ$ wme#ˆ$ Hyperparameter Pre-trained word dim. Lemma dim. Coarse (universal) POS tag dim. Fine-grained POS tag dim. N"
K18-2010,L16-1680,0,0.0933326,"Missing"
K18-2010,K17-3009,0,0.0399393,", 2016; Reddy et al., 2017) have attempted to recover them using languagespecific rules. Our approach attempts to learn them from data: while only a few UD treebanks contain any enhanced dependencies, similar structures are an integral part of UCCA and its annotated corpora (realized as reentrancy by remote edges; see §2), and TUPA supports them as a standard feature. Introduction In this paper, we present the HUJI submission to the CoNLL 2018 shared task on Universal Dependency parsing (Zeman et al., 2018). We focus only on parsing, using the baseline system, UDPipe 1.2 (Straka et al., 2016; Straka and Straková, 2017) for tokenization, sentence splitting, part-of-speech tagging and morphological tagging. Our system is based on TUPA (Hershcovich et al., 2017, 2018, see §3), a transition-based UCCA parser. UCCA (Universal Conceptual Cognitive Annotation; Abend and Rappoport, 2013) is a cross-linguistic semantic annotation scheme, representing events, participants, attributes and relations in a directed acyclic graph (DAG) structure. UCCA allows reentrancy to support argument sharing, discontinuity (corresponding to non-projectivity in dependency formalisms) and non-terminal nodes (as opposed to dependencies,"
K18-2010,W03-3017,0,0.212918,"s are attached to its argument “peace” in the basic representation, and the argument itself is attached as an orphan. In the enhanced representation, all arguments are attached to the null node as if the elided predicate was present. While UCCA supports empty nodes without surface realization in the form of implicit units, previous work on UCCA parsing has removed these from the graphs. We do the same for UD parsing, dropping null nodes and their associated 3 General Transition-based DAG Parser We now turn to describing TUPA (Hershcovich et al., 2017, 2018), a general transition-based parser (Nivre, 2003). TUPA uses an extended set of transitions and features that supports reentrancies, discontinuities and non-terminal nodes. The parser state is composed of a buffer B of tokens 105 punct conj root punct obj cc nmod orphan nsubj iobj amod case punct I wish all happy holidays , and moreso E9.1 , punct peace on earth . obj advmod cc punct Figure 4: newsgroup-groups.google.com_GuildWars_086f0f64ab633ab3_ENG_20041111_1735000051 (UD_English-EWT), containing a null node (E9.1) and case information (nmod:on). root punct acl d hea acl:relcl d hea aux was made back t nc pu head ca se det robe that od ob"
K18-2010,W17-0411,0,0.0395841,"lemma, fine-grained tag, prefix and suffix features. We train this model for two epochs using stochastic gradient descent with a learning rate of 0.1 (we only trained this many epochs due to time constraints). 4.4 TUPA Results Official evaluation was done on the TIRA online platform (Potthast et al., 2014). Our system (named “HUJI”) ranked 24th in the LAS-F1 ranking (with an average of 53.69 over all test treebanks), 23rd by MLAS (average of 44.6) and 21st by BLEX (average of 48.05). Since our system only performs dependency parsing and not other pipeline tasks, we henceforth focus on LAS-F1 (Nivre and Fang, 2017) for evaluation. After the official evaluation period ended, we discovered several bugs in the conversion between the CoNLL-U format and the unified DAG format, which is used by TUPA for training and is output by it (see §2). We did not re-train TUPA on the training treebanks after fixing these bugs, but we did re-evaluate the already trained models on all test treebanks, and used the fixed code for converting their output to CoNLL-U. This yielded an unofficial average test LAS-F1 of 58.48, an improvement of 4.79 points over the official average score. In particular, for two test sets, ar_padt"
K18-2010,K18-2001,0,0.0386566,"Missing"
K19-1017,P13-1023,1,0.875202,"á et al., 2003; Oepen et al., 2015; Banarescu et al., 2013). These annotation schemes can be distinguished by various design principles such as language-specificity; the level of granularity of meaning elements; the reliance on morphosyntactic criteria to define the units of semantic annotation; the extent to which human annotators specify semantics from scratch; and many others (Abend and Rappoport, 2017). In this work, we seize an opportunity to unite two previously unrelated—yet complementary— meaning representations in NLP. On the one hand, Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) provides a skeletal structure of semantic units and relations, with typologically-based criteria for marking predicate-argument structures, based on “Basic Linguistic Theory”, an established framework for typological description (Dixon, 2010/2012). On the other hand, a recent approach to annotation of English prepositions and possessives (SNACS; Schneider et al., 2018) provides an inventory of labels that characterize semantic relations. UCCA and SNACS follow similar design principles: they are both language-neutral, with general-purpose coarse-grained labels rather than lexically-specific se"
K19-1017,P17-1008,1,0.850498,"that hold across paraphrasing or translation: for example, semantic dependency relations capturing predicateargument structures or other types of semantic relations that can be annotated within sentences (e.g., Böhmová et al., 2003; Oepen et al., 2015; Banarescu et al., 2013). These annotation schemes can be distinguished by various design principles such as language-specificity; the level of granularity of meaning elements; the reliance on morphosyntactic criteria to define the units of semantic annotation; the extent to which human annotators specify semantics from scratch; and many others (Abend and Rappoport, 2017). In this work, we seize an opportunity to unite two previously unrelated—yet complementary— meaning representations in NLP. On the one hand, Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) provides a skeletal structure of semantic units and relations, with typologically-based criteria for marking predicate-argument structures, based on “Basic Linguistic Theory”, an established framework for typological description (Dixon, 2010/2012). On the other hand, a recent approach to annotation of English prepositions and possessives (SNACS; Schneider et al., 2018) provides a"
K19-1017,W13-2322,1,0.949289,"structure from the prepositions on which they were originally annotated. The following UCCA categories are abbreviated: A = Participant, R = Relator, H = Parallel scene, Q = Quantifier, Fxn = Function. Introduction A common thread in many approaches to meaning representation is the idea that abstract structures can describe semantic invariants that hold across paraphrasing or translation: for example, semantic dependency relations capturing predicateargument structures or other types of semantic relations that can be annotated within sentences (e.g., Böhmová et al., 2003; Oepen et al., 2015; Banarescu et al., 2013). These annotation schemes can be distinguished by various design principles such as language-specificity; the level of granularity of meaning elements; the reliance on morphosyntactic criteria to define the units of semantic annotation; the extent to which human annotators specify semantics from scratch; and many others (Abend and Rappoport, 2017). In this work, we seize an opportunity to unite two previously unrelated—yet complementary— meaning representations in NLP. On the one hand, Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) provides a skeletal structure of"
K19-1017,D16-1134,1,0.803322,"Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018b), 2 Note that mapping between syntactic and semantic relations varies by construction: in She spoke on security, the semantic head of the relation between spoke and security corresponds to the syntactic head (the verb), whereas in Her speech was on security, UD treats security as the syntactic head (§3). 3 Prange et al. (2019) have proposed and piloted a coreference layer that sits above the foundational layer. as well as to the evaluation of a number of textto-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). Formally, UCCA structures are directed acyclic graphs over units (nodes covering a subset of tokens). Atomic units are the leaves of the graph: individual tokens or unanalyzable MWEs. Nonterminal units represent larger semantic constituents, such as scenes and compositional participants/ modifiers. The example in figure 1 has 5 nonterminal units. Each unit (save for the root) has a single incoming primary edge, and may also have incoming reentrant remote edges to express shared argumenthood. The primary edges of a UCCA structure thus form a tree"
K19-1017,N18-2020,1,0.865565,"cessfully used for improving text simplification (Sulem et al., 2018b), 2 Note that mapping between syntactic and semantic relations varies by construction: in She spoke on security, the semantic head of the relation between spoke and security corresponds to the syntactic head (the verb), whereas in Her speech was on security, UD treats security as the syntactic head (§3). 3 Prange et al. (2019) have proposed and piloted a coreference layer that sits above the foundational layer. as well as to the evaluation of a number of textto-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). Formally, UCCA structures are directed acyclic graphs over units (nodes covering a subset of tokens). Atomic units are the leaves of the graph: individual tokens or unanalyzable MWEs. Nonterminal units represent larger semantic constituents, such as scenes and compositional participants/ modifiers. The example in figure 1 has 5 nonterminal units. Each unit (save for the root) has a single incoming primary edge, and may also have incoming reentrant remote edges to express shared argumenthood. The primary edges of a UCCA structure thus form a tree, which along with the remote edges, forms a DA"
K19-1017,D09-1047,0,0.03106,"/possessive relations that we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the tw"
K19-1017,N19-1423,0,0.00703687,"has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work. 7 Conclusion We have introduced a new representation combining UCCA semantic structures and SNACS adpositional semantic roles; automatically merged existing annotations to create a gold standard; and experimented with several alternatives for parsing the integrated representation. Our results show that models profit from having access to both structural and lexical semantic information, confirming our hypothesis that UCCA and SNACS are complementary and compatible. Based on preliminary re"
K19-1017,C16-1256,0,0.0136147,"instead defines roles as a bundle of lexicon-free features, elicited by crowdsourcing. The specific inventory for preposition/possessive relations that we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings ("
K19-1017,N18-1082,0,0.0187017,"here is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work. 7 Conclusion We hav"
K19-1017,P17-1104,1,0.949322,"other. That is, we expect that knowing what semantic role is signaled by an adposition informs the underlying semantic structure of the sentence, and vice versa. In order to test this hypothesis, we use our annotated corpus to parse into the integrated representation. We consider several different ways of orchestrating the prediction of the foundational UCCA structure and the prediction of SNACS roles: modeling SNACS and UCCA in (i) a pipeline, (ii) a multitask setup with shared parameters, (iii) a single joint model. 4.1 Baseline: TUPA We choose the neural transition-based graph parser TUPA (Hershcovich et al., 2017, 2018) as a strong baseline for UCCA parsing. It was the official baseline in the recent SemEval shared task on UCCA parsing (Hershcovich et al., 2019b). TUPA’s transition system is defined to address 178 Avg Test F-Score 70 65 60 55 50 45 40 35 30 25 20 baseline dep MTL/ter Refined:exact pipeline joint/rel indep MTL joint/ter Refined:SNACS dep MTL/rel Refined:UCCA Refined:unlabeled Full Figure 2: Average F1-score on the test set over 5 random restarts with error bars indicating standard deviation. ter stands for terminal-level and rel for relation-level SNACS refinement (prediction or featur"
K19-1017,P18-1035,1,0.685576,"ation. ter stands for terminal-level and rel for relation-level SNACS refinement (prediction or features). the different formal structural phenomena exhibited by UCCA structures, notably reentrancies and discontiguous units. There are transitions for creating nonterminal nodes, and for attaching terminal and nonterminal nodes to a primary or remote (reentrant) parent with an UCCA category label on the edge. The transition system is general enough to be able to tackle parsing into a variety of formalisms, including SDP (Oepen et al., 2015) and a simplified form of AMR (Banarescu et al., 2013); Hershcovich et al. (2018) take advantage of this flexibility in their multitask learning framework. TUPA’s learning architecture largely follows that of Kiperwasser and Goldberg (2016). It encodes the parser’s configuration (buffer, stack and intermediate graph structure) using BiLSTMs, and predicts the next transition using an MLP, stacked on top of them. Token-based features, including POS tags, dependency parses, as well as NER and orthographic features, are embedded in the BiLSTM. Another set of features, taking into account the partially constructed graph and previously predicted transition types, is fed into the"
K19-1017,N19-1047,1,0.837606,"imensions of canonical adpositional phrase constructions. The adposition is bolded, the semantic head is italicized, and the semantic dependent is [ bracketed ]. Rows indicate whether the semantic head is nominal or verbal, while columns differentiate between scene-evoking and nonscene-evoking heads. Scene-evokers are underlined. 3.1 Data We use the STREUSLE 4.0 corpus (Schneider and Smith, 2015; Schneider et al., 2018), which covers the reviews section from the English_EWT treebank of UD 2.3, and lexical semantic annotations for the same text.6 The same corpus has been annotated with UCCA by Hershcovich et al. (2019a). We use the standard train/dev/test split for this dataset (table 2). §3.3 shows the distribution of linguistic phenomena at issue here. 3.2 Procedure Given an UCCA graph with annotated terminals, the integration routine projects a SNACS annotation of a token onto the appropriate edge representing a semantic relation. This is illustrated by dashed arrows in figure 1. The procedure starts with a single terminal node, traversing the graph upwards until it finds an edge that satisfies the criteria given by the rules. The rules concern canonical prepositional phrase modifiers, plus a variety of"
K19-1017,N06-2015,0,0.105766,"est nor the largest model, suggesting that the particular linguistic signals and the method of using them have a genuine effect on performance. 6 Related Work The benefits of integrating lexical analysis and sentence semantic or syntactic structure have been pursued by a vast body of work over the years. Compositional approaches to the syntax-semantics interface, such as CCG (Steedman, 2000) and HPSG (Pollard and Sag, 1994), usually integrate the lexicon at the leaves of the syntactic parse, but propagate grammatically-relevant features up the tree. A different approach is taken by OntoNotes (Hovy et al., 2006), which consists of a number of separate, albeit linked tiers, including syntactic and argument structure, but also the lexical tiers of word senses and coreference. Role semantics frequently features in structured semantic schemes. Some approaches, such as PropBank and AMR (Palmer et al., 2005; Banarescu et al., 2013), follow a lexical approach. The Prague Dependency Treebank tectogrammatical layer (Böhmová et al., 2003) uses a few lexiconfree roles, but their semantics is determined by virtue of their linkage to a lexicalized valency lexicon. Universal Decompositional Semantics (White et al."
K19-1017,S17-1022,1,0.88944,"Missing"
K19-1017,S19-2002,0,0.0124667,"and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work. 7 Conclusion We have introduced a new representation combining UCCA semantic structures and SNACS adpositional semantic roles; automatically merged existing annotations to create a gold standard; and experimented with several alternatives for parsing the integrated representation. Our results show that models profit from having access to both"
K19-1017,Q16-1023,0,0.242456,"bited by UCCA structures, notably reentrancies and discontiguous units. There are transitions for creating nonterminal nodes, and for attaching terminal and nonterminal nodes to a primary or remote (reentrant) parent with an UCCA category label on the edge. The transition system is general enough to be able to tackle parsing into a variety of formalisms, including SDP (Oepen et al., 2015) and a simplified form of AMR (Banarescu et al., 2013); Hershcovich et al. (2018) take advantage of this flexibility in their multitask learning framework. TUPA’s learning architecture largely follows that of Kiperwasser and Goldberg (2016). It encodes the parser’s configuration (buffer, stack and intermediate graph structure) using BiLSTMs, and predicts the next transition using an MLP, stacked on top of them. Token-based features, including POS tags, dependency parses, as well as NER and orthographic features, are embedded in the BiLSTM. Another set of features, taking into account the partially constructed graph and previously predicted transition types, is fed into the MLP directly. 4.2 Pipeline We extend TUPA by providing the SNACS label as a feature on the adposition token.9 This is added in preprocessing in the same way a"
K19-1017,S07-1005,0,0.0584806,", but their semantics is determined by virtue of their linkage to a lexicalized valency lexicon. Universal Decompositional Semantics (White et al., 2016) instead defines roles as a bundle of lexicon-free features, elicited by crowdsourcing. The specific inventory for preposition/possessive relations that we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in"
K19-1017,N19-1112,0,0.0135803,"h coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work. 7 Conclusion We have introduced a new representation combining UCCA semantic structures and SNACS adpositional semantic roles; automatically merged existing annotations to create a gold standard; and experimented with several alternatives for parsing the integrated representation. Our results show that models profit from having access to both structural and lexi"
K19-1017,W04-2609,0,0.0977459,"ollow a lexical approach. The Prague Dependency Treebank tectogrammatical layer (Böhmová et al., 2003) uses a few lexiconfree roles, but their semantics is determined by virtue of their linkage to a lexicalized valency lexicon. Universal Decompositional Semantics (White et al., 2016) instead defines roles as a bundle of lexicon-free features, elicited by crowdsourcing. The specific inventory for preposition/possessive relations that we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be e"
K19-1017,S15-2153,0,0.202558,"Missing"
K19-1017,W03-0411,0,0.223052,"Missing"
K19-1017,J09-2002,0,0.0857217,"Missing"
K19-1017,J05-1004,0,0.796137,"the intended roles to a human reader. 2.2 SNACS SNACS is an inventory of 50 roles/relations used to disambiguate adpositions and possessives in multiple languages, including English (Schneider et al., 2018, 2019), Mandarin Chinese (Zhu et al., 2019), and to a lesser extent, Korean, Hindi, and Hebrew (Hwang et al., 2017). Many of the SNACS labels, such as AGENT, T HEME, and T OPIC, are derived from VerbNet’s (Kipper et al., 2008) core roles of predicates. (Others, such as Q UANTITY and W HOLE, are for entity modification.) But unlike VerbNet, FrameNet (Fillmore and Baker, 2009), and PropBank (Palmer et al., 2005), SNACS does not require a language-specific predicate lexicon (hence Schneider et al. (2018) use the term “supersenses”, which we adopt in the remainder of this paper)—and is therefore compatible with UCCA’s design principle of crosslinguistic applicability.5 Currently, SNACS labels are applied directly to lexical items, without marking up underlying structure on either the subword (morphological) or the sentence-structure level. 3 Automatically Integrating Semantic Roles with UCCA With the benefit of a jointly annotated corpus, we examined the data and determined that the proper placement of"
K19-1017,P18-1018,1,0.0571484,"b Prange Nathan Schneider Georgetown University Omri Abend The Hebrew University of Jerusalem jakob@cs.georgetown.edu nathan.schneider@georgetown.edu oabend@cs.huji.ac.il N A TY TI N R Q er nt Fxn Ce r Cente R NA TIO UA AL A Process O A LA ∣Q G 1 Linker A∣ Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) is a typologically-informed, broad-coverage semantic annotation scheme that describes coarse-grained predicate-argument structure but currently lacks semantic roles. We argue that lexicon-free annotation of the semantic roles marked by prepositions, as formulated by Schneider et al. (2018), is complementary and suitable for integration within UCCA. We show empirically for English that the schemes, though annotated independently, are compatible and can be combined in a single semantic graph. A comparison of several approaches to parsing the integrated representation lays the groundwork for future research on this task. XP ces s H Pro H∣E Abstract I went to ohm after reading some of the reviews Figure 1: Semantic parse illustrating the integrated representation proposed here. Solid edges are the UCCA parse’s primary edges, and the dotted edge is a remote edge. Dashed arrows show"
K19-1017,N15-1177,1,0.884414,"Missing"
K19-1017,W19-3316,1,0.823685,"atz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work. 7 Conclusion We have introduced a new representation combining UCCA semantic structures and SNACS adpositional semantic roles; automatically merged existing annotations to create a gold standard; and experimented with"
K19-1017,D11-1012,0,0.0274562,"at we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work."
K19-1017,Q13-1019,0,0.0221331,"ics (White et al., 2016) instead defines roles as a bundle of lexicon-free features, elicited by crowdsourcing. The specific inventory for preposition/possessive relations that we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT conte"
K19-1017,W15-3502,1,0.789181,"rser code: https://github.com/ integration routine and evaluation scripts are being released as part of the UCCA PyPI package and under https://github.com/jakpra/ucca. ucca-streusle; jakpra/tupa; the 175 UCCA UCCA is a semantic annotation scheme rooted in typological and cognitive linguistic theory. It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA’s foundational layer, which is the only layer annotated over text so far,3 reflects a coarse-grained level of semantics that has been shown to be preserved remarkably well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018b), 2 Note that mapping between syntactic and semantic relations varies by construction: in She spoke on security, the semantic head of the relation between spoke and security corresponds to the syntactic head (the verb), whereas in Her speech was on security, UD treats security as the syntactic head (§3). 3 Prange et al. (2019) have proposed and piloted a coreference layer that sits above the foundational layer. as well as to the evaluation of a number of textto-text generation tasks (Birch et al., 2016;"
K19-1017,N18-1202,0,0.0115588,". Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019). State-of-the-art results on UCCA parsing and SNACS disambiguation are described in contemporaneous work by Jiang et al. (2019); Liu et al. (2019), who achieve substantial gains using the ELMo and BERT contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019). This is an orthogonal direction to the one we pursue here, and combining the two is left to future work. 7 Conclusion We have introduced a new representation combining UCCA semantic structures and SNACS adpositional semantic roles; automatically merged existing annotations to create a gold standard; and experimented with several alternatives for parsing the integrated representation. Our results show that models profit from having access to both structural and lexical semantic information, confirming our hypothesis that UCCA and SNACS are complementary and compatible. B"
K19-1017,N18-1063,1,0.864593,"d as part of the UCCA PyPI package and under https://github.com/jakpra/ucca. ucca-streusle; jakpra/tupa; the 175 UCCA UCCA is a semantic annotation scheme rooted in typological and cognitive linguistic theory. It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA’s foundational layer, which is the only layer annotated over text so far,3 reflects a coarse-grained level of semantics that has been shown to be preserved remarkably well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018b), 2 Note that mapping between syntactic and semantic relations varies by construction: in She spoke on security, the semantic head of the relation between spoke and security corresponds to the syntactic head (the verb), whereas in Her speech was on security, UD treats security as the syntactic head (§3). 3 Prange et al. (2019) have proposed and piloted a coreference layer that sits above the foundational layer. as well as to the evaluation of a number of textto-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). Formally, UCCA structures are directed acy"
K19-1017,P18-1016,1,0.812053,"d as part of the UCCA PyPI package and under https://github.com/jakpra/ucca. ucca-streusle; jakpra/tupa; the 175 UCCA UCCA is a semantic annotation scheme rooted in typological and cognitive linguistic theory. It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA’s foundational layer, which is the only layer annotated over text so far,3 reflects a coarse-grained level of semantics that has been shown to be preserved remarkably well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018b), 2 Note that mapping between syntactic and semantic relations varies by construction: in She spoke on security, the semantic head of the relation between spoke and security corresponds to the syntactic head (the verb), whereas in Her speech was on security, UD treats security as the syntactic head (§3). 3 Prange et al. (2019) have proposed and piloted a coreference layer that sits above the foundational layer. as well as to the evaluation of a number of textto-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). Formally, UCCA structures are directed acy"
K19-1017,W19-3319,1,0.586466,"al layer, which is the only layer annotated over text so far,3 reflects a coarse-grained level of semantics that has been shown to be preserved remarkably well across translations (Sulem et al., 2015). It has also been successfully used for improving text simplification (Sulem et al., 2018b), 2 Note that mapping between syntactic and semantic relations varies by construction: in She spoke on security, the semantic head of the relation between spoke and security corresponds to the syntactic head (the verb), whereas in Her speech was on security, UD treats security as the syntactic head (§3). 3 Prange et al. (2019) have proposed and piloted a coreference layer that sits above the foundational layer. as well as to the evaluation of a number of textto-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). Formally, UCCA structures are directed acyclic graphs over units (nodes covering a subset of tokens). Atomic units are the leaves of the graph: individual tokens or unanalyzable MWEs. Nonterminal units represent larger semantic constituents, such as scenes and compositional participants/ modifiers. The example in figure 1 has 5 nonterminal units. Each unit (save for the"
K19-1017,P13-1037,0,0.0246997,"ank tectogrammatical layer (Böhmová et al., 2003) uses a few lexiconfree roles, but their semantics is determined by virtue of their linkage to a lexicalized valency lexicon. Universal Decompositional Semantics (White et al., 2016) instead defines roles as a bundle of lexicon-free features, elicited by crowdsourcing. The specific inventory for preposition/possessive relations that we use is SNACS, but there is a wider history of disambiguation of these items, especially in English: disambiguation systems have been described for possessives (Moldovan et al., 2004; Badulescu and Moldovan, 2009; Tratz and Hovy, 2013), prepositions with lexicalized sense definitions (e.g., Litkowski and Hargraves, 2007; Tratz and Hovy, 2011), and prepositions with coarsegrained classes (O’Hara and Wiebe, 2003, 2009; Srikumar and Roth, 2013; Gonen and Goldberg, 2016). Such disambiguation has also been investigated in tandem with semantic role labeling and parsing (Dahlmeier et al., 2009; Srikumar and Roth, 2011; Gong et al., 2018). Preliminary work suggests that SNACS may be applicable to subjects and objects, not just PPs, and thus in the future this framework could be extended to all UCCA participants (Shalev et al., 2019"
K19-1017,D16-1177,0,0.21101,"Missing"
K19-1017,S14-2008,0,\N,Missing
K19-1017,D11-1116,0,\N,Missing
K19-1017,D18-1412,0,\N,Missing
K19-1017,S19-2001,1,\N,Missing
K19-1028,W19-5351,0,0.0203816,"system presents a locality bias in a controlled environment, we examine a setting of arbitrary absolute order of the source-side tokens. In this case, systems that are predisposed towards monotonic decoding are likely to present lower performance, while systems that have no predisposition as to the order of the target side tokens relative to the source-side tokens are not expected to show any change in per2 In WMT 2019 English-German phenomena were tested with a new corpus, using both human and automatic evaluation. It is not possible, however, to use this evaluation outside the competition (Avramidis et al., 2019). 293 formance. In order to create a controlled setting, where source-side token order is arbitrary, we extract fixed length sentences, and apply the same permutation to all of them. We then train systems with the permuted source-side data (and the same target-side data), and compare results to a control condition where no permutation is applied. Concretely, we experiment on a GermanEnglish setting, extracting all sentences of the most common length (18) from the WMT2015 (Bojar et al., 2015) training data. This results in 130,983 sentences, of which we hold out 1,000 sentences for testing. It"
K19-1028,J17-3002,0,0.0228455,"and French). These corpora are carefully made but are small in size (ten examples per phenomenon), which means that evaluation must be done manually as well. As our methodology extracts sentences automatically based on parser output, we are able to compile much larger challenge sets, which allows us to apply standard MT measures to each subcorpus corresponding to a specific phenomenon. The methodology is, therefore, more flexible, and can be straightforwardly adapted to accommodate future advances in MT evaluation. and languages present a countless number of longdistance reordering phenomena (Deng and Xue, 2017). One example is subject-verb agreement, where a correct translation requires that the verb is inflected according to the headword of the subject (e.g., in English “dogs that ..., bark”, while “a dog that ..., barks”). When translating such cases, a locality bias may impede performance, by biasing the model not to attend to both the subject’s head and the main verb (which may be arbitrarily distant), thereby disallowing it to correctly inflect the main verb. Due to the benefits of the locality bias, it featured prominently in statistical MT, including in the IBM models, where alignments are co"
K19-1028,N13-1073,0,0.016367,"in German. However, as befinden always appears with the reflexive sich, it might not pose a challenge to NMT systems, which can essentially ignore the reflexive pronoun upon translation. 4.2 A Test Case on Extracting Sets Next, we discuss the compilation of GermanEnglish and English-German corpora. We select these pairs, as they are among the most studied in MT, and comparatively high results are obtained for them (Bojar et al., 2017). Hence, they are more likely to benefit from a fine-grained analysis. For the reordering LDD corpus, we align each source and target sentences using FastAlign (Dyer et al., 2013) and collect all sentences with at least one pair of source-side and target-side tokens, whose indices have a difference of at least d = 5. For example: Phrasal Verbs are verbs that are made up of a verb and a particle (or several particles), which may change the meaning of the verb unpredictably. Examples of English phrasal verbs include run into (in the sense of meet) and give in, and in German they include examples such as einladen (invite), consisting morphologically of the particle ein and the verb laden (load). A source sentence is said to include a phrasal verb if a particle dependent ("
K19-1028,D16-1134,1,0.900319,"Missing"
K19-1028,P17-2012,0,0.015027,"difference to the absolute word order. Therefore, word distance in itself is not what makes such phenomena challenging, contrary to what one might expect from the definition of LDD. It seems then that these 298 phenomena are especially challenging due to the non-standard linguistic structure (e.g., syntactic and lexical structure), and the varying distances in which LDD manifest themselves. The models, therefore, seem to be unable to learn the linguistic structure underlying these phenomena, which may motivate more explicit modelling of linguistic biases into NMT models, as proposed by, e.g., Eriguchi et al. (2017) and Song et al. (2019). We note that our experiments were not designed to compare the performance of BiLSTM and selfattention models. We, therefore, do not see the Transformer’s inferior performance on Books, relative to Nematus as an indication of the general ability of this model in out-of-domain settings. What is evident from the results is that translating Books is a challenge in itself, probably due to the register of the language, and the presence of frequent non-literal translations. A potential confound is that performance might change with the length of the source in BiLSTMs (Carpuat"
K19-1028,W15-3001,0,0.20004,"automatic evaluation. It is not possible, however, to use this evaluation outside the competition (Avramidis et al., 2019). 293 formance. In order to create a controlled setting, where source-side token order is arbitrary, we extract fixed length sentences, and apply the same permutation to all of them. We then train systems with the permuted source-side data (and the same target-side data), and compare results to a control condition where no permutation is applied. Concretely, we experiment on a GermanEnglish setting, extracting all sentences of the most common length (18) from the WMT2015 (Bojar et al., 2015) training data. This results in 130,983 sentences, of which we hold out 1,000 sentences for testing. It is comparable in training set size to a low-resource language setting. We set a fixed permutation σ : [18] → [18] and train systems on three versions of the training data (settings): (1) R EGULAR, to be used for control; (2) P ERMUTED source-side, in which we apply σ over all source-side tokens; (3) P ER P OS E MB where the positional embeddings of the sourceside tokens are permuted;3 and (4) R EVERSED, where tokens are input in a reverse order. We apply the following permutation, σ, to the"
K19-1028,J93-2003,0,0.0691286,"tem performance. To support our claim, we compile challenge sets for English-German and German-English, which are much larger than any previously released challenge set for MT. The extracted sets are large enough to allow reliable automatic evaluation, which makes the proposed approach a scalable and practical solution for evaluating MT performance on the long-tail of syntactic phenomena.1 1 Introduction The assumption that proximate source words are more likely to correspond to proximate target words has often been introduced as a bias (henceforth, locality bias) into statistical MT systems (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005). While reordering phenomena, abundant for some language pairs, violate this simplifying assumption, it has often proved to be a useful inductive bias in practice, especially when complemented with targeted techniques for addressing non-monotonic translation (e.g., Och, 2002; Chiang, 2005). For example, if an adjective precedes a noun in one language and modifies it syntactically, it is likely that their corresponding words 1 Our extracted challenge sets and codebase are found in https://github.com/borgr/auto_challenge_ sets. 291 Proceedings of the 23rd Confe"
K19-1028,N18-1108,0,0.0281922,"targeted techniques for addressing non-monotonic translation (e.g., Och, 2002; Chiang, 2005). For example, if an adjective precedes a noun in one language and modifies it syntactically, it is likely that their corresponding words 1 Our extracted challenge sets and codebase are found in https://github.com/borgr/auto_challenge_ sets. 291 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 291–303 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics extent structural information even without being instructed to do so explicitly (Gulordava et al., 2018). Futrell and Levy (2018) discuss similar linguistic phenomena to what we discuss in §4.2, and show that LSTM encoder-decoder systems handle them better than previous N-gram based systems, despite being profoundly affected by distance. Transformer (Vaswani et al., 2017) models are also encoder-decoder, but instead of LSTMs, they use self-attention. Self-attention is based on gating all outputs of the previous layer as inputs for the current one; put differently, it aggregates all the input in one step. This approach makes information from all parts of the input sequence equally reachable. Whil"
K19-1028,E06-1032,0,0.107318,"with attention (see §2), do exhibit a locality bias, but that the Transformer, whose encoder is based on self-attention, and in which token position is encoded only through learnedPEs, does not present any such bias. MT Evaluation With major improvements in system performance, crude assessments of performance are becoming less satisfying, i.e., evaluation metrics do not give an indication on the performance of MT systems on important challenges for the field (Isabelle and Kuhn, 2018). String-similarity metrics against a reference are known to be partial and coarsegrained aspects of the task (Callison-Burch et al., 2006), but are still the common practice in various text generation tasks. However, their opaqueness and difficulty to interpret have led to efforts to improve evaluation measures so that they will better reflect the requirements of the task (Anderson et al., 2016; Sulem et al., 2018; Choshen and Abend, 2018b), and to increased interest in defin3.1 Methodology In order to test whether an NMT system presents a locality bias in a controlled environment, we examine a setting of arbitrary absolute order of the source-side tokens. In this case, systems that are predisposed towards monotonic decoding are"
K19-1028,W14-4000,0,0.23833,"Missing"
K19-1028,hashemi-hwa-2014-comparison,0,0.0135006,"nd column), including reordering LDD (Reorder), Verb-Particle Constructions (Particle), Reflexive Verbs (Reflexive) and Preposition Stranding. Columns correspond to the models (Tranformer/Nematus), and the domains (Books/News). (and in some cases their accompanying verbs) is dependent on the prepositional object, which in the case of preposition stranding, may be distant from the preposition itself. For example, translating the car we looked for into German usually uses the verb suchen (search), while translating the car we looked at does not. Translating prepositions is difficult in general (Hashemi and Hwa, 2014), but preposition stranding is especially so, as there is no adjacent object to assist disambiguation. A source sentence is said to include preposition stranding if it contains two nodes with an edge of the type obl (oblique) or a subcategory thereof between them, and the UD POS tag of the dependent is adposition (ADP). For example, Source: [...] wherever she wanted to send the hedgehog to [...] Gloss: [...] where she the hedgehog rolledtowards wanted [...] Target: [...] wo sie den Igel hinrollen wollte [...] 4.3 Baseline Reorder Particle Reflexive Nematus 1k sentences per set respectively.4 T"
K19-1028,N19-1419,0,0.0254029,"ormance is constantly improving, more reliable methods for identifying and classifying their failures are needed. Much research effort is therefore devoted to developing more fine-grained and interpretable evaluation methods, including challenge-set approaches. In this paper, we showed that, using a UD parser, it is possible to extract challenge sets that are large enough to allow scalable MT evaluation of important and challenging phenomena. An accumulating body of research is devoted to the ability of modern neural architectures such as LSTMs (Linzen et al., 2016) and pretrained embeddings (Hewitt and Manning, 2019; Liu et al., 2019; Jawahar et al., 2019) to represent linguistic features. This paper makes a contribution to this literature in confirming that the Transformer model can indeed be made indifferent to the absolute order of the words, but also shows that this does not entail that the model can overcome the difficulties of LDD in naturalistic data. We may carefully conclude then that despite the remarkable feats of current NMT models, inducing linguistic structure in its more evasive and challenging instances is still beyond the reach of stateof-the-art NMT, which motivates exploring more lingu"
K19-1028,P05-1033,0,0.298357,"e compile challenge sets for English-German and German-English, which are much larger than any previously released challenge set for MT. The extracted sets are large enough to allow reliable automatic evaluation, which makes the proposed approach a scalable and practical solution for evaluating MT performance on the long-tail of syntactic phenomena.1 1 Introduction The assumption that proximate source words are more likely to correspond to proximate target words has often been introduced as a bias (henceforth, locality bias) into statistical MT systems (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005). While reordering phenomena, abundant for some language pairs, violate this simplifying assumption, it has often proved to be a useful inductive bias in practice, especially when complemented with targeted techniques for addressing non-monotonic translation (e.g., Och, 2002; Chiang, 2005). For example, if an adjective precedes a noun in one language and modifies it syntactically, it is likely that their corresponding words 1 Our extracted challenge sets and codebase are found in https://github.com/borgr/auto_challenge_ sets. 291 Proceedings of the 23rd Conference on Computational Natural Lang"
K19-1028,P18-1127,1,0.833563,"ce are becoming less satisfying, i.e., evaluation metrics do not give an indication on the performance of MT systems on important challenges for the field (Isabelle and Kuhn, 2018). String-similarity metrics against a reference are known to be partial and coarsegrained aspects of the task (Callison-Burch et al., 2006), but are still the common practice in various text generation tasks. However, their opaqueness and difficulty to interpret have led to efforts to improve evaluation measures so that they will better reflect the requirements of the task (Anderson et al., 2016; Sulem et al., 2018; Choshen and Abend, 2018b), and to increased interest in defin3.1 Methodology In order to test whether an NMT system presents a locality bias in a controlled environment, we examine a setting of arbitrary absolute order of the source-side tokens. In this case, systems that are predisposed towards monotonic decoding are likely to present lower performance, while systems that have no predisposition as to the order of the target side tokens relative to the source-side tokens are not expected to show any change in per2 In WMT 2019 English-German phenomena were tested with a new corpus, using both human and automatic eval"
K19-1028,C18-1054,0,0.0225103,"handle them better than previous N-gram based systems, despite being profoundly affected by distance. Transformer (Vaswani et al., 2017) models are also encoder-decoder, but instead of LSTMs, they use self-attention. Self-attention is based on gating all outputs of the previous layer as inputs for the current one; put differently, it aggregates all the input in one step. This approach makes information from all parts of the input sequence equally reachable. While this is not the only architecture with such attributes (van den Oord et al., 2016), we focus on it due to its SoTA results for MT (Lakew et al., 2018). The Transformer’s use of self-attention inspired other works in related fields (Devlin et al., 2018), some of which attributed their performance gains to the model’s ability to capture long-range context (Müller et al., 2018). As the Transformer does not aggregate input sequentially, token positions must be represented through other means. For that purpose, the embedding of each input token W is concatenated with an embedding of its position in the source sentence P . While positional embeddings can generally be any vectors, two implementations are commonly used (Tebbifakhr et al., 2018; Guo"
K19-1028,Q16-1037,0,0.17078,"it updates. At every step, it discards some of the current and past information and aggregates the rest into the state. Any information about the past comes from this state, which is a learned “summary” of the previous states (cf. Greff et al., 2017). Hence, for information to reach a certain prediction step, it should be stored and then kept throughout the intermediate steps (tokens). While theoretically information could be kept indefinitely (Hochreiter and Schmidhuber, 1997), practical evidence shows that LSTMs performance decreases with the distance between the trigger and the prediction (Linzen et al., 2016; Liu et al., 2018), and that they have difficulties generalizing over sequence lengths (Suzgun et al., 2018). Despite being affected by absolute distances between syntactically dependent tokens (Linzen et al., 2016), LSTMs tend to learn to a certain P(pos,2i) = sin(pos/10, 0002i/dim ) P(pos,2i+1) = cos(pos/10, 0002i/dim ) where dim is the dimension of the embedding. Vaswani et al. (2017) report that they see no benefit in learnedPEs, and hence use SinePEs, which have much fewer parameters. Most of the dependencies between words are short. Short-distance linguistic dependencies include some of"
K19-1028,D17-1263,0,0.0626495,"pendencies become a priority, 292 ing more interpretable and telling measures (Lo and Wu, 2011; Hodosh et al., 2013; Birch et al., 2016; Choshen and Abend, 2018a). A promising path forward is complementing string-similarity evaluation with linguistically meaningful challenge sets. Such sets have the advantage of being interpretable: they test for specific phenomena that are important for humans and are crucial for language understanding. Interpretability also means that evaluation artefacts are more likely to be detected earlier. So far, such challenge sets were constructed for FrenchEnglish (Isabelle et al., 2017; Isabelle and Kuhn, 2018) and English-Swedish (Ahrenberg, 2018) 2 . Previous challenge sets were compiled by manually searching corpora for specific phenomena of interest (e.g., yes-no questions which are formulated differently in English and French). These corpora are carefully made but are small in size (ten examples per phenomenon), which means that evaluation must be done manually as well. As our methodology extracts sentences automatically based on parser output, we are able to compile much larger challenge sets, which allows us to apply standard MT measures to each subcorpus correspondi"
K19-1028,N19-1112,0,0.022965,"oving, more reliable methods for identifying and classifying their failures are needed. Much research effort is therefore devoted to developing more fine-grained and interpretable evaluation methods, including challenge-set approaches. In this paper, we showed that, using a UD parser, it is possible to extract challenge sets that are large enough to allow scalable MT evaluation of important and challenging phenomena. An accumulating body of research is devoted to the ability of modern neural architectures such as LSTMs (Linzen et al., 2016) and pretrained embeddings (Hewitt and Manning, 2019; Liu et al., 2019; Jawahar et al., 2019) to represent linguistic features. This paper makes a contribution to this literature in confirming that the Transformer model can indeed be made indifferent to the absolute order of the words, but also shows that this does not entail that the model can overcome the difficulties of LDD in naturalistic data. We may carefully conclude then that despite the remarkable feats of current NMT models, inducing linguistic structure in its more evasive and challenging instances is still beyond the reach of stateof-the-art NMT, which motivates exploring more linguistically-informed"
K19-1028,D10-1092,0,0.0244693,"al., 2015) from the news domain that is commonly used as a development set for EnglishGerman. The other is the relatively unused Books corpus (Tiedemann, 2012) from the more challenging domain of literary translation. The corpora are of sizes 51K and 3K respectively. For lexical LDD, we took the distance (d) between the relevant words to be at least 1, meaning there is at least one word separating them. See Tables 2, 3 for the sizes of the extracted corpora. For evaluation, we use the MOSES implementation of BLEU (Papineni et al., 2002; Koehn et al., 2007), and for reordering LDD, also RIBES (Isozaki et al., 2010), which focuses on reordering. RIBES measures the correlation of n-gram ranks between the output and the reference, where n-gram appears uniquely and in both. Experiments We turn to evaluate SoTA NMT performance on the extracted challenge sets. Experimental Setup. We trained the Transformer on WMT2015 training data (Bojar et al., 2015), for parameters see §3.1. For Nematus we used the non-ensemble pre-trained model from (Sennrich et al., 2017a). Each of the test sets, either a baseline or a challenge sets, for the Transformer and Nematus used a maximum of 10k and Manual Validation. To assess t"
K19-1028,W18-3024,0,0.0144744,"step, it discards some of the current and past information and aggregates the rest into the state. Any information about the past comes from this state, which is a learned “summary” of the previous states (cf. Greff et al., 2017). Hence, for information to reach a certain prediction step, it should be stored and then kept throughout the intermediate steps (tokens). While theoretically information could be kept indefinitely (Hochreiter and Schmidhuber, 1997), practical evidence shows that LSTMs performance decreases with the distance between the trigger and the prediction (Linzen et al., 2016; Liu et al., 2018), and that they have difficulties generalizing over sequence lengths (Suzgun et al., 2018). Despite being affected by absolute distances between syntactically dependent tokens (Linzen et al., 2016), LSTMs tend to learn to a certain P(pos,2i) = sin(pos/10, 0002i/dim ) P(pos,2i+1) = cos(pos/10, 0002i/dim ) where dim is the dimension of the embedding. Vaswani et al. (2017) report that they see no benefit in learnedPEs, and hence use SinePEs, which have much fewer parameters. Most of the dependencies between words are short. Short-distance linguistic dependencies include some of the most common ph"
K19-1028,P11-1023,0,0.0768939,"Missing"
K19-1028,P19-1356,0,0.0146616,"le methods for identifying and classifying their failures are needed. Much research effort is therefore devoted to developing more fine-grained and interpretable evaluation methods, including challenge-set approaches. In this paper, we showed that, using a UD parser, it is possible to extract challenge sets that are large enough to allow scalable MT evaluation of important and challenging phenomena. An accumulating body of research is devoted to the ability of modern neural architectures such as LSTMs (Linzen et al., 2016) and pretrained embeddings (Hewitt and Manning, 2019; Liu et al., 2019; Jawahar et al., 2019) to represent linguistic features. This paper makes a contribution to this literature in confirming that the Transformer model can indeed be made indifferent to the absolute order of the words, but also shows that this does not entail that the model can overcome the difficulties of LDD in naturalistic data. We may carefully conclude then that despite the remarkable feats of current NMT models, inducing linguistic structure in its more evasive and challenging instances is still beyond the reach of stateof-the-art NMT, which motivates exploring more linguistically-informed models. 6 Acknowledgme"
K19-1028,W18-6307,0,0.0200778,"ention is based on gating all outputs of the previous layer as inputs for the current one; put differently, it aggregates all the input in one step. This approach makes information from all parts of the input sequence equally reachable. While this is not the only architecture with such attributes (van den Oord et al., 2016), we focus on it due to its SoTA results for MT (Lakew et al., 2018). The Transformer’s use of self-attention inspired other works in related fields (Devlin et al., 2018), some of which attributed their performance gains to the model’s ability to capture long-range context (Müller et al., 2018). As the Transformer does not aggregate input sequentially, token positions must be represented through other means. For that purpose, the embedding of each input token W is concatenated with an embedding of its position in the source sentence P . While positional embeddings can generally be any vectors, two implementations are commonly used (Tebbifakhr et al., 2018; Guo et al., 2018): learned positional embeddings (learnedPEs; P is randomly initialized), and sine positional embeddings (SinePEs) defined as: and use a dependency parser to find instances of these phenomena in the source side of"
K19-1028,Q17-1024,0,0.0290812,"translation of a single word or phrase is determined by nonadjacent words on the source side. This requires attending to two or more regions that can be arbitrarily distant from one another. Several phenomena, such as light verbs (Isabelle and Kuhn, 2018), are known from the linguistic and MT literature to yield lexical LDD. Our methodology takes a predefined set of such phenomena, and defines rules for detecting each of them over dependency parses of the source-side. See §4.2 for the list of phenomena we experiment on in this paper. information, e.g., the target language in multilingual MT (Johnson et al., 2017). Having no locality bias implies this additional information can be added at any fixed point in the sequence fed to a Transformer, provided that the positional embeddings do not themselves introduce such a bias. This is not the case with BiLSTMs, which often require introducing the same information at each input token to allow them to be effectively used by the system (Yao et al., 2017; Rennie et al., 2017). 4 LDD Challenge Sets Focusing on LDD, we restrict ourselves to instances where the absolute distance between the word and the dependent is at least d ∈ N. Selecting large enough d entails"
K19-1028,W18-6322,0,0.0201504,". (2019). We note that our experiments were not designed to compare the performance of BiLSTM and selfattention models. We, therefore, do not see the Transformer’s inferior performance on Books, relative to Nematus as an indication of the general ability of this model in out-of-domain settings. What is evident from the results is that translating Books is a challenge in itself, probably due to the register of the language, and the presence of frequent non-literal translations. A potential confound is that performance might change with the length of the source in BiLSTMs (Carpuat et al., 2013; Murray and Chiang, 2018), in Transformers it was reported to increase (Zhang et al., 2018). Length is generally greater in the challenge set than in the full test set, and generally increases with d, showing if anything a decrease of performance by length. To assess whether our corpora are challenging due to a length bias, we randomly sample from Books 1,000 corpora with 1,000, 100 and 10 sentences each. The correlation between their corresponding average length and the Transformers’ BLEU score on them was 0.06,0.09 and 0.03 respectively. While this suggests length is not a strong predictor of performance, to verify"
K19-1028,N03-1017,0,0.128809,"support our claim, we compile challenge sets for English-German and German-English, which are much larger than any previously released challenge set for MT. The extracted sets are large enough to allow reliable automatic evaluation, which makes the proposed approach a scalable and practical solution for evaluating MT performance on the long-tail of syntactic phenomena.1 1 Introduction The assumption that proximate source words are more likely to correspond to proximate target words has often been introduced as a bias (henceforth, locality bias) into statistical MT systems (Brown et al., 1993; Koehn et al., 2003; Chiang, 2005). While reordering phenomena, abundant for some language pairs, violate this simplifying assumption, it has often proved to be a useful inductive bias in practice, especially when complemented with targeted techniques for addressing non-monotonic translation (e.g., Och, 2002; Chiang, 2005). For example, if an adjective precedes a noun in one language and modifies it syntactically, it is likely that their corresponding words 1 Our extracted challenge sets and codebase are found in https://github.com/borgr/auto_challenge_ sets. 291 Proceedings of the 23rd Conference on Computation"
K19-1028,P02-1040,0,0.107343,"h RNN and selfattention NMT architectures, we find that although the latter presents no locality bias, LDD remain challenging. Moreover, lexical LDD become increasingly challenging with their distance, suggesting that syntactic distance remains an important determinant of performance in state-of-the-art (SoTA) NMT. We conclude that evaluating LDD using targeted challenge sets gives a detailed picture of MT performance, and underscores challenges the field has yet to fully address. As particular types of LDD are not frequent enough to significantly affect coarse-grained measures, such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), our evaluation approach provides a complementary perspective on system performance. 2 2.1 Related Work Long-distance Dependencies in MT A common architecture for text-to-text generation tasks is the (Bi)LSTM encoder-decoder (Bahdanau et al., 2015). This architecture consists of several LSTM layers for the encoder and the decoder and a thin attention layer connecting them. LSTM is a recurrent network with a state vector it updates. At every step, it discards some of the current and past information and aggregates the rest into the state. Any information about the"
K19-1028,P09-1103,0,0.0274417,"enefits of the locality bias, it featured prominently in statistical MT, including in the IBM models, where alignments are constrained not to cross too much (Brown et al., 1993), and in predicting probabilities of reorderings (Koehn et al., 2003; Chiang, 2005). Difficulties in handling LDD have motivated the development of syntax-based MT (Yamada and Knight, 2001), that can effectively represent reordering at the phrase level, such as when translating between VSO and SOV languages. However, syntaxbased MT models remain limited in their ability to map between arbitrarily different word orders (Sun et al., 2009; Xiong et al., 2012). For example, reorderings that violate the assumption that the trees form contiguous phrases would be difficult for most such models to capture. In the next section (§3) we show that the Transformer, when implemented with learnedPEs, presents no locality bias, and hence can, in principle, learn dependencies between any two positions of the source, and use them at any step during decoding. 2.2 3 Locality in SoTA NMT In this section we show that encoder-decoder models based on BiLSTM with attention (see §2), do exhibit a locality bias, but that the Transformer, whose encode"
K19-1028,W17-4739,0,0.155634,"this permutation special (examining, e.g., its decomposition into cycles). We therefore assume that similar results will hold for other σs as well. We train a Transformer model, optimizing using Adam (Kingma and Ba, 2015). We set the embedding size to 512, dropout rate of 0.1, 6 stack layers in both the encoder and the decoder and 8 attention heads. We use tokenization, truecasing and BPE (Sennrich et al., 2016) as preprocessing, following the same protocol as (Yang et al., 2018). We experiment both with learnedPEs, and with SinePEs. We train the BiLSTM model using the Nematus implementation (Sennrich et al., 2017b), and use their supplied scripts for preprocessing, training and testing, changing only the datasets used. For all models, we report the highest BLEU score on the test data for any epoch during training, and perform early stopping after 10 consecutive epochs without improvement. In the Transformer with learnedPEs, 5 repetitions were done in the R EGULAR setting, and 5 for 3.3 Discussion Finding that Transformers do not present a locality bias has implications on how to construct their input in MT settings, as well as in other tasks that use self-attention encoders, such as image captioning ("
K19-1028,W18-6471,0,0.0188374,"for MT (Lakew et al., 2018). The Transformer’s use of self-attention inspired other works in related fields (Devlin et al., 2018), some of which attributed their performance gains to the model’s ability to capture long-range context (Müller et al., 2018). As the Transformer does not aggregate input sequentially, token positions must be represented through other means. For that purpose, the embedding of each input token W is concatenated with an embedding of its position in the source sentence P . While positional embeddings can generally be any vectors, two implementations are commonly used (Tebbifakhr et al., 2018; Guo et al., 2018): learned positional embeddings (learnedPEs; P is randomly initialized), and sine positional embeddings (SinePEs) defined as: and use a dependency parser to find instances of these phenomena in the source side of a parallel corpus. As a test case, we apply this method to construct challenge sets (§4.2) for GermanEnglish and English-German. The approach can be easily scaled to other languages for which a good enough parser exists. Experimenting both with RNN and selfattention NMT architectures, we find that although the latter presents no locality bias, LDD remain challenging"
K19-1028,E17-3017,0,0.142469,"this permutation special (examining, e.g., its decomposition into cycles). We therefore assume that similar results will hold for other σs as well. We train a Transformer model, optimizing using Adam (Kingma and Ba, 2015). We set the embedding size to 512, dropout rate of 0.1, 6 stack layers in both the encoder and the decoder and 8 attention heads. We use tokenization, truecasing and BPE (Sennrich et al., 2016) as preprocessing, following the same protocol as (Yang et al., 2018). We experiment both with learnedPEs, and with SinePEs. We train the BiLSTM model using the Nematus implementation (Sennrich et al., 2017b), and use their supplied scripts for preprocessing, training and testing, changing only the datasets used. For all models, we report the highest BLEU score on the test data for any epoch during training, and perform early stopping after 10 consecutive epochs without improvement. In the Transformer with learnedPEs, 5 repetitions were done in the R EGULAR setting, and 5 for 3.3 Discussion Finding that Transformers do not present a locality bias has implications on how to construct their input in MT settings, as well as in other tasks that use self-attention encoders, such as image captioning ("
K19-1028,tiedemann-2012-parallel,0,0.0215838,"gory thereof between them, and the UD POS tag of the dependent is adposition (ADP). For example, Source: [...] wherever she wanted to send the hedgehog to [...] Gloss: [...] where she the hedgehog rolledtowards wanted [...] Target: [...] wo sie den Igel hinrollen wollte [...] 4.3 Baseline Reorder Particle Reflexive Nematus 1k sentences per set respectively.4 Two parallel corpora were used for extracting the challenge sets. One is newstest2013 (Bojar et al., 2015) from the news domain that is commonly used as a development set for EnglishGerman. The other is the relatively unused Books corpus (Tiedemann, 2012) from the more challenging domain of literary translation. The corpora are of sizes 51K and 3K respectively. For lexical LDD, we took the distance (d) between the relevant words to be at least 1, meaning there is at least one word separating them. See Tables 2, 3 for the sizes of the extracted corpora. For evaluation, we use the MOSES implementation of BLEU (Papineni et al., 2002; Koehn et al., 2007), and for reordering LDD, also RIBES (Isozaki et al., 2010), which focuses on reordering. RIBES measures the correlation of n-gram ranks between the output and the reference, where n-gram appears u"
K19-1028,P16-1162,0,0.02536,"inePEs is not without consequences: learnedPEs are preferable if a locality bias is undesired (this is potentially the case for highly divergent language pairs). We did not find any property that would deem this permutation special (examining, e.g., its decomposition into cycles). We therefore assume that similar results will hold for other σs as well. We train a Transformer model, optimizing using Adam (Kingma and Ba, 2015). We set the embedding size to 512, dropout rate of 0.1, 6 stack layers in both the encoder and the decoder and 8 attention heads. We use tokenization, truecasing and BPE (Sennrich et al., 2016) as preprocessing, following the same protocol as (Yang et al., 2018). We experiment both with learnedPEs, and with SinePEs. We train the BiLSTM model using the Nematus implementation (Sennrich et al., 2017b), and use their supplied scripts for preprocessing, training and testing, changing only the datasets used. For all models, we report the highest BLEU score on the test data for any epoch during training, and perform early stopping after 10 consecutive epochs without improvement. In the Transformer with learnedPEs, 5 repetitions were done in the R EGULAR setting, and 5 for 3.3 Discussion Fi"
K19-1028,P12-1095,0,0.0236427,"ality bias, it featured prominently in statistical MT, including in the IBM models, where alignments are constrained not to cross too much (Brown et al., 1993), and in predicting probabilities of reorderings (Koehn et al., 2003; Chiang, 2005). Difficulties in handling LDD have motivated the development of syntax-based MT (Yamada and Knight, 2001), that can effectively represent reordering at the phrase level, such as when translating between VSO and SOV languages. However, syntaxbased MT models remain limited in their ability to map between arbitrarily different word orders (Sun et al., 2009; Xiong et al., 2012). For example, reorderings that violate the assumption that the trees form contiguous phrases would be difficult for most such models to capture. In the next section (§3) we show that the Transformer, when implemented with learnedPEs, presents no locality bias, and hence can, in principle, learn dependencies between any two positions of the source, and use them at any step during decoding. 2.2 3 Locality in SoTA NMT In this section we show that encoder-decoder models based on BiLSTM with attention (see §2), do exhibit a locality bias, but that the Transformer, whose encoder is based on self-at"
K19-1028,silveira-etal-2014-gold,0,0.0459786,"Missing"
K19-1028,N09-1028,0,0.0234618,"ntial nature of the LSTM architecture (Bahdanau et al., 2015, see §2). The influential Transformer model (Vaswani et al., 2017) replaces the sequential LSTMs with self-attention, which does not seem to possess this bias. We show that the default implementation of the Transformer does retain some bias, but that it can be relieved by using learned positional embeddings (§3). Long-distance dependencies (LDD) between words and phrases present a long-standing problem for MT (Sennrich, 2016), as they are generally more difficult to detect (indeed, they pose an ongoing challenge for parsing as well (Xu et al., 2009)), and often result in non-monotonic translation if the target differs from the source in terms of its word order and lexicalization patterns. The Transformer’s indifference to the absolute position of the tokens raises the question of whether longdistance dependencies are still an open problem. We address this question by proposing an automatic method to compile challenge sets for evaluating system performance on LDD (§4). We distinguish between two main LDD types: (1) reordering LDD, namely cases where source and target words largely correspond to one another but are ordered differently; (2)"
K19-1028,2006.amta-papers.25,0,0.0484657,"hitectures, we find that although the latter presents no locality bias, LDD remain challenging. Moreover, lexical LDD become increasingly challenging with their distance, suggesting that syntactic distance remains an important determinant of performance in state-of-the-art (SoTA) NMT. We conclude that evaluating LDD using targeted challenge sets gives a detailed picture of MT performance, and underscores challenges the field has yet to fully address. As particular types of LDD are not frequent enough to significantly affect coarse-grained measures, such as BLEU (Papineni et al., 2002) or TER (Snover et al., 2006), our evaluation approach provides a complementary perspective on system performance. 2 2.1 Related Work Long-distance Dependencies in MT A common architecture for text-to-text generation tasks is the (Bi)LSTM encoder-decoder (Bahdanau et al., 2015). This architecture consists of several LSTM layers for the encoder and the decoder and a thin attention layer connecting them. LSTM is a recurrent network with a state vector it updates. At every step, it discards some of the current and past information and aggregates the rest into the state. Any information about the past comes from this state, w"
K19-1028,P01-1067,0,0.132608,". When translating such cases, a locality bias may impede performance, by biasing the model not to attend to both the subject’s head and the main verb (which may be arbitrarily distant), thereby disallowing it to correctly inflect the main verb. Due to the benefits of the locality bias, it featured prominently in statistical MT, including in the IBM models, where alignments are constrained not to cross too much (Brown et al., 1993), and in predicting probabilities of reorderings (Koehn et al., 2003; Chiang, 2005). Difficulties in handling LDD have motivated the development of syntax-based MT (Yamada and Knight, 2001), that can effectively represent reordering at the phrase level, such as when translating between VSO and SOV languages. However, syntaxbased MT models remain limited in their ability to map between arbitrarily different word orders (Sun et al., 2009; Xiong et al., 2012). For example, reorderings that violate the assumption that the trees form contiguous phrases would be difficult for most such models to capture. In the next section (§3) we show that the Transformer, when implemented with learnedPEs, presents no locality bias, and hence can, in principle, learn dependencies between any two pos"
K19-1028,Q19-1002,0,0.015982,"word order. Therefore, word distance in itself is not what makes such phenomena challenging, contrary to what one might expect from the definition of LDD. It seems then that these 298 phenomena are especially challenging due to the non-standard linguistic structure (e.g., syntactic and lexical structure), and the varying distances in which LDD manifest themselves. The models, therefore, seem to be unable to learn the linguistic structure underlying these phenomena, which may motivate more explicit modelling of linguistic biases into NMT models, as proposed by, e.g., Eriguchi et al. (2017) and Song et al. (2019). We note that our experiments were not designed to compare the performance of BiLSTM and selfattention models. We, therefore, do not see the Transformer’s inferior performance on Books, relative to Nematus as an indication of the general ability of this model in out-of-domain settings. What is evident from the results is that translating Books is a challenge in itself, probably due to the register of the language, and the presence of frequent non-literal translations. A potential confound is that performance might change with the length of the source in BiLSTMs (Carpuat et al., 2013; Murray a"
K19-1028,N18-1122,0,0.0302705,"Missing"
K19-1028,K17-3009,0,0.0138348,"xtracted example below, trat. . . entgegen translates to received. For example: Source: [...] ich trat ihm in wahnsinniger Wut entgegen. Target: [...] I received him in frantic sort. Source: Wäre es ein großer Misserfolg, nicht den Titel in der Ligue 1 zu gewinnen, wie dies in der letzten Saison der Fall war? Gloss: Would-be it a big failure, not the title in the Ligue 1 to win, as this in the last season the case was? Target: In Ligue 1, would not winning the title, like last season, be a big failure? We extract lexical LDD using simple rules over source-side parse trees, parsed with UDPipe (Straka and Straková, 2017). For a sentence to be selected, at least one word should separate the detected pair of words. We picked several wellknown challenging constructions for translation that involve discontiguous phrases: reflexive-verb, verb-particle constructions and preposition stranding. We note that while these constructions often yield lexical LDDs, and are thus expected to be challenging on average, some of their instances can be translated literally (e.g., amuse oneself is translated to amüsieren sich). Preposition Stranding is the case where a preposition does not appear adjacent to the object it refers t"
K19-1028,N18-1063,1,0.835481,"ssments of performance are becoming less satisfying, i.e., evaluation metrics do not give an indication on the performance of MT systems on important challenges for the field (Isabelle and Kuhn, 2018). String-similarity metrics against a reference are known to be partial and coarsegrained aspects of the task (Callison-Burch et al., 2006), but are still the common practice in various text generation tasks. However, their opaqueness and difficulty to interpret have led to efforts to improve evaluation measures so that they will better reflect the requirements of the task (Anderson et al., 2016; Sulem et al., 2018; Choshen and Abend, 2018b), and to increased interest in defin3.1 Methodology In order to test whether an NMT system presents a locality bias in a controlled environment, we examine a setting of arbitrary absolute order of the source-side tokens. In this case, systems that are predisposed towards monotonic decoding are likely to present lower performance, while systems that have no predisposition as to the order of the target side tokens relative to the source-side tokens are not expected to show any change in per2 In WMT 2019 English-German phenomena were tested with a new corpus, using both"
K19-1028,P18-1166,0,0.0182752,"performance of BiLSTM and selfattention models. We, therefore, do not see the Transformer’s inferior performance on Books, relative to Nematus as an indication of the general ability of this model in out-of-domain settings. What is evident from the results is that translating Books is a challenge in itself, probably due to the register of the language, and the presence of frequent non-literal translations. A potential confound is that performance might change with the length of the source in BiLSTMs (Carpuat et al., 2013; Murray and Chiang, 2018), in Transformers it was reported to increase (Zhang et al., 2018). Length is generally greater in the challenge set than in the full test set, and generally increases with d, showing if anything a decrease of performance by length. To assess whether our corpora are challenging due to a length bias, we randomly sample from Books 1,000 corpora with 1,000, 100 and 10 sentences each. The correlation between their corresponding average length and the Transformers’ BLEU score on them was 0.06,0.09 and 0.03 respectively. While this suggests length is not a strong predictor of performance, to verify that difficulty is not a result of the distribution of lengths in"
K19-2001,W13-0101,1,0.863351,"or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of the graph are anchored to possibly discontinuous sequences of surface sub-strings, while interior (or ‘phrasal’) graph nodes are formally unanchored. The UCCA graph for the running example (see the bottom of Figure 2) includes a single scene, whose main relation is the Process (P) evoked by apply. It also contains a secondary relation labeled Adverbial (D), almost impossible, which is broken Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used 5 ing is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such t"
K19-2001,D15-1198,0,0.0255627,"9 parsers also diverge in terms of their assumptions regarding the syntax–semantics interface, some parsing raw text directly to meaning representation graphs, and some producing the graphs from or in parallel with syntactic derivations. and Xue, 2017). The latter approach reached the best performance (Wang et al., 2016; Nguyen and Nguyen, 2017) in two SemEval shared tasks on AMR parsing (May, 2016; May and Priyadarshi, 2017), where in the former it performed as well as a novel character-level neural translation based AMR parser (Barzdins and Gosko, 2016). Compositionbased AMR parsers include Artzi et al. (2015), who combined CCG grammar induction with AMR parsing. Sequence-to-sequence attention-based approaches (Konstas et al., 2017; van Noord and Bos, 2017) use techniques from machine translation to directly generate (linearized) graphs from text. Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target graphs directly instead of a linearization, also treating"
K19-2001,K19-2008,0,0.403223,"niversity of Oslo, Department of Informatics The Hebrew University of Jerusalem, School of Computer Science and Engineering Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics ♦ University of Copenhagen, Department of Computer Science ◦ Link¨oping University, Department of Computer and Information Science ? University of Colorado at Boulder, Department of Linguistics • Brandeis University, Department of Computer Science ♠ ♥ mrp-organizers@nlpl.eu , jchun@brandeis.edu , {straka |uresova}@ufal.mff.cuni.cz Abstract Representation Parsing (MRP 2019). The goal of the task is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, this task combines formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. Participants were invited to develop parsing systems that support five distinct semantic graph frameworks (see §3 below)— which all encode core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in"
K19-2001,D17-1130,0,0.100948,"Missing"
K19-2001,W13-2322,0,0.179348,"aphs need not be rooted trees: Argument sharing across units will give rise to reentrant nodes much like in the other frameworks. For example, technique in Figure 2 is both a Participant in the scene evoked by similar and a Center in the parent unit. UCCA in principle also supports implicit (unexpressed) units which do not correspond to any tokens, but these are currently excluded from parsing evaluation and, thus, suppressed in the UCCA graphs distributed in the context of the shared task. Abstract Meaning Representation Finally, the shared task includes Abstract Meaning Representation (AMR; Banarescu et al., 2013), which in the MRP hierarchy of different formal types of semantic graphs (see §2 above) is simply unanchored, i.e. represents Flavor (2). The AMR framework is independent of particular approaches to derivation and compositionality and, accordingly, does not make explicit how elements of the graph correspond to the surface utterance. Although most AMR parsing research presupposes a pre-processing step that ‘aligns’ graph nodes with (possibly discontinuous) sets of tokens in the underlying input, this anchor6 DM PSD EDS UCCA AMR Flavor 0 0 1 1 2 TRAIN Text Type Sentences Tokens newspaper 35,656"
K19-2001,S16-1176,0,0.0127804,"ition-based, as well as sequence-to-sequence systems. Existing 19 parsers also diverge in terms of their assumptions regarding the syntax–semantics interface, some parsing raw text directly to meaning representation graphs, and some producing the graphs from or in parallel with syntactic derivations. and Xue, 2017). The latter approach reached the best performance (Wang et al., 2016; Nguyen and Nguyen, 2017) in two SemEval shared tasks on AMR parsing (May, 2016; May and Priyadarshi, 2017), where in the former it performed as well as a novel character-level neural translation based AMR parser (Barzdins and Gosko, 2016). Compositionbased AMR parsers include Artzi et al. (2015), who combined CCG grammar induction with AMR parsing. Sequence-to-sequence attention-based approaches (Konstas et al., 2017; van Noord and Bos, 2017) use techniques from machine translation to directly generate (linearized) graphs from text. Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target"
K19-2001,basile-etal-2012-developing,0,0.0638169,"Missing"
K19-2001,W15-0128,1,0.854139,"ing example A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice: DELPH-IN MRS Bi-Lexical Dependencies (DM; top) and Prague Semantic Dependencies (PSD; bottom). DELPH-IN MRS Bi-Lexical Dependencies The DM bi-lexical dependencies (Ivanova et al., 2012) originally derive from the underspecified logical forms computed by the English Resource Grammar (Flickinger et al., 2017; Copestake et al., 2005). These logical forms are not in and of themselves semantic graphs (in the sense of §2 above) and are often refered to as English Resource Semantics (ERS; Bender et al., 2015). The underlying grammar is rooted in the general linguistic theory of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994). Ivanova et al. (2012) propose a two-stage conversion from ERS into bi-lexical semantic dependency graphs, where ERS logical forms are first recast as Elementary Dependency Structures (EDS; Oepen and Lønning, 2006; see below) and then further simplified into pure bi-lexical semantic dependencies, dubbed DELPH-IN MRS Bi-Lexical Dependencies (or DM). As a Flavor (0) framework, graph nodes in DM are restricted to surface tokens. But DM graphs are neither lexica"
K19-2001,D16-1134,1,0.82449,"y Dependency Structures (EDS; top) and Universal Conceptual Cognitive Annotation (UCCA; bottom). into binary predications) do not correspond to individual surface tokens (but are anchored on larger spans, overlapping with anchors from other nodes). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). for improving text simplification (Sulem et al., 2018b), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). The basic unit of annotation is the scene, denoting a situation mentioned in the sentence, typically involving a predicate, participants, and potentially modifiers. Linguistically, UCCA adopts a notion of semantic constituency that transcends pure dependency graphs, in the sense of introducing separate, unlabeled nodes, called units. One or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of the graph are anchored to possibly discontinuous sequences of surface sub-strings, while interior"
K19-2001,P17-1112,0,0.291422,"vich et al. (2018) further showed that multi-task learning with AMR, DM, and UD as auxiliary tasks improves UCCA parsing performance. UCCA also recently featured in a SemEval shared task (Hershcovich et al., 2019), where the composition-based best system (Jiang et al., 2019) outperformed the transition-based baseline by treating the task as constituency tree parsing with the recovery of remote edges as a postprocessing task. EDS, being a result of automatic conversion from English Resource Semantics (Bender et al., 2015), can be derived by any ERG parser (e.g. Callmeier, 2002; Packard, 2012). Buys and Blunsom (2017) were the first to build a purely datadriven EDS parser, combining graph linearization with a custom transition system. Chen et al. (2018) established the state of the art on data-driven EDS parsing, using a neural SHRG-based, ERG-guided parser. Their comparison on in-domain WSJ evaluation data showed parsing accuracies on par or in excess of the full, grammar-based ACE parser of Packard (2012). AMR has been a challenging target representation for parsing, due to the fact that AMRs are Flavor (2), unanchored graphs. AMR parsing was pioneered by Flanigan et al. (2014), who performed alignment a"
K19-2001,P13-2131,0,0.502571,"range of the corresponding sub-strings (rather than by token indices, which would not be robust to tokenization mis-matches). In the Flavor (1) graphs (EDS and UCCA), multiple distinct nodes can have overlapping or even identical anchors; in EDS, for example, the semantics of an adverb like today is decomposed into four nodes, all anchored to the same substring: Evaluation For each of the individual frameworks, there are established ways of evaluating the quality of parser outputs in terms of graph similarity to goldstandard target representations called EDM (Dridan and Oepen, 2011), SMATCH (Cai and Knight, 2013), SDP (Oepen et al., 2014), and UCCA (Hershcovich et al., 2019). There is broad similarity between the framework-specific evaluation metrics used to date, but also some subtle differences. Meaning representation parsing is commonly evaluated in terms of a graph similarity F1 score at implicit q x : time n(x) ∧ today a 1(x) ∧ temp loc(e, x) . The standard EDS and UCCA evaluation metrics determine node identities through anchors (and 6 In principle, one could further view unlabeled edges and their labels as two distinct pieces of information, but the task design shies away from such formal purit"
K19-2001,K19-2006,0,0.10424,"Missing"
K19-2001,K19-2013,0,0.128351,"Missing"
K19-2001,W11-2927,1,0.84568,"quely determined as the character range of the corresponding sub-strings (rather than by token indices, which would not be robust to tokenization mis-matches). In the Flavor (1) graphs (EDS and UCCA), multiple distinct nodes can have overlapping or even identical anchors; in EDS, for example, the semantics of an adverb like today is decomposed into four nodes, all anchored to the same substring: Evaluation For each of the individual frameworks, there are established ways of evaluating the quality of parser outputs in terms of graph similarity to goldstandard target representations called EDM (Dridan and Oepen, 2011), SMATCH (Cai and Knight, 2013), SDP (Oepen et al., 2014), and UCCA (Hershcovich et al., 2019). There is broad similarity between the framework-specific evaluation metrics used to date, but also some subtle differences. Meaning representation parsing is commonly evaluated in terms of a graph similarity F1 score at implicit q x : time n(x) ∧ today a 1(x) ∧ temp loc(e, x) . The standard EDS and UCCA evaluation metrics determine node identities through anchors (and 6 In principle, one could further view unlabeled edges and their labels as two distinct pieces of information, but the task design sh"
K19-2001,P12-2074,1,0.629774,"and UD labeled dependency trees. We then trained the currently best-performing UDPipe architecture (Straka, 2018; Straka et al., 2019), which implements a joint part-of-speech tagger, lemmatizer, and dependency parser employing contextualized BERT embeddings. To avoid overlap of morpho-syntactic training data with the texts underlying the semantic graphs of the shared task, we performed five-fold jack-knifing on the WSJ and EWT corpora. For compatibility with the majority of the training data, the ‘raw’ input strings for the MRP semantic graphs were tokenized using the PTB-style REPP rules of Dridan and Oepen (2012) and input to UDPipe in pre-tokenized form. Whether as merely a source of state-of-the-art PTBstyle tokenization, or as a vantage point for approaches to meaning representation parsing that start from explicit syntactic structure, the optional morpho-syntactic companion data offers community value in its own right. respectively; see Table 2. The shared task has, for the first time, repackaged the five graph banks into a uniform and normalized abstract representation with a common serialization format. The common interchange format for semantic graphs implements the abstract model of Kuhlmann a"
K19-2001,K19-2007,0,0.24925,"niversity of Oslo, Department of Informatics The Hebrew University of Jerusalem, School of Computer Science and Engineering Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics ♦ University of Copenhagen, Department of Computer Science ◦ Link¨oping University, Department of Computer and Information Science ? University of Colorado at Boulder, Department of Linguistics • Brandeis University, Department of Computer Science ♠ ♥ mrp-organizers@nlpl.eu , jchun@brandeis.edu , {straka |uresova}@ufal.mff.cuni.cz Abstract Representation Parsing (MRP 2019). The goal of the task is to advance data-driven parsing into graph-structured representations of sentence meaning. For the first time, this task combines formally and linguistically different approaches to meaning representation in graph form in a uniform training and evaluation setup. Participants were invited to develop parsing systems that support five distinct semantic graph frameworks (see §3 below)— which all encode core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in"
K19-2001,K19-2015,0,0.0373872,"Missing"
K19-2001,P18-1038,0,0.0751759,"recently featured in a SemEval shared task (Hershcovich et al., 2019), where the composition-based best system (Jiang et al., 2019) outperformed the transition-based baseline by treating the task as constituency tree parsing with the recovery of remote edges as a postprocessing task. EDS, being a result of automatic conversion from English Resource Semantics (Bender et al., 2015), can be derived by any ERG parser (e.g. Callmeier, 2002; Packard, 2012). Buys and Blunsom (2017) were the first to build a purely datadriven EDS parser, combining graph linearization with a custom transition system. Chen et al. (2018) established the state of the art on data-driven EDS parsing, using a neural SHRG-based, ERG-guided parser. Their comparison on in-domain WSJ evaluation data showed parsing accuracies on par or in excess of the full, grammar-based ACE parser of Packard (2012). AMR has been a challenging target representation for parsing, due to the fact that AMRs are Flavor (2), unanchored graphs. AMR parsing was pioneered by Flanigan et al. (2014), who performed alignment as a preprocessing step during training. They developed their own rule-based alignment method, complemented by Pourdamghani et al. (2014),"
K19-2001,K19-2016,0,0.133819,"Missing"
K19-2001,S14-2080,0,0.0289716,"lti-task training across frameworks. While some meaning representations have parsers for languages other than English (Oepen et al., 2015; Wang et al., 2018; Damonte and Cohen, 2018; Hershcovich et al., 2019), we limit the discussion here to the state of the art in English meaning representation parsing, as has been the focus of the current shared task. DM and PSD were both among the representations targeted in two SemEval shared tasks on Semantic Dependency Parsing (Oepen et al., 2014, 2015), where the winning system (Kanerva et al., 2015) utilized SVM-based sequence labeling. The runner-up (Du et al., 2014, 2015) used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by using a neural factorization-based multi-task system, sharing parameters between representations and applying joint inference. Stanovsky and Dagan (2018) linearized the bi-lexical graphs and modeled the parsing task as a sequence-to-sequence problem. They also used multi-task learning, adapting multilingual machine translation algorithms to ‘translate’ between text and meaning representations, outperforming the previous best results"
K19-2001,S15-2154,0,0.216771,"Missing"
K19-2001,N18-2020,1,0.780781,"iversal Conceptual Cognitive Annotation (UCCA; bottom). into binary predications) do not correspond to individual surface tokens (but are anchored on larger spans, overlapping with anchors from other nodes). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). for improving text simplification (Sulem et al., 2018b), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). The basic unit of annotation is the scene, denoting a situation mentioned in the sentence, typically involving a predicate, participants, and potentially modifiers. Linguistically, UCCA adopts a notion of semantic constituency that transcends pure dependency graphs, in the sense of introducing separate, unlabeled nodes, called units. One or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of the graph are anchored to possibly discontinuous sequences of surface sub-strings, while interior (or ‘phrasal’) graph nodes are formally unanch"
K19-2001,S16-1186,0,0.0968157,"Missing"
K19-2001,N18-1104,0,0.0451061,"iding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target graphs directly instead of a linearization, also treating alignment as a latent variable with a copy mechanism. Their parser additionally supports UCCA and SDP, and establishes the stateof-the-art in AMR parsing, though without using multi-task training across frameworks. While some meaning representations have parsers for languages other than English (Oepen et al., 2015; Wang et al., 2018; Damonte and Cohen, 2018; Hershcovich et al., 2019), we limit the discussion here to the state of the art in English meaning representation parsing, as has been the focus of the current shared task. DM and PSD were both among the representations targeted in two SemEval shared tasks on Semantic Dependency Parsing (Oepen et al., 2014, 2015), where the winning system (Kanerva et al., 2015) utilized SVM-based sequence labeling. The runner-up (Du et al., 2014, 2015) used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by usi"
K19-2001,P14-1134,0,0.12208,"of the scores of its subgraphs. Factorization-Based Architectures These parsing models for meaning representation also have their roots in syntactic dependency parsing (where they are often called graph-based; McDonald and Pereira, 2006). Given a set of nodes, the basic idea of the factorization-based approach is to find the graph that has the highest score among all possible graphs. In the case of dependency parsing, the goal is to find the Maximum Spanning Tree, and this has been extended to meaning representation parsing, where the goal is to find the Maximum Spanning Connected Subgraphs (Flanigan et al., 2014). To make the computation of the score of a graph practical, the typical strategy is to factorize the score of a graph into the sum of the scores of its subgraphs, and in the case of first-order factorization, into the sum of the scores of its nodes and edges. A popular choice for predicting the edge is to feed the output of an LSTM encoder to a biaffine classifier to predict if an edge exists between a pair of nodes as well as the label of the edge (SJTU– NICT, SUDA–Alibaba, Hitachi, and JBNU), with slight variations as to the input to the LSTM encoder. Due to the difference in anchoring betw"
K19-2001,S15-2161,0,0.0179913,"and establishes the stateof-the-art in AMR parsing, though without using multi-task training across frameworks. While some meaning representations have parsers for languages other than English (Oepen et al., 2015; Wang et al., 2018; Damonte and Cohen, 2018; Hershcovich et al., 2019), we limit the discussion here to the state of the art in English meaning representation parsing, as has been the focus of the current shared task. DM and PSD were both among the representations targeted in two SemEval shared tasks on Semantic Dependency Parsing (Oepen et al., 2014, 2015), where the winning system (Kanerva et al., 2015) utilized SVM-based sequence labeling. The runner-up (Du et al., 2014, 2015) used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by using a neural factorization-based multi-task system, sharing parameters between representations and applying joint inference. Stanovsky and Dagan (2018) linearized the bi-lexical graphs and modeled the parsing task as a sequence-to-sequence problem. They also used multi-task learning, adapting multilingual machine translation algorithms to ‘translate’ between text"
K19-2001,kingsbury-palmer-2002-treebank,0,0.377337,"iyadarshi, 2017). The AMR example graph in Figure 3 has a topology broadly comparable to EDS, with some notable differences. Similar to the UCCA example graph (and unlike EDS), the AMR representation of the coordinate structure is flat. Although most lemmas are linked to derivationally related forms in the sense lexicon, this is not universal, as seen by the nodes corresponding to similar and such as, which are labeled as resemble-01 and exemplify-01, respectively. These sense distinctions (primarily for verbal predicates) are grounded in the inventory of predicates from the PropBank lexicon (Kingsbury and Palmer, 2002; Hovy et al., 2006). Role labels in AMR encode semantic argument positions, with the particular roles defined according to each PropBank sense, though the counting in AMR is zero-based such that the ARG1 and ARG2 roles in Figure 3 often correspond to ARG2 and ARG3, respectively, in the EDS of Figure 2. PropBank distinguishes such numbered arguments from non-core roles labeled from a general semantic inventory, such as frequency, duration, or domain. Figure 3 also shows the use of inverted edges in AMR, for example ARG1-of and mod. These serve to allow annotators (and in principle also parsing"
K19-2001,hajic-etal-2012-announcing,0,0.575983,"Missing"
K19-2001,P19-4002,1,0.832307,"listed. 12 tial submissions declined the invitation to submit a system description for publication in the shared task proceedings (and one team asked to remain anonymous), such that only limited information is available about these parsers, and they will not be considered in further detail in §7. Finally, based on input by task participants, Table 4 also provides an indication of which submissions employed multi-task learning (MTL) and a high-level characterization of the overall parsing approach. The distinction between transition-, factorization-, and composition-based architectures follows Koller et al. (2019) and is discussed in more detail in §7 below. In some submissions there can of course be elements of more than one of these high-level architecture types. Also, not all of the teams who indicate the use of multi-task learning actually apply it across different semantic graph frameworks, but in some cases rather to multiple sub-tasks within the parsing architecture for a single framework.11 The main task results are summarized in Table 6, showing average MRP scores across frameworks, broken down by the different component pieces (see §5 above). These cross-framework averages can only be meaning"
K19-2001,P17-1104,1,0.901739,"e, the sequence of parsing actions will be used to deterministically reconstitute the meaning representation graph. This basic method allows variations in various aspects of the parsing process. First of all, the set of actions can vary from system to system. Apart from the standard actions used in syntactic dependency parsing such as S HIFT, L EFTA RC, R IGHTA RC, and R EDUCE (Nivre, 2003; Yamada and Matsumoto, 2003), transition systems in meaning representation parsing also include actions to create reentrant edges, such as L EFT R EMOTE and R IGHT R EMOTE from the pre-task version of TUPA (Hershcovich et al., 2017). It may also include actions to create abstract concepts that do not correspond to a word token in the input sentence, such as the N ODE action from TUPA, and actions that allow the transition to skip a word token in the input when it does not have semantic content, such as the PASS action from HIT-SCIR. The transition set may also include actions that label the nodes or edges, such as L A BEL in the version of TUPA used in the shared task. CUHK developed a transition-based parser with a general transition system suited for all five frameworks, by including a variable-arity R ESOLVE action. F"
K19-2001,P17-1014,0,0.0341005,"ly to meaning representation graphs, and some producing the graphs from or in parallel with syntactic derivations. and Xue, 2017). The latter approach reached the best performance (Wang et al., 2016; Nguyen and Nguyen, 2017) in two SemEval shared tasks on AMR parsing (May, 2016; May and Priyadarshi, 2017), where in the former it performed as well as a novel character-level neural translation based AMR parser (Barzdins and Gosko, 2016). Compositionbased AMR parsers include Artzi et al. (2015), who combined CCG grammar induction with AMR parsing. Sequence-to-sequence attention-based approaches (Konstas et al., 2017; van Noord and Bos, 2017) use techniques from machine translation to directly generate (linearized) graphs from text. Lyu and Titov (2018) parsed AMR using a joint probabilistic model with latent alignments, avoiding cascading errors due to alignment inaccuracies and outperforming previous approaches. The factorization-based parser by Zhang et al. (2019a,b) uses an attention-based architecture, but derives target graphs directly instead of a linearization, also treating alignment as a latent variable with a copy mechanism. Their parser additionally supports UCCA and SDP, and establishes the s"
K19-2001,P18-1035,1,0.82103,"rticipants were invited to develop parsing systems that support five distinct semantic graph frameworks (see §3 below)— which all encode core predicate–argument structure, among other things—in the same implementation. Ideally, these parsers predict sentence-level meaning representations in all frameworks in parallel. Architectures utilizing complementary knowledge sources (e.g. via parameter sharing) were encouraged, though not required. Learning from multiple flavors of meaning representation in tandem has hardly been explored (with notable exceptions, e.g. the parsers of Peng et al., 2017; Hershcovich et al., 2018; or Stanovsky and Dagan, 2018). The 2019 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks. Five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the training and evaluation data for the task, packaged in a uniform graph abstraction and serialization. The task received submissions from eighteen teams, of which five do not participate in the official ranking because they arrived after the closing deadline, made use of extra training data,"
K19-2001,K19-2011,0,0.145598,"3 3 3 3 7 3 3 3 3 3 3 3 7 7 3 3 7 7 3 3 3 3 3 3 7 7 3 3 7 3 7 ´ UFAL MRPipe§ ∦ Peking § ´ UFAL–Oslo CUHK§ Anonymous§ Peking∦§ 3 3 3 3 7 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 7 3 ∦§† Approach Reference 7 7 3 Composition Transition Transition Oepen and Flickinger (2019) Hershcovich and Arviv (2019) Hershcovich and Arviv (2019) 3 3 3 3 3 3 3 3 7 3 7 7 3 7 7 (3) 7 (3) 7 7 7 7 3 7 ? ? Transition Factorization Factorization Composition Factorization Transition Factorization Factorization Factorization Transition Transition Che et al. (2019) Li et al. (2019) Zhang et al. (2019c) Donatelli et al. (2019) Koreeda et al. (2019) Straka and Strakov´a (2019) Wang et al. (2019) Cao et al. (2019) Na et al. (2019) Bai and Zhao (2019) Droganova et al. (2019) 3 7 3 3 7 7 7 7 7 3 ? 7 Transition Factorization Transition Transition Straka and Strakov´a (2019) Chen et al. (2019) Droganova et al. (2019) Lai et al. (2019) Composition Chen et al. (2019) Table 4: Overview of participating teams. The top and bottom blocks represent ‘unofficial’ submissions, which are not considered for the primary ranking because they used training data beyond the white-listed resources (indicated by the symbol “∦”), arrived after the closing deadli"
K19-2001,S19-2001,1,0.68669,"ng representation parsing. Its contributions include (a) a unifying formal model over different semantic graph banks (§2), (b) uniform representations and scoring (§4 and §6), (c) contrastive evaluation across frameworks (§5), and (d) increased cross-fertilization via transfer and multi-task learning (§7). Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 (Oepen et al., 2014, 2015; May, 2016; May and Priyadarshi, 2017; Hershcovich et al., 2019). Owing to the scarcity of semantic annoAll things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, ‘classic’ (discrete and hierarchically structured) semantic representations will continue to play an important role in ‘making sense’ of natural language. While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate target structures for sentence-level analysis beyond surfac"
K19-2001,J16-4009,1,0.942174,"ng (CoNLL) hosts a shared task (or ‘system bake-off’) on Cross-Framework Meaning 1 Proceedings of the Shared Task on Cross-Framework Meaning Representation Parsing at the 2019 CoNLL, pages 1–27 c Hong Kong, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/K19-2001 tations across frameworks, the MRP 2019 shared task is regrettably limited to parsing English for the time being. 2 A natural generalization of the noncrossing property, where one is allowed to also use the halfplane below the sentence for drawing edges is a property called pagenumber two. Kuhlmann and Oepen (2016) provide additional definitions and a quantitative summary of various formal graph properties across frameworks. Definitions: Graphs and Flavors Reflecting different traditions and communities, there is wide variation in how individual meaning representation frameworks think (and talk) about semantic graphs, down to the level of visual conventions used in rendering graph structures. The following paragraphs provide semi-formal definitions of core graph-theoretic concepts that can be meaningfully applied across the range of frameworks represented in the shared task. Hierarchy of Formal Flavors"
K19-2001,K19-2002,1,0.70568,"ing parser outputs for all target frameworks. Albeit not the ultimate goal of the cross-framework shared task design, such partiality was explicitly allowed to lower the technical barrier to entry and make it possible to include framework-specific parsers in the comparison. Seven (of thirteen) of the official submissions, as well as the two TUPA baselines, provide semantic graphs for all five frameworks. Three highly par. ? ! : ;,“&quot;”‘&apos;’()[]{} 6 Submissions and Results The task received submissions from sixteen teams, plus another two ‘reference’ submissions prepared by the task co-organizers (Hershcovich and Arviv, 2019; Oepen and Flickinger, 2019). These reference points are not considered in the overall ranking. Non-reference submissions are further subdivided into ‘official’ and ‘unofficial’ ones, where the latter are characterized by either arriving after the closing deadline of the evaluation period or using training data beyond the official resources provided (and white-listed) for the task; see §4 above. Table 4 provides an inventory of participating teams, where the top block corresponds to reference submissions from the co-organizers, and 10 In the case of the factorization-based Peking submission,"
K19-2001,K19-2010,0,0.116608,"Missing"
K19-2001,K19-2004,0,0.0682072,"Missing"
K19-2001,W12-3602,1,0.783003,"NNS n rice NN n ADDR.m ACT ADDR.m PAT RSTR technique NN as IN p conj ADDR.m PAT similar JJ ARG2 ADDR.m EXT be almost VBZ RB ev-w218f2 impossible JJ RSTR apply other VB JJ ev-w119f2 CONJ.m APPS.m crop NNS as IN APPS.m cotton NN CONJ.m soybean NNS CONJ.m and CC rice NN Figure 1: Bi-lexical semantic dependencies for the running example A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice: DELPH-IN MRS Bi-Lexical Dependencies (DM; top) and Prague Semantic Dependencies (PSD; bottom). DELPH-IN MRS Bi-Lexical Dependencies The DM bi-lexical dependencies (Ivanova et al., 2012) originally derive from the underspecified logical forms computed by the English Resource Grammar (Flickinger et al., 2017; Copestake et al., 2005). These logical forms are not in and of themselves semantic graphs (in the sense of §2 above) and are often refered to as English Resource Semantics (ERS; Bender et al., 2015). The underlying grammar is rooted in the general linguistic theory of Head-Driven Phrase Structure Grammar (HPSG; Pollard and Sag, 1994). Ivanova et al. (2012) propose a two-stage conversion from ERS into bi-lexical semantic dependency graphs, where ERS logical forms are first"
K19-2001,W19-3304,1,0.80829,"dicated in each cell: ShanghaiTech, for example, ranks much higher in the framework-specific SDP metric than in the official MRP ranks. These divergences likely reflect the more limited scope of the SDP approach to scoring, which essentially only considers labeled edges (and top nodes, as a pseudo-edge) but ignores node labels, properties, and anchors (which all used to be provided as part of the parser inputs in the original SDP parsing tasks; see §3 above). trastive, phenomena-oriented studies would likely be called for, as for example the comparison of parsing accuracies for EDS vs. AMR by Lin and Xue (2019). 7 Overview of Approaches The participating systems in the shared task have approached this multi-meaning representation task in a variety of ways, which we characterize into three broad families of approaches: transition-, factorization-, or composition-based architectures. Transition-Based Architectures In these parsing system, the meaning representation graph is generated via a series of actions, in a process that is very similar to dependency tree parsing, with the difference being that the actions for graph parsing need to allow reentrancies, as well as (possibly) non-token nodes, labels"
K19-2001,S19-2002,0,0.0756992,"Missing"
K19-2001,P19-1450,0,0.171664,"used an ensemble based on factorization-based weighted tree approximation. More recently, Peng et al. (2017, 2018a,b) improved upon previous approaches by using a neural factorization-based multi-task system, sharing parameters between representations and applying joint inference. Stanovsky and Dagan (2018) linearized the bi-lexical graphs and modeled the parsing task as a sequence-to-sequence problem. They also used multi-task learning, adapting multilingual machine translation algorithms to ‘translate’ between text and meaning representations, outperforming the previous best results on PSD. Lindemann et al. (2019) trained a composition-based parser on DM, PAS, PSD, AMR and EDS, using the Apply–Modify algebra, on which the Saarland submission to the shared task is based. They employed multi-task training with all tackled semantic frameworks and UD, establishing the state of the art on all graph banks but AMR 2017. UCCA parsing was first tackled by Hershcovich et al. (2017), who used a neural transition-based parser. Hershcovich et al. (2018) further showed that multi-task learning with AMR, DM, and UD as auxiliary tasks improves UCCA parsing performance. UCCA also recently featured in a SemEval shared t"
K19-2001,S17-2156,0,0.033905,"Missing"
K19-2001,P18-1037,0,0.260682,"le anchoring in the parsing system has a significant impact on parser performance. Some of the participating systems follow early approaches in AMR parsing and use a separate ‘alignment’ model to provide hard anchorings and then proceed with the rest of the parsing process (e.g. the HIT-SCIR system) assuming the alignments are already in place. Other submissions use a soft alignment component that is trained jointly with other components of their systems. For example, the Amazon and the SUDA– Alibaba parsers jointly model anchoring, node detection, and edge detection, adopting the approach of Lyu and Titov (2018), while the SJTU–NICT system uses a sequence-to-sequence model with a pointer-generator network to predict the concepts in AMR, following Zhang et al. (2019a). That sequence-to-sequence model is trained jointly with other components of their system. Benefits of Multi-Task Learning Another research question the shared task seeks to advance is whether and how multi-task learning (MTL) helps with multi-framework meaning representation parsing. The term, in fact, seems to be applied somewhat variably in the system descriptions. In one sense, it is equated with traditional joint learning, where dif"
K19-2001,W03-3017,0,0.384782,"t and a buffer for yet-to-be processed elements, needs to be maintained. Which action to take next is predicted by a classifier using a representation of the parser state as input. When this parsing procedure is complete, the sequence of parsing actions will be used to deterministically reconstitute the meaning representation graph. This basic method allows variations in various aspects of the parsing process. First of all, the set of actions can vary from system to system. Apart from the standard actions used in syntactic dependency parsing such as S HIFT, L EFTA RC, R IGHTA RC, and R EDUCE (Nivre, 2003; Yamada and Matsumoto, 2003), transition systems in meaning representation parsing also include actions to create reentrant edges, such as L EFT R EMOTE and R IGHT R EMOTE from the pre-task version of TUPA (Hershcovich et al., 2017). It may also include actions to create abstract concepts that do not correspond to a word token in the input sentence, such as the N ODE action from TUPA, and actions that allow the transition to skip a word token in the input when it does not have semantic content, such as the PASS action from HIT-SCIR. The transition set may also include actions that label the n"
K19-2001,J93-2004,0,0.0674913,"DM are restricted to surface tokens. But DM graphs are neither lexically fully covering nor rooted trees, i.e. some tokens do not contribute to the graph, and for some nodes there are multiple incoming edges. In the example DM graph in Figure 1, technique semantically depends on the determiner (the quantificational locus), the modifier similar, and the predicate apply. Conversely, the predicative copula, infinitival to, and the vacusection reviews the frameworks and presents example graphs for sentence #20209013 from the venerable Wall Street Journal (WSJ) Corpus from the Penn Treebank (PTB; Marcus et al., 1993): (1) A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice. The example exhibits some interesting linguistic complexity, including what is called a tough adjective (impossible), a scopal adverb (almost), a tripartite coordinate structure, and apposition. The example graphs in Figures 1 through 3 are presented in order of (arguably) increasing ‘abstraction’ from the surface string, i.e. ranging from ordered Flavor (0) to unanchored Flavor (2). Two of the frameworks in the shared task present simplifications into bi-lexical semantic dependencies (i."
K19-2001,S16-1166,0,0.537725,"‘balkanization’ in the field of meaning representation parsing. Its contributions include (a) a unifying formal model over different semantic graph banks (§2), (b) uniform representations and scoring (§4 and §6), (c) contrastive evaluation across frameworks (§5), and (d) increased cross-fertilization via transfer and multi-task learning (§7). Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 (Oepen et al., 2014, 2015; May, 2016; May and Priyadarshi, 2017; Hershcovich et al., 2019). Owing to the scarcity of semantic annoAll things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, ‘classic’ (discrete and hierarchically structured) semantic representations will continue to play an important role in ‘making sense’ of natural language. While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate targe"
K19-2001,W17-7306,0,0.0913721,"Missing"
K19-2001,S17-2090,0,0.515681,"tion’ in the field of meaning representation parsing. Its contributions include (a) a unifying formal model over different semantic graph banks (§2), (b) uniform representations and scoring (§4 and §6), (c) contrastive evaluation across frameworks (§5), and (d) increased cross-fertilization via transfer and multi-task learning (§7). Thus, the task engages the combined community of parser developers for graph-structured output representations, including from prior framework-specific tasks at the Semantic Evaluation (SemEval) exercises between 2014 and 2019 (Oepen et al., 2014, 2015; May, 2016; May and Priyadarshi, 2017; Hershcovich et al., 2019). Owing to the scarcity of semantic annoAll things semantic are receiving heightened attention in recent years, and despite remarkable advances in vector-based (continuous and distributed) encodings of meaning, ‘classic’ (discrete and hierarchically structured) semantic representations will continue to play an important role in ‘making sense’ of natural language. While parsing has long been dominated by tree-structured target representations, there is now growing interest in general graphs as more expressive and arguably more adequate target structures for sentence-l"
K19-2001,K19-2003,1,0.784391,"rget frameworks. Albeit not the ultimate goal of the cross-framework shared task design, such partiality was explicitly allowed to lower the technical barrier to entry and make it possible to include framework-specific parsers in the comparison. Seven (of thirteen) of the official submissions, as well as the two TUPA baselines, provide semantic graphs for all five frameworks. Three highly par. ? ! : ;,“&quot;”‘&apos;’()[]{} 6 Submissions and Results The task received submissions from sixteen teams, plus another two ‘reference’ submissions prepared by the task co-organizers (Hershcovich and Arviv, 2019; Oepen and Flickinger, 2019). These reference points are not considered in the overall ranking. Non-reference submissions are further subdivided into ‘official’ and ‘unofficial’ ones, where the latter are characterized by either arriving after the closing deadline of the evaluation period or using training data beyond the official resources provided (and white-listed) for the task; see §4 above. Table 4 provides an inventory of participating teams, where the top block corresponds to reference submissions from the co-organizers, and 10 In the case of the factorization-based Peking submission, the extra training data is li"
K19-2001,P13-2017,0,0.074515,"Missing"
K19-2001,S15-2153,1,0.900924,"Missing"
K19-2001,E06-1011,0,0.0181086,"of the Saarland parser, the lexical items are produced by a BiLSTM-based supertagger, and the best derivation is selected in a tree dependency parsing process where the edge between a head and its argument or modifier is labeled with the derivation operation. In the case of the Peking system, the SHRG rules are extracted with a context-free parser, and the derivation is scored by a sum of the scores of its subgraphs. Factorization-Based Architectures These parsing models for meaning representation also have their roots in syntactic dependency parsing (where they are often called graph-based; McDonald and Pereira, 2006). Given a set of nodes, the basic idea of the factorization-based approach is to find the graph that has the highest score among all possible graphs. In the case of dependency parsing, the goal is to find the Maximum Spanning Tree, and this has been extended to meaning representation parsing, where the goal is to find the Maximum Spanning Connected Subgraphs (Flanigan et al., 2014). To make the computation of the score of a graph practical, the typical strategy is to factorize the score of a graph into the sum of the scores of its subgraphs, and in the case of first-order factorization, into t"
K19-2001,S14-2008,1,0.915565,"Missing"
K19-2001,S14-2056,1,0.708772,"g covert quantifiers (e.g. on bare nominals, labeled udef q3 ), the two-place such+as p relation, as well as the implicit conj(unction) relation (which reflects recursive decomposition of the coordinate structure Prague Semantic Dependencies Another instance of simplification from richer syntacticosemantic representations into Flavor (0) bi-lexical semantic dependencies is the reduction of tectogrammatical trees (or t-trees) from the linguistic school of Functional Generative Description (FGD; Sgall et al., 1986; Hajiˇc et al., 2012) into what are called Prague Semantic Dependencies (or PSD). Miyao et al. (2014) sketch the nature of this conversion, which essentially collapses empty (or generated, in FGD terminology) t-tree nodes with corresponding surface nodes and forward-projects incoming dependencies onto all members of paratactic constructions, e.g. the appositive and coordinate structures in the bottom of Figure 1. The PSD graph for our running example has many of the same dependency edges as the DM one (albeit using a different labeling scheme and inverse directionality in a few cases), but it analyzes the predicative copula as semantically contentful and does not treat almost as ‘scoping’ ove"
K19-2001,K19-2009,0,0.0353841,"Missing"
K19-2001,N12-2006,0,0.0317062,"parser. Hershcovich et al. (2018) further showed that multi-task learning with AMR, DM, and UD as auxiliary tasks improves UCCA parsing performance. UCCA also recently featured in a SemEval shared task (Hershcovich et al., 2019), where the composition-based best system (Jiang et al., 2019) outperformed the transition-based baseline by treating the task as constituency tree parsing with the recovery of remote edges as a postprocessing task. EDS, being a result of automatic conversion from English Resource Semantics (Bender et al., 2015), can be derived by any ERG parser (e.g. Callmeier, 2002; Packard, 2012). Buys and Blunsom (2017) were the first to build a purely datadriven EDS parser, combining graph linearization with a custom transition system. Chen et al. (2018) established the state of the art on data-driven EDS parsing, using a neural SHRG-based, ERG-guided parser. Their comparison on in-domain WSJ evaluation data showed parsing accuracies on par or in excess of the full, grammar-based ACE parser of Packard (2012). AMR has been a challenging target representation for parsing, due to the fact that AMRs are Flavor (2), unanchored graphs. AMR parsing was pioneered by Flanigan et al. (2014),"
K19-2001,P18-1173,0,0.0339442,"Missing"
K19-2001,N18-1135,0,0.164463,"Missing"
K19-2001,K19-2012,1,0.876098,"Missing"
K19-2001,W15-3502,1,0.870363,"y apply. It also contains a secondary relation labeled Adverbial (D), almost impossible, which is broken Universal Conceptual Cognitive Annotation Universal Cognitive Conceptual Annotation (UCCA; Abend and Rappoport, 2013) is based on cognitive linguistic and typological theories, primarily Basic Linguistic Theory (Dixon, 2010/2012). The shared task targets the UCCA foundational layer, which focuses on argument structure phenomena (where predicates may be verbal, nominal, adjectival, or otherwise). This coarse-grained level of semantics has been shown to be preserved well across translations (Sulem et al., 2015). It has also been successfully used 5 ing is not part of the meaning representation proper. At the same time, AMR frequently invokes lexical decomposition and normalization towards verbal senses, such that AMR graphs often appear to ‘abstract’ furthest from the surface signal. Since the first general release of an AMR graph bank in 2014, the framework has provided a popular target for data-driven meaning representation parsing and has been the subject of two consecutive tasks at SemEval 2016 and 2017 (May, 2016; May and Priyadarshi, 2017). The AMR example graph in Figure 3 has a topology broa"
K19-2001,D14-1048,0,0.0260749,"quences of ‘raw’ sentence strings and (b) in pretokenized, part-of-speech–tagged, lemmatized, and syntactically parsed form. For the latter, premiumquality English morpho-syntactic analyses were provided to participants, described in more detail below. These parser outputs are referred to as the MRP 2019 morpho-syntactic companion trees. Additional companion data available to participants includes automatically generated reference anchorings (commonly called ‘alignments’ in AMR parsing) for the AMR graphs in the training data, obtained from the JAMR and ISI tools of Flanigan et al. (2016) and Pourdamghani et al. (2014), respectively. Because some of the semantic graph banks involved in the shared task had originally been released by the Linguistic Data Consortium (LDC), the training data was made available to task participants by the LDC under no-cost evaluation licenses. Upon completion of the competition, all task data (including system submissions and evaluation results) are being prepared for general release through the LDC, while those subsets that are copyright-free will also become available for direct, open-source download. Additional Resources For reasons of comparability and fairness, the shared t"
K19-2001,P18-1016,1,0.787871,"hnique is almost impossible to apply to other crops, such as cotton, soybeans and rice: Elementary Dependency Structures (EDS; top) and Universal Conceptual Cognitive Annotation (UCCA; bottom). into binary predications) do not correspond to individual surface tokens (but are anchored on larger spans, overlapping with anchors from other nodes). Conversely, the two nodes associated with similar indicate lexical decomposition as a comparative predicate, where the second argument of the comp relation (the ‘point of reference’) remains unexpressed in Example (1). for improving text simplification (Sulem et al., 2018b), as well as to the evaluation of a number of text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018). The basic unit of annotation is the scene, denoting a situation mentioned in the sentence, typically involving a predicate, participants, and potentially modifiers. Linguistically, UCCA adopts a notion of semantic constituency that transcends pure dependency graphs, in the sense of introducing separate, unlabeled nodes, called units. One or more labels are assigned to each edge. Formally, UCCA has a Type (1) flavor, where leaf (or terminal) nodes of"
K19-2001,L16-1376,0,0.145661,"Missing"
K19-2001,I05-2038,0,0.0146077,"he full novel have long served as a common reference point for AMR, and gold-standard DM and EDS graphs could be converted from the ERS inter-annotator agreement study by Bender et al. (2015). For PSD and UCCA, the 100-sentence subset used for MRP evaluation has been annotated specifically for the shared task. 5 See http://svn.nlpl.eu/mrp/2019/public/ resources.txt for the full list of seventeen generally available third-party resources, including a broad range of large English corpora and distributed word representations. 8 Corpus, as well as to the PTB-style annotations of the GENIA Corpus (Tateisi et al., 2005). This conversion targets Universal Dependencies (UD; McDonald et al., 2013; Nivre, 2015) version 2.x, so that the resulting gold-standard annotations could be concatenated with the UD English Web Treebank (Silveira et al., 2014), for a total of 2.2 million tokens annotated with lemmas, Universal and PTBstyle parts of speech, and UD labeled dependency trees. We then trained the currently best-performing UDPipe architecture (Straka, 2018; Straka et al., 2019), which implements a joint part-of-speech tagger, lemmatizer, and dependency parser employing contextualized BERT embeddings. To avoid ove"
K19-2001,silveira-etal-2014-gold,0,0.0451563,"Missing"
K19-2001,N18-2040,1,0.872943,"Missing"
K19-2001,P19-1446,0,0.0232282,"Missing"
K19-2001,D17-1129,1,0.913091,"Missing"
K19-2001,N15-1040,1,0.925482,"Missing"
K19-2001,K19-2005,0,0.0368897,"Missing"
K19-2001,W03-3023,0,0.0220925,"r for yet-to-be processed elements, needs to be maintained. Which action to take next is predicted by a classifier using a representation of the parser state as input. When this parsing procedure is complete, the sequence of parsing actions will be used to deterministically reconstitute the meaning representation graph. This basic method allows variations in various aspects of the parsing process. First of all, the set of actions can vary from system to system. Apart from the standard actions used in syntactic dependency parsing such as S HIFT, L EFTA RC, R IGHTA RC, and R EDUCE (Nivre, 2003; Yamada and Matsumoto, 2003), transition systems in meaning representation parsing also include actions to create reentrant edges, such as L EFT R EMOTE and R IGHT R EMOTE from the pre-task version of TUPA (Hershcovich et al., 2017). It may also include actions to create abstract concepts that do not correspond to a word token in the input sentence, such as the N ODE action from TUPA, and actions that allow the transition to skip a word token in the input when it does not have semantic content, such as the PASS action from HIT-SCIR. The transition set may also include actions that label the nodes or edges, such as L A BE"
K19-2001,P19-1009,0,0.3054,"Missing"
K19-2001,D19-1392,0,0.220016,"Missing"
K19-2001,K19-2014,0,0.380663,"rov 3 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 3 3 7 7 3 3 7 7 3 3 3 3 3 3 7 7 3 3 7 3 7 ´ UFAL MRPipe§ ∦ Peking § ´ UFAL–Oslo CUHK§ Anonymous§ Peking∦§ 3 3 3 3 7 3 3 3 3 3 3 3 3 3 3 3 7 3 3 3 3 3 7 3 ∦§† Approach Reference 7 7 3 Composition Transition Transition Oepen and Flickinger (2019) Hershcovich and Arviv (2019) Hershcovich and Arviv (2019) 3 3 3 3 3 3 3 3 7 3 7 7 3 7 7 (3) 7 (3) 7 7 7 7 3 7 ? ? Transition Factorization Factorization Composition Factorization Transition Factorization Factorization Factorization Transition Transition Che et al. (2019) Li et al. (2019) Zhang et al. (2019c) Donatelli et al. (2019) Koreeda et al. (2019) Straka and Strakov´a (2019) Wang et al. (2019) Cao et al. (2019) Na et al. (2019) Bai and Zhao (2019) Droganova et al. (2019) 3 7 3 3 7 7 7 7 7 3 ? 7 Transition Factorization Transition Transition Straka and Strakov´a (2019) Chen et al. (2019) Droganova et al. (2019) Lai et al. (2019) Composition Chen et al. (2019) Table 4: Overview of participating teams. The top and bottom blocks represent ‘unofficial’ submissions, which are not considered for the primary ranking because they used training data beyond the white-listed resources (indicated by t"
N15-1122,D13-1178,0,0.0871705,"Missing"
N15-1122,J08-1001,0,0.0391789,"ss the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predicting the likely order of event types is a step towards more intricate planning and reasoning scenarios (see §3), and is useful in itself for tasks such as conceptto-text generation (Reiter et al., 2000), or in validating the correctness of instruction sets. A related idea can be found in modeling sentence coherence (Lapata, 2003; Barzilay and Lapata, 2008, inter alia), although here we focus on lexical relations between events, rather than coherence relations between complete sentences. Compiling a resource of temporal tendencies between events can hardly be done manually, given the number and wealth of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the lates"
N15-1122,J92-4003,0,0.0957719,"rstorder Markovian and use the same (greedy) inference procedure. Lapata’s model differs from G REEDY-L OG L IN in being a generative model, where each event is a tuple of features, and the transition probability between events is defined as the product of transition probabilities between feature pairs. G REEDY-L OG L IN is discriminative, so to be maximally comparable to the presented model. 5 The Feature Set Table 1 presents the complete set of features. We consider three sets of features: Lexical encodes the written forms of the event pair predicates and objects; Brown uses Brown clusters (Brown et al., 1992) to encode similar information, but allows generalization between distributionally similar words; and Frequency encodes the empirical distribution of temporally-related phenomena. The feature definitions make use of several functions. For brevity, we sometimes say that an event e is (a, c1 ) if e’s predicate is a and its first argument is c1 , disregarding its other arguments. Let C be a reference corpus of recipes for collecting statistics. The function B(w) gives the Brown cluster of a word w, as determined by clustering C into 50 clusters {1, . . . , 50}. The function ORD(a, c) returns the"
N15-1122,P14-2082,0,0.0379789,"Missing"
N15-1122,D08-1073,0,0.203506,"h of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013). Previous work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012). However, despite their appeal, these methods have mostly fo1161 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics cused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions, and are hard to extend to include rich features (e.g., order-based and pattern-based features). Furthermore, measuring recall without annotated data is notoriously difficult, and evaluation is often"
N15-1122,P08-1090,0,0.373477,"h of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013). Previous work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012). However, despite their appeal, these methods have mostly fo1161 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics cused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions, and are hard to extend to include rich features (e.g., order-based and pattern-based features). Furthermore, measuring recall without annotated data is notoriously difficult, and evaluation is often"
N15-1122,P09-1068,0,0.176291,"Missing"
N15-1122,W04-3205,0,0.0471795,", given the number and wealth of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013). Previous work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012). However, despite their appeal, these methods have mostly fo1161 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics cused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions, and are hard to extend to include rich features (e.g., order-based and pattern-based features). Furthermore, measuring recall without annotated data is notoriously diffic"
N15-1122,W13-2102,0,0.0613741,"Missing"
N15-1122,W02-1001,0,0.0254305,"Missing"
N15-1122,P07-1030,0,0.0179481,"Missing"
N15-1122,W08-1301,0,0.0143165,"Missing"
N15-1122,N13-1112,0,0.0503299,"Missing"
N15-1122,E14-1006,0,0.211782,"Missing"
N15-1122,E12-1034,0,0.229035,"Missing"
N15-1122,P06-1095,0,0.280094,"s in a domain where this order is aligned with their temporal order, namely cooking recipes. 1 Introduction Temporal relations between events are often implicit, and inferring them relies on lexical and world knowledge about the likely order of events. For instance, to execute the instruction “fry the onion,” the hearer should probably obtain oil beforehand, even if not instructed so explicitly. Lexical knowledge about the likely order of events is therefore necessary for any semantic task that requires temporal reasoning or planning, such as classifying temporal relations (Mani et al., 2006; Lapata and Lascarides, 2006; Yoshikawa et al., 2009; D’Souza and Ng, 2013; Mirza and Tonelli, 2014, inter alia), textual entailment (Dagan et al., 2013) or temporal information extraction (Ling and Weld, 2010). Lexical temporal knowledge is further important for modeling grammatical phenomena such as tense and aspect (Steedman, 2002). In this paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predic"
N15-1122,P03-1069,0,0.236224,"paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predicting the likely order of event types is a step towards more intricate planning and reasoning scenarios (see §3), and is useful in itself for tasks such as conceptto-text generation (Reiter et al., 2000), or in validating the correctness of instruction sets. A related idea can be found in modeling sentence coherence (Lapata, 2003; Barzilay and Lapata, 2008, inter alia), although here we focus on lexical relations between events, rather than coherence relations between complete sentences. Compiling a resource of temporal tendencies between events can hardly be done manually, given the number and wealth of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annot"
N15-1122,J06-4002,0,0.0246016,".edu/software/corenlp.shtml Links to the original recipes, the preprocessed recipes and all extracted events can be found in http://homepages. inf.ed.ac.uk/oabend/event_order.html. 1166 This did not result from an extraction problem, but rather from the recipe text itself being too noisy to interpret. 7 Events are parsed manually so to avoid confounding the results with the parser’s performance. of recipes does not suggest that the textual order is the only order of events that would yield the same outcome. We compute the Kendall’s Tau correlation, a standard measure for information ordering (Lapata, 2006), between the temporal and linear orderings for each recipe. In cases of several events that happen simultaneously (including disjunctions), we take their ordinals to be equal. For instance, for three events where the last two happen at the same time, we take their ordering to be (1,2,2) in our analysis. We find that indeed temporal and textual orderings are in very high agreement, with 6 recipes of the 19 perfectly aligned. The average Kendall’s Tau between the temporal ordering and the linear one is 0.924. 7 Experimental Setup Evaluation. We compute the accuracy of our algorithms by comparin"
N15-1122,W14-2407,0,0.0803724,"Missing"
N15-1122,P05-1012,0,0.0307215,"Missing"
N15-1122,E14-1033,0,0.0122346,"cooking recipes. 1 Introduction Temporal relations between events are often implicit, and inferring them relies on lexical and world knowledge about the likely order of events. For instance, to execute the instruction “fry the onion,” the hearer should probably obtain oil beforehand, even if not instructed so explicitly. Lexical knowledge about the likely order of events is therefore necessary for any semantic task that requires temporal reasoning or planning, such as classifying temporal relations (Mani et al., 2006; Lapata and Lascarides, 2006; Yoshikawa et al., 2009; D’Souza and Ng, 2013; Mirza and Tonelli, 2014, inter alia), textual entailment (Dagan et al., 2013) or temporal information extraction (Ling and Weld, 2010). Lexical temporal knowledge is further important for modeling grammatical phenomena such as tense and aspect (Steedman, 2002). In this paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predicting the likely order of event types is a step towards more intricate p"
N15-1122,W14-1606,0,0.176512,"Missing"
N15-1122,E14-1024,0,0.163803,"Missing"
N15-1122,P09-2004,0,0.0384851,"Missing"
N15-1122,P10-1100,0,0.312406,"Missing"
N15-1122,Q13-1003,0,0.0758771,"Missing"
N15-1122,D13-1177,0,0.172065,"Missing"
N15-1122,D09-1105,0,0.0757134,"Missing"
N15-1122,S13-2001,0,0.0440954,"ons between events, rather than coherence relations between complete sentences. Compiling a resource of temporal tendencies between events can hardly be done manually, given the number and wealth of phenomena that have to be accounted for. Temporally annotated corpora, often annotated according to TimeML principles (Pustejovsky et al., 2003), are a useful resource for studying temporal relations. However, due to incurred costs, annotated corpora are too small for most lexical tasks. For instance, the TimeML annotated data used in the latest TempEval shared task contains only 100K words or so (UzZaman et al., 2013). Previous work that does not rely on manually annotated data has had some success in discovering temporal lexical relations between predicates (Chklovski and Pantel, 2004; Chambers and Jurafsky, 2008b; Talukdar et al., 2012). However, despite their appeal, these methods have mostly fo1161 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1161–1171, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics cused on inducing simple event types, consisting of single words (e.g., “buy-own”) or fixed expressions,"
N15-1122,P09-1046,0,0.0272091,"r is aligned with their temporal order, namely cooking recipes. 1 Introduction Temporal relations between events are often implicit, and inferring them relies on lexical and world knowledge about the likely order of events. For instance, to execute the instruction “fry the onion,” the hearer should probably obtain oil beforehand, even if not instructed so explicitly. Lexical knowledge about the likely order of events is therefore necessary for any semantic task that requires temporal reasoning or planning, such as classifying temporal relations (Mani et al., 2006; Lapata and Lascarides, 2006; Yoshikawa et al., 2009; D’Souza and Ng, 2013; Mirza and Tonelli, 2014, inter alia), textual entailment (Dagan et al., 2013) or temporal information extraction (Ling and Weld, 2010). Lexical temporal knowledge is further important for modeling grammatical phenomena such as tense and aspect (Steedman, 2002). In this paper we address the task of lexical event ordering, namely predicting the ordering of events based only on the identity of the words comprising their predicates and arguments. Concretely, the task is to predict the order of an unordered set of predicate-argument structures. Predicting the likely order of"
N18-1063,P13-1023,1,0.939995,"y judgments were not considered in this experiment. In this paper we collect human judgments for grammaticality, meaning preservation and structural simplicity. To our knowledge, this is the first work to target structural simplicity evaluation, and it does so both through elicitation of human judgments and through the definition of SAMSA. Xu et al. (2016) were the first to propose two evaluation measures tailored for simplification, focusing on lexical simplification. The first metric is FKBLEU, a combination of iBLEU (Sun In this paper we use the UCCA scheme for defining semantic structure (Abend and Rappoport, 2013). UCCA has been shown to be preserved remarkably well across translations (Sulem et al., 2015) and has also been successfully used for machine translation evaluation (Birch et al., 2016) (Section 2). We note, however, that SAMSA can be adapted to work with any semantic scheme that captures predicate-argument relations, such as AMR (Banarescu et al., 2013) or Discourse Representation Structures (Kamp, 1981), as used by Narayan and Gardent (2014). We experiment with SAMSA both where semantic annotation is carried out manually, and where it is carried out by a parser. See Section 4. We conduct hu"
N18-1063,P17-1008,1,0.826314,"ve Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme based on typological (Dixon, 2010b,a, 2012) and cognitive (Langacker, 2008) theories which aims to represent the main semantic phenomena in the text, abstracting away from syntactic detail. UCCA structures are directed acyclic graphs whose nodes (or units) correspond either to the leaves of the graph (including the words of the text) or to several elements jointly viewed as a single entity according to some semantic or cognitive consideration. Unlike AMR, UCCA semantic units are directly anchored in the text (Abend and Rappoport, 2017; Birch et al., 2016), which allows easy inclusion of a word-toword alignment in the metric model (Section 4). The SAMSA Metric SAMSA’s main premise is that a structurally correct simplification is one where: (1) each sentence contains a single event from the input (UCCA Scene), (2) the main relation of each of the events and their participants are retained in the output. For example, consider “John wrote a book. I read that book.” as a simplification of “I read the book that John wrote.”. Each output sentence contains one Scene, which has the same Scene elements as the source, and would thus"
N18-1063,P11-2117,0,0.106706,"ntence, and the number of input sentences split by each of the systems. Levenshtein distances are taken as negative in order to capture similarity between the output and source/reference. The measure with the highest correlation in each row is boldfaced. ˇ and Stajner, 2013)13 (the test corpus contains 54 pairs from this dataset), (2) EncBrit: original sentences from the Encyclopedia Britannica (Barzilay and Elhadad, 2003) and their automatic simplifications obtained using ATS systems based on several ˇ phrase-based statistical MT systems (Stajner et al., 2015) trained on Wikipedia TS corpus (Coster and Kauchak, 2011) (24 pairs), and (3) LSLight: sentences from English Wikipedia and their autoˇ matic simplifications (Glavaˇs and Stajner, 2015) by three different lexical simplification systems (Biran et al., 2011; Horn et al., 2014; Glavaˇs and ˇ Stajner, 2015) (48 pairs). Human evaluation is also provided by this resource, with scores for overall quality, grammaticality, meaning preservation and simplicity. Importantly, the simplicity score does not explicitly refer to the output’s structural simplicity, but rather to its readability. We focus on the overall human score, and compare it to SAMSA. Since diff"
N18-1063,P17-4019,1,0.718902,"ion, compared to the input? Is the output simpler than the input, ignoring the complexity of the words? 6 Table 1: Questions for the human evaluation 6 Human Score Computation Experimental Setup We further compute SAMSA for the 100 sentences of the PWKP test set and the corresponding system outputs. Experiments are conducted in two settings: (1) a semi-automatic setting where Each input-output pair was rated by all five annotators. Other questions appeared without any example. 690 UCCA annotation was carried out manually by a single expert UCCA annotator using the UCCAApp annotation software (Abend et al., 2017), and according to the standard annotation guidelines;8 (2) an automatic setting where the UCCA annotation was carried out by the TUPA parser (Hershcovich et al., 2017). Sentence segmentation of the outputs was carried out using the NLTK package (Loper and Bird, 2002). For word alignments, we used the aligner of Sultan et al. (2014).9 7 tural simplicity, it scores somewhat higher than the semi-automatic SAMSA. The highest correlation with structural simplicity is obtained by the number of sentences with splitting, where SAMSA (automatic and semi-automatic) is second and third highest, although"
N18-1063,W11-2107,0,0.0518887,"8 to 20), randomly selected from the test set (Wubben et al., 2012; Narayan and Gardent, 2014, 2016). The most commonly used automatic measure for TS is BLEU (Papineni et al., 2002). Using 20 source sentences from the PWKP test corpus with 5 simplified sentences for each of them, Wubben et al. (2012) investigated the correlation of BLEU with human evaluation, reporting positive correlation for simplicity, but no correlation for adequacy. ˇ Stajner et al. (2014) explored the correlation with human judgments of six automatic metrics: cosine similarity with a bag-of-words representation, METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011) and two sub-components of TINE: T-BLEU (a variant of BLEU which uses lower n-grams when no 4grams are found) and SRL (based on semantic role labeling). Using 280 pairs of a source sentence and a simplified output with only structural modifications, they found positive correlations for all the metrics except TERp with respect to meaning preservation and positive albeit lower correlations for METEOR, T-BLEU and TINE with respect to grammaticality. Human simplicity judgments were not considered in this experiment. In this paper we collect hum"
N18-1063,W13-2322,0,0.058209,"ere the first to propose two evaluation measures tailored for simplification, focusing on lexical simplification. The first metric is FKBLEU, a combination of iBLEU (Sun In this paper we use the UCCA scheme for defining semantic structure (Abend and Rappoport, 2013). UCCA has been shown to be preserved remarkably well across translations (Sulem et al., 2015) and has also been successfully used for machine translation evaluation (Birch et al., 2016) (Section 2). We note, however, that SAMSA can be adapted to work with any semantic scheme that captures predicate-argument relations, such as AMR (Banarescu et al., 2013) or Discourse Representation Structures (Kamp, 1981), as used by Narayan and Gardent (2014). We experiment with SAMSA both where semantic annotation is carried out manually, and where it is carried out by a parser. See Section 4. We conduct human rating experiments and compare the resulting system rankings with those predicted by SAMSA. We find that SAMSA’s rankings obtain high correlations with human rankings, and compare favorably to existing referencebased measures for TS. Moreover, our results show that existing measures, which mainly target lexical simplification, are ill-suited to predic"
N18-1063,W03-1004,0,0.0773081,"m, and reflect how well the tendency of the systems to introduce changes to the input correlates with the human rankings. The block includes -LDSC , the negative Levenshtein distance from the source sentence, and the number of input sentences split by each of the systems. Levenshtein distances are taken as negative in order to capture similarity between the output and source/reference. The measure with the highest correlation in each row is boldfaced. ˇ and Stajner, 2013)13 (the test corpus contains 54 pairs from this dataset), (2) EncBrit: original sentences from the Encyclopedia Britannica (Barzilay and Elhadad, 2003) and their automatic simplifications obtained using ATS systems based on several ˇ phrase-based statistical MT systems (Stajner et al., 2015) trained on Wikipedia TS corpus (Coster and Kauchak, 2011) (24 pairs), and (3) LSLight: sentences from English Wikipedia and their autoˇ matic simplifications (Glavaˇs and Stajner, 2015) by three different lexical simplification systems (Biran et al., 2011; Horn et al., 2014; Glavaˇs and ˇ Stajner, 2015) (48 pairs). Human evaluation is also provided by this resource, with scores for overall quality, grammaticality, meaning preservation and simplicity. Imp"
N18-1063,R13-2011,0,0.549816,"n. In most of the work investigating the structural operations involved in text simplification, both in rulebased systems (Siddharthan and Angrosh, 2014) and in statistical systems (Zhu et al., 2010; Woodsend and Lapata, 2011), the structures that were considered were syntactic. Narayan and Gardent (2014, 2016) proposed to use semantic structures in the simplification model, in particular in order to avoid splits and deletions which are inconsistent with the semantic structures. SAMSA identifies such incoherent splits, e.g., a split of a phrase describing a single event, and penalizes them. ˇ Glavas and Stajner (2013) presented two simplification systems based on event extraction. One of them, named Event-wise Simplification, transforms each factual event motion into a separate sentence. This approach fits with SAMSA’s stipulation, that an optimal structural simplification is one where each (UCCA-) event in the input sentence is assigned a separate output sentence. However, unlike in their model, SAMSA stipulates that not only should multiple events evoked by a verb in the same sentence be avoided in a simplification, but penalizes sentences containing multiple events evoked by a lexical item of any catego"
N18-1063,P11-2087,0,0.12176,"Missing"
N18-1063,P15-2011,0,0.115809,"Missing"
N18-1063,H93-1040,0,0.582465,"Missing"
N18-1063,P17-1104,1,0.799978,"entered the house is John”. They can also be one of the Participants of another Scene, for example, “he will be late” in the sentence: “He said he will be late”. In the other cases, the Scenes are annotated as parallel Scenes (H) which can be linked by a Linker (L): “WhenL [he will arrive at home]H , [he will call them]H ”. 4.1 Matching Scenes to Sentences SAMSA is based on two external linguistic resources. One is a semantic annotation (UCCA in our experiments) of the source side, which can be carried out either manually or automatically, using the TUPA parser3 (Transition-based UCCA parser; Hershcovich et al., 2017) for UCCA. UCCA decomposes each sentence s into a set of Scenes {sc1 , sc2 , .., scn }, where each scene sci contains a main relation mri (sub-span of sci ) and a set of zero or more participants Ai . The second resource is a word-to-word alignment A between the words in the input and one or zero words in the output. The monolingual alignment thus permits SAMSA not to penalize outputs that involve lexical substitutions (e.g., “comUnit Centers. With regard to units which are not Scenes, the category Center denotes the semantic 3 688 https://github.com/danielhers/tupa Given the input sentence Sc"
N18-1063,P14-2075,0,0.0305553,"ion in each row is boldfaced. ˇ and Stajner, 2013)13 (the test corpus contains 54 pairs from this dataset), (2) EncBrit: original sentences from the Encyclopedia Britannica (Barzilay and Elhadad, 2003) and their automatic simplifications obtained using ATS systems based on several ˇ phrase-based statistical MT systems (Stajner et al., 2015) trained on Wikipedia TS corpus (Coster and Kauchak, 2011) (24 pairs), and (3) LSLight: sentences from English Wikipedia and their autoˇ matic simplifications (Glavaˇs and Stajner, 2015) by three different lexical simplification systems (Biran et al., 2011; Horn et al., 2014; Glavaˇs and ˇ Stajner, 2015) (48 pairs). Human evaluation is also provided by this resource, with scores for overall quality, grammaticality, meaning preservation and simplicity. Importantly, the simplicity score does not explicitly refer to the output’s structural simplicity, but rather to its readability. We focus on the overall human score, and compare it to SAMSA. Since different systems were used to simplify different portions of the input, correlation is taken at the sentence level. We use the same implementations of SAMSA. Manual UCCA annotation is here performed by one of the authors"
N18-1063,N15-1022,0,0.123516,"res the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. They found that FKBLEU and even more so SARI correlate better with human simplicity judgments than BLEU. On the other hand, BLEU (with multiple references) outperforms the other metrics on the dimensions of grammaticality and meaning preservation. As the Parallel Wikipedia Corpus (PWKP), usually used in simplification research, has been shown to contain a large portion of problematic simplifications (Xu et al., 2015; Hwang et al., 2015), Xu et al. (2016) further proposed to use multiple references (instead of a single reference) in the evaluation measures. SAMSA addresses this issue by directly comparing the input and the output of the simplification system, without requiring manually curated references. Semantic Structures in Text Simplification. In most of the work investigating the structural operations involved in text simplification, both in rulebased systems (Siddharthan and Angrosh, 2014) and in statistical systems (Zhu et al., 2010; Woodsend and Lapata, 2011), the structures that were considered were syntactic. Naray"
N18-1063,N18-2020,1,0.655694,"variant, which compares the SRL structures of the source and output (without using references). As some frequent constructions like nominal argument structures are not addressed by the SRL annotation, Birch et al. (2016) proposed HUME, a human evaluation metric based on UCCA, using the semantic annotation only on the source side when comparing it to the output. We differ from HUME in proposing an automatic metric, tackling monolingual text simplification, rather than MT. The UCCA annotation has also been recently used for the evaluation of Grammatical Error Correction (GEC). The USIM metric (Choshen and Abend, 2018) measures the semantic faithfulness of the output to the source by comparing their respective UCCA graphs. and Zhou, 2012), originally proposed for evaluating paraphrase generation by comparing the output both to the reference and to the input, and of the Flesch-Kincaid Index (FK), a measure of the readability of the text (Kincaid et al., 1975). The second one is SARI (System output Against References and against the Input sentence) which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, delete"
N18-1063,P06-1048,0,0.0541134,"easures for Text-to-text Generation. Other than measuring the number of splits (Narayan and Gardent, 2014, 2016), which only assesses the frequency of this operation and not its quality, no structural measures were previously proposed for the evaluation of structural simplification. The need for such a measure is pressing, given recent interest in structural simplification, e.g., in the Split and Rephrase task (Narayan et al., 2017), which focuses on sentence splitting. In the task of sentence compression, which is similar to simplification in that they both involve deletion and paraphrasing, Clarke and Lapata (2006) showed that a metric that uses syntactic dependencies better correlates with human evaluation than a metric based on surface sub-strings. Toutanova et al. (2016) found that structure-aware metrics obtain higher correlation with human evaluation over bigram-based metrics, in particular with grammaticality judgments, but that they do not significantly outperform bigram-based metrics on any parameter. Both Clarke and Lapata (2006) and Toutanova et al. (2016) use reference-based metrics that use syntactic structure on both the output and the references. SAMSA on the other hand 687 head of the uni"
N18-1063,P14-2124,0,0.020749,"esents SAMSA. Section 5 details the collection of human judgments. Our experimental setup for comparing our human and automatic rankings is given in Section 6, and results are given in Section 7, showing superior results for SAMSA. A discussion on the results is presented in Section 8. 686 uses linguistic annotation only on the source side, with semantic structures instead of syntactic ones. Semantic structures were used in MT evaluation, for example in the MEANT metric (Lo et al., 2012), which compares the output and the reference sentences, both annotated using SRL (Semantic Role Labeling). Lo et al. (2014) proposes the XMEANT variant, which compares the SRL structures of the source and output (without using references). As some frequent constructions like nominal argument structures are not addressed by the SRL annotation, Birch et al. (2016) proposed HUME, a human evaluation metric based on UCCA, using the semantic annotation only on the source side when comparing it to the output. We differ from HUME in proposing an automatic metric, tackling monolingual text simplification, rather than MT. The UCCA annotation has also been recently used for the evaluation of Grammatical Error Correction (GEC"
N18-1063,W11-2112,0,0.0226085,"2012; Narayan and Gardent, 2014, 2016). The most commonly used automatic measure for TS is BLEU (Papineni et al., 2002). Using 20 source sentences from the PWKP test corpus with 5 simplified sentences for each of them, Wubben et al. (2012) investigated the correlation of BLEU with human evaluation, reporting positive correlation for simplicity, but no correlation for adequacy. ˇ Stajner et al. (2014) explored the correlation with human judgments of six automatic metrics: cosine similarity with a bag-of-words representation, METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011) and two sub-components of TINE: T-BLEU (a variant of BLEU which uses lower n-grams when no 4grams are found) and SRL (based on semantic role labeling). Using 280 pairs of a source sentence and a simplified output with only structural modifications, they found positive correlations for all the metrics except TERp with respect to meaning preservation and positive albeit lower correlations for METEOR, T-BLEU and TINE with respect to grammaticality. Human simplicity judgments were not considered in this experiment. In this paper we collect human judgments for grammaticality, meaning preservation"
N18-1063,W12-3129,0,0.0214234,"d focuses only on structural aspects of simplicity. Section 2 presents previous work. Section 3 discusses UCCA. Section 4 presents SAMSA. Section 5 details the collection of human judgments. Our experimental setup for comparing our human and automatic rankings is given in Section 6, and results are given in Section 7, showing superior results for SAMSA. A discussion on the results is presented in Section 8. 686 uses linguistic annotation only on the source side, with semantic structures instead of syntactic ones. Semantic structures were used in MT evaluation, for example in the MEANT metric (Lo et al., 2012), which compares the output and the reference sentences, both annotated using SRL (Semantic Role Labeling). Lo et al. (2014) proposes the XMEANT variant, which compares the SRL structures of the source and output (without using references). As some frequent constructions like nominal argument structures are not addressed by the SRL annotation, Birch et al. (2016) proposed HUME, a human evaluation metric based on UCCA, using the semantic annotation only on the source side when comparing it to the output. We differ from HUME in proposing an automatic metric, tackling monolingual text simplificat"
N18-1063,E14-1076,0,0.505425,"ation.1 1 Introduction Text simplification (TS) addresses the translation of an input sentence into one or more simpler sentences. It is a useful preprocessing step for several NLP tasks, such as machine translation (Chandrasekar et al., 1996; Mishra et al., 2014) and relation extraction (Niklaus et al., 2016), and has also been shown useful in the development of reading aids, e.g., for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). The task has attracted much attention in the past decade (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014; Narayan and Gardent, 2014), but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly correlates with human judgments. This is in part due to the difficulty to combine the effects of different simplification operations 2 We do not consider other structural operations, such as passive to active transformations (Canning, 2002), that are currently not treated by corpus-based simplification systems. 1 All data and code are available in https://github. com/eliorsulem/SAMSA. 685 Proceedings of NAACL-HLT 2018, pages 685–696 c New Orleans,"
N18-1063,W02-0109,0,0.0283945,"the corresponding system outputs. Experiments are conducted in two settings: (1) a semi-automatic setting where Each input-output pair was rated by all five annotators. Other questions appeared without any example. 690 UCCA annotation was carried out manually by a single expert UCCA annotator using the UCCAApp annotation software (Abend et al., 2017), and according to the standard annotation guidelines;8 (2) an automatic setting where the UCCA annotation was carried out by the TUPA parser (Hershcovich et al., 2017). Sentence segmentation of the outputs was carried out using the NLTK package (Loper and Bird, 2002). For word alignments, we used the aligner of Sultan et al. (2014).9 7 tural simplicity, it scores somewhat higher than the semi-automatic SAMSA. The highest correlation with structural simplicity is obtained by the number of sentences with splitting, where SAMSA (automatic and semi-automatic) is second and third highest, although when restricted to multiScene sentences, the correlation for SAMSA (semi-automatic) is higher (0.89, p = 0.009 and 0.77, p = 0.04). The highest correlation for meaning preservation is obtained by SAMSAabl which provides further evidence that the retainment of semanti"
N18-1063,W09-0441,0,0.0446774,"he test set (Wubben et al., 2012; Narayan and Gardent, 2014, 2016). The most commonly used automatic measure for TS is BLEU (Papineni et al., 2002). Using 20 source sentences from the PWKP test corpus with 5 simplified sentences for each of them, Wubben et al. (2012) investigated the correlation of BLEU with human evaluation, reporting positive correlation for simplicity, but no correlation for adequacy. ˇ Stajner et al. (2014) explored the correlation with human judgments of six automatic metrics: cosine similarity with a bag-of-words representation, METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011) and two sub-components of TINE: T-BLEU (a variant of BLEU which uses lower n-grams when no 4grams are found) and SRL (based on semantic role labeling). Using 280 pairs of a source sentence and a simplified output with only structural modifications, they found positive correlations for all the metrics except TERp with respect to meaning preservation and positive albeit lower correlations for METEOR, T-BLEU and TINE with respect to grammaticality. Human simplicity judgments were not considered in this experiment. In this paper we collect human judgments for grammatical"
N18-1063,W14-5603,0,0.247926,"less automatic evaluation procedure, avoiding the problems that reference-based methods face due to the vast space of valid simplifications for a given sentence. Our human evaluation experiments show both SAMSA’s substantial correlation with human judgments, as well as the deficiency of existing reference-based measures in evaluating structural simplification.1 1 Introduction Text simplification (TS) addresses the translation of an input sentence into one or more simpler sentences. It is a useful preprocessing step for several NLP tasks, such as machine translation (Chandrasekar et al., 1996; Mishra et al., 2014) and relation extraction (Niklaus et al., 2016), and has also been shown useful in the development of reading aids, e.g., for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). The task has attracted much attention in the past decade (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014; Narayan and Gardent, 2014), but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly correlates with human judgments. This is in part due to the difficulty to combine the"
N18-1063,P14-1041,0,0.455115,"Missing"
N18-1063,Q14-1018,0,0.0288912,"settings: (1) a semi-automatic setting where Each input-output pair was rated by all five annotators. Other questions appeared without any example. 690 UCCA annotation was carried out manually by a single expert UCCA annotator using the UCCAApp annotation software (Abend et al., 2017), and according to the standard annotation guidelines;8 (2) an automatic setting where the UCCA annotation was carried out by the TUPA parser (Hershcovich et al., 2017). Sentence segmentation of the outputs was carried out using the NLTK package (Loper and Bird, 2002). For word alignments, we used the aligner of Sultan et al. (2014).9 7 tural simplicity, it scores somewhat higher than the semi-automatic SAMSA. The highest correlation with structural simplicity is obtained by the number of sentences with splitting, where SAMSA (automatic and semi-automatic) is second and third highest, although when restricted to multiScene sentences, the correlation for SAMSA (semi-automatic) is higher (0.89, p = 0.009 and 0.77, p = 0.04). The highest correlation for meaning preservation is obtained by SAMSAabl which provides further evidence that the retainment of semantic structures is a strong predictor of meaning preservation (Sulem"
N18-1063,W16-6620,0,0.357547,"six recent simplification systems for these sentences:5 (1) TSM (Zhu et al., 2010) using Tree-Based SMT, (2) RevILP (Woodsend and Lapata, 2011) using Quasi-Synchronous Grammars, (3) PBMT-R (Wubben et al., 2012) using PhraseBased SMT, (4) Hybrid (Narayan and Gardent, 4 In some cases, the unit u can be a sequence of centers (if there are several minimal centers). In these cases, 1s (u) returns 1 iff the condition holds for all centers. 5 All the data can be found here: http: //homepages.inf.ed.ac.uk/snaraya2/data/ simplification-2016.tgz. 689 5.2 2014), a supervised system using DRS, (5) UNSUP (Narayan and Gardent, 2016), an unsupervised system using DRS, and (6) Split-Deletion (Narayan and Gardent, 2016), the unsupervised system with only structural operations. All these systems explicitly address at least one type of structural simplification operation. The last system, Split-Deletion, performs only structural (i.e., no lexical) operations. It is thus an interesting test case for SAMSA since here the aligner can be replaced by a simple match between identical words. In total we obtain 600 system outputs from the six systems, as well as 100 sentences from the simple Wikipedia side of the corpus, which serve"
N18-1063,P12-2008,0,0.306004,"Missing"
N18-1063,D16-1033,0,0.121306,"on and not its quality, no structural measures were previously proposed for the evaluation of structural simplification. The need for such a measure is pressing, given recent interest in structural simplification, e.g., in the Split and Rephrase task (Narayan et al., 2017), which focuses on sentence splitting. In the task of sentence compression, which is similar to simplification in that they both involve deletion and paraphrasing, Clarke and Lapata (2006) showed that a metric that uses syntactic dependencies better correlates with human evaluation than a metric based on surface sub-strings. Toutanova et al. (2016) found that structure-aware metrics obtain higher correlation with human evaluation over bigram-based metrics, in particular with grammaticality judgments, but that they do not significantly outperform bigram-based metrics on any parameter. Both Clarke and Lapata (2006) and Toutanova et al. (2016) use reference-based metrics that use syntactic structure on both the output and the references. SAMSA on the other hand 687 head of the unit. For example, “dogs” is the center of the expression “big brown dogs” and “box” is the center of “in the box”. There could be more than one Center in a non-Scen"
N18-1063,C16-2036,0,0.119144,"the problems that reference-based methods face due to the vast space of valid simplifications for a given sentence. Our human evaluation experiments show both SAMSA’s substantial correlation with human judgments, as well as the deficiency of existing reference-based measures in evaluating structural simplification.1 1 Introduction Text simplification (TS) addresses the translation of an input sentence into one or more simpler sentences. It is a useful preprocessing step for several NLP tasks, such as machine translation (Chandrasekar et al., 1996; Mishra et al., 2014) and relation extraction (Niklaus et al., 2016), and has also been shown useful in the development of reading aids, e.g., for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). The task has attracted much attention in the past decade (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014; Narayan and Gardent, 2014), but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly correlates with human judgments. This is in part due to the difficulty to combine the effects of different simplification operations"
N18-1063,P15-2135,0,0.172626,", the negative Levenshtein distance from the source sentence, and the number of input sentences split by each of the systems. Levenshtein distances are taken as negative in order to capture similarity between the output and source/reference. The measure with the highest correlation in each row is boldfaced. ˇ and Stajner, 2013)13 (the test corpus contains 54 pairs from this dataset), (2) EncBrit: original sentences from the Encyclopedia Britannica (Barzilay and Elhadad, 2003) and their automatic simplifications obtained using ATS systems based on several ˇ phrase-based statistical MT systems (Stajner et al., 2015) trained on Wikipedia TS corpus (Coster and Kauchak, 2011) (24 pairs), and (3) LSLight: sentences from English Wikipedia and their autoˇ matic simplifications (Glavaˇs and Stajner, 2015) by three different lexical simplification systems (Biran et al., 2011; Horn et al., 2014; Glavaˇs and ˇ Stajner, 2015) (48 pairs). Human evaluation is also provided by this resource, with scores for overall quality, grammaticality, meaning preservation and simplicity. Importantly, the simplicity score does not explicitly refer to the output’s structural simplicity, but rather to its readability. We focus on th"
N18-1063,W14-1201,0,0.486272,"grammaticality (or fluency), meaning preservation (or adequacy) and simplicity. Human evaluation is usually carried out with a small number of sentences (18 to 20), randomly selected from the test set (Wubben et al., 2012; Narayan and Gardent, 2014, 2016). The most commonly used automatic measure for TS is BLEU (Papineni et al., 2002). Using 20 source sentences from the PWKP test corpus with 5 simplified sentences for each of them, Wubben et al. (2012) investigated the correlation of BLEU with human evaluation, reporting positive correlation for simplicity, but no correlation for adequacy. ˇ Stajner et al. (2014) explored the correlation with human judgments of six automatic metrics: cosine similarity with a bag-of-words representation, METEOR (Denkowski and Lavie, 2011), TERp (Snover et al., 2009), TINE (Rios et al., 2011) and two sub-components of TINE: T-BLEU (a variant of BLEU which uses lower n-grams when no 4grams are found) and SRL (based on semantic role labeling). Using 280 pairs of a source sentence and a simplified output with only structural modifications, they found positive correlations for all the metrics except TERp with respect to meaning preservation and positive albeit lower correla"
N18-1063,D11-1038,0,0.807813,"ased measures in evaluating structural simplification.1 1 Introduction Text simplification (TS) addresses the translation of an input sentence into one or more simpler sentences. It is a useful preprocessing step for several NLP tasks, such as machine translation (Chandrasekar et al., 1996; Mishra et al., 2014) and relation extraction (Niklaus et al., 2016), and has also been shown useful in the development of reading aids, e.g., for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). The task has attracted much attention in the past decade (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014; Narayan and Gardent, 2014), but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly correlates with human judgments. This is in part due to the difficulty to combine the effects of different simplification operations 2 We do not consider other structural operations, such as passive to active transformations (Canning, 2002), that are currently not treated by corpus-based simplification systems. 1 All data and code are available in https://github. com/eliorsulem/SAMSA. 685 Proceed"
N18-1063,P12-1107,0,0.426619,"Missing"
N18-1063,Q15-1021,0,0.637778,"Canning, 2002), that are currently not treated by corpus-based simplification systems. 1 All data and code are available in https://github. com/eliorsulem/SAMSA. 685 Proceedings of NAACL-HLT 2018, pages 685–696 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Section 9 presents experiments with SAMSA on the QATS evaluation benchmark. the right granularity. Third, defining a semantic measure that does not require references avoids the difficulties incurred by their non-uniqueness, and the difficulty in collecting high quality references, as reported by Xu et al. (2015) and by Narayan and Gardent (2014) with respect to the Parallel Wikipedia Corpus (PWKP; Zhu et al., 2010). SAMSA is further motivated by its use of semantic annotation only on the source side, which allows to evaluate multiple systems using same source-side annotation, and avoids the need to parse system outputs, which can be garbled. 2 Related Work Evaluation Metrics for Text Simplification. As pointed out by Xu et al. (2016), many of the existing measures for TS evaluation do not generalize across systems, because they fail to capture the combined effects of the different simplification oper"
N18-1063,Q16-1029,0,0.517901,"asure that does not require references avoids the difficulties incurred by their non-uniqueness, and the difficulty in collecting high quality references, as reported by Xu et al. (2015) and by Narayan and Gardent (2014) with respect to the Parallel Wikipedia Corpus (PWKP; Zhu et al., 2010). SAMSA is further motivated by its use of semantic annotation only on the source side, which allows to evaluate multiple systems using same source-side annotation, and avoids the need to parse system outputs, which can be garbled. 2 Related Work Evaluation Metrics for Text Simplification. As pointed out by Xu et al. (2016), many of the existing measures for TS evaluation do not generalize across systems, because they fail to capture the combined effects of the different simplification operations. The two main directions pursued are direct human judgments and automatic measures borrowed from machine translation (MT) evaluation. Human judgments generally include grammaticality (or fluency), meaning preservation (or adequacy) and simplicity. Human evaluation is usually carried out with a small number of sentences (18 to 20), randomly selected from the test set (Wubben et al., 2012; Narayan and Gardent, 2014, 2016)"
N18-1063,C10-1152,0,0.710055,"isting reference-based measures in evaluating structural simplification.1 1 Introduction Text simplification (TS) addresses the translation of an input sentence into one or more simpler sentences. It is a useful preprocessing step for several NLP tasks, such as machine translation (Chandrasekar et al., 1996; Mishra et al., 2014) and relation extraction (Niklaus et al., 2016), and has also been shown useful in the development of reading aids, e.g., for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). The task has attracted much attention in the past decade (Zhu et al., 2010; Woodsend and Lapata, 2011; Wubben et al., 2012; Siddharthan and Angrosh, 2014; Narayan and Gardent, 2014), but has yet to converge on an evaluation protocol that yields comparable results across different methods and strongly correlates with human judgments. This is in part due to the difficulty to combine the effects of different simplification operations 2 We do not consider other structural operations, such as passive to active transformations (Canning, 2002), that are currently not treated by corpus-based simplification systems. 1 All data and code are available in https://github. com/el"
N18-2020,P18-1127,1,0.734337,"ve to variation in the semantic dimensions which it encodes. In UCCA’s case, these distinctions focus on predicate-argument structures, the inter-relations between them, and the semantic heads of complex arguments. These distinctions are widely regarded as fundamental in the NLP and linguistic literature. In order to empirically validate this claim, we present an experiment which shows that corrections of a fairly low quality indeed receive a much lower US IM faithfulness score. Current state-of-the-art systems rarely alter the source sentences enough to yield semantically unfaithful outputs (Choshen and Abend, 2018b). Consequently, their human rankings are not determined by their semantic faithfulness, rendering them unuseful for validating US IM. We instead experiment with 5 partially trained correctors, trained and evaluated on the JFLEG corpus (Napoles et al., 2017) by Sakaguchi et al. (2017). US IM is computed automatically for each system’s output on 754 source sentences. Low faithfulness results are expected, as these outputs include major changes, sometimes deleting full phrases from the output or changing every other word. Indeed, automatic US IM obtains scores of 0.32-0.39 for 4 of the systems,"
N18-2020,P18-1059,1,0.794294,"ve to variation in the semantic dimensions which it encodes. In UCCA’s case, these distinctions focus on predicate-argument structures, the inter-relations between them, and the semantic heads of complex arguments. These distinctions are widely regarded as fundamental in the NLP and linguistic literature. In order to empirically validate this claim, we present an experiment which shows that corrections of a fairly low quality indeed receive a much lower US IM faithfulness score. Current state-of-the-art systems rarely alter the source sentences enough to yield semantically unfaithful outputs (Choshen and Abend, 2018b). Consequently, their human rankings are not determined by their semantic faithfulness, rendering them unuseful for validating US IM. We instead experiment with 5 partially trained correctors, trained and evaluated on the JFLEG corpus (Napoles et al., 2017) by Sakaguchi et al. (2017). US IM is computed automatically for each system’s output on 754 source sentences. Low faithfulness results are expected, as these outputs include major changes, sometimes deleting full phrases from the output or changing every other word. Indeed, automatic US IM obtains scores of 0.32-0.39 for 4 of the systems,"
N18-2020,P13-1023,1,0.935052,"omplements the RLM proposed by Napoles et al. (2016), which uses grammatical error detection techniques to assess the grammaticality of the output, and the work of Asano et al. (2017), who advocate the use of RLMs for fluency, grammaticality and meaning preservation, but state that a meaning preservation measure for GEC is currently lacking. A similar decomposition of output quality to its adequacy (similar to faithfulness) and fluency (related to grammaticality), has been used in machine translation (MT) evaluation (e.g., Banchs et al., 2015). As a test case, we use the UCCA semantic scheme (Abend and Rappoport, 2013), motivated by its recent use in semantic evaluation of MT (Birch et al., 2016) and text simplification (Sulem et al., 2018) systems. Nevertheless, US IM can be easily adapted to other semantic schemes, such as AMR (Banarescu et al., 2013). US IM is conceptually related to RLMs developed for MT (Reeder, 2006; Albrecht and Hwa, 2007; Specia et al., 2009, 2010). Notably, XMEANT (Lo et al., 2014) compares the source to the output in terms of their semantic role labeling structures. Our use of UCCA is motivated by its wider coverage of predicate types, as opposed to MEANT’s focus on verbal predica"
N18-2020,P07-1111,0,0.0240285,"cking. A similar decomposition of output quality to its adequacy (similar to faithfulness) and fluency (related to grammaticality), has been used in machine translation (MT) evaluation (e.g., Banchs et al., 2015). As a test case, we use the UCCA semantic scheme (Abend and Rappoport, 2013), motivated by its recent use in semantic evaluation of MT (Birch et al., 2016) and text simplification (Sulem et al., 2018) systems. Nevertheless, US IM can be easily adapted to other semantic schemes, such as AMR (Banarescu et al., 2013). US IM is conceptually related to RLMs developed for MT (Reeder, 2006; Albrecht and Hwa, 2007; Specia et al., 2009, 2010). Notably, XMEANT (Lo et al., 2014) compares the source to the output in terms of their semantic role labeling structures. Our use of UCCA is motivated by its wider coverage of predicate types, as opposed to MEANT’s focus on verbal predicates, and UCCA’s preservation of structure across translations (Sulem et al., 2015). See (Birch et al., 2016) for further discussion. We conduct experiments to confirm US IM’s validity. Specifically, we show that (1) UCCA can be consistently and automatically applied to learner language (LL) (§4.2), (2) US IM is not prone to unduly"
N18-2020,N12-1067,0,0.102395,"btain a high US IM similarity score to the source; and (3) invalid corrections obtain a lower score.1 1 Introduction Evaluation in Monolingual Translation, and particularly in Grammatical Error Correction (GEC) is a challenging research field, much due to the difficulty in integrating different types of rewriting operations into a single measure, and the vast number of valid outputs (Tetreault and Chodorow, 2008; Madnani et al., 2011; Chodorow et al., 2012; Bryant and Ng, 2015). These difficulties have recently motivated a number of proposals for new, improved reference-based measures (RBMs) (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015). Nevertheless, the size and heterogeneity of the space of valid outputs per sentence often prohibits obtaining a reference set that covers this space well, thereby limiting the applicability of RBMs (Bryant and Ng, 2015). To address this we propose a semantic RLM, US IM, that operates by measuring the graph distance between the semantic representations of the source and the output. Reliable RLMs are appealing both in not relying on references, which are costly to collect, and in avoiding the biases incurred by selecting references that necessar"
N18-2020,I17-2058,0,0.169398,"Missing"
N18-2020,W13-1703,0,0.0986633,"mparable to the parser’s reported performance (0.73 in-domain, 0.68 out-of-domain), despite not performing any domain adaptation to LL. That is, the UCCA parses of the source and the correction are roughly as similar to each other as they are to their gold standard parse. This supports the hypothesis that semantic parsThe Faithfulness of Valid Corrections. We obtain an IAA DAG F -score of 0.845 (Precision 0.834, Recall 0.857), which is comparable to the IAA reported for English Wikipedia texts by Abend and Rappoport (2013). As another point of comparison, we doubly annotate 3 corrected NUCLE (Dahlmeier et al., 2013) passages, obtaining a similar IAA. These results suggest that UCCA annotating LL does not degrade IAA: it can be applied as consistently to LL as to standard English. Table 1 (left-hand side) presents the US IM scores obtained by comparing the NUCLE references and the 2 http://www.cs.huji.ac.il/~oabend/ ucca.html 126 4.5 ing technology is sufficiently mature to be applicable to US IM. Results also suggest an improvement in parsing performance may further improve these scores. 4.4 Sensitivity to Unfaithfulness. We have shown that UCCA is insensitive to differences between a source sentence and"
N18-2020,W13-2322,0,0.142468,"and meaning preservation, but state that a meaning preservation measure for GEC is currently lacking. A similar decomposition of output quality to its adequacy (similar to faithfulness) and fluency (related to grammaticality), has been used in machine translation (MT) evaluation (e.g., Banchs et al., 2015). As a test case, we use the UCCA semantic scheme (Abend and Rappoport, 2013), motivated by its recent use in semantic evaluation of MT (Birch et al., 2016) and text simplification (Sulem et al., 2018) systems. Nevertheless, US IM can be easily adapted to other semantic schemes, such as AMR (Banarescu et al., 2013). US IM is conceptually related to RLMs developed for MT (Reeder, 2006; Albrecht and Hwa, 2007; Specia et al., 2009, 2010). Notably, XMEANT (Lo et al., 2014) compares the source to the output in terms of their semantic role labeling structures. Our use of UCCA is motivated by its wider coverage of predicate types, as opposed to MEANT’s focus on verbal predicates, and UCCA’s preservation of structure across translations (Sulem et al., 2015). See (Birch et al., 2016) for further discussion. We conduct experiments to confirm US IM’s validity. Specifically, we show that (1) UCCA can be consistentl"
N18-2020,N15-1060,0,0.0939724,"arity score to the source; and (3) invalid corrections obtain a lower score.1 1 Introduction Evaluation in Monolingual Translation, and particularly in Grammatical Error Correction (GEC) is a challenging research field, much due to the difficulty in integrating different types of rewriting operations into a single measure, and the vast number of valid outputs (Tetreault and Chodorow, 2008; Madnani et al., 2011; Chodorow et al., 2012; Bryant and Ng, 2015). These difficulties have recently motivated a number of proposals for new, improved reference-based measures (RBMs) (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015). Nevertheless, the size and heterogeneity of the space of valid outputs per sentence often prohibits obtaining a reference set that covers this space well, thereby limiting the applicability of RBMs (Bryant and Ng, 2015). To address this we propose a semantic RLM, US IM, that operates by measuring the graph distance between the semantic representations of the source and the output. Reliable RLMs are appealing both in not relying on references, which are costly to collect, and in avoiding the biases incurred by selecting references that necessarily cannot exhaust the vas"
N18-2020,foster-2004-parsing,0,0.0719403,"ssociation for Computational Linguistics Syntactic schemes for LL annotate syntactically erroneous sentences in different ways. Berzak et al. (2016) and Ragheb and Dickinson (2012) annotate according to the syntax used by the learner, even if this use is not grammatical. Such annotation may be unreliable for measuring faithfulness, as GEC systems aim to alter these erroneous syntactic structures. Nagata and Sakaguchi (2016) take the opposite approach, and remain faithful to the syntax intended by the learner. This has also been the tradition in works on parser robustness (Bigert et al., 2005; Foster, 2004). However, such approach is prone to inconsistencies due to the variety of different syntactic structures that can be used to express a similar meaning. In this paper, we use semantic annotation to structurally represent LL. Semantic structures are faithful to the intended meaning, and not to the formal realization, and thus face fewer conflicts where the syntactic structure used diverges from the one intended. We are not aware of any previous attempts to semantically annotate LL text. A,4 A,1 He gve C,6 R,Empty an apple He gave John P,2 A,1 for john an apple E,5 A,3 C,3 C,6 P A H R C E proces"
N18-2020,P17-1104,1,0.530338,"by Sulem et al. on English-French translations; similarity is comparable to ours. We conduct four types of experiments to validate US IM, showing that: (1) semantic annotation can be consistently applied to LL through inter-annotator agreement (IAA) experiments; (2) a valid corrector scores high on US IM; (3) an automatic UCCA parser can reliably replace human annotation for US IM; (4) US IM is sensitive to changes in meaning. 4.1 s→r 0.85 0.92 0.85 - 4.3 Automatic US IM. We experiment with an automatic variant of US IM, where UCCA structures are parsed automatically. We use the TUPA parser (Hershcovich et al., 2017) to generate UCCA structures, instead of the human annotators. Otherwise the setup is as above. TUPA is used with its biLSTM model, trained on the UCCA English Wikipedia corpus. We obtain a US IM score of 0.7 between the parses of the reference correction and the source, which is comparable to the parser’s reported performance (0.73 in-domain, 0.68 out-of-domain), despite not performing any domain adaptation to LL. That is, the UCCA parses of the source and the correction are roughly as similar to each other as they are to their gold standard parse. This supports the hypothesis that semantic p"
N18-2020,P16-1070,0,0.0211548,"tic theories propose that each learner makes consistent use of syntax (Huebner, 1985; Tarone, 1983), this use may not conform to the syntax of the learned language, or of any other known language. This entails difficulties in defining syntactic annotation for LL, as the annotated syntax differs between learners. 1 Our code is available in https://github.com/ borgr/USim. 124 Proceedings of NAACL-HLT 2018, pages 124–129 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Syntactic schemes for LL annotate syntactically erroneous sentences in different ways. Berzak et al. (2016) and Ragheb and Dickinson (2012) annotate according to the syntax used by the learner, even if this use is not grammatical. Such annotation may be unreliable for measuring faithfulness, as GEC systems aim to alter these erroneous syntactic structures. Nagata and Sakaguchi (2016) take the opposite approach, and remain faithful to the syntax intended by the learner. This has also been the tradition in works on parser robustness (Bigert et al., 2005; Foster, 2004). However, such approach is prone to inconsistencies due to the variety of different syntactic structures that can be used to express a"
N18-2020,W15-3502,1,0.742471,"Missing"
N18-2020,P14-2124,0,0.0565658,"ilar to faithfulness) and fluency (related to grammaticality), has been used in machine translation (MT) evaluation (e.g., Banchs et al., 2015). As a test case, we use the UCCA semantic scheme (Abend and Rappoport, 2013), motivated by its recent use in semantic evaluation of MT (Birch et al., 2016) and text simplification (Sulem et al., 2018) systems. Nevertheless, US IM can be easily adapted to other semantic schemes, such as AMR (Banarescu et al., 2013). US IM is conceptually related to RLMs developed for MT (Reeder, 2006; Albrecht and Hwa, 2007; Specia et al., 2009, 2010). Notably, XMEANT (Lo et al., 2014) compares the source to the output in terms of their semantic role labeling structures. Our use of UCCA is motivated by its wider coverage of predicate types, as opposed to MEANT’s focus on verbal predicates, and UCCA’s preservation of structure across translations (Sulem et al., 2015). See (Birch et al., 2016) for further discussion. We conduct experiments to confirm US IM’s validity. Specifically, we show that (1) UCCA can be consistently and automatically applied to learner language (LL) (§4.2), (2) US IM is not prone to unduly penalize valid corrections (§4.2), and (3) US IM assigns a lowe"
N18-2020,P11-2089,0,0.0604932,"curated references. Our experiments establish the validity of US IM, by showing that (1) semantic annotation can be consistently applied to ungrammatical text; (2) valid corrections obtain a high US IM similarity score to the source; and (3) invalid corrections obtain a lower score.1 1 Introduction Evaluation in Monolingual Translation, and particularly in Grammatical Error Correction (GEC) is a challenging research field, much due to the difficulty in integrating different types of rewriting operations into a single measure, and the vast number of valid outputs (Tetreault and Chodorow, 2008; Madnani et al., 2011; Chodorow et al., 2012; Bryant and Ng, 2015). These difficulties have recently motivated a number of proposals for new, improved reference-based measures (RBMs) (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015). Nevertheless, the size and heterogeneity of the space of valid outputs per sentence often prohibits obtaining a reference set that covers this space well, thereby limiting the applicability of RBMs (Bryant and Ng, 2015). To address this we propose a semantic RLM, US IM, that operates by measuring the graph distance between the semantic representations of the sou"
N18-2020,W08-1205,0,0.112362,", without relying on manually-curated references. Our experiments establish the validity of US IM, by showing that (1) semantic annotation can be consistently applied to ungrammatical text; (2) valid corrections obtain a high US IM similarity score to the source; and (3) invalid corrections obtain a lower score.1 1 Introduction Evaluation in Monolingual Translation, and particularly in Grammatical Error Correction (GEC) is a challenging research field, much due to the difficulty in integrating different types of rewriting operations into a single measure, and the vast number of valid outputs (Tetreault and Chodorow, 2008; Madnani et al., 2011; Chodorow et al., 2012; Bryant and Ng, 2015). These difficulties have recently motivated a number of proposals for new, improved reference-based measures (RBMs) (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015). Nevertheless, the size and heterogeneity of the space of valid outputs per sentence often prohibits obtaining a reference set that covers this space well, thereby limiting the applicability of RBMs (Bryant and Ng, 2015). To address this we propose a semantic RLM, US IM, that operates by measuring the graph distance between the semantic repr"
N18-2020,P16-1173,0,0.0134441,"nnotated syntax differs between learners. 1 Our code is available in https://github.com/ borgr/USim. 124 Proceedings of NAACL-HLT 2018, pages 124–129 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Syntactic schemes for LL annotate syntactically erroneous sentences in different ways. Berzak et al. (2016) and Ragheb and Dickinson (2012) annotate according to the syntax used by the learner, even if this use is not grammatical. Such annotation may be unreliable for measuring faithfulness, as GEC systems aim to alter these erroneous syntactic structures. Nagata and Sakaguchi (2016) take the opposite approach, and remain faithful to the syntax intended by the learner. This has also been the tradition in works on parser robustness (Bigert et al., 2005; Foster, 2004). However, such approach is prone to inconsistencies due to the variety of different syntactic structures that can be used to express a similar meaning. In this paper, we use semantic annotation to structurally represent LL. Semantic structures are faithful to the intended meaning, and not to the formal realization, and thus face fewer conflicts where the syntactic structure used diverges from the one intended."
N18-2020,P15-2097,0,0.328652,"and (3) invalid corrections obtain a lower score.1 1 Introduction Evaluation in Monolingual Translation, and particularly in Grammatical Error Correction (GEC) is a challenging research field, much due to the difficulty in integrating different types of rewriting operations into a single measure, and the vast number of valid outputs (Tetreault and Chodorow, 2008; Madnani et al., 2011; Chodorow et al., 2012; Bryant and Ng, 2015). These difficulties have recently motivated a number of proposals for new, improved reference-based measures (RBMs) (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015). Nevertheless, the size and heterogeneity of the space of valid outputs per sentence often prohibits obtaining a reference set that covers this space well, thereby limiting the applicability of RBMs (Bryant and Ng, 2015). To address this we propose a semantic RLM, US IM, that operates by measuring the graph distance between the semantic representations of the source and the output. Reliable RLMs are appealing both in not relying on references, which are costly to collect, and in avoiding the biases incurred by selecting references that necessarily cannot exhaust the vast space of valid correc"
N18-2020,D16-1228,0,0.168416,"Missing"
N18-2020,E17-2037,0,0.0831829,"Missing"
N18-2020,C12-2094,0,0.0200029,"each learner makes consistent use of syntax (Huebner, 1985; Tarone, 1983), this use may not conform to the syntax of the learned language, or of any other known language. This entails difficulties in defining syntactic annotation for LL, as the annotated syntax differs between learners. 1 Our code is available in https://github.com/ borgr/USim. 124 Proceedings of NAACL-HLT 2018, pages 124–129 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Syntactic schemes for LL annotate syntactically erroneous sentences in different ways. Berzak et al. (2016) and Ragheb and Dickinson (2012) annotate according to the syntax used by the learner, even if this use is not grammatical. Such annotation may be unreliable for measuring faithfulness, as GEC systems aim to alter these erroneous syntactic structures. Nagata and Sakaguchi (2016) take the opposite approach, and remain faithful to the syntax intended by the learner. This has also been the tradition in works on parser robustness (Bigert et al., 2005; Foster, 2004). However, such approach is prone to inconsistencies due to the variety of different syntactic structures that can be used to express a similar meaning. In this paper,"
N18-2020,2006.amta-papers.20,0,0.0547205,"s currently lacking. A similar decomposition of output quality to its adequacy (similar to faithfulness) and fluency (related to grammaticality), has been used in machine translation (MT) evaluation (e.g., Banchs et al., 2015). As a test case, we use the UCCA semantic scheme (Abend and Rappoport, 2013), motivated by its recent use in semantic evaluation of MT (Birch et al., 2016) and text simplification (Sulem et al., 2018) systems. Nevertheless, US IM can be easily adapted to other semantic schemes, such as AMR (Banarescu et al., 2013). US IM is conceptually related to RLMs developed for MT (Reeder, 2006; Albrecht and Hwa, 2007; Specia et al., 2009, 2010). Notably, XMEANT (Lo et al., 2014) compares the source to the output in terms of their semantic role labeling structures. Our use of UCCA is motivated by its wider coverage of predicate types, as opposed to MEANT’s focus on verbal predicates, and UCCA’s preservation of structure across translations (Sulem et al., 2015). See (Birch et al., 2016) for further discussion. We conduct experiments to confirm US IM’s validity. Specifically, we show that (1) UCCA can be consistently and automatically applied to learner language (LL) (§4.2), (2) US IM"
N18-2020,I17-2062,0,0.0371814,"Missing"
N18-2020,2009.eamt-1.5,0,0.0247242,"sition of output quality to its adequacy (similar to faithfulness) and fluency (related to grammaticality), has been used in machine translation (MT) evaluation (e.g., Banchs et al., 2015). As a test case, we use the UCCA semantic scheme (Abend and Rappoport, 2013), motivated by its recent use in semantic evaluation of MT (Birch et al., 2016) and text simplification (Sulem et al., 2018) systems. Nevertheless, US IM can be easily adapted to other semantic schemes, such as AMR (Banarescu et al., 2013). US IM is conceptually related to RLMs developed for MT (Reeder, 2006; Albrecht and Hwa, 2007; Specia et al., 2009, 2010). Notably, XMEANT (Lo et al., 2014) compares the source to the output in terms of their semantic role labeling structures. Our use of UCCA is motivated by its wider coverage of predicate types, as opposed to MEANT’s focus on verbal predicates, and UCCA’s preservation of structure across translations (Sulem et al., 2015). See (Birch et al., 2016) for further discussion. We conduct experiments to confirm US IM’s validity. Specifically, we show that (1) UCCA can be consistently and automatically applied to learner language (LL) (§4.2), (2) US IM is not prone to unduly penalize valid correc"
N19-1047,P17-1104,1,0.942946,"eventive and relational nouns from concrete nouns may allow improving it even further. In the similar case of compounds, lexical resources for light verbs and idioms may increase performance. Experimental Setup Data. In addition to the UCCA EWT data (§3), we use the reviews section of the UD v2.3 English_EWT treebank (Nivre et al., 2018),12 annotated over the exact same sentences. We additionally use UDPipe v1.2 (Straka et al., 2016; Straka and Straková, 2017), trained on English_EWT,13 for feature extraction. We apply the extended converter to UD as before (§4.2). Parser. We train TUPA v1.3 (Hershcovich et al., 2017, 2018a) on the UCCA EWT data, with the standard train/development/test split. TUPA uses POS tags and syntactic dependencies as features. We experiment both with using gold UD for feature extraction, and with using UDPipe outputs. Evaluation by gold-standard UD. UCCA evaluation is generally carried out by considering a predicted unit as correct if there is a gold unit that matches it in terminal yield and labels. Precision, Recall and F-score (F1) are computed accordingly. For the fine-grained analysis, we split the goldstandard, predicted and matched UCCA units according to the labels of the"
N19-1047,P13-1023,1,0.892441,"formation encoded in syntax; (3) pointing at semantic distinctions unlikely to be resolved by syntax. The importance of such an empirical study is emphasized by the ongoing discussion as to what role syntax should play in semantic parsing, if any (Swayamdipta et al., 2018; Strubell et al., 2018; He et al., 2018; Cai et al., 2018). See §8. This paper aims to address this gap, focusing on content differences. As a test case, we compare relatively similar schemes (§2): the syntactic Universal Dependencies (UD; Nivre et al., 2016), and the semantic Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). We UCCA-annotate the entire web reviews section of the UD EWT corpus (§3), and develop a converter to assimilate UD and UCCA, which use formally different graphs (§4). We then align their nodes, and identify which UCCA categories match which UD relations, and which are unmatched. Most content differences are due to (§5): Syntactic analysis plays an important role in semantic parsing, but the nature of this role remains a topic of ongoing debate. The debate has been constrained by the scarcity of empirical comparative studies between syntactic and semantic schemes, which hinders the developme"
N19-1047,P18-1035,1,0.760525,"e phrase and the same element modifying the whole phrase (de Marneffe and Nivre, 2019). An example UD tree is given in Figure 2. UD relations will be written in typewriter font. To facilitate comparison between UCCA and UD, we first assimilate the graphs by abstracting away from formalism differences, obtaining a similar graph format for both schemes. We then match pairs of nodes in the converted UD and UCCA trees if they share all terminals in their yields. UD annotates bi-lexical dependency trees, while UCCA graphs contain non-terminal nodes. In §4.1, we outline the unified DAG converter by Hershcovich et al. (2018a,b),7 which we use to reach a common format. In §4.2, we describe a number of extensions to the converter, which abstract away from further non-content differences. head ca se t nc Basic Conversion Figure 3 presents the same tree from Figure 2 after conversion. The converter adds one pre-terminal per token, and attaches them according to the original dependency tree: traversing it from the root, for each head it creates a non-terminal parent with the edge label head, and adds the dependents as children of the created non-terminal. Relation subtypes are stripped, leaving only universal relatio"
N19-1047,P17-1008,1,0.913294,"Missing"
N19-1047,K18-2010,1,0.86131,"e phrase and the same element modifying the whole phrase (de Marneffe and Nivre, 2019). An example UD tree is given in Figure 2. UD relations will be written in typewriter font. To facilitate comparison between UCCA and UD, we first assimilate the graphs by abstracting away from formalism differences, obtaining a similar graph format for both schemes. We then match pairs of nodes in the converted UD and UCCA trees if they share all terminals in their yields. UD annotates bi-lexical dependency trees, while UCCA graphs contain non-terminal nodes. In §4.1, we outline the unified DAG converter by Hershcovich et al. (2018a,b),7 which we use to reach a common format. In §4.2, we describe a number of extensions to the converter, which abstract away from further non-content differences. head ca se t nc Basic Conversion Figure 3 presents the same tree from Figure 2 after conversion. The converter adds one pre-terminal per token, and attaches them according to the original dependency tree: traversing it from the root, for each head it creates a non-terminal parent with the edge label head, and adds the dependents as children of the created non-terminal. Relation subtypes are stripped, leaving only universal relatio"
N19-1047,W13-2322,0,0.0885814,"Missing"
N19-1047,basile-etal-2012-developing,0,0.0605929,"Missing"
N19-1047,L16-1630,0,0.0465303,"y consistent and coarse-grained treebank annotation. Formally, UD uses bi-lexical trees, with edge labels representing syntactic relations. One aspect of UD similar to UCCA is its preference of lexical (rather than functional) heads. For example, in auxiliary verb constructions (e.g., “is eating”), UD marks the lexical verb (eating) as the head, while other dependency schemes may select the auxiliary is instead. While the approaches are largely inter-translatable (Schwartz et al., 2012), lexical head schemes are more similar in form to semantic schemes, such as UCCA and semantic dependencies (Oepen et al., 2016). Being a dependency representation, UD is structurally underspecified in an important way: it is not possible in UD to mark the distinction between an element modifying the head of the phrase and the same element modifying the whole phrase (de Marneffe and Nivre, 2019). An example UD tree is given in Figure 2. UD relations will be written in typewriter font. To facilitate comparison between UCCA and UD, we first assimilate the graphs by abstracting away from formalism differences, obtaining a similar graph format for both schemes. We then match pairs of nodes in the converted UD and UCCA tree"
N19-1047,Q16-1010,0,0.0164383,"l number of instances of each UD relation; of them, matching UCCA units in gold-standard and in TUPA’s predictions; their intersection, with/without regard to categories. (c) Percentage of correctly categorized edges; for comparison, percentage of most frequent category (see Table 3). (d) Average number of words in corresponding terminal yields. quire a clause to convey the same meaning. The mapping would therefore be more direct using a semantic representation, and we would benefit from breaking the utterance into two Scenes. 8 les the transduction of syntactic structures into semantic ones. Reddy et al. (2016) proposed a rulebased method for converting UD to logical forms. Stanovsky et al. (2016) converted Stanford dependency trees into proposition structures (P ROP S), abstracting away from some syntactic detail. Related Work 9 The use of syntactic parsing as a proxy for semantic structure has a long tradition in NLP. Indeed, semantic parsers have leveraged syntax for output space pruning (Xue and Palmer, 2004), syntactic features (Gildea and Jurafsky, 2002; Hershcovich et al., 2017), joint modeling (Surdeanu et al., 2008; Hajiˇc et al., 2009), and multi-task learning (Swayamdipta et al., 2016, 20"
N19-1047,D16-1134,1,0.839883,"and Rappoport, 2017). A methodology for comparing syntactic and semantic treebanks can also support fine-grained error analysis of semantic parsers, as illustrated by Szubert et al. (2018) for AMR (Banarescu et al., 2013). To demonstrate the utility of our comparison methodology, we perform fine-grained error analysis on UCCA parsing, according to UD relations (§6). Results highlight challenges for current parsing technology, and expose cases where UCCA parsers may benefit from modeling syntactic structure more directly.2 2 A C D E F G H et al., 2018b), and text-to-text generation evaluation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). Formally, UCCA structures are directed acyclic graphs (DAGs) whose nodes (or units) correspond either to words, or to elements viewed as a single entity according to some semantic or cognitive consideration. Edges are labeled, indicating the role of a child in the relation the parent represents. Figure 1 shows a legend of UCCA abbreviations. A Scene is UCCA’s notion of an event or a frame, and is a description of a movement, an action or a state which persists in time. Every Scene contains one primary relation, which can be either a Process or a"
N19-1047,C18-1233,0,0.0405077,"Missing"
N19-1047,N18-2020,1,0.759095,". A methodology for comparing syntactic and semantic treebanks can also support fine-grained error analysis of semantic parsers, as illustrated by Szubert et al. (2018) for AMR (Banarescu et al., 2013). To demonstrate the utility of our comparison methodology, we perform fine-grained error analysis on UCCA parsing, according to UD relations (§6). Results highlight challenges for current parsing technology, and expose cases where UCCA parsers may benefit from modeling syntactic structure more directly.2 2 A C D E F G H et al., 2018b), and text-to-text generation evaluation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). Formally, UCCA structures are directed acyclic graphs (DAGs) whose nodes (or units) correspond either to words, or to elements viewed as a single entity according to some semantic or cognitive consideration. Edges are labeled, indicating the role of a child in the relation the parent represents. Figure 1 shows a legend of UCCA abbreviations. A Scene is UCCA’s notion of an event or a frame, and is a description of a movement, an action or a state which persists in time. Every Scene contains one primary relation, which can be either a Process or a State. Scenes may contai"
N19-1047,W14-2908,0,0.0212426,"Missing"
N19-1047,E17-1051,0,0.0603032,"Missing"
N19-1047,J02-3001,0,0.194352,"Missing"
N19-1047,L16-1376,0,0.0260166,"Parisg Figure 3: Converted UD tree. Non-terminals and head edges are introduced by the unified DAG converter. obl case moved movedg tog he John Johng ad Afterg graduation ,g he , nsubj graduation nsubj ad After punct he case obl obl root obl Comparison Methodology 6 Our data is available at https://github.com/ UniversalConceptualCognitiveAnnotation/ UCCA_English-EWT. 7 https://github.com/huji-nlp/semstr http://bit.ly/ucca_guidelines_v2 480 Reentrancies. Remote edges in UCCA enable reentrancy, forming a DAG together with primary edges. UD allows reentrancy when including enhanced dependencies (Schuster and Manning, 2016),8 which form (bi-lexical) graphs, representing phenomena such as predicate ellipsis (e.g., gapping), and shared arguments due to coordination, control, raising and relative clauses. UCCA is more inclusive in its use of remote edges, and accounts for the entire class of implicit arguments termed Constructional Null Instantiation in FrameNet (Ruppenhofer et al., 2016). For example, in “The Pentagon is bypassing official US intelligence channels [...] in order to create strife” (from EWT), remote edges mark Pentagon as a shared argument of bypassing and create. Another example is “if you call fo"
N19-1047,C12-1147,1,0.868281,"Missing"
N19-1047,P18-1192,0,0.0216691,"are still in many cases the backbone of text understanding systems. Such studies are essential for (1) determining whether and to what extent semantic methods should be adopted for text understanding applications; (2) defining better inductive biases for semantic parsers, and allowing better use of information encoded in syntax; (3) pointing at semantic distinctions unlikely to be resolved by syntax. The importance of such an empirical study is emphasized by the ongoing discussion as to what role syntax should play in semantic parsing, if any (Swayamdipta et al., 2018; Strubell et al., 2018; He et al., 2018; Cai et al., 2018). See §8. This paper aims to address this gap, focusing on content differences. As a test case, we compare relatively similar schemes (§2): the syntactic Universal Dependencies (UD; Nivre et al., 2016), and the semantic Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). We UCCA-annotate the entire web reviews section of the UD EWT corpus (§3), and develop a converter to assimilate UD and UCCA, which use formally different graphs (§4). We then align their nodes, and identify which UCCA categories match which UD relations, and which are unmatched. Mos"
N19-1047,L16-1680,0,0.0439899,"Missing"
N19-1047,K17-3009,0,0.0397875,"Missing"
N19-1047,D18-1548,0,0.0289628,"m syntactic ones, which are still in many cases the backbone of text understanding systems. Such studies are essential for (1) determining whether and to what extent semantic methods should be adopted for text understanding applications; (2) defining better inductive biases for semantic parsers, and allowing better use of information encoded in syntax; (3) pointing at semantic distinctions unlikely to be resolved by syntax. The importance of such an empirical study is emphasized by the ongoing discussion as to what role syntax should play in semantic parsing, if any (Swayamdipta et al., 2018; Strubell et al., 2018; He et al., 2018; Cai et al., 2018). See §8. This paper aims to address this gap, focusing on content differences. As a test case, we compare relatively similar schemes (§2): the syntactic Universal Dependencies (UD; Nivre et al., 2016), and the semantic Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). We UCCA-annotate the entire web reviews section of the UD EWT corpus (§3), and develop a converter to assimilate UD and UCCA, which use formally different graphs (§4). We then align their nodes, and identify which UCCA categories match which UD relations, and which a"
N19-1047,W15-3502,1,0.828791,"ty between UD and UCCA can be traced back to their shared design principles: both are designed to be applicable across languages and domains, to enable rapid annotation and to support text understanding applications. This section provides a brief introduction to each of the schemes, whereas the next sections discuss their content in further detail.3 UCCA is a semantic annotation scheme rooted in typological and cognitive linguistic theory. It aims to represent the main semantic phenomena in text, abstracting away from syntactic forms. Shown to be preserved remarkably well across translations (Sulem et al., 2015), it has been applied to improve text simplification (Sulem 1 This excludes cases of shared argumenthood, which are partially covered by enhanced UD. See §4.1. 2 Our conversion and analysis code is public available at https://github.com/danielhers/synsem. 3 See Supplementary Material for a definition of each category in both schemes, and their abbreviations. 4 Despite the similar terminology, UCCA Adverbials are not necessarily adverbs syntactically. 479 which allow for a unit to participate in several super-ordinate relations. See example in Figure 1. Primary edges form a tree, whereas remote"
N19-1047,P18-1016,1,0.657614,"ring syntactic and semantic treebanks can also support fine-grained error analysis of semantic parsers, as illustrated by Szubert et al. (2018) for AMR (Banarescu et al., 2013). To demonstrate the utility of our comparison methodology, we perform fine-grained error analysis on UCCA parsing, according to UD relations (§6). Results highlight challenges for current parsing technology, and expose cases where UCCA parsers may benefit from modeling syntactic structure more directly.2 2 A C D E F G H et al., 2018b), and text-to-text generation evaluation (Birch et al., 2016; Choshen and Abend, 2018; Sulem et al., 2018a). Formally, UCCA structures are directed acyclic graphs (DAGs) whose nodes (or units) correspond either to words, or to elements viewed as a single entity according to some semantic or cognitive consideration. Edges are labeled, indicating the role of a child in the relation the parent represents. Figure 1 shows a legend of UCCA abbreviations. A Scene is UCCA’s notion of an event or a frame, and is a description of a movement, an action or a state which persists in time. Every Scene contains one primary relation, which can be either a Process or a State. Scenes may contain any number of Part"
N19-1047,K16-1019,0,0.0337937,"Missing"
N19-1047,D18-1412,0,0.0332315,"ishes semantic schemes from syntactic ones, which are still in many cases the backbone of text understanding systems. Such studies are essential for (1) determining whether and to what extent semantic methods should be adopted for text understanding applications; (2) defining better inductive biases for semantic parsers, and allowing better use of information encoded in syntax; (3) pointing at semantic distinctions unlikely to be resolved by syntax. The importance of such an empirical study is emphasized by the ongoing discussion as to what role syntax should play in semantic parsing, if any (Swayamdipta et al., 2018; Strubell et al., 2018; He et al., 2018; Cai et al., 2018). See §8. This paper aims to address this gap, focusing on content differences. As a test case, we compare relatively similar schemes (§2): the syntactic Universal Dependencies (UD; Nivre et al., 2016), and the semantic Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). We UCCA-annotate the entire web reviews section of the UD EWT corpus (§3), and develop a converter to assimilate UD and UCCA, which use formally different graphs (§4). We then align their nodes, and identify which UCCA categories match which UD"
N19-1047,N18-1106,0,0.0257713,"Missing"
N19-1047,D16-1177,0,0.194381,"Missing"
N19-1047,W04-3212,0,0.0953409,"Missing"
N19-1047,W17-6944,0,0.0226913,"Missing"
N19-1047,W08-2121,0,\N,Missing
N19-1047,N15-1007,0,\N,Missing
P09-1004,P98-1013,0,0.349106,"Missing"
P09-1004,A97-1052,0,0.109374,"n carried out in the SRL community. Suggestions include posing SRL as a sequence labeling problem (M`arquez et al., 2005) or as an edge tagging problem in a dependency representation (Hacioglu, 2004). Punyakanok et al. (2008) provide a detailed comparison between the impact of using shallow vs. full constituency syntactic information in an English SRL system. Their results clearly demonstrate the advantage of using full annotation. The identification of arguments has also been carried out in the context of automatic subcategorization frame acquisition. Notable examples include (Manning, 1993; Briscoe and Carroll, 1997; Korhonen, 2002) who all used statistical hypothesis testing to filter a parser’s output for arguments, with the goal of compiling verb subcategorization lexicons. However, these works differ from ours as they attempt to characterize the behavior of a verb type, by collecting statistics from various instances of that verb, and not to determine which are the arguments of specific verb instances. The algorithm presented in this paper performs unsupervised clause detection as an intermediate step towards argument identification. Supervised clause detection was also tackled as a separate task, no"
P09-1004,burchardt-etal-2006-salsa,0,0.0836252,"Missing"
P09-1004,W04-2412,0,0.0805222,"Missing"
P09-1004,W05-0620,0,0.274152,"Missing"
P09-1004,E03-1009,0,0.0425319,"Missing"
P09-1004,P07-1071,0,0.0258833,"Missing"
P09-1004,palmer-etal-2008-pilot,0,0.00647986,"tation of the corpus to be annotated. The three works above are relevant but incomparable to our work, due to the extensive amount of supervision (namely, VerbNet and a rule-based or supervised syntactic system) they used, both in detecting the syntactic structure and in detecting the arguments. Work has been carried out in a few other languages besides English. Chinese has been studied in (Xue, 2008). Experiments on Catalan and Spanish were done in SemEval 2007 (M`arquez et al., 2007) with two participating systems. Attempts to compile corpora for German (Burdchardt et al., 2006) and Arabic (Diab et al., 2008) are also underway. The small number of languages for which extensive SRL annotated data exists reflects the considerable human effort required for such endeavors. Some SRL works have tried to use unannotated data to improve the performance of a base supervised model. Methods used include bootstrapping approaches (Gildea and Jurafsky, 2002; Kate and Mooney, 2007), where large unannotated corpora were tagged with SRL annotation, later to be used to retrain the SRL model. Another ap3 Algorithm In this section we describe our algorithm. It consists of two stages, each of which reduces the set of"
P09-1004,N06-2026,0,0.085771,"Missing"
P09-1004,J05-1004,0,0.746745,"Missing"
P09-1004,J02-3001,0,0.977533,"classifier to determine for each word whether it is inside, outside or in the beginning of an argument (Hacioglu and Ward, 2003). Other works have integrated argument classification and identification into one step (Collobert and Weston, 2007), while others went further and combined the former two along with parsing into a single model (Musillo Related Work The advance of machine learning based approaches in this field owes to the usage of large scale annotated corpora. English is the most stud29 proach used similarity measures either between verbs (Gordon and Swanson, 2007) or between nouns (Gildea and Jurafsky, 2002) to overcome lexical sparsity. These measures were estimated using statistics gathered from corpora augmenting the model’s training data, and were then utilized to generalize across similar verbs or similar arguments. Attempts to substitute full constituency parsing by other sources of syntactic information have been carried out in the SRL community. Suggestions include posing SRL as a sequence labeling problem (M`arquez et al., 2005) or as an edge tagging problem in a dependency representation (Hacioglu, 2004). Punyakanok et al. (2008) provide a detailed comparison between the impact of using"
P09-1004,P06-2038,0,0.0231847,"gorization lexicons. However, these works differ from ours as they attempt to characterize the behavior of a verb type, by collecting statistics from various instances of that verb, and not to determine which are the arguments of specific verb instances. The algorithm presented in this paper performs unsupervised clause detection as an intermediate step towards argument identification. Supervised clause detection was also tackled as a separate task, notably in the CoNLL 2001 shared task (Tjong Kim Sang and D`ejean, 2001). Clause information has been applied to accelerating a syntactic parser (Glaysher and Moldovan, 2006). and Merlo, 2006). Work on less supervised methods has been scarce. Swier and Stevenson (2004) and Swier and Stevenson (2005) presented the first model that does not use an SRL annotated corpus. However, they utilize the extensive verb lexicon VerbNet, which lists the possible argument structures allowable for each verb, and supervised syntactic tools. Using VerbNet along with the output of a rule-based chunker (in 2004) and a supervised syntactic parser (in 2005), they spot instances in the corpus that are very similar to the syntactic patterns listed in VerbNet. They then use these as seed"
P09-1004,J08-2006,0,0.717808,"Missing"
P09-1004,P07-1025,0,0.143726,"equential tagging of words, training an SVM classifier to determine for each word whether it is inside, outside or in the beginning of an argument (Hacioglu and Ward, 2003). Other works have integrated argument classification and identification into one step (Collobert and Weston, 2007), while others went further and combined the former two along with parsing into a single model (Musillo Related Work The advance of machine learning based approaches in this field owes to the usage of large scale annotated corpora. English is the most stud29 proach used similarity measures either between verbs (Gordon and Swanson, 2007) or between nouns (Gildea and Jurafsky, 2002) to overcome lexical sparsity. These measures were estimated using statistics gathered from corpora augmenting the model’s training data, and were then utilized to generalize across similar verbs or similar arguments. Attempts to substitute full constituency parsing by other sources of syntactic information have been carried out in the SRL community. Suggestions include posing SRL as a sequence labeling problem (M`arquez et al., 2005) or as an edge tagging problem in a dependency representation (Hacioglu, 2004). Punyakanok et al. (2008) provide a de"
P09-1004,W06-1601,0,0.77507,"hm requires thousands to dozens of thousands sentences annotated with POS tags, syntactic annotation and SRL annotation. Current algorithms show impressive results but only for languages and domains where plenty of annotated data is available, e.g., English newspaper texts (see Section 2). Results are markedly lower when testing is on a domain wider than the training one, even in English (see the WSJ-Brown results in (Pradhan et al., 2008)). Only a small number of works that do not require manually labeled SRL training data have been done (Swier and Stevenson, 2004; Swier and Stevenson, 2005; Grenager and Manning, 2006). These papers have replaced this data with the VerbNet (Kipper et al., 2000) lexical resource or a set of manually written rules and supervised parsers. A potential answer to the SRL training data bottleneck are unsupervised SRL models that require little to no manual effort for their training. Their output can be used either by itself, or as training material for modern supervised SRL algorithms. In this paper we present an algorithm for unsupervised argument identification. The only type of annotation required by our algorithm is POS tagThe task of Semantic Role Labeling (SRL) is often divi"
P09-1004,P07-1049,0,0.170148,"model. Methods used include bootstrapping approaches (Gildea and Jurafsky, 2002; Kate and Mooney, 2007), where large unannotated corpora were tagged with SRL annotation, later to be used to retrain the SRL model. Another ap3 Algorithm In this section we describe our algorithm. It consists of two stages, each of which reduces the set of argument candidates, which a-priori contains all consecutive sequences of words that do not contain the predicate in question. 3.1 Algorithm overview As pre-processing, we use an unsupervised parser that generates an unlabeled parse tree for each sen30 L tence (Seginer, 2007). This parser is unique in that it is able to induce a bracketing (unlabeled parsing) from raw text (without even using POS tags) achieving state-of-the-art results. Since our algorithm uses millions to tens of millions sentences, we must use very fast tools. The parser’s high speed (thousands of words per second) enables us to process these large amounts of data. L L DT NNS The materials L IN L L L VBP in DT NN reach each The only type of supervised annotation we use is POS tagging. We use the taggers MXPOST (Ratnaparkhi, 1996) for English and TreeTagger (Schmid, 1994) for Spanish, to obtain"
P09-1004,C04-1186,0,0.0108749,"asures either between verbs (Gordon and Swanson, 2007) or between nouns (Gildea and Jurafsky, 2002) to overcome lexical sparsity. These measures were estimated using statistics gathered from corpora augmenting the model’s training data, and were then utilized to generalize across similar verbs or similar arguments. Attempts to substitute full constituency parsing by other sources of syntactic information have been carried out in the SRL community. Suggestions include posing SRL as a sequence labeling problem (M`arquez et al., 2005) or as an edge tagging problem in a dependency representation (Hacioglu, 2004). Punyakanok et al. (2008) provide a detailed comparison between the impact of using shallow vs. full constituency syntactic information in an English SRL system. Their results clearly demonstrate the advantage of using full annotation. The identification of arguments has also been carried out in the context of automatic subcategorization frame acquisition. Notable examples include (Manning, 1993; Briscoe and Carroll, 1997; Korhonen, 2002) who all used statistical hypothesis testing to filter a parser’s output for arguments, with the goal of compiling verb subcategorization lexicons. However,"
P09-1004,P06-1072,0,0.0195592,"Missing"
P09-1004,N03-2009,0,0.0560056,"Missing"
P09-1004,N07-2021,0,0.0249603,"sh. Chinese has been studied in (Xue, 2008). Experiments on Catalan and Spanish were done in SemEval 2007 (M`arquez et al., 2007) with two participating systems. Attempts to compile corpora for German (Burdchardt et al., 2006) and Arabic (Diab et al., 2008) are also underway. The small number of languages for which extensive SRL annotated data exists reflects the considerable human effort required for such endeavors. Some SRL works have tried to use unannotated data to improve the performance of a base supervised model. Methods used include bootstrapping approaches (Gildea and Jurafsky, 2002; Kate and Mooney, 2007), where large unannotated corpora were tagged with SRL annotation, later to be used to retrain the SRL model. Another ap3 Algorithm In this section we describe our algorithm. It consists of two stages, each of which reduces the set of argument candidates, which a-priori contains all consecutive sequences of words that do not contain the predicate in question. 3.1 Algorithm overview As pre-processing, we use an unsupervised parser that generates an unlabeled parse tree for each sen30 L tence (Seginer, 2007). This parser is unique in that it is able to induce a bracketing (unlabeled parsing) fro"
P09-1004,H05-1111,0,0.159997,"cting statistics from various instances of that verb, and not to determine which are the arguments of specific verb instances. The algorithm presented in this paper performs unsupervised clause detection as an intermediate step towards argument identification. Supervised clause detection was also tackled as a separate task, notably in the CoNLL 2001 shared task (Tjong Kim Sang and D`ejean, 2001). Clause information has been applied to accelerating a syntactic parser (Glaysher and Moldovan, 2006). and Merlo, 2006). Work on less supervised methods has been scarce. Swier and Stevenson (2004) and Swier and Stevenson (2005) presented the first model that does not use an SRL annotated corpus. However, they utilize the extensive verb lexicon VerbNet, which lists the possible argument structures allowable for each verb, and supervised syntactic tools. Using VerbNet along with the output of a rule-based chunker (in 2004) and a supervised syntactic parser (in 2005), they spot instances in the corpus that are very similar to the syntactic patterns listed in VerbNet. They then use these as seed for a bootstrapping algorithm, which consequently identifies the verb arguments in the corpus and assigns their semantic roles"
P09-1004,W01-0708,0,0.0857682,"Missing"
P09-1004,W04-3212,0,0.0783785,"e form. WP. The constituent is preceded by a Wh-pronoun. That. The constituent is preceded by a “that” marked by an “IN” POS tag indicating that it is a subordinating conjunction. Argument identification. For each predicate in the corpus, its argument candidates are now defined to be the constituents contained in the minimal clause containing the predicate. However, these constituents may be (and are) nested within each other, violating a major restriction on SRL arguments. Hence we now prune our set, by keeping only the siblings of all of the verb’s ancestors, as is common in supervised SRL (Xue and Palmer, 2004). Spanish CQUE. The constituent is preceded by a word with the POS “CQUE” which denotes the word “que” as a conjunction. INT. The constituent is preceded by a word with the POS “INT” which denotes an interrogative pronoun. CSUB. The constituent is preceded by a word with one of the POSs “CSUBF”, “CSUBI” or “CSUBX”, which denote a subordinating conjunction. Figure 2: The set of lexico-syntactic patterns that mark clauses which were used by our model. 3.3 Using collocations We use the following observation to filter out some superfluous argument candidates: since the arguments of a predicate man"
P09-1004,J08-2004,0,0.0210271,"g a test corpus. However, ARGID was not the task of that work, which dealt solely with argument classification. ARGID was performed by manually-created rules, requiring a supervised or manual syntactic annotation of the corpus to be annotated. The three works above are relevant but incomparable to our work, due to the extensive amount of supervision (namely, VerbNet and a rule-based or supervised syntactic system) they used, both in detecting the syntactic structure and in detecting the arguments. Work has been carried out in a few other languages besides English. Chinese has been studied in (Xue, 2008). Experiments on Catalan and Spanish were done in SemEval 2007 (M`arquez et al., 2007) with two participating systems. Attempts to compile corpora for German (Burdchardt et al., 2006) and Arabic (Diab et al., 2008) are also underway. The small number of languages for which extensive SRL annotated data exists reflects the considerable human effort required for such endeavors. Some SRL works have tried to use unannotated data to improve the performance of a base supervised model. Methods used include bootstrapping approaches (Gildea and Jurafsky, 2002; Kate and Mooney, 2007), where large unannot"
P09-1004,W04-3213,0,\N,Missing
P09-1004,W96-0213,0,\N,Missing
P09-1004,J08-2001,0,\N,Missing
P09-1004,W05-0628,0,\N,Missing
P09-1004,J04-4004,0,\N,Missing
P09-1004,S07-1008,0,\N,Missing
P09-1004,N07-1070,0,\N,Missing
P09-1004,C98-1013,0,\N,Missing
P09-1004,P93-1032,0,\N,Missing
P09-1004,W06-2303,0,\N,Missing
P10-1024,P09-1004,1,0.94955,"to determine semantic compositionality, which is a highly challenging task. Few works addressed unsupervised SRL-related tasks. The setup of (Grenager and Manning, 2006), who presented a Bayesian Network model for argument classification, is perhaps closest to ours. Their work relied on a supervised parser and a rule-based argument identification (both during training and testing). Swier and Stevenson (2004, 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al., 2000) verb lexicon, in addition to supervised parses. Finally, Abend et al. (2009) tackled the argument identification task alone and did not perform argument classification of any sort. Subcategorization Acquisition. This task specifies for each predicate the number, type and order of obligatory arguments. Determining the allowable subcategorization frames for a given predicate necessarily involves separating its cores from its allowable adjuncts (which are not framed). Notable works in the field include (Briscoe and Carroll, 1997; Sarkar and Zeman, 2000; Korhonen, 2002). All these works used a parsed corpus in order to collect, for each predicate, a set of hypothesized su"
P10-1024,P98-1013,0,0.117176,"Missing"
P10-1024,J98-2002,0,0.0393524,"ich are the head words of the argument (see below). The head words themselves, once chosen, are represented by the lemma. We now compute the following measures. Selectional Preference (SP). Since the semantics of cores is more predicate dependent than the semantics of adjuncts, we expect arguments for which the predicate has a strong preference (in a specific slot) to be cores. Selectional preference induction is a wellestablished task in NLP. It aims to quantify the likelihood that a certain argument appears in a certain slot of a predicate. Several methods have been suggested (Resnik, 1996; Li and Abe, 1998; Schulte im Walde et al., 2008). We use the paradigm of (Erk, 2007). For a given predicate slot pair (p, s), we define its preference to the argument head h to be: X SP (p, s, h) = P r(h′ |p, s) · sim(h, h′ ) Predicate-Slot Collocation. Since cores are obligatory, when a predicate persistently appears with an argument in a certain slot, the arguments in this slot tends to be cores. This notion can be captured by the (predicate, slot) joint distribution. We use the Pointwise Mutual Information measure (PMI) to capture the slot and the predicate’s collocation tendency. Let p be a predicate and"
P10-1024,D09-1049,1,0.800923,"ttach to a given frame. For instance, while the ‘Getting’ 1 PropBank annotates modals and negation words as modifiers. Since these are not arguments in the common usage of the term, we exclude them from the discussion in this paper. 227 formulation models the core-adjunct distinction explicitly. Therefore, any CCG parser can be used as a core-adjunct classifier (Hockenmaier, 2003). supervised detection of multiword expressions (MWEs). An important MWE sub-class is that of phrasal verbs, which are also characterized by verb-preposition pairs (Li et al., 2003; Sporleder and Li, 2009) (see also (Boukobza and Rappoport, 2009)). Both tasks aim to determine semantic compositionality, which is a highly challenging task. Few works addressed unsupervised SRL-related tasks. The setup of (Grenager and Manning, 2006), who presented a Bayesian Network model for argument classification, is perhaps closest to ours. Their work relied on a supervised parser and a rule-based argument identification (both during training and testing). Swier and Stevenson (2004, 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al., 2000) verb lexicon, in addition to supervised"
P10-1024,P98-2127,0,0.208825,"wing measures: 1. Simple SP – a selectional preference measure defined to be P r(head|slot, predicate). 2. Vast Corpus SP – similar to ‘Simple SP’ but with a much larger corpus. It uses roughly 100M arguments which were extracted from the web-crawling based corpus of (Gabrilovich and Markovitch, 2005) and the British National Corpus (Burnard, 2000). 3. Thesaurus SP – a selectional preference measure which follows the paradigm of (Erk, 2007) (Section 3.3) and defines the similarity between two heads to be the Jaccard affinity between their two entries in Lin’s automatically compiled thesaurus (Lin, 1998)10 . 4. Pr(slot|predicate) – an alternative to the used predicate-slot collocation measure. 5. PMI(slot, head) – an alternative to the used argument-slot collocation measure. 6. Head Dependence – the entropy of the predicate distribution given the slot and the head (following (Merlo and Esteve Ferrer, 2006)): curate predictions in this context than Clark’s. The 39832 sentences of PropBank’s sections 2– 21 were used as a test set without bounding their lengths8 . Cores were defined to be any argument bearing the labels ‘A0’ – ‘A5’, ‘C-A0’ – ‘C-A5’ or ‘R-A0’ – ‘R-A5’. Adjuncts were defined to be"
P10-1024,W05-0620,0,0.0326707,"Missing"
P10-1024,E03-1009,0,0.241793,"field (Baldwin et al., 2009). 228 3 Algorithm (PSH) joint distribution. This section details the process of extracting samples from this joint distribution given a raw text corpus. We start by parsing the corpus using the Seginer parser (Seginer, 2007). This parser is unique in its ability to induce a bracketing (unlabeled parsing) from raw text (without even using POS tags) with strong results. Its high speed (thousands of words per second) allows us to use millions of sentences, a prohibitive number for other parsers. We continue by tagging the corpus using Clark’s unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abend et al., 2010)2 . The classes corresponding to prepositions and to verbs are manually selected from the induced clusters3 . A preposition is defined to be any word which is the first word of an argument and belongs to a prepositions cluster. A verb is any word belonging to a verb cluster. This manual selection requires only a minute, since the number of classes is very small (34 in our experiments). In addition, knowing what is considered a preposition is part of the task definition itself. Argument identification is hard even for supervised models"
P10-1024,J05-1004,0,0.440004,"Missing"
P10-1024,P07-1028,0,0.168001,"ves, once chosen, are represented by the lemma. We now compute the following measures. Selectional Preference (SP). Since the semantics of cores is more predicate dependent than the semantics of adjuncts, we expect arguments for which the predicate has a strong preference (in a specific slot) to be cores. Selectional preference induction is a wellestablished task in NLP. It aims to quantify the likelihood that a certain argument appears in a certain slot of a predicate. Several methods have been suggested (Resnik, 1996; Li and Abe, 1998; Schulte im Walde et al., 2008). We use the paradigm of (Erk, 2007). For a given predicate slot pair (p, s), we define its preference to the argument head h to be: X SP (p, s, h) = P r(h′ |p, s) · sim(h, h′ ) Predicate-Slot Collocation. Since cores are obligatory, when a predicate persistently appears with an argument in a certain slot, the arguments in this slot tends to be cores. This notion can be captured by the (predicate, slot) joint distribution. We use the Pointwise Mutual Information measure (PMI) to capture the slot and the predicate’s collocation tendency. Let p be a predicate and s a slot, then: P S(p, s) = P M I(p, s) = log = log N (p, s)Σp′ ,s′"
P10-1024,W10-2911,1,0.832945,"to tag each argument separately. Finally, most works address the task at the verb type level, trying to detect the allowable frames for each type. Consequently, the common evaluation focuses on the quality of the allowable frames acquired for each verb type, and not on the classification of specific arguments in a given corpus. Such a token level evaluation was conducted in a few works (Briscoe and Carroll, 1997; Sarkar and Zeman, 2000), but often with a small number of verbs or a small number of frames. A discussion of the differences between type and token level evaluation can be found in (Reichart et al., 2010). PP attachment. PP attachment is the task of determining whether a prepositional phrase which immediately follows a noun phrase attaches to the latter or to the preceding verb. This task’s relation to the core-adjunct distinction was addressed in several works. For instance, the results of (Hindle and Rooth, 1993) indicate that their PP attachment system works better for cores than for adjuncts. Merlo and Esteve Ferrer (2006) suggest a system that jointly tackles the PP attachment and the core-adjunct distinction tasks. Unlike in this work, their classifier requires extensive supervision incl"
P10-1024,W06-1601,0,0.209909,"ude them from the discussion in this paper. 227 formulation models the core-adjunct distinction explicitly. Therefore, any CCG parser can be used as a core-adjunct classifier (Hockenmaier, 2003). supervised detection of multiword expressions (MWEs). An important MWE sub-class is that of phrasal verbs, which are also characterized by verb-preposition pairs (Li et al., 2003; Sporleder and Li, 2009) (see also (Boukobza and Rappoport, 2009)). Both tasks aim to determine semantic compositionality, which is a highly challenging task. Few works addressed unsupervised SRL-related tasks. The setup of (Grenager and Manning, 2006), who presented a Bayesian Network model for argument classification, is perhaps closest to ours. Their work relied on a supervised parser and a rule-based argument identification (both during training and testing). Swier and Stevenson (2004, 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al., 2000) verb lexicon, in addition to supervised parses. Finally, Abend et al. (2009) tackled the argument identification task alone and did not perform argument classification of any sort. Subcategorization Acquisition. This task spec"
P10-1024,saint-dizier-2006-prepnet,0,0.0270763,"Missing"
P10-1024,J93-1005,0,0.205251,"a given corpus. Such a token level evaluation was conducted in a few works (Briscoe and Carroll, 1997; Sarkar and Zeman, 2000), but often with a small number of verbs or a small number of frames. A discussion of the differences between type and token level evaluation can be found in (Reichart et al., 2010). PP attachment. PP attachment is the task of determining whether a prepositional phrase which immediately follows a noun phrase attaches to the latter or to the preceding verb. This task’s relation to the core-adjunct distinction was addressed in several works. For instance, the results of (Hindle and Rooth, 1993) indicate that their PP attachment system works better for cores than for adjuncts. Merlo and Esteve Ferrer (2006) suggest a system that jointly tackles the PP attachment and the core-adjunct distinction tasks. Unlike in this work, their classifier requires extensive supervision including WordNet, language-specific features and a supervised parser. Their features are generally motivated by common linguistic considerations. Features found adaptable to a completely unsupervised scenario are used in this work as well. The core-adjunct distinction task was tackled in the context of child language"
P10-1024,C00-2100,0,0.0345506,"from us as their algorithm uses the VerbNet (Kipper et al., 2000) verb lexicon, in addition to supervised parses. Finally, Abend et al. (2009) tackled the argument identification task alone and did not perform argument classification of any sort. Subcategorization Acquisition. This task specifies for each predicate the number, type and order of obligatory arguments. Determining the allowable subcategorization frames for a given predicate necessarily involves separating its cores from its allowable adjuncts (which are not framed). Notable works in the field include (Briscoe and Carroll, 1997; Sarkar and Zeman, 2000; Korhonen, 2002). All these works used a parsed corpus in order to collect, for each predicate, a set of hypothesized subcategorization frames, to be filtered by hypothesis testing methods. This line of work differs from ours in a few aspects. First, all works use manual or supervised syntactic annotations, usually including a POS tagger. Second, the common approach to the task focuses on syntax and tries to identify the entire frame, rather than to tag each argument separately. Finally, most works address the task at the verb type level, trying to detect the allowable frames for each type. C"
P10-1024,P08-1057,0,0.019391,"argument (see below). The head words themselves, once chosen, are represented by the lemma. We now compute the following measures. Selectional Preference (SP). Since the semantics of cores is more predicate dependent than the semantics of adjuncts, we expect arguments for which the predicate has a strong preference (in a specific slot) to be cores. Selectional preference induction is a wellestablished task in NLP. It aims to quantify the likelihood that a certain argument appears in a certain slot of a predicate. Several methods have been suggested (Resnik, 1996; Li and Abe, 1998; Schulte im Walde et al., 2008). We use the paradigm of (Erk, 2007). For a given predicate slot pair (p, s), we define its preference to the argument head h to be: X SP (p, s, h) = P r(h′ |p, s) · sim(h, h′ ) Predicate-Slot Collocation. Since cores are obligatory, when a predicate persistently appears with an argument in a certain slot, the arguments in this slot tends to be cores. This notion can be captured by the (predicate, slot) joint distribution. We use the Pointwise Mutual Information measure (PMI) to capture the slot and the predicate’s collocation tendency. Let p be a predicate and s a slot, then: P S(p, s) = P M"
P10-1024,P07-1049,0,0.107502,"(Collins, 1999). In his Model 2, Collins modifies his parser to provide a coreadjunct prediction, thereby improving its performance. The Combinatory Categorial Grammar (CCG) The study of prepositions is a vibrant research area in NLP. A special issue of Computational Linguistics, which includes an extensive survey of related work, was recently devoted to the field (Baldwin et al., 2009). 228 3 Algorithm (PSH) joint distribution. This section details the process of extracting samples from this joint distribution given a raw text corpus. We start by parsing the corpus using the Seginer parser (Seginer, 2007). This parser is unique in its ability to induce a bracketing (unlabeled parsing) from raw text (without even using POS tags) with strong results. Its high speed (thousands of words per second) allows us to use millions of sentences, a prohibitive number for other parsers. We continue by tagging the corpus using Clark’s unsupervised POS tagger (Clark, 2003) and the unsupervised Prototype Tagger (Abend et al., 2010)2 . The classes corresponding to prepositions and to verbs are manually selected from the induced clusters3 . A preposition is defined to be any word which is the first word of an ar"
P10-1024,E09-1086,0,0.0241467,"any type of non-core argument to attach to a given frame. For instance, while the ‘Getting’ 1 PropBank annotates modals and negation words as modifiers. Since these are not arguments in the common usage of the term, we exclude them from the discussion in this paper. 227 formulation models the core-adjunct distinction explicitly. Therefore, any CCG parser can be used as a core-adjunct classifier (Hockenmaier, 2003). supervised detection of multiword expressions (MWEs). An important MWE sub-class is that of phrasal verbs, which are also characterized by verb-preposition pairs (Li et al., 2003; Sporleder and Li, 2009) (see also (Boukobza and Rappoport, 2009)). Both tasks aim to determine semantic compositionality, which is a highly challenging task. Few works addressed unsupervised SRL-related tasks. The setup of (Grenager and Manning, 2006), who presented a Bayesian Network model for argument classification, is perhaps closest to ours. Their work relied on a supervised parser and a rule-based argument identification (both during training and testing). Swier and Stevenson (2004, 2005), while addressing an unsupervised SRL task, greatly differ from us as their algorithm uses the VerbNet (Kipper et al., 2000"
P10-1024,J08-2002,0,0.0400788,"Missing"
P10-1024,W02-2033,0,0.0238468,"t their PP attachment system works better for cores than for adjuncts. Merlo and Esteve Ferrer (2006) suggest a system that jointly tackles the PP attachment and the core-adjunct distinction tasks. Unlike in this work, their classifier requires extensive supervision including WordNet, language-specific features and a supervised parser. Their features are generally motivated by common linguistic considerations. Features found adaptable to a completely unsupervised scenario are used in this work as well. The core-adjunct distinction task was tackled in the context of child language acquisition. Villavicencio (2002) developed a classifier based on preposition selection and frequency information for modeling the distinction for locative prepositional phrases. Her approach is not entirely corpus based, as it assumes the input sentences are given in a basic logical form. Syntactic Parsing. The core-adjunct distinction is included in many syntactic annotation schemes. Although the Penn Treebank does not explicitly annotate adjuncts and cores, a few works suggested mapping its annotation (including function tags) to core-adjunct labels. Such a mapping was presented in (Collins, 1999). In his Model 2, Collins"
P10-1024,W04-3212,0,0.0229873,"iments). In addition, knowing what is considered a preposition is part of the task definition itself. Argument identification is hard even for supervised models and is considerably more so for unsupervised ones (Abend et al., 2009). We therefore confine ourselves to sentences of length not greater than 10 (excluding punctuation) which contain a single verb. A sequence of words will be marked as an argument of the verb if it is a constituent that does not contain the verb (according to the unsupervised parse tree), whose parent is an ancestor of the verb. This follows the pruning heuristic of (Xue and Palmer, 2004) often used by SRL algorithms. The corpus is now tagged using an unsupervised POS tagger. Since the sentences in question are short, we consider every word which does not belong to a closed class cluster as a head word (an argument can have several head words). A closed class is a class of function words with relatively few word types, each of which is very frequent. Typical examples include determiners, prepositions and conjunctions. A class which is not closed is open. In this paper, we define closed classes to be clusters in which the ratio between the number of word tokens and the number o"
P10-1024,W04-3213,0,\N,Missing
P10-1024,W96-0213,0,\N,Missing
P10-1024,H05-1111,0,\N,Missing
P10-1024,J08-2005,0,\N,Missing
P10-1024,N07-1070,0,\N,Missing
P10-1024,J09-2001,0,\N,Missing
P10-1024,A97-1052,0,\N,Missing
P10-1024,J03-4003,0,\N,Missing
P10-1024,C98-1013,0,\N,Missing
P10-1024,P09-2019,0,\N,Missing
P10-1024,J06-3002,0,\N,Missing
P10-1024,P10-1132,1,\N,Missing
P10-1024,P03-1065,0,\N,Missing
P10-1024,C98-2122,0,\N,Missing
P10-1132,C04-1080,0,0.200831,"esults (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small numb"
P10-1132,P06-3002,0,0.135348,"omputational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tag"
P10-1132,J92-4003,0,0.310333,"Missing"
P10-1132,E03-1009,0,0.444192,"on the theory of prototypes (Taylor, 2003), which posits that some members in cognitive categories are more central than others. These practically define the category, while the membership of other elements is based on their association with the ∗ Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 2 Related Work Unsupervised and semi-supervised POS tagging have been tackled using a variety of methods. Sch¨utze (1995) applied latent semantic analysis. The best reported results (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005"
P10-1132,D07-1023,0,0.0562564,"Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation eff"
P10-1132,C04-1052,0,0.0150501,"0 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary prov"
P10-1132,D08-1036,0,0.806358,"formance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model"
P10-1132,J01-2001,0,0.345512,"gment as being a stem or an affix. It has been tested on several languages with strong results. Our work has several unique aspects. First, our clustering method discovers prototypes in a fully unsupervised manner, mapping the rest of the words according to their association with the prototypes. Second, we use a distributional representation which has been shown to be effective for unsupervised parsing (Seginer, 2007). Third, we use a morphological representation based on signatures, which are sets of affixes that represent a family of words sharing an inflectional or derivational morphology (Goldsmith, 2001). 3 Distributional Algorithm Our algorithm is given a plain text corpus and optionally a desired number of clusters k. Its output is a partitioning of words into clusters. The algorithm utilizes two representations, distributional and morphological. Although eventually the latter is used before the former, for clarity of presentation we begin by detailing the base distributional algorithm. In the next section we describe the morphological representation and its integration into the base algorithm. Overview. The algorithm consists of two main stages: landmark clusters discovery, and word mappin"
P10-1132,D07-1043,0,0.0843834,"accuracy such that no two induced clusters are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three different schemes in two languages. In addition, the two English schemes we compare with were designed to tag corpora contained in our training set, and have been widely and successfully used with these corpora by a large number of applications. Our algorithm was run with the exact same parameters on both languages: N = 100 (high"
P10-1132,P07-1094,0,0.788838,"lgorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use t"
P10-1132,E95-1020,0,0.340359,"Missing"
P10-1132,P09-1059,0,0.0187048,"words are subsequently mapped to these clusters. We utilize morphological and distributional representations computed in a fully unsupervised manner. We evaluate our algorithm on English and German, achieving the best reported results for this task. 1 Introduction Part-of-speech (POS) tagging is a fundamental NLP task, used by a wide variety of applications. However, there is no single standard POS tagging scheme, even for English. Schemes vary significantly across corpora and even more so across languages, creating difficulties in using POS tags across domains and for multi-lingual systems (Jiang et al., 2009). Automatic induction of POS tags from plain text can greatly alleviate this problem, as well as eliminate the efforts incurred by manual annotations. It is also a problem of great theoretical interest. Consequently, POS induction is a vibrant research area (see Section 2). In this paper we present an algorithm based on the theory of prototypes (Taylor, 2003), which posits that some members in cognitive categories are more central than others. These practically define the category, while the membership of other elements is based on their association with the ∗ Omri Abend is grateful to the Azr"
P10-1132,D07-1031,0,0.388703,"lark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstra"
P10-1132,P07-1049,0,0.547456,"vised POS Induction through Prototype Discovery Omri Abend1∗ Roi Reichart2 1 Ari Rappoport1 Institute of Computer Science, 2 ICNC Hebrew University of Jerusalem {omria01|roiri|arir}@cs.huji.ac.il Abstract central members. Our algorithm first clusters words based on a fine morphological representation. It then clusters the most frequent words, defining landmark clusters which constitute the cores of the categories. Finally, it maps the rest of the words to these categories. The last two stages utilize a distributional representation that has been shown to be effective for unsupervised parsing (Seginer, 2007). We evaluated the algorithm in both English and German, using four different mapping-based and information theoretic clustering evaluation measures. The results obtained are generally better than all existing POS induction algorithms. Section 2 reviews related work. Sections 3 and 4 detail the algorithm. Sections 5, 6 and 7 describe the evaluation, experimental setup and results. We present a novel fully unsupervised algorithm for POS induction from plain text, motivated by the cognitive notion of prototypes. The algorithm first identifies landmark clusters of words, serving as the cores of t"
P10-1132,P05-1044,0,0.156947,"given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding t"
P10-1132,D09-1072,0,0.35004,"to induce a small number of possible tags for each word. The idea of utilizing seeds and expanding them to less reliable data has been used in several papers. Haghighi and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation efforts, it does not induce a tagging scheme but remains tied to an existing one. It is further criticized in (Goldwater and Griffiths, 2007). Morphological representation. Many POS induction models utilize morphology"
P10-1132,J94-2001,0,0.0948809,"best reported results (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics techniques were proposed to improve the HMM model. A Bayesian approach was employed by (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Van Gael et al. (2009) used the infinite HMM with non-parametric priors. Grac¸a et al. (2009) biased the model"
P10-1132,P09-1057,0,0.290719,"and Klein (2006) use POS ‘prototypes’ that are manually provided and tailored to a particular POS tag set of a corpus. Freitag (2004) and Biemann (2006) induce an initial clustering and use it to train an HMM model. Dasgupta and Ng (2007) generate morphological clusters and use them to bootstrap a distributional model. Goldberg et al. (2008) use linguistic considerations for choosing a good starting point for the EM algorithm. Zhao and Marcus (2009) expand a partial dictionary and use it to learn disambiguation rules. Their evaluation is only at the type level and only for half of the words. Ravi and Knight (2009) use a dictionary and an MDLinspired modification to the EM algorithm. Many of these works use a dictionary providing allowable tags for each or some of the words. While this scenario might reduce human annotation efforts, it does not induce a tagging scheme but remains tied to an existing one. It is further criticized in (Goldwater and Griffiths, 2007). Morphological representation. Many POS induction models utilize morphology to some extent. Some use simplistic representations of terminal letter sequences (e.g., (Smith and Eisner, 2005; Haghighi and Klein, 2006)). Clark (2003) models the ent"
P10-1132,C08-1091,1,0.86954,"a derived accuracy. The Many-to-1 measure finds the mapping between the gold standard clusters and the induced clusters which maximizes accuracy, allowing several induced clusters to be mapped to the same gold standard cluster. The 1-to-1 measure finds the mapping between the induced and gold standard clusters which maximizes accuracy such that no two induced clusters are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three d"
P10-1132,W09-1121,1,0.696459,"rs are mapped to the same gold cluster. Computing this mapping is equivalent to finding the maximal weighted matching in a bipartite graph, whose weights are given by the intersection sizes between matched classes/clusters. As in (Reichart and Rappoport, 2008), we use the Kuhn-Munkres algorithm (Kuhn, 1955; Munkres, 1957) to solve this problem. Information theoretic measures. These are based on the observation that a good clustering reduces the uncertainty of the gold tag given the induced cluster, and vice-versa. Several such measures exist; we use V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009), VI’s (Meila, 2007) normalized version. 6 Experimental Setup Since a goal of unsupervised POS tagging is inducing an annotation scheme, comparison to an existing scheme is problematic. To address this problem we compare to three different schemes in two languages. In addition, the two English schemes we compare with were designed to tag corpora contained in our training set, and have been widely and successfully used with these corpora by a large number of applications. Our algorithm was run with the exact same parameters on both languages: N = 100 (high frequency threshold), n = 50 (the para"
P10-1132,W10-2911,1,0.823065,"nts is based on their association with the ∗ Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 2 Related Work Unsupervised and semi-supervised POS tagging have been tackled using a variety of methods. Sch¨utze (1995) applied latent semantic analysis. The best reported results (when taking into account all evaluation measures, see Section 5) are given by (Clark, 2003), which combines distributional and morphological information with the likelihood function of the Brown algorithm (Brown et al., 1992). Clark’s tagger is very sensitive to its initialization. Reichart et al. (2010b) propose a method to identify the high quality runs of this algorithm. In this paper, we show that our algorithm outperforms not only Clark’s mean performance, but often its best among 100 runs. Most research views the task as a sequential labeling problem, using HMMs (Merialdo, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005) and discriminative models (Smith and Eisner, 2005; Haghighi and Klein, 2006). Several 1298 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1298–1307, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computat"
P10-1132,W10-2909,1,0.563823,"Missing"
P10-1132,D09-1071,0,\N,Missing
P10-1132,N06-1041,0,\N,Missing
P10-1132,P08-1085,0,\N,Missing
P11-1067,N10-1083,0,0.0100955,"Missing"
P11-1067,P10-1131,0,0.00560222,"ing logistic normal priors. Cohen and Smith (2009, 2010) further extended it by using a shared logistic normal prior which provided a new way to encode the knowledge that some POS tags are more similar than others. A bilingual joint learning further improved their performance. Headden et al. (2009) obtained the best reported results on WSJ10 by using a lexical extension of DMV. Gillenwater et al. (2010) used posterior regularization to bias the training towards a small number of parent-child combinations. Berg-Kirkpatrick et al. (2010) added new features to the M step of the DMV EM procedure. Berg-Kirkpatrick and Klein (2010) used a phylogenetic tree to model parameter drift between different languages. Spitkovsky et al. (2010a) explored several training protocols for DMV. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ∞ . Druck et al. (2009) took a semi-super"
P11-1067,W04-1501,0,0.0673155,"in three, significantly different, gold standards currently in use. Coordination Structures are composed of two proper nouns, separated by a conjunctor (e.g., “John and Mary”). It is not clear which token should be the head of this structure, if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex"
P11-1067,D10-1117,0,0.186649,"These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 2004; Cohen et al., 2008; Headden et al., 2009; Spitkovsky et al., 2010a; Gillenwater et al., 2010; Berg-Kirkpatrick et al., 2010; Blunsom and Cohn, 2010, inter alia). Parser quality is usually evaluated by comparing its output to a gold standard whose annotations are linguistically motivated. However, there are cases in which there is no linguistic consensus as to what the correct annotation is (K¨ubler et al., 2009). Examples include which verb is the head in a verb group structure (e.g., “can” or “eat” in “can eat”), and which ∗ Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. noun is the head in a sequence of proper nouns (e.g., “John” or “Doe” in “John Doe”). We refer to such annotations as (linguis"
P11-1067,N09-1009,0,0.0289598,"blematic Annotations in Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of researc"
P11-1067,P09-1041,0,0.00977741,"re. Berg-Kirkpatrick and Klein (2010) used a phylogenetic tree to model parameter drift between different languages. Spitkovsky et al. (2010a) explored several training protocols for DMV. Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ∞ . Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent sta"
P11-1067,P10-2036,0,0.0356408,"Missing"
P11-1067,C08-1042,0,0.0740417,"Missing"
P11-1067,N09-1012,0,0.26345,"Missing"
P11-1067,W07-2416,0,0.0665149,"note that these controversies are reflected in the evaluation of this task, resulting in three, significantly different, gold standards currently in use. Coordination Structures are composed of two proper nouns, separated by a conjunctor (e.g., “John and Mary”). It is not clear which token should be the head of this structure, if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s sc"
P11-1067,P04-1061,0,0.712703,"lizing Linguistically Problematic Annotations in Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substa"
P11-1067,J93-2004,0,0.0435294,"Missing"
P11-1067,D10-1120,0,0.0191741,"Spitkovsky et al. (2010c) showed the benefits of Viterbi (“hard”) EM to DMV training. Spitkovsky et al. (2010b) presented a novel lightlysupervised approach that used hyper-text mark-up annotation of web-pages to train DMV. A few non-DMV-based works were recently presented. Daum´e III (2009) used shift-reduce techniques. Blunsom and Cohn (2010) used tree substitution grammar to achieve best results on WSJ∞ . Druck et al. (2009) took a semi-supervised approach, using a set of rules such as “A noun is usually the parent of a determiner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and ve"
P11-1067,nivre-etal-2006-maltparser,0,0.00840752,"applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and verb groups in the Prague TreeBank. They trained the supervised MaltParser (Nivre et al., 2006) on the transformed data, parsed the test data and re-transformed the resulting parse, w3 w2 (a) w1 w3 w2 w1 (b) Figure 1: A dependency structure on the words w1 , w2 , w3 before (Figure 1(a)) and after (Figure 1(b)) an edge-flip of w2 →w1 . thus improving performance. Klein and Manning (2004) observed that a large portion of their errors is caused by predicting the wrong direction of the edge between a noun and its determiner. K¨ubler (2005) compared two different conversion schemes in German supervised constituency parsing and found one to have positive influence on parsing quality. Dependen"
P11-1067,P06-1033,0,0.363717,"iner which is to its left”, experimenting on several languages. Naseem et al. (2010) further extended this idea by using a single set of rules which globally applies to six different languages. The latter used a model similar to DMV. The controversial nature of some dependency structures was discussed in (Nivre, 2006; K¨ubler et al., 2009). Klein (2005) discussed controversial constituency structures and the evaluation problems stemming from them, stressing the importance of a consistent standard of evaluation. A few works explored the effects of annotation conventions on parsing performance. Nilsson et al. (2006) transformed the dependency annotations of coordinations and verb groups in the Prague TreeBank. They trained the supervised MaltParser (Nivre et al., 2006) on the transformed data, parsed the test data and re-transformed the resulting parse, w3 w2 (a) w1 w3 w2 w1 (b) Figure 1: A dependency structure on the words w1 , w2 , w3 before (Figure 1(a)) and after (Figure 1(b)) an edge-flip of w2 →w1 . thus improving performance. Klein and Manning (2004) observed that a large portion of their errors is caused by predicting the wrong direction of the edge between a noun and its determiner. K¨ubler (200"
P11-1067,rambow-etal-2002-dependency,0,0.0689242,", if any (Nilsson et al., 2006). Prepositional Phrases (e.g., “in the house” or “in Rome”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex scheme (Dredze et al., 2007). 667 Evaluation Inconsistency Across Papers. A fact that may not be recognized by some readers is that comparing the results of unsupervised dependency parsers across different papers is not directly possib"
P11-1067,P06-1072,0,0.0773069,"Missing"
P11-1067,N10-1116,0,0.195398,"Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200"
P11-1067,P10-1130,0,0.232129,"Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200"
P11-1067,W10-2902,0,0.312419,"Unsupervised Dependency Parsing Evaluation Roy Schwartz1 Omri Abend1∗ Roi Reichart2 Ari Rappoport1 1 Institute of Computer Science Hebrew University of Jerusalem {roys02|omria01|arir}@cs.huji.ac.il 2 Computer Science and Artificial Intelligence Laboratory Massachusetts Institute of Technology roiri@csail.mit.edu Abstract Dependency parsing is a central NLP task. In this paper we show that the common evaluation for unsupervised dependency parsing is highly sensitive to problematic annotations. We show that for three leading unsupervised parsers (Klein and Manning, 2004; Cohen and Smith, 2009; Spitkovsky et al., 2010a), a small set of parameters can be found whose modification yields a significant improvement in standard evaluation measures. These parameters correspond to local cases where no linguistic consensus exists as to the proper gold annotation. Therefore, the standard evaluation does not provide a true indication of algorithm quality. We present a new measure, Neutral Edge Direction (NED), and show that it greatly reduces this undesired phenomenon. 1 Introduction Unsupervised induction of dependency parsers is a major NLP task that attracts a substantial amount of research (Klein and Manning, 200"
P11-1067,W05-1516,0,0.038097,"Missing"
P11-1067,W06-2904,0,0.0476608,"Missing"
P11-1067,W03-3023,0,0.776919,"”), where every word is a reasonable candidate to head this structure. For example, in the annotation scheme used by (Collins, 1999) the preposition is the head, in the scheme used by (Johansson and Nugues, 2007) the noun is the head, while TUT annotation, presented in (Bosco and Lombardo, 2004), takes the determiner to be the noun’s head. Verb Groups are composed of a verb and an auxiliary or a modal verb (e.g., “can eat”). Some schemes choose the modal as the head (Collins, 1999), others choose the verb (Rambow et al., 2002). Infinitive Verbs (e.g., “to eat”) are also in controversy, as in (Yamada and Matsumoto, 2003) the verb is the head while in (Collins, 1999; Bosco and Lombardo, 2004) the “to” token is the head. Sequences of Proper Nouns (e.g., “John Doe”) are also subject to debate, as PTB’s scheme takes the last proper noun as the head, and BIO’s scheme defines a more complex scheme (Dredze et al., 2007). 667 Evaluation Inconsistency Across Papers. A fact that may not be recognized by some readers is that comparing the results of unsupervised dependency parsers across different papers is not directly possible, since different papers use different gold standard annotations even when they are all deriv"
P11-1067,J03-4003,0,\N,Missing
P11-1067,D07-1112,0,\N,Missing
P11-1067,D07-1096,0,\N,Missing
P13-1023,W12-3602,0,0.0249551,"d language teaching) (Siddharthan, 2006), paraphrase detection (Dolan et al., 2004), summarization (Knight and Marcu, 2000), and question answering (Wang et al., 2007). 5 2009)), recent work has also successfully tackled the task of predicting semantic structures in the form of DAGs (Jones et al., 2012). The most prominent annotation scheme in NLP for English syntax is the Penn Treebank. Many syntactic schemes are built or derived from it. An increasingly popular alternative to the PTB are dependency structures, which are usually represented as trees whose nodes are the words of the sentence (Ivanova et al., 2012). Such representations are limited due to their inability to naturally represent constructions that have more than one head, or in which the identity of the head is not clear. They also face difficulties in representing units that participate in multiple relations. UCCA proposes a different formalism that addresses these problems by introducing a new node for every relation (cf. (Sangati and Mazza, 2009)). Related Work Several annotated corpora offer a joint syntactic and semantic representation. Examples include the Groningen Meaning bank (Basile et al., 2012), Treebank Semantics (Butler and"
P13-1023,W13-0101,1,0.811533,"d and determine its semantic type. There can be one or more Cs in a non-Scene unit4 . Other sub-units of non-Scene units are categorized into three types. First, units that apply to a single C are annotated as Elaborators (E). For instance, “big” in “big dogs” is an E, while “dogs” is a C. We also mark determiners as Es in this coarsegrained layer5 . Second, relations that relate two or 2 As UCCA annotates categories on its edges, Scene nodes bear no special indication. They can be identified by examining the labels on their outgoing edges (see below). 3 Repeated here with minor changes from (Abend and Rappoport, 2013), which focuses on the categories themselves. 4 By allowing several Cs we avoid the difficulties incurred by the common single head assumption. In some cases the Cs are inferred from context and can be implicit. 5 Several Es that apply to a single C are often placed in 230 more Cs, highlighting a common feature or role (usually coordination), are called Connectors (N). See an example in Figure 2(b). Relators (R) cover all other types of relations between two or more Cs. Rs appear in two main varieties. In one, Rs relate a single entity to a super-ordinate relation. For instance, in “I heard no"
P13-1023,C12-1083,0,0.0412168,"Missing"
P13-1023,J08-4004,0,0.0169918,"Missing"
P13-1023,P98-1013,0,0.064756,"by the structural pattern it appears in. Table 1: The complete set of categories in UCCA’s foundational layer. specifications. The definition of a Scene is motivated cross-linguistically and is similar to the semantic aspect of the definition of a “clause” in Basic Linguistic Theory2 . Table 1 provides a concise description of the categories used by the foundational layer3 . We turn to a brief description of them. Simple Scenes. Every Scene contains one main relation, which is the anchor of the Scene, the most important relation it describes (similar to frameevoking lexical units in FrameNet (Baker et al., 1998)). We distinguish between static Scenes, that describe a temporally persistent state, and processual Scenes that describe a temporally evolving event, usually a movement or an action. The main relation receives the category State (S) in static and Process (P) in processual Scenes. We note that the S-P distinction is introduced here mostly for practical purposes, and that both categories can be viewed as sub-categories of the more abstract category Main Relation. A Scene contains one or more Participants (A). This category subsumes concrete and abstract participants as well as embedded Scenes ("
P13-1023,basile-etal-2012-developing,0,0.0780182,"are the words of the sentence (Ivanova et al., 2012). Such representations are limited due to their inability to naturally represent constructions that have more than one head, or in which the identity of the head is not clear. They also face difficulties in representing units that participate in multiple relations. UCCA proposes a different formalism that addresses these problems by introducing a new node for every relation (cf. (Sangati and Mazza, 2009)). Related Work Several annotated corpora offer a joint syntactic and semantic representation. Examples include the Groningen Meaning bank (Basile et al., 2012), Treebank Semantics (Butler and Yoshimoto, 2012) and the Lingo Redwoods treebank (Oepen et al., 2004). UCCA diverges from these projects in aiming to abstract away from syntactic variation, and is therefore less coupled with a specific syntactic theory. In this section we compare UCCA to some of the major approaches to grammatical representation in NLP. We focus on English, which is the most studied language and the focus of this paper. Syntactic annotation schemes come in many forms, from lexical categories such as POS tags to intricate hierarchical structures. Some formalisms focus particul"
P13-1023,D10-1119,0,0.0565017,"ly represent semantic distinctions. Put differently, UCCA advocates an approach that treats syntax as a hidden layer when learning the mapping between form and meaning, while existing syntactic approaches aim to model it manually and explicitly. UCCA does not build on any other annotation layers and therefore implicitly assumes that semantic annotation can be learned directly. Recent work suggests that indeed structured prediction methods have reached sufficient maturity to allow direct learning of semantic distinctions. Examples include Naradowsky et al. (2012) for semantic role labeling and Kwiatkowski et al. (2010) for semantic parsing to logical forms. While structured prediction for the task of predicting tree structures is already well established (e.g., (Suzuki et al., A different strand of work addresses the construction of an interlingual representation, often with a motivation of applying it to machine translation. Examples include the UNL project (Uchida and Zhu, 2001), the IAMTC project (Dorr et al., 2010) and the AMR project (Banarescu et al., 2012). These projects share with UCCA their emphasis on cross-linguistically valid annotations, but diverge from UCCA in three important respects. First"
P13-1023,J93-2004,0,0.0680554,"ound, and describe the compilation of a UCCAannotated corpus. 1 Introduction Syntactic structures are mainly committed to representing the formal patterns of a language, and only indirectly reflect semantic distinctions. For instance, while virtually all syntactic annotation schemes are sensitive to the structural difference between (a) “John took a shower” and (b) “John showered”, they seldom distinguish between (a) and the markedly different (c) “John took my book”. In fact, the annotations of (a) and (c) are identical under the most widely-used schemes for English, the Penn Treebank (PTB) (Marcus et al., 1993) and CoNLL-style dependencies (Surdeanu et al., 2008) (see Figure 1). ∗ Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 228 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 228–238, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics and represent the descendant unit’s role in forming the semantics of the parent unit. Therefore, the internal structure of a unit is represented by its outbound edges and their categories, while the roles a unit plays in the relations it participat"
P13-1023,meyers-etal-2004-annotating,0,0.0538404,"Missing"
P13-1023,D12-1074,0,0.0193629,"to abstract away from specific syntactic forms and to only represent semantic distinctions. Put differently, UCCA advocates an approach that treats syntax as a hidden layer when learning the mapping between form and meaning, while existing syntactic approaches aim to model it manually and explicitly. UCCA does not build on any other annotation layers and therefore implicitly assumes that semantic annotation can be learned directly. Recent work suggests that indeed structured prediction methods have reached sufficient maturity to allow direct learning of semantic distinctions. Examples include Naradowsky et al. (2012) for semantic role labeling and Kwiatkowski et al. (2010) for semantic parsing to logical forms. While structured prediction for the task of predicting tree structures is already well established (e.g., (Suzuki et al., A different strand of work addresses the construction of an interlingual representation, often with a motivation of applying it to machine translation. Examples include the UNL project (Uchida and Zhu, 2001), the IAMTC project (Dorr et al., 2010) and the AMR project (Banarescu et al., 2012). These projects share with UCCA their emphasis on cross-linguistically valid annotations,"
P13-1023,C04-1051,0,0.010373,"g in the park”). The second contains a simple clause headed by “playing”. While the parse trees of these sentences are very different, their UCCA annotation in the foundational layer differ only in terms of Function units: “ChildrenA [areF playingC ]P [inR theE parkC ]A ” and “ThereF areF childrenA [playing]P [inR theE parkC ]A ”10 . Aside from machine translation, a great variety of semantic tasks can benefit from a scheme that is relatively insensitive to syntactic variation. Examples include text simplification (e.g., for second language teaching) (Siddharthan, 2006), paraphrase detection (Dolan et al., 2004), summarization (Knight and Marcu, 2000), and question answering (Wang et al., 2007). 5 2009)), recent work has also successfully tackled the task of predicting semantic structures in the form of DAGs (Jones et al., 2012). The most prominent annotation scheme in NLP for English syntax is the Penn Treebank. Many syntactic schemes are built or derived from it. An increasingly popular alternative to the PTB are dependency structures, which are usually represented as trees whose nodes are the words of the sentence (Ivanova et al., 2012). Such representations are limited due to their inability to n"
P13-1023,J05-1004,0,0.0450142,"Missing"
P13-1023,P11-1067,1,0.370397,"Missing"
P13-1023,W08-2121,0,0.028666,"Missing"
P13-1023,D09-1058,0,0.0606921,"Missing"
P13-1023,J11-4006,0,0.0206347,"Missing"
P13-1023,D07-1003,0,0.0118337,"arse trees of these sentences are very different, their UCCA annotation in the foundational layer differ only in terms of Function units: “ChildrenA [areF playingC ]P [inR theE parkC ]A ” and “ThereF areF childrenA [playing]P [inR theE parkC ]A ”10 . Aside from machine translation, a great variety of semantic tasks can benefit from a scheme that is relatively insensitive to syntactic variation. Examples include text simplification (e.g., for second language teaching) (Siddharthan, 2006), paraphrase detection (Dolan et al., 2004), summarization (Knight and Marcu, 2000), and question answering (Wang et al., 2007). 5 2009)), recent work has also successfully tackled the task of predicting semantic structures in the form of DAGs (Jones et al., 2012). The most prominent annotation scheme in NLP for English syntax is the Penn Treebank. Many syntactic schemes are built or derived from it. An increasingly popular alternative to the PTB are dependency structures, which are usually represented as trees whose nodes are the words of the sentence (Ivanova et al., 2012). Such representations are limited due to their inability to naturally represent constructions that have more than one head, or in which the ident"
P13-1023,C98-1013,0,\N,Missing
P14-1061,W03-1812,0,0.0281587,"to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs"
P14-1061,D10-1115,0,0.0291917,"nes of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 We propose a method for addressing MWPs of varying degrees of compositionality through the integration of the distributional representation of"
P14-1061,D13-1147,0,0.0157785,"ositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 We propose a method for addressing MWPs of varying degrees of compositionality through the integration of the distributional representation of multiple sub-sets of the predicate’s words (LCs). We use it to tackle a supervised prediction task that represents predicates distributionally. Our model assumes a latent distribution over the LCs, and estimates its parameters so to best conf"
P14-1061,P11-1062,0,0.424813,"difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of ar"
P14-1061,Q13-1015,1,0.929241,"between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 2008; Melamud et al., 2013b), or as part of a more comprehensive system (Berant et al., 2011; Lewis and Steedman, 2013). For example, consider the verb “take”. While the inference relation “have → take” does not generally hold, it does hold in the case of some light verbs, such as “have a look → take a look”, underscoring the importance of taking more inclusive LCs into account. On the other hand, the predicate “likely to give a green light” is unlikely to appear often even within a very large corpus, and could benefit from taking its lexical sub-units (e.g., “likely” or “give a green light”) into account. We present a novel approach to the task that models the selection and relative weighting of the predicate"
P14-1061,W11-1304,0,0.0207168,"fication of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositio"
P14-1061,P98-2127,0,0.0575403,"Missing"
P14-1061,P99-1041,0,0.0817136,"ultiple LCs to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly b"
P14-1061,W02-0109,0,0.0615685,"Missing"
P14-1061,W03-1810,0,0.0446691,"Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess"
P14-1061,P13-1131,0,0.299606,"applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the r"
P14-1061,C10-2029,0,0.0623497,"Missing"
P14-1061,P13-2051,0,0.0727332,"applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the r"
P14-1061,E09-1025,0,0.0154396,"icate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 200"
P14-1061,D11-1142,0,0.0351763,"This section lists the features used for our experiments. We intentionally select a feature set that relies on either completely unsupervised or shallow processing tools that are available for a wide variety of languages and domains. Given a predicate pair p(i) , a label y ∈ {1, −1} and a latent state h ∈ H (i) , we define their feature vector as Φ(p(i) , y, h) = y · Φ(p(i) , h). The computation of Φ(p(i) , h) requires a reference corpus R that contains triplets of the type (p, x, y) where p is a binary predicate and x and y are its arguments. We use the Reverb corpus as R in our experiments (Fader et al., 2011; see Section 4). We refrain from encoding features that directly reflect the vocabulary of the training set. Such features are not applicable beyond that set’s vocabulary, and as available datasets contain no more than a few thousand examples, these features are unlikely to generalize well. Table 1 presents the set of features we use in our experiments. The features can be divided into two main categories: similarity features between the LHS and the RHS predicates (table’s top), and features that reflect the individual properties of each ∇L = Eh [Φ(pi , yi , h)] − Eh,y [Φ(pi , y, h)] − λ · w"
P14-1061,P06-2075,0,0.0230173,"te inference rules with token-level information from their arguments in a specific context. Melamud et al. (2013b) used lexical expansion to improve the representation of infrequent predicates. Lewis and Steedman (2013) combined distributional and symbolic representations, evaluating on a Question Answering task, as well as on a quantification-focused entailment dataset. Several studies tackled the task using supervised systems. Weisman et al. (2012) used a set of linguistically motivated features, but evaluated their system on a corpus that consists almost entirely of single-word predicates. Mirkin et al. (2006) presented a system for learning inference rules between nouns, using distributional similarity and pattern-based features. Hagiwara et al. (2009) identified synonyms using a supervised approach relying on distributional and syntactic features. Berant et al. (2011) used distributional similarity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtained a more accurate set of inference rules. Previous work used simple methods to select the predicate’s LC. Some filtered out frequent highly ambiguous verbs (Lewis and"
P14-1061,E06-1043,0,0.0497327,"Missing"
P14-1061,D07-1110,0,0.0218242,"Missing"
P14-1061,D13-1060,0,0.0132378,"tt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWE"
P14-1061,P13-2046,0,0.0173758,"bject of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional"
P14-1061,P02-1006,0,0.00907062,"h addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it gener"
P14-1061,W03-1011,0,0.152252,"pic|hL ) for each of the induced topics. The entropy of the topic distribution P (topic|hL ) Table 1: The feature set used in our experiments. The top part presents the similarity measures based on the DIRT approach. The rest of the listed features apply to the LHS predicate (hL ), and to the first word in it (hA L ). Analogous features are A introduced for the second word, hB L , and for the RHS predicate. The upper-middle part presents the word features for hL . The lower-middle part presents features that apply where hL is of size 2. The bottom part lists the LDA-based features. (1998) and Weeds and Weir (2003) did not yield additional improvements. We encode the similarity of all measures for the A pair hL and hR as well as the pair hA L and hR . The latter feature is an approximation to the similarity between the heads of the predicates, as heads in English tend to be to the left of the predicates. These two features coincide for h values of size 1. Word and Pair Features. These features encode the basic properties of the LC. The motivation behind them is to allow a more accurate leveraging of the similarity features, as well as to better determine the relative weights of h ∈ H (i) . The feature s"
P14-1061,I11-1024,0,0.02046,"rm treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 We propose a method for addressing MWPs of varying degrees of compositionality through the integration of the distributional representation of multiple sub-sets of the predicate’s words (LCs). We use it to tackle a supervised prediction task that represents predicates distributionally. Our model assumes a latent distribution over the"
P14-1061,D12-1018,0,0.153469,"presented an unsupervised system for learning inference rules directly from open-domain web data. Melamud et al. (2013a) used topic models to combine typelevel predicate inference rules with token-level information from their arguments in a specific context. Melamud et al. (2013b) used lexical expansion to improve the representation of infrequent predicates. Lewis and Steedman (2013) combined distributional and symbolic representations, evaluating on a Question Answering task, as well as on a quantification-focused entailment dataset. Several studies tackled the task using supervised systems. Weisman et al. (2012) used a set of linguistically motivated features, but evaluated their system on a corpus that consists almost entirely of single-word predicates. Mirkin et al. (2006) presented a system for learning inference rules between nouns, using distributional similarity and pattern-based features. Hagiwara et al. (2009) identified synonyms using a supervised approach relying on distributional and syntactic features. Berant et al. (2011) used distributional similarity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtaine"
P14-1061,P10-1044,0,0.0605228,"Missing"
P14-1061,P06-1107,0,0.0264756,"to define and model, and consequently pose serious difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation"
P14-1061,D10-1106,0,0.510611,"nd consequently pose serious difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally hold"
P14-1061,P12-2031,0,0.683025,"selection and relative weighting of the predicate’s LCs using latent variables. This approach allows the classifier that uses the distributional representations to take into account the most relevant LCs in order to make the prediction. By doing so, we avoid the notoriously difficult problem of defining and identifying MWPs and account for predicates of various sizes and degrees of compositionality. To our knowledge, this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics. We conduct experiments on the dataset of Zeichner et al. (2012) and compare our methods with analogous ones that select a fixed LC, using stateof-the-art feature sets. Our method obtains substantial performance gains across all scenarios. Finally, we note that our approach is cognitively appealing. Significant cognitive findings support the claim that a speaker’s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (200"
P14-1061,I05-5011,0,0.0293266,"main difficult to define and model, and consequently pose serious difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said t"
P14-1061,N06-1039,0,0.0171201,"paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distri"
P14-1061,C08-1107,0,0.405935,"ms (Dinu and Wang, 2009; Dagan et al., 2013). challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation pL → pR is said to hold if the relation denoted by pR generally holds between a set of arguments whenever the relation pL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 2008; Melamud et al., 2013b), or as part of a more comprehensive system (Berant et al., 2011; Lewis and Steedman, 2013). For example, consider the verb “take”. While the inference relation “have → take” does not generally hold, it does hold in the case of some light verbs, such as “have a look → take a look”, underscoring the importance of taking more inclusive LCs into account. On the other hand, the predicate “likely to give a green light” is unlikely to appear often even within a very large corpus, and could benefit from taking its lexical sub-units (e.g., “likely” or “give a green light”) into"
P14-1061,W11-0807,0,0.0965207,"it, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used composi"
P14-1061,P06-4018,0,\N,Missing
P14-1061,P10-1045,0,\N,Missing
P14-1061,C98-2122,0,\N,Missing
P17-1008,P16-1231,0,0.0157234,"G and G IVING, but D EPARTING may also be evoked by “depart” and “exit”, and G IVING by “donate” and “gift”. A different approach to SRT is taken by Vector Space Models (VSM), which eschew the use of symbolic structures, instead modeling all linguistic elements as vectors, from the level of words to phrases and sentences. Proponents of this approach generally invoke neural network methods, obtaining impressive results on a variety of tasks including lexical tasks such as cross-linguistic word similarity (Ammar et al., 2016), machine translation (Bahdanau et al., 2015), and dependency parsing (Andor et al., 2016). VSMs are also attractive in being flexible enough to model non-local and gradient phenomena (e.g., Socher et al., 2013). However, more research is needed to clarify the scope of semantic phenomena that such models are able to reliably capture. We therefore only lightly touch on VSMs in this survey. Finally, a major consideration in semantic analysis, and one of its great potential advantages, is its cross-linguistic universality. While languages differ in terms of their form (e.g., in their phonology, lexicon, and syntax), they have often been as2 We use the term “Text Semantics”, rather tha"
P17-1008,D14-1059,0,0.00455111,"retical linguistics (Levin and Hovav, 2005) and in NLP, through approaches such as Semantic Role Labeling (SRL; Gildea and Jurafsky, 2002), formal semantic analysis (e.g., Bos, 2008), and Abstract Meaning Representation (AMR; Banarescu et al., 2013). Many other useful meaning components have been proposed, and are discussed at a greater depth in Section 3. 3 Another approach to defining an SRT is through external (extra-textual) criteria or applications. For instance, a semantic representation can be defined to support inference, as in textual entailment (Dagan et al., 2006) or natural logic (Angeli and Manning, 2014). Other examples include defining a semantic representation in terms of supporting knowledge base querying (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005), or defining semantics through a different modality, for instance interpreting text in terms of images that correspond to it (Kiros et al., 2014), or in terms of embodied motor and perceptual schemas (Feldman et al., 2010). Semantic Content We turn to discussing the main content types encoded by semantic representation schemes. Due to space limitations, we focus only on text semantics, which studies the meaning relationships between"
P17-1008,Q13-1005,0,0.0440543,"ections and motions, and their relative configuration. Logical Structure. Logical structure, including quantification, negation, coordination and their associated scope distinctions, is the cornerstone of semantic analysis in much of theoretical linguistics, and has attracted much attention in NLP as well. Common representations are often based on variants of predicate calculus, and are useful for applications that require mapping text into an external, often executable, formal language, such as a querying language (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005) or robot instructions (Artzi and Zettlemoyer, 2013). Logical structures are also useful for recognizing entailment relations between sentences, as some entailments can be computed from the text’s logical structure by formal provers (Bos and Markert, 2005; Lewis and Steedman, 2013). Discourse Relations encompass any semantic relation between events or larger semantic units. For example, in (1) the leaving and the giving events are sometimes related through a discourse relation of type C ONCESSION, evoked by “although”. Such information is useful, often essential for a variety of NLP tasks such as summarization, machine translation and informati"
P17-1008,W13-2322,0,0.580068,"(e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field. 1 Introduction Schemes for Semantic Representation of Text (SRT) aim to reflect the meaning of sentences and texts in a transparent way. There has recently been an influx of proposals for semantic representations and corpora, e.g. GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013b) and Universal Decompositional Semantics (UDS; White et al., 2016). Nevertheless, no detailed assessment of the relative merits of the different schemes has been carried out, nor their comparison to previous sentential analysis schemes, notably syntactic ones. An understanding of the achievements and gaps of semantic analysis in NLP is crucial to its future prospects. In this paper we begin to chart the various proposals for semantic schemes according to the content they support. As not many semantic queries on texts can at present be answered with near human"
P17-1008,P08-1090,0,0.0150874,"6, pp. 23-24). Temporal Relations. Most temporal semantic work in NLP has focused on temporal relations between events, either by timestamping them according to time expressions found in the text, or by predicting their relative order in time. Important resources include TimeML, a specification language for temporal relations (Pustejovsky et al., 2003), and the TempEval series of shared tasks and annotated corpora (Verhagen et al., 2009, 2010; UzZaman et al., 2013). A different line of work explores scripts: schematic, temporally ordered sequences of events associated with a certain scenario (Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010). For instance, going to a restaurant includes sitting at a table, ordering, eating and paying, generally in this order. Related to temporal relations, are causal relations between events, which are ubiquitous in language, and central for a variety of applications, Semantic Roles. Semantic roles are categories of arguments. Many different semantic role inventories have been proposed and used in NLP over the years, the most prominent being FrameNet (where roles are shared across predicates that 79 including planning and entailment. See (Mirza et al., 2014) and (Duni"
P17-1008,P09-1068,0,0.0359219,"Missing"
P17-1008,basile-etal-2012-developing,0,0.0126281,"posals for semantic schemes (e.g., AMR, UCCA, GMB, UDS) have been put forth. Yet, little has been done to assess the achievements and the shortcomings of these new contenders, compare them with syntactic schemes, and clarify the general goals of research on semantic representation. We address these gaps by critically surveying the state of the art in the field. 1 Introduction Schemes for Semantic Representation of Text (SRT) aim to reflect the meaning of sentences and texts in a transparent way. There has recently been an influx of proposals for semantic representations and corpora, e.g. GMB (Basile et al., 2012), AMR (Banarescu et al., 2013), UCCA (Abend and Rappoport, 2013b) and Universal Decompositional Semantics (UDS; White et al., 2016). Nevertheless, no detailed assessment of the relative merits of the different schemes has been carried out, nor their comparison to previous sentential analysis schemes, notably syntactic ones. An understanding of the achievements and gaps of semantic analysis in NLP is crucial to its future prospects. In this paper we begin to chart the various proposals for semantic schemes according to the content they support. As not many semantic queries on texts can at prese"
P17-1008,I05-2035,0,0.217307,"ions (Xue et al., 2014). As an example, UDS. Universal Decompositional Semantics (White et al., 2016) is a multi-layered scheme, which currently includes semantic role anno81 Structure Grammar (HPSG; Pollard and Sag, 1994), where syntactic and semantic features are represented as feature bundles, which are iteratively composed through unification rules to form composite units. HPSG-based SRT schemes commonly use the Minimal Recursion Semantics (Copestake et al., 2005) formalism. Annotated corpora and manually crafted grammars exist for multiple languages (Flickinger, 2002; Oepen et al., 2004; Bender and Flickinger, 2005, inter alia), and generally focus on argument structural and logical semantic phenomena. The Broad-coverage Semantic Dependency Parsing shared task and corpora (Oepen et al., 2014, 2015) include corpora annotated with the PDT-TL, and dependencies extracted from the HPSG grammars Enju (Miyao, 2006) and the LinGO English Reference Grammar (ERG; Flickinger, 2002). tation, word senses and aspectual classes (e.g., realis/irrealis). UDS emphasizes accessible distinctions, which can be collected through crowd-sourcing. However, the skeletal structure of UDS representations is derived from syntactic"
P17-1008,W15-0128,0,0.0248707,"s immediate constituents and their syntactic relationships, which are generally assumed to form a closed set (Montague, 1970, and much subsequent work). Thus, the interpretation of a sentence can be computed bottom-up, by establishing the meaning of individual words, and recursively composing them, to obtain the full sentential semantics. The order and type of these compositions are determined by the syntactic structure. Compositionality is employed by linguistically expressive grammars, such as those based on CCG and HPSG, and has proven to be a powerful method for various applications. See (Bender et al., 2015) for a recent discussion of the advantages of compositional SRTs. Nevertheless, a compositional account meets difficulties when faced with multi-word expressions and in accounting for cases like “he sneezed the napkin off the table”, where it is difficult to determine whether “sneezed” or “off” account for the constructional meaning. Construction Grammar (Fillmore et al., 1988; Goldberg, 1995) answers these issues by using an open set of construction-specific compositional operators, and supporting lexical en84 tries of varying lengths. Several ongoing projects address the implementation of th"
P17-1008,P16-1166,0,0.0211545,"notation schemes for causality and its sub-types. Mostafazadeh et al. (2016) integrated causal and TimeML-style temporal relations into a unified representation. The internal temporal structure of events has been less frequently tackled. Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., “climbing a mountain”), or its culmination (“reaching its top”). Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual classes (Siegel and McKeown, 2000; Palmer et al., 2007; Friedrich et al., 2016; White et al., 2016), and tense distinctions (Elson and McKeown, 2010). Still, casting events in terms of their temporal components, characterizing an annotation scheme for doing so and rooting it in theoretical foundations, is an open challenge for NLP. bank (Carlson et al., 2003), which places more focus on higher-order discourse structures, resulting in deeper hierarchical structures than the PeDT’s, which focuses on local discourse structure. Another discourse information type explored in NLP is discourse segmentation, where texts are partitioned into shallow structures of discourse units"
P17-1008,P10-1160,0,0.0252622,"ries across schemes. Most SRL schemes cover a wide variety of verbal predicates, but differ in which nominal and adjectival predicates are covered. For example, PropBank (Palmer et al., 2005), one of the major resources for SRL, covers verbs, and in its recent versions also eventive nouns and multi-argument adjectives. FrameNet (Ruppenhofer et al., 2016) covers all these, but also covers relational nouns that do not evoke an event, such as “president”. Other lines of work address semantic arguments that appear outside sentence boundaries, or that do not explicitly appear anywhere in the text (Gerber and Chai, 2010; Roth and Frank, 2015). Core and Non-core Arguments. Perhaps the most common distinction between argument types is between core and non-core arguments (Dowty, 2003). While it is possible to define the distinction distributionally as one between obligatory and optional arguments, here we focus on the semantic dimension, which distinguishes arguments whose meaning is predicate-specific and are necessary components of the described event (core), and those which are predicate-general (non-core). For example, FrameNet defines core arguments as conceptually necessary components of a frame, that mak"
P17-1008,J02-3001,0,0.0351407,"rface. We note that this definition of semantics is somewhat different from the one intended here, which defines semantic schemes as theories of meaning. We stipulate that a fundamental component of the content conveyed by SRTs is argument structure – who did what to whom, where, when and why, i.e., events, their participants and the relations between them. Indeed, the fundamental status of argument structure has been recognized by essentially all approaches to semantics both in theoretical linguistics (Levin and Hovav, 2005) and in NLP, through approaches such as Semantic Role Labeling (SRL; Gildea and Jurafsky, 2002), formal semantic analysis (e.g., Bos, 2008), and Abstract Meaning Representation (AMR; Banarescu et al., 2013). Many other useful meaning components have been proposed, and are discussed at a greater depth in Section 3. 3 Another approach to defining an SRT is through external (extra-textual) criteria or applications. For instance, a semantic representation can be defined to support inference, as in textual entailment (Dagan et al., 2006) or natural logic (Angeli and Manning, 2014). Other examples include defining a semantic representation in terms of supporting knowledge base querying (Zelle"
P17-1008,C12-1089,0,0.0055984,", these schemes require considerable adaptation when ported across languages (Kozhevnikov and Titov, 2013). Ongoing research tackles the generalization of VerbNet’s unlexicalized roles to a universally applicable set (e.g., Schneider et al., 2015). Few SRT schemes place cross-linguistically applicability as one of their main criteria, examples include UCCA, and the LinGO Grammar Matrix (Bender and Flickinger, 2005), both of which draw on typological theory. Vector space models, which embed words and sentences in a vector space, have also been applied to induce a shared cross-linguistic space (Klementiev et al., 2012; Rajendran et al., 2015; Wu et al., 2016). However, further evaluation is required in order to determine what aspects of meaning these representations reflect reliably. 6 6.1 Syntax and Semantics Syntactic and Semantic Generalization 6.2 Syntactic distinctions are generally guided by a combination of semantic and distributional considerations, where emphasis varies across schemes. Consider phrase-based syntactic structures, common examples of which, such as the Penn Treebank for English (Marcus et al., 1993) and the Penn Chinese Treebank (Xue et al., 2005), are adaptations of X-bar theory. Co"
P17-1008,S12-1048,0,0.0213673,"relations beyond the scope of a single sentence are often represented by specialized semantic resources and not by general ones, despite the absence of a clear boundary line between them. This, however, is beginning to change with some schemes, e.g., GMB and UCCA, already supporting cross-sentence semantic relations.3 Spatial Relations. The representation of spatial relations is pivotal in cognitive theories of meaning (e.g., Langacker, 2008), and in application domains such as geographical information systems or robotic navigation. Important tasks in this field include Spatial Role Labeling (Kordjamshidi et al., 2012) and the more recent SpaceEval (Pustejovsky et al., 2015). The tasks include the identification and classification of spatial elements and relations, such as places, paths, directions and motions, and their relative configuration. Logical Structure. Logical structure, including quantification, negation, coordination and their associated scope distinctions, is the cornerstone of semantic analysis in much of theoretical linguistics, and has attracted much attention in NLP as well. Common representations are often based on variants of predicate calculus, and are useful for applications that requi"
P17-1008,P13-1117,0,0.024359,"purely semantic schemes such as AMR and UCCA consider (1) “founding of the school”, (2) “president of the United States” and (3) “United States president”. UD is faithful to the syntactic structure and represents (1) and (2) similarly, while assigning a different structure to (3). In contrast, AMR and UCCA perform a semantic generalization and represents examples (2) and (3) similarly and differently from (1). ever, as both PropBank and FrameNet are lexicalized schemes, and as lexicons diverge wildly across languages, these schemes require considerable adaptation when ported across languages (Kozhevnikov and Titov, 2013). Ongoing research tackles the generalization of VerbNet’s unlexicalized roles to a universally applicable set (e.g., Schneider et al., 2015). Few SRT schemes place cross-linguistically applicability as one of their main criteria, examples include UCCA, and the LinGO Grammar Matrix (Bender and Flickinger, 2005), both of which draw on typological theory. Vector space models, which embed words and sentences in a vector space, have also been applied to induce a shared cross-linguistic space (Klementiev et al., 2012; Rajendran et al., 2015; Wu et al., 2016). However, further evaluation is required"
P17-1008,E12-1059,0,0.00970673,"terms of semantic features computed from them, whose validity is established by human annotators (e.g., Agirre et al., 2013, 2014). Finally, where semantic schemes are induced through manual annotation (and not through auUniversality. One of the great promises of semantic analysis (over more surface forms of analysis) is its cross-linguistic potential. However, while the theoretical and applicative importance of universality in semantics has long been recognized (Goddard, 2011), the nature of universal semantics remains unknown. Recently, projects such as BabelNet (Ehrmann et al., 2014), UBY (Gurevych et al., 2012) and Open Multilingual Wordnet4 , constructed huge multi-lingual semantic nets, by linking resources such as Wikipedia and WordNet and processing them using modern NLP. However, such projects currently focus on lexical semantic and encyclopedic information rather than on text semantics. Symbolic SRT schemes such as SRL schemes and AMR have also been studied for their crosslinguistic applicability (Pad´o and Lapata, 2009; Sun et al., 2010; Xue et al., 2014), indicating partial portability across languages. Translated versions of PropBank and FrameNet have been constructed for multiple languages"
P17-1008,D10-1119,0,0.0189537,"el for different senses of the English possessive construction, regardless of whether they correspond to ownership (e.g., “John’s dog”) or to a different meaning, such as marking an argument of a nominal predicate (e.g., “John’s kick”). See Section 6. CCG-based Schemes. CCG (Steedman, 2000) is a lexicalized grammar (i.e., nearly all semantic content is encoded in the lexicon), which defines a theory of how lexical information is composed to form the meaning of phrases and sentences (see Section 6.2), and has proven effective in a variety of semantic tasks (Zettlemoyer and Collins, 2005, 2007; Kwiatkowski et al., 2010; Artzi and Zettlemoyer, 2013, inter alia). Several projects have constructed logical representations by associating CCG with semantic forms (by assigning logical forms to the leaves). For example, Boxer (Bos, 2008) and GMB, which builds on Boxer, use Discourse Representation Structures (Kamp and Reyle, 1993), while Lewis and Steedman (2013) used Davidsonian-style λ-expressions, accompanied by lexical categorization of the predicates. These schemes encode events with their argument structures, and include an elaborate logical structure, as well as lexical and discourse information. OntoNotes i"
P17-1008,P13-1134,0,0.0333479,"Missing"
P17-1008,D13-1149,0,0.027642,"man evaluation is the ultimate criterion for validating an SRT scheme given our definition of semantics as meaning as it is understood by a language speaker. Determining how well an SRT scheme corresponds to human interpretation of a text is ideally carried out by asking annotators to make some semantic prediction or annotation according to pre-specified guidelines, and to compare this to the information extracted from the SRT. Question Answering SRL (QASRL; He et al., 2015) is an SRL scheme which solicits nonexperts to answer mostly wh-questions, converting their output to an SRL annotation. Hartshorne et al. (2013) and Reisinger et al. (2015) use crowdsourcing to elicit semantic role features, such as whether the argument was volitional in the described event, in order to evaluate proposals for semantic role sets. Another evaluation approach is task-based evaluation. Many semantic representations in NLP are defined with an application in mind, making this type of evaluation natural. For instance, a major motivation for AMR is its applicability to machine translation, making MT a natural (albeit hitherto unexplored) testbed for AMR evaluation. Another example is using question answering to evaluate seman"
P17-1008,Q13-1015,0,0.0481678,"theoretical linguistics, and has attracted much attention in NLP as well. Common representations are often based on variants of predicate calculus, and are useful for applications that require mapping text into an external, often executable, formal language, such as a querying language (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005) or robot instructions (Artzi and Zettlemoyer, 2013). Logical structures are also useful for recognizing entailment relations between sentences, as some entailments can be computed from the text’s logical structure by formal provers (Bos and Markert, 2005; Lewis and Steedman, 2013). Discourse Relations encompass any semantic relation between events or larger semantic units. For example, in (1) the leaving and the giving events are sometimes related through a discourse relation of type C ONCESSION, evoked by “although”. Such information is useful, often essential for a variety of NLP tasks such as summarization, machine translation and information extraction, but is commonly overlooked in the development of such systems (Webber and Joshi, 2012). The Penn Discourse Treebank (PeDT; Miltsakaki et al., 2004) annotates discourse units, and classifies the relations between the"
P17-1008,D15-1076,0,0.00372503,"tegies for collecting some aspects of the annotation. Schemes also differ in other aspects discussed in Sections 5 and 6. 5 Evaluation Human evaluation is the ultimate criterion for validating an SRT scheme given our definition of semantics as meaning as it is understood by a language speaker. Determining how well an SRT scheme corresponds to human interpretation of a text is ideally carried out by asking annotators to make some semantic prediction or annotation according to pre-specified guidelines, and to compare this to the information extracted from the SRT. Question Answering SRL (QASRL; He et al., 2015) is an SRL scheme which solicits nonexperts to answer mostly wh-questions, converting their output to an SRL annotation. Hartshorne et al. (2013) and Reisinger et al. (2015) use crowdsourcing to elicit semantic role features, such as whether the argument was volitional in the described event, in order to evaluate proposals for semantic role sets. Another evaluation approach is task-based evaluation. Many semantic representations in NLP are defined with an application in mind, making this type of evaluation natural. For instance, a major motivation for AMR is its applicability to machine transl"
P17-1008,liakata-etal-2010-corpora,0,0.028923,"lenge for NLP. bank (Carlson et al., 2003), which places more focus on higher-order discourse structures, resulting in deeper hierarchical structures than the PeDT’s, which focuses on local discourse structure. Another discourse information type explored in NLP is discourse segmentation, where texts are partitioned into shallow structures of discourse units categorized either according to their topic or according to their function within the text. An example is the segmentation of scientific papers into functional segments and their labeling with categories such as BACKGROUND and D ISCUSSION (Liakata et al., 2010). See (Webber et al., 2011) for a survey of discourse structure in NLP. Discourse relations beyond the scope of a single sentence are often represented by specialized semantic resources and not by general ones, despite the absence of a clear boundary line between them. This, however, is beginning to change with some schemes, e.g., GMB and UCCA, already supporting cross-sentence semantic relations.3 Spatial Relations. The representation of spatial relations is pivotal in cognitive theories of meaning (e.g., Langacker, 2008), and in application domains such as geographical information systems or"
P17-1008,W97-1311,0,0.296131,"78 evoke the same frame type, such as “leave” and “depart”), and PropBank (where roles are verbspecific). PropBank’s role sets were extended by subsequent projects such as AMR. Another prominent semantic role inventory is VerbNet (Kipper et al., 2008) and subsequent projects (Bonial et al., 2011; Schneider et al., 2015), which define a closed set of abstract semantic roles (such as AGENT, PATIENT and I NSTRUMENT) that apply to all predicate arguments. The events discussed here should not be confused with events as defined in Information Extraction and related tasks such as event coreference (Humphreys et al., 1997), which correspond more closely to the everyday notion of an event, such as a political or financial event, and generally consist of multiple events in the sense discussed here. The representation of such events is recently receiving considerable interest within NLP, e.g. the Richer Event Descriptions framework (RED; Ikuta et al., 2014). Co-reference and Anaphora. Co-reference allows to abstract away from the different ways to refer to the same entity, and is commonly included in semantic resources. Coreference interacts with argument structure annotation, as in its absence each argument is ar"
P17-1008,P07-1113,0,0.0157312,"recently proposed annotation schemes for causality and its sub-types. Mostafazadeh et al. (2016) integrated causal and TimeML-style temporal relations into a unified representation. The internal temporal structure of events has been less frequently tackled. Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., “climbing a mountain”), or its culmination (“reaching its top”). Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual classes (Siegel and McKeown, 2000; Palmer et al., 2007; Friedrich et al., 2016; White et al., 2016), and tense distinctions (Elson and McKeown, 2010). Still, casting events in terms of their temporal components, characterizing an annotation scheme for doing so and rooting it in theoretical foundations, is an open challenge for NLP. bank (Carlson et al., 2003), which places more focus on higher-order discourse structures, resulting in deeper hierarchical structures than the PeDT’s, which focuses on local discourse structure. Another discourse information type explored in NLP is discourse segmentation, where texts are partitioned into shallow struc"
P17-1008,miltsakaki-etal-2004-penn,0,0.0823945,"text’s logical structure by formal provers (Bos and Markert, 2005; Lewis and Steedman, 2013). Discourse Relations encompass any semantic relation between events or larger semantic units. For example, in (1) the leaving and the giving events are sometimes related through a discourse relation of type C ONCESSION, evoked by “although”. Such information is useful, often essential for a variety of NLP tasks such as summarization, machine translation and information extraction, but is commonly overlooked in the development of such systems (Webber and Joshi, 2012). The Penn Discourse Treebank (PeDT; Miltsakaki et al., 2004) annotates discourse units, and classifies the relations between them into a hierarchical, closed category set, including high-level relation types like T EMPORAL, C OMPARISON and C ONTINGENCY and finer-grained ones such as J USTIFICATION and E XCEPTION. Another commonly used resource is the RST Discourse TreeInference and Entailment. A primary motivation for many semantic schemes is their ability to support inference and entailment. Indeed, means for predicting logical entailment are built into many forms of semantic representations. A different approach was taken in the tasks of Recognizing"
P17-1008,J05-1004,0,0.341427,")). This distinction may be important for text understanding, as the inferred cases tend to be more ambiguous (“she” in (1) might not refer to “Ann”). Other schemes, such as AMR, eschew this distinction and use the same terms to represent all cases of coreference. Predicates and Arguments. While predicateargument relations are universally recognized as fundamental to semantic representation, the interpretation of the terms varies across schemes. Most SRL schemes cover a wide variety of verbal predicates, but differ in which nominal and adjectival predicates are covered. For example, PropBank (Palmer et al., 2005), one of the major resources for SRL, covers verbs, and in its recent versions also eventive nouns and multi-argument adjectives. FrameNet (Ruppenhofer et al., 2016) covers all these, but also covers relational nouns that do not evoke an event, such as “president”. Other lines of work address semantic arguments that appear outside sentence boundaries, or that do not explicitly appear anywhere in the text (Gerber and Chai, 2010; Roth and Frank, 2015). Core and Non-core Arguments. Perhaps the most common distinction between argument types is between core and non-core arguments (Dowty, 2003). Whi"
P17-1008,W14-0702,0,0.0194485,"(Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010). For instance, going to a restaurant includes sitting at a table, ordering, eating and paying, generally in this order. Related to temporal relations, are causal relations between events, which are ubiquitous in language, and central for a variety of applications, Semantic Roles. Semantic roles are categories of arguments. Many different semantic role inventories have been proposed and used in NLP over the years, the most prominent being FrameNet (where roles are shared across predicates that 79 including planning and entailment. See (Mirza et al., 2014) and (Dunietz et al., 2015) for recently proposed annotation schemes for causality and its sub-types. Mostafazadeh et al. (2016) integrated causal and TimeML-style temporal relations into a unified representation. The internal temporal structure of events has been less frequently tackled. Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., “climbing a mountain”), or its culmination (“reaching its top”). Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual clas"
P17-1008,J88-2003,0,0.861713,"applications, Semantic Roles. Semantic roles are categories of arguments. Many different semantic role inventories have been proposed and used in NLP over the years, the most prominent being FrameNet (where roles are shared across predicates that 79 including planning and entailment. See (Mirza et al., 2014) and (Dunietz et al., 2015) for recently proposed annotation schemes for causality and its sub-types. Mostafazadeh et al. (2016) integrated causal and TimeML-style temporal relations into a unified representation. The internal temporal structure of events has been less frequently tackled. Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., “climbing a mountain”), or its culmination (“reaching its top”). Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual classes (Siegel and McKeown, 2000; Palmer et al., 2007; Friedrich et al., 2016; White et al., 2016), and tense distinctions (Elson and McKeown, 2010). Still, casting events in terms of their temporal components, characterizing an annotation scheme for doing so and rooting it in theoretical foundations, is an open chal"
P17-1008,W16-1007,0,0.0048061,"le, ordering, eating and paying, generally in this order. Related to temporal relations, are causal relations between events, which are ubiquitous in language, and central for a variety of applications, Semantic Roles. Semantic roles are categories of arguments. Many different semantic role inventories have been proposed and used in NLP over the years, the most prominent being FrameNet (where roles are shared across predicates that 79 including planning and entailment. See (Mirza et al., 2014) and (Dunietz et al., 2015) for recently proposed annotation schemes for causality and its sub-types. Mostafazadeh et al. (2016) integrated causal and TimeML-style temporal relations into a unified representation. The internal temporal structure of events has been less frequently tackled. Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., “climbing a mountain”), or its culmination (“reaching its top”). Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual classes (Siegel and McKeown, 2000; Palmer et al., 2007; Friedrich et al., 2016; White et al., 2016), and tense distinctions (Elson a"
P17-1008,Q16-1010,0,0.0535261,"Missing"
P17-1008,P10-1100,0,0.0164222,"Most temporal semantic work in NLP has focused on temporal relations between events, either by timestamping them according to time expressions found in the text, or by predicting their relative order in time. Important resources include TimeML, a specification language for temporal relations (Pustejovsky et al., 2003), and the TempEval series of shared tasks and annotated corpora (Verhagen et al., 2009, 2010; UzZaman et al., 2013). A different line of work explores scripts: schematic, temporally ordered sequences of events associated with a certain scenario (Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010). For instance, going to a restaurant includes sitting at a table, ordering, eating and paying, generally in this order. Related to temporal relations, are causal relations between events, which are ubiquitous in language, and central for a variety of applications, Semantic Roles. Semantic roles are categories of arguments. Many different semantic role inventories have been proposed and used in NLP over the years, the most prominent being FrameNet (where roles are shared across predicates that 79 including planning and entailment. See (Mirza et al., 2014) and (Dunietz et al., 2015) for recentl"
P17-1008,S15-2153,0,0.129344,"Missing"
P17-1008,Q15-1034,0,0.142523,"Missing"
P17-1008,S14-2008,0,0.0868123,"Missing"
P17-1008,J15-4003,0,0.0161424,"st SRL schemes cover a wide variety of verbal predicates, but differ in which nominal and adjectival predicates are covered. For example, PropBank (Palmer et al., 2005), one of the major resources for SRL, covers verbs, and in its recent versions also eventive nouns and multi-argument adjectives. FrameNet (Ruppenhofer et al., 2016) covers all these, but also covers relational nouns that do not evoke an event, such as “president”. Other lines of work address semantic arguments that appear outside sentence boundaries, or that do not explicitly appear anywhere in the text (Gerber and Chai, 2010; Roth and Frank, 2015). Core and Non-core Arguments. Perhaps the most common distinction between argument types is between core and non-core arguments (Dowty, 2003). While it is possible to define the distinction distributionally as one between obligatory and optional arguments, here we focus on the semantic dimension, which distinguishes arguments whose meaning is predicate-specific and are necessary components of the described event (core), and those which are predicate-general (non-core). For example, FrameNet defines core arguments as conceptually necessary components of a frame, that make the frame unique and"
P17-1008,W15-1612,0,0.0752109,"ges, is its cross-linguistic universality. While languages differ in terms of their form (e.g., in their phonology, lexicon, and syntax), they have often been as2 We use the term “Text Semantics”, rather than the commonly used “Sentence Semantics” to include inter-sentence semantic relations as well. 78 evoke the same frame type, such as “leave” and “depart”), and PropBank (where roles are verbspecific). PropBank’s role sets were extended by subsequent projects such as AMR. Another prominent semantic role inventory is VerbNet (Kipper et al., 2008) and subsequent projects (Bonial et al., 2011; Schneider et al., 2015), which define a closed set of abstract semantic roles (such as AGENT, PATIENT and I NSTRUMENT) that apply to all predicate arguments. The events discussed here should not be confused with events as defined in Information Extraction and related tasks such as event coreference (Humphreys et al., 1997), which correspond more closely to the everyday notion of an event, such as a political or financial event, and generally consist of multiple events in the sense discussed here. The representation of such events is recently receiving considerable interest within NLP, e.g. the Richer Event Descripti"
P17-1008,W12-3205,0,0.0188224,"een sentences, as some entailments can be computed from the text’s logical structure by formal provers (Bos and Markert, 2005; Lewis and Steedman, 2013). Discourse Relations encompass any semantic relation between events or larger semantic units. For example, in (1) the leaving and the giving events are sometimes related through a discourse relation of type C ONCESSION, evoked by “although”. Such information is useful, often essential for a variety of NLP tasks such as summarization, machine translation and information extraction, but is commonly overlooked in the development of such systems (Webber and Joshi, 2012). The Penn Discourse Treebank (PeDT; Miltsakaki et al., 2004) annotates discourse units, and classifies the relations between them into a hierarchical, closed category set, including high-level relation types like T EMPORAL, C OMPARISON and C ONTINGENCY and finer-grained ones such as J USTIFICATION and E XCEPTION. Another commonly used resource is the RST Discourse TreeInference and Entailment. A primary motivation for many semantic schemes is their ability to support inference and entailment. Indeed, means for predicting logical entailment are built into many forms of semantic representations"
P17-1008,D16-1177,0,0.189031,"Missing"
P17-1008,J00-4004,0,0.123845,"(Dunietz et al., 2015) for recently proposed annotation schemes for causality and its sub-types. Mostafazadeh et al. (2016) integrated causal and TimeML-style temporal relations into a unified representation. The internal temporal structure of events has been less frequently tackled. Moens and Steedman (1988) defined an ontology for the temporal components of an event, such as its preparatory process (e.g., “climbing a mountain”), or its culmination (“reaching its top”). Statistical work on this topic is unfortunately scarce, and mostly focuses on lexical categories such as aspectual classes (Siegel and McKeown, 2000; Palmer et al., 2007; Friedrich et al., 2016; White et al., 2016), and tense distinctions (Elson and McKeown, 2010). Still, casting events in terms of their temporal components, characterizing an annotation scheme for doing so and rooting it in theoretical foundations, is an open challenge for NLP. bank (Carlson et al., 2003), which places more focus on higher-order discourse structures, resulting in deeper hierarchical structures than the PeDT’s, which focuses on local discourse structure. Another discourse information type explored in NLP is discourse segmentation, where texts are partition"
P17-1008,P13-1045,0,0.0269841,"approach to SRT is taken by Vector Space Models (VSM), which eschew the use of symbolic structures, instead modeling all linguistic elements as vectors, from the level of words to phrases and sentences. Proponents of this approach generally invoke neural network methods, obtaining impressive results on a variety of tasks including lexical tasks such as cross-linguistic word similarity (Ammar et al., 2016), machine translation (Bahdanau et al., 2015), and dependency parsing (Andor et al., 2016). VSMs are also attractive in being flexible enough to model non-local and gradient phenomena (e.g., Socher et al., 2013). However, more research is needed to clarify the scope of semantic phenomena that such models are able to reliably capture. We therefore only lightly touch on VSMs in this survey. Finally, a major consideration in semantic analysis, and one of its great potential advantages, is its cross-linguistic universality. While languages differ in terms of their form (e.g., in their phonology, lexicon, and syntax), they have often been as2 We use the term “Text Semantics”, rather than the commonly used “Sentence Semantics” to include inter-sentence semantic relations as well. 78 evoke the same frame ty"
P17-1008,W06-3510,0,0.0923512,"Missing"
P17-1008,W15-3502,1,0.702518,"Missing"
P17-1008,C10-1119,0,0.0610223,"ong been recognized (Goddard, 2011), the nature of universal semantics remains unknown. Recently, projects such as BabelNet (Ehrmann et al., 2014), UBY (Gurevych et al., 2012) and Open Multilingual Wordnet4 , constructed huge multi-lingual semantic nets, by linking resources such as Wikipedia and WordNet and processing them using modern NLP. However, such projects currently focus on lexical semantic and encyclopedic information rather than on text semantics. Symbolic SRT schemes such as SRL schemes and AMR have also been studied for their crosslinguistic applicability (Pad´o and Lapata, 2009; Sun et al., 2010; Xue et al., 2014), indicating partial portability across languages. Translated versions of PropBank and FrameNet have been constructed for multiple languages (e.g., Akbik et al., 2016; Hartmann and Gurevych, 2013). How4 83 http://compling.hss.ntu.edu.sg/omw/ ing a cross-linguistically consistent dependencybased annotation, and whose categories are motivated by a combination of distributional and semantic considerations. For example, UD would distinguish between the dependency type between “John” and “brother” in “John, my brother, arrived” and “John, who is my brother, arrived”, despite thei"
P17-1008,xue-etal-2014-interlingua,0,0.303418,"AMR covers predicate-argument relations, including semantic roles (adapted from PropBank) that apply to a wide variety of predicates (including verbal, nominal and adjectival predicates), modifiers, co-reference, named entities and some time expressions. AMR does not currently support relations above the sentence level, and is admittedly Englishcentric, which results in an occasional conflation of semantic phenomena that happen to be similarly realized in English, into a single semantic category. AMR thus faces difficulties when assessing the invariance of its structures across translations (Xue et al., 2014). As an example, UDS. Universal Decompositional Semantics (White et al., 2016) is a multi-layered scheme, which currently includes semantic role anno81 Structure Grammar (HPSG; Pollard and Sag, 1994), where syntactic and semantic features are represented as feature bundles, which are iteratively composed through unification rules to form composite units. HPSG-based SRT schemes commonly use the Minimal Recursion Semantics (Copestake et al., 2005) formalism. Annotated corpora and manually crafted grammars exist for multiple languages (Flickinger, 2002; Oepen et al., 2004; Bender and Flickinger,"
P17-1008,S13-2001,0,0.00881579,"at introduce additional, independent or distinct relations from that of the frame such as time, place, manner, means and degree (Ruppenhofer et al., 2016, pp. 23-24). Temporal Relations. Most temporal semantic work in NLP has focused on temporal relations between events, either by timestamping them according to time expressions found in the text, or by predicting their relative order in time. Important resources include TimeML, a specification language for temporal relations (Pustejovsky et al., 2003), and the TempEval series of shared tasks and annotated corpora (Verhagen et al., 2009, 2010; UzZaman et al., 2013). A different line of work explores scripts: schematic, temporally ordered sequences of events associated with a certain scenario (Chambers and Jurafsky, 2008, 2009; Regneri et al., 2010). For instance, going to a restaurant includes sitting at a table, ordering, eating and paying, generally in this order. Related to temporal relations, are causal relations between events, which are ubiquitous in language, and central for a variety of applications, Semantic Roles. Semantic roles are categories of arguments. Many different semantic role inventories have been proposed and used in NLP over the ye"
P17-1008,D07-1071,0,0.0476805,"Missing"
P17-1008,W10-4205,0,\N,Missing
P17-1008,J93-2004,0,\N,Missing
P17-1008,D10-1089,0,\N,Missing
P17-1008,W08-2222,0,\N,Missing
P17-1008,H05-1079,0,\N,Missing
P17-1008,S14-2010,0,\N,Missing
P17-1008,S13-1004,0,\N,Missing
P17-1008,Q14-1022,0,\N,Missing
P17-1008,P13-2131,0,\N,Missing
P17-1008,P13-1023,1,\N,Missing
P17-1008,W15-1622,0,\N,Missing
P17-1008,D16-1102,0,\N,Missing
P17-1008,L16-1262,0,\N,Missing
P17-1008,ehrmann-etal-2014-representing,0,\N,Missing
P17-1008,W14-2903,0,\N,Missing
P17-1104,W15-0126,0,0.0787572,"Missing"
P17-1104,S15-2162,0,0.169588,"ation approach used for dependency graph parsing (Agi´c et al., 2015; Fern´andez-Gonz´alez and Martins, 2015), where dependency graphs were converted into dependency trees and then parsed by dependency tree parsers. In our setting, the conversion to trees consists simply of removing remote edges from the graph, and then to bilexical trees by applying the same procedure as for bilexical graphs. Baseline parsers. We evaluate two bilexical graph semantic dependency parsers: DAGParser (Ribeyre et al., 2014), the leading transition-based parser in SemEval 2014 (Oepen et al., 2014) and TurboParser (Almeida and Martins, 2015), a graph-based parser from SemEval 2015 (Oepen et al., 2015); UPARSE (Maier and Lichte, 2016), a transition-based constituency parser supporting discontinuous constituents; and two bilexical tree parsers: MaltParser (Nivre et al., 2007), and the stack LSTM-based parser of Dyer et al. (2015, henceforce “LSTM Parser”). Default settings are used in all cases.9 DAGParser and UPARSE use beam search by default, with a beam size of 5 and 4 respectively. The other parsers are greedy. 5 Results Comparison to tree parsers. For completeness, and as parsing technology is considerably more Table 2 present"
P17-1104,N15-1006,0,0.022221,"n-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as trained parser models, are available at https://github.com/ danielhers/tupa. 1127 Proceedings of the 55th Annual Meeting of the Association for Computational Ling"
P17-1104,N16-1052,0,0.028808,"Missing"
P17-1104,P16-1231,0,0.0374852,"ility has been so far limited, a gap this work addresses. We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing novel transitions and features for UCCA. Transition-based techniques are a natural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, in"
P17-1104,D15-1198,0,0.0200772,"Missing"
P17-1104,W13-2322,0,0.278436,"ene, despite not being overtly marked. Third, consider the possessive construction in Figure 1c. While in UCCA “trip” evokes a scene in which “John and Mary” is a Participant, a syntactic scheme would analyze this phrase similarly to “John and Mary’s shoes”. These examples demonstrate that a UCCA parser, and more generally semantic parsers, face an additional level of ambiguity compared to their syntactic counterparts (e.g., “after graduation” is formally very similar to “after 2pm”, which does not evoke a scene). Section 6 discusses UCCA in the context of other semantic schemes, such as AMR (Banarescu et al., 2013). Alongside recent progress in dependency parsing into projective trees, there is increasing interest in parsing into representations with more general structural properties (see Section 6). One such property is reentrancy, namely the sharing of semantic units between predicates. For instance, in Figure 1a, “John” is an argument of both “gradu1128 ation” and “moved”, yielding a DAG rather than a tree. A second property is discontinuity, as in Figure 1b, where “gave up” forms a discontinuous semantic unit. Discontinuities are pervasive, e.g., with multi-word expressions (Schneider et al., 2014)"
P17-1104,D16-1134,1,0.37369,"tion (UCCA, Abend and Rappoport, 2013) is a crosslinguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010a,b, 2012), and Cognitive Linguistics literature (Croft and Cruse, 2004). It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation by non-experts (assisted by an accessible annotation interface (Abend et al., 2017)), and stability under translation (Sulem et al., 2015). It has also proven useful for machine translation evaluation (Birch et al., 2016). UCCA differs from syntactic schemes in terms of content and formal structure. It exhibits reentrancy, discontinuous nodes and non-terminals, which no single existing parser supports. Lacking a parser, UCCA’s applicability has been so far limited, a gap this work addresses. We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing novel transitions and features for UCCA. Transition-based techniques are a natural starting point for UCCA parsing, given the conceptual simi"
P17-1104,P17-1008,1,0.0617527,"Missing"
P17-1104,P17-4019,1,0.688987,"mantic DAG structures, and in languages that frequently use discontinuous structures. 1 Introduction Universal Conceptual Cognitive Annotation (UCCA, Abend and Rappoport, 2013) is a crosslinguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010a,b, 2012), and Cognitive Linguistics literature (Croft and Cruse, 2004). It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation by non-experts (assisted by an accessible annotation interface (Abend et al., 2017)), and stability under translation (Sulem et al., 2015). It has also proven useful for machine translation evaluation (Birch et al., 2016). UCCA differs from syntactic schemes in terms of content and formal structure. It exhibits reentrancy, discontinuous nodes and non-terminals, which no single existing parser supports. Lacking a parser, UCCA’s applicability has been so far limited, a gap this work addresses. We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing nov"
P17-1104,D14-1082,0,0.0488151,"ransition-based dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015). R EDUCE pops the stack, to allow removing a node once all its edges have been created. To handle discontinuous nodes, S WAP pops the second node on the stack and adds it to the top of the buffer, as with the similarly named transition in previous work (Nivre, 2009; Maier, 2015). Finally, F INISH pops the root node and marks the state as terminal. Classifier. The choice of classifier and feature representation has been shown to play an important role in transition-based parsing (Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). To investigate the impact of the type of transition classifier in UCCA parsing, we experiment with three different models. 1. Starting with a simple and common choice (e.g., Maier and Lichte, 2016), TUPASparse uses a linear classifier with sparse features, trained with the averaged structured perceptron algorithm (Collins and Roark, 2004) and M IN U PDATE (Goldberg and Elhadad, 2011): each feature requires a minimum number of updates in training to be included in the model.2 2. Changing the model to a feedforward neural network with dense"
P17-1104,P04-1015,0,0.0157842,"5). Finally, F INISH pops the root node and marks the state as terminal. Classifier. The choice of classifier and feature representation has been shown to play an important role in transition-based parsing (Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). To investigate the impact of the type of transition classifier in UCCA parsing, we experiment with three different models. 1. Starting with a simple and common choice (e.g., Maier and Lichte, 2016), TUPASparse uses a linear classifier with sparse features, trained with the averaged structured perceptron algorithm (Collins and Roark, 2004) and M IN U PDATE (Goldberg and Elhadad, 2011): each feature requires a minimum number of updates in training to be included in the model.2 2. Changing the model to a feedforward neural network with dense embedding features, TUPAMLP (“multi-layer perceptron”), uses an architecture similar to that of Chen and Manning (2014), but with two rectified linear layers 2 We also experimented with a linear model using dense embedding features, trained with the averaged structured perceptron algorithm. It performed worse than the sparse perceptron model and was hence discarded. 1129 Before Transition Tra"
P17-1104,E17-1051,0,0.0497003,"essed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as trained parser models, are available at https://github.com/ danielhers/tupa. 1127 Proceedings of the 5"
P17-1104,S15-2154,0,0.0768183,"Missing"
P17-1104,P15-1033,0,0.0724261,"er, UCCA’s applicability has been so far limited, a gap this work addresses. We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing novel transitions and features for UCCA. Transition-based techniques are a natural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the Engl"
P17-1104,P15-1147,0,0.0513024,"Missing"
P17-1104,P14-1134,0,0.0453764,"Missing"
P17-1104,C12-1059,0,0.035982,"ord dropout (Kiperwasser and Goldberg, 2016): with a certain probability, the embedding for a word is replaced with a zero vector. We do not apply word dropout to the external word embeddings. Finally, for all classifiers we add a novel realvalued feature to the input vector, ratio, corresponding to the ratio between the number of terminals to number of nodes in the graph G. This feature serves as a regularizer for the creation of new nodes, and should be beneficial for other transition-based constituency parsers too. Training. For training the transition classifiers, we use a dynamic oracle (Goldberg and Nivre, 2012), i.e., an oracle that outputs a set of optimal transitions: when applied to the current parser state, the gold standard graph is reachable from the resulting state. For example, the oracle would predict a N ODE transition if the stack has on its top a parent in the gold graph that has not been created, but would predict a R IGHT-E DGE transition if the second stack element is a parent of the first element according to the gold graph and the edge between them has not been created. The transition predicted by the classifier is deemed correct and is applied to the parser state to reach the subse"
P17-1104,P16-1001,0,0.00460807,"argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as trained parser models, are available at https://github.com/"
P17-1104,J13-4006,0,0.014711,"Missing"
P17-1104,D15-1162,0,0.00802403,"STM ... LSTM LSTM After graduation ... to Paris Figure 3: Illustration of the TUPA model. Top: parser state (stack, buffer and intermediate graph). Bottom: TUPABiLTSM architecture. Vector representation for the input tokens is computed by two layers of bidirectional LSTMs. The vectors for specific tokens are concatenated with embedding and numeric features from the parser state (for existing edge labels, number of children, etc.), and fed into the MLP for selecting the next transition. according to the perceptron update rule. POS tags and syntactic dependency labels are extracted using spaCy (Honnibal and Johnson, 2015).5 We use the categorical cross-entropy objective function and optimize the NN classifiers with the Adam optimizer (Kingma and Ba, 2014). 4 Experimental Setup Data. We conduct our experiments on the UCCA Wikipedia corpus (henceforth, Wiki), and use the English part of the UCCA Twenty Thousand Leagues Under the Sea English-French parallel corpus (henceforth, 20K Leagues) as outof-domain data.6 Table 1 presents some statistics for the two corpora. We use passages of indices up to 676 of the Wiki corpus as our training set, passages 688–808 as development set, and passages 942–1028 as in-domain t"
P17-1104,W12-3602,0,0.0376842,"Missing"
P17-1104,N15-1080,0,0.0474402,"Missing"
P17-1104,N15-1114,0,0.00764079,"xplore different conversion procedures (Kong et al., 2015) to compare different representations, suggesting ways for a data-driven design of semantic annotation. A parser for UCCA will enable using the framework for new tasks, in addition to existing applications such as machine translation evaluation (Birch et al., 2016). We believe UCCA’s merits in providing a cross-linguistically applicable, broadcoverage annotation will support ongoing efforts to incorporate deeper semantic structures into various applications, such as sentence simplification (Narayan and Gardent, 2014) and summarization (Liu et al., 2015). Acknowledgments This work was supported by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister’s Office, and by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI). The first author was supported by a fellowship from the Edmond and Lily Safra Center for Brain Sciences. We thank Wolfgang Maier, Nathan Schneider, Elior Sulem and the anonymous reviewers for their helpful comments. References Omri Abend and Ari Rappoport. 2013. Universal Conceptual Cognitive Annotation (UCCA). In Proc. of ACL. pages 22"
P17-1104,P15-1116,0,0.289762,"t for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as foll"
P17-1104,W16-0906,0,0.40494,"rsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and co"
P17-1104,P14-1041,0,0.0073383,"mits of its generality. In addition, we will explore different conversion procedures (Kong et al., 2015) to compare different representations, suggesting ways for a data-driven design of semantic annotation. A parser for UCCA will enable using the framework for new tasks, in addition to existing applications such as machine translation evaluation (Birch et al., 2016). We believe UCCA’s merits in providing a cross-linguistically applicable, broadcoverage annotation will support ongoing efforts to incorporate deeper semantic structures into various applications, such as sentence simplification (Narayan and Gardent, 2014) and summarization (Liu et al., 2015). Acknowledgments This work was supported by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister’s Office, and by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI). The first author was supported by a fellowship from the Edmond and Lily Safra Center for Brain Sciences. We thank Wolfgang Maier, Nathan Schneider, Elior Sulem and the anonymous reviewers for their helpful comments. References Omri Abend and Ari Rappoport. 2013. Universal Conceptual Cognitive Annotat"
P17-1104,W03-3017,0,0.099497,"ures (e.g., “John and Mary” in Figure 1c), some multi-word expressions (e.g., “The Haves and the Have Nots”), and prepositional phrases (either the preposition or the head noun can serve as the constituent’s head). To our knowledge, no existing parser supports all structural properties required for UCCA parsing. 3 Transition-based UCCA Parsing We now turn to presenting TUPA. Building on previous work on parsing reentrancies, discontinuities and non-terminal nodes, we define an extended set of transitions and features that supports the conjunction of these properties. Transition-based parsers (Nivre, 2003) scan the text from start to end, and create the parse incrementally by applying a transition at each step to the parser’s state, defined using three data structures: a buffer B of tokens and nodes to be processed, a stack S of nodes currently being processed, and a graph G = (V, E, `) of constructed nodes and edges, where V is the set of nodes, E is the set of edges, and ` : E → L is the label function, L being the set of possible labels. Some states are marked as terminal, meaning that G is the final output. A classifier is used at each step to select the next transition based on features en"
P17-1104,P09-1040,0,0.0295173,"transitions to allow the parser to create remote edges without the possibility of producing invalid graphs. To support the prediction of multiple parents, node and edge transitions leave the stack unchanged, as in other work on transition-based dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015). R EDUCE pops the stack, to allow removing a node once all its edges have been created. To handle discontinuous nodes, S WAP pops the second node on the stack and adds it to the top of the buffer, as with the similarly named transition in previous work (Nivre, 2009; Maier, 2015). Finally, F INISH pops the root node and marks the state as terminal. Classifier. The choice of classifier and feature representation has been shown to play an important role in transition-based parsing (Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016). To investigate the impact of the type of transition classifier in UCCA parsing, we experiment with three different models. 1. Starting with a simple and common choice (e.g., Maier and Lichte, 2016), TUPASparse uses a linear classifier with sparse features, trained with the averaged structured perceptron"
P17-1104,S15-2153,0,0.548009,"Missing"
P17-1104,S14-2008,0,0.193615,"Missing"
P17-1104,D14-1048,0,0.103799,"Missing"
P17-1104,D15-1136,0,0.0427713,"Missing"
P17-1104,W05-1513,0,0.0531703,"or UCCA. Transition-based techniques are a natural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented"
P17-1104,C08-1095,0,0.421265,"nstituency and dependency graph parsing, and further introducing novel transitions and features for UCCA. Transition-based techniques are a natural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and tree"
P17-1104,S14-2034,0,0.0808372,"Missing"
P17-1104,Q14-1016,0,0.0262482,"Banarescu et al., 2013). Alongside recent progress in dependency parsing into projective trees, there is increasing interest in parsing into representations with more general structural properties (see Section 6). One such property is reentrancy, namely the sharing of semantic units between predicates. For instance, in Figure 1a, “John” is an argument of both “gradu1128 ation” and “moved”, yielding a DAG rather than a tree. A second property is discontinuity, as in Figure 1b, where “gave up” forms a discontinuous semantic unit. Discontinuities are pervasive, e.g., with multi-word expressions (Schneider et al., 2014). Finally, unlike most dependency schemes, UCCA uses non-terminal nodes to represent units comprising more than one word. The use of non-terminal nodes is motivated by constructions with no clear head, including coordination structures (e.g., “John and Mary” in Figure 1c), some multi-word expressions (e.g., “The Haves and the Have Nots”), and prepositional phrases (either the preposition or the head noun can serve as the constituent’s head). To our knowledge, no existing parser supports all structural properties required for UCCA parsing. 3 Transition-based UCCA Parsing We now turn to presenti"
P17-1104,W15-3502,1,0.653204,"use discontinuous structures. 1 Introduction Universal Conceptual Cognitive Annotation (UCCA, Abend and Rappoport, 2013) is a crosslinguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010a,b, 2012), and Cognitive Linguistics literature (Croft and Cruse, 2004). It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation by non-experts (assisted by an accessible annotation interface (Abend et al., 2017)), and stability under translation (Sulem et al., 2015). It has also proven useful for machine translation evaluation (Birch et al., 2016). UCCA differs from syntactic schemes in terms of content and formal structure. It exhibits reentrancy, discontinuous nodes and non-terminals, which no single existing parser supports. Lacking a parser, UCCA’s applicability has been so far limited, a gap this work addresses. We present the first UCCA parser, TUPA (Transition-based UCCA Parser), building on recent advances in discontinuous constituency and dependency graph parsing, and further introducing novel transitions and features for UCCA. Transition-based"
P17-1104,K16-1019,0,0.0758887,"Missing"
P17-1104,S14-2027,0,0.0497783,"Missing"
P17-1104,P15-3004,0,0.115722,"Missing"
P17-1104,N15-3006,0,0.0603445,"Missing"
P17-1104,S16-1181,0,0.0214062,"Missing"
P17-1104,P15-2141,0,0.0149878,"of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as traine"
P17-1104,N15-1040,0,0.0220755,"of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as traine"
P17-1104,P15-1095,0,0.0608038,"Missing"
P17-1104,W09-3825,0,0.232693,"ed techniques are a natural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of"
P17-1104,P11-1069,0,0.013152,"hievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as trained parser models, are available at https://github.com/ danielhers/tupa. 1127 Proceedings of the 55th Annual Meeting of the Association f"
P17-1104,D16-1065,0,0.0304795,"o distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is structured as follows: 1 All parsing and conversion code, as well as trained parser models, are available at https://github.com/ danielhers/tupa. 11"
P17-1104,P13-1043,0,0.0342008,"ural starting point for UCCA parsing, given the conceptual similarity of UCCA’s distinctions, centered around predicate-argument structures, to distinctions expressed by dependency schemes, and the achievements of transition-based methods in dependency parsing (Dyer et al., 2015; Andor et al., 2016; Kiperwasser and Goldberg, 2016). We are further motivated by the strength of transition-based methods in related tasks, including dependency graph parsing (Sagae and Tsujii, 2008; Ribeyre et al., 2014; Tokg¨oz and Eryi˘git, 2015), constituency parsing (Sagae and Lavie, 2005; Zhang and Clark, 2009; Zhu et al., 2013; Maier, 2015; Maier and Lichte, 2016), AMR parsing (Wang et al., 2015a,b, 2016; Misra and Artzi, 2016; Goodman et al., 2016; Zhou et al., 2016; Damonte et al., 2017) and CCG parsing (Zhang and Clark, 2011; Ambati et al., 2015, 2016). We evaluate TUPA on the English UCCA corpora, including in-domain and out-of-domain settings. To assess the ability of existing parsers to tackle the task, we develop a conversion procedure from UCCA to bilexical graphs and trees. Results show superior performance for TUPA, demonstrating the effectiveness of the presented approach.1 The rest of the paper is struc"
P17-1104,S14-2081,0,\N,Missing
P17-1104,Q16-1023,0,\N,Missing
P17-1104,J16-4009,0,\N,Missing
P17-1104,D16-1183,0,\N,Missing
P17-4019,P13-1023,1,0.878492,"o non-expert annotators and stability under translation (Sulem et al., 2015). The scheme has recently proven useful for machine translation evaluation (Birch et al., 2016). UCCA emphasizes accessibility and intuitive distinctions in the definition of its categories. UCCAApp complements this effort by offering an intuitive user interface (see Figure 1 and Section 4), which does not require background in formal representation and linguistics, as attested by the steep learning curve of annotators with no background in these fields that used UCCAApp in the annotation of the UCCA Wikipedia corpus (Abend and Rappoport, 2013).2 Aside from the annotation interface, the application includes modules for defining annotation schemes (layers), and for project management. Importantly, the system supports a multi-layered architecture, where the same text passage may be annotated by multiple layers. See Section 3. In order to facilitate the adoption of UCCAApp by other research groups, we built UCCAApp using recent, standard web technology. The server is based on Django, accompanied with a PostWe present UCCAApp, an open-source, flexible web-application for syntactic and semantic phrase-based annotation in general, and for"
P17-4019,W13-2322,0,0.0855499,"AT6 is an open-source web-application with support for distributed collaborative annotation in a variety of annotation formats, including phrase-structure annotation. However, as it is mostly geared towards flat annotations, phrase structure annotation with FLAT is somewhat difficult. SynTree7 also supports phrase-structure annotation, focusing on Chinese. Its applicability, however, has so far been limited due to its documentation being formulated only in Chinese. To the best of our knowledge, none of these applications support DAG annotation, or full keyboard functionality. The AMR editor8 (Banarescu et al., 2013) supports annotation through an interface based on the linearization of the AMR DAGs using the PENMAN notation. The application has a number of dedicated functionalities both for facilitating the annotation process (e.g., nodes and edges can be added either by directly inputting their textual representations, or through more elaborate modals), and for validating that the resulting annotation is indeed well-formed. The editor is also well integrated with the AMR category set, facilitating the navigation through its rich ontology. UCCAApp supports many of the properties required for AMR annotati"
P17-4019,basile-etal-2012-developing,0,0.0151492,"ure 3 presents a screenshot of the admin screens used for managing the different resources. The admin interface is designed to scale to tens of thousands of tasks and passages, and supports advanced pagination, search and filtering capabilities. 4 Annotation Interface The system’s core is its annotation interface. Annotation is carried out over passages, rather than individual sentences. The application thus supports the annotation of inter-sentence relations, which feature in many annotation schemes, including the Penn Discourse Treebank (Miltsakaki et al., 2004), the Groningen Meaning Bank (Basile et al., 2012), and UCCA. UCCAApp fully supports both mouse functionality, for novice annotators, and keyboard functionality, which we have found to be faster, and more suitable for experienced annotators. allows annotators to do. • Tokenization tasks, whose input is a passage of text and whose output is a sequence of tokens. Tokenization tasks are carried out by hand-correcting the output of an automatic tokenizer. • Annotation task, whose input is a set of tokens, and whose output is a DAG structure, whose leaves are the tokens. The task proceeds by iteratively grouping tokens into units. • Review task, w"
P17-4019,D16-1134,1,0.752785,"5), a gap we address in this work. UCCA (Universal Conceptual Cognitive Annotation) is a cross-linguistically applicable semantic representation scheme, building on the Basic Linguistic Theory typological framework (Dixon, 2010a,b, 2012), and Cognitive Linguistics literature (Croft and Cruse, 2004). It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation, accessibility to non-expert annotators and stability under translation (Sulem et al., 2015). The scheme has recently proven useful for machine translation evaluation (Birch et al., 2016). UCCA emphasizes accessibility and intuitive distinctions in the definition of its categories. UCCAApp complements this effort by offering an intuitive user interface (see Figure 1 and Section 4), which does not require background in formal representation and linguistics, as attested by the steep learning curve of annotators with no background in these fields that used UCCAApp in the annotation of the UCCA Wikipedia corpus (Abend and Rappoport, 2013).2 Aside from the annotation interface, the application includes modules for defining annotation schemes (layers), and for project management. Im"
P17-4019,P10-1160,0,0.0730564,"n formal notation. While not all functionalities required for AMR annotation are currently supported in UCCAApp (importantly, the application does not support representations that are not anchored in the words and phrases of the text), future work will address the adaptation of UCCAApp to AMR annotation. A number of recent annotation web applications support dependency annotation, and provide efImplicit Sub-Units. Many annotation schemes encode units that do not correspond to any span of text. Examples include traces, such as in the Penn Treebank (Marcus et al., 1993), and implicit arguments (Gerber and Chai, 2010). UCCAApp supports this functionality by allowing units to have remote sub-units that do not correspond to any span of text. Implicit sub-units may receive categories just like any other unit. Restrictions Module. UCCAApp supports the introduction of restrictions over the joint occurrence of different categories within a layer. Restrictions may either forbid a category from having any children, require that two categories appear together as siblings or children of one another, or forbid them from doing so. Restrictions are validated when a unit is declared finished, or when the passage is subm"
P17-4019,W13-3711,0,0.0627427,"Missing"
P17-4019,J93-2004,0,0.0598354,"hat does not require annotators to be versed in formal notation. While not all functionalities required for AMR annotation are currently supported in UCCAApp (importantly, the application does not support representations that are not anchored in the words and phrases of the text), future work will address the adaptation of UCCAApp to AMR annotation. A number of recent annotation web applications support dependency annotation, and provide efImplicit Sub-Units. Many annotation schemes encode units that do not correspond to any span of text. Examples include traces, such as in the Penn Treebank (Marcus et al., 1993), and implicit arguments (Gerber and Chai, 2010). UCCAApp supports this functionality by allowing units to have remote sub-units that do not correspond to any span of text. Implicit sub-units may receive categories just like any other unit. Restrictions Module. UCCAApp supports the introduction of restrictions over the joint occurrence of different categories within a layer. Restrictions may either forbid a category from having any children, require that two categories appear together as siblings or children of one another, or forbid them from doing so. Restrictions are validated when a unit i"
P17-4019,miltsakaki-etal-2004-penn,0,0.0403127,"f the API is included in the project’s repository. Figure 3 presents a screenshot of the admin screens used for managing the different resources. The admin interface is designed to scale to tens of thousands of tasks and passages, and supports advanced pagination, search and filtering capabilities. 4 Annotation Interface The system’s core is its annotation interface. Annotation is carried out over passages, rather than individual sentences. The application thus supports the annotation of inter-sentence relations, which feature in many annotation schemes, including the Penn Discourse Treebank (Miltsakaki et al., 2004), the Groningen Meaning Bank (Basile et al., 2012), and UCCA. UCCAApp fully supports both mouse functionality, for novice annotators, and keyboard functionality, which we have found to be faster, and more suitable for experienced annotators. allows annotators to do. • Tokenization tasks, whose input is a passage of text and whose output is a sequence of tokens. Tokenization tasks are carried out by hand-correcting the output of an automatic tokenizer. • Annotation task, whose input is a set of tokens, and whose output is a DAG structure, whose leaves are the tokens. The task proceeds by iterat"
P17-4019,W15-3502,1,0.699361,"plications, very few open source applications support phrase-structure annotation (see Section 5), a gap we address in this work. UCCA (Universal Conceptual Cognitive Annotation) is a cross-linguistically applicable semantic representation scheme, building on the Basic Linguistic Theory typological framework (Dixon, 2010a,b, 2012), and Cognitive Linguistics literature (Croft and Cruse, 2004). It has demonstrated applicability to multiple languages, including English, French, German and Czech, support for rapid annotation, accessibility to non-expert annotators and stability under translation (Sulem et al., 2015). The scheme has recently proven useful for machine translation evaluation (Birch et al., 2016). UCCA emphasizes accessibility and intuitive distinctions in the definition of its categories. UCCAApp complements this effort by offering an intuitive user interface (see Figure 1 and Section 4), which does not require background in formal representation and linguistics, as attested by the steep learning curve of annotators with no background in these fields that used UCCAApp in the annotation of the UCCA Wikipedia corpus (Abend and Rappoport, 2013).2 Aside from the annotation interface, the applic"
P17-4019,W16-4011,0,\N,Missing
P18-1016,P13-1023,1,0.898652,"by event boundaries. In this work, which is the first to combine structural semantics and neural methods for TS, we propose an intermediate way for performing sentence splitting, presenting Direct Semantic Splitting (DSS), a simple and efficient algorithm based on a semantic parser which supports the direct decomposition of the sentence into its main semantic constituents. After splitting, NMT-based simplification is performed, using the NTS system. We show that the resulting system outperforms H YBRID in both automatic and human evaluation. We use the UCCA scheme for semantic representation (Abend and Rappoport, 2013), where the semantic units are anchored in the text, which simplifies the splitting operation. We further leverage the explicit distinction in UCCA between types of Scenes (events), applying a specific rule for each of the cases. Nevertheless, the DSS approach can be adapted to other semantic schemes, like AMR (Banarescu et al., 2013). We collect human judgments for multiple variants of our system, its sub-components, H YBRID and similar systems that use phrase-based MT. This results in a sizable human evaluation benchmark, which includes 28 systems, totaling at 1960 complex-simple sentence pa"
P18-1016,P17-4019,1,0.74298,"the deletion module, and trained on WEB-SPLIT (Narayan et al., 2017). DSS gets a higher BLEU score (46.45 vs. 39.97) and performs more splittings (number of output sentences per input sentence of 1.73 vs. 1.26). 7 Additional Experiments Replacing the parser by manual annotation. In order to isolate the influence of the parser on the results, we implement a semi-automatic version of the semantic component, which uses manual UCCA annotation instead of the parser, focusing of the first 70 sentences of the test corpus. We employ a single expert UCCA annotator and use the UCCAApp annotation tool (Abend et al., 2017). Results are presented in Table 6, for both SENTS and SEMoses. In the case of SEMoses, meaning preservation is improved when manual UCCA annotation is used. On the other hand, simplicity degrades, possibly due to the larger number of Scenes marked by the human annotator (TUPA tends to under-predict Scenes). This effect doesn’t 14 We use the evaluation tools provided in https:// github.com/danielhers/ucca, ignoring 9 sentences for which different tokenizations of proper nouns are used in the automatic and manual parsing. 169 Identity G M S StS In return, Rollo swore fealty to Charles, converte"
P18-1016,W10-1607,0,0.0610648,"Missing"
P18-1016,N13-1092,0,0.0245249,"Missing"
P18-1016,P17-1017,0,0.0256153,"The eventwise simplification one, which separates events to separate output sentences, is similar to our semantic component. Differences are in that we use a single semantic representation for defining the rules (rather than a combination of semantic and syntactic criteria), and avoid the need for complex rules for retaining grammaticality by using a subsequent neural component. Split and Rephrase. Narayan et al. (2017) recently proposed the Split and Rephrase task, focusing on sentence splitting. For this purpose they presented a specialized parallel corpus, derived from the WebNLG dataset (Gardent et al., 2017). The latter is obtained from the DBPedia knowledge base (Mendes et al., 2012) using content selection and crowdsourcing, and is annotated with semantic triplets of subject-relation-object, obtained semi-automatically. They experimented with five systems, including one similar to H YBRID , as well as sequence-to-sequence methods for generating sentences from the source text and its semantic forms. The present paper tackles both structural and lexical simplification, and examines the effect of sentence splitting on the subsequent application of a neural system, in terms of its tendency to perfo"
P18-1016,I17-1030,0,0.453564,"ere Levenshtein distance to the source is used for re-ranking to overcome conservatism. The NTS NMT-based system (Nisioi et al., 2017) (henceforth, N17) reported superior performance over PBMT in terms of BLEU and human evaluation scores, and serves as a component in our system (see Section 4). Zhang et al. (2017) took a similar approach, adding lexical constraints to an NMT model. Zhang and Lapata (2017) combined NMT with reinforcement learning, using SARI (Xu et al., 2016), BLEU, and cosine similarity to the source as the reward. None of these models explicitly addresses sentence splitting. Alva-Manchego et al. (2017) proposed to reduce conservatism, observed in PBMT and NMT systems, by first identifying simplification operations in a parallel corpus and then using sequencelabeling to perform the simplification. However, they did not address common structural operations, such as sentence splitting, and claimed that their method is not applicable to them. Xu et al. (2016) used Syntax-based Machine Translation (SBMT) for sentence simplification, using a large scale paraphrase dataset (Ganitketitch et al., 2013) for training. While it does not target structural simplification, we include it in our evaluation"
P18-1016,R13-2011,0,0.0567807,"Missing"
P18-1016,W13-2322,0,0.105637,"into its main semantic constituents. After splitting, NMT-based simplification is performed, using the NTS system. We show that the resulting system outperforms H YBRID in both automatic and human evaluation. We use the UCCA scheme for semantic representation (Abend and Rappoport, 2013), where the semantic units are anchored in the text, which simplifies the splitting operation. We further leverage the explicit distinction in UCCA between types of Scenes (events), applying a specific rule for each of the cases. Nevertheless, the DSS approach can be adapted to other semantic schemes, like AMR (Banarescu et al., 2013). We collect human judgments for multiple variants of our system, its sub-components, H YBRID and similar systems that use phrase-based MT. This results in a sizable human evaluation benchmark, which includes 28 systems, totaling at 1960 complex-simple sentence pairs, each annotated by three annotators using four criteria.1 This benchmark will support the future analysis of TS systems, and evaluation practices. Previous work is discussed in §2, the semantic and NMT components we use in §3 and §4 respectively. The experimental setup is detailed in §5. Our main results are presented in §6, while"
P18-1016,P17-1104,1,0.57328,"64 Figure 1: Example applications of rules 1 (Figure 1a) and 2 (Figure 1b). In both cases, the original sentence, the semantic parse, the extracted Scenes with the required modifications, and the output of the rules are presented top to bottom. The UCCA categories used are: Parallel Scene (H), Linker (L), Participant (A), Process/State (P/S), Center (C), Elaborator (E), Relator (R). minimal center of a UCCA unit u to be the UCCA graph’s leaf reached by starting from u and iteratively selecting the child tagged as Center. For generating UCCA’s structures we use TUPA, a transition-based parser (Hershcovich et al., 2017) (specifically, the TUPABiLST M model). TUPA uses an expressive set of transitions, able to support all structural properties required by the UCCA scheme. Its transition classifier is based on an MLP that receives a BiLSTM encoding of elements in the parser state (buffer, stack and intermediate graph), given word embeddings and other features. A Scene is UCCA’s notion of an event or a frame, and is a unit that corresponds to a movement, an action or a state which persists in time. Every Scene contains one main relation, which can be either a Process or a State. Scenes contain one or more Parti"
P18-1016,P11-2087,0,0.0245507,"tient). Semantic annotation is used on the source side in both training and test. Lexical simplification is performed using the Moses system. H YBRID is the most similar system to ours architecturally, in that it uses a combination of a semantic structural component and an MT component. Narayan and Gardent (2016) proposed instead an unsupervised pipeline, where sentences are split based on a probabilistic model trained on the semantic structures of Simple Wikipedia as well as a language model trained on the same corpus. Lexical simplification is there performed using the unsupervised model of Biran et al. (2011). As their BLEU and adequacy scores are lower than H YBRID’s, we use the latter for comparison. ˇ Stajner and Glavaˇs (2017) combined rule-based simplification conditioned on event extraction, to3 3.1 Direct Semantic Splitting Semantic Representation UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a, 2012; Langacker, 2008). It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA has been shown to be preserved remarkabl"
P18-1016,N15-1022,0,0.0124421,"Sc1 |· · · |Scn i=1 where S −A is S without the unit A. For example, this rule converts the sentence “He observed the planet which has 14 known satellites” to “He observed the planet |Planet has 14 known satellites.”. Article regeneration is not covered by the rule, as its output is directly fed into the NMT component. After the extraction of Parallel Scenes and Elaborator Scenes, the resulting simplified Parallel Scenes are placed before the Elaborator Scenes. See Figure 1. 4 Experimental Setup Neural component. We use the NTS-w2v model6 provided by N17, obtained by training on the corpus of Hwang et al. (2015) and tuning on the corpus of Xu et al. (2016). The training set is based on manual and automatic alignments between standard English Wikipedia and Simple English Wikipedia, including both good matches and partial matches whose similarity score is above the 0.45 scale threshold (Hwang et al., 2015). The total size of the training set is about 280K aligned sentences, of which 150K sentences are full matches and 130K are partial matches.7 Neural Component The split sentences are run through the NTS stateof-the-art neural TS system (Nisioi et al., 2017), built using the OpenNMT neural machine tran"
P18-1016,D16-1134,1,0.637953,"mbined rule-based simplification conditioned on event extraction, to3 3.1 Direct Semantic Splitting Semantic Representation UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a, 2012; Langacker, 2008). It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA has been shown to be preserved remarkably well across translations (Sulem et al., 2015) and has also been successfully used for the evaluation of machine translation (Birch et al., 2016) and, recently, for the evaluation of TS (Sulem et al., 2018) and grammatical error correction (Choshen and Abend, 2018). Formally, UCCA structures are directed acyclic graphs whose nodes (or units) correspond either to the leaves of the graph or to several elements viewed as a single entity according to some semantic or cognitive consideration. 164 Figure 1: Example applications of rules 1 (Figure 1a) and 2 (Figure 1b). In both cases, the original sentence, the semantic parse, the extracted Scenes with the required modifications, and the output of the rules are presented top to bottom. The UC"
P18-1016,P13-1151,0,0.0245926,"ools and the SBMT-SARI system.) 4 https://github.com/danielhers/tupa 5 http://www.cs.huji.ac.il/˜oabend/ ucca.html 6 https://github.com/senisioi/ NeuralTextSimplification 7 We also considered the default initialization for the neural component, using the NTS model without word embeddings. Experimenting on the tuning set, the w2v approach got higher BLEU and SARI scores (for h1 and h4 respectively) than the default approach. 166 mented by Zhang and Lapata (2017).8 We use the released output of H YBRID, trained on a corpus extracted from Wikipedia, which includes the aligned sentence pairs from Kauchak (2013), the aligned revision sentence pairs in Woodsend and Lapata (2011), and the PWKP corpus, totaling about 296K sentence pairs. The tuning set is the same as for the above systems. In order to isolate the effect of NMT, we also implement SEMoses, where the neural-based component is replaced by the phrase-based MT system Moses,9 which is also used in H YBRID. The training, tuning and test sets are the same as in the case of SENTS. MGIZA10 is used for word alignment. The KenLM language model is trained using the target side of the training corpus. Human evaluation. Human evaluation is carried out"
P18-1016,N18-2020,1,0.633278,"entation UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a, 2012; Langacker, 2008). It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA has been shown to be preserved remarkably well across translations (Sulem et al., 2015) and has also been successfully used for the evaluation of machine translation (Birch et al., 2016) and, recently, for the evaluation of TS (Sulem et al., 2018) and grammatical error correction (Choshen and Abend, 2018). Formally, UCCA structures are directed acyclic graphs whose nodes (or units) correspond either to the leaves of the graph or to several elements viewed as a single entity according to some semantic or cognitive consideration. 164 Figure 1: Example applications of rules 1 (Figure 1a) and 2 (Figure 1b). In both cases, the original sentence, the semantic parse, the extracted Scenes with the required modifications, and the output of the rules are presented top to bottom. The UCCA categories used are: Parallel Scene (H), Linker (L), Participant (A), Process/State (P/S), Center (C), Elaborator (E)"
P18-1016,P17-4012,0,0.101354,"corpus of Xu et al. (2016). The training set is based on manual and automatic alignments between standard English Wikipedia and Simple English Wikipedia, including both good matches and partial matches whose similarity score is above the 0.45 scale threshold (Hwang et al., 2015). The total size of the training set is about 280K aligned sentences, of which 150K sentences are full matches and 130K are partial matches.7 Neural Component The split sentences are run through the NTS stateof-the-art neural TS system (Nisioi et al., 2017), built using the OpenNMT neural machine translation framework (Klein et al., 2017). The architecture includes two LSTM layers, with hidden states of 500 units in each, as well as global attention combined with input feeding (Luong et al., 2015). Training is done with a 0.3 dropout probability (Srivastava et al., 2014). This model uses alignment probabilities between the predictions and the original sentences, rather than characterbased models, to retrieve the original words. We here consider the w2v initialization for NTS (N17), where word2vec embeddings of size 300 are trained on Google News (Mikolov et al., 2013a) and local embeddings of size 200 are trained on the traini"
P18-1016,P07-2045,0,0.00578241,"tems, totaling at 1960 complex-simple sentence pairs, each annotated by three annotators using four criteria.1 This benchmark will support the future analysis of TS systems, and evaluation practices. Previous work is discussed in §2, the semantic and NMT components we use in §3 and §4 respectively. The experimental setup is detailed in §5. Our main results are presented in §6, while §7 presents a more detailed analysis of the system’s sub-components and related settings. 2 highly similar to the target. Other PBMT for TS systems include the work of Coster and Kauchak (2011b), which uses Moses (Koehn et al., 2007), the work of Coster and Kauchak (2011a), where the model is extended to include deletion, and PBMT-R (Wubben et al., 2012), where Levenshtein distance to the source is used for re-ranking to overcome conservatism. The NTS NMT-based system (Nisioi et al., 2017) (henceforth, N17) reported superior performance over PBMT in terms of BLEU and human evaluation scores, and serves as a component in our system (see Section 4). Zhang et al. (2017) took a similar approach, adding lexical constraints to an NMT model. Zhang and Lapata (2017) combined NMT with reinforcement learning, using SARI (Xu et al.,"
P18-1016,W11-1601,0,0.186464,"an evaluation benchmark, which includes 28 systems, totaling at 1960 complex-simple sentence pairs, each annotated by three annotators using four criteria.1 This benchmark will support the future analysis of TS systems, and evaluation practices. Previous work is discussed in §2, the semantic and NMT components we use in §3 and §4 respectively. The experimental setup is detailed in §5. Our main results are presented in §6, while §7 presents a more detailed analysis of the system’s sub-components and related settings. 2 highly similar to the target. Other PBMT for TS systems include the work of Coster and Kauchak (2011b), which uses Moses (Koehn et al., 2007), the work of Coster and Kauchak (2011a), where the model is extended to include deletion, and PBMT-R (Wubben et al., 2012), where Levenshtein distance to the source is used for re-ranking to overcome conservatism. The NTS NMT-based system (Nisioi et al., 2017) (henceforth, N17) reported superior performance over PBMT in terms of BLEU and human evaluation scores, and serves as a component in our system (see Section 4). Zhang et al. (2017) took a similar approach, adding lexical constraints to an NMT model. Zhang and Lapata (2017) combined NMT with reinf"
P18-1016,P11-2117,0,0.0596756,"an evaluation benchmark, which includes 28 systems, totaling at 1960 complex-simple sentence pairs, each annotated by three annotators using four criteria.1 This benchmark will support the future analysis of TS systems, and evaluation practices. Previous work is discussed in §2, the semantic and NMT components we use in §3 and §4 respectively. The experimental setup is detailed in §5. Our main results are presented in §6, while §7 presents a more detailed analysis of the system’s sub-components and related settings. 2 highly similar to the target. Other PBMT for TS systems include the work of Coster and Kauchak (2011b), which uses Moses (Koehn et al., 2007), the work of Coster and Kauchak (2011a), where the model is extended to include deletion, and PBMT-R (Wubben et al., 2012), where Levenshtein distance to the source is used for re-ranking to overcome conservatism. The NTS NMT-based system (Nisioi et al., 2017) (henceforth, N17) reported superior performance over PBMT in terms of BLEU and human evaluation scores, and serves as a component in our system (see Section 4). Zhang et al. (2017) took a similar approach, adding lexical constraints to an NMT model. Zhang and Lapata (2017) combined NMT with reinf"
P18-1016,N03-1017,0,0.0466807,"syntactic ones reduces the number of rules. For example, relative clauses and appositives can correspond to the same semantic category. In syntax-based splitting, a generation module is sometimes added after the split (Siddharthan, 2004), addressing issues such as reordering and determiner selection. In our model, no explicit regeneration is applied to the split sentences, which are fed directly to an NMT system. ˇ Glavaˇs and Stajner (2013) used a rule-based system conditioned on event extraction and syntax Related Work MT-based sentence simplification. Phrasebased Machine Translation (PBMT; Koehn et al., 2003) was first used for TS by Specia (2010), who showed good performance on lexical simplification and simple rewriting, but under-prediction ˇ of other operations. Stajner et al. (2015) took a similar approach, finding that it is beneficial to use training data where the source side is 1 The benchmark can be found in https://github. com/eliorsulem/simplification-acl2018. 163 gether with an unsupervised lexical simplifier. They tackle a different setting, and aim to simplify texts (rather than sentences), by allowing the deletion of entire input sentences. for defining two simplification models. T"
P18-1016,W02-0109,0,0.0690134,"Missing"
P18-1016,D15-1166,0,0.139512,"Missing"
P18-1016,seretan-2012-acquisition,0,0.0329057,"Missing"
P18-1016,mendes-etal-2012-dbpedia,0,0.012139,"ntences, is similar to our semantic component. Differences are in that we use a single semantic representation for defining the rules (rather than a combination of semantic and syntactic criteria), and avoid the need for complex rules for retaining grammaticality by using a subsequent neural component. Split and Rephrase. Narayan et al. (2017) recently proposed the Split and Rephrase task, focusing on sentence splitting. For this purpose they presented a specialized parallel corpus, derived from the WebNLG dataset (Gardent et al., 2017). The latter is obtained from the DBPedia knowledge base (Mendes et al., 2012) using content selection and crowdsourcing, and is annotated with semantic triplets of subject-relation-object, obtained semi-automatically. They experimented with five systems, including one similar to H YBRID , as well as sequence-to-sequence methods for generating sentences from the source text and its semantic forms. The present paper tackles both structural and lexical simplification, and examines the effect of sentence splitting on the subsequent application of a neural system, in terms of its tendency to perform other simplification operations. For this purpose, we adopt a semantic corp"
P18-1016,E14-1076,0,0.17511,"), confirm that the references are indeed simpler than the source, indicating that the observed conservatism is excessive. Our methods for performing sentence splitting as pre-processing allows the TS system to perform other structural (e.g. deletions) and lexical (e.g. word substitutions) operations, thus increasing both structural and lexical simplicity. For combining linguistically informed sentence splitting with data-driven TS, two main methods have been proposed. The first involves handcrafted syntactic rules, whose compilation and validation are laborious (Shardlow, 2014). For example, Siddharthan and Angrosh (2014) used 111 rules for relative clauses, appositions, subordination and coordination. Moreover, syntactic splitting rules, which form a substantial part of the rules, are usually language specific, requiring the development of new rules when ported to other languages (Alu´ısio and Gasperin, 2010; Seretan, 2012; Hung et al., 2012; Barlacchi and Tonelli, 2013, for Portuguese, French, Vietnamese, and Italian respectively). The second method uses linguistic information for detecting potential splitting points, while splitting probabilities are learned usSentence splitting is a major simplification op"
P18-1016,W14-5603,0,0.0667224,"ation suffers from a considerable disadvantage in that they are overconservative, often failing to modify the source in any way. Splitting based on semantic parsing, as proposed here, alleviates this issue. Extensive automatic and human evaluation shows that the proposed method compares favorably to the stateof-the-art in combined lexical and structural simplification. 1 Introduction Text Simplification (TS) is generally defined as the conversion of a sentence into one or more simpler sentences. It has been shown useful both as a preprocessing step for tasks such as Machine ˇ Translation (MT; Mishra et al., 2014; Stajner and Popovi´c, 2016) and relation extraction (Niklaus et al., 2016), as well as for developing reading aids, e.g. for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). TS includes both structural and lexical operations. The main structural simplification operation is sentence splitting, namely rewriting a single sentence into multiple sentences while preserving its meaning. While recent improvement in TS has been achieved by the use of neural MT (NMT) approaches (Nisioi et al., 2017; Zhang et al., 2017; Zhang and Lapata, 2017), where TS is consid162"
P18-1016,W11-2802,0,0.0331345,"to perform the simplification. However, they did not address common structural operations, such as sentence splitting, and claimed that their method is not applicable to them. Xu et al. (2016) used Syntax-based Machine Translation (SBMT) for sentence simplification, using a large scale paraphrase dataset (Ganitketitch et al., 2013) for training. While it does not target structural simplification, we include it in our evaluation for completeness. Structural sentence simplification. Syntactic hand-crafted sentence splitting rules were proposed by Chandrasekar et al. (1996), Siddharthan (2002), Siddhathan (2011) in the context of rulebased TS. The rules separate relative clauses and coordinated clauses and un-embed appositives. In our method, the use of semantic distinctions instead of syntactic ones reduces the number of rules. For example, relative clauses and appositives can correspond to the same semantic category. In syntax-based splitting, a generation module is sometimes added after the split (Siddharthan, 2004), addressing issues such as reordering and determiner selection. In our model, no explicit regeneration is applied to the split sentences, which are fed directly to an NMT system. ˇ Gla"
P18-1016,P14-1041,0,0.562608,"s. The main structural simplification operation is sentence splitting, namely rewriting a single sentence into multiple sentences while preserving its meaning. While recent improvement in TS has been achieved by the use of neural MT (NMT) approaches (Nisioi et al., 2017; Zhang et al., 2017; Zhang and Lapata, 2017), where TS is consid162 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 162–173 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ing a parallel corpus. For example, in the system of Narayan and Gardent (2014) (henceforth, H YBRID), the state-of-the-art for joint structural and lexical TS, potential splitting points are determined by event boundaries. In this work, which is the first to combine structural semantics and neural methods for TS, we propose an intermediate way for performing sentence splitting, presenting Direct Semantic Splitting (DSS), a simple and efficient algorithm based on a semantic parser which supports the direct decomposition of the sentence into its main semantic constituents. After splitting, NMT-based simplification is performed, using the NTS system. We show that the resul"
P18-1016,W06-3104,0,0.0216327,"that can be easily integrated in any simplification system. Another difference is that the semantic forms in Split and Rephrase are derived semi-automatically (during corpus compilation), while we automatically extract the semantic form, using a UCCA parser. Combined structural and lexical TS. Earlier TS models used syntactic information for splitting. Zhu et al. (2010) used syntactic information on the source side, based on the SBMT model of Yamada and Knight (2001). Syntactic structures were used on both sides in the model of Woodsend and Lapata (2011), based on a quasi-synchronous grammar (Smith and Eisner, 2006), which resulted in 438 learned splitting rules. The model of Siddharthan and Angrosh (2014) is similar to ours in that it combines linguistic rules for structural simplification and statistical methods for lexical simplification. However, we use 2 semantic splitting rules instead of their 26 syntactic rules for relative clauses and appositions, and 85 syntactic rules for subordination and coordination. Narayan and Gardent (2014) argued that syntactic structures do not always capture the semantic arguments of a frame, which may result in wrong splitting boundaries. Consequently, they proposed"
P18-1016,W16-6620,0,0.0603997,"litting boundaries. Consequently, they proposed a supervised system (H YBRID) that uses semantic structures (Discourse Semantic Representations, (Kamp, 1981)) for sentence splitting and deletion. Splitting candidates are pairs of event variables associated with at least one core thematic role (e.g., agent or patient). Semantic annotation is used on the source side in both training and test. Lexical simplification is performed using the Moses system. H YBRID is the most similar system to ours architecturally, in that it uses a combination of a semantic structural component and an MT component. Narayan and Gardent (2016) proposed instead an unsupervised pipeline, where sentences are split based on a probabilistic model trained on the semantic structures of Simple Wikipedia as well as a language model trained on the same corpus. Lexical simplification is there performed using the unsupervised model of Biran et al. (2011). As their BLEU and adequacy scores are lower than H YBRID’s, we use the latter for comparison. ˇ Stajner and Glavaˇs (2017) combined rule-based simplification conditioned on event extraction, to3 3.1 Direct Semantic Splitting Semantic Representation UCCA (Universal Cognitive Conceptual Annotat"
P18-1016,D17-1064,0,0.0398096,"d lexical simplifier. They tackle a different setting, and aim to simplify texts (rather than sentences), by allowing the deletion of entire input sentences. for defining two simplification models. The eventwise simplification one, which separates events to separate output sentences, is similar to our semantic component. Differences are in that we use a single semantic representation for defining the rules (rather than a combination of semantic and syntactic criteria), and avoid the need for complex rules for retaining grammaticality by using a subsequent neural component. Split and Rephrase. Narayan et al. (2017) recently proposed the Split and Rephrase task, focusing on sentence splitting. For this purpose they presented a specialized parallel corpus, derived from the WebNLG dataset (Gardent et al., 2017). The latter is obtained from the DBPedia knowledge base (Mendes et al., 2012) using content selection and crowdsourcing, and is annotated with semantic triplets of subject-relation-object, obtained semi-automatically. They experimented with five systems, including one similar to H YBRID , as well as sequence-to-sequence methods for generating sentences from the source text and its semantic forms. Th"
P18-1016,C16-2036,0,0.0172535,"vative, often failing to modify the source in any way. Splitting based on semantic parsing, as proposed here, alleviates this issue. Extensive automatic and human evaluation shows that the proposed method compares favorably to the stateof-the-art in combined lexical and structural simplification. 1 Introduction Text Simplification (TS) is generally defined as the conversion of a sentence into one or more simpler sentences. It has been shown useful both as a preprocessing step for tasks such as Machine ˇ Translation (MT; Mishra et al., 2014; Stajner and Popovi´c, 2016) and relation extraction (Niklaus et al., 2016), as well as for developing reading aids, e.g. for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). TS includes both structural and lexical operations. The main structural simplification operation is sentence splitting, namely rewriting a single sentence into multiple sentences while preserving its meaning. While recent improvement in TS has been achieved by the use of neural MT (NMT) approaches (Nisioi et al., 2017; Zhang et al., 2017; Zhang and Lapata, 2017), where TS is consid162 Proceedings of the 56th Annual Meeting of the Association for Computational"
P18-1016,P17-2014,0,0.644536,"s a preprocessing step for tasks such as Machine ˇ Translation (MT; Mishra et al., 2014; Stajner and Popovi´c, 2016) and relation extraction (Niklaus et al., 2016), as well as for developing reading aids, e.g. for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). TS includes both structural and lexical operations. The main structural simplification operation is sentence splitting, namely rewriting a single sentence into multiple sentences while preserving its meaning. While recent improvement in TS has been achieved by the use of neural MT (NMT) approaches (Nisioi et al., 2017; Zhang et al., 2017; Zhang and Lapata, 2017), where TS is consid162 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 162–173 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ing a parallel corpus. For example, in the system of Narayan and Gardent (2014) (henceforth, H YBRID), the state-of-the-art for joint structural and lexical TS, potential splitting points are determined by event boundaries. In this work, which is the first to combine structural semantics and neural methods for TS, we prop"
P18-1016,P15-2135,0,0.0658153,"le is sometimes added after the split (Siddharthan, 2004), addressing issues such as reordering and determiner selection. In our model, no explicit regeneration is applied to the split sentences, which are fed directly to an NMT system. ˇ Glavaˇs and Stajner (2013) used a rule-based system conditioned on event extraction and syntax Related Work MT-based sentence simplification. Phrasebased Machine Translation (PBMT; Koehn et al., 2003) was first used for TS by Specia (2010), who showed good performance on lexical simplification and simple rewriting, but under-prediction ˇ of other operations. Stajner et al. (2015) took a similar approach, finding that it is beneficial to use training data where the source side is 1 The benchmark can be found in https://github. com/eliorsulem/simplification-acl2018. 163 gether with an unsupervised lexical simplifier. They tackle a different setting, and aim to simplify texts (rather than sentences), by allowing the deletion of entire input sentences. for defining two simplification models. The eventwise simplification one, which separates events to separate output sentences, is similar to our semantic component. Differences are in that we use a single semantic represent"
P18-1016,P02-1040,0,0.103091,"est corpus.13 The resulting corpus, totaling 1960 sentence pairs, each annotated by 3 annotators, also include Additional baselines. We report human and automatic evaluation scores for Identity (where the output is identical to the input), for Simple Wikipedia where the output is the corresponding aligned sentence in the PWKP corpus, and for the SBMT-SARI system, tuned against SARI (Xu et al., 2016), which maximized the SARI score on this test set in previous works (Nisioi et al., 2017; Zhang and Lapata, 2017). Automatic evaluation. The automatic metrics used for the evaluation are: (1) BLEU (Papineni et al., 2002) (2) SARI (System output Against References and against the Input sentence; Xu et al., 2016), which compares the n-grams of the system output with those of the input and the human references, separately evaluating the quality of words that are added, deleted and kept by the systems. (3) Fadd : the addition component of the SARI score (F-score); (4) Fkeep : the keeping component of the SARI score (F-score); (5) Pdel : the deletion component of the SARI score (precision).11 Each metric is computed against the 8 available references. We also assess system conservatism, reporting the percentage of"
P18-1016,W16-3411,0,0.0971014,"Missing"
P18-1016,N18-1063,1,0.890064,"ion, to3 3.1 Direct Semantic Splitting Semantic Representation UCCA (Universal Cognitive Conceptual Annotation; Abend and Rappoport, 2013) is a semantic annotation scheme rooted in typological and cognitive linguistic theory (Dixon, 2010b,a, 2012; Langacker, 2008). It aims to represent the main semantic phenomena in the text, abstracting away from syntactic forms. UCCA has been shown to be preserved remarkably well across translations (Sulem et al., 2015) and has also been successfully used for the evaluation of machine translation (Birch et al., 2016) and, recently, for the evaluation of TS (Sulem et al., 2018) and grammatical error correction (Choshen and Abend, 2018). Formally, UCCA structures are directed acyclic graphs whose nodes (or units) correspond either to the leaves of the graph or to several elements viewed as a single entity according to some semantic or cognitive consideration. 164 Figure 1: Example applications of rules 1 (Figure 1a) and 2 (Figure 1b). In both cases, the original sentence, the semantic parse, the extracted Scenes with the required modifications, and the output of the rules are presented top to bottom. The UCCA categories used are: Parallel Scene (H), Linker (L), Parti"
P18-1016,D11-1038,0,0.63656,"opt a semantic corpus-independent approach for sentence splitting that can be easily integrated in any simplification system. Another difference is that the semantic forms in Split and Rephrase are derived semi-automatically (during corpus compilation), while we automatically extract the semantic form, using a UCCA parser. Combined structural and lexical TS. Earlier TS models used syntactic information for splitting. Zhu et al. (2010) used syntactic information on the source side, based on the SBMT model of Yamada and Knight (2001). Syntactic structures were used on both sides in the model of Woodsend and Lapata (2011), based on a quasi-synchronous grammar (Smith and Eisner, 2006), which resulted in 438 learned splitting rules. The model of Siddharthan and Angrosh (2014) is similar to ours in that it combines linguistic rules for structural simplification and statistical methods for lexical simplification. However, we use 2 semantic splitting rules instead of their 26 syntactic rules for relative clauses and appositions, and 85 syntactic rules for subordination and coordination. Narayan and Gardent (2014) argued that syntactic structures do not always capture the semantic arguments of a frame, which may res"
P18-1016,P12-1107,0,0.425457,"Missing"
P18-1016,Q15-1021,0,0.319562,"Missing"
P18-1016,Q16-1029,0,0.531781,"al., 2007), the work of Coster and Kauchak (2011a), where the model is extended to include deletion, and PBMT-R (Wubben et al., 2012), where Levenshtein distance to the source is used for re-ranking to overcome conservatism. The NTS NMT-based system (Nisioi et al., 2017) (henceforth, N17) reported superior performance over PBMT in terms of BLEU and human evaluation scores, and serves as a component in our system (see Section 4). Zhang et al. (2017) took a similar approach, adding lexical constraints to an NMT model. Zhang and Lapata (2017) combined NMT with reinforcement learning, using SARI (Xu et al., 2016), BLEU, and cosine similarity to the source as the reward. None of these models explicitly addresses sentence splitting. Alva-Manchego et al. (2017) proposed to reduce conservatism, observed in PBMT and NMT systems, by first identifying simplification operations in a parallel corpus and then using sequencelabeling to perform the simplification. However, they did not address common structural operations, such as sentence splitting, and claimed that their method is not applicable to them. Xu et al. (2016) used Syntax-based Machine Translation (SBMT) for sentence simplification, using a large sca"
P18-1016,P01-1067,0,0.386144,"rms of its tendency to perform other simplification operations. For this purpose, we adopt a semantic corpus-independent approach for sentence splitting that can be easily integrated in any simplification system. Another difference is that the semantic forms in Split and Rephrase are derived semi-automatically (during corpus compilation), while we automatically extract the semantic form, using a UCCA parser. Combined structural and lexical TS. Earlier TS models used syntactic information for splitting. Zhu et al. (2010) used syntactic information on the source side, based on the SBMT model of Yamada and Knight (2001). Syntactic structures were used on both sides in the model of Woodsend and Lapata (2011), based on a quasi-synchronous grammar (Smith and Eisner, 2006), which resulted in 438 learned splitting rules. The model of Siddharthan and Angrosh (2014) is similar to ours in that it combines linguistic rules for structural simplification and statistical methods for lexical simplification. However, we use 2 semantic splitting rules instead of their 26 syntactic rules for relative clauses and appositions, and 85 syntactic rules for subordination and coordination. Narayan and Gardent (2014) argued that sy"
P18-1016,D17-1062,0,0.483685,"Machine ˇ Translation (MT; Mishra et al., 2014; Stajner and Popovi´c, 2016) and relation extraction (Niklaus et al., 2016), as well as for developing reading aids, e.g. for people with dyslexia (Rello et al., 2013) or non-native speakers (Siddharthan, 2002). TS includes both structural and lexical operations. The main structural simplification operation is sentence splitting, namely rewriting a single sentence into multiple sentences while preserving its meaning. While recent improvement in TS has been achieved by the use of neural MT (NMT) approaches (Nisioi et al., 2017; Zhang et al., 2017; Zhang and Lapata, 2017), where TS is consid162 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 162–173 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ing a parallel corpus. For example, in the system of Narayan and Gardent (2014) (henceforth, H YBRID), the state-of-the-art for joint structural and lexical TS, potential splitting points are determined by event boundaries. In this work, which is the first to combine structural semantics and neural methods for TS, we propose an intermediate way for performing senten"
P18-1016,C10-1152,0,0.774435,"nes the effect of sentence splitting on the subsequent application of a neural system, in terms of its tendency to perform other simplification operations. For this purpose, we adopt a semantic corpus-independent approach for sentence splitting that can be easily integrated in any simplification system. Another difference is that the semantic forms in Split and Rephrase are derived semi-automatically (during corpus compilation), while we automatically extract the semantic form, using a UCCA parser. Combined structural and lexical TS. Earlier TS models used syntactic information for splitting. Zhu et al. (2010) used syntactic information on the source side, based on the SBMT model of Yamada and Knight (2001). Syntactic structures were used on both sides in the model of Woodsend and Lapata (2011), based on a quasi-synchronous grammar (Smith and Eisner, 2006), which resulted in 438 learned splitting rules. The model of Siddharthan and Angrosh (2014) is similar to ours in that it combines linguistic rules for structural simplification and statistical methods for lexical simplification. However, we use 2 semantic splitting rules instead of their 26 syntactic rules for relative clauses and appositions, a"
P18-1018,P17-4019,1,0.730567,"ittle Prince, both to assess whether the scheme was applicable without major guidelines changes and to prepare the annotators for this genre. For the final annotation study, we chose chapters 4 and 5, in which 242 markables of 52 types were identified heuristically (§6.2). The types of, to, in, as, from, and for, as well as possessives, occurred at least 10 times. Annotators had the option to mark units as false positives using special labels (see §4) in addition to expressing uncertainty about the unit. For the annotation process, we adapted the open source web-based annotation tool UCCAApp (Abend et al., 2017) to our workflow, by extending it with a type-sensitive ranking module for the list of categories presented to the annotators. Annotators. Five annotators (A, B, C, D, E), all authors of this paper, took part in this study. All are computational linguistics researchers with advanced training in linguistics. Their involvement in the development of the scheme falls on a spectrum, with annotator A being the most active figure in guidelines development, and annotator E not being Labels involved in developing the guidelines and learning the scheme solely from reading the manual. Annotators A, B, an"
P18-1018,W17-6901,0,0.0631207,"OPIC ) flows from O RIG INATOR to R ECIPIENT , perhaps via an I NSTRU MENT . For AGENT , C O -AGENT , E XPERIENCER , O RIGINATOR, R ECIPIENT, B ENEFICIARY, P OS SESSOR, and S OCIAL R EL, the object of the preposition is prototypically animate. Because prepositions and possessives cover a vast swath of semantic space, limiting ourselves to 50 categories means we need to address a great many nonprototypical, borderline, and special cases. We have done so in a 75-page annotation manual with over 400 example sentences (Schneider et al., 2018). Finally, we note that the Universal Semantic Tagset (Abzianidze and Bos, 2017) defines a crosslinguistic inventory of semantic classes for content and function words. SNACS takes a similar approach to prepositions and possessives, which in Abzianidze and Bos’s (2017) specification are simply tagged REL, which does not disambiguate the nature of the relational meaning. Our categories can thus be understood as refinements to REL. 3.3 Adopting the Construal Analysis Hwang et al. (2017) have pointed out the perils of teasing apart and generalizing preposition semantics so that each use has a clear supersense label. One key challenge they identified is that the preposition i"
P18-1018,W13-2322,1,0.846996,"l., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012). ize more easily to new types and usages. The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016). It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017). The framework was realized in version 3.0 of our comprehensively annotated corpus, STREUSLE3 (Schneider et al., 2016). However, several limitati"
P18-1018,C16-1256,0,0.420421,"t to a predicate, and the lexical semantics of the preposition itself. Hwang et al. (2017) suggested a solution, discussed in §3.3, but did not conduct an annotation study or release a corpus to establish its feasibility empirically. We address that gap here. Second, 75 categories is an unwieldy number for both annotators and disambiguation systems. Some are quite specialized and extremely rare in STREUSLE 3.0, which causes data sparseness issues for supervised learning. In fact, the only published disambiguation system for preposition supersenses collapsed the distinctions to just 12 labels (Gonen and Goldberg, 2016). Hwang et al. (2017) remarked that solving the aforementioned problem could remove the need for many of the specialized categories and make the taxonomy more tractable for annotators and systems. We substantiate this here, defining a new hierarchy with just 50 categories (SNACS, §3) and providing disambiguation results for the full set of distinctions. Finally, given the semantic overlap of possessive case and the preposition of, we saw an opportunity to broaden the application of the scheme to include possessives. Our reannotated corpus, STREUSLE 4.0, thus has supersense annotations for over"
P18-1018,C10-2052,0,0.0584241,"Missing"
P18-1018,L18-1242,1,0.843466,". Three labels never appear in the annotated corpus: T EMPORAL from the C IRCUMSTANCE hierarchy, and PARTI CIPANT and C ONFIGURATION which are both the highest supersense in their respective hierarchies. While all remaining supersenses are attested as scene roles, there are some that never occur as functions, such as O RIGINATOR, which is most often realized as P OSSESSOR or S OURCE, and E XPERI ENCER . It is interesting to note that every subtype of C IRCUMSTANCE (except T EMPORAL) appears as both scene role and function, whereas many of the subtypes of the other two hierarchies are lim189 8 Blodgett and Schneider (2018) detail the extension of the scheme to possessives. 9 In the corpus, lexical expression tokens appear alongside a lexical category indicating which inventory of supersenses, if any, applies. SNACS-annotated units are those with ADP (adposition), PP, PRON . POSS (possessive pronoun), etc., whereas DISC (discourse) and CCONJ expressions do not receive any supersense. Refer to the STREUSLE README for details. ited to either role or function. This reflects our view that prepositions primarily capture circumstantial notions such as space and time, but have been extended to cover other semantic rela"
P18-1018,P11-2056,0,0.0371708,"Missing"
P18-1018,D09-1047,0,0.0768296,"a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to"
P18-1018,S17-1022,1,0.841894,"Missing"
P18-1018,P14-1120,0,0.460243,"omberg, 2010). Possessive constructions can likewise denote a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on c"
P18-1018,L16-1630,0,0.0318812,"2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012). ize more easily to new types and usages. The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016). It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017). The framework was realized in version 3.0 of our comprehensively annotated corpus, STREUSLE3 (Schneider et al., 2016)."
P18-1018,S07-1005,0,0.14038,"Missing"
P18-1018,J09-2002,0,0.169265,"Missing"
P18-1018,P14-5010,0,0.00262509,"tems are trained on the training set only and evaluated on the test set; the development set was used for tuning hyperparameters. Gold tokenization was used throughout. Only targets with a semantic supersense analysis involving labels from figure 2 were included in training and evaluation—i.e., tokens with special labels (see §4) were excluded. To test the impact of automatic syntactic parsing, models in the auto syntax condition were trained and evaluated on automatic lemmas, POS tags, and Basic Universal Dependencies (according to the v1 standard) produced by Stanford CoreNLP version 3.8.0 (Manning et al., 2014).13 Named entity tags from the default 12-class CoreNLP model were used in all conditions. 6.2 Target Identification §3.1 explains that the categories in our scheme apply not only to (transitive) adpositions in a very narrow definition of the term, but also to lexical items that traditionally belong to variety of syntactic classes (such as adverbs and particles), as 13 The CoreNLP parser was trained on all 5 genres of the English Web Treebank—i.e., a superset of our training set. Gold syntax follows the UDv2 standard, whereas the classifiers in the auto syntax conditions are trained and tested"
P18-1018,W04-2609,0,0.110573,"ll be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbas"
P18-1018,J05-1004,0,0.433013,"Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012). ize more easily to new types and usages. The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016). It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017). The framework was realized in version 3.0 of our comprehensively annotate"
P18-1018,saint-dizier-2006-prepnet,0,0.045623,"can likewise denote a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple le"
P18-1018,W16-1712,1,0.912313,"Missing"
P18-1018,W15-1612,1,0.845402,"oridou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banaresc"
P18-1018,D11-1012,1,0.862736,"ternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009"
P18-1018,Q13-1019,1,0.934474,"ings marked by prepositions/possessives are to some extent captured in predicate-argument or graphbased meaning representations (e.g., Palmer et al., 2005; Fillmore and Baker, 2009; Oepen et al., 2016; Banarescu et al., 2013) and domain-centric representations like TimeML and ISO-Space (Pustejovsky et al., 2003, 2012). ize more easily to new types and usages. The most recent class-based approach to prepositions was our initial framework of 75 preposition supersenses arranged in a multiple inheritance taxonomy (Schneider et al., 2015, 2016). It was based largely on relation/role inventories of Srikumar and Roth (2013) and VerbNet (Bonial et al., 2011; Palmer et al., 2017). The framework was realized in version 3.0 of our comprehensively annotated corpus, STREUSLE3 (Schneider et al., 2016). However, several limitations of our approach became clear to us over time. First, as pointed out by Hwang et al. (2017), the one-label-per-token assumption in STREUSLE is flawed because it in some cases puts into conflict the semantic role of the PP with respect to a predicate, and the lexical semantics of the preposition itself. Hwang et al. (2017) suggested a solution, discussed in §3.3, but did not conduct an annotati"
P18-1018,N09-3017,0,0.0322753,"lations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, m"
P18-1018,P13-1037,0,0.0377858,"s—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meaning across multiple lexical items, and aims to general2 Of course, meanings marked by prepositions/possessives are t"
P18-1018,S07-1051,0,0.0481426,"ssessive constructions can likewise denote a number of semantic relations, and various factors—including semantics—influence whether attributive possession in English will be expressed with of, or with ’s and possessive pronouns (the ‘genitive alternation’; Taylor, 1996; Nikiforidou, 1991; Rosenbach, 2002; Heine, 2006; Wolk et al., 2013; Shih et al., 2015). Corpus-based computational work on semantic disambiguation specifically of prepositions and possessives2 falls into two categories: the lexicographic/word sense disambiguation approach (Litkowski and Hargraves, 2005, 2007; Litkowski, 2014; Ye and Baldwin, 2007; Saint-Dizier, 2006; Dahlmeier et al., 2009; Tratz and Hovy, 2009; Hovy et al., 2010, 2011; Tratz and Hovy, 2013), and the semantic class approach (Moldovan et al., 2004; Badulescu and Moldovan, 2009; O’Hara and Wiebe, 2009; Srikumar and Roth, 2011, 2013; Schneider et al., 2015, 2016; Hwang et al., 2017, see also Müller et al., 2012 for German). The lexicographic approach can capture finer-grained meaning distinctions, at a risk of relying upon idiosyncratic and potentially incomplete dictionary definitions. The semantic class approach, which we follow here, focuses on commonalities in meanin"
P18-1018,L16-1262,0,\N,Missing
P18-1035,C16-1179,0,0.0293305,"dataset targeted in SemEval 2017 (May and Priyadarshi, 2017).9 For SDP, we use the DM representation from the SDP 2016 dataset (Oepen Multitask Transition-based Parsing Now that the same model can be applied to different tasks, we can train it in a multitask setting. The fairly small training set available for UCCA (see §7) makes MTL particularly appealing, and we focus on it in this paper, treating AMR, DM and UD parsing as auxiliary tasks. Following previous work, we share only some of the parameters (Klerke et al., 2016; Søgaard and Goldberg, 2016; Bollmann and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017; Peng et al., 2017a, 2018), leaving task-specific sub-networks as well. Concretely, we keep the BiLSTM used by TUPA for the main task (UCCA parsing), add a BiLSTM that is shared 8 9 378 http://github.com/huji-nlp/ucca-corpora http://catalog.ldc.upenn.edu/LDC2017T10 English French German # tokens # sentences # tokens # sentences # tokens # sentences train dev test train dev test train dev test train dev test train dev test train dev test UCCA Wiki 20K AMR DM UD 128444 14676 15313 12339 648950 765025 458277 4268 454 503 506 10047 1558 1324 413 67 67 79894 10059"
P18-1035,P13-1023,1,0.689843,"17). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SDP; Oepen et al., 2016) and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). While these schemes are formally different and focus on different distinctions, much of their semantic content is shared (Abend and Rappoport, 2017). Multitask learning (MTL; Caruana, 1997) allows exploiting the overlap between tasks to ef1 Related Work http://github.com/danielhers/tupa 373 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics shown great benefit for transition-based syntactic parsing, when jointly training with POS tagging"
P18-1035,P17-1008,1,0.849635,"ion to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SDP; Oepen et al., 2016) and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). While these schemes are formally different and focus on different distinctions, much of their semantic content is shared (Abend and Rappoport, 2017). Multitask learning (MTL; Caruana, 1997) allows exploiting the overlap between tasks to ef1 Related Work http://github.com/danielhers/tupa 373 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics shown great benefit for transition-based syntactic parsing, when jointly training with POS tagging (Bohnet and Nivre, 2012; Zhang and Weiss, 2016), and with lexical analysis (Constant and Nivre, 2016; More, 2016). Recent work has achieved state-of-"
P18-1035,S17-2157,0,0.0294137,"Missing"
P18-1035,P17-1112,0,0.40641,"ks, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consistent and coarse-grained t"
P18-1035,D15-1198,0,0.0487784,"a tendency used by AMR parsers, which align a subset of the graph nodes to a subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing usin"
P18-1035,P13-2131,0,0.301159,"Missing"
P18-1035,D17-1130,0,0.0636435,"d scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bilexical trees, with edge labels representing syntactic relations between words. We use UD as an auxiliary task, inspired by previous work on joint syntactic and semantic parsing (see §2). In order to reach comparable analyses c"
P18-1035,W13-2322,0,0.537693,"et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SDP; Oepen et al., 2016) and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). While these schemes are formally different and focus on different distinctions, much of their semantic content is shared (Abend and Rappoport, 2017). Multitask learning (MTL; Caruana, 1997) allows exploiting the overlap between tasks to ef1 Related Work http://github.com/danielhers/tupa 373 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 A"
P18-1035,P16-1016,0,0.0209388,"ons, much of their semantic content is shared (Abend and Rappoport, 2017). Multitask learning (MTL; Caruana, 1997) allows exploiting the overlap between tasks to ef1 Related Work http://github.com/danielhers/tupa 373 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics shown great benefit for transition-based syntactic parsing, when jointly training with POS tagging (Bohnet and Nivre, 2012; Zhang and Weiss, 2016), and with lexical analysis (Constant and Nivre, 2016; More, 2016). Recent work has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 201"
P18-1035,S16-1176,0,0.0250755,"period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many language"
P18-1035,copestake-flickinger-2000-open,0,0.268289,"AG. ing predicate-argument relations between content words in a sentence. All are based on semantic formalisms converted into bilexical dependencies— directed graphs whose nodes are text tokens. Edges are labeled, encoding semantic relations between the tokens. Non-content tokens, such as punctuation, are left out of the analysis (see Figure 1c). Graphs containing cycles have been removed from the SDP datasets. We use one of the representations from the SemEval shared tasks: DM (DELPH-IN MRS), converted from DeepBank (Flickinger et al., 2012), a corpus of hand-corrected parses from LinGO ERG (Copestake and Flickinger, 2000), an HPSG (Pollard and Sag, 1994) using Minimal Recursion Semantics (Copestake et al., 2005). Abstract Meaning Representation. AMR (Banarescu et al., 2013) is a semantic representation that encodes information about named entities, argument structure, semantic roles, word sense and co-reference. AMRs are rooted directed graphs, in which both nodes and edges are labeled. Most AMRs are DAGs, although cycles are permitted. AMR differs from the other schemes we consider in that it does not anchor its graphs in the words of the sentence (Figure 1b). Instead, AMR graphs connect variables, concepts ("
P18-1035,R13-1008,0,0.0636649,"Missing"
P18-1035,D12-1091,0,0.0687685,"Missing"
P18-1035,E17-1051,0,0.0811458,"dentifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bilexical trees, with edge labels representing syntactic relations between words. We use UD as an auxiliary task, inspired by previous work on joint syntactic and semantic parsing (see §2). In o"
P18-1035,E17-2026,0,0.0280809,"-of-domain settings. Converting AMR. In the conversion from AMR, node labels are dropped. Since alignments are not part of the AMR graph (see Figure 3b), we use automatic alignments (see §7), and attach each node with an edge to each of its aligned terminals. Named entities in AMR are represented as a subgraph, whose name-labeled root has a child for each token in the name (see the two name nodes in Figure 1b). We collapse this subgraph into a single node whose children are the name tokens. 6 Unlabeled parsing for auxiliary tasks. To simplify the auxiliary tasks and facilitate generalization (Bingel and Søgaard, 2017), we perform unlabeled parsing for AMR, DM and UD, while still predicting edge labels in UCCA parsing. To support unlabeled parsing, we simply remove all labels from the E DGE, R EMOTE and N ODE transitions output by the oracle. This results in a much smaller number of transitions the classifier has to select from (no more than 10, as opposed to 45 in labeled UCCA parsing), allowing us to use no BiLSTMs and fewer dimensions and layers for task-specific MLPs of auxiliary tasks (see §7). This limited capacity forces the network to use the shared parameters for all tasks, increasing generalizatio"
P18-1035,P07-1033,0,0.0687063,"Missing"
P18-1035,K17-3002,0,0.061272,"Missing"
P18-1035,W06-1615,0,0.113866,"ble conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings. Our code is publicly available.1 1 2 MTL has been used over the years for NLP tasks with varying degrees of similarity, examples including joint classification of different arguments in semantic role labeling (Toutanova et al., 2005), and joint parsing and named entity recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably ye"
P18-1035,S15-2154,0,0.118318,"roperties. TUPA’s transition system can yield any labeled DAG whose terminals are anchored in the text tokens. To support parsing into AMR, which uses graphs that are not anchored in the tokens, we take advantage of existing alignments of the graphs with the text tokens during training (§5). First used for projective syntactic dependency tree parsing (Nivre, 2003), transition-based parsers have since been generalized to parse into many other graph families, such as (discontinuous) constituency trees (e.g., Zhang and Clark, 2009; Maier and Lichte, 2016), and DAGs (e.g., Sagae and Tsujii, 2008; Du et al., 2015). Transition-based parsers apply transitions incrementally to an internal state defined by a buffer B of remaining tokens and nodes, a stack S of unresolved nodes, and a labeled graph G of constructed nodes and edges. When a terminal state is reached, the graph G is the final output. A classifier is used at each step to select the next transition, based on features that encode the current state. 4.1 S B , G L John moved to Paris . H After P graduation Classifier transition softmax MLP BiLSTM Embeddings After graduation ... to Paris Figure 2: Illustration of the TUPA model, adapted from Hershco"
P18-1035,D12-1133,0,0.0289112,"While these schemes are formally different and focus on different distinctions, much of their semantic content is shared (Abend and Rappoport, 2017). Multitask learning (MTL; Caruana, 1997) allows exploiting the overlap between tasks to ef1 Related Work http://github.com/danielhers/tupa 373 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics shown great benefit for transition-based syntactic parsing, when jointly training with POS tagging (Bohnet and Nivre, 2012; Zhang and Weiss, 2016), and with lexical analysis (Constant and Nivre, 2016; More, 2016). Recent work has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outpe"
P18-1035,Q17-1010,0,0.101123,"Missing"
P18-1035,K17-1038,0,0.0366366,"recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SD"
P18-1035,C16-1013,0,0.0141455,"AMR, we use LDC2017T10, identical to the dataset targeted in SemEval 2017 (May and Priyadarshi, 2017).9 For SDP, we use the DM representation from the SDP 2016 dataset (Oepen Multitask Transition-based Parsing Now that the same model can be applied to different tasks, we can train it in a multitask setting. The fairly small training set available for UCCA (see §7) makes MTL particularly appealing, and we focus on it in this paper, treating AMR, DM and UD parsing as auxiliary tasks. Following previous work, we share only some of the parameters (Klerke et al., 2016; Søgaard and Goldberg, 2016; Bollmann and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017; Peng et al., 2017a, 2018), leaving task-specific sub-networks as well. Concretely, we keep the BiLSTM used by TUPA for the main task (UCCA parsing), add a BiLSTM that is shared 8 9 378 http://github.com/huji-nlp/ucca-corpora http://catalog.ldc.upenn.edu/LDC2017T10 English French German # tokens # sentences # tokens # sentences # tokens # sentences train dev test train dev test train dev test train dev test train dev test train dev test UCCA Wiki 20K AMR DM UD 128444 14676 15313 12339 648950 765025 458277 4268 454 503 506 1004"
P18-1035,W17-2607,0,0.0258614,"ss models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SDP; Oepen et al., 2016) and Universal Conceptual Cognitive Annotation (UCCA; Abe"
P18-1035,N09-1037,0,0.0416094,"as auxiliary tasks. We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks. Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings. Our code is publicly available.1 1 2 MTL has been used over the years for NLP tasks with varying degrees of similarity, examples including joint classification of different arguments in semantic role labeling (Toutanova et al., 2005), and joint parsing and named entity recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain s"
P18-1035,P17-2098,0,0.0295376,"of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SDP; Oepen et al., 2016) and Universal Conceptual Cognitive An"
P18-1035,P14-1134,0,0.0947973,"ignable to text tokens, a tendency used by AMR parsers, which align a subset of the graph nodes to a subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in address"
P18-1035,Q16-1023,0,0.0770141,"training and testing on 20K; (4) German indomain setting on 20K, with somewhat noisy annotation. For MTL experiments, we use unlabeled AMR, DM and UD++ parsing as auxiliary tasks in English, and unlabeled UD parsing in French and German.12 We also report baseline results training only the UCCA training sets. Hyperparameters. We initialize embeddings randomly. We use dropout (Srivastava et al., 2014) between MLP layers, and recurrent dropout (Gal and Ghahramani, 2016) between BiLSTM layers, both with p = 0.4. We also use word (α = 0.2), tag (α = 0.2) and dependency relation (α = 0.5) dropout (Kiperwasser and Goldberg, 2016).14 In addition, we use a novel form of dropout, node dropout: with a probability of 0.1 at each step, all features associated with a single node in the parser state are replaced with zero vectors. For optimization we use a minibatch size of 100, decaying all weights by 10−5 at each update, and train with stochastic gradient descent for N epochs with a learning rate of 0.1, followed by AMSGrad (Sashank J. Reddi, 2018) for N epochs with α = 0.001, β1 = 0.9 and β2 = 0.999. We use N = 50 for English and German, and N = 400 for French. We found this training strategy better than using only one of"
P18-1035,P17-1043,0,0.0162428,"align a subset of the graph nodes to a subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other"
P18-1035,N16-1179,0,0.0248575,"performed on the respective development sets. For AMR, we use LDC2017T10, identical to the dataset targeted in SemEval 2017 (May and Priyadarshi, 2017).9 For SDP, we use the DM representation from the SDP 2016 dataset (Oepen Multitask Transition-based Parsing Now that the same model can be applied to different tasks, we can train it in a multitask setting. The fairly small training set available for UCCA (see §7) makes MTL particularly appealing, and we focus on it in this paper, treating AMR, DM and UD parsing as auxiliary tasks. Following previous work, we share only some of the parameters (Klerke et al., 2016; Søgaard and Goldberg, 2016; Bollmann and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017; Peng et al., 2017a, 2018), leaving task-specific sub-networks as well. Concretely, we keep the BiLSTM used by TUPA for the main task (UCCA parsing), add a BiLSTM that is shared 8 9 378 http://github.com/huji-nlp/ucca-corpora http://catalog.ldc.upenn.edu/LDC2017T10 English French German # tokens # sentences # tokens # sentences # tokens # sentences train dev test train dev test train dev test train dev test train dev test train dev test UCCA Wiki 20K AMR DM UD 128444 1467"
P18-1035,P17-1014,0,0.0333107,"ted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consist"
P18-1035,J02-3001,0,0.403436,"epresented by their concepts. Figure 1c presents a DM semantic dependency graph, containing multiple roots: “After”, “moved” and “to”, of which “moved” is marked as top. Punctuation tokens are excluded from SDP graphs. Figure 1d presents a UD tree. Edge labels express syntactic relations. In this section, we outline the parsing tasks we address. We focus on representations that produce full-sentence analyses, i.e., produce a graph covering all (content) words in the text, or the lexical concepts they evoke. This contrasts with “shallow” semantic parsing, primarily semantic role labeling (SRL; Gildea and Jurafsky, 2002; Palmer et al., 2005), which targets argument structure phenomena using flat structures. We consider four formalisms: UCCA, AMR, SDP and Universal Dependencies. Figure 1 presents one sentence annotated in each scheme. terminal nodes to semantic units that participate in some super-ordinate relation. Edges are labeled, indicating the role of a child in the relation the parent represents. Nodes and edges belong to one of several layers, each corresponding to a “module” of semantic distinctions. UCCA’s foundational layer (the only layer for which annotated data exists) mostly covers predicate-ar"
P18-1035,D15-1169,0,0.0278292,"ore, 2016). Recent work has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharing the same formal structures (bilexical DAGs), with considerable edge overlap, but differing in target representations (see §3). For all tasks, they reported an increase of 0.5-1 labeled F1 points. Recently, Peng et al. (2018) applied a similar approach to joint frame-semantic parsing and semantic dependency parsing, using disjoint datasets, and reported further improvements. L LR H H After LA U , A A P A P graduatio"
P18-1035,P16-1001,0,0.0136125,"sers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bilexical trees, with edge labels representing syntactic relations between words. We use UD as an auxiliary task, inspir"
P18-1035,W08-2124,0,0.347516,"Missing"
P18-1035,W16-0906,0,0.0721203,"ginally developed for UCCA, as it supports all these structural properties. TUPA’s transition system can yield any labeled DAG whose terminals are anchored in the text tokens. To support parsing into AMR, which uses graphs that are not anchored in the tokens, we take advantage of existing alignments of the graphs with the text tokens during training (§5). First used for projective syntactic dependency tree parsing (Nivre, 2003), transition-based parsers have since been generalized to parse into many other graph families, such as (discontinuous) constituency trees (e.g., Zhang and Clark, 2009; Maier and Lichte, 2016), and DAGs (e.g., Sagae and Tsujii, 2008; Du et al., 2015). Transition-based parsers apply transitions incrementally to an internal state defined by a buffer B of remaining tokens and nodes, a stack S of unresolved nodes, and a labeled graph G of constructed nodes and edges. When a terminal state is reached, the graph G is the final output. A classifier is used at each step to select the next transition, based on features that encode the current state. 4.1 S B , G L John moved to Paris . H After P graduation Classifier transition softmax MLP BiLSTM Embeddings After graduation ... to Paris Figu"
P18-1035,E17-1005,0,0.143027,"unlabeled parsing for AMR, DM and UD, while still predicting edge labels in UCCA parsing. To support unlabeled parsing, we simply remove all labels from the E DGE, R EMOTE and N ODE transitions output by the oracle. This results in a much smaller number of transitions the classifier has to select from (no more than 10, as opposed to 45 in labeled UCCA parsing), allowing us to use no BiLSTMs and fewer dimensions and layers for task-specific MLPs of auxiliary tasks (see §7). This limited capacity forces the network to use the shared parameters for all tasks, increasing generalization (Mart´ınez Alonso and Plank, 2017). Data. For UCCA, we use v1.2 of the English Wikipedia corpus (Wiki; Abend and Rappoport, 2013), with the standard train/dev/test split (see Table 1), and the Twenty Thousand Leagues Under the Sea corpora (20K; Sulem et al., 2015), annotated in English, French and German.8 For English and French we use 20K v1.0, a small parallel corpus comprising the first five chapters of the book. As in previous work (Hershcovich et al., 2017), we use the English part only as an out-of-domain test set. We train and test on the French part using the standard split, as well as the German corpus (v0.9), which i"
P18-1035,S16-1166,0,0.236391,"MR differs from the other schemes we consider in that it does not anchor its graphs in the words of the sentence (Figure 1b). Instead, AMR graphs connect variables, concepts (from a predefined set) and constants (which may be strings or numbers). Still, most AMR nodes are alignable to text tokens, a tendency used by AMR parsers, which align a subset of the graph nodes to a subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR par"
P18-1035,D17-1206,0,0.0339219,"of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics shown great benefit for transition-based syntactic parsing, when jointly training with POS tagging (Bohnet and Nivre, 2012; Zhang and Weiss, 2016), and with lexical analysis (Constant and Nivre, 2016; More, 2016). Recent work has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharing the same formal structur"
P18-1035,S17-2090,0,0.399132,"from the other schemes we consider in that it does not anchor its graphs in the words of the sentence (Figure 1b). Instead, AMR graphs connect variables, concepts (from a predefined set) and constants (which may be strings or numbers). Still, most AMR nodes are alignable to text tokens, a tendency used by AMR parsers, which align a subset of the graph nodes to a subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency t"
P18-1035,J13-4006,0,0.0493299,"stant and Nivre, 2016; More, 2016). Recent work has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharing the same formal structures (bilexical DAGs), with considerable edge overlap, but differing in target representations (see §3). For all tasks, they reported an increase of 0.5-1 labeled F1 points. Recently, Peng et al. (2018) applied a similar approach to joint frame-semantic parsing and semantic dependency parsing, using disjoint datasets, and reported further improvements. L LR H H After LA U ,"
P18-1035,N10-1004,0,0.0377672,"ode is publicly available.1 1 2 MTL has been used over the years for NLP tasks with varying degrees of similarity, examples including joint classification of different arguments in semantic role labeling (Toutanova et al., 2005), and joint parsing and named entity recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data."
P18-1035,P17-1104,1,0.788059,"nstruction. 4 Semantic Dependency Parsing. SDP uses a set of related representations, targeted in two recent SemEval shared tasks (Oepen et al., 2014, 2015), and extended by Oepen et al. (2016). They correspond to four semantic representation schemes, referred to as DM, PAS, PSD and CCD, representGeneral Transition-based DAG Parser All schemes considered in this work exhibit reentrancy and discontinuity (or non-projectivity), to varying degrees. In addition, UCCA and AMR 2 375 http://github.com/stanfordnlp/CoreNLP Parser state contain non-terminal nodes. To parse these graphs, we extend TUPA (Hershcovich et al., 2017), a transition-based parser originally developed for UCCA, as it supports all these structural properties. TUPA’s transition system can yield any labeled DAG whose terminals are anchored in the text tokens. To support parsing into AMR, which uses graphs that are not anchored in the tokens, we take advantage of existing alignments of the graphs with the text tokens during training (§5). First used for projective syntactic dependency tree parsing (Nivre, 2003), transition-based parsers have since been generalized to parse into many other graph families, such as (discontinuous) constituency trees"
P18-1035,W03-3017,0,0.184239,"2 375 http://github.com/stanfordnlp/CoreNLP Parser state contain non-terminal nodes. To parse these graphs, we extend TUPA (Hershcovich et al., 2017), a transition-based parser originally developed for UCCA, as it supports all these structural properties. TUPA’s transition system can yield any labeled DAG whose terminals are anchored in the text tokens. To support parsing into AMR, which uses graphs that are not anchored in the tokens, we take advantage of existing alignments of the graphs with the text tokens during training (§5). First used for projective syntactic dependency tree parsing (Nivre, 2003), transition-based parsers have since been generalized to parse into many other graph families, such as (discontinuous) constituency trees (e.g., Zhang and Clark, 2009; Maier and Lichte, 2016), and DAGs (e.g., Sagae and Tsujii, 2008; Du et al., 2015). Transition-based parsers apply transitions incrementally to an internal state defined by a buffer B of remaining tokens and nodes, a stack S of unresolved nodes, and a labeled graph G of constructed nodes and edges. When a terminal state is reached, the graph G is the final output. A classifier is used at each step to select the next transition,"
P18-1035,L16-1630,0,0.315096,"nd cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in semantic parsing has targeted, among others, Abstract Meaning Representation (AMR; Banarescu et al., 2013), bilexical Semantic Dependencies (SDP; Oepen et al., 2016) and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). While these schemes are formally different and focus on different distinctions, much of their semantic content is shared (Abend and Rappoport, 2017). Multitask learning (MTL; Caruana, 1997) allows exploiting the overlap between tasks to ef1 Related Work http://github.com/danielhers/tupa 373 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 373–385 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics shown great benefi"
P18-1035,S15-2153,0,0.68935,"Missing"
P18-1035,S14-2008,0,0.45662,"Missing"
P18-1035,J05-1004,0,0.173338,"s. Figure 1c presents a DM semantic dependency graph, containing multiple roots: “After”, “moved” and “to”, of which “moved” is marked as top. Punctuation tokens are excluded from SDP graphs. Figure 1d presents a UD tree. Edge labels express syntactic relations. In this section, we outline the parsing tasks we address. We focus on representations that produce full-sentence analyses, i.e., produce a graph covering all (content) words in the text, or the lexical concepts they evoke. This contrasts with “shallow” semantic parsing, primarily semantic role labeling (SRL; Gildea and Jurafsky, 2002; Palmer et al., 2005), which targets argument structure phenomena using flat structures. We consider four formalisms: UCCA, AMR, SDP and Universal Dependencies. Figure 1 presents one sentence annotated in each scheme. terminal nodes to semantic units that participate in some super-ordinate relation. Edges are labeled, indicating the role of a child in the relation the parent represents. Nodes and edges belong to one of several layers, each corresponding to a “module” of semantic distinctions. UCCA’s foundational layer (the only layer for which annotated data exists) mostly covers predicate-argument structure, sema"
P18-1035,P17-1186,0,0.412812,"Missing"
P18-1035,N18-1135,0,0.14592,"et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharing the same formal structures (bilexical DAGs), with considerable edge overlap, but differing in target representations (see §3). For all tasks, they reported an increase of 0.5-1 labeled F1 points. Recently, Peng et al. (2018) applied a similar approach to joint frame-semantic parsing and semantic dependency parsing, using disjoint datasets, and reported further improvements. L LR H H After LA U , A A P A P graduation John moved C R to Paris (a) UCCA move-01 AR G ARG0 e tim 2 city person after 0 graduate-01 name ARG name op1 name name op1 op1 3 LA ”John” ”Paris” (b) AMR top ARG1 ARG2 After graduation ARG1 , John ARG2 ARG1 moved ARG2 to Paris (c) DM root obl case After graduation punct , obl nsubj John case moved to Paris (d) UD Tackled Parsing Tasks Figure 1: Example graph for each task. Figure 1a presents a UCCA g"
P18-1035,E17-1035,0,0.26432,"results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharing the same formal structures (bilexical DAGs), with considerable edge overlap, but differing in target representations (see §3). For all tasks, they reported an increase of 0.5-1 labeled F1 points. Recently, Peng et al. (2018) applied a similar approach to joint frame-semantic parsing and semantic dependency parsing, using disjoint datasets, and reported further improvements. L LR H H After LA U , A A P A P graduation John moved C R to Paris (a) UCCA move-01 AR G ARG0"
P18-1035,P05-1073,0,0.0770883,"sing as a test case, and AMR, SDP and Universal Dependencies (UD) parsing as auxiliary tasks. We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks. Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings. Our code is publicly available.1 1 2 MTL has been used over the years for NLP tasks with varying degrees of similarity, examples including joint classification of different arguments in semantic role labeling (Toutanova et al., 2005), and joint parsing and named entity recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et a"
P18-1035,C16-1059,0,0.0181885,"ntical to the dataset targeted in SemEval 2017 (May and Priyadarshi, 2017).9 For SDP, we use the DM representation from the SDP 2016 dataset (Oepen Multitask Transition-based Parsing Now that the same model can be applied to different tasks, we can train it in a multitask setting. The fairly small training set available for UCCA (see §7) makes MTL particularly appealing, and we focus on it in this paper, treating AMR, DM and UD parsing as auxiliary tasks. Following previous work, we share only some of the parameters (Klerke et al., 2016; Søgaard and Goldberg, 2016; Bollmann and Søgaard, 2016; Plank, 2016; Braud et al., 2016; Mart´ınez Alonso and Plank, 2017; Peng et al., 2017a, 2018), leaving task-specific sub-networks as well. Concretely, we keep the BiLSTM used by TUPA for the main task (UCCA parsing), add a BiLSTM that is shared 8 9 378 http://github.com/huji-nlp/ucca-corpora http://catalog.ldc.upenn.edu/LDC2017T10 English French German # tokens # sentences # tokens # sentences # tokens # sentences train dev test train dev test train dev test train dev test train dev test train dev test UCCA Wiki 20K AMR DM UD 128444 14676 15313 12339 648950 765025 458277 4268 454 503 506 10047 1558 1324 4"
P18-1035,P11-1157,0,0.0691334,"Missing"
P18-1035,P15-2141,0,0.0251523,"ted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bilexical trees, with edge labels representing syntactic relations between words. We use"
P18-1035,N15-1040,0,0.0470044,"ted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies. UD (Nivre et al., 2016, 2017) has quickly become the dominant dependency scheme for syntactic annotation in many languages, aiming for cross-linguistically consistent and coarse-grained treebank annotation. Formally, UD uses bilexical trees, with edge labels representing syntactic relations between words. We use"
P18-1035,D15-1136,0,0.026836,"AMR parsers, which align a subset of the graph nodes to a subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general"
P18-1035,P16-1147,0,0.0808505,"Missing"
P18-1035,W09-3825,0,0.0260949,"sition-based parser originally developed for UCCA, as it supports all these structural properties. TUPA’s transition system can yield any labeled DAG whose terminals are anchored in the text tokens. To support parsing into AMR, which uses graphs that are not anchored in the tokens, we take advantage of existing alignments of the graphs with the text tokens during training (§5). First used for projective syntactic dependency tree parsing (Nivre, 2003), transition-based parsers have since been generalized to parse into many other graph families, such as (discontinuous) constituency trees (e.g., Zhang and Clark, 2009; Maier and Lichte, 2016), and DAGs (e.g., Sagae and Tsujii, 2008; Du et al., 2015). Transition-based parsers apply transitions incrementally to an internal state defined by a buffer B of remaining tokens and nodes, a stack S of unresolved nodes, and a labeled graph G of constructed nodes and edges. When a terminal state is reached, the graph G is the final output. A classifier is used at each step to select the next transition, based on features that encode the current state. 4.1 S B , G L John moved to Paris . H After P graduation Classifier transition softmax MLP BiLSTM Embeddings After gra"
P18-1035,C08-1095,0,0.13319,"s all these structural properties. TUPA’s transition system can yield any labeled DAG whose terminals are anchored in the text tokens. To support parsing into AMR, which uses graphs that are not anchored in the tokens, we take advantage of existing alignments of the graphs with the text tokens during training (§5). First used for projective syntactic dependency tree parsing (Nivre, 2003), transition-based parsers have since been generalized to parse into many other graph families, such as (discontinuous) constituency trees (e.g., Zhang and Clark, 2009; Maier and Lichte, 2016), and DAGs (e.g., Sagae and Tsujii, 2008; Du et al., 2015). Transition-based parsers apply transitions incrementally to an internal state defined by a buffer B of remaining tokens and nodes, a stack S of unresolved nodes, and a labeled graph G of constructed nodes and edges. When a terminal state is reached, the graph G is the final output. A classifier is used at each step to select the next transition, based on features that encode the current state. 4.1 S B , G L John moved to Paris . H After P graduation Classifier transition softmax MLP BiLSTM Embeddings After graduation ... to Paris Figure 2: Illustration of the TUPA model, ad"
P18-1035,D16-1065,0,0.0211895,"subset of the text tokens (concept identification). In this work, we use pre-aligned AMR graphs. Despite the brief period since its inception, AMR has been targeted by a number of works, notably in two SemEval shared tasks (May, 2016; May and Priyadarshi, 2017). To tackle its variety of distinctions and unrestricted graph structure, AMR parsers often use specialized methods. Graph-based parsers construct AMRs by identifying concepts and scoring edges between them, either in a pipeline fashion (Flanigan et al., 2014; Artzi et al., 2015; Pust et al., 2015; Foland and Martin, 2017), or jointly (Zhou et al., 2016). Another line of work trains machine translation models to convert strings into linearized AMRs (Barzdins and Gosko, 2016; Peng et al., 2017b; Konstas et al., 2017; Buys and Blunsom, 2017b). Transition-based AMR parsers either use dependency trees as pre-processing, then mapping them into AMRs (Wang et al., 2015a,b, 2016; Goodman et al., 2016), or use a transition system tailored to AMR parsing (Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017). We differ from the above approaches in addressing AMR parsing using the same general DAG parser used for other schemes. Universal Dependencies."
P18-1035,L16-1376,0,0.11312,"as an auxiliary task, inspired by previous work on joint syntactic and semantic parsing (see §2). In order to reach comparable analyses cross-linguistically, UD often ends up in annotation that is similar to the common practice in semantic treebanks, such as linking content words to content words wherever possible. Using UD further allows conducting experiments on languages other than English, for which AMR and SDP annotated data is not available (§7). In addition to basic UD trees, we use the enhanced++ UD graphs available for English, which are generated by the Stanford CoreNLP converters (Schuster and Manning, 2016).2 These include additional and augmented relations between content words, partially overlapping with the notion of remote edges in UCCA: in the case of control verbs, for example, a direct relation is added in enhanced++ UD between the subordinated verb and its controller, which is similar to the semantic schemes’ treatment of this construction. 4 Semantic Dependency Parsing. SDP uses a set of related representations, targeted in two recent SemEval shared tasks (Oepen et al., 2014, 2015), and extended by Oepen et al. (2016). They correspond to four semantic representation schemes, referred to"
P18-1035,K17-1040,0,0.0240475,"rences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings. Our code is publicly available.1 1 2 MTL has been used over the years for NLP tasks with varying degrees of similarity, examples including joint classification of different arguments in semantic role labeling (Toutanova et al., 2005), and joint parsing and named entity recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of it"
P18-1035,P16-2038,0,0.25454,"mples including joint classification of different arguments in semantic role labeling (Toutanova et al., 2005), and joint parsing and named entity recognition (Finkel and Manning, 2009). Similar ideas, of parameter sharing across models trained with different datasets, can be found in studies of domain adaptation (Blitzer et al., 2006; Daume III, 2007; Ziser and Reichart, 2017). For parsing, domain adaptation has been applied successfully in parser combination and co-training (McClosky et al., 2010; Baucom et al., 2013). Neural MTL has mostly been effective in tackling formally similar tasks (Søgaard and Goldberg, 2016), including multilingual syntactic dependency parsing (Ammar et al., 2016; Guo et al., 2016), as well as multilingual (Duong et al., 2017), and cross-domain semantic parsing (Herzig and Berant, 2017; Fan et al., 2017). Sharing parameters with a low-level task has Introduction Semantic parsing has arguably yet to reach its full potential in terms of its contribution to downstream linguistic tasks, partially due to the limited amount of semantically annotated training data. This shortage is more pronounced in languages other than English, and less researched domains. Indeed, recent work in seman"
P18-1035,W15-3502,1,0.792046,"much smaller number of transitions the classifier has to select from (no more than 10, as opposed to 45 in labeled UCCA parsing), allowing us to use no BiLSTMs and fewer dimensions and layers for task-specific MLPs of auxiliary tasks (see §7). This limited capacity forces the network to use the shared parameters for all tasks, increasing generalization (Mart´ınez Alonso and Plank, 2017). Data. For UCCA, we use v1.2 of the English Wikipedia corpus (Wiki; Abend and Rappoport, 2013), with the standard train/dev/test split (see Table 1), and the Twenty Thousand Leagues Under the Sea corpora (20K; Sulem et al., 2015), annotated in English, French and German.8 For English and French we use 20K v1.0, a small parallel corpus comprising the first five chapters of the book. As in previous work (Hershcovich et al., 2017), we use the English part only as an out-of-domain test set. We train and test on the French part using the standard split, as well as the German corpus (v0.9), which is a pre-release and still contains a considerable amount of noisy annotation. Tuning is performed on the respective development sets. For AMR, we use LDC2017T10, identical to the dataset targeted in SemEval 2017 (May and Priyadars"
P18-1035,K16-1019,0,0.0296507,"ork has achieved state-of-the-art results in multiple NLP tasks by jointly learning the tasks forming the NLP standard pipeline using a single neural model (Collobert et al., 2011; Hashimoto et al., 2017), thereby avoiding cascading errors, common in pipelines. Much effort has been devoted to joint learning of syntactic and semantic parsing, including two CoNLL shared tasks (Surdeanu et al., 2008; Hajiˇc et al., 2009). Despite their conceptual and practical appeal, such joint models rarely outperform the pipeline approach (Llu´ıs and M`arquez, 2008; Henderson et al., 2013; Lewis et al., 2015; Swayamdipta et al., 2016, 2017). Peng et al. (2017a) performed MTL for SDP in a closely related setting to ours. They tackled three tasks, annotated over the same text and sharing the same formal structures (bilexical DAGs), with considerable edge overlap, but differing in target representations (see §3). For all tasks, they reported an increase of 0.5-1 labeled F1 points. Recently, Peng et al. (2018) applied a similar approach to joint frame-semantic parsing and semantic dependency parsing, using disjoint datasets, and reported further improvements. L LR H H After LA U , A A P A P graduation John moved C R to Paris"
P18-1035,S16-1181,0,\N,Missing
P18-1059,I17-1030,0,0.0137651,"where e is an error and the k-best list contains a valid correction of it. Whenever the reference allows both keeping e and altering e, the re-ranker selects keeping e. Indeed, our experimental results show that word changes increase with M (Figure 4), indicating that low coverage may play a role in the observed tendency of GEC systems to under-correct. No significant difference is found for word order. 638 3.4 Under-correction by Error Types lar trends recur in this setting as well. The tendency of TS systems to under-predict changes to the source has already been observed by previous work (Alva-Manchego et al., 2017), showing that TS systems under-predict word additions, deletions, substitutions, and sequence shifts (Zhang and Lapata, 2017), and have low edit distance from the source (Narayan and Gardent, 2016). Our experiments show that LCB may account for this under-prediction. Concretely, we show that (1) the distribution of valid references for a given sentence is long-tailed; (2) common evaluation measures suffer from LCB, taking SARI (Xu et al., 2016) as an example RBM (similar trends are obtained with Accuracy); (3) under-prediction is alleviated with M in oracle re-ranking experiments. In this sec"
P18-1059,I17-2058,0,0.289907,"ut is obtained by making zero or more edits to the source. RBMs are the standard for TS evaluation, much like they are in GEC. Our experiments on TS demonstrate that simiWe therefore restrict oracle re-ranking experi639 Variants Mass veloping alternative evaluation measures that transcend n-gram overlap, and use deeper analysis tools, e.g., by comparing the semantics of the reference and the source to the output (cf. Lo and Wu, 2011). Napoles et al. (2016) have made progress towards this goal in proposing a reference-less grammaticality measure, using Grammatical Error Detection tools, as did Asano et al. (2017), who added a fluency measure to the grammaticality. In a recent project (Choshen and Abend, 2018b), we proposed a complementary measure that measures the semantic faithfulness of the output to the source, in order to form a combined semantic measure that bypasses the pitfalls of low coverage. Frequency Threshold (γ) 0 0.001 0.01 0.1 2636.29 111.19 4.68 0.13 1 0.42 0.14 0.02 Table 4: Estimating the distribution of simplifications Dx . The table presents the mean number of simplifications per sentence with probability more than γ (top row), as well as their total probability mass (bottom row)."
P18-1059,P17-1074,0,0.073305,"ilar trends are obtained with Accuracy); (3) under-prediction is alleviated with M in oracle re-ranking experiments. In this section we study the prevalence of undercorrection according to edit types, finding that open-class types of errors (such as replacing a word with another word) are more starkly undercorrected, than closed-class errors. Evaluating with low coverage RBMs does not incentivize systems to address open-class errors (in fact, it disincentivizes them to). Therefore, even if LCB is not the cause for this trend, current evaluation procedures may perpetuate it. We use the data of Bryant et al. (2017), which automatically assigned types to each edit in the output of all CoNLL 2014 systems on the NUCLE test set. As a measure of under-correction tendency, we take the ratio between the mean number of corrections produced by the systems and by the references. We note that this analysis does not consider whether the predicted correction is valid or not, but only how many of the errors of each type the systems attempted to correct. We find that all edit types are under-predicted on average, but that the least under-predicted ones are mostly closed-class types. Concretely, the top quarter of erro"
P18-1059,P11-1023,0,0.0865786,"Similar Effects on Simplification We now turn to replicating our experiments on Text Simplification (TS). From a formal point of view, evaluation of the tasks is similar: the output is obtained by making zero or more edits to the source. RBMs are the standard for TS evaluation, much like they are in GEC. Our experiments on TS demonstrate that simiWe therefore restrict oracle re-ranking experi639 Variants Mass veloping alternative evaluation measures that transcend n-gram overlap, and use deeper analysis tools, e.g., by comparing the semantics of the reference and the source to the output (cf. Lo and Wu, 2011). Napoles et al. (2016) have made progress towards this goal in proposing a reference-less grammaticality measure, using Grammatical Error Detection tools, as did Asano et al. (2017), who added a fluency measure to the grammaticality. In a recent project (Choshen and Abend, 2018b), we proposed a complementary measure that measures the semantic faithfulness of the output to the source, in order to form a combined semantic measure that bypasses the pitfalls of low coverage. Frequency Threshold (γ) 0 0.001 0.01 0.1 2636.29 111.19 4.68 0.13 1 0.42 0.14 0.02 Table 4: Estimating the distribution of"
P18-1059,N18-2020,1,0.794002,"ation, much like they are in GEC. Our experiments on TS demonstrate that simiWe therefore restrict oracle re-ranking experi639 Variants Mass veloping alternative evaluation measures that transcend n-gram overlap, and use deeper analysis tools, e.g., by comparing the semantics of the reference and the source to the output (cf. Lo and Wu, 2011). Napoles et al. (2016) have made progress towards this goal in proposing a reference-less grammaticality measure, using Grammatical Error Detection tools, as did Asano et al. (2017), who added a fluency measure to the grammaticality. In a recent project (Choshen and Abend, 2018b), we proposed a complementary measure that measures the semantic faithfulness of the output to the source, in order to form a combined semantic measure that bypasses the pitfalls of low coverage. Frequency Threshold (γ) 0 0.001 0.01 0.1 2636.29 111.19 4.68 0.13 1 0.42 0.14 0.02 Table 4: Estimating the distribution of simplifications Dx . The table presents the mean number of simplifications per sentence with probability more than γ (top row), as well as their total probability mass (bottom row). ments to MAX-SARI, conducting re-ranking experiments on k-best lists in two settings: Moses (Koeh"
P18-1059,P11-2089,0,0.443196,"easible range, contrary to previous suggestions. This is due to the long-tailed distribution of valid corrections for a sentence. Concretely, we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction. Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on Text Simplification further support our claims. 1 Introduction Evaluation in monolingual translation (Xu et al., 2015; Mani, 2009) and in particular in GEC (Tetreault and Chodorow, 2008; Madnani et al., 2011; Felice and Briscoe, 2015; Bryant and Ng, 2015; Napoles et al., 2015) has gained notoriety for its difficulty, due in part to the heterogeneity and size of the space of valid corrections (Chodorow et al., 2012; Dreyer and Marcu, 2012). Reference-based evaluation measures (RBM) are the common practice in GEC, including the standard M 2 (Dahlmeier and Ng, 2012), GLEU (Napoles et al., 2015) and I-measure (Felice and Briscoe, 2015). The Low Coverage Bias (LCB) was previously discussed by Bryant and Ng (2015), who showed that inter-annotator agreement in producing ref632 Proceedings of the 56th An"
P18-1059,N12-1067,0,0.649451,"but targeted changes to the input. Similar effects on Text Simplification further support our claims. 1 Introduction Evaluation in monolingual translation (Xu et al., 2015; Mani, 2009) and in particular in GEC (Tetreault and Chodorow, 2008; Madnani et al., 2011; Felice and Briscoe, 2015; Bryant and Ng, 2015; Napoles et al., 2015) has gained notoriety for its difficulty, due in part to the heterogeneity and size of the space of valid corrections (Chodorow et al., 2012; Dreyer and Marcu, 2012). Reference-based evaluation measures (RBM) are the common practice in GEC, including the standard M 2 (Dahlmeier and Ng, 2012), GLEU (Napoles et al., 2015) and I-measure (Felice and Briscoe, 2015). The Low Coverage Bias (LCB) was previously discussed by Bryant and Ng (2015), who showed that inter-annotator agreement in producing ref632 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 632–642 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics et al., 2013). NUCLE is a parallel corpus of essays written by language learners and their corrected versions, containing 1414 essays and 50 test essays, each of about 500 words. W"
P18-1059,W13-1703,0,0.410308,"Missing"
P18-1059,P15-2097,0,0.324039,"ong-tailed distribution of valid corrections for a sentence. Concretely, we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction. Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on Text Simplification further support our claims. 1 Introduction Evaluation in monolingual translation (Xu et al., 2015; Mani, 2009) and in particular in GEC (Tetreault and Chodorow, 2008; Madnani et al., 2011; Felice and Briscoe, 2015; Bryant and Ng, 2015; Napoles et al., 2015) has gained notoriety for its difficulty, due in part to the heterogeneity and size of the space of valid corrections (Chodorow et al., 2012; Dreyer and Marcu, 2012). Reference-based evaluation measures (RBM) are the common practice in GEC, including the standard M 2 (Dahlmeier and Ng, 2012), GLEU (Napoles et al., 2015) and I-measure (Felice and Briscoe, 2015). The Low Coverage Bias (LCB) was previously discussed by Bryant and Ng (2015), who showed that inter-annotator agreement in producing ref632 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Pa"
P18-1059,N12-1017,0,0.027851,"a valid correction. Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on Text Simplification further support our claims. 1 Introduction Evaluation in monolingual translation (Xu et al., 2015; Mani, 2009) and in particular in GEC (Tetreault and Chodorow, 2008; Madnani et al., 2011; Felice and Briscoe, 2015; Bryant and Ng, 2015; Napoles et al., 2015) has gained notoriety for its difficulty, due in part to the heterogeneity and size of the space of valid corrections (Chodorow et al., 2012; Dreyer and Marcu, 2012). Reference-based evaluation measures (RBM) are the common practice in GEC, including the standard M 2 (Dahlmeier and Ng, 2012), GLEU (Napoles et al., 2015) and I-measure (Felice and Briscoe, 2015). The Low Coverage Bias (LCB) was previously discussed by Bryant and Ng (2015), who showed that inter-annotator agreement in producing ref632 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 632–642 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics et al., 2013). NUCLE is a parallel corpus of essays w"
P18-1059,D16-1228,0,0.237748,"Simplification We now turn to replicating our experiments on Text Simplification (TS). From a formal point of view, evaluation of the tasks is similar: the output is obtained by making zero or more edits to the source. RBMs are the standard for TS evaluation, much like they are in GEC. Our experiments on TS demonstrate that simiWe therefore restrict oracle re-ranking experi639 Variants Mass veloping alternative evaluation measures that transcend n-gram overlap, and use deeper analysis tools, e.g., by comparing the semantics of the reference and the source to the output (cf. Lo and Wu, 2011). Napoles et al. (2016) have made progress towards this goal in proposing a reference-less grammaticality measure, using Grammatical Error Detection tools, as did Asano et al. (2017), who added a fluency measure to the grammaticality. In a recent project (Choshen and Abend, 2018b), we proposed a complementary measure that measures the semantic faithfulness of the output to the source, in order to form a combined semantic measure that bypasses the pitfalls of low coverage. Frequency Threshold (γ) 0 0.001 0.01 0.1 2636.29 111.19 4.68 0.13 1 0.42 0.14 0.02 Table 4: Estimating the distribution of simplifications Dx . Th"
P18-1059,N15-1060,0,0.19787,"y to previous suggestions. This is due to the long-tailed distribution of valid corrections for a sentence. Concretely, we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction. Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on Text Simplification further support our claims. 1 Introduction Evaluation in monolingual translation (Xu et al., 2015; Mani, 2009) and in particular in GEC (Tetreault and Chodorow, 2008; Madnani et al., 2011; Felice and Briscoe, 2015; Bryant and Ng, 2015; Napoles et al., 2015) has gained notoriety for its difficulty, due in part to the heterogeneity and size of the space of valid corrections (Chodorow et al., 2012; Dreyer and Marcu, 2012). Reference-based evaluation measures (RBM) are the common practice in GEC, including the standard M 2 (Dahlmeier and Ng, 2012), GLEU (Napoles et al., 2015) and I-measure (Felice and Briscoe, 2015). The Low Coverage Bias (LCB) was previously discussed by Bryant and Ng (2015), who showed that inter-annotator agreement in producing ref632 Proceedings of the 56th Annual Meeting of the Associ"
P18-1059,W16-6620,0,0.0201468,"lts show that word changes increase with M (Figure 4), indicating that low coverage may play a role in the observed tendency of GEC systems to under-correct. No significant difference is found for word order. 638 3.4 Under-correction by Error Types lar trends recur in this setting as well. The tendency of TS systems to under-predict changes to the source has already been observed by previous work (Alva-Manchego et al., 2017), showing that TS systems under-predict word additions, deletions, substitutions, and sequence shifts (Zhang and Lapata, 2017), and have low edit distance from the source (Narayan and Gardent, 2016). Our experiments show that LCB may account for this under-prediction. Concretely, we show that (1) the distribution of valid references for a given sentence is long-tailed; (2) common evaluation measures suffer from LCB, taking SARI (Xu et al., 2016) as an example RBM (similar trends are obtained with Accuracy); (3) under-prediction is alleviated with M in oracle re-ranking experiments. In this section we study the prevalence of undercorrection according to edit types, finding that open-class types of errors (such as replacing a word with another word) are more starkly undercorrected, than cl"
P18-1059,D15-1052,0,0.23689,"Missing"
P18-1059,P17-2014,0,0.0256813,"res the semantic faithfulness of the output to the source, in order to form a combined semantic measure that bypasses the pitfalls of low coverage. Frequency Threshold (γ) 0 0.001 0.01 0.1 2636.29 111.19 4.68 0.13 1 0.42 0.14 0.02 Table 4: Estimating the distribution of simplifications Dx . The table presents the mean number of simplifications per sentence with probability more than γ (top row), as well as their total probability mass (bottom row). ments to MAX-SARI, conducting re-ranking experiments on k-best lists in two settings: Moses (Koehn et al., 2007) with k = 100, and a neural model (Nisioi et al., 2017) with k = 12. Our results indeed show that under-prediction is alleviated with M in both settings. For example, the least under-predicting model (the neural one) did not change 50 sentences with M = 1, but only 29 weren’t changed with M = 8. See Appendix G. 5 Acknowledgments This work was supported by the Israel Science Foundation (grant No. 929/17), and by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister’s Office. We thank Nathan Schneider, Courtney Napoles and Joel Tetreault for helpful feedback. Conclusion We argue that using"
P18-1059,D16-1161,0,0.130632,"632 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 632–642 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics et al., 2013). NUCLE is a parallel corpus of essays written by language learners and their corrected versions, containing 1414 essays and 50 test essays, each of about 500 words. We evaluate all participating systems in the CoNLL 2014 shared task, in addition to three of the best performing systems on this dataset, a hybrid system (Rozovskaya and Roth, 2016), a phrase-based MT system (Junczys-Dowmunt and Grundkiewicz, 2016) and a neural network system (Xie et al., 2016). Appendix A lists system names and abbreviations. for a sentence and their long-tailed distribution. Indeed, even short sentences have over 1000 valid corrections on average. Empirically assessing the effect of increasing M on the bias, we find diminishing returns using three standard GEC measures (M 2 , accuracy and GLEU), underscoring the difficulty in this approach. Similar trends are found when conducting such experiments to Text Simplification (TS) (§4). Specifically we show that (1) the distribution of valid simplifications for a given sent"
P18-1059,W12-3152,0,0.0273949,". We turn to estimating the number of corrections per sentence, and their histogram. The experiments in the following section are run on a random sample of 52 short sentences from the NUCLE test data, i.e. with 15 words or less. Through the length restriction, we avoid introducing too many independent errors that may drastically increase the number of annotation variants (as every combination of corrections for these errors is possible), thus resulting in unreliable estimation for Dx . Proven effective in GEC and related tasks such as MT (Zaidan and Callison-Burch, 2011; Madnani et al., 2011; Post et al., 2012), we use crowdsourcing to sample from Dx (see Appendix B). Aiming to judge grammaticality rather than fluency, we instructed the workers to correct only when necessary, not for styling. We begin by estimating the histogram of Dx for each sentence, using the crowdsourced corrections. We use U N SEEN E ST (Zou et al., 2016), a non-parametric algorithm to estimate a discrete distribution in which the individual values do not matter, only their probability. U NSEEN E ST aims to minimize the “earthmover distance”, between the estimated histogram and the histogram of the distribution. Intuitively, i"
P18-1059,P07-2045,0,0.00495004,"2018b), we proposed a complementary measure that measures the semantic faithfulness of the output to the source, in order to form a combined semantic measure that bypasses the pitfalls of low coverage. Frequency Threshold (γ) 0 0.001 0.01 0.1 2636.29 111.19 4.68 0.13 1 0.42 0.14 0.02 Table 4: Estimating the distribution of simplifications Dx . The table presents the mean number of simplifications per sentence with probability more than γ (top row), as well as their total probability mass (bottom row). ments to MAX-SARI, conducting re-ranking experiments on k-best lists in two settings: Moses (Koehn et al., 2007) with k = 100, and a neural model (Nisioi et al., 2017) with k = 12. Our results indeed show that under-prediction is alleviated with M in both settings. For example, the least under-predicting model (the neural one) did not change 50 sentences with M = 1, but only 29 weren’t changed with M = 8. See Appendix G. 5 Acknowledgments This work was supported by the Israel Science Foundation (grant No. 929/17), and by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister’s Office. We thank Nathan Schneider, Courtney Napoles and Joel Tetreau"
P18-1059,D13-1074,0,0.0256703,"estimation as a Function of M After estimating the histogram of valid corrections for a sentence, we turn to estimating the resulting bias (LCB), for different M values. We study sentence-level accuracy, F -Score and GLEU. N X ˆbM = 1 − 1 (y ∈ Y ) . P M N i=1 Y ∼Di ,y∼Di Sentence-level Accuracy. Sentence-level accuracy is the percentage of corrections that exactly match one of the references. Accuracy is a basic, interpretable measure, used in GEC by, e.g., Rozovskaya and Roth (2010). It is also closely related to the 0-1 loss function commonly used for training in GEC (Chodorow et al., 2012; Rozovskaya and Roth, 2013). Formally, given test sentences X = {x1 , . . . , xN }, their references Y1 , . . . , YN and a system C, we define C’s accuracy to be Acc (C; X, Y ) = N 1 X 1C(xi )∈Yi . N i=1 (3) (4) (5) (6) We observe that the bias, denoted bM , is not affected by N , only by M . As M grows, Y better approximates Correctx , and bM tends to 0. In order to abstract away from the idiosyncrasies of specific systems, we consider an idealized learner, which, when correct, produces a valid correction with the same distribution as a human annotator (i.e., according to Dx ). Formally, we assume that, if C(x) ∈ Corre"
P18-1059,P16-1208,0,0.0556324,"showed that inter-annotator agreement in producing ref632 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 632–642 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics et al., 2013). NUCLE is a parallel corpus of essays written by language learners and their corrected versions, containing 1414 essays and 50 test essays, each of about 500 words. We evaluate all participating systems in the CoNLL 2014 shared task, in addition to three of the best performing systems on this dataset, a hybrid system (Rozovskaya and Roth, 2016), a phrase-based MT system (Junczys-Dowmunt and Grundkiewicz, 2016) and a neural network system (Xie et al., 2016). Appendix A lists system names and abbreviations. for a sentence and their long-tailed distribution. Indeed, even short sentences have over 1000 valid corrections on average. Empirically assessing the effect of increasing M on the bias, we find diminishing returns using three standard GEC measures (M 2 , accuracy and GLEU), underscoring the difficulty in this approach. Similar trends are found when conducting such experiments to Text Simplification (TS) (§4). Specifically we show"
P18-1059,Q16-1013,0,0.136383,"ms from making changes to the source in cases where there are plentiful valid corrections (open class errors), as necessarily only some of them are covered by the reference set. To support our claim we show that (1) existing GEC systems under-correct, often performing an order of magnitude less corrections than a human does (§3.2); (2) increasing the number of references alleviates under-correction (§3.3); and (3) under-correction is more pronounced in error types that are more varied in their valid corrections (§3.4). A different approach for addressing LCB was taken by (Bryant and Ng, 2015; Sakaguchi et al., 2016), who propose to increase the number of references (henceforth, M ). In Section 2 we estimate the distribution of corrections per sentence, and find that increasing M is unlikely to overcome LCB, due to the vast number of valid corrections The prevalent use of too few references for evaluating text-to-text generation is known to bias estimates of their quality (henceforth, low coverage bias or LCB). This paper shows that overcoming LCB in Grammatical Error Correction (GEC) evaluation cannot be attained by re-scaling or by increasing the number of references in any feasible range, contrary to p"
P18-1059,W08-1205,0,0.236448,"number of references in any feasible range, contrary to previous suggestions. This is due to the long-tailed distribution of valid corrections for a sentence. Concretely, we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction. Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on Text Simplification further support our claims. 1 Introduction Evaluation in monolingual translation (Xu et al., 2015; Mani, 2009) and in particular in GEC (Tetreault and Chodorow, 2008; Madnani et al., 2011; Felice and Briscoe, 2015; Bryant and Ng, 2015; Napoles et al., 2015) has gained notoriety for its difficulty, due in part to the heterogeneity and size of the space of valid corrections (Chodorow et al., 2012; Dreyer and Marcu, 2012). Reference-based evaluation measures (RBM) are the common practice in GEC, including the standard M 2 (Dahlmeier and Ng, 2012), GLEU (Napoles et al., 2015) and I-measure (Felice and Briscoe, 2015). The Low Coverage Bias (LCB) was previously discussed by Bryant and Ng (2015), who showed that inter-annotator agreement in producing ref632 Proc"
P18-1059,Q15-1021,0,0.025525,"n cannot be attained by re-scaling or by increasing the number of references in any feasible range, contrary to previous suggestions. This is due to the long-tailed distribution of valid corrections for a sentence. Concretely, we show that LCB incentivizes GEC systems to avoid correcting even when they can generate a valid correction. Consequently, existing systems obtain comparable or superior performance compared to humans, by making few but targeted changes to the input. Similar effects on Text Simplification further support our claims. 1 Introduction Evaluation in monolingual translation (Xu et al., 2015; Mani, 2009) and in particular in GEC (Tetreault and Chodorow, 2008; Madnani et al., 2011; Felice and Briscoe, 2015; Bryant and Ng, 2015; Napoles et al., 2015) has gained notoriety for its difficulty, due in part to the heterogeneity and size of the space of valid corrections (Chodorow et al., 2012; Dreyer and Marcu, 2012). Reference-based evaluation measures (RBM) are the common practice in GEC, including the standard M 2 (Dahlmeier and Ng, 2012), GLEU (Napoles et al., 2015) and I-measure (Felice and Briscoe, 2015). The Low Coverage Bias (LCB) was previously discussed by Bryant and Ng (2015)"
P18-1059,Q16-1029,0,0.197724,"ecur in this setting as well. The tendency of TS systems to under-predict changes to the source has already been observed by previous work (Alva-Manchego et al., 2017), showing that TS systems under-predict word additions, deletions, substitutions, and sequence shifts (Zhang and Lapata, 2017), and have low edit distance from the source (Narayan and Gardent, 2016). Our experiments show that LCB may account for this under-prediction. Concretely, we show that (1) the distribution of valid references for a given sentence is long-tailed; (2) common evaluation measures suffer from LCB, taking SARI (Xu et al., 2016) as an example RBM (similar trends are obtained with Accuracy); (3) under-prediction is alleviated with M in oracle re-ranking experiments. In this section we study the prevalence of undercorrection according to edit types, finding that open-class types of errors (such as replacing a word with another word) are more starkly undercorrected, than closed-class errors. Evaluating with low coverage RBMs does not incentivize systems to address open-class errors (in fact, it disincentivizes them to). Therefore, even if LCB is not the cause for this trend, current evaluation procedures may perpetuate"
P18-1059,P11-1019,0,0.0707288,"s especially true for people who are overseas. This is especially relevant to people who are overseas. Results. Results (Figure 3) show that humans make considerably more changes than systems according to all measures of under-correction, both in terms of the number of sentences modified and the number of modifications within them. Differences are often an order of magnitude large. For example, 36 reference sentences include 6 word changes, where the maximal number of sentences with 6 word changes by any system is 5. We find similar trends on the references of the TreeBank of Learner English (Yannakoudakis et al., 2011). Table 2: Example for a sentence and proposed corrections by different systems (top part) and by the two NUCLE annotators (bottom part). Systems not mentioned in the table retain the source. No system produces a new word as needed. The two references differ in their corrections. Precision-oriented measures (e.g., F0.5 ) penalize invalidly correcting more harshly than not correcting an ungrammatical sentence. In these cases, Condition (7) should be written as 3.3 pcorrect ·pcoverage −(1 − pcorrect · pcoverage ) α &lt; 1−pdetect where α is the ratio between the penalty for introducing a wrong corr"
P18-1059,C00-2137,0,0.36867,"Missing"
P18-1059,P11-1122,0,0.0707994,"Missing"
P18-1059,D17-1062,0,0.0158378,"ing e, the re-ranker selects keeping e. Indeed, our experimental results show that word changes increase with M (Figure 4), indicating that low coverage may play a role in the observed tendency of GEC systems to under-correct. No significant difference is found for word order. 638 3.4 Under-correction by Error Types lar trends recur in this setting as well. The tendency of TS systems to under-predict changes to the source has already been observed by previous work (Alva-Manchego et al., 2017), showing that TS systems under-predict word additions, deletions, substitutions, and sequence shifts (Zhang and Lapata, 2017), and have low edit distance from the source (Narayan and Gardent, 2016). Our experiments show that LCB may account for this under-prediction. Concretely, we show that (1) the distribution of valid references for a given sentence is long-tailed; (2) common evaluation measures suffer from LCB, taking SARI (Xu et al., 2016) as an example RBM (similar trends are obtained with Accuracy); (3) under-prediction is alleviated with M in oracle re-ranking experiments. In this section we study the prevalence of undercorrection according to edit types, finding that open-class types of errors (such as repl"
P18-1127,I17-2058,0,0.0549103,"y). In order to convert these distance measures into measures of similarity, we report 1 − LD(c1,c2) len(c1) . Grammaticality is a reference-less metric, which uses grammatical error detection tools to assess the grammaticality of GEC system outputs. We use LT (Miłkowski, 2010), the best performing non-proprietary grammaticality metric (Napoles et al., 2016b). The detection tool at the base of LT can be much improved. Indeed, Napoles et al. (2016b) reported that the proprietary tool they used detected 15 times more errors than LT. A #errors sentence’s score is defined to be 1 − #tokens . See (Asano et al., 2017; Choshen and Abend, 2018b) for additional reference-less measures, published concurrently with this work. I-Measure. I-Measure (Felice and Briscoe, 2015) is a weighted accuracy metric over tokens. I-measure rank determines whether a correction is better than the source and to what extent. Unlike in this paper, I-measure assumes that every pair of intersecting edits (i.e., edits whose spans of tokens overlap) are alternating, and that non-intersecting edits are independent. Consequently, where multiple references are present, it extends the set of references, by generating every possible combi"
P18-1127,D16-1134,1,0.881341,"al., 2015) fares best. In addition to measuring metric reliability, MAEGE can also be used to analyze the sensitivities of the metrics to corrections of different types, which to our knowledge is a novel contribution of this work. Specifically, we find that not only are valid edits of some error types better rewarded than others, but that correcting certain error types is consistently penalized by existing metrics (Section 7). The importance of interpretability and detail in evaluation practices (as opposed to just providing bottom-line figures), has also been stressed in MT evaluation (e.g., Birch et al., 2016). 2 Examined Metrics We turn to presenting the metrics we experiment with. The standard practice in GEC evaluation is to define differences between the source and a correction (or a reference) as a set of edits (Dale et al., 2012). An edit is a contiguous span of tokens to be edited, a substitute string, and the corrected error type. For example: “I want book” might have an edit (2-3, “a book”, ArtOrDet); applying the edit results in “I want a book”. Edits are defined (by the annotation guidelines) to be maximally independent, so that each edit can be applied independently of the others. We de"
P18-1127,W11-2101,0,0.0794188,"Missing"
P18-1127,W16-2302,0,0.0592175,"Missing"
P18-1127,P17-1074,0,0.0475335,"rce, and not only overlap with the reference: iBLEU (S, R, O) = αBLEU (O, R)−(1−α)BLEU (O, S) We set α = 0.8 as suggested by Sun and Zhou. F -Score computes the overlap of edits to the source in the reference, and in the output. As system edits can be constructed in multiple ways, the standard M 2 scorer (Dahlmeier and Ng, 2012) computes the set of edits that yields the maximum F -score. As M 2 requires edits from the source to the reference, and as MAEGE generates new source sentences, we use an established protocol to automatically construct edits from pairs of strings (Felice et al., 2016; Bryant et al., 2017). The protocol was shown to produce similar M 2 scores to those produced with manual edits. Following common practice, we use the Precision-oriented F0.5 . SARI. SARI (Xu et al., 2016) is a referencebased metric proposed for sentence simplification. 1373 SARI averages three scores, measuring the extent to which n-grams are correctly added to the source, deleted from it and retained in it. Where multiple references are present, SARI’s score is determined not as the maximum single-reference score, but some averaging over them. As this may lead to an unintuitive case, where a correction which is"
P18-1127,P15-1068,0,0.0426285,". Figure 3: An example chain from a corrections lattice – each sentence is the result of applying a single edit to the sentence below it. The top sentence is a perfect correction, while the bottom is the original. Figure 4: A scatter plot of the corpus-level correlation of metrics according to the different methodologies. The x-axis corresponds to the correlation according to human rankings (Combined setting), and the y-axis corresponds to the correlation according to MAEGE. While some get similar correlation (e.g., GLEU), other metrics change drastically (e.g., SARI). references, produced by Bryant and Ng (2015), are used as references for the reference-based metrics. Denote the set of references for s with Rs . Sentences which require no correction according to at least one of the two annotations are discarded. In 26 cases where two edit spans intersect in the same annotation (out of a total of about 40K edits), the edits are manually merged or split. 5 Corpus-level Analysis We conduct a corpus-level analysis, namely testing the ability of metrics to determine which corpus of corrections is of better quality. In practice, this procedure is used to rank systems based on their outputs on the test corp"
P18-1127,W14-3346,0,0.0280734,"each edit can be applied independently of the others. We denote the examined set of metrics with METRICS. BLEU. BLEU (Papineni et al., 2002) is a reference-based metric that averages the outputreference n-gram overlap precision values over different ns. While commonly used in MT and other text generation tasks (Sennrich et al., 2017; Krishna et al., 2017; Yu et al., 2017), BLEU was shown to be a problematic metric in monolingual translation tasks, in which much of the source sentence should remain unchanged (Xu et al., 2016). We use the NLTK implementation of BLEU, using smoothing method 3 by Chen and Cherry (2014). GLEU. GLEU (Napoles et al., 2015) is a reference-based GEC metric inspired by BLEU. Recently, it was updated to better address multiple references (Napoles et al., 2016a). GLEU rewards n-gram overlap of the correction with the reference and penalizes unchanged n-grams in the correction that are changed in the reference. iBLEU. iBLEU (Sun and Zhou, 2012) was introduced to monolingual translation in order to balance BLEU, by averaging it with the BLEU score of the source and the output. This yields a metric that rewards similarity to the source, and not only overlap with the reference: iBLEU ("
P18-1127,P18-1059,1,0.869242,"res no human rankings, and instead uses a corpus with gold standard GEC annotation to generate lattices of corrections with similar meanings but varying degrees of grammaticality. For each such lattice, MAEGE generates a partial order of correction quality, a quality score for each correction, and the number and types of edits required to fully correct each. It then computes the correlation of the induced partial order with the metric-induced rankings. MAEGE addresses many of the problems with existing methodology: • Human rankings yield low inter-rater and intra-rater agreement (§3). Indeed, Choshen and Abend (2018a) show that while annotators often generate different corrections given a sentence, they generally agree on whether a correction is valid or not. Unlike CHR, MAEGE bases its scores on human corrections, rather than on rankings. 1372 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1372–1382 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics • CHR uses system outputs to obtain human rankings, which may be misleading, as systems may share similar biases, thus neglecting to evaluate some types of"
P18-1127,N18-2020,1,0.835189,"res no human rankings, and instead uses a corpus with gold standard GEC annotation to generate lattices of corrections with similar meanings but varying degrees of grammaticality. For each such lattice, MAEGE generates a partial order of correction quality, a quality score for each correction, and the number and types of edits required to fully correct each. It then computes the correlation of the induced partial order with the metric-induced rankings. MAEGE addresses many of the problems with existing methodology: • Human rankings yield low inter-rater and intra-rater agreement (§3). Indeed, Choshen and Abend (2018a) show that while annotators often generate different corrections given a sentence, they generally agree on whether a correction is valid or not. Unlike CHR, MAEGE bases its scores on human corrections, rather than on rankings. 1372 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1372–1382 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics • CHR uses system outputs to obtain human rankings, which may be misleading, as systems may share similar biases, thus neglecting to evaluate some types of"
P18-1127,N12-1067,0,0.50311,"opose MAEGE, an automatic methodology for GEC metric validation, that overcomes many of the difficulties with existing practices. Experiments with MAEGE shed a new light on metric quality, showing for example that the standard M 2 metric fares poorly on corpus-level ranking. Moreover, we use MAEGE to perform a detailed analysis of metric behavior, showing that correcting some types of errors is consistently penalized by existing metrics. 1 Introduction Much recent effort has been devoted to automatic evaluation, both within GEC (Napoles et al., 2015; Felice and Briscoe, 2015; Ng et al., 2014; Dahlmeier and Ng, 2012, see §2), and more generally in text-to-text generation tasks. Within Machine Translation (MT), an annual shared task is devoted to automatic metric development, accompanied by an extensive analysis of metric behavior (Bojar et al., 2017). Metric validation is also raising interest in GEC, with several recent works on the subject (Grundkiewicz et al., 2015; Napoles et al., 2015, 2016b; Sakaguchi et al., 2016), all using correlation with human rankings (henceforth, CHR) as their methodology. Human rankings are often considered as ground truth in text-to-text generation, but using them reliably"
P18-1127,W13-1703,0,0.347151,"may share similar biases, thus neglecting to evaluate some types of valid corrections (§7). MAEGE addresses this issue by systematically traversing an inclusive space of corrections. • The difficulty in handling ties is addressed by only evaluating correction pairs where one contains a sub-set of the errors of the other, and is therefore clearly better. • MAEGE uses established statistical tests for determining the significance of its results, thereby avoiding ad-hoc methodologies used in CHR to tackle potential biases in human rankings (§5, §6). In experiments on the standard NUCLE test set (Dahlmeier et al., 2013), we find that MAEGE often disagrees with CHR as to the quality of existing metrics. For example, we find that the standard GEC metric, M 2 , is a poor predictor of corpuslevel ranking, but a good predictor of sentencelevel pair-wise rankings. The best predictor of corpus-level quality by MAEGE is the referenceless LT metric (Miłkowski, 2010; Napoles et al., 2016b), while of the reference-based metrics, GLEU (Napoles et al., 2015) fares best. In addition to measuring metric reliability, MAEGE can also be used to analyze the sensitivities of the metrics to corrections of different types, which"
P18-1127,W12-2006,0,0.0273758,"Specifically, we find that not only are valid edits of some error types better rewarded than others, but that correcting certain error types is consistently penalized by existing metrics (Section 7). The importance of interpretability and detail in evaluation practices (as opposed to just providing bottom-line figures), has also been stressed in MT evaluation (e.g., Birch et al., 2016). 2 Examined Metrics We turn to presenting the metrics we experiment with. The standard practice in GEC evaluation is to define differences between the source and a correction (or a reference) as a set of edits (Dale et al., 2012). An edit is a contiguous span of tokens to be edited, a substitute string, and the corrected error type. For example: “I want book” might have an edit (2-3, “a book”, ArtOrDet); applying the edit results in “I want a book”. Edits are defined (by the annotation guidelines) to be maximally independent, so that each edit can be applied independently of the others. We denote the examined set of metrics with METRICS. BLEU. BLEU (Papineni et al., 2002) is a reference-based metric that averages the outputreference n-gram overlap precision values over different ns. While commonly used in MT and other"
P18-1127,J15-2005,0,0.216531,"ir methodology. Human rankings are often considered as ground truth in text-to-text generation, but using them reliably can be challenging. Other than the costs of compiling a sizable validation set, human rankings are known to yield poor inter-rater agreement in MT (Bojar et al., 2011; Lopez, 2012; Graham et al., 2012), and to introduce a number of methodological problems that are difficult to overcome, notably the treatment of ties in the rankings and uncomparable sentences (see §3). These difficulties have motivated several proposals to alter the MT metric validation protocol (Koehn, 2012; Dras, 2015), leading to a recent abandoning of evaluation by human rankings due to its unreliability (Graham et al., 2015; Bojar et al., 2016). These conclusions have not yet been implemented in GEC, despite their relevance. In §3 we show that human rankings in GEC also suffer from low inter-rater agreement, motivating the development of alternative methodologies. The main contribution of this paper is an automatic methodology for metric validation in GEC called MAEGE (Methodology for Automatic Evaluation of GEC Evaluation), which addresses these difficulties. MAEGE requires no human rankings, and instea"
P18-1127,N15-1060,0,0.655212,"uffer from low inter-rater agreement. We propose MAEGE, an automatic methodology for GEC metric validation, that overcomes many of the difficulties with existing practices. Experiments with MAEGE shed a new light on metric quality, showing for example that the standard M 2 metric fares poorly on corpus-level ranking. Moreover, we use MAEGE to perform a detailed analysis of metric behavior, showing that correcting some types of errors is consistently penalized by existing metrics. 1 Introduction Much recent effort has been devoted to automatic evaluation, both within GEC (Napoles et al., 2015; Felice and Briscoe, 2015; Ng et al., 2014; Dahlmeier and Ng, 2012, see §2), and more generally in text-to-text generation tasks. Within Machine Translation (MT), an annual shared task is devoted to automatic metric development, accompanied by an extensive analysis of metric behavior (Bojar et al., 2017). Metric validation is also raising interest in GEC, with several recent works on the subject (Grundkiewicz et al., 2015; Napoles et al., 2015, 2016b; Sakaguchi et al., 2016), all using correlation with human rankings (henceforth, CHR) as their methodology. Human rankings are often considered as ground truth in text-to"
P18-1127,C16-1079,0,0.0138096,"similarity to the source, and not only overlap with the reference: iBLEU (S, R, O) = αBLEU (O, R)−(1−α)BLEU (O, S) We set α = 0.8 as suggested by Sun and Zhou. F -Score computes the overlap of edits to the source in the reference, and in the output. As system edits can be constructed in multiple ways, the standard M 2 scorer (Dahlmeier and Ng, 2012) computes the set of edits that yields the maximum F -score. As M 2 requires edits from the source to the reference, and as MAEGE generates new source sentences, we use an established protocol to automatically construct edits from pairs of strings (Felice et al., 2016; Bryant et al., 2017). The protocol was shown to produce similar M 2 scores to those produced with manual edits. Following common practice, we use the Precision-oriented F0.5 . SARI. SARI (Xu et al., 2016) is a referencebased metric proposed for sentence simplification. 1373 SARI averages three scores, measuring the extent to which n-grams are correctly added to the source, deleted from it and retained in it. Where multiple references are present, SARI’s score is determined not as the maximum single-reference score, but some averaging over them. As this may lead to an unintuitive case, where"
P18-1127,U12-1010,0,0.0251183,"xtensive analysis of metric behavior (Bojar et al., 2017). Metric validation is also raising interest in GEC, with several recent works on the subject (Grundkiewicz et al., 2015; Napoles et al., 2015, 2016b; Sakaguchi et al., 2016), all using correlation with human rankings (henceforth, CHR) as their methodology. Human rankings are often considered as ground truth in text-to-text generation, but using them reliably can be challenging. Other than the costs of compiling a sizable validation set, human rankings are known to yield poor inter-rater agreement in MT (Bojar et al., 2011; Lopez, 2012; Graham et al., 2012), and to introduce a number of methodological problems that are difficult to overcome, notably the treatment of ties in the rankings and uncomparable sentences (see §3). These difficulties have motivated several proposals to alter the MT metric validation protocol (Koehn, 2012; Dras, 2015), leading to a recent abandoning of evaluation by human rankings due to its unreliability (Graham et al., 2015; Bojar et al., 2016). These conclusions have not yet been implemented in GEC, despite their relevance. In §3 we show that human rankings in GEC also suffer from low inter-rater agreement, motivating"
P18-1127,N15-1124,0,0.046958,"ing them reliably can be challenging. Other than the costs of compiling a sizable validation set, human rankings are known to yield poor inter-rater agreement in MT (Bojar et al., 2011; Lopez, 2012; Graham et al., 2012), and to introduce a number of methodological problems that are difficult to overcome, notably the treatment of ties in the rankings and uncomparable sentences (see §3). These difficulties have motivated several proposals to alter the MT metric validation protocol (Koehn, 2012; Dras, 2015), leading to a recent abandoning of evaluation by human rankings due to its unreliability (Graham et al., 2015; Bojar et al., 2016). These conclusions have not yet been implemented in GEC, despite their relevance. In §3 we show that human rankings in GEC also suffer from low inter-rater agreement, motivating the development of alternative methodologies. The main contribution of this paper is an automatic methodology for metric validation in GEC called MAEGE (Methodology for Automatic Evaluation of GEC Evaluation), which addresses these difficulties. MAEGE requires no human rankings, and instead uses a corpus with gold standard GEC annotation to generate lattices of corrections with similar meanings bu"
P18-1127,D15-1052,0,0.237052,"g that correcting some types of errors is consistently penalized by existing metrics. 1 Introduction Much recent effort has been devoted to automatic evaluation, both within GEC (Napoles et al., 2015; Felice and Briscoe, 2015; Ng et al., 2014; Dahlmeier and Ng, 2012, see §2), and more generally in text-to-text generation tasks. Within Machine Translation (MT), an annual shared task is devoted to automatic metric development, accompanied by an extensive analysis of metric behavior (Bojar et al., 2017). Metric validation is also raising interest in GEC, with several recent works on the subject (Grundkiewicz et al., 2015; Napoles et al., 2015, 2016b; Sakaguchi et al., 2016), all using correlation with human rankings (henceforth, CHR) as their methodology. Human rankings are often considered as ground truth in text-to-text generation, but using them reliably can be challenging. Other than the costs of compiling a sizable validation set, human rankings are known to yield poor inter-rater agreement in MT (Bojar et al., 2011; Lopez, 2012; Graham et al., 2012), and to introduce a number of methodological problems that are difficult to overcome, notably the treatment of ties in the rankings and uncomparable sentenc"
P18-1127,2012.iwslt-papers.5,0,0.0284974,", CHR) as their methodology. Human rankings are often considered as ground truth in text-to-text generation, but using them reliably can be challenging. Other than the costs of compiling a sizable validation set, human rankings are known to yield poor inter-rater agreement in MT (Bojar et al., 2011; Lopez, 2012; Graham et al., 2012), and to introduce a number of methodological problems that are difficult to overcome, notably the treatment of ties in the rankings and uncomparable sentences (see §3). These difficulties have motivated several proposals to alter the MT metric validation protocol (Koehn, 2012; Dras, 2015), leading to a recent abandoning of evaluation by human rankings due to its unreliability (Graham et al., 2015; Bojar et al., 2016). These conclusions have not yet been implemented in GEC, despite their relevance. In §3 we show that human rankings in GEC also suffer from low inter-rater agreement, motivating the development of alternative methodologies. The main contribution of this paper is an automatic methodology for metric validation in GEC called MAEGE (Methodology for Automatic Evaluation of GEC Evaluation), which addresses these difficulties. MAEGE requires no human ranking"
P18-1127,W12-3101,0,0.180374,"anied by an extensive analysis of metric behavior (Bojar et al., 2017). Metric validation is also raising interest in GEC, with several recent works on the subject (Grundkiewicz et al., 2015; Napoles et al., 2015, 2016b; Sakaguchi et al., 2016), all using correlation with human rankings (henceforth, CHR) as their methodology. Human rankings are often considered as ground truth in text-to-text generation, but using them reliably can be challenging. Other than the costs of compiling a sizable validation set, human rankings are known to yield poor inter-rater agreement in MT (Bojar et al., 2011; Lopez, 2012; Graham et al., 2012), and to introduce a number of methodological problems that are difficult to overcome, notably the treatment of ties in the rankings and uncomparable sentences (see §3). These difficulties have motivated several proposals to alter the MT metric validation protocol (Koehn, 2012; Dras, 2015), leading to a recent abandoning of evaluation by human rankings due to its unreliability (Graham et al., 2015; Bojar et al., 2016). These conclusions have not yet been implemented in GEC, despite their relevance. In §3 we show that human rankings in GEC also suffer from low inter-rater"
P18-1127,P15-2097,0,0.679419,"lly troublesome, and suffer from low inter-rater agreement. We propose MAEGE, an automatic methodology for GEC metric validation, that overcomes many of the difficulties with existing practices. Experiments with MAEGE shed a new light on metric quality, showing for example that the standard M 2 metric fares poorly on corpus-level ranking. Moreover, we use MAEGE to perform a detailed analysis of metric behavior, showing that correcting some types of errors is consistently penalized by existing metrics. 1 Introduction Much recent effort has been devoted to automatic evaluation, both within GEC (Napoles et al., 2015; Felice and Briscoe, 2015; Ng et al., 2014; Dahlmeier and Ng, 2012, see §2), and more generally in text-to-text generation tasks. Within Machine Translation (MT), an annual shared task is devoted to automatic metric development, accompanied by an extensive analysis of metric behavior (Bojar et al., 2017). Metric validation is also raising interest in GEC, with several recent works on the subject (Grundkiewicz et al., 2015; Napoles et al., 2015, 2016b; Sakaguchi et al., 2016), all using correlation with human rankings (henceforth, CHR) as their methodology. Human rankings are often considered"
P18-1127,D16-1228,0,0.219311,"Missing"
P18-1127,W14-1701,0,0.574051,"agreement. We propose MAEGE, an automatic methodology for GEC metric validation, that overcomes many of the difficulties with existing practices. Experiments with MAEGE shed a new light on metric quality, showing for example that the standard M 2 metric fares poorly on corpus-level ranking. Moreover, we use MAEGE to perform a detailed analysis of metric behavior, showing that correcting some types of errors is consistently penalized by existing metrics. 1 Introduction Much recent effort has been devoted to automatic evaluation, both within GEC (Napoles et al., 2015; Felice and Briscoe, 2015; Ng et al., 2014; Dahlmeier and Ng, 2012, see §2), and more generally in text-to-text generation tasks. Within Machine Translation (MT), an annual shared task is devoted to automatic metric development, accompanied by an extensive analysis of metric behavior (Bojar et al., 2017). Metric validation is also raising interest in GEC, with several recent works on the subject (Grundkiewicz et al., 2015; Napoles et al., 2015, 2016b; Sakaguchi et al., 2016), all using correlation with human rankings (henceforth, CHR) as their methodology. Human rankings are often considered as ground truth in text-to-text generation,"
P18-1127,P02-1040,0,0.121895,"xperiment with. The standard practice in GEC evaluation is to define differences between the source and a correction (or a reference) as a set of edits (Dale et al., 2012). An edit is a contiguous span of tokens to be edited, a substitute string, and the corrected error type. For example: “I want book” might have an edit (2-3, “a book”, ArtOrDet); applying the edit results in “I want a book”. Edits are defined (by the annotation guidelines) to be maximally independent, so that each edit can be applied independently of the others. We denote the examined set of metrics with METRICS. BLEU. BLEU (Papineni et al., 2002) is a reference-based metric that averages the outputreference n-gram overlap precision values over different ns. While commonly used in MT and other text generation tasks (Sennrich et al., 2017; Krishna et al., 2017; Yu et al., 2017), BLEU was shown to be a problematic metric in monolingual translation tasks, in which much of the source sentence should remain unchanged (Xu et al., 2016). We use the NLTK implementation of BLEU, using smoothing method 3 by Chen and Cherry (2014). GLEU. GLEU (Napoles et al., 2015) is a reference-based GEC metric inspired by BLEU. Recently, it was updated to bett"
P18-1127,Q16-1013,0,0.381123,"enalized by existing metrics. 1 Introduction Much recent effort has been devoted to automatic evaluation, both within GEC (Napoles et al., 2015; Felice and Briscoe, 2015; Ng et al., 2014; Dahlmeier and Ng, 2012, see §2), and more generally in text-to-text generation tasks. Within Machine Translation (MT), an annual shared task is devoted to automatic metric development, accompanied by an extensive analysis of metric behavior (Bojar et al., 2017). Metric validation is also raising interest in GEC, with several recent works on the subject (Grundkiewicz et al., 2015; Napoles et al., 2015, 2016b; Sakaguchi et al., 2016), all using correlation with human rankings (henceforth, CHR) as their methodology. Human rankings are often considered as ground truth in text-to-text generation, but using them reliably can be challenging. Other than the costs of compiling a sizable validation set, human rankings are known to yield poor inter-rater agreement in MT (Bojar et al., 2011; Lopez, 2012; Graham et al., 2012), and to introduce a number of methodological problems that are difficult to overcome, notably the treatment of ties in the rankings and uncomparable sentences (see §3). These difficulties have motivated several"
P18-1127,W14-3301,0,0.0403954,"Missing"
P18-1127,E17-3017,0,0.0294501,"s span of tokens to be edited, a substitute string, and the corrected error type. For example: “I want book” might have an edit (2-3, “a book”, ArtOrDet); applying the edit results in “I want a book”. Edits are defined (by the annotation guidelines) to be maximally independent, so that each edit can be applied independently of the others. We denote the examined set of metrics with METRICS. BLEU. BLEU (Papineni et al., 2002) is a reference-based metric that averages the outputreference n-gram overlap precision values over different ns. While commonly used in MT and other text generation tasks (Sennrich et al., 2017; Krishna et al., 2017; Yu et al., 2017), BLEU was shown to be a problematic metric in monolingual translation tasks, in which much of the source sentence should remain unchanged (Xu et al., 2016). We use the NLTK implementation of BLEU, using smoothing method 3 by Chen and Cherry (2014). GLEU. GLEU (Napoles et al., 2015) is a reference-based GEC metric inspired by BLEU. Recently, it was updated to better address multiple references (Napoles et al., 2016a). GLEU rewards n-gram overlap of the correction with the reference and penalizes unchanged n-grams in the correction that are changed in the"
P18-1127,P12-2008,0,0.0302485,"et al., 2017), BLEU was shown to be a problematic metric in monolingual translation tasks, in which much of the source sentence should remain unchanged (Xu et al., 2016). We use the NLTK implementation of BLEU, using smoothing method 3 by Chen and Cherry (2014). GLEU. GLEU (Napoles et al., 2015) is a reference-based GEC metric inspired by BLEU. Recently, it was updated to better address multiple references (Napoles et al., 2016a). GLEU rewards n-gram overlap of the correction with the reference and penalizes unchanged n-grams in the correction that are changed in the reference. iBLEU. iBLEU (Sun and Zhou, 2012) was introduced to monolingual translation in order to balance BLEU, by averaging it with the BLEU score of the source and the output. This yields a metric that rewards similarity to the source, and not only overlap with the reference: iBLEU (S, R, O) = αBLEU (O, R)−(1−α)BLEU (O, S) We set α = 0.8 as suggested by Sun and Zhou. F -Score computes the overlap of edits to the source in the reference, and in the output. As system edits can be constructed in multiple ways, the standard M 2 scorer (Dahlmeier and Ng, 2012) computes the set of edits that yields the maximum F -score. As M 2 requires edi"
P18-1127,Q16-1029,0,0.107511,"Edits are defined (by the annotation guidelines) to be maximally independent, so that each edit can be applied independently of the others. We denote the examined set of metrics with METRICS. BLEU. BLEU (Papineni et al., 2002) is a reference-based metric that averages the outputreference n-gram overlap precision values over different ns. While commonly used in MT and other text generation tasks (Sennrich et al., 2017; Krishna et al., 2017; Yu et al., 2017), BLEU was shown to be a problematic metric in monolingual translation tasks, in which much of the source sentence should remain unchanged (Xu et al., 2016). We use the NLTK implementation of BLEU, using smoothing method 3 by Chen and Cherry (2014). GLEU. GLEU (Napoles et al., 2015) is a reference-based GEC metric inspired by BLEU. Recently, it was updated to better address multiple references (Napoles et al., 2016a). GLEU rewards n-gram overlap of the correction with the reference and penalizes unchanged n-grams in the correction that are changed in the reference. iBLEU. iBLEU (Sun and Zhou, 2012) was introduced to monolingual translation in order to balance BLEU, by averaging it with the BLEU score of the source and the output. This yields a me"
P18-1127,W14-3302,0,\N,Missing
P18-1127,W17-4717,0,\N,Missing
P19-1419,E17-1004,0,0.0640293,"Missing"
P19-1419,W06-1615,0,0.0200078,"adata and URLs. We remove such text from the web pages, and join paragraphs to single lines (as newlines are sometimes present in the original dataset for display purposes only). We then remove any duplicate paragraphs, where paragraphs are considered identical if they share all but numbers (to avoid an over-representation of some remaining surrounding text from the websites, e.g. “Showing all 9 results”). 4 Domain Differences As pointed out by Plank (2011), there is no common ground as to what constitutes a domain. Domain differences are attributed in some works to differences in vocabulary (Blitzer et al., 2006) and in other works to differences in style, genre and medium (McClosky, 2010). While here we adopt an existing classification, based on the DUTA-10K corpus, we show in which way and to what extent it translates to distinct properties of the texts. This question bears on the possibility of distinguishing between legal and illegal drug-related websites based on their text alone (i.e., without recourse to additional information, such as meta-data or network structure). We examine two types of domain differences between legal and illegal texts: vocabulary differences and named entities. 4.1 Vocab"
P19-1419,E06-1002,0,0.266042,"Missing"
P19-1419,N10-1004,0,0.0227419,"e lines (as newlines are sometimes present in the original dataset for display purposes only). We then remove any duplicate paragraphs, where paragraphs are considered identical if they share all but numbers (to avoid an over-representation of some remaining surrounding text from the websites, e.g. “Showing all 9 results”). 4 Domain Differences As pointed out by Plank (2011), there is no common ground as to what constitutes a domain. Domain differences are attributed in some works to differences in vocabulary (Blitzer et al., 2006) and in other works to differences in style, genre and medium (McClosky, 2010). While here we adopt an existing classification, based on the DUTA-10K corpus, we show in which way and to what extent it translates to distinct properties of the texts. This question bears on the possibility of distinguishing between legal and illegal drug-related websites based on their text alone (i.e., without recourse to additional information, such as meta-data or network structure). We examine two types of domain differences between legal and illegal texts: vocabulary differences and named entities. 4.1 Vocabulary Differences To quantify the domain differences between texts from legal"
P19-1419,D14-1162,0,0.0870356,"Missing"
P19-1419,N18-1202,0,0.0379337,"ensional fully-connected layer with ReLU nonlinearity and dropout p = 0.2. The word vectors are not updated during training. Vectors for words not found in GloVe are set 2 randomly ∼ N (µGloVe , σGloVe ). • seq2vec: same as BoE, but instead of averaging word vectors, we apply a single-layer 100-dimensional BiLSTM to the word vectors, and take the concatenated final hidden vectors from the forward and backward part as the input to a fully-connected layer (same hyper-parameters as above). • attention: we replace the word representations with contextualized pre-trained representations from ELMo (Peters et al., 2018). We then apply a self-attentive classification network (McCann et al., 2017) over the contextualized representations. This architecture has proved very effective for classification in recent work (Tutek and Šnajder, 2018; Shen et al., 2018). For the NB classifier we use BernoulliNB from scikit-learn7 with α = 1, and for the SVM classifier we use SVC, also from scikit-learn, with γ = scale and tolerance=10−5 . We use the AllenNLP library8 (Gardner et al., 2018) to implement the neural network classifiers. 7 8 https://scikit-learn.org https://allennlp.org Data manipulation. In order to isolate"
P19-1419,P11-1157,0,0.0651403,"Missing"
P19-1419,W18-5427,0,0.0134599,"me as BoE, but instead of averaging word vectors, we apply a single-layer 100-dimensional BiLSTM to the word vectors, and take the concatenated final hidden vectors from the forward and backward part as the input to a fully-connected layer (same hyper-parameters as above). • attention: we replace the word representations with contextualized pre-trained representations from ELMo (Peters et al., 2018). We then apply a self-attentive classification network (McCann et al., 2017) over the contextualized representations. This architecture has proved very effective for classification in recent work (Tutek and Šnajder, 2018; Shen et al., 2018). For the NB classifier we use BernoulliNB from scikit-learn7 with α = 1, and for the SVM classifier we use SVC, also from scikit-learn, with γ = scale and tolerance=10−5 . We use the AllenNLP library8 (Gardner et al., 2018) to implement the neural network classifiers. 7 8 https://scikit-learn.org https://allennlp.org Data manipulation. In order to isolate what factors contribute to the classifiers’ performance, we experiment with four manipulations to the input data (in training, validation and testing). Specifically, we examine the impact of variations in the content word"
P19-1419,W18-2501,0,0.0292199,"Missing"
P19-1419,D18-1401,0,\N,Missing
S19-2001,P13-1023,1,0.793151,"anguages and settings. Full results can be found in the task’s website https://competitions. codalab.org/competitions/19160. 1 Overview Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes have recently been put forth. Examples include Abstract Meaning Representation (AMR; Banarescu et al., 2013), Broad-coverage Semantic Dependencies (SDP; Oepen et al., 2016), Universal Decompositional Semantics (UDS; White et al., 2016), Parallel Meaning Bank (Abzianidze et al., 2017), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). These advances in semantic representation, along with corresponding advances in semantic parsing, can potentially benefit essentially all text understanding tasks, and have already demonstrated applicability to a variety of tasks, including summarization (Liu et al., 2015; Dohare and Karnick, 2017), paraphrase detection (Issa et al., 2018), and semantic evaluation (using UCCA; see below). In this shared task, we focus on UCCA parsing in multiple languages. U After , A P graduation John A P A moved R to C Paris Figure 1: An example UCCA graph. One of our goals is to benefit semantic parsing i"
S19-2001,P17-1104,1,0.828203,"in UCCA’s foundational layer. tion (Sulem et al., 2018b), as well as for defining semantic evaluation measures for text-to-text generation tasks, including machine translation (Birch et al., 2016), text simplification (Sulem et al., 2018a) and grammatical error correction (Choshen and Abend, 2018). The shared task defines a number of tracks, based on the different corpora and the availability of external resources (see §5). It received submissions from eight research groups around the world. In all settings at least one of the submitted systems improved over the state-of-the-art TUPA parser (Hershcovich et al., 2017, 2018), used as a baseline. 2 Task Definition UCCA represents the semantics of linguistic utterances as directed acyclic graphs (DAGs), where terminal (childless) nodes correspond to the text tokens, and non-terminal nodes to semantic units that participate in some super-ordinate relation. Edges are labeled, indicating the role of a child in the relation the parent represents. Nodes and edges belong to one of several layers, each corresponding to a “module” of semantic distinctions. UCCA’s foundational layer covers the predicate-argument structure evoked by predicates of all grammatical categ"
S19-2001,P17-1008,1,0.873462,"e model TUPA (which has been the only available parser for UCCA), the task opens a variety of paths for future improvement. Cross-lingual transfer, which capitalizes on UCCA’s tendency to be preserved in translation, was employed by a number of systems and has proven remarkably effective. Indeed, the high scores obtained for French parsing in a low-resource setting suggest that high quality UCCA parsing can be straightforwardly extended to additional languages, with only a minimal amount of manual labor. Moreover, given the conceptual similarity between the different semantic representations (Abend and Rappoport, 2017), it is likely the parsers developed for the shared task will directly contribute to the development of other semantic parsing technology. Such a contribution is facilitated by the available conversion scripts available between UCCA and other formats. Acknowledgments We are deeply grateful to Dotan Dvir and the UCCA annotation team for their diligent work on the corpora used in this shared task. This work was supported by the Israel Science Foundation (grant No. 929/17), and by the HUJI Cyber Security Research Center in conjunction with the Israel National Cyber Bureau in the Prime Minister’s"
S19-2001,P18-1035,1,0.686571,"Missing"
S19-2001,P17-4019,1,0.839017,"across predicates), and the modeling of the interface with lexical semantics. UCCA is a cross-linguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010b,a, 2012). It has demonstrated applicability to multiple languages, including English, French and German, and pilot annotation projects were conducted on a few languages more. UCCA structures have been shown to be well-preserved in translation (Sulem et al., 2015), and to support rapid annotation by nonexperts, assisted by an accessible annotation interface (Abend et al., 2017).1 UCCA has already shown applicative value for text simplifica1 https://github.com/omriabnd/UCCA-App P S A D Process State Participant Adverbial C E N R Center Elaborator Connector Relator H L G Parallel Scene Linker Ground F Function Scene Elements The main relation of a Scene that evolves in time (usually an action or movement). The main relation of a Scene that does not evolve in time. Scene participant (including locations, abstract entities and Scenes serving as arguments). A secondary relation in a Scene. Elements of Non-Scene Units Necessary for the conceptualization of the parent unit"
S19-2001,N18-1041,0,0.0193142,"escu et al., 2013), Broad-coverage Semantic Dependencies (SDP; Oepen et al., 2016), Universal Decompositional Semantics (UDS; White et al., 2016), Parallel Meaning Bank (Abzianidze et al., 2017), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). These advances in semantic representation, along with corresponding advances in semantic parsing, can potentially benefit essentially all text understanding tasks, and have already demonstrated applicability to a variety of tasks, including summarization (Liu et al., 2015; Dohare and Karnick, 2017), paraphrase detection (Issa et al., 2018), and semantic evaluation (using UCCA; see below). In this shared task, we focus on UCCA parsing in multiple languages. U After , A P graduation John A P A moved R to C Paris Figure 1: An example UCCA graph. One of our goals is to benefit semantic parsing in languages with less annotated resources by making use of data from more resource-rich languages. We refer to this approach as cross-lingual parsing, while other works (Zhang et al., 2017, 2018) define cross-lingual parsing as the task of parsing text in one language to meaning representation in another language. In addition to its potentia"
S19-2001,E17-2039,0,0.166987,"Missing"
S19-2001,S19-2002,0,0.452346,"Missing"
S19-2001,W13-2322,0,0.360278,"ing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. The shared task has yielded improvements over the state-of-the-art baseline in all languages and settings. Full results can be found in the task’s website https://competitions. codalab.org/competitions/19160. 1 Overview Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes have recently been put forth. Examples include Abstract Meaning Representation (AMR; Banarescu et al., 2013), Broad-coverage Semantic Dependencies (SDP; Oepen et al., 2016), Universal Decompositional Semantics (UDS; White et al., 2016), Parallel Meaning Bank (Abzianidze et al., 2017), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). These advances in semantic representation, along with corresponding advances in semantic parsing, can potentially benefit essentially all text understanding tasks, and have already demonstrated applicability to a variety of tasks, including summarization (Liu et al., 2015; Dohare and Karnick, 2017), paraphrase detection (Issa et al., 2018)"
S19-2001,D16-1134,1,0.805636,"nt aspects of the parent unit. Inter-Scene Relations A Scene linked to other Scenes by regular linkage (e.g., temporal, logical, purposive). A relation between two or more Hs (e.g., “when”, “if”, “in order to”). A relation between the speech event and the uttered Scene (e.g., “surprisingly”). Other Does not introduce a relation or participant. Required by some structural pattern. Table 1: The complete set of categories in UCCA’s foundational layer. tion (Sulem et al., 2018b), as well as for defining semantic evaluation measures for text-to-text generation tasks, including machine translation (Birch et al., 2016), text simplification (Sulem et al., 2018a) and grammatical error correction (Choshen and Abend, 2018). The shared task defines a number of tracks, based on the different corpora and the availability of external resources (see §5). It received submissions from eight research groups around the world. In all settings at least one of the submitted systems improved over the state-of-the-art TUPA parser (Hershcovich et al., 2017, 2018), used as a baseline. 2 Task Definition UCCA represents the semantics of linguistic utterances as directed acyclic graphs (DAGs), where terminal (childless) nodes cor"
S19-2001,Q17-1010,0,0.00930481,"urces, we held both an open and a closed track for submissions in the English and German settings. Closed track submissions were only allowed to use the gold-standard UCCA annotation distributed for the task in the target language, and were limited in their use of additional resources. Concretely, the only additional data they were allowed to use is that used by TUPA, which consists of automatic annotations provided by spaCy:10 POS tags, syntactic dependency relations, and named entity types and spans. In addition, the closed track only allowed the use of word embeddings provided by fastText (Bojanowski et al., 2017)11 for all languages. Systems in the open track, on the other hand, were allowed to use any additional resource, such as UCCA annotation in other languages, dictionaries or datasets for other tasks, provided that they make sure not to use any additional gold standard annotation over the same text used in the UCCA 10 11 http://spacy.io http://fasttext.cc corpora.12 In both tracks, we required that submitted systems are not trained on the development data. We only held an open track for French, due to the paucity of training data. The four settings and two tracks result in a total of 7 competiti"
S19-2001,N18-2020,1,0.825476,"nkage (e.g., temporal, logical, purposive). A relation between two or more Hs (e.g., “when”, “if”, “in order to”). A relation between the speech event and the uttered Scene (e.g., “surprisingly”). Other Does not introduce a relation or participant. Required by some structural pattern. Table 1: The complete set of categories in UCCA’s foundational layer. tion (Sulem et al., 2018b), as well as for defining semantic evaluation measures for text-to-text generation tasks, including machine translation (Birch et al., 2016), text simplification (Sulem et al., 2018a) and grammatical error correction (Choshen and Abend, 2018). The shared task defines a number of tracks, based on the different corpora and the availability of external resources (see §5). It received submissions from eight research groups around the world. In all settings at least one of the submitted systems improved over the state-of-the-art TUPA parser (Hershcovich et al., 2017, 2018), used as a baseline. 2 Task Definition UCCA represents the semantics of linguistic utterances as directed acyclic graphs (DAGs), where terminal (childless) nodes correspond to the text tokens, and non-terminal nodes to semantic units that participate in some super-or"
S19-2001,N19-1423,0,0.0188459,"Three of the teams adapted the baseline TUPA parser, or parts of it to form their parser, namely TüPa, CUNY-PekingU and XLangMo; HLT@SUDA used a constituency parser (Stern et al., 2017) as a component in their model; DANGNT@UIT.VNU-HCM is a rule-based system over the Stanford Parser, and the rest are newly constructed parsers. All teams found it useful to use external resources beyond those provided by the Shared Task. Four submissions used external embeddings, MUSE (Conneau et al., 2017) in the case of MaskParse@Deskiñ and XLangMo, ELMo (Peters et al., 2018) in the case of TüPa,15 and BERT (Devlin et al., 2019) in the case of HLT@SUDA. 15 GCN-Sem used ELMo in the closed tracks, training on the available data. # Team English-Wiki (closed) 1 HLT@SUDA 2 baseline 3 Davis 4 CUNY-PekingU 5 DANGNT@UIT. VNU-HCM 6 GCN-Sem English-Wiki (open) 1 HLT@SUDA 2 baseline 3 TüPa 4 XLangMo 5 DANGNT@UIT. VNU-HCM English-20K (closed) 1 HLT@SUDA 2 baseline 3 CUNY-PekingU 4 GCN-Sem English-20K (open) 1 HLT@SUDA 2 TüPa 3 XLangMo 4 baseline German-20K (closed) 1 HLT@SUDA 2 CUNY-PekingU 3 baseline 4 GCN-Sem German-20K (open) 1 HLT@SUDA 2 baseline 3 TüPa 4 XLangMo French-20K (open) 1 HLT@SUDA 2 XLangMo 3 MaskParse@Deskiñ 4 ba"
S19-2001,N15-1114,0,0.02374,"forth. Examples include Abstract Meaning Representation (AMR; Banarescu et al., 2013), Broad-coverage Semantic Dependencies (SDP; Oepen et al., 2016), Universal Decompositional Semantics (UDS; White et al., 2016), Parallel Meaning Bank (Abzianidze et al., 2017), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). These advances in semantic representation, along with corresponding advances in semantic parsing, can potentially benefit essentially all text understanding tasks, and have already demonstrated applicability to a variety of tasks, including summarization (Liu et al., 2015; Dohare and Karnick, 2017), paraphrase detection (Issa et al., 2018), and semantic evaluation (using UCCA; see below). In this shared task, we focus on UCCA parsing in multiple languages. U After , A P graduation John A P A moved R to C Paris Figure 1: An example UCCA graph. One of our goals is to benefit semantic parsing in languages with less annotated resources by making use of data from more resource-rich languages. We refer to this approach as cross-lingual parsing, while other works (Zhang et al., 2017, 2018) define cross-lingual parsing as the task of parsing text in one language to me"
S19-2001,S19-2012,0,0.0438323,"Missing"
S19-2001,S19-2015,0,0.0128217,"= `2 . Labeled precision and recall are defined by dividing the number of matching edges in G1 and G2 by |E1 |and |E2 |, respectively. F1 is their harmonic mean: 2· Precision · Recall Precision + Recall Unlabeled precision, recall and F1 are the same, but without requiring that `1 = `2 for the edges to match. We evaluate these measures for primary and remote edges separately. For a more finegrained evaluation, we additionally report precision, recall and F1 on edges of each category.13 6 Participating Systems We received a total of eight submissions to the different tracks: MaskParse@Deskiñ (Marzinotto et al., 2019) from Orange Labs and Aix-Marseille University, HLT@SUDA (Jiang et al., 2019) from Soochow University, TüPa (Pütz and Glocker, 2019) from the University of Tübingen, UC Davis (Yu and Sagae, 2019) from the University of California, Davis , GCN-Sem (Taslimipoor et al., 2019) from the University of Wolverhampton, CUNY-PekingU (Lyu et al., 2019) from the City University of New York and Peking University, DANGNT@UIT.VNU-HCM (Nguyen and Tran, 2019) from the University of Information Technology VNU-HCM, and XLangMo from Zhejiang University. Two systems (HLT@SUDA and CUNY-PekingU) participated in all"
S19-2001,S19-2013,0,0.0121127,"recall and F1 on edges of each category.13 6 Participating Systems We received a total of eight submissions to the different tracks: MaskParse@Deskiñ (Marzinotto et al., 2019) from Orange Labs and Aix-Marseille University, HLT@SUDA (Jiang et al., 2019) from Soochow University, TüPa (Pütz and Glocker, 2019) from the University of Tübingen, UC Davis (Yu and Sagae, 2019) from the University of California, Davis , GCN-Sem (Taslimipoor et al., 2019) from the University of Wolverhampton, CUNY-PekingU (Lyu et al., 2019) from the City University of New York and Peking University, DANGNT@UIT.VNU-HCM (Nguyen and Tran, 2019) from the University of Information Technology VNU-HCM, and XLangMo from Zhejiang University. Two systems (HLT@SUDA and CUNY-PekingU) participated in all the tracks.14 12 We are not aware of any such annotation, but include this restriction for completeness. 13 The official evaluation script providing both coarse-grained and fine-grained scores can be found in https://github.com/huji-nlp/ucca/blob/ master/scripts/evaluate_standard.py. 14 It was later discovered that CUNY-PekingU used some of the evaluation data for training in the open tracks, and they were thus disqualified from these tracks."
S19-2001,P05-1013,0,0.222956,"Missing"
S19-2001,L16-1630,0,0.0839468,"structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. The shared task has yielded improvements over the state-of-the-art baseline in all languages and settings. Full results can be found in the task’s website https://competitions. codalab.org/competitions/19160. 1 Overview Semantic representation is receiving growing attention in NLP in the past few years, and many proposals for semantic schemes have recently been put forth. Examples include Abstract Meaning Representation (AMR; Banarescu et al., 2013), Broad-coverage Semantic Dependencies (SDP; Oepen et al., 2016), Universal Decompositional Semantics (UDS; White et al., 2016), Parallel Meaning Bank (Abzianidze et al., 2017), and Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013). These advances in semantic representation, along with corresponding advances in semantic parsing, can potentially benefit essentially all text understanding tasks, and have already demonstrated applicability to a variety of tasks, including summarization (Liu et al., 2015; Dohare and Karnick, 2017), paraphrase detection (Issa et al., 2018), and semantic evaluation (using UCCA; see below). In this share"
S19-2001,S15-2153,0,0.699415,"Missing"
S19-2001,N18-1202,0,0.0167804,"g the number of non-terminal nodes in the UCCA graphs. Three of the teams adapted the baseline TUPA parser, or parts of it to form their parser, namely TüPa, CUNY-PekingU and XLangMo; HLT@SUDA used a constituency parser (Stern et al., 2017) as a component in their model; DANGNT@UIT.VNU-HCM is a rule-based system over the Stanford Parser, and the rest are newly constructed parsers. All teams found it useful to use external resources beyond those provided by the Shared Task. Four submissions used external embeddings, MUSE (Conneau et al., 2017) in the case of MaskParse@Deskiñ and XLangMo, ELMo (Peters et al., 2018) in the case of TüPa,15 and BERT (Devlin et al., 2019) in the case of HLT@SUDA. 15 GCN-Sem used ELMo in the closed tracks, training on the available data. # Team English-Wiki (closed) 1 HLT@SUDA 2 baseline 3 Davis 4 CUNY-PekingU 5 DANGNT@UIT. VNU-HCM 6 GCN-Sem English-Wiki (open) 1 HLT@SUDA 2 baseline 3 TüPa 4 XLangMo 5 DANGNT@UIT. VNU-HCM English-20K (closed) 1 HLT@SUDA 2 baseline 3 CUNY-PekingU 4 GCN-Sem English-20K (open) 1 HLT@SUDA 2 TüPa 3 XLangMo 4 baseline German-20K (closed) 1 HLT@SUDA 2 CUNY-PekingU 3 baseline 4 GCN-Sem German-20K (open) 1 HLT@SUDA 2 baseline 3 TüPa 4 XLangMo French-2"
S19-2001,S19-2016,0,0.028329,"Missing"
S19-2001,P17-1076,0,0.0200505,"etection threshold on the output probabilities. In terms of using the data, all teams but one used the UCCA XML format, two used the CoNLLU format, which is derived by a lossy conversion process, and only one team found the other data formats helpful. One of the teams (MaskParse@Deskiñ) built a new training data adapted to their model by repeating each sentence N times, N being the number of non-terminal nodes in the UCCA graphs. Three of the teams adapted the baseline TUPA parser, or parts of it to form their parser, namely TüPa, CUNY-PekingU and XLangMo; HLT@SUDA used a constituency parser (Stern et al., 2017) as a component in their model; DANGNT@UIT.VNU-HCM is a rule-based system over the Stanford Parser, and the rest are newly constructed parsers. All teams found it useful to use external resources beyond those provided by the Shared Task. Four submissions used external embeddings, MUSE (Conneau et al., 2017) in the case of MaskParse@Deskiñ and XLangMo, ELMo (Peters et al., 2018) in the case of TüPa,15 and BERT (Devlin et al., 2019) in the case of HLT@SUDA. 15 GCN-Sem used ELMo in the closed tracks, training on the available data. # Team English-Wiki (closed) 1 HLT@SUDA 2 baseline 3 Davis 4 CUNY"
S19-2001,W15-3502,1,0.833068,"hich are often different from those tackled in syntactic parsing, including reentrancy (e.g., for sharing arguments across predicates), and the modeling of the interface with lexical semantics. UCCA is a cross-linguistically applicable semantic representation scheme, building on the established Basic Linguistic Theory typological framework (Dixon, 2010b,a, 2012). It has demonstrated applicability to multiple languages, including English, French and German, and pilot annotation projects were conducted on a few languages more. UCCA structures have been shown to be well-preserved in translation (Sulem et al., 2015), and to support rapid annotation by nonexperts, assisted by an accessible annotation interface (Abend et al., 2017).1 UCCA has already shown applicative value for text simplifica1 https://github.com/omriabnd/UCCA-App P S A D Process State Participant Adverbial C E N R Center Elaborator Connector Relator H L G Parallel Scene Linker Ground F Function Scene Elements The main relation of a Scene that evolves in time (usually an action or movement). The main relation of a Scene that does not evolve in time. Scene participant (including locations, abstract entities and Scenes serving as arguments)."
S19-2001,P18-1016,1,0.825356,"her types of non-Scene relations: (1) Rs that relate a C to some super-ordinate relation, and (2) Rs that relate two Cs pertaining to different aspects of the parent unit. Inter-Scene Relations A Scene linked to other Scenes by regular linkage (e.g., temporal, logical, purposive). A relation between two or more Hs (e.g., “when”, “if”, “in order to”). A relation between the speech event and the uttered Scene (e.g., “surprisingly”). Other Does not introduce a relation or participant. Required by some structural pattern. Table 1: The complete set of categories in UCCA’s foundational layer. tion (Sulem et al., 2018b), as well as for defining semantic evaluation measures for text-to-text generation tasks, including machine translation (Birch et al., 2016), text simplification (Sulem et al., 2018a) and grammatical error correction (Choshen and Abend, 2018). The shared task defines a number of tracks, based on the different corpora and the availability of external resources (see §5). It received submissions from eight research groups around the world. In all settings at least one of the submitted systems improved over the state-of-the-art TUPA parser (Hershcovich et al., 2017, 2018), used as a baseline. 2"
S19-2001,S19-2014,0,0.0201593,"ng that `1 = `2 for the edges to match. We evaluate these measures for primary and remote edges separately. For a more finegrained evaluation, we additionally report precision, recall and F1 on edges of each category.13 6 Participating Systems We received a total of eight submissions to the different tracks: MaskParse@Deskiñ (Marzinotto et al., 2019) from Orange Labs and Aix-Marseille University, HLT@SUDA (Jiang et al., 2019) from Soochow University, TüPa (Pütz and Glocker, 2019) from the University of Tübingen, UC Davis (Yu and Sagae, 2019) from the University of California, Davis , GCN-Sem (Taslimipoor et al., 2019) from the University of Wolverhampton, CUNY-PekingU (Lyu et al., 2019) from the City University of New York and Peking University, DANGNT@UIT.VNU-HCM (Nguyen and Tran, 2019) from the University of Information Technology VNU-HCM, and XLangMo from Zhejiang University. Two systems (HLT@SUDA and CUNY-PekingU) participated in all the tracks.14 12 We are not aware of any such annotation, but include this restriction for completeness. 13 The official evaluation script providing both coarse-grained and fine-grained scores can be found in https://github.com/huji-nlp/ucca/blob/ master/scripts/evaluate_s"
S19-2001,D16-1177,0,0.0906443,"Missing"
S19-2001,S19-2017,0,0.0169292,"all Unlabeled precision, recall and F1 are the same, but without requiring that `1 = `2 for the edges to match. We evaluate these measures for primary and remote edges separately. For a more finegrained evaluation, we additionally report precision, recall and F1 on edges of each category.13 6 Participating Systems We received a total of eight submissions to the different tracks: MaskParse@Deskiñ (Marzinotto et al., 2019) from Orange Labs and Aix-Marseille University, HLT@SUDA (Jiang et al., 2019) from Soochow University, TüPa (Pütz and Glocker, 2019) from the University of Tübingen, UC Davis (Yu and Sagae, 2019) from the University of California, Davis , GCN-Sem (Taslimipoor et al., 2019) from the University of Wolverhampton, CUNY-PekingU (Lyu et al., 2019) from the City University of New York and Peking University, DANGNT@UIT.VNU-HCM (Nguyen and Tran, 2019) from the University of Information Technology VNU-HCM, and XLangMo from Zhejiang University. Two systems (HLT@SUDA and CUNY-PekingU) participated in all the tracks.14 12 We are not aware of any such annotation, but include this restriction for completeness. 13 The official evaluation script providing both coarse-grained and fine-grained scores ca"
S19-2001,I17-1084,0,0.0291596,"Missing"
S19-2001,D18-1194,0,0.0739063,"Missing"
W10-2911,E03-1009,0,0.841377,"great importance. In this paper we focus on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level e"
W10-2911,D07-1023,0,0.0965229,"e two average variants, micro and macro. Macro average computes the total number of matches over all words and normalizes in the end. Recall (R), Precision (P) and their harmonic average (Fscore) are accordingly defined: R= IM Pl i=1 |Ai | P = M acroI = In the example in Section 3 showing an unreasonable behavior of IT-based measures, the score depends on r for both MacroI and MicroI. With our new measures, recall is always 1, but precision is nr . This is true both for 1-1 and M-1 mappings. Hence, the new measures show reasonable behavior in this example for all r values. MicroI was used in (Dasgupta and Ng, 2007) with a manually compiled mapping. Their mapping was not based on a well-defined scheme but on a heuristic. Moreover, providing a manual mapping might be impractical when the number of clusters is large, and can be inaccurate, especially when the clustering is not of very high quality. In the following we discuss how to compute the 1-1 and M-1 greedy mappings for each measure. IM Pl i=1 |h(Bi )| 2RP = R+P l 1-1 Mapping. We compute h by finding the maximal weighted matching in a bipartite graph. In this graph one side represents the induced clusters, the other represents the gold classes and th"
W10-2911,P06-1038,1,0.769359,"tudy, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document ∗ * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 77 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77–87, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pus statistics from the induced clustering when the latter is to be used for annotating corpora that exhibit different statistics. In other words, if we evaluate an algorithm that will be invoked on a diverse set of cor"
W10-2911,P08-1079,1,0.802772,"iment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document ∗ * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 77 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77–87, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pus statistics from the induced clustering when the latter is to be used for annotating corpora that exhibit different statistics. In other words, if we evaluate an algorithm that will be invoked on a diverse set of corpora having different token st"
W10-2911,N09-1019,0,0.0203112,"against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuable, since it may cast light on the relative or absolute merits of different a"
W10-2911,D08-1036,0,0.454036,"tion, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuable, since it may cast light on the relative or absolute"
W10-2911,P06-1144,0,0.0349584,"Missing"
W10-2911,P07-1094,0,0.337074,"s on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuable, since it may cast light on t"
W10-2911,W06-1633,0,0.0304537,"ance. In this paper we focus on external clustering evaluation, i.e., evaluation against manually annotated gold standards, which exist for almost all such NLP tasks. External evaluation is the dominant form of clustering evaluation in NLP, although other methods have been proposed (see e.g. (Frank et al., 2009)). In this paper we discuss type level evaluation, which evaluates the set membership structure created by the clustering, independently of the token statistics of the gold standard corpus. Many clustering algorithms are evaluated by their success in tagging corpus tokens (Clark, 2003; Nicolae and Nicolae, 2006; Goldwater and Griffiths, 2007; Gao and Johnson, 2008; Elsner et al., 2009). However, in many cases a type level evaluation is the natural one. This is the case, for example, when a POS induction algorithm is used to compute a tag dictionary (the set of tags that each word can take), or when a lexical acquisition algorithm is used for constructing a lexicon containing the set of frames that a verb can participate in, or when a sense induction algorithm computes the set of possible senses of each word. In addition, even when the goal is corpus tagging, a type level evaluation is highly valuabl"
W10-2911,C08-1042,0,0.109725,"Missing"
W10-2911,C08-1091,1,0.872091,"polysemous, the common case in NLP. We demonstrate the benefits of our measures using a detailed case study, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document ∗ * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 77 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77–87, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pus statistics from the induced clustering when the latter is to be used for annotating corpora that exhibit different"
W10-2911,W09-1121,1,0.936781,"n Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among H(C) = − P|C| H(C|K) = − c=1 P|K| I k=1 ck N P|K |P|C| c=1 k=1 log Ick N P|K| I k=1 ck N I log P|C|ck c=1 Ick H(K) and H(K|C) are defined similarly. In Section 5 we use two IT measures for token level evaluation, V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009) (a normalized version of VI (Meila, 2007)). The appealing properties of these measures have been extensively discussed in these references; see also (Pfitzner et al., 2008). V and NVI are defined as follows: h= ( c= ( 1 1− H(C|K) H(C) H(C) = 0 H(C) 6= 0 1 1− H(K|C) H(K) H(K) = 0 H(K) 6= 0 V = 79 2hc h+c N V I(C, K) = ( H(C|K)+H(K|C) H(C) H(K) show below, these measures do not suffer from the problems discussed for IT measures in Section 3. All measures are mapping-based: first, a mapping between the induced and gold clusters is performed, and then a measure E is computed. As is common in the"
W10-2911,D07-1043,0,0.0829272,"sion and recall (Dhillon et al., 2003). In Section 4 we show why existing mapping-based measures cannot be applied to the polysemous type case and present new mapping-based measures for this case. Counting pairs measures are based on a combinatorial approach which examines the number of data element pairs that are clustered similarly in the reference and proposed clustering. Among H(C) = − P|C| H(C|K) = − c=1 P|K| I k=1 ck N P|K |P|C| c=1 k=1 log Ick N P|K| I k=1 ck N I log P|C|ck c=1 Ick H(K) and H(K|C) are defined similarly. In Section 5 we use two IT measures for token level evaluation, V (Rosenberg and Hirschberg, 2007) and NVI (Reichart and Rappoport, 2009) (a normalized version of VI (Meila, 2007)). The appealing properties of these measures have been extensively discussed in these references; see also (Pfitzner et al., 2008). V and NVI are defined as follows: h= ( c= ( 1 1− H(C|K) H(C) H(C) = 0 H(C) 6= 0 1 1− H(K|C) H(K) H(K) = 0 H(K) 6= 0 V = 79 2hc h+c N V I(C, K) = ( H(C|K)+H(K|C) H(C) H(K) show below, these measures do not suffer from the problems discussed for IT measures in Section 3. All measures are mapping-based: first, a mapping between the induced and gold clusters is performed, and then a meas"
W10-2911,J06-2001,0,0.335372,"ur measures using a detailed case study, POS induction. We experiment with seven leading algorithms, obtaining useful insights and showing that token and type level measures can weakly or even negatively correlate, which underscores the fact that these two approaches reveal different aspects of clustering quality. 1 Introduction Clustering is a central machine learning technique. In NLP, clustering has been used for virtually every semi- and unsupervised task, including POS tagging (Clark, 2003), labeled parse tree induction (Reichart and Rappoport, 2008), verb-type classification (Schulte im Walde, 2006), lexical acquisition (Davidov and Rappoport, 2006; Davidov and Rappoport, 2008), multilingual document ∗ * Both authors equally contributed to this paper. † Omri Abend is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship. 77 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 77–87, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics pus statistics from the induced clustering when the latter is to be used for annotating corpora that exhibit different statistics. In other words, if we evaluate an algo"
W10-2911,P04-1062,0,0.0714122,"Missing"
W10-2911,J93-2004,0,\N,Missing
W10-2911,D09-1071,0,\N,Missing
W10-2911,P10-1132,1,\N,Missing
W13-0101,P10-1024,1,0.832932,"ut the cake”. “quickly” meets both the syntactic and the semantic criteria for an adjunct: it is optional and it serves to restrict the meaning of “walked”. It also has a similar semantic content when appearing with different verbs (“walk quickly”, “eat quickly”, “talk quickly” etc.). “the cake” meets both the syntactic and the semantic criteria for a core: it is obligatory, and completes the meaning of “cut”. However, many other cases are not as obvious. For instance, in “he walked into his office”, the boldfaced argument is a core according to Framenet, but an adjunct according to PropBank (Abend and Rappoport, 2010). The core-adjunct distinction in UCCA is translated into the distinction between D’s (Adverbials) and A’s (Participants). UCCA is a semantic scheme and therefore the syntactic criterion of “obligatoriness” is not applicable, and is instead left to be detected by statistical means. Instead, UCCA defines A’s as units that introduce a new participant to the Scene and D’s as units that add more information to the Scene without introducing a participant. Revisiting our earlier examples, in “Woody cut the cake”, “the cake” introduces a new participant and is therefore an A, while in “Woody walked q"
W13-0101,P09-1004,1,0.904978,"Missing"
W13-0101,P98-1013,0,0.553651,"ers of annotation, special categories will be devoted to annotating part-whole relations and the semantic relations described by determiners. Figure 2(c) presents the annotation of this example. 4 3.2 Beyond Simple Scenes Nominal Predicates. The foundational layer of UCCA annotates the argument structure of nominal predicates much in the same fashion as that of verbal predicates. This accords with the standard practice in several NLP resources, which tend to use the same formal devices for annotating nominal and verbal argument structure (see, e.g., NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)). For example, consider “his speech against the motion”. “speech” evokes a Scene that evolves in time and is therefore a P. The Scene has two Participants, namely “his” and “against the motion”. Multiple Parents. In general, a unit may participate in more than one relation. To this end, UCCA allows a unit to have multiple parents. Recall that in UCCA, a non-terminal node represents a relation, and its descendants are the sub-units comprising it. A unit’s category is a label over the edge connecting love E A A P A Woody bones D P generally E big dogs P C bike his (a) John home rides C A A A (b"
W13-0101,W12-3602,0,0.185448,"irst, UCCA rejects the assumption that every structure has a unique head. Formally, instead of selecting a single head whose descendants are (the heads of) the argument units, UCCA introduces a new node for each relation, whose descendants are all the sub-units comprising that relation, including the predicate and its arguments. The symmetry between the descendants is broken through the features placed on the edges. Consider coordination structures as an example. The difficulty of dependency grammar to capture such structures is exemplified by the 8 possible annotations in current use in NLP (Ivanova et al., 2012). In UCCA, all elements of the coordination (i.e., the conjunction along with its conjuncts) are descendants of a mutual parent, where only their categories distinguish between their roles. For instance, in “John and Mary”, “John”, “Mary” and “and” are all listed under a joint parent. Discontiguous conjunctions (such as “either John or Mary”) are also handled straightforwardly by placing “either” and “or” under a single parent, which in turn serves as a Connector (Figure 2(h)). Note that the edges between “either” and “or” and their mutual parent have no category labels, since the unit “either"
W13-0101,J93-2004,0,0.0515533,"tic structure is represented using a combinatorial apparatus and a set of categories assigned to the linguistic units it defines. The categories are often based on distributional considerations and reflect the formal patterns in which that unit may occur. The use of distributional categories leads to intricate annotation schemes. As languages greatly differ in their inventory of constructions, such schemes tend to be tuned to one language or domain. In addition, the complexity of the schemes requires highly proficient workforce for its annotation. For example, the Penn Treebank project (PTB) (Marcus et al., 1993) used linguistics graduates as annotators. In this paper we propose a radically different approach to grammatical annotation. Under this approach, only semantic distinctions are manually annotated, while distributional regularities are induced using statistical algorithms and without any direct supervision. This approach has four main advantages. First, it facilitates manual annotation that would no longer require close acquaintance with syntactic theory. Second, a data-driven approach for detecting distributional regularities is less prone to errors and to the incorporation of implicit biases"
W13-0101,J08-2001,0,0.0283594,"Missing"
W13-0101,meyers-etal-2004-annotating,0,0.34244,"otated as an E. In more refined layers of annotation, special categories will be devoted to annotating part-whole relations and the semantic relations described by determiners. Figure 2(c) presents the annotation of this example. 4 3.2 Beyond Simple Scenes Nominal Predicates. The foundational layer of UCCA annotates the argument structure of nominal predicates much in the same fashion as that of verbal predicates. This accords with the standard practice in several NLP resources, which tend to use the same formal devices for annotating nominal and verbal argument structure (see, e.g., NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998)). For example, consider “his speech against the motion”. “speech” evokes a Scene that evolves in time and is therefore a P. The Scene has two Participants, namely “his” and “against the motion”. Multiple Parents. In general, a unit may participate in more than one relation. To this end, UCCA allows a unit to have multiple parents. Recall that in UCCA, a non-terminal node represents a relation, and its descendants are the sub-units comprising it. A unit’s category is a label over the edge connecting love E A A P A Woody bones D P generally E big dogs P C bike"
W13-0101,J05-1004,0,0.17863,"their mother who abandoned ... children]A ” • “motherA ... [gave up acting]P ” • “motherA ... [focus on raising]P [her children]A ” 4 Previous Work Many grammatical annotation schemes have been proposed over the years in an attempt to capture the richness of grammatical phenomena. In this section, we focus on approaches that provide a sizable corpus of annotated text. We put specific emphasis on English corpora, which is the most studied language and the focus language of this paper. Semantic Role Labeling Schemes. The most prominent schemes to SRL are FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and VerbNet (Schuler, 2005) for verbal predicates and NomBank for nominal predicates (Meyers et al., 2004). They share with UCCA their focus on semanticallymotivated rather than distributionally-motivated distinctions. However, unlike UCCA, they annotate each predicate separately, yielding shallow representations which are hard to learn directly without using syntactic parsing as preprocessing (Punyakanok et al., 2008). In addition, UCCA has a wider coverage than these projects, as it addresses both verbal, nominal and adjectival predicates. Recently, the Framenet Constructicon project (Fillm"
W13-0101,J08-2005,0,0.0468631,"A for Universal Conceptual Cognitive Annotation. The scheme covers many of the most important elements and relations present in linguistic utterances, including verb-argument structure, optional adjuncts such as adverbials, clause embeddings, and the linkage between them. The scheme is supported by extensive typological crosslinguistic evidence and accords with the leading Cognitive Linguistics theories. 1 Introduction Syntactic annotation is used as scaffolding in a wide variety of NLP applications. Examples include Machine Translation (Yamada and Knight, 2001), Semantic Role Labeling (SRL) (Punyakanok et al., 2008) and Textual Entailment (Yuret et al., 2010). Syntactic structure is represented using a combinatorial apparatus and a set of categories assigned to the linguistic units it defines. The categories are often based on distributional considerations and reflect the formal patterns in which that unit may occur. The use of distributional categories leads to intricate annotation schemes. As languages greatly differ in their inventory of constructions, such schemes tend to be tuned to one language or domain. In addition, the complexity of the schemes requires highly proficient workforce for its annota"
W13-0101,P11-1067,1,0.910132,"Missing"
W13-0101,P01-1067,0,0.0330633,"end, we propose a simple semantic annotation scheme, UCCA for Universal Conceptual Cognitive Annotation. The scheme covers many of the most important elements and relations present in linguistic utterances, including verb-argument structure, optional adjuncts such as adverbials, clause embeddings, and the linkage between them. The scheme is supported by extensive typological crosslinguistic evidence and accords with the leading Cognitive Linguistics theories. 1 Introduction Syntactic annotation is used as scaffolding in a wide variety of NLP applications. Examples include Machine Translation (Yamada and Knight, 2001), Semantic Role Labeling (SRL) (Punyakanok et al., 2008) and Textual Entailment (Yuret et al., 2010). Syntactic structure is represented using a combinatorial apparatus and a set of categories assigned to the linguistic units it defines. The categories are often based on distributional considerations and reflect the formal patterns in which that unit may occur. The use of distributional categories leads to intricate annotation schemes. As languages greatly differ in their inventory of constructions, such schemes tend to be tuned to one language or domain. In addition, the complexity of the sch"
W13-0101,S10-1009,0,0.0251657,"The scheme covers many of the most important elements and relations present in linguistic utterances, including verb-argument structure, optional adjuncts such as adverbials, clause embeddings, and the linkage between them. The scheme is supported by extensive typological crosslinguistic evidence and accords with the leading Cognitive Linguistics theories. 1 Introduction Syntactic annotation is used as scaffolding in a wide variety of NLP applications. Examples include Machine Translation (Yamada and Knight, 2001), Semantic Role Labeling (SRL) (Punyakanok et al., 2008) and Textual Entailment (Yuret et al., 2010). Syntactic structure is represented using a combinatorial apparatus and a set of categories assigned to the linguistic units it defines. The categories are often based on distributional considerations and reflect the formal patterns in which that unit may occur. The use of distributional categories leads to intricate annotation schemes. As languages greatly differ in their inventory of constructions, such schemes tend to be tuned to one language or domain. In addition, the complexity of the schemes requires highly proficient workforce for its annotation. For example, the Penn Treebank project"
W13-0101,nissim-etal-2004-annotation,0,\N,Missing
W13-0101,W04-2705,0,\N,Missing
W13-0101,C08-2024,0,\N,Missing
W13-0101,W11-0908,0,\N,Missing
W13-0101,de-marneffe-etal-2006-generating,0,\N,Missing
W13-0101,N07-1071,0,\N,Missing
W13-0101,W10-4205,0,\N,Missing
W13-0101,alvez-etal-2008-complete,0,\N,Missing
W13-0101,N10-1011,0,\N,Missing
W13-0101,E12-1024,0,\N,Missing
W13-0101,fillmore-etal-2002-seeing,0,\N,Missing
W13-0101,N07-1021,0,\N,Missing
W13-0101,D10-1115,0,\N,Missing
W13-0101,D12-1016,0,\N,Missing
W13-0101,W01-1315,0,\N,Missing
W13-0101,S10-1076,0,\N,Missing
W13-0101,E06-1040,0,\N,Missing
W13-0101,W97-0301,0,\N,Missing
W13-0101,N10-1138,0,\N,Missing
W13-0101,J97-1003,0,\N,Missing
W13-0101,W11-2507,0,\N,Missing
W13-0101,D12-1110,0,\N,Missing
W13-0101,D10-1089,0,\N,Missing
W13-0101,C10-1018,0,\N,Missing
W13-0101,W11-2805,0,\N,Missing
W13-0101,W01-1311,0,\N,Missing
W13-0101,W97-1301,0,\N,Missing
W13-0101,D08-1094,0,\N,Missing
W13-0101,N09-1041,0,\N,Missing
W13-0101,W02-1006,0,\N,Missing
W13-0101,W08-0606,0,\N,Missing
W13-0101,P86-1004,0,\N,Missing
W13-0101,H86-1011,0,\N,Missing
W13-0101,C08-1107,0,\N,Missing
W13-0101,J05-3002,0,\N,Missing
W13-0101,W08-2205,0,\N,Missing
W13-0101,W08-2222,0,\N,Missing
W13-0101,W11-0114,0,\N,Missing
W13-0101,J12-1002,0,\N,Missing
W13-0101,D09-1143,0,\N,Missing
W13-0101,P05-1053,0,\N,Missing
W13-0101,N01-1021,0,\N,Missing
W13-0101,S10-1008,0,\N,Missing
W13-0101,W03-1016,0,\N,Missing
W13-0101,C02-1151,0,\N,Missing
W13-0101,C92-2082,0,\N,Missing
W13-0101,C02-1114,0,\N,Missing
W13-0101,W08-1105,0,\N,Missing
W13-0101,P92-1017,0,\N,Missing
W13-0101,H92-1024,0,\N,Missing
W13-0101,J03-4003,0,\N,Missing
W13-0101,W08-1301,0,\N,Missing
W13-0101,D07-1109,0,\N,Missing
W13-0101,C08-1108,0,\N,Missing
W13-0101,P03-1054,0,\N,Missing
W13-0101,S12-1047,0,\N,Missing
W13-0101,W09-2415,0,\N,Missing
W13-0101,P04-1072,0,\N,Missing
W13-0101,P02-1040,0,\N,Missing
W13-0101,P11-1062,0,\N,Missing
W13-0101,W09-3941,0,\N,Missing
W13-0101,P04-1015,0,\N,Missing
W13-0101,P98-1046,0,\N,Missing
W13-0101,C98-1046,0,\N,Missing
W13-0101,J95-1001,0,\N,Missing
W13-0101,P98-1116,0,\N,Missing
W13-0101,C98-1112,0,\N,Missing
W13-0101,P10-1160,0,\N,Missing
W13-0101,P09-1077,0,\N,Missing
W13-0101,J06-3003,0,\N,Missing
W13-0101,D07-1071,0,\N,Missing
W13-0101,P11-1154,0,\N,Missing
W13-0101,D11-1123,0,\N,Missing
W13-0101,P03-1017,0,\N,Missing
W13-0101,C98-1013,0,\N,Missing
W13-0101,J05-3004,0,\N,Missing
W13-0101,P06-1038,1,\N,Missing
W13-0101,P98-2241,0,\N,Missing
W13-0101,C98-2236,0,\N,Missing
W13-0101,W13-0112,0,\N,Missing
W13-0101,S10-1065,0,\N,Missing
W13-0101,C04-1185,0,\N,Missing
W13-0101,P04-1054,0,\N,Missing
W13-0101,P05-3027,0,\N,Missing
W13-0101,J95-2003,0,\N,Missing
W13-0101,P06-1095,0,\N,Missing
W13-0101,J10-4006,0,\N,Missing
W13-0101,J98-1004,0,\N,Missing
W13-0101,D11-1024,0,\N,Missing
W13-0101,P08-1118,0,\N,Missing
W13-0101,P08-1028,0,\N,Missing
W13-0101,P12-1015,0,\N,Missing
W13-0101,P05-3021,0,\N,Missing
W13-0101,P04-1011,0,\N,Missing
W13-0101,P87-1022,0,\N,Missing
W13-0101,J04-1003,0,\N,Missing
W13-0101,S10-1010,0,\N,Missing
W13-0101,J08-1001,0,\N,Missing
W13-0101,S10-1059,0,\N,Missing
W13-0101,P10-1005,0,\N,Missing
W13-0101,J12-4003,0,\N,Missing
W13-0101,P10-1045,0,\N,Missing
W13-0101,P09-1046,0,\N,Missing
W13-0101,J02-3001,0,\N,Missing
W13-0101,P08-1090,0,\N,Missing
W13-0101,R11-1037,0,\N,Missing
W13-0101,P09-1068,0,\N,Missing
W13-0101,D08-1083,0,\N,Missing
W13-0101,P10-1143,0,\N,Missing
W13-0101,P98-2182,0,\N,Missing
W13-0101,C98-2177,0,\N,Missing
W13-0101,P08-2045,0,\N,Missing
W13-0101,P10-1100,0,\N,Missing
W13-0101,N10-1013,0,\N,Missing
W13-0101,P98-2127,0,\N,Missing
W13-0101,C98-2122,0,\N,Missing
W13-0101,prasad-etal-2008-penn,0,\N,Missing
W13-0101,P87-1021,0,\N,Missing
W13-0101,J86-3001,0,\N,Missing
W13-0101,P10-1044,0,\N,Missing
W13-0101,D11-1050,0,\N,Missing
W13-0101,P04-1019,0,\N,Missing
W13-0101,P98-2143,0,\N,Missing
W13-0101,C98-2138,0,\N,Missing
W13-0101,E06-1015,0,\N,Missing
W13-0101,roberts-etal-2012-annotating,0,\N,Missing
W13-0101,S12-1048,0,\N,Missing
W13-0101,S12-1030,0,\N,Missing
W13-0101,costa-branco-2012-timebankpt,0,\N,Missing
W13-0101,W10-2805,0,\N,Missing
W13-0101,R11-1046,0,\N,Missing
W13-0101,I11-1127,0,\N,Missing
W13-0101,mendes-etal-2012-dbpedia,0,\N,Missing
W13-0101,R11-1004,0,\N,Missing
W13-0101,W11-2819,0,\N,Missing
W13-0101,W11-0144,0,\N,Missing
W13-0101,islam-inkpen-2006-second,0,\N,Missing
W13-0101,W03-2605,0,\N,Missing
W13-0101,W93-0102,0,\N,Missing
W13-0101,W95-0103,0,\N,Missing
W13-0101,D11-1016,0,\N,Missing
W13-0101,W09-4303,0,\N,Missing
W13-0101,mani-etal-2008-spatialml,0,\N,Missing
W13-0101,D10-1113,0,\N,Missing
W13-0101,S12-1001,0,\N,Missing
W13-0101,D11-1014,0,\N,Missing
W13-0101,N12-1065,0,\N,Missing
W13-0101,D10-1048,0,\N,Missing
W13-0101,W07-2316,0,\N,Missing
W13-0101,W06-1623,0,\N,Missing
W13-0101,D07-1076,0,\N,Missing
W13-0101,P11-1053,0,\N,Missing
W13-0101,P11-1056,0,\N,Missing
W13-0101,H05-1091,0,\N,Missing
W13-0101,S12-1056,0,\N,Missing
W13-0101,S12-1055,0,\N,Missing
W15-3502,W13-0101,1,0.89651,"n-corresponding units in the two languages according to various parameters, and show that many of them are due to ambiguity or semantic changes. These results offer a better understanding of UCCA’s stability and suggest paths for further improvements. translations is seldom addressed and has yet to be adequately supported (see Section 2), a gap we address in this paper using a detailed analysis of a semantically annotated parallel corpus. Universal Cognitive Conceptual Annotation (UCCA) is a coarse-grained semantic annotation scheme which builds on typological and cognitive linguistic theory (Abend and Rappoport, 2013a; Abend and Rappoport, 2013b). The scheme aims to be applicable cross-linguistically, to abstract away from specific syntactic forms and to directly represent semantic distinctions. These properties make UCCA an appealing source of structural annotation which is cross-linguistically stable. We give an overview of UCCA in Section 3. This paper focuses on the case study of EnglishFrench, a well studied language pair in MT. We demonstrate through this language pair both UCCA’s portability, namely its ability to be applied to different languages, and its stability, namely its ability to preserve"
W15-3502,dorr-etal-2002-duster,0,0.0497329,"fore disregarded in this work. We find that even for French-specific phenomena, current UCCA categories permit their annotation in the foundational layer without requiring changes in the definitions or additional categories. Due to space limitations, we only present here one case of interest. The full analysis according to the grammar book can be found in Sulem (2014) (Ap4.2 Stability Overcoming cross-linguistic divergences (or translation divergences) is one of the main challenges in machine translation. We briefly review the main examples of translation divergences presented in (Dorr, 1994; Dorr et al., 2002; Dorr et al., 2004), adapting the original English-Spanish examples to English-French analogues. Then, for each example, we present its annotation according 3 www.cs.huji.ac.il/˜eliors/papers/ elior_sulem_thesis.pdf 14 glish example contains a Process (“to run”) and a Participant (“in”). The annotation in French is somewhat different, where “entrer” (“enter”) is a Process, while “en courant” (“running”) is an Adverbial. to UCCA. The resulting annotations show that UCCA abstracts away from almost all of these divergences and exposes the semantic similarity, demonstrating the stability of the s"
W15-3502,P13-1023,1,0.89616,"n-corresponding units in the two languages according to various parameters, and show that many of them are due to ambiguity or semantic changes. These results offer a better understanding of UCCA’s stability and suggest paths for further improvements. translations is seldom addressed and has yet to be adequately supported (see Section 2), a gap we address in this paper using a detailed analysis of a semantically annotated parallel corpus. Universal Cognitive Conceptual Annotation (UCCA) is a coarse-grained semantic annotation scheme which builds on typological and cognitive linguistic theory (Abend and Rappoport, 2013a; Abend and Rappoport, 2013b). The scheme aims to be applicable cross-linguistically, to abstract away from specific syntactic forms and to directly represent semantic distinctions. These properties make UCCA an appealing source of structural annotation which is cross-linguistically stable. We give an overview of UCCA in Section 3. This paper focuses on the case study of EnglishFrench, a well studied language pair in MT. We demonstrate through this language pair both UCCA’s portability, namely its ability to be applied to different languages, and its stability, namely its ability to preserve"
W15-3502,P98-1013,0,0.108902,"Missing"
W15-3502,W13-2322,0,0.791641,"s. Semantic annotation is an appealing avenue for constructing cross-linguistically stable structures, since a major goal of translation is to preserve the meaning of a sentence. Cross-linguistically stable schemes have further benefits for applications such as knowledge projection across languages (Kozhevnikov and Titov, 2013), the induction of cross-lingual semantic relations (Lewis and Steedman, 2013), or in translation studies (Lembersky et al., 2013) (see Section 7.3). A recent example of a semantic scheme aiming to be cross-linguistically stable is AMR (Abstract Meaning Representation) (Banarescu et al., 2013) which uses elaborate hierarchical structures in order to abstractly represent semantic information and presents promising preliminary results for SMT improvement (Jones et al., 2012). Nevertheless, the stability of semantic annotation across Divergence of syntactic structures between languages constitutes a major challenge in using linguistic structure in Machine Translation (MT) systems. Here, we examine the potential of semantic structures. While semantic annotation is appealing as a source of cross-linguistically stable structures, little has been accomplished in demonstrating this stabili"
W15-3502,P13-2074,0,0.0256835,"Missing"
W15-3502,P90-1017,0,0.626931,"ans la maison” (“enter in the house”). In UCCA there is a Participant in both languages. Thematic divergence: Realization of verb arguments in syntactic configurations that reflect different thematic to syntactic mapping orders. For example, “I like this house” – “Cette maison me plaˆıt” (“this house pleases to me”). In UCCA there are two Participants in English as well as two Participants in French (“cette maison” / “this house” and “me” / “me”). Promotional/Demotional divergence: Promotion is the case where a modifier in the source language is promoted to a main verb in the target language (Dorr, 1990; Gola, 2012). Demotion is its mirror image, where a main verb in the source language becomes a modifier in the target language. An example where an English adverb is promoted to a main verb is the French: “John usually goes home” – “John a l’habitude de rentrer a` la maison” (“John has the habit to go home”). In UCCA, both “usually” and “a l’habitude” (“has the habit”) are annotated as Adverbials. An example where an English verb is demoted to an adverb is the French “to run in” – “entrer en courant” (“enter running”). In UCCA, the EnTo summarize, aside from the case of demotional divergence,"
W15-3502,J94-4004,0,0.756645,"ss Translations: A French-English Case Study Elior Sulem Omri Abend Ari Rappoport Institute of Computer Science School of Informatics Institute of Computer Science Hebrew University of Jerusalem University of Edinburgh Hebrew University of Jerusalem eliors@cs.huji.ac.il oabend@inf.ed.ac.uk arir@cs.huji.ac.il Abstract models are effective at improving reordering at the phrase level, they are limited in their ability to map between arbitrarily divergent structures. Crosslinguistic divergences therefore pose a difficult problem for the integration of structural knowledge into statistical models (Dorr, 1994; Ding and Palmer, 2004; Zhang et al., 2008). Consequently, an annotation scheme that assigns similar structures to translations has direct applicative value for structure-aware MT systems. Such structures can be used either as features in phrase-based systems, yielding more robust decoding, or as a structural scheme which directs the translation, replacing the PCFG trees often used today. Using more stable schemes is likely to result in simpler MT systems, avoiding structure modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based system"
W15-3502,Y09-2025,0,0.0492529,"Missing"
W15-3502,W13-2203,0,0.141644,"Missing"
W15-3502,C12-1053,0,0.0452236,"Missing"
W15-3502,P14-1134,0,0.0517551,"Missing"
W15-3502,2007.tmi-papers.10,0,0.0706023,"Missing"
W15-3502,P05-1033,0,0.011631,"ysis of the major French grammatical phenomena. Second, we annotate a parallel English-French corpus with UCCA, and quantify the similarity of the structures on both sides. Results show a high degree of stability across translations, supporting the usage of semantic annotations over syntactic ones in structure-aware MT systems. 1 Introduction Structural information, be it syntactic or semantic, has the potential to address long-standing problems in Statistical Machine Translation (SMT), such as phrase-level (rather than word-level) reordering and discontiguous phrases. Structureaware models1 (Chiang, 2005; Liu et al., 2006; Mi et al., 2008) aim to address these and other problems by taking into account the hierarchical structure of language. However, while structure-aware 1 We use the term “structure-aware” rather than “syntaxbased” so to include any type of hierarchical structure. 11 Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation, pages 11–22, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics Scenes (a similar notion to a “frame”; see Section 3) in both languages have a correspondent in the other. We analyze the non-correspond"
W15-3502,W04-1513,0,0.0414053,"ons: A French-English Case Study Elior Sulem Omri Abend Ari Rappoport Institute of Computer Science School of Informatics Institute of Computer Science Hebrew University of Jerusalem University of Edinburgh Hebrew University of Jerusalem eliors@cs.huji.ac.il oabend@inf.ed.ac.uk arir@cs.huji.ac.il Abstract models are effective at improving reordering at the phrase level, they are limited in their ability to map between arbitrarily divergent structures. Crosslinguistic divergences therefore pose a difficult problem for the integration of structural knowledge into statistical models (Dorr, 1994; Ding and Palmer, 2004; Zhang et al., 2008). Consequently, an annotation scheme that assigns similar structures to translations has direct applicative value for structure-aware MT systems. Such structures can be used either as features in phrase-based systems, yielding more robust decoding, or as a structural scheme which directs the translation, replacing the PCFG trees often used today. Using more stable schemes is likely to result in simpler MT systems, avoiding structure modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based systems to handle cross-lingu"
W15-3502,D11-1067,0,0.019995,"Missing"
W15-3502,C12-1083,0,0.0279686,"nguistically stable schemes have further benefits for applications such as knowledge projection across languages (Kozhevnikov and Titov, 2013), the induction of cross-lingual semantic relations (Lewis and Steedman, 2013), or in translation studies (Lembersky et al., 2013) (see Section 7.3). A recent example of a semantic scheme aiming to be cross-linguistically stable is AMR (Abstract Meaning Representation) (Banarescu et al., 2013) which uses elaborate hierarchical structures in order to abstractly represent semantic information and presents promising preliminary results for SMT improvement (Jones et al., 2012). Nevertheless, the stability of semantic annotation across Divergence of syntactic structures between languages constitutes a major challenge in using linguistic structure in Machine Translation (MT) systems. Here, we examine the potential of semantic structures. While semantic annotation is appealing as a source of cross-linguistically stable structures, little has been accomplished in demonstrating this stability through a detailed corpus study. In this paper, we experiment with the UCCA conceptual-cognitive annotation scheme in an English-French case study. First, we show that UCCA can be"
W15-3502,P03-1054,0,0.00769169,"anova et al., 2003). We compute the number of verbs in the parallel corpus and compare them to the number of Scenes. We exclude auxiliaries since such verbs tend to differ considerably between languages. We manually correct the tagging (by a single annotator, highly proficient in both languages), and therefore expect these numbers to be comparable in quality to a gold standard7 . The syntactic constituents we study are noun phrases (NP), prepositional phrases (PP) and adverb phrases (ADVP in English and AdP in French). We used the Stanford parser’s pretrained models for English (englishPCFG, (Klein and Manning, 2003)) and French (the frenchFactored (Green et al., 2011)), with the same manual tokenization taken from the UCCA annotation. Six passages which contain very long sentences in French and for which the parser was unable to produce a parse were omitted from this evaluation. We note that we include in our analysis Scenes marked as unanalyzable (For example: “Hello!”), but exclude Scenes appearing as remote Participants, so to avoid double counting. In order to correct for possible biases of the parsers towards overprediction or underprediction of certain syntactic constituents, we conduct the followi"
W15-3502,P13-1117,0,0.0599532,"nslation, replacing the PCFG trees often used today. Using more stable schemes is likely to result in simpler MT systems, avoiding structure modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based systems to handle cross-linguistic divergences. Semantic annotation is an appealing avenue for constructing cross-linguistically stable structures, since a major goal of translation is to preserve the meaning of a sentence. Cross-linguistically stable schemes have further benefits for applications such as knowledge projection across languages (Kozhevnikov and Titov, 2013), the induction of cross-lingual semantic relations (Lewis and Steedman, 2013), or in translation studies (Lembersky et al., 2013) (see Section 7.3). A recent example of a semantic scheme aiming to be cross-linguistically stable is AMR (Abstract Meaning Representation) (Banarescu et al., 2013) which uses elaborate hierarchical structures in order to abstractly represent semantic information and presents promising preliminary results for SMT improvement (Jones et al., 2012). Nevertheless, the stability of semantic annotation across Divergence of syntactic structures between languages constitute"
W15-3502,N03-1033,0,0.005738,"= i ni . The F-score F is the harmonic mean of P and R. This measure provides an upper bound of the number of aligned units in the two languages, looking at the category of the units and their appearance in aligned passages. We note that the measures described are more applicable in this context than statistical correlation measures (e.g., the Pearson correlation coefficient). This is because a stable scheme is determined by the similarity of the count vectors in absolute terms, rather than their statistical correlation. Experimental setup. For tagging, we use the Stanford POS tagger package (Toutanova et al., 2003). We compute the number of verbs in the parallel corpus and compare them to the number of Scenes. We exclude auxiliaries since such verbs tend to differ considerably between languages. We manually correct the tagging (by a single annotator, highly proficient in both languages), and therefore expect these numbers to be comparable in quality to a gold standard7 . The syntactic constituents we study are noun phrases (NP), prepositional phrases (PP) and adverb phrases (ADVP in English and AdP in French). We used the Stanford parser’s pretrained models for English (englishPCFG, (Klein and Manning,"
W15-3502,J13-4007,0,0.0570847,"cture modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based systems to handle cross-linguistic divergences. Semantic annotation is an appealing avenue for constructing cross-linguistically stable structures, since a major goal of translation is to preserve the meaning of a sentence. Cross-linguistically stable schemes have further benefits for applications such as knowledge projection across languages (Kozhevnikov and Titov, 2013), the induction of cross-lingual semantic relations (Lewis and Steedman, 2013), or in translation studies (Lembersky et al., 2013) (see Section 7.3). A recent example of a semantic scheme aiming to be cross-linguistically stable is AMR (Abstract Meaning Representation) (Banarescu et al., 2013) which uses elaborate hierarchical structures in order to abstractly represent semantic information and presents promising preliminary results for SMT improvement (Jones et al., 2012). Nevertheless, the stability of semantic annotation across Divergence of syntactic structures between languages constitutes a major challenge in using linguistic structure in Machine Translation (MT) systems. Here, we examine the potential of semantic"
W15-3502,1987.mtsummit-1.10,0,0.876098,"Missing"
W15-3502,D13-1064,0,0.0153984,"likely to result in simpler MT systems, avoiding structure modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based systems to handle cross-linguistic divergences. Semantic annotation is an appealing avenue for constructing cross-linguistically stable structures, since a major goal of translation is to preserve the meaning of a sentence. Cross-linguistically stable schemes have further benefits for applications such as knowledge projection across languages (Kozhevnikov and Titov, 2013), the induction of cross-lingual semantic relations (Lewis and Steedman, 2013), or in translation studies (Lembersky et al., 2013) (see Section 7.3). A recent example of a semantic scheme aiming to be cross-linguistically stable is AMR (Abstract Meaning Representation) (Banarescu et al., 2013) which uses elaborate hierarchical structures in order to abstractly represent semantic information and presents promising preliminary results for SMT improvement (Jones et al., 2012). Nevertheless, the stability of semantic annotation across Divergence of syntactic structures between languages constitutes a major challenge in using linguistic structure in Machine Translation (MT)"
W15-3502,W15-1613,0,0.0380493,"Missing"
W15-3502,N13-1060,0,0.0254615,"Missing"
W15-3502,W10-1814,0,0.137732,"Missing"
W15-3502,C10-1081,0,0.0208106,"Missing"
W15-3502,P06-1077,0,0.0232994,"jor French grammatical phenomena. Second, we annotate a parallel English-French corpus with UCCA, and quantify the similarity of the structures on both sides. Results show a high degree of stability across translations, supporting the usage of semantic annotations over syntactic ones in structure-aware MT systems. 1 Introduction Structural information, be it syntactic or semantic, has the potential to address long-standing problems in Statistical Machine Translation (SMT), such as phrase-level (rather than word-level) reordering and discontiguous phrases. Structureaware models1 (Chiang, 2005; Liu et al., 2006; Mi et al., 2008) aim to address these and other problems by taking into account the hierarchical structure of language. However, while structure-aware 1 We use the term “structure-aware” rather than “syntaxbased” so to include any type of hierarchical structure. 11 Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation, pages 11–22, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics Scenes (a similar notion to a “frame”; see Section 3) in both languages have a correspondent in the other. We analyze the non-corresponding units in the t"
W15-3502,W06-1606,0,0.0249226,"roblem for the integration of structural knowledge into statistical models (Dorr, 1994; Ding and Palmer, 2004; Zhang et al., 2008). Consequently, an annotation scheme that assigns similar structures to translations has direct applicative value for structure-aware MT systems. Such structures can be used either as features in phrase-based systems, yielding more robust decoding, or as a structural scheme which directs the translation, replacing the PCFG trees often used today. Using more stable schemes is likely to result in simpler MT systems, avoiding structure modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based systems to handle cross-linguistic divergences. Semantic annotation is an appealing avenue for constructing cross-linguistically stable structures, since a major goal of translation is to preserve the meaning of a sentence. Cross-linguistically stable schemes have further benefits for applications such as knowledge projection across languages (Kozhevnikov and Titov, 2013), the induction of cross-lingual semantic relations (Lewis and Steedman, 2013), or in translation studies (Lembersky et al., 2013) (see Section 7.3). A recent exampl"
W15-3502,P08-1023,0,0.0224531,"ical phenomena. Second, we annotate a parallel English-French corpus with UCCA, and quantify the similarity of the structures on both sides. Results show a high degree of stability across translations, supporting the usage of semantic annotations over syntactic ones in structure-aware MT systems. 1 Introduction Structural information, be it syntactic or semantic, has the potential to address long-standing problems in Statistical Machine Translation (SMT), such as phrase-level (rather than word-level) reordering and discontiguous phrases. Structureaware models1 (Chiang, 2005; Liu et al., 2006; Mi et al., 2008) aim to address these and other problems by taking into account the hierarchical structure of language. However, while structure-aware 1 We use the term “structure-aware” rather than “syntaxbased” so to include any type of hierarchical structure. 11 Proceedings of the 1st Workshop on Semantics-Driven Statistical Machine Translation, pages 11–22, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics Scenes (a similar notion to a “frame”; see Section 3) in both languages have a correspondent in the other. We analyze the non-corresponding units in the two languages accor"
W15-3502,N09-2004,0,0.0396594,"Missing"
W15-3502,J05-1004,0,0.125686,"essed the portability of semantic annotation schemes, namely whether the same scheme, often originally developed for English, can be applied to other languages. Burchardt et al. (2009) addressed the application of the English FrameNet (Baker et al., 1998) to German. They found that about a third of the verb senses identified in the German corpus were not covered by FrameNet. Their analysis further revealed that the English category set is not always sufficient, resulting in the introduction of a new category for German. Van der Plas et al. (2010) addressed the application of English PropBank (Palmer et al., 2005) to French, and found that while the scheme can be applied to French, the annotation requires proficiency in both languages. Samardzic et al. (2010a; 2010b) also studied the portability of the English PropBank to French, and found that the overwhelming majority of the French verbal predicates in the corpus correspond to a verb sense in the PropBank lexicon. The portability of PropBank was also examined in the case of English-Chinese through the construction of annotated parallel corpora used in the OntoNotes project (Weischedel et al., 2012). Portability has also been studied in the context of"
W15-3502,P12-1095,0,0.036912,"Missing"
W15-3502,xue-etal-2014-interlingua,0,0.0606707,"Missing"
W15-3502,C12-1185,0,0.0269247,"Missing"
W15-3502,P08-1064,0,0.0265816,"ase Study Elior Sulem Omri Abend Ari Rappoport Institute of Computer Science School of Informatics Institute of Computer Science Hebrew University of Jerusalem University of Edinburgh Hebrew University of Jerusalem eliors@cs.huji.ac.il oabend@inf.ed.ac.uk arir@cs.huji.ac.il Abstract models are effective at improving reordering at the phrase level, they are limited in their ability to map between arbitrarily divergent structures. Crosslinguistic divergences therefore pose a difficult problem for the integration of structural knowledge into statistical models (Dorr, 1994; Ding and Palmer, 2004; Zhang et al., 2008). Consequently, an annotation scheme that assigns similar structures to translations has direct applicative value for structure-aware MT systems. Such structures can be used either as features in phrase-based systems, yielding more robust decoding, or as a structural scheme which directs the translation, replacing the PCFG trees often used today. Using more stable schemes is likely to result in simpler MT systems, avoiding structure modifications like pseudo-nodes (Marcu et al., 2006) or tree sequences (Zhang et al., 2008) used in syntax-based systems to handle cross-linguistic divergences. Se"
W15-3502,C98-1013,0,\N,Missing
W19-3316,P13-1023,1,0.855673,"e subject cannot be ranked lower than the direct object (e.g., a subject construed as a T HEME cannot have a direct object construed as an AGENT). Indirect objects in the English double object construction4 are treated as R ECIPIENT construals. (18) I sent [John]R ECIPIENT↝R ECIPIENT a cake. (19) I sent a cake [to John]R ECIPIENT↝G OAL . (20) I baked [John]R ECIPIENT↝R ECIPIENT a cake. (21) I paid [John]R ECIPIENT↝R ECIPIENT [$10]C OST↝C OST . Interannotator Agreement Study Data. We piloted our guidelines using a sample of 100 scenes from the English UCCA-annotated Wiki corpus5 as detailed by Abend and Rappoport (2013). UCCA is a scheme for annotating coarsegrained predicate-argument structure such that syntactically varied paraphrases and translations should receive similar analyses. It captures both static and dynamic scenes and their participants, but does not mark semantic roles. Annotators. Four annotators (A, B, C, D), all authors of this paper, took part in this study. All are computational linguistics researchers. Datasets. Prior to development of guidelines for subjects and objects, one of the annotators (Annotator A) sampled 106 Wiki documents (44k tokens) and tagged all 10k instances of UCCA Part"
W19-3316,S17-1022,1,0.738698,"Missing"
W19-3316,J05-1004,0,0.198024,"positions/ case may improve the meaning representation of core syntactic arguments, or vice versa. In this paper, we investigate whether SNACS (Schneider et al., 2018b), an approach to semantic disambiguation of adpositions and possessives, can be adapted to cover syntactically core grammatical relations (subjects and objects). We believe this may have several practical advantages for NLP. First, many of the semantic labels in SNACS derive from VerbNet (Kipper et al., 2008) role labels. However, VerbNet and other frame-semantic approaches like FrameNet (Fillmore and Baker, 2009) and PropBank (Palmer et al., 2005) assume a lexicon as a prerequisite for semantic role annotation. This can be an obstacle to comprehensive corpus annotation when out-of-vocabulary predicates are encountered. But is a lexicon really necessary for role annotation? A general-purpose set of role labels with detailed criteria for each can potentially bypass coverage limitations of lexicon-based approaches, while still supporting some degree of generalization across grammatical paraphrases. Second, the nonreliance on a lexicon potentially simplifies the annotation process in some respects. For example, no explicit predicate disamb"
W19-3316,Q15-1034,0,0.0554848,"Missing"
W19-3316,P18-1018,1,0.601633,"orical semantic roles (Fillmore, 1968, 1982; Levin, 1993) or bundles of proto-properties (Dowty, 1991; Reisinger et al., 2015) that generalize across verbs. A parallel line of work (§2) has looked at the meanings coded by grammatical phrase-markers such as prepositions and possessives and how to disambiguate them. These inquiries necessarily overlap because many prepositions mark verb arguments or modifiers. Consequently, insights from the study of prepositions/ case may improve the meaning representation of core syntactic arguments, or vice versa. In this paper, we investigate whether SNACS (Schneider et al., 2018b), an approach to semantic disambiguation of adpositions and possessives, can be adapted to cover syntactically core grammatical relations (subjects and objects). We believe this may have several practical advantages for NLP. First, many of the semantic labels in SNACS derive from VerbNet (Kipper et al., 2008) role labels. However, VerbNet and other frame-semantic approaches like FrameNet (Fillmore and Baker, 2009) and PropBank (Palmer et al., 2005) assume a lexicon as a prerequisite for semantic role annotation. This can be an obstacle to comprehensive corpus annotation when out-of-vocabular"
W19-3319,P13-1023,1,0.797973,"identify the precise span of complex mentions. Recognition of this limitation in the field has recently prompted the Universal Coreference initiative,1 which aims to settle on a single cross-linguistically applicable annotation standard. We think that many issues stem from the common practice of creating mention annotations from scratch on the raw or tokenized text, and we suggest that they could be overcome by reusing structures from existing semantic annotation, thereby ensuring compatibility between the layers. We advocate Here we argue that Universal Conceptual Cognitive Annotation (UCCA; Abend and Rappoport, 2013) is an ideal choice, as it defines a foundational layer of predicate-argument structure whose main design principles are cross-linguistic applicability and fast annotatability by non-experts. To that end, we develop and pilot a new layer for UCCA which adds coreference information.2 This coreference layer is constrained by the spans already specified in the foundational predicate-argument layer. We compare these manual annotations to existing gold coreference annotations in multiple frameworks, finding a healthy level of overlap. ∗ Contact: jakob@cs.georgetown.edu 1 https://sites.google.com/vi"
W19-3319,W13-2322,1,0.668005,"” tectogrammatical layer, which is built on the syntactic analytic layer, can be considered quasi-semantic (Böhmová et al., 2003). Transformed syntax. In other cases, semantic label annotations enrich skeletal semantic representations that have been deterministically converted from syntactic structures. One example is Universal Decompositional Semantics (White et al., 2016), whose annotations are anchored with PredPatt, a way of converting Universal Dependencies trees (Nivre et al., 2016) to approximate predicateargument structures. Sentence-anchored. The Abstract Meaning Representation (AMR; Banarescu et al., 2013) is an example of a highly integrative (anti-modular) approach to sentence-level meaning, without anchoring below the sentence level. AMR annotations take the form of a single graph per sentence, capturing a variety of kinds of information, including predicate-argument structure, sentence focus, modality, lexical semantic distinctions, coreference, named entity typing, and entity linking (“Wikification”). English AMR annotators provide the full graph at once (with the exception of entity linking, done as a separate pass), and do not mark how pieces of the graph are anchored in tokens, which ha"
W19-3319,P10-1143,0,0.0242326,"nces. RED always uses minimum spans, except for time expressions, which follow the TIMEX3 standard (Pustejovsky et al., 2010). One of the main advantages of UCoref is that the preexisting predicate-argument and headmodifier structures of the foundational layer allow a flexible and reliable mapping between minimum and maximum span annotations. AdditionImplicit units. Implicit units may be identified as mentions and linked to coreferring expressions just like any other unit, as long as they meet the criteria outlined above. 168 7 For event coreference specifically, see also EventCorefBank (ECB; Bejan and Harabagiu, 2010) and the TAC-KBP Event Track (Mitamura et al., 2015), which uses the ACE 2005 dataset (LDC2006T06; Doddington et al., 2004). 8 A separate layer records all named entities, however, and non-coreferent ones can be considered singleton mentions. 9 The GUM guidelines specify that clausal modifiers should not be included in a nominal mention. ally, UCoref has ‘null’ spans, corresponding to implicit units in UCCA.10 cludes prepositions and case markers within mentions. This does not have a major effect on coreference, but contributes to consistency between languages that vary in the grammaticalizati"
W19-3319,E17-1053,0,0.0129158,"nnotations take the form of a single graph per sentence, capturing a variety of kinds of information, including predicate-argument structure, sentence focus, modality, lexical semantic distinctions, coreference, named entity typing, and entity linking (“Wikification”). English AMR annotators provide the full graph at once (with the exception of entity linking, done as a separate pass), and do not mark how pieces of the graph are anchored in tokens, which has spawned a line of research on various forms of token-level alignment for parsing (e.g. Flanigan et al., 2014; Pourdamghani et al., 2014; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). Chinese AMR, by contrast, is annotated in a way that aligns nodes with tokens (Li et al., 2016). Semantics-anchored. The approach we explore here is the use of a semantic layer as a foundation for a different type of semantic layer. Such approaches support modularity, while still allowing annotation reuse. A recent example for this approach is multi-sentence AMR (O’Gorman et al., 2018), which links together the previously annotated per-sentence AMR graphs to indicate coreference across sentences. 2.2 UCCA’s Foundational Layer UCCA is a coarse-grained,"
W19-3319,M98-1001,0,0.483388,"Missing"
W19-3319,doddington-etal-2004-automatic,0,0.0114935,". One of the main advantages of UCoref is that the preexisting predicate-argument and headmodifier structures of the foundational layer allow a flexible and reliable mapping between minimum and maximum span annotations. AdditionImplicit units. Implicit units may be identified as mentions and linked to coreferring expressions just like any other unit, as long as they meet the criteria outlined above. 168 7 For event coreference specifically, see also EventCorefBank (ECB; Bejan and Harabagiu, 2010) and the TAC-KBP Event Track (Mitamura et al., 2015), which uses the ACE 2005 dataset (LDC2006T06; Doddington et al., 2004). 8 A separate layer records all named entities, however, and non-coreferent ones can be considered singleton mentions. 9 The GUM guidelines specify that clausal modifiers should not be included in a nominal mention. ally, UCoref has ‘null’ spans, corresponding to implicit units in UCCA.10 cludes prepositions and case markers within mentions. This does not have a major effect on coreference, but contributes to consistency between languages that vary in the grammaticalization of their case marking. Coordination. Our treatment of coordinate entity mentions is adopted and expanded from the GUM gu"
W19-3319,P14-1134,0,0.0145439,"without anchoring below the sentence level. AMR annotations take the form of a single graph per sentence, capturing a variety of kinds of information, including predicate-argument structure, sentence focus, modality, lexical semantic distinctions, coreference, named entity typing, and entity linking (“Wikification”). English AMR annotators provide the full graph at once (with the exception of entity linking, done as a separate pass), and do not mark how pieces of the graph are anchored in tokens, which has spawned a line of research on various forms of token-level alignment for parsing (e.g. Flanigan et al., 2014; Pourdamghani et al., 2014; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). Chinese AMR, by contrast, is annotated in a way that aligns nodes with tokens (Li et al., 2016). Semantics-anchored. The approach we explore here is the use of a semantic layer as a foundation for a different type of semantic layer. Such approaches support modularity, while still allowing annotation reuse. A recent example for this approach is multi-sentence AMR (O’Gorman et al., 2018), which links together the previously annotated per-sentence AMR graphs to indicate coreference across sentences. 2.2 U"
W19-3319,N06-2015,0,0.672377,"ing semantic annotation schemes with regard to two closely related criteria: a) anchoring, i.e. the previously determined underlying structure (characters, tokens, syntax, etc.) that defines the set of possible annotation targets in a new layer; and b) modularity, the extent to which multiple kinds of information are expressed as separate (possibly linked) structures/layers, which may be annotated in different phases. Massively multilayer corpora. A few corpora comprise several layers of annotation, including semantics, with an emphasis on modularity of these layers. One example is OntoNotes (Hovy et al., 2006), annotated for syntax, named entities, word senses, PropBank (Palmer et al., 2005) predicateargument structures, and coreference. Another example is GUM (Zeldes, 2017), with layers for syntactic, coreference, discourse, and document structure. Both of these resources cover multiple genres. Different layers in these resources are anchored differently, as noted below. Token-anchored. Many semantic annotation layers are specified in terms of character or token offsets. This is the case for UCCA’s Foundational Layer (§2.2), FrameNet (Fillmore and Baker, 2009), RED (O’Gorman et al., 2016), all of"
W19-3319,W16-1702,0,0.0601897,"entence focus, modality, lexical semantic distinctions, coreference, named entity typing, and entity linking (“Wikification”). English AMR annotators provide the full graph at once (with the exception of entity linking, done as a separate pass), and do not mark how pieces of the graph are anchored in tokens, which has spawned a line of research on various forms of token-level alignment for parsing (e.g. Flanigan et al., 2014; Pourdamghani et al., 2014; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). Chinese AMR, by contrast, is annotated in a way that aligns nodes with tokens (Li et al., 2016). Semantics-anchored. The approach we explore here is the use of a semantic layer as a foundation for a different type of semantic layer. Such approaches support modularity, while still allowing annotation reuse. A recent example for this approach is multi-sentence AMR (O’Gorman et al., 2018), which links together the previously annotated per-sentence AMR graphs to indicate coreference across sentences. 2.2 UCCA’s Foundational Layer UCCA is a coarse-grained, typologically-motivated scheme for analyzing abstract semantic structures in text. It is designed to expose commonalities in semantic str"
W19-3319,D18-1264,0,0.0151652,"r sentence, capturing a variety of kinds of information, including predicate-argument structure, sentence focus, modality, lexical semantic distinctions, coreference, named entity typing, and entity linking (“Wikification”). English AMR annotators provide the full graph at once (with the exception of entity linking, done as a separate pass), and do not mark how pieces of the graph are anchored in tokens, which has spawned a line of research on various forms of token-level alignment for parsing (e.g. Flanigan et al., 2014; Pourdamghani et al., 2014; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). Chinese AMR, by contrast, is annotated in a way that aligns nodes with tokens (Li et al., 2016). Semantics-anchored. The approach we explore here is the use of a semantic layer as a foundation for a different type of semantic layer. Such approaches support modularity, while still allowing annotation reuse. A recent example for this approach is multi-sentence AMR (O’Gorman et al., 2018), which links together the previously annotated per-sentence AMR graphs to indicate coreference across sentences. 2.2 UCCA’s Foundational Layer UCCA is a coarse-grained, typologically-motivated scheme for analy"
W19-3319,D14-1048,0,0.0170057,"w the sentence level. AMR annotations take the form of a single graph per sentence, capturing a variety of kinds of information, including predicate-argument structure, sentence focus, modality, lexical semantic distinctions, coreference, named entity typing, and entity linking (“Wikification”). English AMR annotators provide the full graph at once (with the exception of entity linking, done as a separate pass), and do not mark how pieces of the graph are anchored in tokens, which has spawned a line of research on various forms of token-level alignment for parsing (e.g. Flanigan et al., 2014; Pourdamghani et al., 2014; Chen and Palmer, 2017; Szubert et al., 2018; Liu et al., 2018). Chinese AMR, by contrast, is annotated in a way that aligns nodes with tokens (Li et al., 2016). Semantics-anchored. The approach we explore here is the use of a semantic layer as a foundation for a different type of semantic layer. Such approaches support modularity, while still allowing annotation reuse. A recent example for this approach is multi-sentence AMR (O’Gorman et al., 2018), which links together the previously annotated per-sentence AMR graphs to indicate coreference across sentences. 2.2 UCCA’s Foundational Layer UC"
W19-3319,W04-2705,0,0.330429,"ntities that are contributing to or affected by a scene/event (including locations and other scenes/events). Special attention should be paid to relational nouns like teacher or friend that both refer to an entity and evoke a process or state in which the entity generally or habitually participates.6 According to the UCCA guidelines, these words are analyzed internally (as both P/S and A within a nested unit over the same span), in addition to the 6 A teacher is a person who teaches and a friend is a person who stands in a friendship relation with another person. Cf. Newell and Cheung (2018); Meyers et al. (2004). 167 context-dependent incoming edge from their parent. However, the inherent scene (of teaching or friendship) is merely evoked, but not referred to, and it is usually invariant with respect to the explicit context it occurs in. Moreover, treating one span of words as two mentions would pose a significant complication. Thus, we consider these units only in their role as Participant (and not scene) mentions. Non-scene-non-participant units. A certain subset of the remaining unit types are considered to be mention candidates. This subset is comprised of the categories, Time, Elaborator, Relato"
W19-3319,P14-2006,0,0.0273479,"allows coreference annotators to sidestep some of the challenges faced in other schemes, which do not enforce consistency with predicate-argument structure and vary widely in what kinds of mentions they annotate and how. The proposed approach is examined with a pilot annotation study and compared with annotations from other schemes. 1 Introduction Unlike some NLP tasks, coreference resolution lacks an agreed-upon standard for annotation and evaluation (Poesio et al., 2016). It has been approached using a multitude of different markup schemas, and the several evaluation metrics commonly used (Pradhan et al., 2014) are controversial (Moosavi and Strube, 2016). In particular, these schemas use divergent and often (languagespecific) syntactic criteria for defining candidate mentions in text. This includes the questions of whether to annotate entity and/or event coreference, whether to include singletons, and how to identify the precise span of complex mentions. Recognition of this limitation in the field has recently prompted the Universal Coreference initiative,1 which aims to settle on a single cross-linguistically applicable annotation standard. We think that many issues stem from the common practice o"
W19-3319,pustejovsky-etal-2010-iso,0,0.0391117,"mention spans is often one of two extremes: minimum spans (also called triggers or nuggets), which typically only consist of the head word or expression that sufficiently describes the type of entity or event; or maximum spans (also called full mentions), containing all arguments and modifiers. GUM and OntoNotes generally apply a maximum span policy for nominal mentions, with just a few exceptions.9 For verbal mentions, OntoNotes chooses minimum spans, whereas GUM annotates full clauses or sentences. RED always uses minimum spans, except for time expressions, which follow the TIMEX3 standard (Pustejovsky et al., 2010). One of the main advantages of UCoref is that the preexisting predicate-argument and headmodifier structures of the foundational layer allow a flexible and reliable mapping between minimum and maximum span annotations. AdditionImplicit units. Implicit units may be identified as mentions and linked to coreferring expressions just like any other unit, as long as they meet the criteria outlined above. 168 7 For event coreference specifically, see also EventCorefBank (ECB; Bejan and Harabagiu, 2010) and the TAC-KBP Event Track (Mitamura et al., 2015), which uses the ACE 2005 dataset (LDC2006T06;"
W19-3319,P16-1060,0,0.0257005,"p some of the challenges faced in other schemes, which do not enforce consistency with predicate-argument structure and vary widely in what kinds of mentions they annotate and how. The proposed approach is examined with a pilot annotation study and compared with annotations from other schemes. 1 Introduction Unlike some NLP tasks, coreference resolution lacks an agreed-upon standard for annotation and evaluation (Poesio et al., 2016). It has been approached using a multitude of different markup schemas, and the several evaluation metrics commonly used (Pradhan et al., 2014) are controversial (Moosavi and Strube, 2016). In particular, these schemas use divergent and often (languagespecific) syntactic criteria for defining candidate mentions in text. This includes the questions of whether to annotate entity and/or event coreference, whether to include singletons, and how to identify the precise span of complex mentions. Recognition of this limitation in the field has recently prompted the Universal Coreference initiative,1 which aims to settle on a single cross-linguistically applicable annotation standard. We think that many issues stem from the common practice of creating mention annotations from scratch o"
W19-3319,W16-0701,0,0.0618674,"Missing"
W19-3319,L16-1026,0,0.0942791,"r, 2009), RED (O’Gorman et al., 2016), all of the layers in GUM, and the named entity and word sense annotations in OntoNotes. Though the guidelines may mention syntactic criteria for deciding what units to semantically annotate, the annotated data does not explicitly tie these layers to syntactic units, and to the best of our knowledge the annotator is not constrained by the syntactic annotation. Syntax-anchored. Semantic annotations explicitly defined in terms of syntactic units include: PropBank (such as in OntoNotes); and the coreference annotations in the Prague Dependency Treebank (PDT; Nedoluzhko et al., 2016). In addition, PDT’s “deep syntactic” tectogrammatical layer, which is built on the syntactic analytic layer, can be considered quasi-semantic (Böhmová et al., 2003). Transformed syntax. In other cases, semantic label annotations enrich skeletal semantic representations that have been deterministically converted from syntactic structures. One example is Universal Decompositional Semantics (White et al., 2016), whose annotations are anchored with PredPatt, a way of converting Universal Dependencies trees (Nivre et al., 2016) to approximate predicateargument structures. Sentence-anchored. The Ab"
W19-3319,N18-1106,1,0.896893,"Missing"
W19-3319,L18-1537,0,0.016262,"entions as they describe entities that are contributing to or affected by a scene/event (including locations and other scenes/events). Special attention should be paid to relational nouns like teacher or friend that both refer to an entity and evoke a process or state in which the entity generally or habitually participates.6 According to the UCCA guidelines, these words are analyzed internally (as both P/S and A within a nested unit over the same span), in addition to the 6 A teacher is a person who teaches and a friend is a person who stands in a friendship relation with another person. Cf. Newell and Cheung (2018); Meyers et al. (2004). 167 context-dependent incoming edge from their parent. However, the inherent scene (of teaching or friendship) is merely evoked, but not referred to, and it is usually invariant with respect to the explicit context it occurs in. Moreover, treating one span of words as two mentions would pose a significant complication. Thus, we consider these units only in their role as Participant (and not scene) mentions. Non-scene-non-participant units. A certain subset of the remaining unit types are considered to be mention candidates. This subset is comprised of the categories, Ti"
W19-3319,C18-1313,0,0.106736,"Missing"
W19-3319,W16-5706,0,0.0526032,"Missing"
W19-3319,J05-1004,0,0.23596,"nchoring, i.e. the previously determined underlying structure (characters, tokens, syntax, etc.) that defines the set of possible annotation targets in a new layer; and b) modularity, the extent to which multiple kinds of information are expressed as separate (possibly linked) structures/layers, which may be annotated in different phases. Massively multilayer corpora. A few corpora comprise several layers of annotation, including semantics, with an emphasis on modularity of these layers. One example is OntoNotes (Hovy et al., 2006), annotated for syntax, named entities, word senses, PropBank (Palmer et al., 2005) predicateargument structures, and coreference. Another example is GUM (Zeldes, 2017), with layers for syntactic, coreference, discourse, and document structure. Both of these resources cover multiple genres. Different layers in these resources are anchored differently, as noted below. Token-anchored. Many semantic annotation layers are specified in terms of character or token offsets. This is the case for UCCA’s Foundational Layer (§2.2), FrameNet (Fillmore and Baker, 2009), RED (O’Gorman et al., 2016), all of the layers in GUM, and the named entity and word sense annotations in OntoNotes. Th"
W19-3319,poesio-artstein-2008-anaphoric,0,0.078252,"Missing"
W19-3319,D16-1177,0,0.1746,"Missing"
W19-3319,W18-0704,0,0.0266861,"Missing"
W19-3319,W16-0713,0,0.0135454,"and how we can (re)cover annotations in established schemas from our new schema. We can interpret this experiment as asking: If we had a perfect system for UCoref, could we use that to predict GUM/OntoNotes/RED-style coreference? And vice versa, if we had an oracle in one of those schemes, and possibly oracle UCoref mentions, how closely could we convert to UCoref?16 Exact mention matches. A naïve approach would be to look at the token spans covered by all mentions and reference clusters and count how often we can find an exact match between UCoref and one of the existing schemes. 16 See also Zeldes and Zhang (2016), who base a full coreference resolution system on this idea. In UCoref, we use maximum spans by default, but thanks to the nature of the UCCA foundational layer, minimum spans can easily be recovered from Centers and scene-evokers. For schemas with a minimum span approach, we can switch to a minimum span approach in UCoref by choosing the head unit of each maximum span unit as its representative mention. This works well between UCoref and RED as they have similar policies for determining semantic heads, which is crucial for, e.g., light verb constructions. This would be problematic, however,"
