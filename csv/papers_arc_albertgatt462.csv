2021.findings-emnlp.132,Entity-Based Semantic Adequacy for Data-to-Text Generation,2021,-1,-1,2,0,6763,juliette faille,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"While powerful pre-trained language models have improved the fluency of text generation models, semantic adequacy -the ability to generate text that is semantically faithful to the input- remains an unsolved issue. In this paper, we introduce a novel automatic evaluation metric, Entity-Based Semantic Adequacy, which can be used to assess to what extent generation models that verbalise RDF (Resource Description Framework) graphs produce text that contains mentions of the entities occurring in the RDF input. This is important as RDF subject and object entities make up 2/3 of the input. We use our metric to compare 25 models from the WebNLG Shared Tasks and we examine correlation with results from human evaluations of semantic adequacy. We show that while our metric correlates with human evaluation scores, this correlation varies with the specifics of the human evaluation setup. This suggests that in order to measure the entity-based adequacy of generated texts, an automatic metric such as the one proposed here might be more reliable, as less subjective and more focused on correct verbalisation of the input, than human evaluation measures."
2021.blackboxnlp-1.15,On the Language-specificity of Multilingual {BERT} and the Impact of Fine-tuning,2021,-1,-1,4,1,12095,marc tanti,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,"Recent work has shown evidence that the knowledge acquired by multilingual BERT (mBERT) has two components: a language-specific and a language-neutral one. This paper analyses the relationship between them, in the context of fine-tuning on two tasks {--} POS tagging and natural language inference {--} which require the model to bring to bear different degrees of language-specific knowledge. Visualisations reveal that mBERT loses the ability to cluster representations by language after fine-tuning, a result that is supported by evidence from language identification experiments. However, further experiments on {`}unlearning{'} language-specific representations using gradient reversal and iterative adversarial learning are shown not to add further improvement to the language-independent component over and above the effect of fine-tuning. The results presented here suggest that the process of fine-tuning causes a reorganisation of the model{'}s limited representational capacity, enhancing language-independent representations at the expense of language-specific ones."
2020.nl4xai-1.5,"The Natural Language Pipeline, Neural Text Generation and Explainability",2020,-1,-1,2,0,6763,juliette faille,2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence,0,End-to-end encoder-decoder approaches to data-to-text generation are often black boxes whose predictions are difficult to explain. Breaking up the end-to-end model into sub-modules is a natural way to address this problem. The traditional pre-neural Natural Language Generation (NLG) pipeline provides a framework for breaking up the end-to-end encoder-decoder. We survey recent papers that integrate traditional NLG submodules in neural approaches and analyse their explainability. Our survey is a first step towards building explainable neural NLG models.
2020.nl4xai-1.6,Towards Harnessing Natural Language Generation to Explain Black-box Models,2020,-1,-1,3,0,16441,ettore mariotti,2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence,0,"The opaque nature of many machine learning techniques prevents the wide adoption of powerful information processing tools for high stakes scenarios. The emerging field eXplainable Artificial Intelligence (XAI) aims at providing justifications for automatic decision-making systems in order to ensure reliability and trustworthiness in the users. For achieving this vision, we emphasize the importance of a natural language textual modality as a key component for a future intelligent interactive agent. We outline the challenges of XAI and review a set of publications that work in this direction."
2020.lrec-1.626,Annotating for Hate Speech: The {M}a{N}e{C}o Corpus and Some Input from Critical Discourse Analysis,2020,-1,-1,4,0,17903,stavros assimakopoulos,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This paper presents a novel scheme for the annotation of hate speech in corpora of Web 2.0 commentary. The proposed scheme is motivated by the critical analysis of posts made in reaction to news reports on the Mediterranean migration crisis and LGBTIQ+ matters in Malta, which was conducted under the auspices of the EU-funded C.O.N.T.A.C.T. project. Based on the realisation that hate speech is not a clear-cut category to begin with, appears to belong to a continuum of discriminatory discourse and is often realised through the use of indirect linguistic means, it is argued that annotation schemes for its detection should refrain from directly including the label {`}hate speech,{'} as different annotators might have different thresholds as to what constitutes hate speech and what not. In view of this, we propose a multi-layer annotation scheme, which is pilot-tested against a binary {\mbox{$\pm$}}hate speech classification and appears to yield higher inter-annotator agreement. Motivating the postulation of our scheme, we then present the MaNeCo corpus on which it will eventually be used; a substantial corpus of on-line newspaper comments spanning 10 years."
2020.lrec-1.784,{MASRI}-{HEADSET}: A {M}altese Corpus for Speech Recognition,2020,-1,-1,2,0,18163,carlos mena,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Maltese, the national language of Malta, is spoken by approximately 500,000 people. Speech processing for Maltese is still in its early stages of development. In this paper, we present the first spoken Maltese corpus designed purposely for Automatic Speech Recognition (ASR). The MASRI-HEADSET corpus was developed by the MASRI project at the University of Malta. It consists of 8 hours of speech paired with text, recorded by using short text snippets in a laboratory environment. The speakers were recruited from different geographical locations all over the Maltese islands, and were roughly evenly distributed by gender. This paper also presents some initial results achieved in baseline experiments for Maltese ASR using Sphinx and Kaldi. The MASRI HEADSET Corpus is publicly available for research/academic purposes."
2020.inlg-1.45,Gradations of Error Severity in Automatic Image Descriptions,2020,-1,-1,4,0,3387,emiel miltenburg,Proceedings of the 13th International Conference on Natural Language Generation,0,"Earlier research has shown that evaluation metrics based on textual similarity (e.g., BLEU, CIDEr, Meteor) do not correlate well with human evaluation scores for automatically generated text. We carried out an experiment with Chinese speakers, where we systematically manipulated image descriptions to contain different kinds of errors. Because our manipulated descriptions form minimal pairs with the reference descriptions, we are able to assess the impact of different kinds of errors on the perceived quality of the descriptions. Our results show that different kinds of errors elicit significantly different evaluation scores, even though all erroneous descriptions differ in only one character from the reference descriptions. Evaluation metrics based solely on textual similarity are unable to capture these differences, which (at least partially) explains their poor correlation with human judgments. Our work provides the foundations for future work, where we aim to understand why different errors are seen as more or less severe."
2020.gebnlp-1.1,Unmasking Contextual Stereotypes: Measuring and Mitigating {BERT}{'}s Gender Bias,2020,-1,-1,3,0,19257,marion bartl,Proceedings of the Second Workshop on Gender Bias in Natural Language Processing,0,"Contextualized word embeddings have been replacing standard embeddings as the representational knowledge source of choice in NLP systems. Since a variety of biases have previously been found in standard word embeddings, it is crucial to assess biases encoded in their replacements as well. Focusing on BERT (Devlin et al., 2018), we measure gender bias by studying associations between gender-denoting target words and names of professions in English and German, comparing the findings with real-world workforce statistics. We mitigate bias by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that our method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German. Our results highlight the importance of investigating bias and mitigation techniques cross-linguistically,especially in view of the current emphasis on large-scale, multilingual language models."
2020.evalnlgeval-1.5,On the interaction of automatic evaluation and task framing in headline style transfer,2020,-1,-1,6,0,6234,lorenzo mattei,Proceedings of the 1st Workshop on Evaluating NLG Evaluation,0,"An ongoing debate in the NLG community concerns the best way to evaluate systems, with human evaluation often being considered the most reliable method, compared to corpus-based metrics. However, tasks involving subtle textual differences, such as style transfer, tend to be hard for humans to perform. In this paper, we propose an evaluation method for this task based on purposely-trained classifiers, showing that it better reflects system differences than traditional metrics such as BLEU."
W19-8625,Visually grounded generation of entailments from premises,2019,0,0,2,0,21292,somayeh jafaritazehjani,Proceedings of the 12th International Conference on Natural Language Generation,0,"Natural Language Inference (NLI) is the task of determining the semantic relationship between a premise and a hypothesis. In this paper, we focus on the generation of hypotheses from premises in a multimodal setting, to generate a sentence (hypothesis) given an image and/or its description (premise) as the input. The main goals of this paper are (a) to investigate whether it is reasonable to frame NLI as a generation task; and (b) to consider the degree to which grounding textual premises in visual information is beneficial to generation. We compare different neural architectures, showing through automatic and human evaluation that entailments can indeed be generated successfully. We also show that multimodal models outperform unimodal models in this task, albeit marginally"
W19-8643,Best practices for the human evaluation of automatically generated text,2019,0,4,2,0,3388,chris lee,Proceedings of the 12th International Conference on Natural Language Generation,0,"Currently, there is little agreement as to how Natural Language Generation (NLG) systems should be evaluated. While there is some agreement regarding automatic metrics, there is a high degree of variation in the way that human evaluation is carried out. This paper provides an overview of how human evaluation is currently conducted, and presents a set of best practices, grounded in the literature. With this paper, we hope to contribute to the quality and consistency of human evaluations in NLG."
P19-1246,You Write like You Eat: Stylistic Variation as a Predictor of Social Stratification,2019,46,0,2,0,11971,angelo basile,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Inspired by Labov{'}s seminal work on stylisticvariation as a function of social stratification,we develop and compare neural models thatpredict a person{'}s presumed socio-economicstatus, obtained through distant supervision,from their writing style on social media. Thefocus of our work is on identifying the mostimportant stylistic parameters to predict socio-economic group. In particular, we show theeffectiveness of morpho-syntactic features aspredictors of style, in contrast to lexical fea-tures, which are good predictors of topic"
W18-6551,Meteorologists and Students: A resource for language grounding of geographical descriptors,2018,8,0,5,0,27652,alejandro ramossoto,Proceedings of the 11th International Conference on Natural Language Generation,0,"We present a data resource which can be useful for research purposes on language grounding tasks in the context of geographical referring expression generation. The resource is composed of two data sets that encompass 25 different geographical descriptors and a set of associated graphical representations, drawn as polygons on a map by two groups of human subjects: teenage students and expert meteorologists."
W18-6562,Specificity measures and reference,2018,18,0,1,1,6764,albert gatt,Proceedings of the 11th International Conference on Natural Language Generation,0,"In this paper we study empirically the validity of measures of referential success for referring expressions involving gradual properties. More specifically, we study the ability of several measures of referential success to predict the success of a user in choosing the right object, given a referring expression. Experimental results indicate that certain fuzzy measures of success are able to predict human accuracy in reference resolution. Such measures are therefore suitable for the estimation of the success or otherwise of a referring expression produced by a generation algorithm, especially in case the properties in a domain cannot be assumed to have crisp denotations."
L18-1525,{F}ace2{T}ext: Collecting an Annotated Image Description Corpus for the Generation of Rich Face Descriptions,2018,21,3,1,1,6764,albert gatt,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"The past few years have witnessed renewed interest in NLP tasks at the interface between vision and language. One intensively-studied problem is that of automatically generating text from images. In this paper, we extend this problem to the more specific domain of face description. Unlike scene descriptions, face descriptions are more fine-grained and rely on attributes extracted from the image, rather than objects and relations. Given that no data exists for this task, we present an ongoing crowdsourcing study to collect a corpus of descriptions of face images taken `in the wild'. To gain a better understanding of the variation we find in face description and the possible issues that this may raise, we also conducted an annotation study on a subset of the corpus. Primarily, we found descriptions to refer to a mixture of attributes, not only physical, but also emotional and inferential, which is bound to create further challenges for current image-to-text methods."
C18-1199,Grounded Textual Entailment,2018,39,5,9,1,30853,hoa vu,Proceedings of the 27th International Conference on Computational Linguistics,0,"Capturing semantic relations between sentences, such as entailment, is a long-standing challenge for computational semantics. Logic-based models analyse entailment in terms of possible worlds (interpretations, or situations) where a premise P entails a hypothesis H iff in all worlds where P is true, H is also true. Statistical models view this relationship probabilistically, addressing it in terms of whether a human would likely infer H from P. In this paper, we wish to bridge these two perspectives, by arguing for a visually-grounded version of the Textual Entailment task. Specifically, we ask whether models can perform better if, in addition to P and H, there is also an image (corresponding to the relevant {``}world{''} or {``}situation{''}). We use a multimodal version of the SNLI dataset (Bowman et al., 2015) and we compare {``}blind{''} and visually-augmented models of textual entailment. We show that visual information is beneficial, but we also conduct an in-depth error analysis that reveals that current multimodal models are not performing {``}grounding{''} in an optimal fashion."
W17-5311,{LCT}-{MALTA}{'}s Submission to {R}ep{E}val 2017 Shared Task,2017,8,5,6,1,30853,hoa vu,Proceedings of the 2nd Workshop on Evaluating Vector Space Representations for {NLP},0,"System using BiLSTM and max pooling. Embedding is enhanced by POS, character and dependency info."
W17-3506,What is the Role of Recurrent Neural Networks ({RNN}s) in an Image Caption Generator?,2017,37,1,2,1,12095,marc tanti,Proceedings of the 10th International Conference on Natural Language Generation,0,"Image captioning has evolved into a core task for Natural Language Generation and has also proved to be an important testbed for deep learning approaches to handling multimodal representations. Most contemporary approaches rely on a combination of a convolutional network to handle image features, and a recurrent network to encode linguistic information. The latter is typically viewed as the primary {``}generation{''} component. Beyond this high-level characterisation, a CNN+RNN model supports a variety of architectural designs. The dominant model in the literature is one in which visual features encoded by a CNN are {``}injected{''} as part of the linguistic encoding process, driving the RNN{'}s linguistic choices. By contrast, it is possible to envisage an architecture in which visual and linguistic features are encoded separately, and merged at a subsequent stage. In this paper, we address two related questions: (1) Is direct injection the best way of combining multimodal information, or is a late merging alternative better for the image captioning task? (2) To what extent should a recurrent network be viewed as actually generating, rather than simply encoding, linguistic information?"
W17-1304,Morphological Analysis for the {M}altese Language: The challenges of a hybrid system,2017,24,0,2,1,12097,claudia borg,Proceedings of the Third {A}rabic Natural Language Processing Workshop,0,"Maltese is a morphologically rich language with a hybrid morphological system which features both concatenative and non-concatenative processes. This paper analyses the impact of this hybridity on the performance of machine learning techniques for morphological labelling and clustering. In particular, we analyse a dataset of morphologically related word clusters to evaluate the difference in results for concatenative and non-concatenative clusters. We also describe research carried out in morphological labelling, with a particular focus on the verb category. Two evaluations were carried out, one using an unseen dataset, and another one using a gold standard dataset which was manually labelled. The gold standard dataset was split into concatenative and non-concatenative to analyse the difference in results between the two morphological systems."
borg-gatt-2014-crowd,"Crowd-sourcing evaluation of automatically acquired, morphologically related word groupings",2014,19,1,2,1,12097,claudia borg,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The automatic discovery and clustering of morphologically related words is an important problem with several practical applications. This paper describes the evaluation of word clusters carried out through crowd-sourcing techniques for the Maltese language. The hybrid (Semitic-Romance) nature of Maltese morphology, together with the fact that no large-scale lexical resources are available for Maltese, make this an interesting and challenging problem."
C14-1189,Learning when to point: A data-driven approach,2014,19,4,1,1,6764,albert gatt,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"The relationship between how people describe objects and when they choose to point is complex and likely to be influenced by factors related to both perceptual and discourse context. In this paper, we explore these interactions using machine-learning on a dialogue corpus, to identify multimodal referential strategies that can be used in automatic multimodal generation. We show that the decision to use a pointing gesture depends on features of the accompanying description (especially whether it contains spatial information), and on visual properties, especially distance or separation of a referent from its previous referent."
W13-2109,What and Where: An Empirical Investigation of Pointing Gestures and Descriptions in Multimodal Referring Actions,2013,30,6,1,1,6764,albert gatt,Proceedings of the 14th {E}uropean Workshop on Natural Language Generation,0,"Pointing gestures are pervasive in human referring actions, and are often combined with spoken descriptions. Combining gesture and speech naturally to refer to objects is an essential task in multimodal NLG systems. However, the way gesture and speech should be combined in a referring act remains an open question. In particular, it is not clear whether, in planning a pointing gesture in conjunction with a description, an NLG system should seek to minimise the redundancy between them, e.g. by letting the pointing gesture indicate locative information, with other, nonlocative properties of a referent included in the description. This question has a bearing on whether the gestural and spoken parts of referring acts are planned separately or arise from a common underlying computational mechanism. This paper investigates this question empirically, using machine-learning techniques on a new corpus of dialogues involving multimodal references to objects. Our results indicate that human pointing strategies interact with descriptive strategies. In particular, pointing gestures are strongly associated with the use of locative features in referring expressions."
belz-gatt-2012-repository,A Repository of Data and Evaluation Resources for Natural Language Generation,2012,10,1,2,0,26421,anja belz,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Starting in 2007, the field of natural language generation (NLG) has organised shared-task evaluation events every year, under the Generation Challenges umbrella. In the course of these shared tasks, a wealth of data has been created, along with associated task definitions and evaluation regimes. In other contexts too, sharable NLG data is now being created. In this paper, we describe the online repository that we have created as a one-stop resource for obtaining NLG task materials, both from Generation Challenges tasks and from other sources, where the set of materials provided for each task consists of (i) task definition, (ii) input and output data, (iii) evaluation software, (iv) documentation, and (v) publications reporting previous results."
rosner-etal-2012-incorporating,Incorporating an Error Corpus into a Spellchecker for {M}altese,2012,11,0,2,0,16561,michael rosner,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper discusses the ongoing development of a new Maltese spell checker, highlighting the methodologies which would best suit such a language. We thus discuss several previous attempts, highlighting what we believe to be their weakest point: a lack of attention to context. Two developments are of particular interest, both of which concern the availability of language resources relevant to spellchecking: (i) the Maltese Language Resource Server (MLRS) which now includes a representative corpus of c. 100M words extracted from diverse documents including the Maltese Legislation, press releases and extracts from Maltese web-pages and (ii) an extensive and detailed corpus of spelling errors that was collected whilst part of the MLRS texts were being prepared. We describe the structure of these resources as well as the experimental approaches focused on context that we are now in a position to adopt. We describe the framework within which a variety of different approaches to spellchecking and evaluation will be carried out, and briefly discuss the first baseline system we have implemented. We conclude the paper with a roadmap for future improvements."
W11-2804,What is in a text and what does it do: Qualitative Evaluations of an {NLG} system {--} the {BT}-Nurse {--} using content analysis and discourse analysis,2011,15,4,6,0,44143,rahul sambaraju,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"Evaluations of NLG systems generally are quantiative, that is, based on corpus comparison statistics and/or results of experiments with people. Outcomes of such evaluations are important in demonstrating whether or not an NLG system is successful, but leave gaps in understanding why this is the case. Alternatively, qualitative evaluations carried out by experts provide knowledge on where a system needs to be improved. In this paper we describe two such evaluations carried out for the BT-Nurse system, using two different methodologies (content analysis and discourse analysis). The outcomes of such evaluations are discussed in comparison to what was learnt from a quantitiave evaluation of BT-Nurse. Implications for the role of similar evaluations in NLG are also discussed."
W11-2812,"If it may have happened before, it happened, but not necessarily before",2011,32,3,1,1,6764,albert gatt,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"Temporal uncertainty in raw data can impede the inference of temporal and causal relationships between events and compromise the output of data-to-text NLG systems. In this paper, we introduce a framework to reason with and represent temporal uncertainty from the raw data to the generated text, in order to provide a faithful picture to the user of a particular situation. The model is grounded in experimental data from multiple languages, shedding light on the generality of the approach."
W11-2829,Generation Challenges 2011 Preface,2011,0,0,2,0.272996,26421,anja belz,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"Generation Challenges 2011 (GenChal'11) was the fifth round of shared-task evaluation competitions (STECs) involving the generation of natural language. It followed four previous events: the Pilot Attribute Selection for Generating Referring Expressions (ASGRE) Challenge in 2007 which had its results meeting at UCNLGMT in Copenhagen, Denmark; Referring Expression Generation (REG) Challenges in 2008, with a results meeting at INLG'08 in Ohio, US; Generation Challenges 2009 with a results meeting at ENLG'09 in Athens, Greece; and most recently Generation Challenges 2010 with a results meeting at INLG'10 in Trim, Ireland. More information about all these NLG STEC events can be found via the links on the Generation Challenges homepage (http://www.nltg.brighton.ac.uk/research/genchal11)."
W10-4206,"Textual Properties and Task-based Evaluation: Investigating the Role of Surface Properties, Structure and Content",2010,26,1,1,1,6764,albert gatt,Proceedings of the 6th International Natural Language Generation Conference,0,"This paper investigates the relationship between the results of an extrinsic, task-based evaluation of an NLG system and various metrics measuring both surface and deep semantic textual properties, including relevance. The latter rely heavily on domain knowledge. We show that they correlate systematically with some measures of performance. The core argument of this paper is that more domain knowledge-based metrics shed more light on the relationship between deep semantic properties of a text and task performance."
W10-4225,Generation Challenges 2010 Preface,2010,0,0,2,0.323654,26421,anja belz,Proceedings of the 6th International Natural Language Generation Conference,0,None
W09-2816,The {GREC} Main Subject Reference Generation Challenge 2009: Overview and Evaluation Results,2009,9,9,4,0.435923,26421,anja belz,Proceedings of the 2009 Workshop on Language Generation and Summarisation ({UCNLG}+{S}um 2009),0,"The GREC-MSR Task at Generation Challenges 2009 required participating systems to select coreference chains to the main subject of short encyclopaedic texts collected from Wikipedia. Three teams submitted one system each, and we additionally created four baseline systems. Systems were tested automatically using existing intrinsic metrics. We also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools. In addition, systems were tested in an intrinsic evaluation involving human judges. This report describes the GREC-MSR Task and the evaluation methods applied, gives brief descriptions of the participating systems, and presents the evaluation results."
W09-0613,{S}imple{NLG}: A Realisation Engine for Practical Applications,2009,12,205,1,1,6764,albert gatt,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"This paper describes SimpleNLG, a realisation engine for English which aims to provide simple and robust interfaces to generate syntactic structures and linearise them. The library is also flexible in allowing the use of mixed (canned and non-canned) representations."
W09-0615,A Hearer-Oriented Evaluation of Referring Expression Generation,2009,15,4,4,0,47096,imtiaz khan,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,This paper discusses the evaluation of a Generation of Referring Expressions algorithm that takes structural ambiguity into account. We describe an ongoing study with human readers.
W09-0627,Generation {C}hallenges 2009: Preface,2009,0,0,2,0.435923,26421,anja belz,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"Generation Challenges 2009 was the third round of shared-task evaluation competitions (STECs) that involve the generation of natural language, and followed the Pilot Attribute Selection for Generating Referring Expressions Challenge in 2007 (ASGRE'07) and Referring Expression Generation Challenges in 2008 (REG'08). More information about all these NLG STEC activities can be found via the links on the Generation Challenges homepage: http://www.nltg.brighton.ac.uk/research/genchal09"
W09-0629,The {TUNA}-{REG} {C}hallenge 2009: Overview and Evaluation Results,2009,17,87,1,1,6764,albert gatt,Proceedings of the 12th {E}uropean Workshop on Natural Language Generation ({ENLG} 2009),0,"The GREC Task at REG '08 required participating systems to select coreference chains to the main subject of short encyclopaedic texts collected from Wikipedia. Three teams submitted a total of 6 systems, and we additionally created four baseline systems. Systems were tested automatically using a range of existing intrinsic metrics. We also evaluated systems extrinsically by applying coreference resolution tools to the outputs and measuring the success of the tools. In addition, systems were tested in a reading/comprehension experiment involving human subjects. This report describes the GREC Task and the evaluation methods, gives brief descriptions of the participating systems, and presents the evaluation results."
R09-1021,Text Content and Task Performance in the Evaluation of a Natural Language Generation System,2009,17,17,1,1,6764,albert gatt,Proceedings of the International Conference {RANLP}-2009,0,"An important question in the evaluation of Natural Language Generation systems concerns the relationship between textual characteristics and task performance. If the results of task-based evaluation can be correlated to properties of the text, there are better prospects for improving the system. The present paper investigates this relationship by focusing on the outcomes of a task-based evaluation of a system that generates summaries of patient data, attempting to correlate these with the results of an analysis of the systemxe2x80x99s texts, compared to a set of gold standard human-authored summaries."
2009.jeptalnrecital-long.13,Le projet {B}aby{T}alk : g{\\'e}n{\\'e}ration de texte {\\`a} partir de donn{\\'e}es h{\\'e}t{\\'e}rog{\\`e}nes pour la prise de d{\\'e}cision en unit{\\'e} n{\\'e}onatale,2009,-1,-1,2,0,14071,franccois portet,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Notre soci{\'e}t{\'e} g{\'e}n{\`e}re une masse d{'}information toujours croissante, que ce soit en m{\'e}decine, en m{\'e}t{\'e}orologie, etc. La m{\'e}thode la plus employ{\'e}e pour analyser ces donn{\'e}es est de les r{\'e}sumer sous forme graphique. Cependant, il a {\'e}t{\'e} d{\'e}montr{\'e} qu{'}un r{\'e}sum{\'e} textuel est aussi un mode de pr{\'e}sentation efficace. L{'}objectif du prototype BT-45, d{\'e}velopp{\'e} dans le cadre du projet Babytalk, est de g{\'e}n{\'e}rer des r{\'e}sum{\'e}s de 45 minutes de signaux physiologiques continus et d{'}{\'e}v{\'e}nements temporels discrets en unit{\'e} n{\'e}onatale de soins intensifs (NICU). L{'}article pr{\'e}sente l{'}aspect g{\'e}n{\'e}ration de texte de ce prototype. Une exp{\'e}rimentation clinique a montr{\'e} que les r{\'e}sum{\'e}s humains am{\'e}liorent la prise de d{\'e}cision par rapport {\`a} l{'}approche graphique, tandis que les textes de BT-45 donnent des r{\'e}sultats similaires {\`a} l{'}approche graphique. Une analyse a identifi{\'e} certaines des limitations de BT-45 mais en d{\'e}pit de cellesci, notre travail montre qu{'}il est possible de produire automatiquement des r{\'e}sum{\'e}s textuels efficaces de donn{\'e}es complexes."
W08-1108,Attribute Selection for Referring Expression Generation: New Algorithms and Evaluation Methods,2008,27,22,1,1,6764,albert gatt,Proceedings of the Fifth International Natural Language Generation Conference,0,"Referring expression generation has recently been the subject of the first Shared Task Challenge in NLG. In this paper, we analyse the systems that participated in the Challenge in terms of their algorithmic properties, comparing new techniques to classic ones, based on results from a new human task-performance experiment and from the intrinsic measures that were used in the Challenge. We also consider the relationship between different evaluation methods, showing that extrinsic task-performance experiments and intrinsic evaluation methods yield results that are not significantly correlated. We argue that this highlights the importance of including extrinsic evaluation methods in comparative NLG evaluations."
W08-1119,The Importance of Narrative and Other Lessons from an Evaluation of an {NLG} System that Summarises Clinical Data,2008,19,34,2,0,5931,ehud reiter,Proceedings of the Fifth International Natural Language Generation Conference,0,"The BABYTALK BT-45 system generates textual summaries of clinical data about babies in a neonatal intensive care unit. A recent task-based evaluation of the system suggested that these summaries are useful, but not as effective as they could be. In this paper we present a qualitative analysis of problems that the evaluation highlighted in BT-45 texts. Many of these problems are due to the fact that BT-45 does not generate good narrative texts; this is a topic which has not previously received much attention from the NLG research community, but seems to be quite important for creating good data-to-text systems."
W08-1126,{REG} Challenge Preface,2008,0,0,2,0.643209,26421,anja belz,Proceedings of the Fifth International Natural Language Generation Conference,0,None
W08-1127,The {GREC} Challenge 2008: Overview and Evaluation Results,2008,-1,-1,4,0.643209,26421,anja belz,Proceedings of the Fifth International Natural Language Generation Conference,0,None
W08-1131,The {TUNA} Challenge 2008: Overview and Evaluation Results,2008,-1,-1,1,1,6764,albert gatt,Proceedings of the Fifth International Natural Language Generation Conference,0,None
P08-2050,Intrinsic vs. Extrinsic Evaluation Measures for Referring Expression Generation,2008,11,39,2,0.643209,26421,anja belz,"Proceedings of ACL-08: HLT, Short Papers",0,"In this paper we present research in which we apply (i) the kind of intrinsic evaluation metrics that are characteristic of current comparative HLT evaluation, and (ii) extrinsic, human task-performance evaluations more in keeping with NLG traditions, to 15 systems implementing a language generation task. We analyse the evaluation results and find that there are no significant correlations between intrinsic and extrinsic evaluation measures for this task."
W07-2307,Evaluating algorithms for the Generation of Referring Expressions using a balanced corpus,2007,20,81,1,1,6764,albert gatt,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"Despite being the focus of intensive research, evaluation of algorithms that generate referring expressions is still in its infancy. We describe a corpus-based evaluation methodology, applied to a number of classic algorithms in this area. The methodology focuses on balance and semantic transparency to enable comparison of human and algorithmic output. Although the Incremental Algorithm emerges as the best match, we found that its dependency on manually-set parameters makes its performance difficult to predict."
D07-1011,Incremental Generation of Plural Descriptions: Similarity and Partitioning,2007,21,9,1,1,6764,albert gatt,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"Approaches to plural reference generation emphasise descriptive brevity, but often lack empirical backing. This paper describes a corpus-based study of plural descriptions, and proposes a psycholinguisticallymotivated algorithm for plural reference generation. The descriptive strategy is based on partitioning and incorporates corpusderived heuristics. An exhaustive evaluation shows that the output closely matches human data."
2007.mtsummit-ucnlg.13,The attribute selection for generation of referring expressions challenge. [Introduction to Shared Task Evaluation Challenge.],2007,-1,-1,2,0.643209,26421,anja belz,Proceedings of the Workshop on Using corpora for natural language generation,0,None
2007.mtsummit-ucnlg.14,The attribute selection for {GRE} challenge: overview and evaluation results,2007,4,40,2,0.643209,26421,anja belz,Proceedings of the Workshop on Using corpora for natural language generation,0,") Challenge wasthe xefxacx81rst shared-task evaluation challenge inthe xefxacx81eld of Natural Language Generation.Six teams submitted a total of 22 systems.All submitted systems were tested automat-ically for minimality, uniqueness and xe2x80x98hu-manlikenessxe2x80x99. In addition, the output of 15systems was tested in a task-based exper-iment where subjects were asked to iden-tify referents, and the speed and accuracy ofidentixefxacx81cation was measured. This report de-scribes the"
2007.mtsummit-ucnlg.21,Content determination in {GRE}: evaluating the evaluator,2007,4,3,2,0.271773,5942,kees deemter,Proceedings of the Workshop on Using corpora for natural language generation,0,None
W06-1420,Building a Semantically Transparent Corpus for the Generation of Referring Expressions.,2006,6,64,3,0,5942,kees deemter,Proceedings of the Fourth International Natural Language Generation Conference,0,"This paper discusses the construction of a corpus for the evaluation of algorithms that generate referring expressions. It is argued that such an evaluation task requires a semantically transparent corpus, and controlled experiments are the best way to create such a resource. We address a number of issues that have arisen in an ongoing evaluation study, among which is the problem of judging the output of GRE algorithms against a human gold standard."
P06-2033,Conceptual Coherence in the Generation of Referring Expressions,2006,19,7,1,1,6764,albert gatt,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"One of the challenges in the automatic generation of referring expressions is to identify a set of domain entities coherently, that is, from the same conceptual perspective. We describe and evaluate an algorithm that generates a conceptually coherent description of a target set. The design of the algorithm is motivated by the results of psycholinguistic experiments."
E06-1041,Structuring Knowledge for Reference Generation: A Clustering Algorithm,2006,24,15,1,1,6764,albert gatt,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"This paper discusses two problems that arise in the Generation of Referring Expressions: (a) numeric-valued attributes, such as size or location; (b) perspective-taking in reference. Both problems, it is argued, can be resolved if some structure is imposed on the available knowledge prior to content determination. We describe a clustering algorithm which is sufficiently general to be applied to these diverse problems, discuss its application, and evaluate its performance."
