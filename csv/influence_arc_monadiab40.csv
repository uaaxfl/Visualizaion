2007.mtsummit-papers.20,E06-1032,0,0.0410528,"Missing"
2007.mtsummit-papers.20,H05-1085,0,0.0261799,"Missing"
2007.mtsummit-papers.20,P05-1071,1,0.813749,"r investigation, we define different diacritization schemes (DS) highlighting the different linguistic phenomena observed in natural text. We preprocess the Arabic source text in the context of phrase-based SMT using these different DSs. We also explore two different alignment modes, where a diacritization scheme is either used or not used for alignment purposes. 4.1 Diacritization Schemes We define six different diacritization schemes that are inspired by our observations of the relevant naturally occurring diacritics. For all of the schemes, we use the MADA system for Arabic disambiguation (Habash and Rambow, 2005; Habash and Rambow, 2007). The fully disambiguated form of a word is marked for all its morphological features and is also fully diacritized. For each scheme, we selectively delete diacritics that are irrelevant to that scheme given the scheme’s defined features of interest. The following are the defined diacritization schemes: • NONE: This is the baseline DS, in which all diacritics are absent, including the naturally occurring ones; • PASS: This is an inflectional DS which marks the verb passivization (u) only. It is only used on verbs marked by MADA as passive and where the (u) is explicit"
2007.mtsummit-papers.20,N07-2014,1,0.770482,"e different diacritization schemes (DS) highlighting the different linguistic phenomena observed in natural text. We preprocess the Arabic source text in the context of phrase-based SMT using these different DSs. We also explore two different alignment modes, where a diacritization scheme is either used or not used for alignment purposes. 4.1 Diacritization Schemes We define six different diacritization schemes that are inspired by our observations of the relevant naturally occurring diacritics. For all of the schemes, we use the MADA system for Arabic disambiguation (Habash and Rambow, 2005; Habash and Rambow, 2007). The fully disambiguated form of a word is marked for all its morphological features and is also fully diacritized. For each scheme, we selectively delete diacritics that are irrelevant to that scheme given the scheme’s defined features of interest. The following are the defined diacritization schemes: • NONE: This is the baseline DS, in which all diacritics are absent, including the naturally occurring ones; • PASS: This is an inflectional DS which marks the verb passivization (u) only. It is only used on verbs marked by MADA as passive and where the (u) is explicitly present;6 • C-M: This i"
2007.mtsummit-papers.20,N06-2013,1,0.830406,"Missing"
2007.mtsummit-papers.20,koen-2004-pharaoh,0,0.035562,"95 0.4195 0.4389 PASS 0.4507 0.4538 0.4202 0.4141 0.4416 0.4389 C-M 0.4354 0.4411 0.3977 0.4047 0.4217 0.4290 GEM 0.4341 0.4444 0.4128 0.4059 0.4310 0.4304 SUK 0.4482 0.4536 0.4173 0.4195 0.4410 0.4392 FULL 0.4293 0.4307 0.3898 0.3938 0.4177 0.4183 Table 2: BLEU score results obtained for all the diacritization schemes in both alignment strategies on 3 different test sets, MT03, MT04 and MT05 MT evaluation test set (M T 02). We report results on the 2003, 2004 and 2005 NIST MT evaluation test sets. 5.2 SMT System In all our experiments, we use an off-the-shelf phrasebased SMT system, Pharaoh (Koehn, 2004). Trigram language models are implemented using the SRILM toolkit (Stolcke, 2002). Decoding weights are optimized using Och’s algorithm (Och, 2003). The weights are optimized over the BLEU metric (Papineni et al., 2002), which is the evaluation metric we use.7 We use the BLEU metric in a mode insensitive to casing. For each of the diacritization schemes described above, we train two systems per each alignment strategy. The results are described in the next section. 5.3 Results and Discussion Table 2 illustrates the BLEU scores obtained for the different diacritization schemes (DS) and the diff"
2007.mtsummit-papers.20,N04-4015,0,0.0862022,"Missing"
2007.mtsummit-papers.20,J03-1002,0,0.00653548,"Missing"
2007.mtsummit-papers.20,P03-1021,0,0.00601817,"0.4304 SUK 0.4482 0.4536 0.4173 0.4195 0.4410 0.4392 FULL 0.4293 0.4307 0.3898 0.3938 0.4177 0.4183 Table 2: BLEU score results obtained for all the diacritization schemes in both alignment strategies on 3 different test sets, MT03, MT04 and MT05 MT evaluation test set (M T 02). We report results on the 2003, 2004 and 2005 NIST MT evaluation test sets. 5.2 SMT System In all our experiments, we use an off-the-shelf phrasebased SMT system, Pharaoh (Koehn, 2004). Trigram language models are implemented using the SRILM toolkit (Stolcke, 2002). Decoding weights are optimized using Och’s algorithm (Och, 2003). The weights are optimized over the BLEU metric (Papineni et al., 2002), which is the evaluation metric we use.7 We use the BLEU metric in a mode insensitive to casing. For each of the diacritization schemes described above, we train two systems per each alignment strategy. The results are described in the next section. 5.3 Results and Discussion Table 2 illustrates the BLEU scores obtained for the different diacritization schemes (DS) and the different alignment strategies A LIGN BASIC and A LIGN R EMAP. From Table 2, the worst results are those obtained with the FULL condition across the th"
2007.mtsummit-papers.20,P02-1040,0,0.0760371,"4293 0.4307 0.3898 0.3938 0.4177 0.4183 Table 2: BLEU score results obtained for all the diacritization schemes in both alignment strategies on 3 different test sets, MT03, MT04 and MT05 MT evaluation test set (M T 02). We report results on the 2003, 2004 and 2005 NIST MT evaluation test sets. 5.2 SMT System In all our experiments, we use an off-the-shelf phrasebased SMT system, Pharaoh (Koehn, 2004). Trigram language models are implemented using the SRILM toolkit (Stolcke, 2002). Decoding weights are optimized using Och’s algorithm (Och, 2003). The weights are optimized over the BLEU metric (Papineni et al., 2002), which is the evaluation metric we use.7 We use the BLEU metric in a mode insensitive to casing. For each of the diacritization schemes described above, we train two systems per each alignment strategy. The results are described in the next section. 5.3 Results and Discussion Table 2 illustrates the BLEU scores obtained for the different diacritization schemes (DS) and the different alignment strategies A LIGN BASIC and A LIGN R EMAP. From Table 2, the worst results are those obtained with the FULL condition across the three evaluation sets for both alignment strategies A LIGN BASIC and A LIG"
2007.mtsummit-papers.20,popovic-ney-2004-towards,0,0.0543886,"Missing"
2007.mtsummit-papers.20,W04-1612,0,0.186591,"Missing"
2007.mtsummit-papers.20,P06-1073,0,0.240084,"Missing"
2007.mtsummit-papers.39,W05-0909,0,0.0435449,"to characteristics of the translation input is growing more and more difficult. Problems are aggravated further in the case of speech translation (of e.g. broadcast news, talkshows, etc.), where the input to the translation module is provided by an automatic speech recognition (ASR) system whose performance also influences the quality of the final translation output. During machine translation (MT) system development, automatic evaluation criteria are commonly used to judge performance, such as the BLEU score (Papineni et al. 2002), the NIST score (Doddington 2002), or, more recently, METEOR (Banerjee & Lavie 2005). Although the use of fully automated evaluation criteria is helpful in accelerating the system development cycle, all of the above criteria have shown to be inferior to human judgments of translation performance. Moreover, they do not yield any insight into precisely which input characteristics caused particular translation errors, or which system components need to be improved in order to reach the desired performance level. Human analysis of machine translation errors, on the other hand, is costly and time-intensive and can typically not be performed on a regular basis in the course of syst"
2007.mtsummit-papers.39,E06-1005,0,0.0419776,"Missing"
2007.mtsummit-papers.39,niessen-etal-2000-evaluation,0,0.0362707,"tained from human annotations and are statistically related to measurements of the overall system performance. The various input document features are then ranked with respect to their impact on translation performance. Previous Work Most work on error analysis in statistical machine translation has made use of extensive human analysis, such as classifying unsatisfactory output into categories such as wrong word choice, missing content words, missing function words, etc. (see e.g. Koehn 2003, Och et al. 2003). Previous work on automatic or semi-automatic error analysis in SMT systems includes Niessen et al. (2000), Popovic et al. (2006a) and Popovic et al. (2006b). In Niessen et al. (2002), a graphical user interface was presented that automatically extracts various error measures for translation candidates and thus facilitates manual error analysis. In Popovic et al. (2006a) and Popovic et al. (2006b), errors in an English-Spanish statistical MT system were analyzed with respect to their syntactic and morphological origin. This was done by modifying the references and the machine translation output by eliminating morphological inflections or suspected reordered constituents, and by analyzing the resul"
2007.mtsummit-papers.39,P02-1040,0,0.0911609,"As a consequence, diagnosing problems in translation performance and relating them to characteristics of the translation input is growing more and more difficult. Problems are aggravated further in the case of speech translation (of e.g. broadcast news, talkshows, etc.), where the input to the translation module is provided by an automatic speech recognition (ASR) system whose performance also influences the quality of the final translation output. During machine translation (MT) system development, automatic evaluation criteria are commonly used to judge performance, such as the BLEU score (Papineni et al. 2002), the NIST score (Doddington 2002), or, more recently, METEOR (Banerjee & Lavie 2005). Although the use of fully automated evaluation criteria is helpful in accelerating the system development cycle, all of the above criteria have shown to be inferior to human judgments of translation performance. Moreover, they do not yield any insight into precisely which input characteristics caused particular translation errors, or which system components need to be improved in order to reach the desired performance level. Human analysis of machine translation errors, on the other hand, is costly and time-"
2007.mtsummit-papers.39,W06-3101,0,0.229094,"Missing"
2016.amta-researchers.15,W15-1005,1,0.774519,"nerate models to ﬁnd an optimal level of diacritization in some NLP applications. Although the result of this minimally-diacritized annotation has been highly affected by the annotators’ subjectivity and background, it has shown some promising results for future studies. The idea of integrating Word Sense Disambiguation (WSD) technologies into the SMT framework has been studied previously, tackling different aspects of the phenomenon and showing statistically signiﬁcant improvement integrating explicit WSD into the SMT system (Chan et al., 2007; Carpuat and Wu, 2007; Yang and Kirchhoff, 2012; Aminian et al., 2015). Mainly, WSD integration improves the ability of the system to choose the target translation if it has been incorporated efﬁciently. Carpuat and Wu (2007) show an improvement in Chinese-to-English SMT system in eight different automatic evaluation metrics when they integrate WSD in their translation system at decode time. They use the same parallel corpus used for training and the phrase translation table generated by the SMT tool to disambiguate senses of the words by using the aligned phrases in the target language. All of the previous work incorporates features that help disambiguate sense"
2016.amta-researchers.15,W15-3209,1,0.853292,"(2015) studies the impact of partial diacritics in improving Arabic speakers’ reading comprehension. Their study shows the effectiveness of having some level of diacritization between none and fully diacritized forms that help the readers disambiguate homographs that cannot be understood by the surrounding contexts. This shows the importance of having accurate automatic partial diacritization not only in improving different NLP applications but also to diacritize texts to help readers understand Arabic texts better. Having the goal of helping other researchers develop partial diacritization, Bouamor et al. (2015) has conducted a pilot study that minimally diacritize the dataset to reduce lexical ambiguity and help generate models to ﬁnd an optimal level of diacritization in some NLP applications. Although the result of this minimally-diacritized annotation has been highly affected by the annotators’ subjectivity and background, it has shown some promising results for future studies. The idea of integrating Word Sense Disambiguation (WSD) technologies into the SMT framework has been studied previously, tackling different aspects of the phenomenon and showing statistically signiﬁcant improvement integra"
2016.amta-researchers.15,D07-1007,0,0.0376727,"dataset to reduce lexical ambiguity and help generate models to ﬁnd an optimal level of diacritization in some NLP applications. Although the result of this minimally-diacritized annotation has been highly affected by the annotators’ subjectivity and background, it has shown some promising results for future studies. The idea of integrating Word Sense Disambiguation (WSD) technologies into the SMT framework has been studied previously, tackling different aspects of the phenomenon and showing statistically signiﬁcant improvement integrating explicit WSD into the SMT system (Chan et al., 2007; Carpuat and Wu, 2007; Yang and Kirchhoff, 2012; Aminian et al., 2015). Mainly, WSD integration improves the ability of the system to choose the target translation if it has been incorporated efﬁciently. Carpuat and Wu (2007) show an improvement in Chinese-to-English SMT system in eight different automatic evaluation metrics when they integrate WSD in their translation system at decode time. They use the same parallel corpus used for training and the phrase translation table generated by the SMT tool to disambiguate senses of the words by using the aligned phrases in the target language. All of the previous work i"
2016.amta-researchers.15,P07-1005,0,0.0435549,"ally diacritize the dataset to reduce lexical ambiguity and help generate models to ﬁnd an optimal level of diacritization in some NLP applications. Although the result of this minimally-diacritized annotation has been highly affected by the annotators’ subjectivity and background, it has shown some promising results for future studies. The idea of integrating Word Sense Disambiguation (WSD) technologies into the SMT framework has been studied previously, tackling different aspects of the phenomenon and showing statistically signiﬁcant improvement integrating explicit WSD into the SMT system (Chan et al., 2007; Carpuat and Wu, 2007; Yang and Kirchhoff, 2012; Aminian et al., 2015). Mainly, WSD integration improves the ability of the system to choose the target translation if it has been incorporated efﬁciently. Carpuat and Wu (2007) show an improvement in Chinese-to-English SMT system in eight different automatic evaluation metrics when they integrate WSD in their translation system at decode time. They use the same parallel corpus used for training and the phrase translation table generated by the SMT tool to disambiguate senses of the words by using the aligned phrases in the target language. All"
2016.amta-researchers.15,W98-1006,0,0.209979,"roperties of the Arabic language. 3- we test the robustness of our observations taking into consideration varying training size and cross genre evaluation. 2 Related Work Automatic Arabic diacritization has been addressed thoroughly in (Zitouni et al., 2006; Elshafei et al., 2006; Nelken and Shieber, 2005; Habash and Rambow, 2007; Pasha et al., 2014; Maamouri et al., 2008). Full diacritization indicates rendering the text with all the most prominent diacritics, namely (a, i, u, o, ∼).2 Initial efforts in automatic diacritization include rulebased approaches to add all diacritics in the texts (Debili and Achour, 1998; El-Imam, 2004); however, it is expensive to maintain these rules to be generalized for unseen instances. Most studies focused on full diacritic restoration. For Automatic Speech Recognition (ASR), (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005) perform full diacritization on MSA speech transcripts for language modeling. They show that developing ASR models on fully diacritized datasets improves performance signiﬁcantly. Supervised classiﬁers such as Hidden Markov Model (HMM) and Maximum Entropy (MaxEnt) have been employed for diacritization (Gal, 2002; Bebah et al., 2014; Zitouni"
2016.amta-researchers.15,2007.mtsummit-papers.20,1,0.94405,"tics which are short vowels (i, u, a), the syllable boundary marker, known as sukoon (o), indeﬁniteness marker, known as nunation (F, K, N), and the consonantal doubling marker (gemination) known as shadda (∼)1 . In this study, we aim to investigate what is the appropriate level and type of diacritic restoration that would have the biggest impact on natural language understanding as tested and evaluated via machine translation. Hence we experiment with various diacritization schemes based on lexical and/or syntactic information. This current work is a follow on to the pilot work presented in (Diab et al., 2007). However it is different in the following respects: 1- we explore automatically diacritized data; 2- we deﬁne more schemes that target both lexical and/or syntactic properties of the Arabic language. 3- we test the robustness of our observations taking into consideration varying training size and cross genre evaluation. 2 Related Work Automatic Arabic diacritization has been addressed thoroughly in (Zitouni et al., 2006; Elshafei et al., 2006; Nelken and Shieber, 2005; Habash and Rambow, 2007; Pasha et al., 2014; Maamouri et al., 2008). Full diacritization indicates rendering the text with al"
2016.amta-researchers.15,W02-0504,0,0.062047,"in the texts (Debili and Achour, 1998; El-Imam, 2004); however, it is expensive to maintain these rules to be generalized for unseen instances. Most studies focused on full diacritic restoration. For Automatic Speech Recognition (ASR), (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005) perform full diacritization on MSA speech transcripts for language modeling. They show that developing ASR models on fully diacritized datasets improves performance signiﬁcantly. Supervised classiﬁers such as Hidden Markov Model (HMM) and Maximum Entropy (MaxEnt) have been employed for diacritization (Gal, 2002; Bebah et al., 2014; Zitouni and Sarikaya, 2009; Zitouni et al., 2006). In a study conducted by Ananthakrishnan et al. (2005), the researchers use MaxEnt trained on MSA with lexical and n-gram features to improve ASR. Another study uses decision trees and stochastic language models to fully diacritize texts in order to render graphemes to synthesized speech (Cherif et al., 2015). The Buckwalter Arabic Morphological Analysis (BAMA) (Buckwalter, 2002) system has been used along with a single tagger or a language model to select amongst the diacritized analyses in context to render text fully di"
2016.amta-researchers.15,N07-2014,0,0.20086,"on lexical and/or syntactic information. This current work is a follow on to the pilot work presented in (Diab et al., 2007). However it is different in the following respects: 1- we explore automatically diacritized data; 2- we deﬁne more schemes that target both lexical and/or syntactic properties of the Arabic language. 3- we test the robustness of our observations taking into consideration varying training size and cross genre evaluation. 2 Related Work Automatic Arabic diacritization has been addressed thoroughly in (Zitouni et al., 2006; Elshafei et al., 2006; Nelken and Shieber, 2005; Habash and Rambow, 2007; Pasha et al., 2014; Maamouri et al., 2008). Full diacritization indicates rendering the text with all the most prominent diacritics, namely (a, i, u, o, ∼).2 Initial efforts in automatic diacritization include rulebased approaches to add all diacritics in the texts (Debili and Achour, 1998; El-Imam, 2004); however, it is expensive to maintain these rules to be generalized for unseen instances. Most studies focused on full diacritic restoration. For Automatic Speech Recognition (ASR), (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005) perform full diacritization on MSA speech transcr"
2016.amta-researchers.15,P07-2045,0,0.00859051,"/ 3.41 28.11 / 28.94 / 28.79 4.40 / 4.86 / 3.77 1.33 / 1.51 / 1.19 1.41 /1.61 / 1.35 2.07 / 2.43 / 1.91 Table 1: This table shows the number of types for each diacritic scheme for test and train datasets. Type Increase columns indicate the percentage of increase in the number of types compared to NONE. 5 Catalog 6 Catalog Numbers: LDC2010T23 (MT09), LDC2010T17 (MT06), LDC2010T14 (MT05). Number: LDC2010T21. 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S 4.2 SMT System We train standard phrase-based SMT system using Moses toolkit version 2.1.1 (Koehn et al., 2007). The parallel corpus is word aligned using GIZA++ (Och and Ney, 2003) with a maximum sentence length of 250 words. The phrase tables contains up to 8-words phrases. We use SRILM (Stolcke et al., 2002) to build 5-gram language model with modiﬁed Kneser-Ney smoothing (James, 2000). Our language modeling data consists of the English Gigaword 5th edition LDC2011T07 and the English side of the training datasets. The best weight parameters are tuned using the Minimum Error Rate Training (MERT) algorithm (Och, 2003) to maximize BLEU score (Papineni et al., 2002). To account for optimization algorith"
2016.amta-researchers.15,maamouri-etal-2008-enhancing,0,0.138472,"s current work is a follow on to the pilot work presented in (Diab et al., 2007). However it is different in the following respects: 1- we explore automatically diacritized data; 2- we deﬁne more schemes that target both lexical and/or syntactic properties of the Arabic language. 3- we test the robustness of our observations taking into consideration varying training size and cross genre evaluation. 2 Related Work Automatic Arabic diacritization has been addressed thoroughly in (Zitouni et al., 2006; Elshafei et al., 2006; Nelken and Shieber, 2005; Habash and Rambow, 2007; Pasha et al., 2014; Maamouri et al., 2008). Full diacritization indicates rendering the text with all the most prominent diacritics, namely (a, i, u, o, ∼).2 Initial efforts in automatic diacritization include rulebased approaches to add all diacritics in the texts (Debili and Achour, 1998; El-Imam, 2004); however, it is expensive to maintain these rules to be generalized for unseen instances. Most studies focused on full diacritic restoration. For Automatic Speech Recognition (ASR), (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005) perform full diacritization on MSA speech transcripts for language modeling. They show that d"
2016.amta-researchers.15,W10-1402,0,0.0263474,"n a study conducted by Ananthakrishnan et al. (2005), the researchers use MaxEnt trained on MSA with lexical and n-gram features to improve ASR. Another study uses decision trees and stochastic language models to fully diacritize texts in order to render graphemes to synthesized speech (Cherif et al., 2015). The Buckwalter Arabic Morphological Analysis (BAMA) (Buckwalter, 2002) system has been used along with a single tagger or a language model to select amongst the diacritized analyses in context to render text fully diacritized (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005). In Marton et al. (2010), the authors show that some inﬂectional and lexical related morphological features improve the performance of syntactic parsing in Arabic. Although Marton et al. (2010) have not used diacritics directly in their work, they use the same essential information that is used to diacritize Arabic texts. Diab et al. (2007) not only investigate the impact of full diacritization on Statistical Machine Translation (SMT) but also introduce the notion of partial diacritization. They also show that several schemes have a small positive effect albeit not signiﬁcant on SMT performance over none and full dia"
2016.amta-researchers.15,W05-0711,0,0.469532,"acritization schemes based on lexical and/or syntactic information. This current work is a follow on to the pilot work presented in (Diab et al., 2007). However it is different in the following respects: 1- we explore automatically diacritized data; 2- we deﬁne more schemes that target both lexical and/or syntactic properties of the Arabic language. 3- we test the robustness of our observations taking into consideration varying training size and cross genre evaluation. 2 Related Work Automatic Arabic diacritization has been addressed thoroughly in (Zitouni et al., 2006; Elshafei et al., 2006; Nelken and Shieber, 2005; Habash and Rambow, 2007; Pasha et al., 2014; Maamouri et al., 2008). Full diacritization indicates rendering the text with all the most prominent diacritics, namely (a, i, u, o, ∼).2 Initial efforts in automatic diacritization include rulebased approaches to add all diacritics in the texts (Debili and Achour, 1998; El-Imam, 2004); however, it is expensive to maintain these rules to be generalized for unseen instances. Most studies focused on full diacritic restoration. For Automatic Speech Recognition (ASR), (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005) perform full diacritizat"
2016.amta-researchers.15,P03-1021,0,0.0418056,"e train standard phrase-based SMT system using Moses toolkit version 2.1.1 (Koehn et al., 2007). The parallel corpus is word aligned using GIZA++ (Och and Ney, 2003) with a maximum sentence length of 250 words. The phrase tables contains up to 8-words phrases. We use SRILM (Stolcke et al., 2002) to build 5-gram language model with modiﬁed Kneser-Ney smoothing (James, 2000). Our language modeling data consists of the English Gigaword 5th edition LDC2011T07 and the English side of the training datasets. The best weight parameters are tuned using the Minimum Error Rate Training (MERT) algorithm (Och, 2003) to maximize BLEU score (Papineni et al., 2002). To account for optimization algorithm instability, we replicate optimization three times per experiment. We use bootstrap resampling and approximate randomization (Clark et al., 2011) to statistically test for signiﬁcant differences using two evaluation metrics: BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). As BLEU reﬂects a bias toward ﬂuency in the target language and TER identiﬁes the least post editing, they capture complementary aspects of the translation. We consider NONE and FULL as the baselines which show the the impact of"
2016.amta-researchers.15,J03-1002,0,0.00555576,"1 /1.61 / 1.35 2.07 / 2.43 / 1.91 Table 1: This table shows the number of types for each diacritic scheme for test and train datasets. Type Increase columns indicate the percentage of increase in the number of types compared to NONE. 5 Catalog 6 Catalog Numbers: LDC2010T23 (MT09), LDC2010T17 (MT06), LDC2010T14 (MT05). Number: LDC2010T21. 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S 4.2 SMT System We train standard phrase-based SMT system using Moses toolkit version 2.1.1 (Koehn et al., 2007). The parallel corpus is word aligned using GIZA++ (Och and Ney, 2003) with a maximum sentence length of 250 words. The phrase tables contains up to 8-words phrases. We use SRILM (Stolcke et al., 2002) to build 5-gram language model with modiﬁed Kneser-Ney smoothing (James, 2000). Our language modeling data consists of the English Gigaword 5th edition LDC2011T07 and the English side of the training datasets. The best weight parameters are tuned using the Minimum Error Rate Training (MERT) algorithm (Och, 2003) to maximize BLEU score (Papineni et al., 2002). To account for optimization algorithm instability, we replicate optimization three times per experiment. W"
2016.amta-researchers.15,P02-1040,0,0.0959824,"ystem using Moses toolkit version 2.1.1 (Koehn et al., 2007). The parallel corpus is word aligned using GIZA++ (Och and Ney, 2003) with a maximum sentence length of 250 words. The phrase tables contains up to 8-words phrases. We use SRILM (Stolcke et al., 2002) to build 5-gram language model with modiﬁed Kneser-Ney smoothing (James, 2000). Our language modeling data consists of the English Gigaword 5th edition LDC2011T07 and the English side of the training datasets. The best weight parameters are tuned using the Minimum Error Rate Training (MERT) algorithm (Och, 2003) to maximize BLEU score (Papineni et al., 2002). To account for optimization algorithm instability, we replicate optimization three times per experiment. We use bootstrap resampling and approximate randomization (Clark et al., 2011) to statistically test for signiﬁcant differences using two evaluation metrics: BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). As BLEU reﬂects a bias toward ﬂuency in the target language and TER identiﬁes the least post editing, they capture complementary aspects of the translation. We consider NONE and FULL as the baselines which show the the impact of under- and over-speciﬁcation of the diacritics"
2016.amta-researchers.15,pasha-etal-2014-madamira,1,0.893064,"tic information. This current work is a follow on to the pilot work presented in (Diab et al., 2007). However it is different in the following respects: 1- we explore automatically diacritized data; 2- we deﬁne more schemes that target both lexical and/or syntactic properties of the Arabic language. 3- we test the robustness of our observations taking into consideration varying training size and cross genre evaluation. 2 Related Work Automatic Arabic diacritization has been addressed thoroughly in (Zitouni et al., 2006; Elshafei et al., 2006; Nelken and Shieber, 2005; Habash and Rambow, 2007; Pasha et al., 2014; Maamouri et al., 2008). Full diacritization indicates rendering the text with all the most prominent diacritics, namely (a, i, u, o, ∼).2 Initial efforts in automatic diacritization include rulebased approaches to add all diacritics in the texts (Debili and Achour, 1998; El-Imam, 2004); however, it is expensive to maintain these rules to be generalized for unseen instances. Most studies focused on full diacritic restoration. For Automatic Speech Recognition (ASR), (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005) perform full diacritization on MSA speech transcripts for language mo"
2016.amta-researchers.15,2006.amta-papers.25,0,0.0120992,"g (James, 2000). Our language modeling data consists of the English Gigaword 5th edition LDC2011T07 and the English side of the training datasets. The best weight parameters are tuned using the Minimum Error Rate Training (MERT) algorithm (Och, 2003) to maximize BLEU score (Papineni et al., 2002). To account for optimization algorithm instability, we replicate optimization three times per experiment. We use bootstrap resampling and approximate randomization (Clark et al., 2011) to statistically test for signiﬁcant differences using two evaluation metrics: BLEU (Papineni et al., 2002) and TER (Snover et al., 2006). As BLEU reﬂects a bias toward ﬂuency in the target language and TER identiﬁes the least post editing, they capture complementary aspects of the translation. We consider NONE and FULL as the baselines which show the the impact of under- and over-speciﬁcation of the diacritics. As discussed before, NONE accounts for the dataset without any diacritics added (consonants only) which is the default setting for most current SMT systems whereas FULL shows the impact of all automatically generated lexical and syntactic diacritic marks. 5 Results & Discussion Diacritic Pattern Dataset NONE FULL SUK GE"
2016.amta-researchers.15,W04-1612,0,0.596955,"horoughly in (Zitouni et al., 2006; Elshafei et al., 2006; Nelken and Shieber, 2005; Habash and Rambow, 2007; Pasha et al., 2014; Maamouri et al., 2008). Full diacritization indicates rendering the text with all the most prominent diacritics, namely (a, i, u, o, ∼).2 Initial efforts in automatic diacritization include rulebased approaches to add all diacritics in the texts (Debili and Achour, 1998; El-Imam, 2004); however, it is expensive to maintain these rules to be generalized for unseen instances. Most studies focused on full diacritic restoration. For Automatic Speech Recognition (ASR), (Vergyri and Kirchhoff, 2004; Ananthakrishnan et al., 2005) perform full diacritization on MSA speech transcripts for language modeling. They show that developing ASR models on fully diacritized datasets improves performance signiﬁcantly. Supervised classiﬁers such as Hidden Markov Model (HMM) and Maximum Entropy (MaxEnt) have been employed for diacritization (Gal, 2002; Bebah et al., 2014; Zitouni and Sarikaya, 2009; Zitouni et al., 2006). In a study conducted by Ananthakrishnan et al. (2005), the researchers use MaxEnt trained on MSA with lexical and n-gram features to improve ASR. Another study uses decision trees and"
2016.amta-researchers.15,2012.amta-papers.29,0,0.0206824,"ical ambiguity and help generate models to ﬁnd an optimal level of diacritization in some NLP applications. Although the result of this minimally-diacritized annotation has been highly affected by the annotators’ subjectivity and background, it has shown some promising results for future studies. The idea of integrating Word Sense Disambiguation (WSD) technologies into the SMT framework has been studied previously, tackling different aspects of the phenomenon and showing statistically signiﬁcant improvement integrating explicit WSD into the SMT system (Chan et al., 2007; Carpuat and Wu, 2007; Yang and Kirchhoff, 2012; Aminian et al., 2015). Mainly, WSD integration improves the ability of the system to choose the target translation if it has been incorporated efﬁciently. Carpuat and Wu (2007) show an improvement in Chinese-to-English SMT system in eight different automatic evaluation metrics when they integrate WSD in their translation system at decode time. They use the same parallel corpus used for training and the phrase translation table generated by the SMT tool to disambiguate senses of the words by using the aligned phrases in the target language. All of the previous work incorporates features that"
2016.amta-researchers.15,P06-1073,0,0.253034,"nslation. Hence we experiment with various diacritization schemes based on lexical and/or syntactic information. This current work is a follow on to the pilot work presented in (Diab et al., 2007). However it is different in the following respects: 1- we explore automatically diacritized data; 2- we deﬁne more schemes that target both lexical and/or syntactic properties of the Arabic language. 3- we test the robustness of our observations taking into consideration varying training size and cross genre evaluation. 2 Related Work Automatic Arabic diacritization has been addressed thoroughly in (Zitouni et al., 2006; Elshafei et al., 2006; Nelken and Shieber, 2005; Habash and Rambow, 2007; Pasha et al., 2014; Maamouri et al., 2008). Full diacritization indicates rendering the text with all the most prominent diacritics, namely (a, i, u, o, ∼).2 Initial efforts in automatic diacritization include rulebased approaches to add all diacritics in the texts (Debili and Achour, 1998; El-Imam, 2004); however, it is expensive to maintain these rules to be generalized for unseen instances. Most studies focused on full diacritic restoration. For Automatic Speech Recognition (ASR), (Vergyri and Kirchhoff, 2004; Anant"
2020.acl-main.454,P18-1060,0,0.0807514,"Missing"
2020.acl-main.454,P17-1152,0,0.0325421,"e maximum score since it has better performance. Model-based metrics. In addition to QA, recent work has used relation extraction and textual entailment models for faithfulness evaluation (Falke et al., 2019a; Goodrich et al., 2019). For the relation extraction metric (RE), we compute the precision for the relation triplets extracted from the summary sentence and the source document using an off-the-shelf model (Angeli et al., 2015) from Stanford Open IE. For the textual entailment metric (ENT), we measure whether the summary sentence is entailed by the source using the pretrained ESIM model (Chen et al., 2017) from AllenNLP (Gardner et al., 2018). 4.2 Results Metric Comparison. We first compute scores for each metric on document and output sentence pairs on both CNN/DM and XSum datasets (748 and 286 pairs respectively). We then compute Pearson and Spearman correlation coefficients between scores given by each metric and humanannotated scores. Table 7 includes correlation coefficients for the examples from CNN/DM and XSum, respectively. We observe that for both CNN/DM and XSum, the score of QA-based evaluation has a higher correlation with faithfulness than other metrics. Although word-overlap based"
2020.acl-main.454,P18-1063,0,0.0526725,"e-to-sequence models. They differ in how summarization-specific operations such as copying/extraction are instantiated. We consider 5 prominent models and sumSystems Extractor Encoder Decoder PGC − sentences words − − LSTM LSTM LSTM CNN+topic BERT-based LSTM+copy LSTM+copy LSTM+copy CNN Transformer FAST RL B OTTOM U P T CONV B ERT S UM Table 2: Comparison of summarization systems in terms of model architecture. marize their characteristics in Table 2.2 Details of each model can be found in Appendix B. PGC (See et al., 2017) uses the copy mechanism during decoding to allow extraction. FAST RL (Chen and Bansal, 2018) and B OTTOM U P (Gehrmann et al., 2018) decouple extraction and abstractive generation by learning to select sentences and words respectively in the first step; this model has been shown to generate more abstractive summaries compared to PGC. T CONV (Narayan et al., 2018) is initially designed for XSum, thus it does not include any explicit copying/extraction components and focuses on long text representation using convolutional neural networks. B ERT S UM (Liu and Lapata, 2019) consists of a BERT-based encoder and a 6-layer Transformer decoder. It incorporates extraction implicitly by first"
2020.acl-main.454,D18-1443,0,0.347079,"stion answering (QA) based metric for faithfulness, FEQA,1 which leverages recent advances in reading comprehension. Given questionanswer pairs generated from the summary, a QA model extracts answers from the document; non-matched answers indicate unfaithful information in the summary. Among metrics based on word overlap, embedding similarity, and learned language understanding models, our QA-based metric has significantly higher correlation with human faithfulness scores, especially on highly abstractive summaries. 1 Table 1: An example of unfaithful output (highlighted in red); generated by Gehrmann et al. (2018). Introduction Abstractive summarization models must aggregate salient content from the source document(s) and remain faithful, i.e. being factually consistent with information in the source documents. Neural abstractive models are effective at identifying salient content and producing fluent summaries (See et al., 2017; Chen and Bansal, 2018; Gehrmann et al., 2018). However, the generated summary may not always contain faithful information, which is vital for real-world applications. ∗ Mona Diab The George Washington University mtdiab@gwu.edu Most of the work is done while the authors were at"
2020.acl-main.454,N18-1065,0,0.0753104,"Missing"
2020.acl-main.454,N19-1169,0,0.0345989,"ur work is the first to apply automated question generation. While we focus on faithfulness, our QAbased metric is applicable to semantic comparison between any two pieces of text. Automated evaluation for NLG. Automated NLG evaluation is challenging as it often requires deep understanding of the text. Although metrics based on word overlap with the reference text are commonly used, it is widely known that they do not correlate well with human judgments (Novikova et al., 2017; Liu et al., 2016). Recently, more work has focused on model-based evaluation using discriminators (Lowe et al., 2017; Hashimoto et al., 2019), entailment models (Falke et al., 2019a), information extraction (Wiseman et al., 2017; Goodrich et al., 2019), and question answering (Chen et al., 2018; Eyal et al., 2019). 6 Conclusion We investigate the faithfulness problem in neural abstractive summarization and propose a QAbased metric for evaluating summary faithfulness. We show that current models suffer from an inherent trade-off between abstractiveness and faithfulness. They are good at copying important source content, but tend to concatenate unrelated spans and hallucinate details when generating more abstractive sentences. A new"
2020.acl-main.454,P14-5010,0,0.0042642,"2 shows the workflow of FEQA. Question generation. Prior work (Eyal et al., 2019; Scialom et al., 2019) uses cloze tests as questions by masking entities. To go beyond cloze-style QA and leverage more recent extractive (Rajpurkar et al., 2016) or even generative (Alec et al., 2019) QA models, we generate natural language questions from the summary sentence automatically. Specifically, we mask important text spans in a sentence, including noun phrases extracted by a constituency parser (Kitaev and Klein, 2018) and named entities extracted by the Stanford CoreNLP NER model (Finkel et al., 2005; Manning et al., 2014). We consider each span as the gold answer and generate its corresponding question by fine-tuning a pretrained BART language model (Lewis et al., 2019). To train the question generator, we adapt the QA2D dataset Demszky et al. (2018). The input is a declarative sentence with masked answers and the output is a question. A training example might look like: 5060 Input: Output: Sally was born in <m> 1958 </m> When was Sally born ? Since the transformation from declarative sentences to questions is almost rule-based without much paraphrasing, we expect the model to generalize to various domains. An"
2020.acl-main.454,P19-1334,0,0.0285077,"Missing"
2020.acl-main.454,D19-1051,0,0.177222,"Missing"
2020.acl-main.454,D19-5413,0,0.331326,"is formed. A more abstractive summary sentence aggregates content over a larger chunk of source text; consequently it must copy fewer words to maintain brevity. Therefore, we define the following abstractiveness types based on the amount of copying, e.g. copying a source sentence, one or more partial fragments from the source sentence, and individual words. The Abstractiveness-Faithfulness Tradeoff While extractive summarizers are largely faithful (since they copy sentences from the source document), current abstractive models struggle to produce faithful summaries without copying. Similar to Lebanoff et al. (2019), we observe that factual errors occur more frequently as models generate more abstractive summary sentences, i.e. less overlap with the source document. In this section, we analyze generated summaries along two dimensions: abstractiveness and faithfulness. Specifically, we aim to answer the following questions: (1) How to quantify abstractiveness of a summary? (2) Is abstractiveness encouraged more by the data or the model? (3) How does being abstractive affect faithfulness? 5056 1. Sentence extraction: the summary sentence is exactly the same as one of the source sentences. 2. Span extractio"
2020.acl-main.454,K16-1028,0,0.181054,"Missing"
2020.acl-main.454,2020.acl-main.703,0,0.146591,"Missing"
2020.acl-main.454,N16-1014,0,0.0742441,"Missing"
2020.acl-main.454,D16-1230,0,0.0366419,"ol, including rule-based approaches (Chen et al., 2018) and cloze-test QA (Eyal et al., 2019; Scialom et al., 2019). Our work is the first to apply automated question generation. While we focus on faithfulness, our QAbased metric is applicable to semantic comparison between any two pieces of text. Automated evaluation for NLG. Automated NLG evaluation is challenging as it often requires deep understanding of the text. Although metrics based on word overlap with the reference text are commonly used, it is widely known that they do not correlate well with human judgments (Novikova et al., 2017; Liu et al., 2016). Recently, more work has focused on model-based evaluation using discriminators (Lowe et al., 2017; Hashimoto et al., 2019), entailment models (Falke et al., 2019a), information extraction (Wiseman et al., 2017; Goodrich et al., 2019), and question answering (Chen et al., 2018; Eyal et al., 2019). 6 Conclusion We investigate the faithfulness problem in neural abstractive summarization and propose a QAbased metric for evaluating summary faithfulness. We show that current models suffer from an inherent trade-off between abstractiveness and faithfulness. They are good at copying important source"
2020.acl-main.454,D19-1387,0,0.153669,"found in Appendix B. PGC (See et al., 2017) uses the copy mechanism during decoding to allow extraction. FAST RL (Chen and Bansal, 2018) and B OTTOM U P (Gehrmann et al., 2018) decouple extraction and abstractive generation by learning to select sentences and words respectively in the first step; this model has been shown to generate more abstractive summaries compared to PGC. T CONV (Narayan et al., 2018) is initially designed for XSum, thus it does not include any explicit copying/extraction components and focuses on long text representation using convolutional neural networks. B ERT S UM (Liu and Lapata, 2019) consists of a BERT-based encoder and a 6-layer Transformer decoder. It incorporates extraction implicitly by first fine-tuning the encoder on the extractive summarization task.3 Results. Our goal is to understand the level of abstractiveness of summaries generated by different models, and the influence on abstractiveness from the training data. Therefore, we analyzed summaries generated by the above models on CNN/DM and XSum. We computed the metrics described in Section 2.1 for both the generated summaries and the reference summaries on the test sets. The results are shown in Table 3. First,"
2020.acl-main.454,D18-1206,0,0.339468,"t do not appear in the source document as another metric for abstractiveness. 2.2 Is abstractiveness from the model or the data? Equipped with the metrics for abstractiveness above, we want to further understand how abstractive the generated summaries are, and whether the amount of abstractiveness is a result of the training data or the model. Therefore, we compute abstractiveness scores for both the reference summaries and summaries generated from a diverse set of models on two datasets. Datasets. We use the CNN/DailyMail (Hermann et al., 2015; Nallapati et al., 2016b) (CNN/DM) and the XSum (Narayan et al., 2018) datasets, which are both used for single-document news summarization tasks. CNN/DM consists of articles from the CNN and Daily Mail websites, where the summaries comprise highlights in bullet points. XSum consists of BBC articles, where the summaries comprise a single-sentence summary that is written as the opening introductory sentence for the article. XSum was released in particular to promote research on highly abstractive summarization systems. Appendix A provides statistics on CNN/DM and XSum datasets: they contain around 288k and 204k training examples, respectively; CNN/DM includes lon"
2020.acl-main.454,D17-1238,0,0.0994523,"Missing"
2020.acl-main.454,P18-2124,0,0.0283068,"ns is almost rule-based without much paraphrasing, we expect the model to generalize to various domains. Answer verification. Given the QA pairs generated from a summary sentence, we run off-theshelf QA models to get answers to these questions from the source document. We then measure the average F1 score against the “gold” answers from the summary, which is our faithfulness score for the given sentence. This step does not have any constraint on the QA model. We experiment with the pretrained BERT-base model (Devlin et al., 2019) fine-tuned on SQuAD-1.1 (Rajpurkar et al., 2016) and SQuAD-2.0 (Rajpurkar et al., 2018). Note that in the case of SQuAD-2.0, the model may be able to hypothesize that a question is unanswerable. This case is equivalent to getting an answer incorrect (i.e. unfaithful). 4 Experiments We aim to understand to what extent the proposed QA-based metric and existing metrics capture faithfulness of a summary. Given pairs of documents and summary sentences without reference summaries, we measure correlations between human-annotated faithfulness scores (Section 2.3) and scores computed using each metric described below. 4.1 Automated Metrics for Faithfulness Word overlap-based metrics. A s"
2020.acl-main.454,D16-1264,0,0.393676,"verify this information against the source, we use a QA model to predict answers from the document. The questions and the QA model thus extract comparable information from two pieces of text. More matched answers from the document implies a more faithful summary since the information addressing these questions are consistent between the summary and the source document. Figure 2 shows the workflow of FEQA. Question generation. Prior work (Eyal et al., 2019; Scialom et al., 2019) uses cloze tests as questions by masking entities. To go beyond cloze-style QA and leverage more recent extractive (Rajpurkar et al., 2016) or even generative (Alec et al., 2019) QA models, we generate natural language questions from the summary sentence automatically. Specifically, we mask important text spans in a sentence, including noun phrases extracted by a constituency parser (Kitaev and Klein, 2018) and named entities extracted by the Stanford CoreNLP NER model (Finkel et al., 2005; Manning et al., 2014). We consider each span as the gold answer and generate its corresponding question by fine-tuning a pretrained BART language model (Lewis et al., 2019). To train the question generator, we adapt the QA2D dataset Demszky et"
2020.acl-main.454,D19-1320,0,0.496206,"metrics such as ROUGE, BERTScore (Zhang et al., 2019a), and learned entailment models. We find that their correlations with human scores of faithfulness drop significantly on highly abstractive summaries, where deeper text understanding beyond surface similarity is needed. Recently, question answering (QA) based automatic metrics have been proposed for evaluating 5055 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5055–5070 c July 5 - 10, 2020. 2020 Association for Computational Linguistics content selection in summarization (Eyal et al., 2019; Scialom et al., 2019; Chen et al., 2018). Specifically, cloze-style QA is used to evaluate whether important information in the source is recovered from the summary. Inspired by prior work, we use automatically generated QA pairs to represent information in the summary and validate it against the source. Concretely, we generate a set of “groundtruth” QA pairs from the summary, using a learned model that converts a declarative sentence and an answer span to a question (Section 3). Then, off-the-shelf reading comprehension models are evaluated on this set by extracting answer spans from the source documents. High a"
2020.acl-main.454,P17-1099,0,0.25352,"es on average. Models. Most neural abstractive summarization models are based on sequence-to-sequence models. They differ in how summarization-specific operations such as copying/extraction are instantiated. We consider 5 prominent models and sumSystems Extractor Encoder Decoder PGC − sentences words − − LSTM LSTM LSTM CNN+topic BERT-based LSTM+copy LSTM+copy LSTM+copy CNN Transformer FAST RL B OTTOM U P T CONV B ERT S UM Table 2: Comparison of summarization systems in terms of model architecture. marize their characteristics in Table 2.2 Details of each model can be found in Appendix B. PGC (See et al., 2017) uses the copy mechanism during decoding to allow extraction. FAST RL (Chen and Bansal, 2018) and B OTTOM U P (Gehrmann et al., 2018) decouple extraction and abstractive generation by learning to select sentences and words respectively in the first step; this model has been shown to generate more abstractive summaries compared to PGC. T CONV (Narayan et al., 2018) is initially designed for XSum, thus it does not include any explicit copying/extraction components and focuses on long text representation using convolutional neural networks. B ERT S UM (Liu and Lapata, 2019) consists of a BERT-bas"
2020.acl-main.454,N15-1020,0,0.0701775,"Missing"
2020.acl-main.454,P16-1008,0,0.0703019,"Missing"
2020.acl-main.454,D17-1239,0,0.0240451,"s, our QAbased metric is applicable to semantic comparison between any two pieces of text. Automated evaluation for NLG. Automated NLG evaluation is challenging as it often requires deep understanding of the text. Although metrics based on word overlap with the reference text are commonly used, it is widely known that they do not correlate well with human judgments (Novikova et al., 2017; Liu et al., 2016). Recently, more work has focused on model-based evaluation using discriminators (Lowe et al., 2017; Hashimoto et al., 2019), entailment models (Falke et al., 2019a), information extraction (Wiseman et al., 2017; Goodrich et al., 2019), and question answering (Chen et al., 2018; Eyal et al., 2019). 6 Conclusion We investigate the faithfulness problem in neural abstractive summarization and propose a QAbased metric for evaluating summary faithfulness. We show that current models suffer from an inherent trade-off between abstractiveness and faithfulness. They are good at copying important source content, but tend to concatenate unrelated spans and hallucinate details when generating more abstractive sentences. A new inductive bias or additional supervision is needed for learning reliable models. While"
2020.acl-main.454,D18-1089,0,0.0673438,"Missing"
2020.acl-main.454,J10-3005,0,\N,Missing
2020.acl-main.454,E99-1011,0,\N,Missing
2020.acl-main.454,P15-1034,0,\N,Missing
2020.acl-main.454,P18-5002,0,\N,Missing
2020.acl-main.454,N19-1395,0,\N,Missing
2020.acl-main.454,P19-1213,0,\N,Missing
2020.acl-main.454,N19-1423,0,\N,Missing
2020.acl-main.454,P05-1045,0,\N,Missing
2020.acl-main.732,D19-1151,1,0.666562,"“flag” or “knowledge”, but the meaning as well as pronunciation is specified when the word is diacritized ( ÕÎ « Ealamu means “flag” while ÕÎ « Eilomo means “knowledge”). As an illustrative example in English, if we omit the vowels in the word pn, the word can be read as pan, pin, pun, and pen, each of these variants have different pronunciations and meanings if it composes a valid word in the language. The state-of-the-art diacritic restoration models reached a decent performance over the years using recurrent or convolutional neural networks in terms of accuracy (Zalmout and Habash, 2017; Alqahtani et al., 2019; Orife, 2018) and/or efficiency (Alqahtani et al., 2019; Orife, 2018); yet, there is still room for further improvements. Most of these models are built on character level information which help generalize the model to unseen data, but presumably lose some useful information at the word level. Since word level resources are insufficient to be relied upon for training diacritic restoration models, we integrate additional linguistic information that considers word morphology as well as word relationships within a sentence to partially compensate for this loss. 2 We use Buckwalter Transliteratio"
2020.acl-main.732,Q17-1010,0,0.0219414,"what is described in Watson et al. (2018)’s study. This helps distinguishing the individual characters based on the surrounding context, implicitly capturing additional semantic and syntactic information. 8 We also evaluated the use of a feedforward layer and unidirectional Long Short Term Memory (LSTM) but a BiLSTM layer yielded better results. 8240 Figure 2: The diacritic restoration joint model. All Char Embed entities refer to the same randomly initialized character embedding learned during the training process. Pretrained embeddings refer to fixed word embeddings obtained from fastText (Bojanowski et al., 2017). (i) shows the input representation for CharToWord and WordToChar embedding which is the same as in Figure 1. (ii) represents the diacritic restoration joint model; output labels from each task are concatenated with WordToChar embedding and optionally with segmentation hidden. 3.2 The Joint Model For all architectures, the main component is BiLSTM (Hochreiter and Schmidhuber, 1997; Schuster and Paliwal, 1997), which preserves the temporal order of the sequence and has been shown to provide the state-of-the-art performance in terms of accuracy (Zalmout and Habash, 2017; Alqahtani et al., 2019)"
2020.acl-main.732,W17-1302,0,0.0116856,") or neural based approaches for different languages that include diacritics such as Arabic, Vietnamese, and Yoruba. Neural based approaches yield stateof-the-art performance for diacritic restoration by using Bidirectional LSTM or temporal convolutional networks (Zalmout and Habash, 2017; Orife, 2018; Alqahtani et al., 2019; Alqahtani and Diab, 2019a). Arabic syntactic diacritization has been consistently reported to be difficult, degrading the performance of full diacritic restoration (Zitouni et al., 2006; Habash et al., 2007; Said et al., 2013; Shaalan et al., 2009; Shahrour et al., 2015; Darwish et al., 2017). To improve the performance of syntactic diacritization or full diacritic restoration in general, previous studies followed different approaches. Some studies separate lexical from syntactic diacritization (Shaalan et al., 2009; Darwish et al., 2017). Other studies consider additional linguistic features such as POS tags and word segmentation (i.e. tokens or morphemes) (Ananthakrishnan et al., 2005; Zitouni et al., 2006; Zitouni and Sarikaya, 2009; Shaalan et al., 2009). Hifny (2018) addresses syntactic diacritization by building BiLSTM model in which its input embeddings are augmented with m"
2020.acl-main.732,2007.mtsummit-papers.20,1,0.741473,"Missing"
2020.acl-main.732,N04-4038,1,0.666649,"Missing"
2020.acl-main.732,P18-1128,0,0.0124889,"of incorrectly diacritized words and characters, respectively. In order to approximate errors in the syntactic diacritics, we use Last Diacritic Error Rate (LER), the percentage of words that have incorrect diacritics in the last positions of words. To evaluate the models’ ability to generalize beyond observed data, we compute WER on OOV (out-of-vocabulary) words.10 Significance testing: We ran each experiment three times and reported the mean score.11 We used the t-test with p = 0.05 to evaluate whether the difference between models’ performance and the diacritic restoration is significant (Dror et al., 2018). 5 Results and Analysis Table 2 shows the performance of joint diacritic restoration models when different tasks are considered. When we consider WordToChar as input to the diacritic restoration model, we observe statistically significant improvements for all evaluation metrics. This is justified by the ability of word embeddings to capture syntactic and semantic information at the sentence level. The same character is disambiguated in terms of the surrounding context 10 Words that appear in the training dataset but do not appear in the test dataset. 11 Higher number of experiments provide mo"
2020.acl-main.732,D07-1116,0,0.0610343,"m Entropy and Support Vector Machine) (Zitouni and Sarikaya, 2009; Pasha et al., 2014) or neural based approaches for different languages that include diacritics such as Arabic, Vietnamese, and Yoruba. Neural based approaches yield stateof-the-art performance for diacritic restoration by using Bidirectional LSTM or temporal convolutional networks (Zalmout and Habash, 2017; Orife, 2018; Alqahtani et al., 2019; Alqahtani and Diab, 2019a). Arabic syntactic diacritization has been consistently reported to be difficult, degrading the performance of full diacritic restoration (Zitouni et al., 2006; Habash et al., 2007; Said et al., 2013; Shaalan et al., 2009; Shahrour et al., 2015; Darwish et al., 2017). To improve the performance of syntactic diacritization or full diacritic restoration in general, previous studies followed different approaches. Some studies separate lexical from syntactic diacritization (Shaalan et al., 2009; Darwish et al., 2017). Other studies consider additional linguistic features such as POS tags and word segmentation (i.e. tokens or morphemes) (Ananthakrishnan et al., 2005; Zitouni et al., 2006; Zitouni and Sarikaya, 2009; Shaalan et al., 2009). Hifny (2018) addresses syntactic dia"
2020.acl-main.732,pasha-etal-2014-madamira,1,0.873843,"Missing"
2020.acl-main.732,D17-1073,0,0.490744,"ic word ÕÎ« Elm2 can mean “flag” or “knowledge”, but the meaning as well as pronunciation is specified when the word is diacritized ( ÕÎ « Ealamu means “flag” while ÕÎ « Eilomo means “knowledge”). As an illustrative example in English, if we omit the vowels in the word pn, the word can be read as pan, pin, pun, and pen, each of these variants have different pronunciations and meanings if it composes a valid word in the language. The state-of-the-art diacritic restoration models reached a decent performance over the years using recurrent or convolutional neural networks in terms of accuracy (Zalmout and Habash, 2017; Alqahtani et al., 2019; Orife, 2018) and/or efficiency (Alqahtani et al., 2019; Orife, 2018); yet, there is still room for further improvements. Most of these models are built on character level information which help generalize the model to unseen data, but presumably lose some useful information at the word level. Since word level resources are insufficient to be relied upon for training diacritic restoration models, we integrate additional linguistic information that considers word morphology as well as word relationships within a sentence to partially compensate for this loss. 2 We use B"
2020.acl-main.732,P19-1173,0,0.0365633,"Missing"
2020.acl-main.732,P06-1073,0,0.0911559,"pproaches (e.g. Maximum Entropy and Support Vector Machine) (Zitouni and Sarikaya, 2009; Pasha et al., 2014) or neural based approaches for different languages that include diacritics such as Arabic, Vietnamese, and Yoruba. Neural based approaches yield stateof-the-art performance for diacritic restoration by using Bidirectional LSTM or temporal convolutional networks (Zalmout and Habash, 2017; Orife, 2018; Alqahtani et al., 2019; Alqahtani and Diab, 2019a). Arabic syntactic diacritization has been consistently reported to be difficult, degrading the performance of full diacritic restoration (Zitouni et al., 2006; Habash et al., 2007; Said et al., 2013; Shaalan et al., 2009; Shahrour et al., 2015; Darwish et al., 2017). To improve the performance of syntactic diacritization or full diacritic restoration in general, previous studies followed different approaches. Some studies separate lexical from syntactic diacritization (Shaalan et al., 2009; Darwish et al., 2017). Other studies consider additional linguistic features such as POS tags and word segmentation (i.e. tokens or morphemes) (Ananthakrishnan et al., 2005; Zitouni et al., 2006; Zitouni and Sarikaya, 2009; Shaalan et al., 2009). Hifny (2018) ad"
2020.acl-main.732,W09-0804,0,0.0338904,"itouni and Sarikaya, 2009; Pasha et al., 2014) or neural based approaches for different languages that include diacritics such as Arabic, Vietnamese, and Yoruba. Neural based approaches yield stateof-the-art performance for diacritic restoration by using Bidirectional LSTM or temporal convolutional networks (Zalmout and Habash, 2017; Orife, 2018; Alqahtani et al., 2019; Alqahtani and Diab, 2019a). Arabic syntactic diacritization has been consistently reported to be difficult, degrading the performance of full diacritic restoration (Zitouni et al., 2006; Habash et al., 2007; Said et al., 2013; Shaalan et al., 2009; Shahrour et al., 2015; Darwish et al., 2017). To improve the performance of syntactic diacritization or full diacritic restoration in general, previous studies followed different approaches. Some studies separate lexical from syntactic diacritization (Shaalan et al., 2009; Darwish et al., 2017). Other studies consider additional linguistic features such as POS tags and word segmentation (i.e. tokens or morphemes) (Ananthakrishnan et al., 2005; Zitouni et al., 2006; Zitouni and Sarikaya, 2009; Shaalan et al., 2009). Hifny (2018) addresses syntactic diacritization by building BiLSTM model in w"
2020.acl-main.732,D15-1152,0,0.0162758,"009; Pasha et al., 2014) or neural based approaches for different languages that include diacritics such as Arabic, Vietnamese, and Yoruba. Neural based approaches yield stateof-the-art performance for diacritic restoration by using Bidirectional LSTM or temporal convolutional networks (Zalmout and Habash, 2017; Orife, 2018; Alqahtani et al., 2019; Alqahtani and Diab, 2019a). Arabic syntactic diacritization has been consistently reported to be difficult, degrading the performance of full diacritic restoration (Zitouni et al., 2006; Habash et al., 2007; Said et al., 2013; Shaalan et al., 2009; Shahrour et al., 2015; Darwish et al., 2017). To improve the performance of syntactic diacritization or full diacritic restoration in general, previous studies followed different approaches. Some studies separate lexical from syntactic diacritization (Shaalan et al., 2009; Darwish et al., 2017). Other studies consider additional linguistic features such as POS tags and word segmentation (i.e. tokens or morphemes) (Ananthakrishnan et al., 2005; Zitouni et al., 2006; Zitouni and Sarikaya, 2009; Shaalan et al., 2009). Hifny (2018) addresses syntactic diacritization by building BiLSTM model in which its input embeddin"
2020.acl-main.732,W17-1320,0,0.0397987,"Missing"
2020.acl-main.732,D18-1097,0,0.0186072,"ass information learned by character level tasks into word level tasks by composing a word embedding from the word’s characters. We first concatenate the individual embeddings of characters in that word, and then apply a Bidirectional Long Short Term Memory (BiLSTM) layer to generate denser vectors.8 This helps representing morphology and word composition into the model. (2) Word-To-Character Representation: To pass information learned by word level tasks into character level tasks, we concatenate each word with each of its composed characters during each pass, similar to what is described in Watson et al. (2018)’s study. This helps distinguishing the individual characters based on the surrounding context, implicitly capturing additional semantic and syntactic information. 8 We also evaluated the use of a feedforward layer and unidirectional Long Short Term Memory (LSTM) but a BiLSTM layer yielded better results. 8240 Figure 2: The diacritic restoration joint model. All Char Embed entities refer to the same randomly initialized character embedding learned during the training process. Pretrained embeddings refer to fixed word embeddings obtained from fastText (Bojanowski et al., 2017). (i) shows the in"
2020.acl-main.761,D15-1075,0,0.0539384,"have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline for the FEVER 2.0 shared task with entailment-based perturbations. Other participants generated adversarial claims using implicative phrases such as “not clear” (Kim and Allan, 2019) or GPT-2 (Niewinski et al., 2019). In comparison, we present a diverse set of attacks motivated by"
2020.acl-main.761,W18-5521,1,0.935823,"· · · pt−1 ) for evidence ep at time t < k. At time t, we update the hidden state zt of the pointer network decoder. Then we compute the weighted average hqt of the entire evidence set using q hops over the evidence (Vinyals et al., 2016; Sukhbaatar et al., 2015):4 Figure 3: Pointer network architecture. Claim and evidence (page title or sentence) are embedded with BERT and evidence is sequentially predicted (for sentence selection the relation sequence is jointly predicted). (1a) by combining the top M pages from a TF-IDF search using DrQA (Chen et al., 2017) with pages from the approach of Chakrabarty et al. (2018), which provides results from Google search and predicted named entities and noun phrases. Then, we perform document ranking by selecting the top D < M pages with a pointer network (1b). Next, an N -long sequence of evidence sentences (2) and veracity relation labels (3) are predicted jointly by another pointer network. Prior to training, we fine-tune BERT for document and sentence ranking on claim/title and claim/sentence pairs, respectively. Each claim and evidence pair in the FEVER 1.0 dataset has both the title of the Wikipedia article and at least one sentence associated with the evidence"
2020.acl-main.761,P17-1171,0,0.0283485,"y by computing the extraction probability P (pt |p0 · · · pt−1 ) for evidence ep at time t < k. At time t, we update the hidden state zt of the pointer network decoder. Then we compute the weighted average hqt of the entire evidence set using q hops over the evidence (Vinyals et al., 2016; Sukhbaatar et al., 2015):4 Figure 3: Pointer network architecture. Claim and evidence (page title or sentence) are embedded with BERT and evidence is sequentially predicted (for sentence selection the relation sequence is jointly predicted). (1a) by combining the top M pages from a TF-IDF search using DrQA (Chen et al., 2017) with pages from the approach of Chakrabarty et al. (2018), which provides results from Google search and predicted named entities and noun phrases. Then, we perform document ranking by selecting the top D < M pages with a pointer network (1b). Next, an N -long sequence of evidence sentences (2) and veracity relation labels (3) are predicted jointly by another pointer network. Prior to training, we fine-tune BERT for document and sentence ranking on claim/title and claim/sentence pairs, respectively. Each claim and evidence pair in the FEVER 1.0 dataset has both the title of the Wikipedia arti"
2020.acl-main.761,P18-1063,0,0.0241854,"d evaluate on FV1/FV2-dev/test (Section 3). We report accuracy (percentage of correct labels) and recall (whether the gold evidence is contained in selected evidence at k = 5). We also report the FEVER score, the percentage of correct evidence sentences (for S and R) that also have correct labels, and potency, the inverse FEVER score (subtracted from one) for evaluating adversarial claims. Our Baseline-RL: For baseline experiments, to compare different loss functions, we use the approach of Chakrabarty et al. (2018) for document selection and ranking, the reinforcement learning (RL) method of Chen and Bansal (2018) for sentence selection, and BERT (Devlin et al., 2019) for relation prediction. The RL approach using a pointer network is detailed by Chen and Bansal (2018) for extractive summarization, with the only difference that we use our fine-tuned BERT on claim/gold sentence pairs to represent each evidence sentence in the pointer network (as with our full system) and use the FEVER score as a reward. The reward is obtained by selecting sentences with the pointer network and then predicting the relation using an MLP (updated during training) and the concatenation of all claim/predicted sentence repres"
2020.acl-main.761,N19-1423,0,0.0681987,"m to 1) make sequential decisions to handle multiple propositions, 2) support temporal reasoning, and 3) handle ambiguity and complex lexical relations. To address the first requirement we make use of a pointer network (Vinyals et al., 2015) in two novel ways: i) to rerank candidate documents and ii) to jointly predict a sequence of evidence sentences and veracity relations in order to compose evidence (Figure 3). To address the second we add a post-processing step for simple temporal reasoning. To address the third we use rich, contextualized representations. Specifically, we fine-tune BERT (Devlin et al., 2019) as this model has shown excellent performance on related tasks and was pre-trained on Wikipedia. Figure 2: Our FEVER pipeline: 1) Retrieving Wikipedia pages by selecting an initial candidate set (1a) and ranking the top D (1b); 2) Identifying the top N sentences; 3) Predicting supports, refutes, or not enough info. Dashed arrows indicate fine-tuning steps. Our full pipeline is presented in Figure 2. We first identify an initial candidate set of documents 8596 features for every claim c and evidence ep pair by summing the [CLS] embedding for the top 4 layers (as recommended by Devlin et al. (2"
2020.acl-main.761,P18-2103,0,0.0149649,"ays make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline for the FEVER 2.0 shared task with entailment-based perturbations. Other participants generated adversarial claims using implicative phrases such as “not clear” (Kim and Allan, 2019) or GPT-2 (Niewinski et al., 2019). In comparison, we present a diverse set of attacks motivated by realistic, challenging categories and further develop models to address those attacks. 3 Problem Formulation and Datasets We address the end-to-end fact-checking p"
2020.acl-main.761,W18-5516,0,0.150254,"g the veracity of naturallyoccurring claims have focused on statements factchecked by journalists or organizations such as PolitiFact.org (Vlachos and Riedel, 2014; Alhindi et al., 2018), news articles (Pomerleau and Rao, 2017), or answers in community forums (Mihaylova et al., 2018, 2019). However, those datasets are not suited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieva"
2020.acl-main.761,W18-5525,1,0.929968,"uited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in"
2020.acl-main.761,P16-1135,1,0.838677,"Our Baseline-RL, S: Our System. *: p < 0.05 **: p < 0.01 ***: p < 0.001 by approximate randomization test diction), simple temporal reasoning (by rule-based date handling), and ambiguity and variation (by fine-tuned contextualized representations). There are many unaddressed vulnerabilities that are relevant for fact-checking. The Facebook bAbI tasks (Weston et al., 2016) include other types of reasoning (e.g. positional or size-based). The DROP dataset (Dua et al., 2019) requires mathematical operations for question answering such as addition or counting. Propositions with causal relations (Hidey and McKeown, 2016), which are eventbased rather than attribute-based as in FEVER, are also challenging. Finally, many verifiable claims are non-experiential (Park and Cardie, 2014), e.g. personal testimonies, which would require predicting whether a reported event was actually possible. Finally, our system could be improved in many ways. Future work in multi-hop reasoning could represent the relation between consecutive pieces of evidence and future work in temporal reasoning could incorporate numerical operations with BERT (Andor et al., 2019). One limitation of our system is the pipeline nature, which may req"
2020.acl-main.761,D17-1215,0,0.140729,"question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have b"
2020.acl-main.761,D19-6615,0,0.254756,"factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline for the FEVER 2.0 shared task with entailment-based perturbations. Other participants generated adversarial claims using implicative phrases such as “not clear” (Kim and Allan, 2019) or GPT-2 (Niewinski et al., 2019). In comparison, we present a diverse set of attacks motivated by realistic, challenging categories and further develop models to address those attacks. 3 Problem Formulation and Datasets We address the end-to-end fact-checking problem in the context of FEVER (Thorne et al., 2018), a task where a system is required to verify a claim by providing evidence from Wikipedia. To be successful, a system needs to predict both the correct veracity relation– supported (S), refuted (R), or not enough information (NEI)– and the correct set of evidence sentences (not appli"
2020.acl-main.761,W18-5517,0,0.45819,"for predicting the veracity of naturallyoccurring claims have focused on statements factchecked by journalists or organizations such as PolitiFact.org (Vlachos and Riedel, 2014; Alhindi et al., 2018), news articles (Pomerleau and Rao, 2017), or answers in community forums (Mihaylova et al., 2018, 2019). However, those datasets are not suited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framew"
2020.acl-main.761,S19-2149,0,0.0456459,"re experts examine ”check-worthy” claims (Hassan et al., 2017) published by others for their “shades” of truth (e.g., FactCheck.org or PolitiFact). However, this process is time-consuming, and thus building computational models for automatic fact-checking has become an active area of research (Graves, 2018). Advances were made possible by new open source datasets and shared tasks: the Fact Extraction and Verification Shared Task (FEVER) 1.0 and 2.0 (Thorne et al., 2018; Thorne ∗ Work completed in part at Amazon and Vlachos, 2019), SemEval 2019 Shared Task 8: Fact-Checking in Community Forums (Mihaylova et al., 2019), and LIAR(+) datasets with claims from PolitiFact (Wang, 2017; Alhindi et al., 2018). The FEVER 1.0 shared task dataset (Thorne et al., 2018) has enabled the development of endto-end fact-checking systems, requiring document retrieval and evidence sentence extraction to corroborate a veracity relation prediction (supports, refutes, not enough info). An example is given in Figure 1. Since the claims in FEVER 1.0 were manually written using information from Wikipedia, the dataset may lack linguistic challenges that occur in verifying naturally occurring check-worthy claims, such as temporal rea"
2020.acl-main.761,C16-1007,0,0.0648274,"Missing"
2020.acl-main.761,P14-1095,0,0.161316,"acks for lexical variation, where words may be inserted or replaced or changed with some other edit operation, have been shown to be effective for similar tasks such as natural language inference (Nie et al., 2019b) and question answering (Jia and Liang, 2017), so we include these types of attacks as well. For the fact-checking task, models must match words and entities across claim and evidence to make a veracity prediction. As claims often contain ambiguous entities (Thorne and Vlachos, 2018) or lexical features indicative 2 As determined by NER using Spacy: https://spacy.io of credibility (Nakashole and Mitchell, 2014), we desire models resilient to minor changes in entities (Hanselowski et al., 2018) and words (Alzantot et al., 2018). We thus create an adversarial dataset of 1000 examples, with 417 multi-propositional, 313 temporal and 270 lexically variational. Representative examples are provided in Appendix A. Multiple Propositions Check-worthy claims often consist of multiple propositions (Graves, 2018). In the FEVER task, checking these claims may require retrieving evidence sequentially after resolving entities and events, understanding discourse connectives, and evaluating each proposition. Consider"
2020.acl-main.761,P19-1225,0,0.016203,"ers in community forums (Mihaylova et al., 2018, 2019). However, those datasets are not suited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model"
2020.acl-main.761,W14-2105,0,0.0198557,"andling), and ambiguity and variation (by fine-tuned contextualized representations). There are many unaddressed vulnerabilities that are relevant for fact-checking. The Facebook bAbI tasks (Weston et al., 2016) include other types of reasoning (e.g. positional or size-based). The DROP dataset (Dua et al., 2019) requires mathematical operations for question answering such as addition or counting. Propositions with causal relations (Hidey and McKeown, 2016), which are eventbased rather than attribute-based as in FEVER, are also challenging. Finally, many verifiable claims are non-experiential (Park and Cardie, 2014), e.g. personal testimonies, which would require predicting whether a reported event was actually possible. Finally, our system could be improved in many ways. Future work in multi-hop reasoning could represent the relation between consecutive pieces of evidence and future work in temporal reasoning could incorporate numerical operations with BERT (Andor et al., 2019). One limitation of our system is the pipeline nature, which may require addressing each type of attack individually as adversaries adjust their techniques. An end-to-end approach or a query reformulation step (re-writing claims t"
2020.acl-main.761,D17-1317,0,0.040915,"val may resolve references, the model must understand the meaning of “lived long enough to see” and evaluate the comparative statement. To create claims of this type, we mine Wikipedia by selecting a page X and extracting sentences with the pattern “is/was/named the A of Y ” (e.g. A is “first governor”) where Y links to another page. Then we manually create temporal claims by examining dates on X and Y and describing the relation between the entities and events. Named Entity Ambiguity and Lexical Variation As fact-checking systems are sensitive to lexical choice (Nakashole and Mitchell, 2014; Rashkin et al., 2017), we consider how variations in entities and words may affect veracity relation prediction. E NTITY DISAMBIGUATION has been shown to be important for retrieving the correct page for an entity among multiple candidates (Hanselowski et al., 2018). To create examples that contain ambiguous entities we selected claims from FV1-dev where at least one Wikipedia disambiguation page was returned by the Wikipedia python API.3 We then created a new claim using one of the documents returned from the disambiguation list. For example the claim “Patrick Stewart is someone who does acting for a living.” retu"
2020.acl-main.761,P19-1103,0,0.0291828,"cting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline for the FEVER 2.0 shared task with entailment-based perturbations."
2020.acl-main.761,P18-1079,0,0.0385504,"n architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline"
2020.acl-main.761,D19-6616,0,0.0367825,"Missing"
2020.acl-main.761,N18-1081,0,0.0288541,"evidence, i.e. “teacher forcing” (Williams and Zipser, 1989), and the model prediction from the previous step, so that we have training data for NEI. Combining Equations 3 and 5, our loss is: L(θ) = λL(θptr ) + L(θrel seq ) (6) Finally, to predict a relation at inference, we ensemble the sequence of predicted labels by averaging the probabilities over every time step.5 Post-processing for Simple Temporal Reasoning As neural models are unreliable for handling numerical statements, we introduce a rule-based step to extract and reason about dates. We use the Open Information Extraction system of Stanovsky et al. (2018) to extract tuples. For example, given the claim “The Latvian Soviet Socialist Republic was a republic of the Soviet Union 3 years after 2009,” the system would identify ARG0 as preceding the verb was and ARG1 following. After identifying tuples in claims and predicted sentences, we discard those lacking dates (e.g. ARG0). Given more than one candidate sentence, we select the one ranked higher by the pointer network. Once we have both the claim and evidence date-tuple we apply one of three rules to resolve the relation prediction based on the corresponding temporal phrase. We either evaluate w"
2020.acl-main.761,D19-6604,0,0.106964,"ral language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline for the FEVER 2.0 shared task with entailment-based perturbations. Other participants generated adversarial claims using implicative phrases such as “not clear” (Kim and Allan, 2019) or GPT-2 (Niewinski et al., 2019). In comparison, we present a diverse set of attacks motivated by realistic, challenging categories and further develop models to address those attacks. 3 Problem Formulation and Datasets We address the end-to-end fact-checking problem in the context of FEVER (Thorne et al., 2018), a task where a system is required to verify a claim by providing evidence from Wikipedia. To be successful, a system needs to predict both the correct veracity relation– supported (S), refuted (R), or not enough information (NEI)– and the correct set of evidence sentences (not applicable for NEI). The FEVER 1.0 data"
2020.acl-main.761,E17-3010,0,0.0265089,", composition of multiple propositions may also be necessary for NEI, as the relation of the claim and evidence may be changed by more general/specific phrases. We thus add ADDITIONAL UNVERIFIABLE PROPOSITIONS that change the gold label to NEI. We selected claims from FV1-dev and added propositions which have no evidence in Wikipedia (e.g. for the claim “Duff McKagan is an American citizen,” we can add the reduced relative clause “born in Seattle“). 8595 Temporal Reasoning Many check-worthy claims contain dates or time periods and to verify them requires models that handle temporal reasoning (Thorne and Vlachos, 2017). In order to evaluate the ability of current systems to handle temporal reasoning we modify claims from FV1-dev. More specifically, using claims with the phrase ”in <date>” we automatically generate seven modified claims using simple DATE MANIP ULATION heuristics: arithmetic (e.g., “in 2001” → “4 years before 2005”), range (“in 2001” → “before 2008”), and verbalization (“in 2001” → “in the first decade of the 21st century”). We also create examples requiring MULTI - HOP TEMPORAL REASONING , where the system must evaluate an event in relation to another. Consider the S claim “The first governo"
2020.acl-main.761,C18-1283,0,0.123658,"Missing"
2020.acl-main.761,N18-1074,0,0.390308,"rom newswire to social media (Vosoughi et al., 2018). To overcome this challenge, fact-checking has emerged as a necessary part of journalism, where experts examine ”check-worthy” claims (Hassan et al., 2017) published by others for their “shades” of truth (e.g., FactCheck.org or PolitiFact). However, this process is time-consuming, and thus building computational models for automatic fact-checking has become an active area of research (Graves, 2018). Advances were made possible by new open source datasets and shared tasks: the Fact Extraction and Verification Shared Task (FEVER) 1.0 and 2.0 (Thorne et al., 2018; Thorne ∗ Work completed in part at Amazon and Vlachos, 2019), SemEval 2019 Shared Task 8: Fact-Checking in Community Forums (Mihaylova et al., 2019), and LIAR(+) datasets with claims from PolitiFact (Wang, 2017; Alhindi et al., 2018). The FEVER 1.0 shared task dataset (Thorne et al., 2018) has enabled the development of endto-end fact-checking systems, requiring document retrieval and evidence sentence extraction to corroborate a veracity relation prediction (supports, refutes, not enough info). An example is given in Figure 1. Since the claims in FEVER 1.0 were manually written using inform"
2020.acl-main.761,N19-1230,0,0.0125915,"yoccurring claims have focused on statements factchecked by journalists or organizations such as PolitiFact.org (Vlachos and Riedel, 2014; Alhindi et al., 2018), news articles (Pomerleau and Rao, 2017), or answers in community forums (Mihaylova et al., 2018, 2019). However, those datasets are not suited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model ev"
2020.acl-main.761,W14-2508,0,0.237851,"tion for Computational Linguistics butions using pointer networks: 1) a document ranking model; and 2) a joint model for evidence sentence selection and veracity relation prediction framed as a sequence labeling task (Section 5). Our new system achieves state-of-the-art results for FEVER and we present an evaluation of our models including ablation studies (Section 6). Data and code will be released to the community.1 2 Related Work Approaches for predicting the veracity of naturallyoccurring claims have focused on statements factchecked by journalists or organizations such as PolitiFact.org (Vlachos and Riedel, 2014; Alhindi et al., 2018), news articles (Pomerleau and Rao, 2017), or answers in community forums (Mihaylova et al., 2018, 2019). However, those datasets are not suited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other wor"
2020.acl-main.761,D19-1221,0,0.0277601,"prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and impro"
2020.acl-main.761,P17-2067,0,0.0613097,"others for their “shades” of truth (e.g., FactCheck.org or PolitiFact). However, this process is time-consuming, and thus building computational models for automatic fact-checking has become an active area of research (Graves, 2018). Advances were made possible by new open source datasets and shared tasks: the Fact Extraction and Verification Shared Task (FEVER) 1.0 and 2.0 (Thorne et al., 2018; Thorne ∗ Work completed in part at Amazon and Vlachos, 2019), SemEval 2019 Shared Task 8: Fact-Checking in Community Forums (Mihaylova et al., 2019), and LIAR(+) datasets with claims from PolitiFact (Wang, 2017; Alhindi et al., 2018). The FEVER 1.0 shared task dataset (Thorne et al., 2018) has enabled the development of endto-end fact-checking systems, requiring document retrieval and evidence sentence extraction to corroborate a veracity relation prediction (supports, refutes, not enough info). An example is given in Figure 1. Since the claims in FEVER 1.0 were manually written using information from Wikipedia, the dataset may lack linguistic challenges that occur in verifying naturally occurring check-worthy claims, such as temporal reasoning or lexical generalization/specification. Thorne and Vla"
2020.acl-main.761,N18-1101,0,0.0608056,"ransformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we focus specifically on challenges in factchecking. The task of natural language inference 1 https://github.com/chridey/ fever2-columbia (Bowman et al., 2015; Williams et al., 2018) provides similar challenges: examples for numerical reasoning and lexical inference have been shown to be difficult (Glockner et al., 2018; Nie et al., 2019b) and improved models on these types are likely to be useful for fact-checking. Finally, (Thorne and Vlachos, 2019) provided a baseline for the FEVER 2.0 shared task with entailment-based perturbations. Other participants generated adversarial claims using implicative phrases such as “not clear” (Kim and Allan, 2019) or GPT-2 (Niewinski et al., 2019). In comparison, we present a diverse set of attacks motivated by realistic, challenging c"
2020.acl-main.761,D18-1010,0,0.0845557,"e datasets are not suited for end-to-end fact-checking as they provide sources and evidence while FEVER (Thorne et al., 2018) requires retrieval. Initial work on FEVER focused on a pipeline approach of retrieving documents, selecting sentences, and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other r"
2020.acl-main.761,W18-5515,0,0.0524187,"Missing"
2020.acl-main.761,P19-1085,0,0.566224,"and then using an entailment module (Malon, 2018; Hanselowski et al., 2018; Tokala et al., 2019); the winning entry for the FEVER 1.0 shared task (Nie et al., 2019a) used three homogeneous neural models. Other work has jointly learned either evidence extraction and question answering (Nishida et al., 2019) or sentence selection and relation prediction (Yin and Roth, 2018; Hidey and Diab, 2018); unlike these approaches, we use the same sequential evidence prediction architecture for both document and sentence selection, jointly predicting a sequence of labels in the latter step. More recently, Zhou et al. (2019) proposed a graph-based framework for multi-hop retrieval, whereas we model evidence sequentially. Language-based adversarial attacks have often involved transformations of the input such as phrase insertion to distract question answering systems (Jia and Liang, 2017) or to force a model to always make the same prediction (Wallace et al., 2019). Other research has resulted in adversarial methods for paraphrasing with universal replacement rules (Ribeiro et al., 2018) or lexical substitution (Alzantot et al., 2018; Ren et al., 2019). While our strategies include insertion and replacement, we fo"
2020.coling-main.414,Q19-1038,0,0.0233204,"responding Sinhala or Odia dataset for evaluation. For the English classification task, we implement classifiers of different architectures adopting various embeddings including BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLM-R (Conneau et al., 2020), which we use to extract features, then train a classifier that takes the contextualized representations of the tweets into account. For the cross-lingual classification task, we build classifiers using the same set of architectures, but deploying various cross-lingual embeddings that are constructed using different methods: LASER (Artetxe and Schwenk, 2019) and XLM-R (Conneau et al., 2020). For both tasks, we employ semi-supervised approaches by generating pseudo-labels for a large amount of unlabeled tweets that are crisis related, in order to improve system performance. Last but not least, we ensemble different classifiers to boost performance further. 2 Dataset Tweets about many natural and human-induced disasters such as earthquakes, typhoons, and landslides are collected by (Imran et al., 2016). We annotate a subset of them at the tweet-level on the Figure-Eight data annotation platform3 as seen in Figure 1. The annotation tag set comprises"
2020.coling-main.414,Q17-1010,0,0.0314269,"ion for Sinhala/Odia. 3.2 English Classification To start with, we build classifiers for detecting urgency given tweets in English to establish an understanding of the baseline performance of this task without the effect of transferring between languages. 3.2.1 Monolingual Embeddings For all of our classifiers, we first use word/sentence embeddings to extract features of the input tweets. We experiment with the following variations when choosing the English embeddings: contextual and non-contextual, and out-of-domain and in-domain. We choose two non-contextual embeddings: fastText embeddings (Bojanowski et al., 2017) and CrisisNLP embeddings (Nguyen et al., 2016). fastText embeddings are trained on texts from Wikipedia and Common Crawl (both out-of-crisis-domain) whereas CrisisNLP embeddings are trained on disaster related tweets, i.e. in-domain. Both embeddings project each word in a sentence to a 300 dimensional vector representation. We also use BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLM-R (Conneau et al., 2020) to generate contextual representations of the tweets for English.8 A list of embeddings and their availability for each language is shown in Table 3. Embeddings Lang. Dimens"
2020.coling-main.414,2020.acl-main.747,0,0.0749318,"Missing"
2020.coling-main.414,N19-1423,0,0.489571,"https://github.com/niless/urgency This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons. org/licenses/by/4.0/. 1 4693 Proceedings of the 28th International Conference on Computational Linguistics, pages 4693–4703 Barcelona, Spain (Online), December 8-13, 2020 classification, for which we use the entire English dataset for training and the corresponding Sinhala or Odia dataset for evaluation. For the English classification task, we implement classifiers of different architectures adopting various embeddings including BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and XLM-R (Conneau et al., 2020), which we use to extract features, then train a classifier that takes the contextualized representations of the tweets into account. For the cross-lingual classification task, we build classifiers using the same set of architectures, but deploying various cross-lingual embeddings that are constructed using different methods: LASER (Artetxe and Schwenk, 2019) and XLM-R (Conneau et al., 2020). For both tasks, we employ semi-supervised approaches by generating pseudo-labels for a large amount of unlabeled tweets that are crisis related"
2020.coling-main.414,N13-1073,0,0.0100306,"fer approach is shown in Figure 3 and Algorithm 1. 3.3.1 Cross-lingual Embeddings To generate a cross-lingual embedding that can be used to transfer from English to Sinhala, we use a parallel corpus that contains English-Sinhala sentence pairs as well as pre-trained English embeddings and Sinhala embeddings. There are many approaches for generating cross-lingual embeddings given the above resources, but in our study we focus on the projection-based methods of training the embeddings: 4698 VecMap (Artetxe et al., 2018) and Proc-B (Glavaˇs et al., 2019). As a first step, we use fast-align tool (Dyer et al., 2013) to create symmetric word alignments between source and target words given the parallel corpus, then we choose the most frequent translation for each word (Rasooli et al., 2018). This generates a bilingual dictionary with 72K approximate vocabulary size for each language, which is used as a seed dictionary to generate the cross-lingual embeddings by projecting the pre-trained English and Sinhala monolingual embeddings to the same semantic space. We employ the same procedure to generate the English-Odia embeddings as well, given the English-Odia parallel corpus and the pre-trained English and O"
2020.coling-main.414,P19-1070,0,0.0326001,"Missing"
2020.coling-main.414,L18-1550,0,0.0565385,"Missing"
2020.coling-main.414,L16-1259,0,0.169168,"iers using the same set of architectures, but deploying various cross-lingual embeddings that are constructed using different methods: LASER (Artetxe and Schwenk, 2019) and XLM-R (Conneau et al., 2020). For both tasks, we employ semi-supervised approaches by generating pseudo-labels for a large amount of unlabeled tweets that are crisis related, in order to improve system performance. Last but not least, we ensemble different classifiers to boost performance further. 2 Dataset Tweets about many natural and human-induced disasters such as earthquakes, typhoons, and landslides are collected by (Imran et al., 2016). We annotate a subset of them at the tweet-level on the Figure-Eight data annotation platform3 as seen in Figure 1. The annotation tag set comprises the following four levels of urgency: Figure 1: Annotation interface • Extremely Urgent: aspects of the tweet refer to an extremely urgent and difficult situation; e.g. MT @SushmaSwaraj my uncle is in kathmandu, trapped, suffers from jaundice, chest infection, diabetes, his number #NepalQuake • Definitely Urgent: tweet contains content that is urgent but the level of urgency is not as high; e.g. @MountainGuides1 Please help us find my friends par"
2020.coling-main.414,W15-1521,0,0.0192207,"et al., 2019; Chaudhary et al., 2019). The work of Kejriwal and Zhou (2019) apply a manual feature based approach to transfer urgency labels from English to several low resource languages combined with active learning to increase the amount of labels. Recent successful techniques in transfer learning, however, use cross-lingual embeddings combined with deep learning based classifiers. Cross-lingual embeddings map words in different languages into same semantic space and among them, we use projection based approaches, i.e. VecMap and ProcB, rather than parallel corpora based ones e.g. BiSkip (Luong et al., 2015) due to their superior performance. This has been shown to work well for sentiment (Socher et al., 2013; Rasooli et al., 2018) and emotion (Tafreshi and Diab, 2018). In addition, after the success of contextual language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) in many NLP tasks, their multilingual versions became available i.e. Multilingual BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) which we experimented with based on their availability for our languages. Their adaptation to low resource settings, e.g. fine-tuning with small datasets, is not tri"
2020.coling-main.414,N06-1020,0,0.0718263,"utputs. Figure 2: MLP Architecture INPUT CROSS-LINGUAL LEARNING CROSS-LINGUAL OUTPUT Parallel Corpora Align words & extract dictionary Bilingual Dictionary Train cross-lingual embedding Cross-lingual Embedding Train cross-lingual classifier Cross-lingual Classifier IL Monolingual Embedding English Monolingual Embedding English Data with Labels Figure 3: System Architecture: Transfer Learning in Zero-shot setting 3.2.3 Data Augmentation We experiment with a semi-supervised training scheme to augment the training dataset (shown in Algorithm 1). We adopt self-training approaches (Yarowsky, 1995; McClosky et al., 2006), in which we add the best performing classifier’s predictions on unlabeled data to the initial training dataset, which is manually annotated. We sample the unlabeled tweets from the same collection of disaster related tweets (Imran et al., 2016) where we select and annotate a subset to create our English training dataset as described in Section 2, and we make sure the set of the unlabeled tweets and the set of training data 4697 Algorithm 1 Incremental Training Workflow Let source language training dataset be S Let unlabelled source language dataset be U Let target language testing set be T w"
2020.coling-main.414,P18-1096,0,0.0178834,"lingual BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) which we experimented with based on their availability for our languages. Their adaptation to low resource settings, e.g. fine-tuning with small datasets, is not trivial and is not as reliable as in high resource settings. As such, we show how this can be achieved with our experimental setup. Specifically, we use a self-learning method by voting (Zhou and Goldman, 2004) to increase the size of the high resource language dataset on unlabelled Crisis NLP tweets. We decide not to use tri-training (Zhi-Hua Zhou and Ming Li, 2005; Ruder and Plank, 2018) due to the size of original English data despite the fact that tri-training has shown good results in domain-shift NLP tasks. 6 Conclusion In this study, we release an urgency dataset consisting of English tweets about natural crisis and their urgency status. In addition, we release two evaluation datasets for urgency detection in Sinhala and Odia. We train monolingual classifiers for English and cross-lingual classifiers for Sinhala and Odia that are zero-shot learners. For the design of our classifiers, beside exploring different architectures, we adopt different monolingual or cross-lingua"
2020.coling-main.414,N19-5004,0,0.114998,"de. During an emergent crisis, people post to report their well-being, ask for help, or give updates about the ongoing situation. This type of text data can be utilized to provide situational awareness to support missions such as humanitarian assistance/disaster relief, peacekeeping or infectious disease response. However, with the existence of more than 7,000 languages worldwide, automated human language technology does not exist for many languages.1 A possible solution to this problem is to transfer models learned in high resource language settings such as English to low resource languages (Ruder et al., 2019). In addition, there has been significant research in the use of transfer models in semantic analysis of texts such as sentiment (Socher et al., 2013; Rasooli et al., 2018) and emotion (Tafreshi and Diab, 2018). To this end, we collect and release English, Sinhala and Odia urgency datasets that consist of tweets relating to natural crises, annotated with urgency status. 2 To demonstrate that we are able to effectively transfer the task of urgency detection from English to low-resource languages, we use English annotated tweets for training, and Sinhala/Odia annotated tweets for evaluation only"
2020.coling-main.414,D13-1170,0,0.00954867,"ed approach to transfer urgency labels from English to several low resource languages combined with active learning to increase the amount of labels. Recent successful techniques in transfer learning, however, use cross-lingual embeddings combined with deep learning based classifiers. Cross-lingual embeddings map words in different languages into same semantic space and among them, we use projection based approaches, i.e. VecMap and ProcB, rather than parallel corpora based ones e.g. BiSkip (Luong et al., 2015) due to their superior performance. This has been shown to work well for sentiment (Socher et al., 2013; Rasooli et al., 2018) and emotion (Tafreshi and Diab, 2018). In addition, after the success of contextual language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) in many NLP tasks, their multilingual versions became available i.e. Multilingual BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) which we experimented with based on their availability for our languages. Their adaptation to low resource settings, e.g. fine-tuning with small datasets, is not trivial and is not as reliable as in high resource settings. As such, we show how this can be achieved wit"
2020.coling-main.414,C18-1246,1,0.803396,"everal low resource languages combined with active learning to increase the amount of labels. Recent successful techniques in transfer learning, however, use cross-lingual embeddings combined with deep learning based classifiers. Cross-lingual embeddings map words in different languages into same semantic space and among them, we use projection based approaches, i.e. VecMap and ProcB, rather than parallel corpora based ones e.g. BiSkip (Luong et al., 2015) due to their superior performance. This has been shown to work well for sentiment (Socher et al., 2013; Rasooli et al., 2018) and emotion (Tafreshi and Diab, 2018). In addition, after the success of contextual language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019) in many NLP tasks, their multilingual versions became available i.e. Multilingual BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) which we experimented with based on their availability for our languages. Their adaptation to low resource settings, e.g. fine-tuning with small datasets, is not trivial and is not as reliable as in high resource settings. As such, we show how this can be achieved with our experimental setup. Specifically, we use a self-learnin"
2020.coling-main.414,P95-1026,0,0.61215,"ous CNN layers outputs. Figure 2: MLP Architecture INPUT CROSS-LINGUAL LEARNING CROSS-LINGUAL OUTPUT Parallel Corpora Align words & extract dictionary Bilingual Dictionary Train cross-lingual embedding Cross-lingual Embedding Train cross-lingual classifier Cross-lingual Classifier IL Monolingual Embedding English Monolingual Embedding English Data with Labels Figure 3: System Architecture: Transfer Learning in Zero-shot setting 3.2.3 Data Augmentation We experiment with a semi-supervised training scheme to augment the training dataset (shown in Algorithm 1). We adopt self-training approaches (Yarowsky, 1995; McClosky et al., 2006), in which we add the best performing classifier’s predictions on unlabeled data to the initial training dataset, which is manually annotated. We sample the unlabeled tweets from the same collection of disaster related tweets (Imran et al., 2016) where we select and annotate a subset to create our English training dataset as described in Section 2, and we make sure the set of the unlabeled tweets and the set of training data 4697 Algorithm 1 Incremental Training Workflow Let source language training dataset be S Let unlabelled source language dataset be U Let target lan"
2020.emnlp-main.663,S15-2162,0,0.0268456,"ontextual features. To the best of our knowledge, this work is the first study to develop an enhanced semantic depen8268 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8268–8274, c November 16–20, 2020. 2020 Association for Computational Linguistics dency parser through multitasking in the absence of annotated data. 2 Related Work After the SemEval shared tasks on broad-coverage semantic dependency parsing (Oepen et al., 2014, 2015; Che et al., 2016), there have been many studies to build supervised SDP models (Du et al., 2015; Chen et al., 2018; Almeida and Martins, 2015; Wang et al., 2018; Dozat and Manning, 2018; Stanovsky and Dagan, 2018; Kurita and Søgaard, 2019), however, all efforts were restricted to the three languages released through SemEval shared tasks. There have been extensive number of studies that use annotation projection to cure data scarcity in different tasks such as part-of-speech tagging (Täckström et al., 2013), syntactic parsing (McDonald et al., 2011), semantic role labeling (Padó and Lapata, 2005) and semantic parsing (Hershcovich et al., 2019). Nevertheless, none of the previous works, to the best of our knowledge, looked into using"
2020.emnlp-main.663,P98-1013,0,0.712273,"projected semantic dependencies (bottom) for the target sentence. Introduction Broad-coverage semantic dependency parsing (SDP)1 was first introduced in the SemEval shared task (Oepen et al., 2014) and aims to provide semantic analysis of sentences by capturing semantic relations between all content-bearing words in a sentence. The rich graph structure introduced by SDP allows the model to cover a wide range of semantic phenomena such as negation, comparatives, possessives and various types of modifications that have not been previously analyzed in other models such as semantic role labeling (Baker et al., 1998). Despite all advantages provided by SDP, resources with annotated semantic dependencies are limited to the three languages released in the SemEval shared tasks (Oepen et al., 2014, 2015; Che et al., 2016) namely English, Czech and Chinese. This data scarcity motivates us to use well-known and traditionally used transfer methods such as annotation projection for building SDP models for languages without semantically annotated data. In annotation projection, we assume that we have access to sentence-aligned corpora that can be used for transferring semantic annotations from a richresource sourc"
2020.emnlp-main.663,K18-2005,0,0.0692826,"Missing"
2020.emnlp-main.663,S16-1167,0,0.117664,"to provide semantic analysis of sentences by capturing semantic relations between all content-bearing words in a sentence. The rich graph structure introduced by SDP allows the model to cover a wide range of semantic phenomena such as negation, comparatives, possessives and various types of modifications that have not been previously analyzed in other models such as semantic role labeling (Baker et al., 1998). Despite all advantages provided by SDP, resources with annotated semantic dependencies are limited to the three languages released in the SemEval shared tasks (Oepen et al., 2014, 2015; Che et al., 2016) namely English, Czech and Chinese. This data scarcity motivates us to use well-known and traditionally used transfer methods such as annotation projection for building SDP models for languages without semantically annotated data. In annotation projection, we assume that we have access to sentence-aligned corpora that can be used for transferring semantic annotations from a richresource source language to the target language 1 We use broad-coverage semantic dependencies and semantic dependencies interchangeably throughout this paper. through word alignment links. Figure 1 shows an example of a"
2020.emnlp-main.663,K18-1054,0,0.0191623,"in by using those contextual features. To the best of our knowledge, this work is the first study to develop an enhanced semantic depen8268 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8268–8274, c November 16–20, 2020. 2020 Association for Computational Linguistics dency parser through multitasking in the absence of annotated data. 2 Related Work After the SemEval shared tasks on broad-coverage semantic dependency parsing (Oepen et al., 2014, 2015; Che et al., 2016), there have been many studies to build supervised SDP models (Du et al., 2015; Chen et al., 2018; Almeida and Martins, 2015; Wang et al., 2018; Dozat and Manning, 2018; Stanovsky and Dagan, 2018; Kurita and Søgaard, 2019), however, all efforts were restricted to the three languages released through SemEval shared tasks. There have been extensive number of studies that use annotation projection to cure data scarcity in different tasks such as part-of-speech tagging (Täckström et al., 2013), syntactic parsing (McDonald et al., 2011), semantic role labeling (Padó and Lapata, 2005) and semantic parsing (Hershcovich et al., 2019). Nevertheless, none of the previous works, to the best of our k"
2020.emnlp-main.663,N19-1423,0,0.0180187,"77.1 78.6 75.0 76.6 76.4 78.1 85.4 70.4 85.1 70.8 85.1 70.8 84.3 69.7 85.4 71.1 91.1 86.8 90.1 87.0 91.0 87.3 90.2 86.4 91.0 87.3 Single X X X X X X Mutitask X X X X X X Transfer + ELMO + mBERT Supervised Table 1: Results on the Czech SemEval test data. LF and UF denote Labeled and Unlabeled F1 respectively. The Transfer column does not use contextualized word embeddings. Task RNN refers to an extra task-specific embedding in the multitask setting. The shaded rows show results on out-of-domain test data. hyper-parameters as Peters et al. (2018). We use the pretrained multilingual BERT models (Devlin et al., 2019) of size 768 from Xiao (2018) with 12 layers and 12 heads. Due to computational limitations, we only use the pretrained BERT models in the input layer without finetuning. 6.1 Results Table 1 shows the results on in-domain and outof-domain data with and without contextual word embeddings. The Single row shows the baseline where we use Czech projection data to train the model. The Multitask rows show the results when we utilize syntactic parses through multitasking. The last column shows results of the supervised model trained on the gold data provided as part of the SemEval 2015 shared task, wh"
2020.emnlp-main.663,P18-2077,0,0.221803,"ge, this work is the first study to develop an enhanced semantic depen8268 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8268–8274, c November 16–20, 2020. 2020 Association for Computational Linguistics dency parser through multitasking in the absence of annotated data. 2 Related Work After the SemEval shared tasks on broad-coverage semantic dependency parsing (Oepen et al., 2014, 2015; Che et al., 2016), there have been many studies to build supervised SDP models (Du et al., 2015; Chen et al., 2018; Almeida and Martins, 2015; Wang et al., 2018; Dozat and Manning, 2018; Stanovsky and Dagan, 2018; Kurita and Søgaard, 2019), however, all efforts were restricted to the three languages released through SemEval shared tasks. There have been extensive number of studies that use annotation projection to cure data scarcity in different tasks such as part-of-speech tagging (Täckström et al., 2013), syntactic parsing (McDonald et al., 2011), semantic role labeling (Padó and Lapata, 2005) and semantic parsing (Hershcovich et al., 2019). Nevertheless, none of the previous works, to the best of our knowledge, looked into using annotation projection for building SDP mode"
2020.emnlp-main.663,S15-2154,0,0.0256812,"ind a marginal gain by using those contextual features. To the best of our knowledge, this work is the first study to develop an enhanced semantic depen8268 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8268–8274, c November 16–20, 2020. 2020 Association for Computational Linguistics dency parser through multitasking in the absence of annotated data. 2 Related Work After the SemEval shared tasks on broad-coverage semantic dependency parsing (Oepen et al., 2014, 2015; Che et al., 2016), there have been many studies to build supervised SDP models (Du et al., 2015; Chen et al., 2018; Almeida and Martins, 2015; Wang et al., 2018; Dozat and Manning, 2018; Stanovsky and Dagan, 2018; Kurita and Søgaard, 2019), however, all efforts were restricted to the three languages released through SemEval shared tasks. There have been extensive number of studies that use annotation projection to cure data scarcity in different tasks such as part-of-speech tagging (Täckström et al., 2013), syntactic parsing (McDonald et al., 2011), semantic role labeling (Padó and Lapata, 2005) and semantic parsing (Hershcovich et al., 2019). Nevertheless, none of the previous works, t"
2020.emnlp-main.663,W17-0237,0,0.0605776,"Missing"
2020.emnlp-main.663,P18-1035,0,0.0205049,"kström et al., 2013), syntactic parsing (McDonald et al., 2011), semantic role labeling (Padó and Lapata, 2005) and semantic parsing (Hershcovich et al., 2019). Nevertheless, none of the previous works, to the best of our knowledge, looked into using annotation projection for building SDP models for languages without semantically annotated data. Motivated by the fact that different semantic representations or formalisms cover different aspects of sentence-level semantics, there has been a line of studies to apply multitask learning over different semantic annotations (Peng et al., 2017, 2018; Hershcovich et al., 2018; Kurita and Søgaard, 2019) or target cross-framework meaning representation (Oepen et al., 2019). These studies use the shared semantic information across different representations to enhance the SDP model for a given language, however, none of them addressed the case that no semantically annotated data is available for a language. This paper is the first work that aims to build an SDP model based on cross-lingual transfer without any annotation in the target language of interest. 3 The Parsing Model For an input sentence x = x1 , · · · , xn with n words, the goal of a semantic dependency par"
2020.emnlp-main.663,S19-2001,0,0.0186246,"have been many studies to build supervised SDP models (Du et al., 2015; Chen et al., 2018; Almeida and Martins, 2015; Wang et al., 2018; Dozat and Manning, 2018; Stanovsky and Dagan, 2018; Kurita and Søgaard, 2019), however, all efforts were restricted to the three languages released through SemEval shared tasks. There have been extensive number of studies that use annotation projection to cure data scarcity in different tasks such as part-of-speech tagging (Täckström et al., 2013), syntactic parsing (McDonald et al., 2011), semantic role labeling (Padó and Lapata, 2005) and semantic parsing (Hershcovich et al., 2019). Nevertheless, none of the previous works, to the best of our knowledge, looked into using annotation projection for building SDP models for languages without semantically annotated data. Motivated by the fact that different semantic representations or formalisms cover different aspects of sentence-level semantics, there has been a line of studies to apply multitask learning over different semantic annotations (Peng et al., 2017, 2018; Hershcovich et al., 2018; Kurita and Søgaard, 2019) or target cross-framework meaning representation (Oepen et al., 2019). These studies use the shared semanti"
2020.emnlp-main.663,2005.mtsummit-papers.11,0,0.223325,"Missing"
2020.emnlp-main.663,P19-1232,0,0.0142686,"ced semantic depen8268 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8268–8274, c November 16–20, 2020. 2020 Association for Computational Linguistics dency parser through multitasking in the absence of annotated data. 2 Related Work After the SemEval shared tasks on broad-coverage semantic dependency parsing (Oepen et al., 2014, 2015; Che et al., 2016), there have been many studies to build supervised SDP models (Du et al., 2015; Chen et al., 2018; Almeida and Martins, 2015; Wang et al., 2018; Dozat and Manning, 2018; Stanovsky and Dagan, 2018; Kurita and Søgaard, 2019), however, all efforts were restricted to the three languages released through SemEval shared tasks. There have been extensive number of studies that use annotation projection to cure data scarcity in different tasks such as part-of-speech tagging (Täckström et al., 2013), syntactic parsing (McDonald et al., 2011), semantic role labeling (Padó and Lapata, 2005) and semantic parsing (Hershcovich et al., 2019). Nevertheless, none of the previous works, to the best of our knowledge, looked into using annotation projection for building SDP models for languages without semantically annotated data."
2020.emnlp-main.663,N15-1142,0,0.0208632,"Missing"
2020.emnlp-main.663,D11-1006,0,0.0726435,"Missing"
2020.emnlp-main.663,J03-1002,0,0.0153975,"Missing"
2020.emnlp-main.663,K19-2001,0,0.0406367,"Missing"
2020.emnlp-main.663,S15-2153,0,0.0583237,"Missing"
2020.emnlp-main.663,S14-2008,0,0.0770568,"Missing"
2020.emnlp-main.663,K17-3009,0,0.0698106,"Missing"
2020.emnlp-main.663,Q13-1001,0,0.0332516,"ed Work After the SemEval shared tasks on broad-coverage semantic dependency parsing (Oepen et al., 2014, 2015; Che et al., 2016), there have been many studies to build supervised SDP models (Du et al., 2015; Chen et al., 2018; Almeida and Martins, 2015; Wang et al., 2018; Dozat and Manning, 2018; Stanovsky and Dagan, 2018; Kurita and Søgaard, 2019), however, all efforts were restricted to the three languages released through SemEval shared tasks. There have been extensive number of studies that use annotation projection to cure data scarcity in different tasks such as part-of-speech tagging (Täckström et al., 2013), syntactic parsing (McDonald et al., 2011), semantic role labeling (Padó and Lapata, 2005) and semantic parsing (Hershcovich et al., 2019). Nevertheless, none of the previous works, to the best of our knowledge, looked into using annotation projection for building SDP models for languages without semantically annotated data. Motivated by the fact that different semantic representations or formalisms cover different aspects of sentence-level semantics, there has been a line of studies to apply multitask learning over different semantic annotations (Peng et al., 2017, 2018; Hershcovich et al.,"
2020.emnlp-main.663,H05-1108,0,0.0508583,"et al., 2014, 2015; Che et al., 2016), there have been many studies to build supervised SDP models (Du et al., 2015; Chen et al., 2018; Almeida and Martins, 2015; Wang et al., 2018; Dozat and Manning, 2018; Stanovsky and Dagan, 2018; Kurita and Søgaard, 2019), however, all efforts were restricted to the three languages released through SemEval shared tasks. There have been extensive number of studies that use annotation projection to cure data scarcity in different tasks such as part-of-speech tagging (Täckström et al., 2013), syntactic parsing (McDonald et al., 2011), semantic role labeling (Padó and Lapata, 2005) and semantic parsing (Hershcovich et al., 2019). Nevertheless, none of the previous works, to the best of our knowledge, looked into using annotation projection for building SDP models for languages without semantically annotated data. Motivated by the fact that different semantic representations or formalisms cover different aspects of sentence-level semantics, there has been a line of studies to apply multitask learning over different semantic annotations (Peng et al., 2017, 2018; Hershcovich et al., 2018; Kurita and Søgaard, 2019) or target cross-framework meaning representation (Oepen et"
2020.emnlp-main.663,P17-1186,0,0.0363018,"Missing"
2020.emnlp-main.663,N18-1135,0,0.0350164,"Missing"
2020.emnlp-main.663,N18-1202,0,0.0260677,"56.3 57.9 58.3 60.6 58.4 60.4 55.7 58.2 57.6 59.9 75.0 76.3 77.0 78.6 77.1 78.6 75.0 76.6 76.4 78.1 85.4 70.4 85.1 70.8 85.1 70.8 84.3 69.7 85.4 71.1 91.1 86.8 90.1 87.0 91.0 87.3 90.2 86.4 91.0 87.3 Single X X X X X X Mutitask X X X X X X Transfer + ELMO + mBERT Supervised Table 1: Results on the Czech SemEval test data. LF and UF denote Labeled and Unlabeled F1 respectively. The Transfer column does not use contextualized word embeddings. Task RNN refers to an extra task-specific embedding in the multitask setting. The shaded rows show results on out-of-domain test data. hyper-parameters as Peters et al. (2018). We use the pretrained multilingual BERT models (Devlin et al., 2019) of size 768 from Xiao (2018) with 12 layers and 12 heads. Due to computational limitations, we only use the pretrained BERT models in the input layer without finetuning. 6.1 Results Table 1 shows the results on in-domain and outof-domain data with and without contextual word embeddings. The Single row shows the baseline where we use Czech projection data to train the model. The Multitask rows show the results when we utilize syntactic parses through multitasking. The last column shows results of the supervised model trained"
2020.emnlp-main.663,D18-1263,0,0.0205682,"t study to develop an enhanced semantic depen8268 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8268–8274, c November 16–20, 2020. 2020 Association for Computational Linguistics dency parser through multitasking in the absence of annotated data. 2 Related Work After the SemEval shared tasks on broad-coverage semantic dependency parsing (Oepen et al., 2014, 2015; Che et al., 2016), there have been many studies to build supervised SDP models (Du et al., 2015; Chen et al., 2018; Almeida and Martins, 2015; Wang et al., 2018; Dozat and Manning, 2018; Stanovsky and Dagan, 2018; Kurita and Søgaard, 2019), however, all efforts were restricted to the three languages released through SemEval shared tasks. There have been extensive number of studies that use annotation projection to cure data scarcity in different tasks such as part-of-speech tagging (Täckström et al., 2013), syntactic parsing (McDonald et al., 2011), semantic role labeling (Padó and Lapata, 2005) and semantic parsing (Hershcovich et al., 2019). Nevertheless, none of the previous works, to the best of our knowledge, looked into using annotation projection for building SDP models for languages without se"
2020.emnlp-main.663,W08-2121,0,\N,Missing
2020.emnlp-main.663,H01-1035,0,\N,Missing
2020.emnlp-main.663,C98-1013,0,\N,Missing
2020.emnlp-main.663,P07-1123,0,\N,Missing
2020.emnlp-main.663,P15-1039,0,\N,Missing
2020.emnlp-main.663,P13-1023,0,\N,Missing
2020.emnlp-main.663,R11-1017,0,\N,Missing
2020.emnlp-main.663,W13-2322,0,\N,Missing
2020.emnlp-main.663,Q13-1018,0,\N,Missing
2020.emnlp-main.663,N18-1104,0,\N,Missing
2020.emnlp-main.663,D18-1412,0,\N,Missing
2020.lrec-1.215,W19-5906,0,0.0159006,"ive statistics that summarizes a data collection by a group of unsupervised measures such as mean or median for central tendency, variance or minimum-maximum for dispersion, skewness for symmetry, and kurtosis for heavy-tailed analysis. In recent years, text classification, a category of Natural Language Processing (NLP) tasks, has drawn much attention (Zhang et al., 2015; Joulin et al., 2016; Howard and Ruder, 2018) for its wide-ranging real-world applications such as fake news detection (Shu et al., 2017), document classification (Yang et al., 2016), and spoken language understanding (SLU) (Gupta et al., 2019a; Gupta et al., 2019b; Zhang et al., 2018), a core task of conversational assistants like Amazon Alexa or Google Assistant. However, there are still insufficient characteristic metrics to describe a collection of texts. Unlike numeric or categorical data, simple descriptive statistics alone such as word counts and vocabulary size are difficult to capture the syntactic and semantic properties of a text collection. In this work, we propose a set of characteristic metrics: diversity, density, and homogeneity to quantitatively summarize a collection of texts where the unit of texts could be a phr"
2020.lrec-1.215,D19-1127,1,0.856168,"Missing"
2020.lrec-1.215,P18-1031,0,0.0639551,"ribe or summarize the properties of a data collection. These metrics generally do not use groundtruth labels and only measure the intrinsic characteristics of data. The most prominent example is descriptive statistics that summarizes a data collection by a group of unsupervised measures such as mean or median for central tendency, variance or minimum-maximum for dispersion, skewness for symmetry, and kurtosis for heavy-tailed analysis. In recent years, text classification, a category of Natural Language Processing (NLP) tasks, has drawn much attention (Zhang et al., 2015; Joulin et al., 2016; Howard and Ruder, 2018) for its wide-ranging real-world applications such as fake news detection (Shu et al., 2017), document classification (Yang et al., 2016), and spoken language understanding (SLU) (Gupta et al., 2019a; Gupta et al., 2019b; Zhang et al., 2018), a core task of conversational assistants like Amazon Alexa or Google Assistant. However, there are still insufficient characteristic metrics to describe a collection of texts. Unlike numeric or categorical data, simple descriptive statistics alone such as word counts and vocabulary size are difficult to capture the syntactic and semantic properties of a t"
2020.lrec-1.215,P17-4015,0,0.0201776,"ignificantly improving many NLP tasks. Among the most popular approaches are ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018). In this work, we adopt BERT, a transformer-based technique for NLP pretraining, as the backbone to embed a sentence or a paragraph into a representation vector. Another stream of related works is the evaluation metrics for cluster analysis. As measuring property or quality of outputs from a clustering algorithm is difficult, human judgment with cluster visualization tools (Kwon et al., 2017; Kessler, 2017) are often used. There are unsupervised metrics to measure the quality of a clustering result such as the Calinski-Harabasz score (Cali´nski and Harabasz, 1974), the Davies-Bouldin index (Davies and Bouldin, 1979), and the Silhouette coefficients (Rousseeuw, 1987). Complementary to these works that model cross-cluster similarities or relationships, our proposed diversity, density and homogeneity metrics focus on the characteristics of each single cluster, i.e., intra cluster rather than inter cluster relationships. 3. Proposed Characteristic Metrics We introduce our proposed diversity, density"
2020.lrec-1.215,D14-1162,0,0.0847869,"Missing"
2020.lrec-1.215,N18-1202,0,0.00924372,"hought vectors (Kiros et al., 2015), and self-attentive sentence encoders (Lin et al., 2017). 1739 More recently, there is a paradigm shift from noncontextualized word embeddings to self-supervised language model (LM) pretraining. Language encoders are pretrained on a large text corpus using a LM-based objective and then re-used for other NLP tasks in a transfer learning manner. These methods can produce contextualized word representations, which have proven to be effective for significantly improving many NLP tasks. Among the most popular approaches are ULMFiT (Howard and Ruder, 2018), ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2018). In this work, we adopt BERT, a transformer-based technique for NLP pretraining, as the backbone to embed a sentence or a paragraph into a representation vector. Another stream of related works is the evaluation metrics for cluster analysis. As measuring property or quality of outputs from a clustering algorithm is difficult, human judgment with cluster visualization tools (Kwon et al., 2017; Kessler, 2017) are often used. There are unsupervised metrics to measure the quality of a clustering result such as the Calinski-Harabas"
2020.lrec-1.215,P13-1045,0,0.0105301,"here ei ∈ RH . A text collection {x1 , ..., xm }, i.e., a set of token sequences, is then transformed into a group of Hdimensional vectors {e1 , ..., em }. 4 https://gluon-nlp.mxnet.io/model_zoo/ bert/index.html We compute each metric as described previously, using three BERT layers L1, L6, and L12 as the embedding space, respectively. The calculated metric values are averaged over layers for each class and averaged over classes weighted by class size as the final value for a dataset. 5.2. Experimental Setup In the first task, we use the SST-2 (Stanford Sentiment Treebank, version 2) dataset (Socher et al., 2013) to conduct sentiment analysis experiments. SST-2 is a sentence binary classification dataset with train/dev/test splits provided and two types of sentence labels, i.e., positive and negative. The second task involves two essential problems in SLU, which are intent classification (IC) and slot labeling (SL). In IC, the model needs to detect the intention of a text input (i.e., utterance, conveys). For example, for an input of I want to book a flight to Seattle, the intention is to book a flight ticket, hence the intent class is bookFlight. In SL, the model needs to extract the semantic entitie"
2020.lrec-1.215,N16-1174,0,0.0461291,"aracteristics of data. The most prominent example is descriptive statistics that summarizes a data collection by a group of unsupervised measures such as mean or median for central tendency, variance or minimum-maximum for dispersion, skewness for symmetry, and kurtosis for heavy-tailed analysis. In recent years, text classification, a category of Natural Language Processing (NLP) tasks, has drawn much attention (Zhang et al., 2015; Joulin et al., 2016; Howard and Ruder, 2018) for its wide-ranging real-world applications such as fake news detection (Shu et al., 2017), document classification (Yang et al., 2016), and spoken language understanding (SLU) (Gupta et al., 2019a; Gupta et al., 2019b; Zhang et al., 2018), a core task of conversational assistants like Amazon Alexa or Google Assistant. However, there are still insufficient characteristic metrics to describe a collection of texts. Unlike numeric or categorical data, simple descriptive statistics alone such as word counts and vocabulary size are difficult to capture the syntactic and semantic properties of a text collection. In this work, we propose a set of characteristic metrics: diversity, density, and homogeneity to quantitatively summarize"
2020.lrec-1.215,P19-1519,0,0.02281,"Missing"
2020.nlp4convai-1.12,N18-2118,0,0.0613971,"e models, ELMo and BERT in our case, are complementary to these few-shot learning methods and yield further gains. 1 Slot Label O O O AddToPlaylist:artist AddToPlaylist:artist O AddToPlaylist:playlist owner O AddToPlaylist:playlist AddToPlaylist:playlist AddToPlaylist:playlist AddToPlaylist:playlist Figure 1: Tokens and corresponding slot labels for an utterance from the AddToPlaylist intent class in the S NIPS dataset prefixed by intent class name. As of late, most state-of-the-art IC/SF models are based on feed-forward, convolutional, or recurrent neural networks (Hakkani-T¨ur et al., 2016; Goo et al., 2018; Gupta et al., 2019). These neural models offer substantial gains in performance, but they often require a large number of labeled examples (on the order of hundreds) per intent class and slot-label to achieve these gains. The relative scarcity of large-scale datasets annotated with intents and slots prohibits the use of neural IC/SF models in many promising domains, such as medical consultation, where it is difficult to obtain large quantities of annotated dialogues. Accordingly, we propose the task of few-shot IC/SF, catering to domain adaptation in low resource scenarios, where there are o"
2020.nlp4convai-1.12,D14-1162,0,0.0839494,"ork Architecture We evaluate the network architectures depicted in Figure 2. These networks consist of an embedding layer, a sequence encoder, and two output layers for slots and intents, respectively. We greedily predict the slot-label for each token in the input sequence, according to the maximum output logit at each position. We plan to explore alternate search algorithms (e.g., beam search) in future work. Each architecture uses a different pre-trained embedding layer type, which are either non-contextual or contextual. We experiment with one noncontextual embedding, G LOV E word vectors (Pennington et al., 2014), as well as two contextual embeddings, G LOV E concatenated with ELM O embeddings (Peters et al., 2018), and BERT embeddings (Devlin et al., 2018). The sequence encoder is a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) with a 512-dimensional hidden state. Output layers are fully connected and take concatenated forward and backward LSTM hidden states as input. Pre-trained embeddings are kept frozen for training and adaptation. Attempts to fine-tune BERT led to inferior results. We refer to each architecture by its embedding type, namely G LOV E, ELM O, or BERT. Baseline We compare th"
2020.nlp4convai-1.12,H90-1021,0,\N,Missing
2020.nlp4convai-1.12,W18-2501,0,\N,Missing
2020.nlp4convai-1.12,D18-1300,0,\N,Missing
2020.nlp4convai-1.12,D18-1514,0,\N,Missing
2020.nlp4convai-1.12,N18-3018,0,\N,Missing
2021.acl-long.66,N19-1121,0,0.0215805,"optimizes four tasks to perform low-resource translation: (1) denoising autoencoder (2) adversarial training (3) high-resource translation and (4) low-resource backtranslation. We test our proposed method and demonstrate its effectiveness in improving low-resource translation from three distinct families: (1) Iberian languages, (2) Indic languages, and (3) Semitic languages, specifically Arabic dialects. We make our code and resources publicly available.2 2 Related Work Zero-shot translation Our work is closely related to that of zero-shot translation (Johnson et al., 2017; Chen et al., 2017; Al-Shedivat and Parikh, 2019). However, while zero-shot translation translates between a language pair with no parallel data, there is an assumption that both languages in the target pair have some parallel data with other languages. As such, the system can learn to process both languages. In one work, Currey and Heafield (2019) improved zero-shot translation using monolingual data on the pivot language. However, in our scenario, there is no parallel data between the low-resource language and any other language. In other work, Arivazhagan et al. (2019) showed that adding adversarial training to the encoder output could he"
2021.acl-long.66,D18-1549,0,0.0199215,"age. However, in our scenario, there is no parallel data between the low-resource language and any other language. In other work, Arivazhagan et al. (2019) showed that adding adversarial training to the encoder output could help zero shot training. We adopt a similar philosophy in our multi-task training to ensure our low-resource target is in the same latent space as the higher-resource language. Unsupervised translation A related set of work is the family of unsupervised translation techniques; these approaches translate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume tha"
2021.acl-long.66,2020.findings-emnlp.283,0,0.0611464,"es; these approaches translate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume that Y and Z are similar languages. These approaches leverage a cross-translation method on a multilingual NMT model where for a parallel data pair (Sx ,Sy ), they translate Sx into language Z with the current model to get Sz0 . Then use (Sy ,Sz0 ) as an additional synthesized data pair to further improve the model. Garcia et al. (2020b) experiment using multilingual cross-translation on low-resource languages with some success. While these approaches view the parallel data as auxiliary, to sup"
2021.acl-long.66,P19-1121,0,0.0235806,"?? ??? ??? ??? ???? Figure 1: Illustration of the training tasks for translating from English into a low-resource language (LRL) and from an LRL to English. English to low-resource backtranslation data. The aim of this task is to capture a language-modeling effect in the low-resource language. We describe how we obtain this data using the high-resource translation model to bootstrap backtranslation in Section 3.3. The objective used is, 0 Lbt = LCE (D(ZEn , [LRL]), XLRL ) (discriminators). The critics are recurrent networks to ensure that they can handle variable-length text input. Similar to Gu et al. (2019b), the adversarial component is trained using a Wasserstein loss, which is the difference of expectations between the two types of data. This loss minimizes the earth mover’s distance between the distributions of different languages. We compute the loss function as follows: (3) 0 , where ZEn = E(YEn , [En]). (YEn , XLRL ) is an English to low-resource backtranslation pair. Ladv1 = E[Disc(ZHRL )] − E[Disc(ZLRL )] (4) Task 4: Adversarial Training The final task aims to make the encoder output language-agnostic features. The representation is language agnostic to the noised high and low-resource"
2021.acl-long.66,2020.findings-emnlp.371,0,0.015344,"ranslate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume that Y and Z are similar languages. These approaches leverage a cross-translation method on a multilingual NMT model where for a parallel data pair (Sx ,Sy ), they translate Sx into language Z with the current model to get Sz0 . Then use (Sy ,Sz0 ) as an additional synthesized data pair to further improve the model. Garcia et al. (2020b) experiment using multilingual cross-translation on low-resource languages with some success. While these approaches view the parallel data as auxiliary, to supplement unsupervis"
2021.acl-long.66,2020.tacl-1.47,1,0.896063,"ng to ensure our low-resource target is in the same latent space as the higher-resource language. Unsupervised translation A related set of work is the family of unsupervised translation techniques; these approaches translate between language pairs with no parallel corpus of any kind. In work by Artetxe et al. (2018); Lample et al. (2018a), unsupervised translation is performed by training denoising autoencoding and backtranslation tasks concurrently. In these approaches, multiple pretraining methods were proposed to better initialize the model (Lample et al., 2018b; Lample and Conneau, 2019; Liu et al., 2020; Song et al., 2019). 2 https://github.com/wjko2/NMT-Adapt Different approaches were proposed that used parallel data between X-Y to improve unsupervised translation between X-Z (Garcia et al., 2020a; Li et al., 2020; Wang et al., 2020). This scenario differs from our setting as it does not assume that Y and Z are similar languages. These approaches leverage a cross-translation method on a multilingual NMT model where for a parallel data pair (Sx ,Sy ), they translate Sx into language Z with the current model to get Sz0 . Then use (Sy ,Sz0 ) as an additional synthesized data pair to further im"
2021.acl-long.66,D19-1632,1,0.901445,"Missing"
2021.acl-long.66,D18-1103,0,0.0196006,"to further improve the model. Garcia et al. (2020b) experiment using multilingual cross-translation on low-resource languages with some success. While these approaches view the parallel data as auxiliary, to supplement unsupervised NMT, our work looks at the problem from a domain adaptation perspective. We attempt to use monolingual data in Z to make the supervised model trained on X-Y generalize to Z. Leveraging High-resource Languages to Improve Low-resource Translation Several works have leveraged data in high-resource languages to improve the translation of similar low-resource languages. Neubig and Hu (2018) showed that it is beneficial to mix the limited parallel data pairs of low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020)"
2021.acl-long.66,2013.iwslt-papers.2,1,0.837467,"Missing"
2021.acl-long.66,L18-1548,0,0.0979839,"Missing"
2021.acl-long.66,W18-6316,0,0.0238497,"n language could be mapped word by word into the dialect vocabulary, and they calculate the corresponding word for substitution using 803 localized projection. This approach differs from our work in that it relies on the existence of a seed bilingual lexicon to the dialect/similar language. Additionally, the approach only considers translating from a dialect to English and not the reverse direction. Other work trains a massively multilingual many-to-many model and demonstrates that high-resource training data improves related lowresource language translation (Fan et al., 2020). In other work, Lakew et al. (2018) compared ways to model translations of different language varieties, in the setting that parallel data for both varieties is available, the variety for some pairs may not be labeled. Another line of work focus on translating between similar languages. In one such work, Pourdamghani and Knight (2017) learned a character-based cipher model. In other work, Wan et al. (2020) improved unsupervised translation between the main language and the dialect by separating the token embeddings into pivot and private parts while performing layer coordination. 3 Method We describe the NMT-Adapt approach to t"
2021.acl-long.66,D17-1266,0,0.0184878,". Additionally, the approach only considers translating from a dialect to English and not the reverse direction. Other work trains a massively multilingual many-to-many model and demonstrates that high-resource training data improves related lowresource language translation (Fan et al., 2020). In other work, Lakew et al. (2018) compared ways to model translations of different language varieties, in the setting that parallel data for both varieties is available, the variety for some pairs may not be labeled. Another line of work focus on translating between similar languages. In one such work, Pourdamghani and Knight (2017) learned a character-based cipher model. In other work, Wan et al. (2020) improved unsupervised translation between the main language and the dialect by separating the token embeddings into pivot and private parts while performing layer coordination. 3 Method We describe the NMT-Adapt approach to translating a low-resource language into and out of English without utilizing any low-resource language parallel data. In Section 3.1, we describe how NMT-Adapt leverages a novel multi-task domain adaptation approach to translating English into a low-resource language. In Section 3.2, we then describe"
2021.acl-long.66,N18-2084,0,0.063027,"Missing"
2021.acl-long.66,2020.semeval-1.271,0,0.033269,"Missing"
2021.acl-long.66,P16-1009,0,0.165315,"Missing"
2021.acl-long.66,2020.acl-main.252,0,0.0202162,"s. Neubig and Hu (2018) showed that it is beneficial to mix the limited parallel data pairs of low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020) trained multilingual translation and denoising simultaneously, and showed that the model could translate languages without parallel data into English near the performance of supervised multilingual NMT. Similar language translation Similar to our work, there have been methods proposed that leverage similar languages to improve translation. Hassan et al. (2017) generated synthetic English-dialect parallel data from English-main language corpus. However, this method assumes that the vocabulary in the main language could be mapped word by word into the dialect vocabulary, and they calculate the"
2021.acl-long.66,tiedemann-2012-parallel,0,0.262167,"Missing"
2021.acl-long.66,2020.lrec-1.494,1,0.836882,"Missing"
2021.acl-long.66,P19-1579,0,0.0175642,"domain adaptation perspective. We attempt to use monolingual data in Z to make the supervised model trained on X-Y generalize to Z. Leveraging High-resource Languages to Improve Low-resource Translation Several works have leveraged data in high-resource languages to improve the translation of similar low-resource languages. Neubig and Hu (2018) showed that it is beneficial to mix the limited parallel data pairs of low-resource languages with high-resource language data. Lakew et al. (2019) proposed selecting high-resource language data with lower perplexity in the low-resource language model. Xia et al. (2019) created synthetic sentence pairs by unsupervised machine translation, using the high-resource language as a pivot. However these previous approaches emphasize translating from the low-resource language to English, while the opposite direction is either unconsidered or shows poor translation performance. Siddhant et al. (2020) trained multilingual translation and denoising simultaneously, and showed that the model could translate languages without parallel data into English near the performance of supervised multilingual NMT. Similar language translation Similar to our work, there have been me"
2021.acl-long.66,P11-2007,0,0.0962119,"Missing"
2021.acl-short.15,2020.acl-main.619,0,0.0552378,"Missing"
2021.acl-short.15,2020.acl-main.690,0,0.0943773,"even small reduction in BLEU could result in more instances of biased translations, especially in female context sentences. 6 with the additional modified sentences, the augmented data set equally represents both genders. Vanmassenhove et al. (2018), Stafanoviˇcs et al. (2020) and Saunders et al. (2020) propose a dataannotation scheme in which the NMT model is trained to obey gender-specific tags provided with the source sentence. While Escud´e Font and Costa-juss`a (2019) employ pre-trained wordembeddings which have undergone a “debiasing” process (Bolukbasi et al., 2016; Zhao et al., 2018). Saunders and Byrne (2020) and Costa-juss`a and de Jorge (2020) propose domain-adaptation on a carefully curated data set that “corrects” the model’s misgendering problems. Costa-juss`a et al. (2020) consider variations involving the amount of parameter-sharing between different language directions in multilingual NMT models. Related Work Previous research investigating gender bias in NMT has focused on data bias, ranging from assessment to mitigation. For example, Stanovsky et al. (2019b) adapted an evaluation data set for co-reference resolution to measure gender biases in machine translation. The sentences in this t"
2021.acl-short.15,2020.gebnlp-1.4,0,0.0395269,"Missing"
2021.acl-short.15,P16-1162,0,0.0338986,"th {(6, 4), (6, 2), (6, 1)}. For each of these 7 configurations, we train AAN versions. Next, we save quantized and non-quantized versions for the 14 models, and decode with beam sizes of 1 and 5. We repeat our analysis for English to Spanish and English to German directions, using WMT13 En-Es and WMT14 En-De data sets, respectively. For the En-Es we limited the training data to 4M sentence pairs (picked at random without replacement) to ensure that the training for the two language directions have comparable data sizes. We apply Byte-Pair Encoding (BPE) with 32k merge operations to the data (Sennrich et al., 2016). We measure decoding times and BLEU scores for the model’s translations using the WMT test sets. Next, we evaluate each model’s performance on SimpleGEN, specifically calculating the percent of correctly gendered nouns, incorrectly gendered nouns as well as inconclusive results. Table 3 shows an example of our evaluation protocol for an example source sentences and four possible translations. We deem the first two as correct even though the second translation incorrectly translates “funny” as “feliz” since we focus on the translation of “physician” only. The third translation is deemed incorr"
2021.acl-short.15,2020.wmt-1.73,0,0.0222507,"Missing"
2021.acl-short.15,P19-1164,0,0.135079,"“lady” when translating the occupation-noun, rendering it in with the masculine gender “doctor/m´edico”. and their impact on gender biases in an NMT system, complementing existing work on data bias. We explore optimizations choices such as (i) search (changing the beam size in beam search); (ii) architecture configurations (changing the number of encoder and decoder layers); (iii) model based speedups (using Averaged attention networks (Zhang et al., 2018)); and (iv) 8-bit quantization of a trained model.. Prominent prior work on gender bias evaluation forces the system to “guess” the gender (Stanovsky et al., 2019a) of certain occupation nouns in the source sentence. Consider, the English source sentence “That physician is funny.”, containing no information regarding the physician’s gender. When translating this sentence into Spanish (where the occupation nouns are explicitly specified for gender), an NMT model is forced to guess the gender of the physician and choose between masculine forms, doctor/m´edico or feminine forms doctora/m´edica. While investigating bias in these settings is valuable, in this paper, we hope to highlight that the problem is much worse — despite an explicit gender reference i"
2021.acl-short.15,D18-1334,0,0.0640033,"Missing"
2021.acl-short.15,P19-1176,0,0.0190096,"horses in NLP and MT (Vaswani et al., 2017). While transformers are faster to train compared to their predecessors, Recurrent Neural Network (RNN) encoderdecoders (Bahdanau et al., 2014; Luong et al., 2015), transformers suffer from slower decoding speed. Subsequently, there has been interest in improving the decoding speed of transformers. Shallow Decoders (SD): Shallow decoder models simply reduce the decoder depth and increase the encoder depth in response to the observation that decoding latency is proportional to the number of decoder layers (Kim et al., 2019; Miceli Barone et al., 2017; Wang et al., 2019; Kasai et al., 2020). Alternatively, one can employ SD models without increasing the encoder layers resulting in smaller (and faster) models. Average Attention Networks (AAN): Average Attention Networks reduce the quadratic complexity of the decoder attention mechanism to linear time by replacing the decoder-side self-attention with an average-attention operation using a fixed weight for all time-steps (Zhang et al., 2018). This results in a ≈ 3-4x decoding speedup over the standard transformer. 4 Experimental Setup Our objective is not to compare the various optimization methods against each"
2021.acl-short.15,P18-1166,0,0.0690817,":::: Table 1: Translation of a simple source sentence by 4 different commercial English to Spanish MT systems. All of these systems fail to consider the token “lady” when translating the occupation-noun, rendering it in with the masculine gender “doctor/m´edico”. and their impact on gender biases in an NMT system, complementing existing work on data bias. We explore optimizations choices such as (i) search (changing the beam size in beam search); (ii) architecture configurations (changing the number of encoder and decoder layers); (iii) model based speedups (using Averaged attention networks (Zhang et al., 2018)); and (iv) 8-bit quantization of a trained model.. Prominent prior work on gender bias evaluation forces the system to “guess” the gender (Stanovsky et al., 2019a) of certain occupation nouns in the source sentence. Consider, the English source sentence “That physician is funny.”, containing no information regarding the physician’s gender. When translating this sentence into Spanish (where the occupation nouns are explicitly specified for gender), an NMT model is forced to guess the gender of the physician and choose between masculine forms, doctor/m´edico or feminine forms doctora/m´edica. W"
2021.acl-short.15,D18-1521,0,0.0155237,"eotypical cases and even small reduction in BLEU could result in more instances of biased translations, especially in female context sentences. 6 with the additional modified sentences, the augmented data set equally represents both genders. Vanmassenhove et al. (2018), Stafanoviˇcs et al. (2020) and Saunders et al. (2020) propose a dataannotation scheme in which the NMT model is trained to obey gender-specific tags provided with the source sentence. While Escud´e Font and Costa-juss`a (2019) employ pre-trained wordembeddings which have undergone a “debiasing” process (Bolukbasi et al., 2016; Zhao et al., 2018). Saunders and Byrne (2020) and Costa-juss`a and de Jorge (2020) propose domain-adaptation on a carefully curated data set that “corrects” the model’s misgendering problems. Costa-juss`a et al. (2020) consider variations involving the amount of parameter-sharing between different language directions in multilingual NMT models. Related Work Previous research investigating gender bias in NMT has focused on data bias, ranging from assessment to mitigation. For example, Stanovsky et al. (2019b) adapted an evaluation data set for co-reference resolution to measure gender biases in machine translati"
2021.acl-short.53,N19-1391,1,0.894286,"Missing"
2021.acl-short.53,D19-1380,1,0.864127,"guage is inefficient and computationally expensive, let alone the impact on the environment. Thus, utilizing a non-parameterized model with ready-to-use word embeddings is an efficient alternative to generate sentence representations in various languages. A number of non-parameterized models have been proposed to derive sentence representations from pre-trained word embeddings (R¨uckl´e et al., 2018; Yang et al., 2019; Kayal and Tsatsaronis, 2019). However, most of these models, including averaging, disregard structure information, which is an important aspect of any given language. Recently, Almarwani et al. (2019) proposed a structure-sensitive sentence encoder, which utilizes Discrete Cosine Transform (DCT) as an efficient alternative to averaging. The authors show that this approach is versatile and scalable because it relies only on word embeddings, which can be easily obtained from large unlabeled data. Hence, in principle, this approach can be adapted to different languages. Furthermore, having an efficient, readyto-use language-independent sentence encoder can enable knowledge transfer between different languages in cross-lingual settings, empowering the development of efficient and performant NL"
2021.acl-short.53,Q17-1010,0,0.0512087,"s to word-level mapping, (sent) refers to sentence-level mapping, and (dict) refers to the baseline (using a static dictionary for mapping). Bold shows the best overall result. Table 3: Accuracy using in-domain FastText (FT) and Contextualized mBERT embeddings. The best results for each row in Bold & for each direction in gray . In-domain embeddings: To ensure comparability to state-of-the-art results, we further utilized indomain FastText embeddings as those used in (Aldarmaki and Diab, 2019) as well as contextualizedbased word embeddings. For the in-domain FastText embeddings, the FastText (Bojanowski et al., 2017) is utilized to generate word embeddings from 1 Billion Word benchmark (Chelba et al., 2014) for English, and equivalent subsets of about 400 million tokens from WMT’13 (Bojar et al., 2013) news crawl data. For the contextualized-based embeddings, we utilized multilingual BERT (mBERT) introduced in (Devlin et al., 2019) as contextual word embeddings, in which representations from the last BERT layer are taken as word embeddings. As shown in Table 3, using in-domain word embeddings yields stronger results compared to the pre-trained embeddings we use in the previous experiments as illustrated i"
2021.acl-short.53,L18-1269,0,0.0246824,"of linguistic properties including surface, syntactic, and semantic information (Conneau et al., 2018; Ravishankar et al., 2019). Ravishankar et al. (2019) used the datasets to evaluate different sentence encoders trained by mapping sentence representations to English. Unlike Ravishankar et al. (2019), we use the datasets to evaluate DCT embeddings for each language independently. As a baseline, in addition to the DCT embeddings, we use vector averaging to extract sentence representations from the pre-trained embeddings. For model evaluations, we utilize the SentEval framework introduced in (Conneau and Kiela, 2018). In all experiments, we use a single-layer MLP on top of DCT sentence embeddings with the following parameters: kfold=10, batch size=128, nhid=50, optim=adam, tenacity=5, epoch size=4. 420 1 Unlike (Almarwani et al., 2019), we note no further improvements with larger coefficients, thus, we only report the results of 1 ≤ K ≤ 4. 2 Refer to (Conneau et al., 2018) and (Ravishankar et al., 2019) for more details about the probing tasks. Figure 1: Results of the probing tasks comparing XX languages performance relative to English. White indicates a value of 1, demonstrating parity in performance wi"
2021.acl-short.53,W19-4318,0,0.101939,"ntence matrix N × d, a sequence of DCT coefficients c[0], c[1], ..., c[K] are calculated by applying the DCT type II along the d-dimensional word embeddings, where c[K] = q 2 PN −1 π 1 n=0 vn cos N (n + 2 )K (Shao and JohnN son, 2008). Finally, a fixed-length sentence vector of size Kd is generated by concatenating the first Description Length prediction Word Content analysis Word order analysis Tree depth prediction Verb tense prediction Coordination Inversion Subject number prediction Object number prediction Semantic odd man out Table 1: Probing Tasks as described in (Conneau et al., 2018; Ravishankar et al., 2019). K DCT coefficients, which we refer to as c[0 : K].1 3 Multi-lingual DCT Embeddings 3.1 Experimental Setups and Results In our study, DCT is used to learn a separate encoder for each language from existing monolingual word embeddings. To evaluate DCT embeddings across different languages, we used the probing benchmark provided by Ravishankar et al. (2019), which includes a set of multi-lingual probing datasets.2 The benchmark covers five languages: English, French, German, Spanish and Russian, derived from Wikipedia. The task set comprises 9 probing tasks, summarized in Table 1, that address"
2021.findings-acl.120,2020.nlpcovid19-2.5,1,0.623006,"Missing"
2021.findings-acl.120,W19-5435,1,0.897806,"Missing"
2021.findings-acl.120,2020.acl-main.747,1,0.696864,"Missing"
2021.findings-acl.120,N19-1423,0,0.00646772,"on the source sentence in the bi-text corpus as the paraphrased target. Let D denote the paraphrased sentence of T and D0 denote the generation from BART conditioned on the noised D. Then we create pseudo labels of D0 denoted LD0 by computing the edit-distance between the D0 and D and use ((S, T, D0 ), LD0 ) as the training data for finetuning. Since the pseudo labels are created based on D, it can prevent the model from learning the edit-distance between T and D0 easily. We provide ablation studies in Appendix D. Masked LM loss We also add the masked language model loss (MLM) Lmlm following (Devlin et al., 2019). To learn this loss, we create a different batch from the above by concatenating only the source S and target T as the input, since the hallucinated target T 0 could provide erroneous information for predicting masked words in T . We find that such multi-task learning objective helps learn better representations of the input and further improves performance on predicting hallucination labels. The final loss is L = Lpred + α · Lmlm where α is a hyperparameter. 4 Evaluation Tasks and Data We examine hallucination in abstractive text summarization and machine translation (MT) tasks, using the mo"
2021.findings-acl.120,2020.acl-main.454,1,0.844635,"sed for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tailored for abstractive text summarization. However, these scores do not directly identify hal1393 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1393–1404 August 1–6, 2021. ©2021 Association for Computational Linguistics lucinated tokens and only correlate weakly with human judgements. We propose a new task for faithfulness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is hallucinated or faithful to the so"
2021.findings-acl.120,D19-1632,1,0.862838,"Missing"
2021.findings-acl.120,D16-1139,0,0.178743,"risingly find our model can generalize well without references, even when they were present during training. To prevent the model from overly relying on the true target T and learning spurious correlations (e.g. the edit distance), we explored two techniques: (1) dropout – randomly drop out tokens in T to force the dependence on the source input; (2) paraphrase – recall that at synthetic data generation time, we generate T 0 from BART conditioned on the noised T . Instead, we can apply noise functions to the paraphrased sentence of T . We create paraphrased targets via knowledge distillation (Kim and Rush, 2016) where we use the output from pretrained Seq2Seq model conditioned on the source sentence in the bi-text corpus as the paraphrased target. Let D denote the paraphrased sentence of T and D0 denote the generation from BART conditioned on the noised D. Then we create pseudo labels of D0 denoted LD0 by computing the edit-distance between the D0 and D and use ((S, T, D0 ), LD0 ) as the training data for finetuning. Since the pseudo labels are created based on D, it can prevent the model from learning the edit-distance between T and D0 easily. We provide ablation studies in Appendix D. Masked LM los"
2021.findings-acl.120,W19-5404,1,0.902402,"Missing"
2021.findings-acl.120,W17-3204,0,0.022642,"nce summaries. They randomly sampled 500 articles from the XS UM test set and evaluated summaries from four abstractive summarization systems: PtGen (See et al., 2017), TConvS2S (Narayan et al., 2018), TranS2S (Vaswani et al., 2017) and BERTS2S (Rothe et al., 2020). Maynez et al. (2020) asked human annotators to label the spans in the machine generated summaries if they were unfaithful to the article. We post-processed their human annotations by majority voting and created test datasets for each of the summarization systems. 4.2 MT Previous work (Wang and Sennrich, 2020; M¨uller et al., 2019; Koehn and Knowles, 2017) has shown that translation models are particularly prone to hallucination when tested out of domain. We similarly focus on this regime and additionally consider the low resource case where a modest amount of out of domain data is available at training time. Data We use a multi-domain Chinese-English (Zh-En) translation dataset (Wang et al., 2020b) which consists of four balanced domains: law, news, patent and subtitles. We create a new training data Dtrain with law (1.46M sentences), news (1.54M), subtitles (1.77M) train data and randomly sample 870 parallel sentences from the patent training"
2021.findings-acl.120,D18-1512,0,0.0477721,"Missing"
2021.findings-acl.120,2020.acl-main.703,1,0.885035,"token by computing the edit distance between T 0 and T . Labels of 1 refer to hallucinated words. a supervised model on this synthetic labeled data set of ((S, T 0 ), LT 0 ). The key challenge is that T 0 should be a fluent sentence that does not differ too much from T . Generation of hallucinated sentences To control this synthetic hallucination process, we build on a pre-trained denoising autoencoder, which maps a corrupted sentence back to the original text it was derived from, learning to reconstruct missing words that have been arbitrarily masked out. Specifically, we use the BART model (Lewis et al., 2020), without providing it any access to the source sentence, thereby encouraging it to insert new content as needed to ensure fluency. As shown in Fig. 2, we first apply a noising function that removes words from the original target sentence T 4 and then use a pretrained BART to generate T 0 conditioned on the noised T with beam search. T Mike T&apos; Jerry 1 goes to the bookstore on happily goes to the bookstore with his friend. 1 0 0 1 1 1 0 0 Thursday. Figure 3: An example of label assignment. Label assignments After obtaining the hallucinated sentence T 0 with BART, we need to assign appropriate l"
2021.findings-acl.120,W04-1013,0,0.0302168,"work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tai"
2021.findings-acl.120,W18-6478,0,0.0259819,"l. (2020), which injects two types of noise into the input sentences: (1) paraphrase noise created by round-trip translations, and (2) random noise from dropping, mask5 We also tried removing hallucinated target words before training. This underperformed, likely because it produces too many ungrammatical target sentences. Case Study II: Improving Corpus Filtering for Low-Resource MT High-quality parallel data is critical for training effective neural MT systems, but acquiring it can be expensive and time-consuming. Many systems instead use mined and filtered parallel data to train NMT models (Junczys-Dowmunt, 2018; Zhang et al., 2020; Koehn et al., 2019). Nonetheless, the selected parallel data can still be noisy, containing misaligned segments. In this section, we demonstrate that token-level hallucination labels can allow us to make better use of noisy data to and improve the overall translation quality. We apply the token loss truncation method proposed in §6 to the filtered parallel data and evaluate it on the WMT2019 low-resource parallel corpus filtering shared task. Experimental Setup The WMT19 shared task focuses on two low-resource languages – Nepali and Sinhala. It released a very noisy 40.6"
2021.findings-acl.120,2020.tacl-1.47,1,0.838219,"ock are our results. Bold indicates best results not using references. the patent test data. In addition, we also test the NMT models on the COVID-19 domain, sampling 100 examples from the dataset of Anastasopoulos et al. (2020). We denote this 250-sentence dataset as Deval and ask human annotators to evaluate the level of hallucinations thereof. Models Our data is generated from two models on which we will measure hallucination (see Appendix B for more details): (1) TranS2S (Vaswani et al., 2017) is the standard Transformer Seq2Seq model with 6 encoder layers and 6 decoder layers. (2) MBART (Liu et al., 2020) is a Seq2Seq denoising auto-encoder pretrained on large-scale monolingual corpora in many languages. We finetune the 12 layer model on Dtrain . 5 5.1 Experiments Experimental setup Synthetic Data Generation We use a pretrained 12 layer BART (Lewis et al., 2020) model in the fairseq toolkit (Ott et al., 2019) for synthetic labeled data generation. We uniformly sample the percentage of tokens pm to mask from [0, hm ] for each sentence. We also uniformly sample the probability of replacing a token with a random token from [0, hr ] denoted pr . pm and pr are two important factors that affect the"
2021.findings-acl.120,2020.acl-main.66,0,0.0496903,"Missing"
2021.findings-acl.120,2021.ccl-1.108,0,0.0517646,"Missing"
2021.findings-acl.120,P19-3020,0,0.0349607,"Missing"
2021.findings-acl.120,W19-6623,0,0.0233024,"seline methods. We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings 1 . 1 1 1 0 0 0 0 1 1 1 Jerry happily goes to the bookstore with his friend. Figure 1: An example of token-level hallucination detection from MT. The grey box is an example of MT output and the labels above indicate if each word is faithful (0) to the input or hallucinated (1). tency (Marcus and Davis, 2020), are dull and repetitive (Welleck et al., 2019), or contain hallucinated content that is not entailed by the input (Maynez et al., 2020; Martindale et al., 2019). In this paper, we focus on tackling the latter problem, aiming to automatically identify and quantify content in the output that is not faithful to the input text. Neural sequence models for tasks such as data-totext generation (Puduppully et al., 2019), machine translation (MT; Vaswani et al. (2017); Wu et al. (2016)) and text summarization (Rothe et al., 2020) can often generate fluent text that is sometimes preferred to human-written content (L¨aubli et al., 2018; Brown et al., 2020). However, they also often generate texts that lack global logical consis∗ 。 Machine Translation Introducti"
2021.findings-acl.120,2020.acl-main.173,0,0.114553,"ements over strong baseline methods. We also apply our method to word-level quality estimation for MT and show its effectiveness in both supervised and unsupervised settings 1 . 1 1 1 0 0 0 0 1 1 1 Jerry happily goes to the bookstore with his friend. Figure 1: An example of token-level hallucination detection from MT. The grey box is an example of MT output and the labels above indicate if each word is faithful (0) to the input or hallucinated (1). tency (Marcus and Davis, 2020), are dull and repetitive (Welleck et al., 2019), or contain hallucinated content that is not entailed by the input (Maynez et al., 2020; Martindale et al., 2019). In this paper, we focus on tackling the latter problem, aiming to automatically identify and quantify content in the output that is not faithful to the input text. Neural sequence models for tasks such as data-totext generation (Puduppully et al., 2019), machine translation (MT; Vaswani et al. (2017); Wu et al. (2016)) and text summarization (Rothe et al., 2020) can often generate fluent text that is sometimes preferred to human-written content (L¨aubli et al., 2018; Brown et al., 2020). However, they also often generate texts that lack global logical consis∗ 。 Mach"
2021.findings-acl.120,D18-1206,0,0.0429095,"hyperparameter. 4 Evaluation Tasks and Data We examine hallucination in abstractive text summarization and machine translation (MT) tasks, using the models and datasets described below. 4.1 Abstractive Text Summarization Maynez et al. (2020) studied hallucination problems in extreme summarization on the XS UM dataset which comprises 226,711 British Broadcasting Corporation (BBC) articles paired with their single-sentence summaries. They randomly sampled 500 articles from the XS UM test set and evaluated summaries from four abstractive summarization systems: PtGen (See et al., 2017), TConvS2S (Narayan et al., 2018), TranS2S (Vaswani et al., 2017) and BERTS2S (Rothe et al., 2020). Maynez et al. (2020) asked human annotators to label the spans in the machine generated summaries if they were unfaithful to the article. We post-processed their human annotations by majority voting and created test datasets for each of the summarization systems. 4.2 MT Previous work (Wang and Sennrich, 2020; M¨uller et al., 2019; Koehn and Knowles, 2017) has shown that translation models are particularly prone to hallucination when tested out of domain. We similarly focus on this regime and additionally consider the low resour"
2021.findings-acl.120,N19-4009,0,0.0150506,"e the level of hallucinations thereof. Models Our data is generated from two models on which we will measure hallucination (see Appendix B for more details): (1) TranS2S (Vaswani et al., 2017) is the standard Transformer Seq2Seq model with 6 encoder layers and 6 decoder layers. (2) MBART (Liu et al., 2020) is a Seq2Seq denoising auto-encoder pretrained on large-scale monolingual corpora in many languages. We finetune the 12 layer model on Dtrain . 5 5.1 Experiments Experimental setup Synthetic Data Generation We use a pretrained 12 layer BART (Lewis et al., 2020) model in the fairseq toolkit (Ott et al., 2019) for synthetic labeled data generation. We uniformly sample the percentage of tokens pm to mask from [0, hm ] for each sentence. We also uniformly sample the probability of replacing a token with a random token from [0, hr ] denoted pr . pm and pr are two important factors that affect the noise level when generating the synthetic data. For MT, we set hm and hr to 0.6 and 0.3 respectively. For abstractive summarization, we use 0.4 and 0.2. We use beam search for decoding from BART with beam size of 4 and length penalty of 3. For MT, we first create paraphrased target sentences D0 through knowle"
2021.findings-acl.120,P02-1040,0,0.111696,"s∗ 。 Machine Translation Introduction Most work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez"
2021.findings-acl.120,W18-6319,0,0.0420902,"n Introduction Most work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020"
2021.findings-acl.120,2020.tacl-1.18,0,0.170716,"each word is faithful (0) to the input or hallucinated (1). tency (Marcus and Davis, 2020), are dull and repetitive (Welleck et al., 2019), or contain hallucinated content that is not entailed by the input (Maynez et al., 2020; Martindale et al., 2019). In this paper, we focus on tackling the latter problem, aiming to automatically identify and quantify content in the output that is not faithful to the input text. Neural sequence models for tasks such as data-totext generation (Puduppully et al., 2019), machine translation (MT; Vaswani et al. (2017); Wu et al. (2016)) and text summarization (Rothe et al., 2020) can often generate fluent text that is sometimes preferred to human-written content (L¨aubli et al., 2018; Brown et al., 2020). However, they also often generate texts that lack global logical consis∗ 。 Machine Translation Introduction Most work was done during an internship at FAIR. Codes and data available at https://github.com/ violet-zct/fairseq-detect-hallucination. 1 (Source meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building mode"
2021.findings-acl.120,2020.findings-emnlp.147,0,0.0501994,"Missing"
2021.findings-acl.120,P17-1099,0,0.0248872,"pred + α · Lmlm where α is a hyperparameter. 4 Evaluation Tasks and Data We examine hallucination in abstractive text summarization and machine translation (MT) tasks, using the models and datasets described below. 4.1 Abstractive Text Summarization Maynez et al. (2020) studied hallucination problems in extreme summarization on the XS UM dataset which comprises 226,711 British Broadcasting Corporation (BBC) articles paired with their single-sentence summaries. They randomly sampled 500 articles from the XS UM test set and evaluated summaries from four abstractive summarization systems: PtGen (See et al., 2017), TConvS2S (Narayan et al., 2018), TranS2S (Vaswani et al., 2017) and BERTS2S (Rothe et al., 2020). Maynez et al. (2020) asked human annotators to label the spans in the machine generated summaries if they were unfaithful to the article. We post-processed their human annotations by majority voting and created test datasets for each of the summarization systems. 4.2 MT Previous work (Wang and Sennrich, 2020; M¨uller et al., 2019; Koehn and Knowles, 2017) has shown that translation models are particularly prone to hallucination when tested out of domain. We similarly focus on this regime and add"
2021.findings-acl.120,2020.acl-main.704,0,0.0160313,"48 12.55 ST + seq loss truncation ST-R + seq loss truncation 19.91 19.37 -0.048 -0.057 8.26 10.06 ST + token loss truncation ST + decoder HS masking ST-R + token loss truncation ST-R + decoder HS masking 20.32 20.57 21.02 20.64 0.00244 -0.0001 0.043 0.0308 6.37 6.38 7.34 8.70 ing and shuffling input tokens. We also compare with the recently proposed loss truncation method (Kang and Hashimoto, 2020) that adaptively removes entire examples with high log loss, which was shown to reduce hallucinations. Results and Analysis We present the tokenized BLEU score (Papineni et al., 2002), BLEURT score (Sellam et al., 2020) and the percentage of hallucinated tokens predicted by our system in Tab. 4. We can see that ST improves over the baseline by around 3 BLEU and our best result further improves ST by 1.7 BLEU. Compared with strong baseline methods, our method not only achieves the best translation quality measured by BLEU and BLEURT but also the largest hallucination reduction. We also observe that: (1) Our method with ST alone can outperform other baseline methods, when combined with perturbed ST (noise), and using fine-grained control over the target tokens can further improve the results. (2) ST with parap"
2021.findings-acl.120,P16-1162,0,0.0953211,"Missing"
2021.findings-acl.120,2020.wmt-1.79,1,0.831521,"Missing"
2021.findings-acl.120,2011.mtsummit-papers.58,0,0.0597625,"lness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is hallucinated or faithful to the source input. This task does not use the reference output to assess faithfulness, which offers us the ability to also apply it at run-time. Similar to the spirit of our proposed task, word-level quality estimation (Specia et al., 2018; Fonseca et al., 2019) in the MT community predicts if tokens are correctly translated based on human post-editing. However, these methods generally do not distinguish errors in terms of fluency and adequacy (Specia et al., 2011), with the exception of a subset of the WMT 2020 shared task on quality estimation (Specia et al., 2020), where different types and levels of severity of word-level errors are defined. Our proposed task specifically focuses on hallucination errors, and we define these errors in a simpler way with only binary labels, which we argue makes them simpler to use and more conducive to labeling at large scale. The proposed hallucination detection method (described below) is also applicable to the word-level quality estimation task as demonstrated in §5.4. We measure hallucination for two conditional s"
2021.findings-acl.120,N03-1033,0,0.021265,"odels for Conditional Sequence Generation Recent work (Maynez et al., 2020) has shown that pretrained models are better at generating faithful summaries as evaluated by humans. In Tab. 2, summaries generated from BERTS2S contain significantly fewer hallucinations than other model outputs. We also confirmed this trend in MT that translations from MBART contain less hallucinated content than that from TranS2S. Analysis on Hallucinated Words and their Partof-Speech Tags In Fig. 5, we present the percentage of hallucinated tokens categorized by their part-of-speech tags predicted by a POS tagger (Toutanova et al., 2003). First, we see that for both MT and summarization datasets, nouns are the most hallucinated words. In abstractive summarization, verbs also account for a certain number of hallucinations. Second, our model predicted hallucinated words match well with gold annotations on the distributions of POS tags. We also compare the percentage of hallucinations within each POS tag in Appendix E.2. In addition, we provide more 1398 Normalized Hallucination Ratio Normalized Hallucination Ratio MT 0.6 Gold Our predictions 0.4 0.2 0.0 NN others JJ VB IN POS tag CD RB SYM PRP XSum 0.5 Gold Our predictions 0.4"
2021.findings-acl.120,N03-1000,0,0.110311,"Missing"
2021.findings-acl.120,2020.acl-main.450,0,0.299588,"t standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tailored for abstractive text summarization. However, these scores do not directly identify hal1393 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1393–1404 August 1–6, 2021. ©2021 Association for Computational Linguistics lucinated tokens and only correlate weakly with human judgements. We propose a new task for faithfulness assessment - hallucination detection at the token level, which aims to predict if each token in the machine output is hallucinate"
2021.findings-acl.120,2020.acl-main.326,0,0.261799,"rce meaning: Mike goes to the bookstore on Thursday. ) 迈 克周 四 去书店. Source Input The risk of generating unfaithful content impedes the safe deployment of neural sequence generation models. The first step to building models that do not suffer from these failures is the assessment and identification of such hallucinated outputs. Prior work has shown that standard metrics used for text evaluation, such as BLEU scores (Papineni et al., 2002; Post, 2018), ROUGE (Lin and Hovy, 2004) and BERTScore (Zhang et al., 2019), do not correlate well with the faithfulness of model outputs (Maynez et al., 2020; Wang and Sennrich, 2020; Tian et al., 2019). They also require reference output text, limiting their applicability in a deployed system at run-time. Very recent efforts have started to develop automatic metrics to measure the faithfulness of output sequences using external semantic models, e.g. the question-generation and question-answering systems (Wang et al., 2020a; Durmus et al., 2020) or textual entailment inference models (Maynez et al., 2020), to score faithfulness tailored for abstractive text summarization. However, these scores do not directly identify hal1393 Findings of the Association for Computational"
2021.findings-acl.120,2020.acl-main.756,0,0.0338433,"Missing"
2021.findings-acl.120,2020.acl-main.552,0,0.0260132,"Missing"
2021.findings-acl.120,N19-1161,1,0.858552,"Missing"
2021.findings-emnlp.387,N19-1423,0,0.010478,"ging other sampling strategies such as Query batched committee (QBC), Epsilon-Greedy (EG), and Ranked Batch (Batch-LC). Epsilon-Greedy (EG) In order to find a balance between exploration and exploitation, we use a method inspired by -greedy (EG) strategy in Reinforcement Learning. We implement this approach in two ways: inter-batch and intra-batch. Interbatch EG (EG-inter) selects query data at each iteration randomly with probability  otherwise chooses 4558 2 Values have been determined empirically on a tuning set. 4.1 Representation We use SOTA representations for this task, such as BERT (Devlin et al., 2019) and TweetBERT (Qudar and Mago, 2020). TweetBERT, a domain-specific BERT based language model trained specifically on social media data. TweetBERT was trained on about 680 million Tweets . We also use earlier representations such as GloVe (Pennington et al., 2014). For each sentence, we average the GloVe vector representation of all tokens in the sentence and use it as the input to the models. We use Twitter GloVe, which is consistent with the domain of our work. Twitter GloVe was trained over by 2B Tweets, 27B tokens, and 1.2M vocab. A dimensionality of 200 was determined empirically to yield"
2021.findings-emnlp.387,W16-0403,1,0.890103,"Missing"
2021.findings-emnlp.387,S19-2148,0,0.0610793,"Missing"
2021.findings-emnlp.387,D14-1162,0,0.0887687,"einforcement Learning. We implement this approach in two ways: inter-batch and intra-batch. Interbatch EG (EG-inter) selects query data at each iteration randomly with probability  otherwise chooses 4558 2 Values have been determined empirically on a tuning set. 4.1 Representation We use SOTA representations for this task, such as BERT (Devlin et al., 2019) and TweetBERT (Qudar and Mago, 2020). TweetBERT, a domain-specific BERT based language model trained specifically on social media data. TweetBERT was trained on about 680 million Tweets . We also use earlier representations such as GloVe (Pennington et al., 2014). For each sentence, we average the GloVe vector representation of all tokens in the sentence and use it as the input to the models. We use Twitter GloVe, which is consistent with the domain of our work. Twitter GloVe was trained over by 2B Tweets, 27B tokens, and 1.2M vocab. A dimensionality of 200 was determined empirically to yield best results.3 The representations are frozen during training. 4.2 Model We also examine different models that have been mainly used for short text classification tasks, namely, MLP (Hinton, 1990), Support Vector Machines (SVM) (Platt et al., 1999), Random Forest"
2021.findings-emnlp.387,D11-1147,0,0.155383,"Missing"
2021.findings-emnlp.387,P17-2067,0,0.0830839,"Missing"
abdul-mageed-diab-2012-awatif,J94-2004,0,\N,Missing
abdul-mageed-diab-2012-awatif,W11-0413,1,\N,Missing
abdul-mageed-diab-2012-awatif,W10-1401,0,\N,Missing
abdul-mageed-diab-2012-awatif,P11-2103,1,\N,Missing
abdul-mageed-diab-2012-awatif,W06-2915,0,\N,Missing
abdul-mageed-diab-2012-awatif,P99-1032,0,\N,Missing
abdul-mageed-diab-2014-sana,W04-3253,0,\N,Missing
abdul-mageed-diab-2014-sana,J90-1003,0,\N,Missing
abdul-mageed-diab-2014-sana,W11-0413,1,\N,Missing
abdul-mageed-diab-2014-sana,W12-3705,1,\N,Missing
abdul-mageed-diab-2014-sana,baccianella-etal-2010-sentiwordnet,0,\N,Missing
abdul-mageed-diab-2014-sana,kamps-etal-2004-using,0,\N,Missing
abdul-mageed-diab-2014-sana,C04-1200,0,\N,Missing
abdul-mageed-diab-2014-sana,P11-2103,1,\N,Missing
abdul-mageed-diab-2014-sana,P02-1053,0,\N,Missing
abdul-mageed-diab-2014-sana,P97-1023,0,\N,Missing
abdul-mageed-diab-2014-sana,R11-1096,1,\N,Missing
abdul-mageed-diab-2014-sana,abdul-mageed-diab-2012-awatif,1,\N,Missing
abdul-mageed-diab-2014-sana,S12-1033,0,\N,Missing
C10-2117,J99-2004,0,0.0120921,"for the sentence Republican leader Bill Frist said the Senate was hijacked. said Frist hijacked Senate Republican leader Bill was the In the above sentence, said and hijacked are the propositions that should be tagged. Let’s look at hijacked in detail. The feature haveReportingAncestor of hijacked is ‘Y’ because it is a verb with a parent verb said. Similarly, the feature haveDaughterAux would also be ’Y’ because of daughter was, whereas whichAuxIsMyDaughter would get the value was. We also considered several other features which did not yield good results. For example, the token’s supertag (Bangalore and Joshi, 1999), the parent token’s supertag, a binary feature isRoot (Is the word the root of the parse tree?) were deemed not useful. We list the features we experimented with and decided to discard in Table 1. For finding the best performing features, we did an exhaustive search on the feature space, incrementally pruning away features that are not useful. Class Description LC LN SN Lexical features with Context Lexical and Syntactic features with Nocontext Lexical features with Context and Syntactic features with No-context Lexical and Syntactic features with Context LC SN LC SC Table 2: YAMCHA Experimen"
C10-2117,N09-2047,1,0.838087,"llum, 2002) toolkit.3 3.2 Features We divided our features into two types - Lexical and Syntactic. Lexical features are at the token level and can be extracted without any parsing with relatively high accuracy. We expect these features to be useful for our task. For example, isNumeric, which denotes whether the word is a number or alphabetic, is a lexical feature. Syntactic features of a token access its syntactic context in the dependency tree. For example, parentPOS, the POS tag of the parent word in the dependency parse tree, is a syntactic feature. We used the MICA deep dependency parser (Bangalore et al., 2009) for parsing in order to derive the syntactic features. We use MICA because we assume that the relevant information is the 3 1016 http://MALLET.cs.umass.edu/ predicate-argument structure of the verbs, which is explicit in the MICA output. While it is clear that having a perfect parse would yield useful features, current parsers perform at levels of accuracy lower than that of part-of-speech taggers, so that it is not a foregone conclusion that using automatic parser output helps in our task. The list of features we used in our experiments are summarized in Table 1. The column ’Type’ denotes th"
C10-2117,J80-3003,0,0.485333,"Missing"
C10-2117,W09-3012,1,0.488085,"e result of text processing is not a list of facts about the world, but a list of facts about different people’s cognitive states. In this paper, we limit ourselves to the writer’s beliefs, but we specifically want to determine which propositions he or she intends us to believe he or she holds as beliefs, and with what strength. The result of such processing will be a much more fine-grained representation of the information contained in written text than has been available so far. 2 Belief Annotation and Data We use a corpus of 10,000 words annotated for speaker belief of stated propositions (Diab et al., 2009). The corpus is very diverse in terms of genre, and it includes newswire text, email, instructions, and solicitations. The corpus annotates each verbal proposition (clause or small clause), by attaching one of the following tags to the head of the proposition (verbs and heads of nominal, adjectival, and prepositional predications). • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. Committed belief can also include propositions about the future: p"
C10-2117,krestel-etal-2008-minding,0,0.0199995,"Missing"
C10-2117,W00-0730,0,0.0160175,"ures Used a joint model, in which the heads are chosen and classified simultaneously, and a pipeline model, in which heads are chosen first and then classified. In this paper, we consider the joint model in detail and in Section 3.5.3, we present results of the pipeline model; they support our choice. In the joint model, we define a four-way classification task where each token is tagged as one of four classes – CB, NCB, NA, or O (nothing) – as defined in Section 2. For tagging, we experimented with Support Vector Machines (SVM) and Conditional Random Fields (CRF). For SVM, we used the YAMCHA(Kudo and Matsumoto, 2000) sequence labeling system,1 which uses the TinySVM package for classification.2 For CRF, we used the linear chain CRF implementation of 1 2 http://chasen.org/ taku/software/YAMCHA/ http://chasen.org/ taku/software/TinySVM/ the MALLET(McCallum, 2002) toolkit.3 3.2 Features We divided our features into two types - Lexical and Syntactic. Lexical features are at the token level and can be extracted without any parsing with relatively high accuracy. We expect these features to be useful for our task. For example, isNumeric, which denotes whether the word is a number or alphabetic, is a lexical feat"
C10-2117,C08-1101,0,0.0132576,"ning whether a belief in the requirement of p entails the belief in p; instead, we are only interested in whether the writer wants the reader to understand whether the writer holds a belief in the requirement that p or in p directly. This paper is also not concerned with subjectivity (Wiebe et al., 2004), the nature of the proposition p (statement about interior world or external world) is not of interest, only whether the writer wants the reader to believe the writer believes p. This paper is also not concerned with opinion and determining the polarity (or strength) of opinion (for example: (Somasundaran et al., 2008)), which corresponds to the desire dimension. Thus, this work is orthogonal to the extensive literature on opinion classification. The work of (Saur´ı and Pustejovsky, 2007; Saur´ı and Pustejovsky, 2008) is, in many respects, very similar to ours. They propose Factbank, which represents the factual interpretation as modality-polarity pairs, extracted from the basic structural elements denoting factuality encoded by Timebank. Also, they attribute the factuality to specific sources within the text. Our work 1020 is more limited in several ways: we currently only model the writer’s beliefs; we do"
C10-2117,J00-3003,0,0.0798494,"Missing"
C10-2117,J04-3002,0,0.0415265,"n dialog act tagging. This paper is not concerned with issues relating to logics for belief representation or inferencing that can be done on beliefs (for an overview, see (McArthur, 1988)), nor theories of automatic belief ascription (Wilks and Ballim, 1987). For example, this paper is not concerned with determining whether a belief in the requirement of p entails the belief in p; instead, we are only interested in whether the writer wants the reader to understand whether the writer holds a belief in the requirement that p or in p directly. This paper is also not concerned with subjectivity (Wiebe et al., 2004), the nature of the proposition p (statement about interior world or external world) is not of interest, only whether the writer wants the reader to believe the writer believes p. This paper is also not concerned with opinion and determining the polarity (or strength) of opinion (for example: (Somasundaran et al., 2008)), which corresponds to the desire dimension. Thus, this work is orthogonal to the extensive literature on opinion classification. The work of (Saur´ı and Pustejovsky, 2007; Saur´ı and Pustejovsky, 2008) is, in many respects, very similar to ours. They propose Factbank, which re"
C12-1138,P12-2032,1,0.681939,"Missing"
C12-1138,P11-1078,0,0.127936,"ogstruktur und Wörter in den Emails benutzt, um Dialogteilnehmer mit situativer Macht zu identifizieren. Keywords in German: Komputationalle Soziolinguistik, Soziale Netzwerke, Macht, Dialogakte, Dialog. Proceedings of COLING 2012: Technical Papers, pages 2259–2274, COLING 2012, Mumbai, December 2012. 2259 1 Introduction Within an interaction, there is often a power differential between the interactants. This differential is often drawn from a static source external to the interaction, such as a formal or informal power structure or hierarchy. Most computational studies (Creamer et al., 2009; Bramsen et al., 2011; Gilbert, 2012) that analyze power within interactions have used such an external power structure (namely, a corporate hierarchy) as the definition of the power differential. However, the power differential may also be dynamic and specific to the situation of the interaction. We define a person to have situational power if there is another person such that he or she has power or authority over the first person in the context of a situation or task. Such situational power may not always align with the external power structures, if one exists. For example, a Human Resources department employee"
C12-1138,W09-3953,1,0.943339,"they were quoted (Yeh and Harnly, 2006). Most emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely social emails. Table 1 presents some statistics on participants and messages in the corpus. We define an active participant of a given thread as someone who has sent at least one email message in the thread. Apart from the thread level annotations for different types of power, the corpus also contains utterance level annotations for overt displays of power. The same corpus has been previously annotated with dialog act annotations (Hu et al., 2009). We utilize these annotations in our study 2262 Statistic Number of email threads Number of participants Ave. Participants / thread Number of active participants Ave. Active participants / thread Number of messages Ave. Messages / thread Ave. Messages / active participant Number of word tokens Situational Power (SP) Count / Mean (SD) 122 1033 8.47 (13.82) 221 1.81 (0.73) 360 2.95 (2.24) 1.45 (1.01) 20,740 81 Table 1: Corpus statistics and will describe them in more detail in the following sections. We give an example thread and corresponding situational power annotations in Table 2. The examp"
C12-1138,W11-0711,0,0.10461,"needs something that x can choose to provide or not”. They model this dependence “using the exchange-theoretic principle that the need to convince someone who disagrees with you creates a form of dependence.” We adopt a broader definition of situational power based on context and perception. Strzalkowski et al. (2010) are also interested in power in written dialog. However, their work concentrates on lower-level constructs called Language Uses which will be used to predict power in subsequent work. They model power using notions of topic switching, exploiting mainly complex lexical features. Peterson et al. (2011) focus on formality in Enron email messages and relates it to social distance and power. 3 3.1 Data Corpus We use the email corpus presented in (Prabhakaran et al., 2012a) which contains manual annotations for various types of power relations between participants. In this study, we focus only on Situational Power (SP) and leave the other types of power for future work. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in whi"
C12-1138,prabhakaran-etal-2012-annotations,1,0.628308,"es with you creates a form of dependence.” We adopt a broader definition of situational power based on context and perception. Strzalkowski et al. (2010) are also interested in power in written dialog. However, their work concentrates on lower-level constructs called Language Uses which will be used to predict power in subsequent work. They model power using notions of topic switching, exploiting mainly complex lexical features. Peterson et al. (2011) focus on formality in Enron email messages and relates it to social distance and power. 3 3.1 Data Corpus We use the email corpus presented in (Prabhakaran et al., 2012a) which contains manual annotations for various types of power relations between participants. In this study, we focus only on Situational Power (SP) and leave the other types of power for future work. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006). Most emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely s"
C12-1138,N12-1057,1,0.715086,"es with you creates a form of dependence.” We adopt a broader definition of situational power based on context and perception. Strzalkowski et al. (2010) are also interested in power in written dialog. However, their work concentrates on lower-level constructs called Language Uses which will be used to predict power in subsequent work. They model power using notions of topic switching, exploiting mainly complex lexical features. Peterson et al. (2011) focus on formality in Enron email messages and relates it to social distance and power. 3 3.1 Data Corpus We use the email corpus presented in (Prabhakaran et al., 2012a) which contains manual annotations for various types of power relations between participants. In this study, we focus only on Situational Power (SP) and leave the other types of power for future work. The corpus contains 122 email threads with a total of 360 messages and 20,740 word tokens. This set of email threads is chosen from a version of the Enron email corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006). Most emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely s"
C12-1138,C10-1117,0,0.0789069,"l attributes such as power, gender, etc. They perform their study on Wikipedia discussion forums and Supreme Court hearings. They also look into situational power; however they define situational power in terms of the dependence between interactants: “x may have power over y in a given situation because y needs something that x can choose to provide or not”. They model this dependence “using the exchange-theoretic principle that the need to convince someone who disagrees with you creates a form of dependence.” We adopt a broader definition of situational power based on context and perception. Strzalkowski et al. (2010) are also interested in power in written dialog. However, their work concentrates on lower-level constructs called Language Uses which will be used to predict power in subsequent work. They model power using notions of topic switching, exploiting mainly complex lexical features. Peterson et al. (2011) focus on formality in Enron email messages and relates it to social distance and power. 3 3.1 Data Corpus We use the email corpus presented in (Prabhakaran et al., 2012a) which contains manual annotations for various types of power relations between participants. In this study, we focus only on S"
C12-1138,C00-2137,0,0.030515,"oth gave the best performing system with an 2270 F measure of 64.4. The best performing single feature was ODPCount, which by itself gave an F measure of 60.0. The results we obtained are in line with the findings from the statistical significance study presented in Section 7. For example, DAP contained the least significant features while ODP contained the most significant feature. The tagger also performed worst when only DAP features were used and best when ODP was used. We assessed the statistical significance of F-measure improvements over baseline, using the Approximate Randomness Test (Yeh, 2000; Noreen, 1989).3 . We found the improvements to be statistically significant (p = 0.001). For the best performing feature set — DLC+ODP — we obtained a mean F-measure (macroaverage) of 64.92 with a standard deviation of 8.82 (please note that Table 4 reports micro-averaged F-measure: 64.4). The low standard deviation suggests that the model built in this setting will obtain comparable performances for new unseen data. This also means that the data from which it was trained on the different folds of the cross-validation is sufficiently consistent to learn a model with predictive power. Put dif"
C12-2029,W09-0807,0,0.141689,"Missing"
C12-2029,I11-1036,1,0.771943,"ssigned a score of 1. If the word is analyzed by ALMOR and it belongs to the dialectal-entries’ list we assume it is DA and it is assigned a score of 0.5. We limit the number of produced English glosses by having the internal MSA SAMA database entries ranked by their frequency of occurrence in the AGW4. 3.3.1 Using Sound Change Rules for OOVs If the word isn’t successfully analyzed by ALMOR and is not in our DA dictionaries, we attempt a relaxed match on the token using sound change rules (SCR) that model the possible phonological variants of the token. We use a subset of the SCR proposed by (Dasigi and Diab, 2011). Table 2 shows the SCR used. In this case, if the relaxed approximated phonological variant of the word is found by ALMOR, we tag the input word as DA not MSA, and assign it a DA score of 0.5; and not 1 since the word might be a misspelled MSA word and not a DA variant; but return the MSA relaxed variant as the MSA equivalent and the corresponding English gloss. 3.4 Language Models (LM) 3.4.1 Data Collection Our data collection comprises various genres. For the MSA-LM we used a subset of the Arabic Gigaword (AGW4) (Parker et al., 2009), Broadcast News, Broadcast Conversations, and Web-Logs ob"
C12-2029,W11-3407,1,0.815058,"(Solorio and Liu, 2008b) and (Manandise and Gdaniec, 2011), been little research in computational approaches to the problem. Predictive models of how and when LCS typically occurs, as well as how to interpret LCS items in the context of the matrix language, have yet to be developed. A major barrier to research on LCS has been the lack of large, consistently and accurately annotated corpora of LCS data. In fact, there has been very little discussion even of how such data should be collected and annotated to best support the interests of both the theoretical and the computational communities. (Diab and Kamboj, 2011), and (Elfardy and Diab, 2012) attempted to tackle this problem by annotating corpora of Hindi-English and MSA-DA code switched social media text. For Chinese English LCS, (Lyu et al., 2006) found that building a unified acoustic model of the regional dialects to be detected, a bilingual pronunciation model, and a Chinese character-based tree-structured search strategy improved ASR performance significantly. For Spanish-English LCS, 288 Input Eng-GL MSA-GL No-Context Contextual dh 1 Al˜y byHSl fy Alwqt that what in time *lk Al*y fy Alwqt DA DA DA MSA-DA MSA-DA DA DA DA MSA MSA Table 1: An exam"
C12-2029,elfardy-diab-2012-simplified,1,0.651382,"(Manandise and Gdaniec, 2011), been little research in computational approaches to the problem. Predictive models of how and when LCS typically occurs, as well as how to interpret LCS items in the context of the matrix language, have yet to be developed. A major barrier to research on LCS has been the lack of large, consistently and accurately annotated corpora of LCS data. In fact, there has been very little discussion even of how such data should be collected and annotated to best support the interests of both the theoretical and the computational communities. (Diab and Kamboj, 2011), and (Elfardy and Diab, 2012) attempted to tackle this problem by annotating corpora of Hindi-English and MSA-DA code switched social media text. For Chinese English LCS, (Lyu et al., 2006) found that building a unified acoustic model of the regional dialects to be detected, a bilingual pronunciation model, and a Chinese character-based tree-structured search strategy improved ASR performance significantly. For Spanish-English LCS, 288 Input Eng-GL MSA-GL No-Context Contextual dh 1 Al˜y byHSl fy Alwqt that what in time *lk Al*y fy Alwqt DA DA DA MSA-DA MSA-DA DA DA DA MSA MSA Table 1: An example of the output of AIDA. Alr"
C12-2029,D08-1102,0,0.252681,"ied DA words. Identifying the classes and sequences of MSA vs. DA words in an utterance can allow for better modeling of Arabic language usage and processing. Moreover the dialect id component can be used for smart filtering for various levels of domain adaptation and targeted document search in an Information Retrieval framework in a rapid process of identifying whether a document is predominantly MSA or DA. 2 Related Work While there has been considerable interest in LCS from the theoretical and socio-linguistic communities, there has, with few exceptions (Joshi, 1985) (Chan et al., 2004), (Solorio and Liu, 2008a), (Solorio and Liu, 2008b) and (Manandise and Gdaniec, 2011), been little research in computational approaches to the problem. Predictive models of how and when LCS typically occurs, as well as how to interpret LCS items in the context of the matrix language, have yet to be developed. A major barrier to research on LCS has been the lack of large, consistently and accurately annotated corpora of LCS data. In fact, there has been very little discussion even of how such data should be collected and annotated to best support the interests of both the theoretical and the computational communities"
C12-2029,D08-1110,0,0.114667,"ied DA words. Identifying the classes and sequences of MSA vs. DA words in an utterance can allow for better modeling of Arabic language usage and processing. Moreover the dialect id component can be used for smart filtering for various levels of domain adaptation and targeted document search in an Information Retrieval framework in a rapid process of identifying whether a document is predominantly MSA or DA. 2 Related Work While there has been considerable interest in LCS from the theoretical and socio-linguistic communities, there has, with few exceptions (Joshi, 1985) (Chan et al., 2004), (Solorio and Liu, 2008a), (Solorio and Liu, 2008b) and (Manandise and Gdaniec, 2011), been little research in computational approaches to the problem. Predictive models of how and when LCS typically occurs, as well as how to interpret LCS items in the context of the matrix language, have yet to be developed. A major barrier to research on LCS has been the lack of large, consistently and accurately annotated corpora of LCS data. In fact, there has been very little discussion even of how such data should be collected and annotated to best support the interests of both the theoretical and the computational communities"
C12-2029,C82-1023,0,\N,Missing
C14-1047,S12-1051,1,0.747067,"ted dataset, there exist only 11 words per tweet on average. We address the sparsity issue pertaining to tweet data by converting our previously proposed topic modelWeighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012) to a binarized version. WTMF maps a tweet to a low-dimensional semantic vector which can easily be transformed to a binary code by virtue of a sign function. We consider WTMF a good baseline for the task of tweet retrieval, as it has achieved state-of-the-art performance among unsupervised systems on two benchmark short-text datasets released by Li et al. (2006) and Agirre et al. (2012). In this paper, we improve WTMF in two aspects. The first drawback of the WTMF model is that it focuses on exhaustively encoding the local context, and hence introduces some overlapping information that is reflected in its associated projections. In order to remove the redundant information and meanwhile discover more distinct topics, we employ a gradient descent method to make the projection directions nearly orthogonal. The second aspect is to enrich each tweet by its neighbors. Because of the short context, most tweets do not contain sufficient information of an event, as noticed by previo"
C14-1047,P11-1040,0,0.0207332,"odel shows significant performance gains over competing methods. 1 Introduction Twitter is rapidly gaining worldwide popularity, with 500 million active users generating more than 340 million tweets daily1 . Massive-scale tweet data which is freely available on the Web contains rich linguistic phenomena and valuable information, therefore making it one of most favorite data sources used by a variety of Natural Language Processing (NLP) applications. Successful examples include first story detection (Petrovic et al., 2010), local event detection (Agarwal et al., 2012), Twitter event discovery (Benson et al., 2011) and summarization (Chakrabarti and Punera, 2011), etc. In these NLP applications, one of core technical components is tweet similarity computing to search for the desired tweets with respect to some sample tweets. For example, in first story detection (Petrovic et al., 2010), the purpose is to find an incoming tweet that is expected to report a novel event not revealed by the previous tweets. This is done by measuring cosine similarity between the incoming tweet and each previous tweet. One obvious issue is that cosine similarity computations among tweet data will become very slow once the sc"
C14-1047,P08-1077,0,0.0228254,"Hashing (Liu et al., 2012b), Kernelized LSH (Kulis and Grauman, 2012), etc. Concurrently, supervised information defined among training data samples was incorporated into coding function learning such as Minimal Loss Hashing (Norouzi and Fleet, 2011) and Kernel-Based Supervised Hashing (Liu et al., 2012a). Our proposed method falls into the category of unsupervised, linear, data-dependent binary coding. 2.3 Applications in NLP The NLP community has successfully applied LSH in several tasks such as first story detection (Petrovic et al., 2010), and paraphrase retrieval for relation extraction (Bhagat and Ravichandran, 2008), etc. This paper shows that our proposed data-dependent binary coding approach is superior to data-independent LSH in terms of the quality of generated binary codes. Subercaze et al. (2013) proposed a binary coding approach to encode user profiles for recommendations. Compared to (Subercaze et al., 2013) in which a data unit is a whole user profile consisting of all his/her Twitter posts, we tackle a more challenging problem, since our data units are extremely short – namely, a single tweet. 2 We recognize that different hardware exploiting techniques such as GPU or parallelization accelerate"
C14-1047,P12-1091,1,0.773689,"rresponding to the i-th tweet in the corpus. ¯ i = xi − µ. The vector subtracted by the mean µ of the tweet corpus: x The tweet corpus in a matrix format, and the zero-centered tweet data. The number of binary coding functions, i.e., the number of latent topics. The k-th binary coding function. Table 1: Symbols used in binary coding. latent topical semantics. For instance, in our collected dataset, there exist only 11 words per tweet on average. We address the sparsity issue pertaining to tweet data by converting our previously proposed topic modelWeighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012) to a binarized version. WTMF maps a tweet to a low-dimensional semantic vector which can easily be transformed to a binary code by virtue of a sign function. We consider WTMF a good baseline for the task of tweet retrieval, as it has achieved state-of-the-art performance among unsupervised systems on two benchmark short-text datasets released by Li et al. (2006) and Agirre et al. (2012). In this paper, we improve WTMF in two aspects. The first drawback of the WTMF model is that it focuses on exhaustively encoding the local context, and hence introduces some overlapping information that is ref"
C14-1047,P13-1024,1,0.89836,"F in two aspects. The first drawback of the WTMF model is that it focuses on exhaustively encoding the local context, and hence introduces some overlapping information that is reflected in its associated projections. In order to remove the redundant information and meanwhile discover more distinct topics, we employ a gradient descent method to make the projection directions nearly orthogonal. The second aspect is to enrich each tweet by its neighbors. Because of the short context, most tweets do not contain sufficient information of an event, as noticed by previous work (Agarwal et al., 2012; Guo et al., 2013). Ideally, we would like to learn a model such that the tweets related to the same event are mapped to adjacent binary codes. We fulfill this purpose by augmenting each tweet in a given training dataset with its neighboring tweets within a temporal window, and assuming that these neighboring (or similar) tweets are triggered by the same event. We name the improved model Orthogonal Matrix Factorization with Neighbors (OrMFN). In our experiments, we use Twitter hashtags to create the gold (i.e., groundtruth) labels, where tweets with the same hashtag are considered semantically related, hence re"
C14-1047,N10-1021,0,0.618759,"taset using hashtags to create gold labels in an information retrieval scenario, our proposed model shows significant performance gains over competing methods. 1 Introduction Twitter is rapidly gaining worldwide popularity, with 500 million active users generating more than 340 million tweets daily1 . Massive-scale tweet data which is freely available on the Web contains rich linguistic phenomena and valuable information, therefore making it one of most favorite data sources used by a variety of Natural Language Processing (NLP) applications. Successful examples include first story detection (Petrovic et al., 2010), local event detection (Agarwal et al., 2012), Twitter event discovery (Benson et al., 2011) and summarization (Chakrabarti and Punera, 2011), etc. In these NLP applications, one of core technical components is tweet similarity computing to search for the desired tweets with respect to some sample tweets. For example, in first story detection (Petrovic et al., 2010), the purpose is to find an incoming tweet that is expected to report a novel event not revealed by the previous tweets. This is done by measuring cosine similarity between the incoming tweet and each previous tweet. One obvious is"
C14-1047,P08-1000,0,\N,Missing
C16-1115,W14-1604,1,0.848334,"tion (LCSPD). Furthermore, we show the robustness of our approach on the most challenging problem of language variety code switching where the code switching is happening between a standard and dialect, namely we illustrate our performance on Modern Standard Arabic (MSA) mixed with Egyptian Dialectal Arabic data (EGY). 2 Related Work Several systems have recently addressed the problem of LCSPD in written text both within language varieties and across different language pairs. Relevant work on the problem of LCSPD among different language pairs can be summarized in the following works. 3ARRIB (Al-Badrashiny et al., 2014; Eskander et al., 2014) addresses the challenge of how to distinguish between Arabic words written using Roman script (Arabizi) and actual English words in the same context/utterance. The assumption in this framework is that the script is Latin for all words. It trains a finite state transducer (FST) to learn the mapping between the Roman form of the Arabizi words and This work is licenced under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/ Licence details: 1211 Proceedings of COLING 2016, the 26th International Conference on Computation"
C16-1115,K15-1005,1,0.936475,"anguage, the two varieties typically share a common space of cognates and often faux amis, where there are homographs but the words have very different semantic meanings, hence adding another layer of complexity to the problem. In this set up the assumed script is Arabic script. AIDA (Elfardy et al., 2014) uses a weakly supervised rule based approach that relies on a language model to tag each word in the given sentence. Then it uses the LM decision for each word in the given sentence/tweet and combine it with other morphological information to decide upon the final class of each word. AIDA2 (Al-Badrashiny et al., 2015) uses a complex system that is based on a mix of language dependent and machine learning components to detect the linguistic code switch between the modern standard Arabic (MSA) and Egyptian dialect (EGY) that are both written using Arabic script. It uses MADAMIRA (Pasha et al., 2014) to find the POS tag, prefix, lemma, suffix, for each word in the input text. Then it models these features together with other features including word level language model probabilities in a series of classifiers where it combines them in a classifier ensemble approach to 1 2 https://code.google.com/p/cld2/ https"
C16-1115,W14-3917,0,0.259196,"mputational Linguistics: Technical Papers, pages 1211–1219, Osaka, Japan, December 11-17 2016. their Arabic form. It uses the resulting FST to find all possible Arabic candidates for each word in the input text. These candidates are filtered using MADAMIRA (Pasha et al., 2014), a state-of-the-art morphological analyzer and POS disambiguation tool, to filter out non-Arabic solutions. Finally, it leverages a decision tree that is trained on language model probabilities of both the Arabic and Romanized forms to render the final decision for each word in context as either being Arabic or English. Bar and Dershowitz (2014) addresses the challenge for Spanish-English LCSPD. The authors use several features to train a sequential Support Vector Machines (SVM) classifier. The used features include previous and following two words, substrings of 1-3 character ngrams from the beginning and end of each word thereby modeling prefix and suffix information, a boolean feature indicating whether the first letter is capitalized or not, and 3-gram character and word ngram language models trained over large corpora of English and Spanish, respectively. Barman et al. (2014) present systems for both Nepali-English and Spanish-E"
C16-1115,W14-3915,0,0.222601,"rd in context as either being Arabic or English. Bar and Dershowitz (2014) addresses the challenge for Spanish-English LCSPD. The authors use several features to train a sequential Support Vector Machines (SVM) classifier. The used features include previous and following two words, substrings of 1-3 character ngrams from the beginning and end of each word thereby modeling prefix and suffix information, a boolean feature indicating whether the first letter is capitalized or not, and 3-gram character and word ngram language models trained over large corpora of English and Spanish, respectively. Barman et al. (2014) present systems for both Nepali-English and Spanish-English LCSPD. The script for both language pairs is Latin based, i.e. Nepali-English is written in Latin script, and Spanish-English is written in Latin script. The authors carry out several experiments using different approaches including dictionary-based methods, linear kernel SVMs, and a k-nearest neighbor approach. The best setup they found is the SVM-based one that uses character n-gram, binary features indicate whether the word is in a language specific dictionary of the most frequent 5000 words they have constructed, length of the wo"
C16-1115,W14-3612,0,0.0959905,"Missing"
C16-1115,cotterell-callison-burch-2014-multi,0,0.155851,"Missing"
C16-1115,W14-3911,1,0.915945,"increase the size of the training data using a semi supervised CRF autoencoder approach (Ammar et al., 2014) coupled with unsupervised word embeddings. MSR-India (Chittaranjan et al., 2014) uses character n-grams to train a maximum entropy classifier that identifies whether a word is language1 or language2. The resultant labels are then used together with word length, existence of special characters in the word, current, previous and next words to train a CRF model that predicts the token level classes of words in a given sentence/tweet. On the other hand, for within language varieties, AIDA (Elfardy et al., 2014) and AIDA2 (AlBadrashiny et al., 2015) are the best published systems attacking this problem in Arabic. In this context, the problem of LCSPD is more complicated than mixing two very different languages since in the case of varieties of the same language, the two varieties typically share a common space of cognates and often faux amis, where there are homographs but the words have very different semantic meanings, hence adding another layer of complexity to the problem. In this set up the assumed script is Arabic script. AIDA (Elfardy et al., 2014) uses a weakly supervised rule based approach"
C16-1115,W14-3901,1,0.946328,"e show the robustness of our approach on the most challenging problem of language variety code switching where the code switching is happening between a standard and dialect, namely we illustrate our performance on Modern Standard Arabic (MSA) mixed with Egyptian Dialectal Arabic data (EGY). 2 Related Work Several systems have recently addressed the problem of LCSPD in written text both within language varieties and across different language pairs. Relevant work on the problem of LCSPD among different language pairs can be summarized in the following works. 3ARRIB (Al-Badrashiny et al., 2014; Eskander et al., 2014) addresses the challenge of how to distinguish between Arabic words written using Roman script (Arabizi) and actual English words in the same context/utterance. The assumption in this framework is that the script is Latin for all words. It trains a finite state transducer (FST) to learn the mapping between the Roman form of the Arabizi words and This work is licenced under a Creative Commons Attribution 4.0 International Licence. http://creativecommons.org/licenses/by/4.0/ Licence details: 1211 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technica"
C16-1115,R15-1033,0,0.0751361,"Missing"
C16-1115,W14-3912,0,0.0347054,"Missing"
C16-1115,W16-5805,1,0.819229,"he training data. Then we apply the unigram LMs to each word in the training data to find their word level probabilities in each language in the training data, i.e. checking whether it pertains to the language or not by virtue of having a higher probability in the corresponding LM than words not in the language. 4 Experimental Setup 4.1 Data We evaluate our proposed framework on different language pairs exhibiting code switching. We use the training and test data sets provided by the shared task for “Language Identification in Code-Switched Data” [ShTk] in 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). The ShTk-2014 datasets includes English-Spanish, English-Nepali, Modern standard Arabic (MSA)-Egyptian Arabic (EGY), and English-Mandarin3 , while the ShTk-2016 datasets includes English-Spanish, and, MSA-EGY. In addition to these languages, we evaluate our system on MSA-Levantine (LEV), MSA-Gulf, Arabizi-English, Arabic-Engari, and English-Hindi datasets4 . 3 4 Unfortunately, we did not manage to get English-Mandarin datasets from the organizers but we got the rest of them. Nepali, Arabizi, and Hindi are written using Roman script. Engari is written using Arabic script 1213 • MSA-LEV and MS"
C16-1115,pasha-etal-2014-madamira,1,0.872266,"Missing"
C16-1115,N03-1028,0,0.168833,"anguage model for the target language to maximize the probabilities of the possible patterns and suppress the probabilities of the impossible ones should provide an approximation to such a closed set rule-based list. Not to mention that producing such a list by hand is quite laborious and error prone as a process. Accordingly, we propose a supervised learning framework to address the challenge of LCSPD. We assume the presence of annotated code switched training data where each token is annotated as either Lang1 or Lang2. We create a sequence model using Conditional Random Fields (CRF++) tool (Sha and Pereira, 2003). For each word in the training data, we create a feature vector comprising word length, character sequence level probabilities, and unigram word level probabilities. Once we derive the learning model, we apply to input text to identify Lang1 tokens vs. Lang2 tokens in context. For the character sequence level probabilities, we build (1, 2, 3, 4, and 5)-gram character language models (CLMs) using the SRILM tool (Stolcke, 2002) for each of the two languages presented in the training data using the annotated words. For example, if the training data contains the two languages “lang1” and “lang2”,"
C16-1115,W14-3907,1,0.891917,"the two languages in the training data. Then we apply the unigram LMs to each word in the training data to find their word level probabilities in each language in the training data, i.e. checking whether it pertains to the language or not by virtue of having a higher probability in the corresponding LM than words not in the language. 4 Experimental Setup 4.1 Data We evaluate our proposed framework on different language pairs exhibiting code switching. We use the training and test data sets provided by the shared task for “Language Identification in Code-Switched Data” [ShTk] in 2014 and 2016 (Solorio et al., 2014; Molina et al., 2016). The ShTk-2014 datasets includes English-Spanish, English-Nepali, Modern standard Arabic (MSA)-Egyptian Arabic (EGY), and English-Mandarin3 , while the ShTk-2016 datasets includes English-Spanish, and, MSA-EGY. In addition to these languages, we evaluate our system on MSA-Levantine (LEV), MSA-Gulf, Arabizi-English, Arabic-Engari, and English-Hindi datasets4 . 3 4 Unfortunately, we did not manage to get English-Mandarin datasets from the organizers but we got the rest of them. Nepali, Arabizi, and Hindi are written using Roman script. Engari is written using Arabic script"
C16-1115,L16-1669,1,\N,Missing
C18-1226,Q17-1010,0,0.0648065,"twork classifier that predicts a word given the surrounding words within a fixed context window (Mikolov et al., 2013a). In Schnabel et al. (2015) word embedding evaluation, CBOW outperformed other word embeddings in word relatedness and analogy tasks. Global Vectors: GloVe is a global log-bilinear regression model (Pennington et al., 2014) that produces word embeddings using weighted matrix factorization of word co-occurrence probabilities. Subword Information Skip-gram si-skip learns representations for n-grams of various lengths, and words are represented as sums of n-gram representations (Bojanowski et al., 2017). The learning architecture is based on the continuous skip-gram model (Mikolov et al., 2013b), which is trained by maximizing the conditional probability of context words within a fixed window with negative sampling. The model exploits the morphological variations within a language to learn more reliable representations, particularly for rare morphological variants. 2.3 Neural Compositional Models Several models have been proposed to overcome some of the weaknesses of bag-of-words and additive representations, such as lack of structure. We evaluated the following context-sensitive models: The"
C18-1226,D15-1075,0,0.0376801,"ences (Kiros et al., 2015), where the encoder and decoder are RNNs with GRU activations (Chung et al., 2014). The model is trained with contiguous sentences extracted from a collection of novels. After training, the model’s vocabulary is expanded by learning a linear mapping from pre-trained CBOW word embeddings to the vector space of the skip-th word embeddings. Natural Language Inference Encoder: In inferSent (Conneau et al., 2017), a bidirectional LSTM encoder with max-pooling is trained jointly with an inference classifier trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which is a large manually-annotated dataset of English sentence pairs and their inference labels: {entailment, contradition, neutral}. 2668 Dataset Train Test pos ratio l Pair-wise Similarity STS SICK MSRP 5,749 4,934 4,076 1,379 4,906 1,725 – – 0.66 12 10 23 CR 3,775 – 0.64 21 Sentiment Analysis MPQA RT-s Subj 10,606 10,662 10,000 – – – 0.31 0.50 0.50 3 21 24 IMDB 25k 25k 0.50 262 REL 1,078 – 0.58 82 Newsgroup SPO COM 1,604 1,694 – – 0.51 0.51 82 82 POL 1,310 – 0.52 92 TREC 5,452 500 – 10 Table 1: Dataset statistics. Train: number of samples in the training set. Test: number of samples in t"
C18-1226,S17-2001,1,0.831532,"samples in the test set, if applicable (CV is applied otherwise). pos ratio: ratio of positive samples in the test set (or total for datasets with no splits). l: average length of all samples. 3 Evaluation Datasets To evaluate the text representations, we used them as features in extrinsic supervised and unsupervised tasks that reflect various semantic aspects, which can be grouped in three categories: pairwise-similarity, sentiment analysis, and categorization. A summary of the dataset statistics is in Table 1.1 3.1 Pairwise Similarity Semantic Textual Similarity: the STS benchmark dataset (Cer et al., 2017) includes a collection of English sentence pairs and human-annotated similarity scores that range from 0 (unrelated sentences) to 5 (paraphrases). The dataset includes training, development, and test sets. This task can be performed without supervision by calculating the cosine similarity between two sentence vectors. We also evaluated the models in a supervised settings using linear regression, where the input vector is a concatenation of the element-wise produce u.v and absolute difference |u − v |of each pair hu, vi. Sentences Involving Compositional Knowledge: SICK dataset is a benchmark f"
C18-1226,D17-1070,0,0.177366,"plexity, from binary bag-of-words to RNNs, on various supervised and unupervised settings. Our objective is to evaluate compositional models against strong baselines and identify the elements that lead to performance gains. We evaluated binary vs. distributed features, weighted vs. unweighted averaging, three different word embedding models, and four context-sensitive models that optimize different objectives: the paragraph vector, the gated recurrent averaging network (Wieting and Gimpel, 2017), skip-thought, and an LSTM encoder trained on labeled natural language inference data (inferSent) (Conneau et al., 2017). We also analyzed the intrinsic structures of the various models by visual inspection and k-means clustering to gain insights into structural differences that may explain the variance in performance. 2 2.1 Background: Unsupervised Compositional Models Baselines The simplest way of representing a sentence is a binary bag-of-words representation, where each word is a feature in the vector space. This results in large and sparse representations that only account for the existence of individual words within a sentence, yet they have been shown to be effective in various supervised classification"
C18-1226,C04-1051,0,0.474607,"ervised settings using linear regression, where the input vector is a concatenation of the element-wise produce u.v and absolute difference |u − v |of each pair hu, vi. Sentences Involving Compositional Knowledge: SICK dataset is a benchmark for evaluating compositional models (Marelli et al., 2014). We evaluated the models on the relatedness subtask, which is constructed in a similar manner as STS benchmark. Paraphrase Detection: This is a binary classification task that involves the identification of paraphrases in similar sentence pairs using the Microsoft Research Paraphrase Corpus, MSRP (Dolan et al., 2004). We evaluated the models in two ways: calculating the cosine similarity between the sentence pairs and classifying them as paraphrases if the similarity is larger than a threshold tuned from the training set. The second approach is to learn a logistic regression classifier using a concatenation of u.v and |u − v|. 3.2 Sentiment Analysis and Text Categorization Sentiment Analysis: We used the following binary classification tasks: CR customer product reviews (Hu and Liu, 2004), MPQA opinion polarity subtask (Wiebe et al., 2005), RT-s short movie reviews (Pang and Lee, 2005), Subj subjectivity/"
C18-1226,D14-1020,0,0.0471765,"Missing"
C18-1226,N16-1162,0,0.0281133,"nce encoder-decoder models, on the other hand, can be trained with various sentence-level objectives, such as neural machine translation (NMT) (Sutskever et al., 2014), predicting surrounding This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2666 Proceedings of the 27th International Conference on Computational Linguistics, pages 2666–2677 Santa Fe, New Mexico, USA, August 20-26, 2018. sentences (i.e. skip-thought) (Kiros et al., 2015), or reconstruction of the input using denoising autoencoders (Hill et al., 2016). These sequential models have been evaluated and compared against other models and baselines on several supervised and unsupervised tasks in Hill et al. (2016). The denoising autoencoder model and skip-thought both performed well in supervised tasks, while the NMT model performed worse than the baselines. All three performed poorly in unsupervised settings. To bridge some of the gaps in evaluation, we evaluated a subset of models with increasing complexity, from binary bag-of-words to RNNs, on various supervised and unupervised settings. Our objective is to evaluate compositional models again"
C18-1226,D13-1090,0,0.0391042,"ng in the unsupervised STS benchmark. skip-th vectors performed poorly in the unsupervised similarity tasks, but outperformed the pre-trained vectors in the supervised similarity tasks, particularly in SICK. The low variance of the performance in the paraphrase detection task also reflects the overall correlation between word overlap and the likelihood of being a paraphrase as seen in Figure 2; for difficult cases, as in the examples in Table 4, the overall similarity is not a good indication of being a paraphrase. Significant improvements in this task may require more nuanced features as in (Ji and Eisenstein, 2013). 5.2 Evaluation on Sentiment Analysis and Categorization Binary BOW Unigram NBSVM CR 77.0/0.821 80.5 Sentiment Analysis mpqa RT-s subj 85.9/0.756 74.8 89.5 85.3 78.1 92.4 imdb 84.1 88.3 rel 66.2/0.710 73.2 Text Categorization spo com pol mult 85.7 78.2 81.6 72.2 93.5 86.7 91.9 93.4 Paragraph Vector (doc2vec) avg si-skip idf sif avg CBOW idf sif avg GloVe idf sif avg Random idf sif 76.6/0.831 81.3/0.857 80.2/0.849 80.6/0.852 81.6/0.859 81.2/0.856 80.8/0.852 80.7/0.851 80.7/0.851 80.6/0.850 70.7/0.779 68.6/0.763 69.2/0.770 82.4/0.688 87.2/0.778 86.2/0.765 86.7/0.774 86.1/0.759 86.0/0.761 85.7/0"
C18-1226,D14-1181,0,0.0226725,"in RNN models can also be useful in certain classification tasks. We analyzed some of the evaluation datasets to identify the aspects of meaning they measure and the characteristics of the various models that explain their performance variance. 1 Introduction Distributed semantic models for words encode latent features that reflect semantic aspects and correlations among words. The goal of compositional semantic models is to induce latent semantic representations that encode the meaning of phrases, sentences, and paragraphs of variable lengths. Some neural architectures such as convolutional (Kim, 2014) and recursive networks (Socher et al., 2013) handle variable-length input by identifying shift-invariant features suitable for the classification problem at hand, which makes it possible to skip composition and work directly with the entire space of individual word embeddings. While such models can achieve excellent performance in supervised classification tasks such as sentiment analysis, we are interested in generic unsupervised fixed-length representations for variable-length text sequences so as to efficiently preserve essential semantic content for later use in various supervised and uns"
C18-1226,W16-1609,0,0.0276945,"ion baseline for GRAN. We compared skip-th against the CBOW embeddings that were used to expand the vocabulary, which account for most words in the final model’s vocabulary. We used the pre-trained inferSent model 6 which uses pre-trained GloVe word embeddings 7 . We also experimented with the post-trained word embeddings for each model with similar results, so we omitted them for brevity. 4.2 Training Settings We trained the unsupervised word embedding models using the optimal parameters recommended for each model. The hyper-parameters in doc2vec were set according to the recommendations in (Lau and Baldwin, 2016). For the supervised sentiment classification and text categorization tasks, we trained and tuned linear SVM models using grid search for datasets that include train/dev/test splits, and nested crossvalidation otherwise. We also experimented with kernel SVMs but didn’t observe notable differences in the results. 5 Evaluation Results 5.1 Pairwise Similarity Evaluation 0.5 0.65 0.5 0.73 0.74 1 1 1 1 1 0.8 0.8 0.8 0.8 0.8 0.6 0.6 0.6 0.6 0.6 0.4 0.4 0.4 0.4 0.4 0.2 0.2 0.2 0.2 0 0 1 2 3 4 5 0 1 2 0.6 3 4 5 0.2 0 1 2 0.63 3 4 5 0 1 2 0.2 3 4 5 1 1 1 1 1 1 0.8 0.8 0.8 0.8 0.6 0.6 0.6 0.6 0.6 0.4 0."
C18-1226,P11-1015,0,0.0773653,"rs and classifying them as paraphrases if the similarity is larger than a threshold tuned from the training set. The second approach is to learn a logistic regression classifier using a concatenation of u.v and |u − v|. 3.2 Sentiment Analysis and Text Categorization Sentiment Analysis: We used the following binary classification tasks: CR customer product reviews (Hu and Liu, 2004), MPQA opinion polarity subtask (Wiebe et al., 2005), RT-s short movie reviews (Pang and Lee, 2005), Subj subjectivity/objectivity classification task (Pang and Lee, 2004), and IMDB full-length movie review dataset (Maas et al., 2011). Newsgroups: Following the setup in (Wang and Manning, 2012) we used the 20-Newsgroup dataset 2 to extract several binary topic categorization tasks. We processed the datasets to remove headers, forwarded text, and signatures, which results in smaller sentences and paragraphs. We used the following newsgroups for binary classification: religion (atheism vs. religion), sports (baseball vs. hockey), computer (windows vs. graphics), and politics (middle east vs. guns). We also trained multi-class classifiers on the 8 newsgroups. Question Classification: We used the TREC 10 coarse question catego"
C18-1226,marelli-etal-2014-sick,0,0.0527694,"ntence pairs and human-annotated similarity scores that range from 0 (unrelated sentences) to 5 (paraphrases). The dataset includes training, development, and test sets. This task can be performed without supervision by calculating the cosine similarity between two sentence vectors. We also evaluated the models in a supervised settings using linear regression, where the input vector is a concatenation of the element-wise produce u.v and absolute difference |u − v |of each pair hu, vi. Sentences Involving Compositional Knowledge: SICK dataset is a benchmark for evaluating compositional models (Marelli et al., 2014). We evaluated the models on the relatedness subtask, which is constructed in a similar manner as STS benchmark. Paraphrase Detection: This is a binary classification task that involves the identification of paraphrases in similar sentence pairs using the Microsoft Research Paraphrase Corpus, MSRP (Dolan et al., 2004). We evaluated the models in two ways: calculating the cosine similarity between the sentence pairs and classifying them as paraphrases if the similarity is larger than a threshold tuned from the training set. The second approach is to learn a logistic regression classifier using"
C18-1226,D14-1079,0,0.0257125,"serve essential semantic content for later use in various supervised and unsupervised settings. Binary bag-of-words are simple and effective representations that serve as a strong baseline in several classification benchmarks (Wang and Manning, 2012). However, they do not exploit the distributional relationships among different words, which limits their applicability and generalization when training data are scarce. Additive compositional functions, such as word vector sum or average, are more effective in semantic similarity tasks even when compared with tensor-based compositional functions (Milajevs et al., 2014) and can outperform more complex and better tuned models based on recurrent neural architectures on out-of-domain data (Wieting et al., 2015a). Yet, averaging also has several drawbacks: unlike binary representations, the individual word identities are lost, and some words that do not carry semantic significance may end up being more prominently represented than essential words. Furthermore, additive compositional models disregard sentence structure and word order, which can lead to loss of semantic nuance. To alleviate the first issue, the weights of various words can be adjusted using word f"
C18-1226,P04-1035,0,0.0126954,"ways: calculating the cosine similarity between the sentence pairs and classifying them as paraphrases if the similarity is larger than a threshold tuned from the training set. The second approach is to learn a logistic regression classifier using a concatenation of u.v and |u − v|. 3.2 Sentiment Analysis and Text Categorization Sentiment Analysis: We used the following binary classification tasks: CR customer product reviews (Hu and Liu, 2004), MPQA opinion polarity subtask (Wiebe et al., 2005), RT-s short movie reviews (Pang and Lee, 2005), Subj subjectivity/objectivity classification task (Pang and Lee, 2004), and IMDB full-length movie review dataset (Maas et al., 2011). Newsgroups: Following the setup in (Wang and Manning, 2012) we used the 20-Newsgroup dataset 2 to extract several binary topic categorization tasks. We processed the datasets to remove headers, forwarded text, and signatures, which results in smaller sentences and paragraphs. We used the following newsgroups for binary classification: religion (atheism vs. religion), sports (baseball vs. hockey), computer (windows vs. graphics), and politics (middle east vs. guns). We also trained multi-class classifiers on the 8 newsgroups. Ques"
C18-1226,P05-1015,0,0.0991213,"hrase Corpus, MSRP (Dolan et al., 2004). We evaluated the models in two ways: calculating the cosine similarity between the sentence pairs and classifying them as paraphrases if the similarity is larger than a threshold tuned from the training set. The second approach is to learn a logistic regression classifier using a concatenation of u.v and |u − v|. 3.2 Sentiment Analysis and Text Categorization Sentiment Analysis: We used the following binary classification tasks: CR customer product reviews (Hu and Liu, 2004), MPQA opinion polarity subtask (Wiebe et al., 2005), RT-s short movie reviews (Pang and Lee, 2005), Subj subjectivity/objectivity classification task (Pang and Lee, 2004), and IMDB full-length movie review dataset (Maas et al., 2011). Newsgroups: Following the setup in (Wang and Manning, 2012) we used the 20-Newsgroup dataset 2 to extract several binary topic categorization tasks. We processed the datasets to remove headers, forwarded text, and signatures, which results in smaller sentences and paragraphs. We used the following newsgroups for binary classification: religion (atheism vs. religion), sports (baseball vs. hockey), computer (windows vs. graphics), and politics (middle east vs."
C18-1226,D14-1162,0,0.0992944,"rd normal distribution for each word in the vocabulary. The vector sum of random word vectors is a low-dimensional projection of binary bag-of-words vectors. . Continuous Bag of Words: CBOW is an efficient log-linear model for learning word embeddings using a feed-forward neural network classifier that predicts a word given the surrounding words within a fixed context window (Mikolov et al., 2013a). In Schnabel et al. (2015) word embedding evaluation, CBOW outperformed other word embeddings in word relatedness and analogy tasks. Global Vectors: GloVe is a global log-bilinear regression model (Pennington et al., 2014) that produces word embeddings using weighted matrix factorization of word co-occurrence probabilities. Subword Information Skip-gram si-skip learns representations for n-grams of various lengths, and words are represented as sums of n-gram representations (Bojanowski et al., 2017). The learning architecture is based on the continuous skip-gram model (Mikolov et al., 2013b), which is trained by maximizing the conditional probability of context words within a fixed window with negative sampling. The model exploits the morphological variations within a language to learn more reliable representat"
C18-1226,D15-1036,0,0.0261522,"first principal component which corresponds to syntactic features associated with common words. 2.2.1 Word Embeddings Random word projections: we generated a random vector drawn from the standard normal distribution for each word in the vocabulary. The vector sum of random word vectors is a low-dimensional projection of binary bag-of-words vectors. . Continuous Bag of Words: CBOW is an efficient log-linear model for learning word embeddings using a feed-forward neural network classifier that predicts a word given the surrounding words within a fixed context window (Mikolov et al., 2013a). In Schnabel et al. (2015) word embedding evaluation, CBOW outperformed other word embeddings in word relatedness and analogy tasks. Global Vectors: GloVe is a global log-bilinear regression model (Pennington et al., 2014) that produces word embeddings using weighted matrix factorization of word co-occurrence probabilities. Subword Information Skip-gram si-skip learns representations for n-grams of various lengths, and words are represented as sums of n-gram representations (Bojanowski et al., 2017). The learning architecture is based on the continuous skip-gram model (Mikolov et al., 2013b), which is trained by maximi"
C18-1226,D13-1170,0,0.0118653,"certain classification tasks. We analyzed some of the evaluation datasets to identify the aspects of meaning they measure and the characteristics of the various models that explain their performance variance. 1 Introduction Distributed semantic models for words encode latent features that reflect semantic aspects and correlations among words. The goal of compositional semantic models is to induce latent semantic representations that encode the meaning of phrases, sentences, and paragraphs of variable lengths. Some neural architectures such as convolutional (Kim, 2014) and recursive networks (Socher et al., 2013) handle variable-length input by identifying shift-invariant features suitable for the classification problem at hand, which makes it possible to skip composition and work directly with the entire space of individual word embeddings. While such models can achieve excellent performance in supervised classification tasks such as sentiment analysis, we are interested in generic unsupervised fixed-length representations for variable-length text sequences so as to efficiently preserve essential semantic content for later use in various supervised and unsupervised settings. Binary bag-of-words are s"
C18-1226,P12-2018,0,0.507487,"lem at hand, which makes it possible to skip composition and work directly with the entire space of individual word embeddings. While such models can achieve excellent performance in supervised classification tasks such as sentiment analysis, we are interested in generic unsupervised fixed-length representations for variable-length text sequences so as to efficiently preserve essential semantic content for later use in various supervised and unsupervised settings. Binary bag-of-words are simple and effective representations that serve as a strong baseline in several classification benchmarks (Wang and Manning, 2012). However, they do not exploit the distributional relationships among different words, which limits their applicability and generalization when training data are scarce. Additive compositional functions, such as word vector sum or average, are more effective in semantic similarity tasks even when compared with tensor-based compositional functions (Milajevs et al., 2014) and can outperform more complex and better tuned models based on recurrent neural architectures on out-of-domain data (Wieting et al., 2015a). Yet, averaging also has several drawbacks: unlike binary representations, the indivi"
C18-1226,P17-1190,0,0.174044,"ta (Wieting et al., 2015a). Yet, averaging also has several drawbacks: unlike binary representations, the individual word identities are lost, and some words that do not carry semantic significance may end up being more prominently represented than essential words. Furthermore, additive compositional models disregard sentence structure and word order, which can lead to loss of semantic nuance. To alleviate the first issue, the weights of various words can be adjusted using word frequency statistics (Riedel et al., 2017) or by inducing context-sensitive weights using recurrent neural networks (Wieting and Gimpel, 2017), both of which have been shown to outperform vector averaging. Context-sensitive feed-forward neural models like the paragraph vector (Le and Mikolov, 2014) potentially incorporate word order, yet the training objective may not be sufficient to model deeper structure. Sequence encoder-decoder models, on the other hand, can be trained with various sentence-level objectives, such as neural machine translation (NMT) (Sutskever et al., 2014), predicting surrounding This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License de"
C18-1226,Q15-1025,0,0.0519684,"epresentations that serve as a strong baseline in several classification benchmarks (Wang and Manning, 2012). However, they do not exploit the distributional relationships among different words, which limits their applicability and generalization when training data are scarce. Additive compositional functions, such as word vector sum or average, are more effective in semantic similarity tasks even when compared with tensor-based compositional functions (Milajevs et al., 2014) and can outperform more complex and better tuned models based on recurrent neural architectures on out-of-domain data (Wieting et al., 2015a). Yet, averaging also has several drawbacks: unlike binary representations, the individual word identities are lost, and some words that do not carry semantic significance may end up being more prominently represented than essential words. Furthermore, additive compositional models disregard sentence structure and word order, which can lead to loss of semantic nuance. To alleviate the first issue, the weights of various words can be adjusted using word frequency statistics (Riedel et al., 2017) or by inducing context-sensitive weights using recurrent neural networks (Wieting and Gimpel, 2017"
C18-1246,P16-2038,0,0.0200805,".2 Multi-Task Learning in Deep Neural Net Multi-Task learning is inspired by human learning. As human, when we learn new tasks, often we apply the knowledge we have gathered from related tasks. In the literature, multi-task learning comes in different forms: joint learning, learning to learn, and learning with auxiliary tasks. The following studies show the benefit of multi-task learning in closely-related or different type of tasks (R, 1993; Ronan and Jason, 2008; Collobert et al., 2011; Xiao et al., 2011; Seltzer and Droppo, 2013; Devries et al., 2014; Xia and Liu, 2015; Luong et al., 2015; Anders and Goldberg, 2016; Kazuma et al., 2016; Andor et al., 2016; Jonathan et al., 2016; Makoto and Bansal, 2016). To the best of our knowledge multi-task learning has not been studied to detect emotion in multigenre text input. The closest work to ours is the work of (Xia and Liu, 2015). In the latter study of (Xia and Liu, 2015), they proposed a multi-task learning framework that leverages activation and valence information for acoustic emotion recognition. Our work contributes the following: a) we empirically illustrate that emotion cues can be learned robustly across genres by framing the problem as a Joint Mult"
C18-1246,P16-1231,0,0.0296751,"Task learning is inspired by human learning. As human, when we learn new tasks, often we apply the knowledge we have gathered from related tasks. In the literature, multi-task learning comes in different forms: joint learning, learning to learn, and learning with auxiliary tasks. The following studies show the benefit of multi-task learning in closely-related or different type of tasks (R, 1993; Ronan and Jason, 2008; Collobert et al., 2011; Xiao et al., 2011; Seltzer and Droppo, 2013; Devries et al., 2014; Xia and Liu, 2015; Luong et al., 2015; Anders and Goldberg, 2016; Kazuma et al., 2016; Andor et al., 2016; Jonathan et al., 2016; Makoto and Bansal, 2016). To the best of our knowledge multi-task learning has not been studied to detect emotion in multigenre text input. The closest work to ours is the work of (Xia and Liu, 2015). In the latter study of (Xia and Liu, 2015), they proposed a multi-task learning framework that leverages activation and valence information for acoustic emotion recognition. Our work contributes the following: a) we empirically illustrate that emotion cues can be learned robustly across genres by framing the problem as a Joint Multi-Task learning problem. 7 Conclusion Com"
C18-1246,W13-1602,0,0.0238861,"cues). Mohammad (2012) investigated word-level affect lexicons ¨ features on sentence-level emotion detection. Ozbal and Daniele (2013) showed the effect of incorporating different levels of syntactic and semantic information on sentence level emotion detection. In recent years, unlimited access to social media data such as Twitter and Facebook, enabled the community to have access to large amount of data. In these works researchers have used supervised learning model trained on lexical, semantic, and stylistic features to classify emotion in Twitter (Wenbo et al., 2012; Roberts et al., 2012; Ashequl and Ellen, 2013; Yan, 2014; Saif and Svetlana, 2015; Yan and Howard, 2016; Svitlana and Yoram, 2016). Muhammad and Ungar (2017) proposed a gated recurrent neural network architecture to classify emotion in tweets. 6.2 Multi-Task Learning in Deep Neural Net Multi-Task learning is inspired by human learning. As human, when we learn new tasks, often we apply the knowledge we have gathered from related tasks. In the literature, multi-task learning comes in different forms: joint learning, learning to learn, and learning with auxiliary tasks. The following studies show the benefit of multi-task learning in closel"
C18-1246,P05-1015,0,0.394731,"Missing"
C18-1246,S07-1013,0,0.0502206,"k Emotion Detection and Classification Emotion detection has attracted several NLP applications like chatbots, stock market, and human personality analysis. Several studies investigated the problem in various genres. We present some of the studies most relevant to this paper. In the literature, emotion detection is cast as a classification problem, hence, the objective is to effectively learn emotion cues. Among the feature engineering approaches, we review the following works:Gilad (2005) collected a set of blog posts - online diary entries - which include an indication of the writers’ mood. Carlo and Rada (2007) collected and manually labeled 1,250 headlines (HLN) for emotion classification and valence (i.e. positive, negative) using the 6 basic emotions identified by Ekman (Paul, 1992) (EK6) tags. Saima and Stan (2007) collected and labeled a blog posts corpus (BLG) using EK6 tags on both the sentence and the phrase levels, they annotated emotion categories, emotion intensity, and identifying emotion phrases in blog posts. Diman et al. (2010) experimented with hierarchical classification for emotion analysis which considers the relation between neutrality, polarity and emotion of a text. Diana and C"
C18-1246,D14-1179,0,0.0146282,"vs. BHM which is annotated completely manually. We balance the distribution of labeled data per emotion across the two tasks. Recurrent Neural Network (RNN)- has been widely used in the literature to model sequential problems. RNN applies the same set of weights recursively as follow: ht = f(Wxt + U ht−1 + b) (1) RNN input vector xt ∈ Rn at time step t is calculated based on a hidden state and an input from the current state based on Eq. 1. The function f is a nonlinearity such as tanh or ReLU. Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Neural Nets- (Cho, 2014; Chung, 2015) are implementations of RNNs that circumvent some of the major issues with RNNs as they are much better at capturing long-term dependencies, and dealing with the vanishing gradient problem (Bengio et al., 1994; Pascanu et al., 2013). Gated Recurrent Neural Nets- (Cho, 2014; Chung, 2015) is very similar to LSTM with the following equations: rt = σ(Wxrt + U r ht−1 + br ) (2) zt = σ(Wxzt + U z ht−1 + bz ) (3) ˆ ˆ hˆt = tanh(Wxt + rt × U h ht−1 + bh ) (4) ht = zt × ht−1 + (1 − zt ) × hˆt (5) GRU has two gates, a reset gate rt , and an update gate zt . Intuitively, the reset gate dete"
C18-1246,W10-0208,0,0.295286,"Rada (2007) collected and manually labeled 1,250 headlines (HLN) for emotion classification and valence (i.e. positive, negative) using the 6 basic emotions identified by Ekman (Paul, 1992) (EK6) tags. Saima and Stan (2007) collected and labeled a blog posts corpus (BLG) using EK6 tags on both the sentence and the phrase levels, they annotated emotion categories, emotion intensity, and identifying emotion phrases in blog posts. Diman et al. (2010) experimented with hierarchical classification for emotion analysis which considers the relation between neutrality, polarity and emotion of a text. Diana and Carlo (2010) presented a categorical model and dimensional model for recognition of affective states (emotion cues). Mohammad (2012) investigated word-level affect lexicons ¨ features on sentence-level emotion detection. Ozbal and Daniele (2013) showed the effect of incorporating different levels of syntactic and semantic information on sentence level emotion detection. In recent years, unlimited access to social media data such as Twitter and Facebook, enabled the community to have access to large amount of data. In these works researchers have used supervised learning model trained on lexical, semantic,"
C18-1246,W10-0217,0,0.0308499,"proaches, we review the following works:Gilad (2005) collected a set of blog posts - online diary entries - which include an indication of the writers’ mood. Carlo and Rada (2007) collected and manually labeled 1,250 headlines (HLN) for emotion classification and valence (i.e. positive, negative) using the 6 basic emotions identified by Ekman (Paul, 1992) (EK6) tags. Saima and Stan (2007) collected and labeled a blog posts corpus (BLG) using EK6 tags on both the sentence and the phrase levels, they annotated emotion categories, emotion intensity, and identifying emotion phrases in blog posts. Diman et al. (2010) experimented with hierarchical classification for emotion analysis which considers the relation between neutrality, polarity and emotion of a text. Diana and Carlo (2010) presented a categorical model and dimensional model for recognition of affective states (emotion cues). Mohammad (2012) investigated word-level affect lexicons ¨ features on sentence-level emotion detection. Ozbal and Daniele (2013) showed the effect of incorporating different levels of syntactic and semantic information on sentence level emotion detection. In recent years, unlimited access to social media data such as Twitt"
C18-1246,P16-1105,0,0.0310693,". As human, when we learn new tasks, often we apply the knowledge we have gathered from related tasks. In the literature, multi-task learning comes in different forms: joint learning, learning to learn, and learning with auxiliary tasks. The following studies show the benefit of multi-task learning in closely-related or different type of tasks (R, 1993; Ronan and Jason, 2008; Collobert et al., 2011; Xiao et al., 2011; Seltzer and Droppo, 2013; Devries et al., 2014; Xia and Liu, 2015; Luong et al., 2015; Anders and Goldberg, 2016; Kazuma et al., 2016; Andor et al., 2016; Jonathan et al., 2016; Makoto and Bansal, 2016). To the best of our knowledge multi-task learning has not been studied to detect emotion in multigenre text input. The closest work to ours is the work of (Xia and Liu, 2015). In the latter study of (Xia and Liu, 2015), they proposed a multi-task learning framework that leverages activation and valence information for acoustic emotion recognition. Our work contributes the following: a) we empirically illustrate that emotion cues can be learned robustly across genres by framing the problem as a Joint Multi-Task learning problem. 7 Conclusion Combination of different genre datasets can improve"
C18-1246,N12-1071,0,0.176442,"elines: a feature engineering architecture LIBLINEAR from the SVM family. We leverage LIBLINEAR architecture implementation in Weka.4 ; and a single GRU model architecture where we use all the data from both TweetEN and BHM in a single model. Feature Engineering Baseline: For the LIBLINEAR setting, we build our model combining a number of features: character and word n-grams (uni-gram and bi-gram); POS: presence of POS tags taken from PennTreebank; syntactic features like presence of adjective, adverbs, or negation; and semantic features like presence of emotion words based on EmoNet lexicon (Mohammad, 2012), and clause feature, which we explain below. We report weighted F1 scores across the 8 PL8 emotion tags. Clause feature - for this feature, we study the distribution of clauses emotion tags in multi-clausal sentences. We note that the majority of those sentences with multiple clauses tend to have clauses with specific emotion labels (e.g. sentence emotion tag joy, have clauses with tags trust, anticipation, no-emotion, and surprise). We model this feature as an 8-dimension vector, where each dimension represent one emotion tag with a binary value: 1 indicates the presence of sub-sentential em"
C18-1246,P17-1067,0,0.0211179,"Ozbal and Daniele (2013) showed the effect of incorporating different levels of syntactic and semantic information on sentence level emotion detection. In recent years, unlimited access to social media data such as Twitter and Facebook, enabled the community to have access to large amount of data. In these works researchers have used supervised learning model trained on lexical, semantic, and stylistic features to classify emotion in Twitter (Wenbo et al., 2012; Roberts et al., 2012; Ashequl and Ellen, 2013; Yan, 2014; Saif and Svetlana, 2015; Yan and Howard, 2016; Svitlana and Yoram, 2016). Muhammad and Ungar (2017) proposed a gated recurrent neural network architecture to classify emotion in tweets. 6.2 Multi-Task Learning in Deep Neural Net Multi-Task learning is inspired by human learning. As human, when we learn new tasks, often we apply the knowledge we have gathered from related tasks. In the literature, multi-task learning comes in different forms: joint learning, learning to learn, and learning with auxiliary tasks. The following studies show the benefit of multi-task learning in closely-related or different type of tasks (R, 1993; Ronan and Jason, 2008; Collobert et al., 2011; Xiao et al., 2011;"
C18-1246,D14-1162,0,0.0798916,"Missing"
C18-1246,roberts-etal-2012-empatweet,0,0.0360807,"ctive states (emotion cues). Mohammad (2012) investigated word-level affect lexicons ¨ features on sentence-level emotion detection. Ozbal and Daniele (2013) showed the effect of incorporating different levels of syntactic and semantic information on sentence level emotion detection. In recent years, unlimited access to social media data such as Twitter and Facebook, enabled the community to have access to large amount of data. In these works researchers have used supervised learning model trained on lexical, semantic, and stylistic features to classify emotion in Twitter (Wenbo et al., 2012; Roberts et al., 2012; Ashequl and Ellen, 2013; Yan, 2014; Saif and Svetlana, 2015; Yan and Howard, 2016; Svitlana and Yoram, 2016). Muhammad and Ungar (2017) proposed a gated recurrent neural network architecture to classify emotion in tweets. 6.2 Multi-Task Learning in Deep Neural Net Multi-Task learning is inspired by human learning. As human, when we learn new tasks, often we apply the knowledge we have gathered from related tasks. In the literature, multi-task learning comes in different forms: joint learning, learning to learn, and learning with auxiliary tasks. The following studies show the benefit of mult"
C18-1246,P16-1148,0,0.0668025,"ce-level emotion detection. Ozbal and Daniele (2013) showed the effect of incorporating different levels of syntactic and semantic information on sentence level emotion detection. In recent years, unlimited access to social media data such as Twitter and Facebook, enabled the community to have access to large amount of data. In these works researchers have used supervised learning model trained on lexical, semantic, and stylistic features to classify emotion in Twitter (Wenbo et al., 2012; Roberts et al., 2012; Ashequl and Ellen, 2013; Yan, 2014; Saif and Svetlana, 2015; Yan and Howard, 2016; Svitlana and Yoram, 2016). Muhammad and Ungar (2017) proposed a gated recurrent neural network architecture to classify emotion in tweets. 6.2 Multi-Task Learning in Deep Neural Net Multi-Task learning is inspired by human learning. As human, when we learn new tasks, often we apply the knowledge we have gathered from related tasks. In the literature, multi-task learning comes in different forms: joint learning, learning to learn, and learning with auxiliary tasks. The following studies show the benefit of multi-task learning in closely-related or different type of tasks (R, 1993; Ronan and Jason, 2008; Collobert et al"
C18-1246,L18-1199,1,0.842197,"Missing"
C18-1246,N16-2011,0,0.106271,"s ¨ features on sentence-level emotion detection. Ozbal and Daniele (2013) showed the effect of incorporating different levels of syntactic and semantic information on sentence level emotion detection. In recent years, unlimited access to social media data such as Twitter and Facebook, enabled the community to have access to large amount of data. In these works researchers have used supervised learning model trained on lexical, semantic, and stylistic features to classify emotion in Twitter (Wenbo et al., 2012; Roberts et al., 2012; Ashequl and Ellen, 2013; Yan, 2014; Saif and Svetlana, 2015; Yan and Howard, 2016; Svitlana and Yoram, 2016). Muhammad and Ungar (2017) proposed a gated recurrent neural network architecture to classify emotion in tweets. 6.2 Multi-Task Learning in Deep Neural Net Multi-Task learning is inspired by human learning. As human, when we learn new tasks, often we apply the knowledge we have gathered from related tasks. In the literature, multi-task learning comes in different forms: joint learning, learning to learn, and learning with auxiliary tasks. The following studies show the benefit of multi-task learning in closely-related or different type of tasks (R, 1993; Ronan and J"
C18-1246,E14-3005,0,0.0307282,"vestigated word-level affect lexicons ¨ features on sentence-level emotion detection. Ozbal and Daniele (2013) showed the effect of incorporating different levels of syntactic and semantic information on sentence level emotion detection. In recent years, unlimited access to social media data such as Twitter and Facebook, enabled the community to have access to large amount of data. In these works researchers have used supervised learning model trained on lexical, semantic, and stylistic features to classify emotion in Twitter (Wenbo et al., 2012; Roberts et al., 2012; Ashequl and Ellen, 2013; Yan, 2014; Saif and Svetlana, 2015; Yan and Howard, 2016; Svitlana and Yoram, 2016). Muhammad and Ungar (2017) proposed a gated recurrent neural network architecture to classify emotion in tweets. 6.2 Multi-Task Learning in Deep Neural Net Multi-Task learning is inspired by human learning. As human, when we learn new tasks, often we apply the knowledge we have gathered from related tasks. In the literature, multi-task learning comes in different forms: joint learning, learning to learn, and learning with auxiliary tasks. The following studies show the benefit of multi-task learning in closely-related o"
D08-1030,W03-2201,0,0.0628018,"ic Content Extraction (ACE)1 , the task of NER has garnered significant attention within the natural language processing (NLP) community. ACE has facilitated evaluation for different languages creating standardized test sets and evaluation metrics. NER systems are typically enabling sub-tasks within 1 http://www.nist.gov/speech/tests/ace/2004/doc/ace04evalplan-v7.pdf large NLP systems. The quality of the NER system has a direct impact on the quality of the overall NLP system. Evidence abound in the literature in areas such as Question Answering, Machine Translation, and Information Retrieval (Babych and Hartley, 2003; Ferr´andez et al., 2004; Toda and Kataoka, 2005). The most prominent NER systems approach the problem as a classification task: identifying the named entities (NE) in the text and then classifying them according to a set of designed features into one of a predefined set of classes (Bender et al., 2003). The number of classes differ depending on the data set. To our knowledge, to date, the approach is always to model the problem with a single set of features for all the classes simultaneously. This research, diverges from this view. We recognize that different classes are sensitive to differi"
D08-1030,W03-0420,0,0.181789,"/www.nist.gov/speech/tests/ace/2004/doc/ace04evalplan-v7.pdf large NLP systems. The quality of the NER system has a direct impact on the quality of the overall NLP system. Evidence abound in the literature in areas such as Question Answering, Machine Translation, and Information Retrieval (Babych and Hartley, 2003; Ferr´andez et al., 2004; Toda and Kataoka, 2005). The most prominent NER systems approach the problem as a classification task: identifying the named entities (NE) in the text and then classifying them according to a set of designed features into one of a predefined set of classes (Bender et al., 2003). The number of classes differ depending on the data set. To our knowledge, to date, the approach is always to model the problem with a single set of features for all the classes simultaneously. This research, diverges from this view. We recognize that different classes are sensitive to differing features. Hence, in this study, we aspire to discover the optimum feature set per NE class. We approach the NER task from a multi-classification perspective. We create a classifier for each NE class independently based on an optimal feature set, then combine the different classifiers for a global NER"
D08-1030,farber-etal-2008-improving,0,0.561935,"Missing"
D08-1030,P05-1071,0,0.257377,"Missing"
D08-1030,W03-0419,0,\N,Missing
D11-1051,S07-1060,0,0.0186248,"of 553 how many words = w are assigned topic z, also Applying Word Sense Disambiguation Techniques We add a sense node between the topic node and the word node based on two linguistic observations: a) Polysemy: many words have more than one meaning. A topic is more directly relevant to a word meaning (sense) than to a word due to polysemy; b) Synonymy: different words may share the same sense. WordNet explicitly models synonymy by linking synonyms to the same sense. In WordNet, each sense has an associated definition. It is worth noting that we model the sense-word relation differently from (Boyd-Graber and Blei, 2007), where in their model words are generated from topics, then senses are generated from words. In our model, we assume that during the generative process, the author picks a concept relevant to the topic, then thinks of a best word that represents that concept. Hence the word choice is dependent on the relatedness of the sense and its fit to the document context. In standard topic models, the topic of a word is sampled from the document level topic mixture θ. The underlying assumption is that all words in a document constitute the context of the target word. However, it is not the case in real"
D11-1051,D07-1109,0,0.362447,"n modeling) in WSD task is an indicator of the impact of using sense definitions in the text categorization task. WSD Data: We choose the all-words WSD task in which an unsupervised WSD system is required to disambiguate all the content words in documents. Our models are evaluated against the SemCor dataset. We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al., 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. Moreover, SemCor is also the dataset used in (Boyd-Graber et al., 2007), where a WordNet based topic model for WSD is introduced. The sense annotated words polysemous words TF-IDF Total 225992 187871 - Noun 86996 70529 0.422 Adjective 31729 21989 0.300 Adverb 18947 11498 0.153 Verb 88320 83855 0.182 Table 3: Statistics of SemCor per POS statistics of SemCor is listed in table 3. We use hyperparameters tuned from the text categorization task: αd =0.1, αs =0.01, β=0.01, δ=1, T =50, and try different values of  ∈ {10, 20, 40} and γ ∈ {0, 2, 10}. The Brown corpus and WordNet definitions corpus are used as augmented data, which means the dashed line in figure 1c will"
D11-1051,E09-1013,0,0.0172607,".2±3.4 34.2±3.4 34.0±3.3 33.0±3.1 35.1±5.4 Verb 15.8 59.6 17.1±1.6 15.9±0.7 17.3±2.4 17.8±1.4 17.3±0.7 17.5±1.4 18.4±1.2 17.6±0.7 17.0±0.9 Table 4: Disambiguation results per POS on polysemous words. generated by choosing a sense path in the hierarchy. Note that no topic information is on the sense path. If a word is generated from the hierarchy, then it is not assigned a topic. Their models based on different dictionaries improve perplexity. Recently, several systems have been proposed to apply topic models to WSD. Cai et al. (2007) incorporate topic features into a supervised WSD framework. Brody and Lapata (2009) place the sense induction in a Baysian framework by assuming each context word is generated from the target word’s senses, and a context is modeled as a multinomial distribution over the target word’s senses rather than topics. Li et al. (2010) design several systems that use latent topics to find a most likely sense based on the sense paraphrases (extracted from WordNet) and context. Their WSD models are unsupervised and outperform state-ofart systems. Our model borrows the local window idea from word sense disambiguation community. In graphbased WSD systems (Mihalcea, 2005; Sinha and Mihalc"
D11-1051,D07-1108,0,0.1119,"Missing"
D11-1051,P09-2074,0,0.0248917,"frequency is calculated based on the sense choice as present in sense annotated data; (2) our model is not designed for WSD, therefore it discards a lot of information when choosing the sense: in our model, the choice of a sense si is only dependent on two facts: the corresponding topic zi and word wi , while in (Li et al., 2010; Banerjee and Pedersen, 2003), they consider all the senses and words in the context words. 5 Related work Various topic models have been developed for many applications. Recently there is a trend of modeling document dependency (Dietz et al., 2007; Mei et al., 2008; Daume, 2009). However, topics are only inferred based on word cooccurrence, while word semantics are ignored. Boyd-Graber et al. (2007) are the first to integrate semantics into the topic model framework. They propose a topic model based on WordNet noun hierarchy for WSD. A word is assumed to be generated by first sampling a topic, then choosing a path from the root node of hierarchy to a sense node corresponding to that word. However, they only focus on WSD. They do not exploit word definitions, neither do they report results on text categorization. Chemudugunta et al. (2008) also incorporate a sense hie"
D11-1051,P10-1156,1,0.919766,"al topic mixtures can only extract global topics in online reviews (e.g., Creative Labs MP3 players and iPods) and ignores local topics (product features such as portability and battery). They design the Multi-grain LDA where the local topic of a word is only determined by topics of surrounding sentences. In word sense disambiguation (WSD), an even narrower context is taken into consideration, for instance in graph based WSD models (Mihalcea, 2005), the choice of a sense for a word only depends on a local window whose size equals the length of the sentence. Later in (Sinha and Mihalcea, 2007; Guo and Diab, 2010; Li et al., 2010), people use a fixed window size containing around 12 neighbor words for WSD. Accordingly, we adopt the WSD inspired local window strategy in our model. However, we do not employ the complicated schema in (Titov and McDonald, 2008). We simply hypothesize that the surrounding  words are semantically related to the considered word, and they construct a local sliding window for that target word. For a document d with Nd words, we represent it as Nd local windows – a window is created for each word. The model is illustrated in the left rectangle in figure 1b. The window size is"
D11-1051,P10-1116,0,0.343848,"n only extract global topics in online reviews (e.g., Creative Labs MP3 players and iPods) and ignores local topics (product features such as portability and battery). They design the Multi-grain LDA where the local topic of a word is only determined by topics of surrounding sentences. In word sense disambiguation (WSD), an even narrower context is taken into consideration, for instance in graph based WSD models (Mihalcea, 2005), the choice of a sense for a word only depends on a local window whose size equals the length of the sentence. Later in (Sinha and Mihalcea, 2007; Guo and Diab, 2010; Li et al., 2010), people use a fixed window size containing around 12 neighbor words for WSD. Accordingly, we adopt the WSD inspired local window strategy in our model. However, we do not employ the complicated schema in (Titov and McDonald, 2008). We simply hypothesize that the surrounding  words are semantically related to the considered word, and they construct a local sliding window for that target word. For a document d with Nd words, we represent it as Nd local windows – a window is created for each word. The model is illustrated in the left rectangle in figure 1b. The window size is fixed for each wor"
D11-1051,H05-1052,0,0.465207,"ll words in a document constitute the context of the target word. However, it is not the case in real world corpora. Titov and McDonald (2008) find that using global topic mixtures can only extract global topics in online reviews (e.g., Creative Labs MP3 players and iPods) and ignores local topics (product features such as portability and battery). They design the Multi-grain LDA where the local topic of a word is only determined by topics of surrounding sentences. In word sense disambiguation (WSD), an even narrower context is taken into consideration, for instance in graph based WSD models (Mihalcea, 2005), the choice of a sense for a word only depends on a local window whose size equals the length of the sentence. Later in (Sinha and Mihalcea, 2007; Guo and Diab, 2010; Li et al., 2010), people use a fixed window size containing around 12 neighbor words for WSD. Accordingly, we adopt the WSD inspired local window strategy in our model. However, we do not employ the complicated schema in (Titov and McDonald, 2008). We simply hypothesize that the surrounding  words are semantically related to the considered word, and they construct a local sliding window for that target word. For a document d wi"
D11-1051,S07-1016,0,0.0174252,"irectly correlated with the choice of a correct topic in our framework. Accordingly, a relative improvement of STMn over STM0 (where the only difference is the explicit sense definition modeling) in WSD task is an indicator of the impact of using sense definitions in the text categorization task. WSD Data: We choose the all-words WSD task in which an unsupervised WSD system is required to disambiguate all the content words in documents. Our models are evaluated against the SemCor dataset. We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al., 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. Moreover, SemCor is also the dataset used in (Boyd-Graber et al., 2007), where a WordNet based topic model for WSD is introduced. The sense annotated words polysemous words TF-IDF Total 225992 187871 - Noun 86996 70529 0.422 Adjective 31729 21989 0.300 Adverb 18947 11498 0.153 Verb 88320 83855 0.182 Table 3: Statistics of SemCor per POS statistics of SemCor is listed in table 3. We use hyperparameters tuned from the text categorization task: αd =0.1, αs =0.01, β=0.01, δ=1, T =50"
D11-1051,W04-0811,0,0.0380388,"that the choice of the correct sense is directly correlated with the choice of a correct topic in our framework. Accordingly, a relative improvement of STMn over STM0 (where the only difference is the explicit sense definition modeling) in WSD task is an indicator of the impact of using sense definitions in the text categorization task. WSD Data: We choose the all-words WSD task in which an unsupervised WSD system is required to disambiguate all the content words in documents. Our models are evaluated against the SemCor dataset. We prefer SemCor to all-words datasets available in Senseval-3 (Snyder and Palmer, 2004) or SemEval-2007 (Pradhan et al., 2007), since it includes many more documents than either set (350 versus 3) and therefore allowing more reliable results. Moreover, SemCor is also the dataset used in (Boyd-Graber et al., 2007), where a WordNet based topic model for WSD is introduced. The sense annotated words polysemous words TF-IDF Total 225992 187871 - Noun 86996 70529 0.422 Adjective 31729 21989 0.300 Adverb 18947 11498 0.153 Verb 88320 83855 0.182 Table 3: Statistics of SemCor per POS statistics of SemCor is listed in table 3. We use hyperparameters tuned from the text categorization task"
D19-1127,E89-1039,0,0.390771,"Missing"
D19-1127,N18-2118,0,0.0342412,"various contextual signals’ contributions to model performance 2 Related Work There have been numerous advancements in NLU systems for dialogues over the past two decades. While the traditional approaches used handcrafted features and word n-gram based features fed to SVM, logistic regression, etc. for IC task and conditional random fields (CRF) for SL task (Jeong and Lee, 2008; Wang1 et al., 2002; Raymond and Riccardi, 2007), more recent approaches rely on deep neural networks to jointly model IC and SL tasks (Yao et al., 2014a,b; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016c; Goo et al., 2018). Attention as introduced by Bahdanau et al. (2014) has played a major role in many of these systems (Liu and Lane, 2016a; Ma et al., 2017; Li et al., 2018a; Goo et al., 2018), for instance, for modeling interaction between intents and slots in (Goo et al., 2018). Dahlb¨ack and J¨onsson (1989) and Bertomeu et al. (2006) studied contextual phenomena and thematic relations in natural language, thereby highlighting the importance of using context. Few previous works focused on modeling turn-level predictions as DST task (Williams et al., 2013). However, these systems predict the possible slotvalu"
D19-1127,H90-1021,0,0.373179,"Missing"
D19-1127,W13-4065,0,0.0152098,"o et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016c; Goo et al., 2018). Attention as introduced by Bahdanau et al. (2014) has played a major role in many of these systems (Liu and Lane, 2016a; Ma et al., 2017; Li et al., 2018a; Goo et al., 2018), for instance, for modeling interaction between intents and slots in (Goo et al., 2018). Dahlb¨ack and J¨onsson (1989) and Bertomeu et al. (2006) studied contextual phenomena and thematic relations in natural language, thereby highlighting the importance of using context. Few previous works focused on modeling turn-level predictions as DST task (Williams et al., 2013). However, these systems predict the possible slotvalue pairs at utterance level (Zhong et al., 2018), making it necessary to maintain ontology of all possible slot values, which is infeasible for certain slot types (e.g., restaurant names). In industry settings, where IC-SL task is predominant, there is also an additional effort involved to invest in rules for converting utterance level dialog state annotations to token level annotations required for SL. Hence, our work mainly focuses on the IC-SL task which eliminates the need for maintaining any ontology or such handcrafted rules. Bhargava"
D19-1127,D18-1417,0,0.234385,"st two decades. While the traditional approaches used handcrafted features and word n-gram based features fed to SVM, logistic regression, etc. for IC task and conditional random fields (CRF) for SL task (Jeong and Lee, 2008; Wang1 et al., 2002; Raymond and Riccardi, 2007), more recent approaches rely on deep neural networks to jointly model IC and SL tasks (Yao et al., 2014a,b; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016c; Goo et al., 2018). Attention as introduced by Bahdanau et al. (2014) has played a major role in many of these systems (Liu and Lane, 2016a; Ma et al., 2017; Li et al., 2018a; Goo et al., 2018), for instance, for modeling interaction between intents and slots in (Goo et al., 2018). Dahlb¨ack and J¨onsson (1989) and Bertomeu et al. (2006) studied contextual phenomena and thematic relations in natural language, thereby highlighting the importance of using context. Few previous works focused on modeling turn-level predictions as DST task (Williams et al., 2013). However, these systems predict the possible slotvalue pairs at utterance level (Zhong et al., 2018), making it necessary to maintain ontology of all possible slot values, which is infeasible for certain slot"
D19-1127,W16-3603,0,0.0898896,"ets • Analysis of the various contextual signals’ contributions to model performance 2 Related Work There have been numerous advancements in NLU systems for dialogues over the past two decades. While the traditional approaches used handcrafted features and word n-gram based features fed to SVM, logistic regression, etc. for IC task and conditional random fields (CRF) for SL task (Jeong and Lee, 2008; Wang1 et al., 2002; Raymond and Riccardi, 2007), more recent approaches rely on deep neural networks to jointly model IC and SL tasks (Yao et al., 2014a,b; Guo et al., 2014; Zhang and Wang, 2016; Liu and Lane, 2016c; Goo et al., 2018). Attention as introduced by Bahdanau et al. (2014) has played a major role in many of these systems (Liu and Lane, 2016a; Ma et al., 2017; Li et al., 2018a; Goo et al., 2018), for instance, for modeling interaction between intents and slots in (Goo et al., 2018). Dahlb¨ack and J¨onsson (1989) and Bertomeu et al. (2006) studied contextual phenomena and thematic relations in natural language, thereby highlighting the importance of using context. Few previous works focused on modeling turn-level predictions as DST task (Williams et al., 2013). However, these systems predict t"
D19-1127,E17-1042,0,0.0370265,"of slot labeling (SL). Over time, human-machine interactions have become more complex with greater reliance on contextual cues for utterance understanding (Figure 1). With traditional NLU frameworks, the resolution of contextual utterances is typically addressed in the DM component of the system using rule-based dialogue state trackers (DST). However, this pushes the problem of context resolution further down the dialogue pipeline, and despite the appeal of modularity in design, it opens the door for significant cascade of errors. To avoid this, end-to-end dialogue systems have been proposed (Wen et al., 2017; Bordes and Weston, 2016), but, to date, such systems are not scalable in industrial settings, and tend to be opaque where a level of transparency is needed, for instance, to understand various dialogue policies. To address the propagation of error while maintaining a modular framework, Shi et al. (2015) proposed adding contextual signals to the joint IC-SL task. However, the contributions of their work were limited in terms of number of signals and how they were used, rendering the contextualization process still less interpretable. In this work, we present a multi-dimensional selfattention"
D19-1151,W19-4606,1,0.88372,"Missing"
D19-1151,2016.amta-researchers.15,1,0.782389,"different languages: Arabic, Vietnamese, and Yoruba. To the best of our knowledge, TCN and A-TCN have not been investigated before for diacritization. In this paper, we show that A-TCN outperforms TCN while yielding comparable performance to BiLSTM but with the added advantage of being more efficient (speed and less computational footprint, i.e. decreased need for significant computational resources). 2 Related Work A fair number of studies have been developed for the task of diacritization for different languages that include diacritics (Yarowsky, 1994; De Pauw et al., 2007; Scannell, 2011; Alqahtani et al., 2016, 2019). Feature engineering and classical machine learning algorithms such as Hidden Markov Models, Maximum Entropy Models, and Finite State Transducer were the dominant approaches (Nelken and Shieber, 2005; Zitouni et al., 2006; Elshafei et al., 2006). However, recent studies show significant improvement using deep neural networks (Belinkov and Glass, 2015; Pham et al., 2017; Orife, 2018). While these deep models achieve state-of-the-art performance, they mainly rely on the use of recurrent architectures such as BiLSTM, which are relatively inefficient. Pham et al. (2017) view the task of di"
D19-1151,D15-1274,0,0.080631,"to increased ambiguity. Diacritization is the process of automatically restoring missing diacritics for each character in written text. It is important in many NLP applications such as automatic speech recognition (Vergyri and Kirchhoff, 2004) and speech synthesis (Ungurean et al., 2008). Most state-of-theart diacritization models use Bidirectional Long Short Term Memory (BiLSTM) (Hochreiter and Schmidhuber, 1997) as a sequential classification problem, or as a sequence-to-sequence transduction problem to convert the original text into diacritized form (Orife, 2018; Zalmout and Habash, 2017; Belinkov and Glass, 2015). Generally speaking, LSTM has shown great success for sequential data, leveraging long range dependencies and preserving the temporal order of the sequence (Cho et al., 2014; Graves, 2013). However, LSTM requires intensive computational resources both for training and inference due to its sequential nature. As an alternative, recent NLP technologies such as machine translation (Gehring et al., 2017) and language modeling (Dauphin et al., 2016) have investigated other models such as convolutional neural networks (CNN). Convolutional-based architectures utilize hierarchical rather than sequenti"
D19-1151,W14-4012,0,0.0424154,"Missing"
D19-1151,L18-1247,0,0.0314439,"Missing"
D19-1151,W05-0711,0,0.0624054,"le yielding comparable performance to BiLSTM but with the added advantage of being more efficient (speed and less computational footprint, i.e. decreased need for significant computational resources). 2 Related Work A fair number of studies have been developed for the task of diacritization for different languages that include diacritics (Yarowsky, 1994; De Pauw et al., 2007; Scannell, 2011; Alqahtani et al., 2016, 2019). Feature engineering and classical machine learning algorithms such as Hidden Markov Models, Maximum Entropy Models, and Finite State Transducer were the dominant approaches (Nelken and Shieber, 2005; Zitouni et al., 2006; Elshafei et al., 2006). However, recent studies show significant improvement using deep neural networks (Belinkov and Glass, 2015; Pham et al., 2017; Orife, 2018). While these deep models achieve state-of-the-art performance, they mainly rely on the use of recurrent architectures such as BiLSTM, which are relatively inefficient. Pham et al. (2017) view the task of diacritization for Vietnamese as a machine transduction problem from undiacritized to diacritized text at the word level. Orife (2018) addresses the problem on Yoruba in a similar way and compares softand self"
D19-1151,pasha-etal-2014-madamira,1,0.91681,"Missing"
D19-1151,W04-1612,0,0.0983638,"nic text (Scannell, 2011). Arabic, on the other hand, considers diacritics as an optional choice in writing even in formal settings,1 and familiarity with the language is relied upon to derive the meaning based on context. Although people can extrapolate missing diacritics with near perfect accuracy in such languages, missing diacritics pose a challenge for computational models due to increased ambiguity. Diacritization is the process of automatically restoring missing diacritics for each character in written text. It is important in many NLP applications such as automatic speech recognition (Vergyri and Kirchhoff, 2004) and speech synthesis (Ungurean et al., 2008). Most state-of-theart diacritization models use Bidirectional Long Short Term Memory (BiLSTM) (Hochreiter and Schmidhuber, 1997) as a sequential classification problem, or as a sequence-to-sequence transduction problem to convert the original text into diacritized form (Orife, 2018; Zalmout and Habash, 2017; Belinkov and Glass, 2015). Generally speaking, LSTM has shown great success for sequential data, leveraging long range dependencies and preserving the temporal order of the sequence (Cho et al., 2014; Graves, 2013). However, LSTM requires inten"
D19-1151,D17-1073,0,0.487659,"r computational models due to increased ambiguity. Diacritization is the process of automatically restoring missing diacritics for each character in written text. It is important in many NLP applications such as automatic speech recognition (Vergyri and Kirchhoff, 2004) and speech synthesis (Ungurean et al., 2008). Most state-of-theart diacritization models use Bidirectional Long Short Term Memory (BiLSTM) (Hochreiter and Schmidhuber, 1997) as a sequential classification problem, or as a sequence-to-sequence transduction problem to convert the original text into diacritized form (Orife, 2018; Zalmout and Habash, 2017; Belinkov and Glass, 2015). Generally speaking, LSTM has shown great success for sequential data, leveraging long range dependencies and preserving the temporal order of the sequence (Cho et al., 2014; Graves, 2013). However, LSTM requires intensive computational resources both for training and inference due to its sequential nature. As an alternative, recent NLP technologies such as machine translation (Gehring et al., 2017) and language modeling (Dauphin et al., 2016) have investigated other models such as convolutional neural networks (CNN). Convolutional-based architectures utilize hierar"
D19-1151,P06-1073,0,0.406989,"formance to BiLSTM but with the added advantage of being more efficient (speed and less computational footprint, i.e. decreased need for significant computational resources). 2 Related Work A fair number of studies have been developed for the task of diacritization for different languages that include diacritics (Yarowsky, 1994; De Pauw et al., 2007; Scannell, 2011; Alqahtani et al., 2016, 2019). Feature engineering and classical machine learning algorithms such as Hidden Markov Models, Maximum Entropy Models, and Finite State Transducer were the dominant approaches (Nelken and Shieber, 2005; Zitouni et al., 2006; Elshafei et al., 2006). However, recent studies show significant improvement using deep neural networks (Belinkov and Glass, 2015; Pham et al., 2017; Orife, 2018). While these deep models achieve state-of-the-art performance, they mainly rely on the use of recurrent architectures such as BiLSTM, which are relatively inefficient. Pham et al. (2017) view the task of diacritization for Vietnamese as a machine transduction problem from undiacritized to diacritized text at the word level. Orife (2018) addresses the problem on Yoruba in a similar way and compares softand self- attention sequence-t"
D19-1380,C18-1226,1,0.788362,") and Glove (Pennington et al., 2014) to contextualized embeddings such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). However, most applications operate at the phrase or sentence level. Hence, the word embeddings are averaged to yield sentence embeddings. Averaging is an efficient compositional operation that leads to good performance. In fact, averaging is difficult to beat by more complex ∗ Both authors contributed equally. compositional models as illustrated across several classification tasks: topic categorization, semantic textual similarity, and sentiment classification (Aldarmaki and Diab, 2018). Encoding sentences into fixed-length vectors that capture various full sentence linguistic properties leading to performance gains across different classification tasks remains a challenge. Given the complexity of most models that attempt to encode sentence structure, such as convolutional, recursive, or recurrent networks, the trade-off between efficiency and performance tips the balance in favor of simpler models like vector averaging. Sequential neural sentence encoders, like Skip-thought (Kiros et al., 2015) and InferSent (Conneau et al., 2017), can potentially encode rich semantic and s"
D19-1380,S17-2001,1,0.820605,", that address varieties of linguistic properties including surface, syntactic, and semantic information (Conneau et al., 2018). The downstream set, on the other hand, includes the following standard classification tasks: binary and fine-grained sentiment classification (MR, SST2, SST5) (Pang and Lee, 2004; Socher et al., 2013), product reviews (CR) (Hu and Liu, 2004), opinion polarity (MPQA) (Wiebe et al., 2005), question type classification (TREC) (Voorhees and Tice, 2000), natural language inference (SICK-E) (Marelli et al., 2014), semantic relatedness (SICK-R, STSB) (Marelli et al., 2014; Cer et al., 2017), paraphrase detection (MRPC) (Dolan et al., 2004), and subjectivity/objectivity (SUBJ) (Pang and Lee, 2004). 3.2 Experimental setup For the word embeddings, we use pre-trained FastText embeddings of size 300 (Mikolov et al., 2018) trained on Common-Crawl. We generate Table 1: Probing Tasks DCT sentence vectors by concatenating the first K DCT coefficients, which we denote by c0:K . We compare the performance against: vector averaging of the same word embeddings, denoted by AVG, and vector max pooling, denoted by MAX.4 For all tasks, we trained multi-layer perceptron (MLP) classifiers followin"
D19-1380,L18-1269,0,0.0295994,"maximum sentence ˆ and a given K, the total number of terms length N ˆ for each feature. The run-time comis (K − 1)N plexity is equivalent to calculating K weighted averages, which is proportional to KN , where K should be set to a small constant relative to the expected length.2 Note also that the number of input parameters in downstream classification models will increase linearly with K. With parallel implementations however, the difference in run-time complexity between AVG and DCT is practically negligible. 3 Experiments and Results 3.1 Evaluation Framework We use the SentEval toolkit3 (Conneau and Kiela, 2018) to evaluate the sentence representations on different probing as well as downstream classification tasks. The probing benchmark set was designed to analyze the quality of sentence embeddings. It contains a set of 10 classification tasks, summarized in Table 1, that address varieties of linguistic properties including surface, syntactic, and semantic information (Conneau et al., 2018). The downstream set, on the other hand, includes the following standard classification tasks: binary and fine-grained sentiment classification (MR, SST2, SST5) (Pang and Lee, 2004; Socher et al., 2013), product r"
D19-1380,D17-1070,0,0.0414666,"milarity, and sentiment classification (Aldarmaki and Diab, 2018). Encoding sentences into fixed-length vectors that capture various full sentence linguistic properties leading to performance gains across different classification tasks remains a challenge. Given the complexity of most models that attempt to encode sentence structure, such as convolutional, recursive, or recurrent networks, the trade-off between efficiency and performance tips the balance in favor of simpler models like vector averaging. Sequential neural sentence encoders, like Skip-thought (Kiros et al., 2015) and InferSent (Conneau et al., 2017), can potentially encode rich semantic and syntactic features from sentence structures. However, for practical applications, sequential models are rather cumbersome and inefficient, and the gains in performance are typically mediocre compared with vector averaging (Aldarmaki and Diab, 2018). In addition, the more complex models typically don’t generalize well to out-of-domain data (Wieting et al., 2015). FastSent (Hill et al., 2016) is an unsupervised alternative approach of lower computational cost, but similar to vector averaging, it disregards word order. Tensor-based composition can effect"
D19-1380,P18-1198,0,0.0537696,"Missing"
D19-1380,C04-1051,0,0.324228,"Missing"
D19-1380,N16-1162,0,0.0498193,"Missing"
D19-1380,P19-1445,0,0.113508,"8 72.24 72.20 20-NG R 54.67 59.16 68.19 71.65 71.79 70.79 66.40 71.62 71.58 F1 54.77 59.78 68.25 71.79 71.36 70.88 66.54 71.78 71.73 P 83.83 90.41 96.34 96.69 94.54 95.52 95.91 97.18 96.98 R-8 R 83.42 90.78 96.30 96.67 91.32 95.39 95.80 97.13 96.98 F1 83.41 90.38 96.27 96.65 91.32 95.39 95.76 97.14 96.94 P 26.47 30.11 27.88 33.77 42.35 39.92 35.32 42.77 37.67 SST-5 R 25.08 30.09 26.44 33.41 41.51 39.38 33.69 41.67 34.47 F1 25.23 29.53 24.81 33.26 41.54 39.35 33.91 41.81 34.54 Table 4: Performance in text classification (20-NG, R-8) and sentiment (SST-5) tasks of various models as reported in (Kayal and Tsatsaronis, 2019), where DCT* refers to the implementation in (Kayal and Tsatsaronis, 2019). Our DCT embeddings are denoted as ck in the bottom row. Bold indicates the best result, and italic indicates secondbest. frequency components, each revealing some information about the source signal, to enable analysis and compression. To the best of our knowledge, spectral methods have only been recently exploited to construct sentence embedding (Kayal and Tsatsaronis, 2019).5 . Kayal and Tsatsaronis propose EigenSent that utilized Higher-Order Dynamic Mode Decomposition (HODMD) (Le Clainche and Vega, 2017) to constru"
D19-1380,S14-2001,0,0.0637538,"ce embeddings. It contains a set of 10 classification tasks, summarized in Table 1, that address varieties of linguistic properties including surface, syntactic, and semantic information (Conneau et al., 2018). The downstream set, on the other hand, includes the following standard classification tasks: binary and fine-grained sentiment classification (MR, SST2, SST5) (Pang and Lee, 2004; Socher et al., 2013), product reviews (CR) (Hu and Liu, 2004), opinion polarity (MPQA) (Wiebe et al., 2005), question type classification (TREC) (Voorhees and Tice, 2000), natural language inference (SICK-E) (Marelli et al., 2014), semantic relatedness (SICK-R, STSB) (Marelli et al., 2014; Cer et al., 2017), paraphrase detection (MRPC) (Dolan et al., 2004), and subjectivity/objectivity (SUBJ) (Pang and Lee, 2004). 3.2 Experimental setup For the word embeddings, we use pre-trained FastText embeddings of size 300 (Mikolov et al., 2018) trained on Common-Crawl. We generate Table 1: Probing Tasks DCT sentence vectors by concatenating the first K DCT coefficients, which we denote by c0:K . We compare the performance against: vector averaging of the same word embeddings, denoted by AVG, and vector max pooling, denoted by MAX"
D19-1380,L18-1008,0,0.0625719,"Missing"
D19-1380,D14-1079,0,0.0314937,"rmance are typically mediocre compared with vector averaging (Aldarmaki and Diab, 2018). In addition, the more complex models typically don’t generalize well to out-of-domain data (Wieting et al., 2015). FastSent (Hill et al., 2016) is an unsupervised alternative approach of lower computational cost, but similar to vector averaging, it disregards word order. Tensor-based composition can effectively capture word order, but current approaches rely on restricted grammatical constructs, such as transitive phrases, and cannot be easily extended to variable-length sequences of arbitrary structures (Milajevs et al., 2014). Therefore, despite its obvious disregard for structural properties, the efficiency and reasonable performance of vector averaging makes them more suitable for practical text classification. In this work, we propose to use the Discrete Cosine Transform (DCT) as a simple and efficient way to model word order and structure in sentences while maintaining practical efficiency. 3672 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3672–3678, c Hong Kong, China, November 3–7, 2019."
D19-1380,P04-1035,0,0.0630142,"use the SentEval toolkit3 (Conneau and Kiela, 2018) to evaluate the sentence representations on different probing as well as downstream classification tasks. The probing benchmark set was designed to analyze the quality of sentence embeddings. It contains a set of 10 classification tasks, summarized in Table 1, that address varieties of linguistic properties including surface, syntactic, and semantic information (Conneau et al., 2018). The downstream set, on the other hand, includes the following standard classification tasks: binary and fine-grained sentiment classification (MR, SST2, SST5) (Pang and Lee, 2004; Socher et al., 2013), product reviews (CR) (Hu and Liu, 2004), opinion polarity (MPQA) (Wiebe et al., 2005), question type classification (TREC) (Voorhees and Tice, 2000), natural language inference (SICK-E) (Marelli et al., 2014), semantic relatedness (SICK-R, STSB) (Marelli et al., 2014; Cer et al., 2017), paraphrase detection (MRPC) (Dolan et al., 2004), and subjectivity/objectivity (SUBJ) (Pang and Lee, 2004). 3.2 Experimental setup For the word embeddings, we use pre-trained FastText embeddings of size 300 (Mikolov et al., 2018) trained on Common-Crawl. We generate Table 1: Probing Task"
D19-1380,D14-1162,0,0.102168,"robing tasks demonstrate that DCT embeddings indeed preserve more syntactic information compared with vector averaging. With practically equivalent complexity, the model yields better overall performance in downstream classification tasks that correlate with syntactic features, which illustrates the capacity of DCT to preserve word order information. 1 Introduction Modern NLP systems rely on word embeddings as input units to encode the statistical semantic and syntactic properties of words, ranging from standard context-independent embeddings such as word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) to contextualized embeddings such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). However, most applications operate at the phrase or sentence level. Hence, the word embeddings are averaged to yield sentence embeddings. Averaging is an efficient compositional operation that leads to good performance. In fact, averaging is difficult to beat by more complex ∗ Both authors contributed equally. compositional models as illustrated across several classification tasks: topic categorization, semantic textual similarity, and sentiment classification (Aldarmaki and Diab, 2018). Encoding s"
D19-1380,N18-1202,0,0.0586734,"ntactic information compared with vector averaging. With practically equivalent complexity, the model yields better overall performance in downstream classification tasks that correlate with syntactic features, which illustrates the capacity of DCT to preserve word order information. 1 Introduction Modern NLP systems rely on word embeddings as input units to encode the statistical semantic and syntactic properties of words, ranging from standard context-independent embeddings such as word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) to contextualized embeddings such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018). However, most applications operate at the phrase or sentence level. Hence, the word embeddings are averaged to yield sentence embeddings. Averaging is an efficient compositional operation that leads to good performance. In fact, averaging is difficult to beat by more complex ∗ Both authors contributed equally. compositional models as illustrated across several classification tasks: topic categorization, semantic textual similarity, and sentiment classification (Aldarmaki and Diab, 2018). Encoding sentences into fixed-length vectors that capture various full sen"
D19-1380,D13-1170,0,0.00885118,"lkit3 (Conneau and Kiela, 2018) to evaluate the sentence representations on different probing as well as downstream classification tasks. The probing benchmark set was designed to analyze the quality of sentence embeddings. It contains a set of 10 classification tasks, summarized in Table 1, that address varieties of linguistic properties including surface, syntactic, and semantic information (Conneau et al., 2018). The downstream set, on the other hand, includes the following standard classification tasks: binary and fine-grained sentiment classification (MR, SST2, SST5) (Pang and Lee, 2004; Socher et al., 2013), product reviews (CR) (Hu and Liu, 2004), opinion polarity (MPQA) (Wiebe et al., 2005), question type classification (TREC) (Voorhees and Tice, 2000), natural language inference (SICK-E) (Marelli et al., 2014), semantic relatedness (SICK-R, STSB) (Marelli et al., 2014; Cer et al., 2017), paraphrase detection (MRPC) (Dolan et al., 2004), and subjectivity/objectivity (SUBJ) (Pang and Lee, 2004). 3.2 Experimental setup For the word embeddings, we use pre-trained FastText embeddings of size 300 (Mikolov et al., 2018) trained on Common-Crawl. We generate Table 1: Probing Tasks DCT sentence vectors"
D19-1460,W17-5526,0,0.0950737,"Missing"
D19-1460,D18-1547,0,0.0768374,"Missing"
D19-1460,P15-1166,0,0.0132237,"roader context. Only by seeing previous utterances, such as requests to book a flight on a specific day to a specific destination, can this task be performed. Additionally, a single intent can be phrased in multiple ways depending on context; “book my flight”, “finalize my reservation”, “Yes, the 6 pm one” may all be referring to a flight-booking intent. Hence, entire conversations, rather than independent utterances, must be collected. Such data is even more pertinent to modeling NLU and related tasks as they require large, varied, and ideally human-generated datasets. Moreover, recent work (Dong et al., 2015; Devlin et al., 2018) has shown the benefit of applying joint-training and transfer learning techniques to natural language processing tasks. However, these approaches have yet to become widely used in dialogue tasks, due to a lack of largescale datasets. Furthermore, the latest state of the art end-to-end neural approaches benefit from such training data even more so than past work on goal-oriented dialogues structured around slot filling (Lemon et al., 2006; Wang and Lemon, 2013). One way to simulate data—and not risk releasing personally identifying information—for a domain is to use a Wiz"
D19-1460,H90-1021,0,0.30112,"tiple existing goal-oriented dialogue collections generated by humans through Wizardof-Oz techniques. The Dialog State Tracking Challenge, aka Dialog Systems Technology Challenge, (DSTC) spans 8 iterations and entails the domains of bus timetables, restaurant reservations, and hotel bookings, travel, alarms, movies, etc. (Williams et al., 2016). Frames (Asri et al., 2017) has 1369 dialogues about vacation packages. MultiWOZ contains 10,438 dialogues about Cambridge hotels and restaurants (Budzianowski et al., 2018). There are several dialogue datasets that specialize in a single domain. ATIS (Hemphill et al., 1990) comprises speech data about airlines structured around formal airline flight tables. Similarly, the Google Airlines dataset purportedly contains 400,000 templated dialogues about airline reservations (Wei 4527 et al., 2018).2 The Ubuntu Dialogue Corpus has over a million dialogues about Ubuntu technical support (Lowe et al., 2015). On the other hand, Chit-chat style dialogues without goals have been popular since ELIZA and have been investigated with neural techniques (Weizenbaum, 1966; Li et al., 2016, 2017). However, these datasets cannot be used for modeling goal-oriented tasks. Related di"
D19-1460,P18-1031,0,0.015712,". Recall that Fastfood had the most diverse dialogues (biases) as per Table 4 and the lowest IAA as per Table 6. sification naturally lends itself to joint training, given agent DAs are shared among all domains. To explore the benefits of multi-domain training, we jointly train an agent DA classification model on all domains and report test results for each domain separately. These results are provided in Table 8. This straightforward technique leads to a consistent but less than one point improvement in F1 scores. We expect that more sophisticated transfer learning methods (Liu et al., 2017; Howard and Ruder, 2018) could generate larger improvements for these domains. Overall, our results demonstrate that there is still headroom for performance improvement, especially for the SL task, across all domains. Consequently, MultiDoGO should be a relevant benchmark for developing new state-of-theart NLU models for the foreseeable future. Sentence vs. Turn Level Annotation Units. Regarding the performance of the LSTM and ELM O models on sentence vs. turn level annotation units, our results suggest that turn level annotations increase the difficulty of the DA classification task. This finding is evidenced by DA"
D19-1460,E06-2009,0,0.0494424,"pertinent to modeling NLU and related tasks as they require large, varied, and ideally human-generated datasets. Moreover, recent work (Dong et al., 2015; Devlin et al., 2018) has shown the benefit of applying joint-training and transfer learning techniques to natural language processing tasks. However, these approaches have yet to become widely used in dialogue tasks, due to a lack of largescale datasets. Furthermore, the latest state of the art end-to-end neural approaches benefit from such training data even more so than past work on goal-oriented dialogues structured around slot filling (Lemon et al., 2006; Wang and Lemon, 2013). One way to simulate data—and not risk releasing personally identifying information—for a domain is to use a Wizard-of-Oz data gathering technique, which requires that participants in a conver4526 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4526–4536, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Role Turn A Hey there! Good morning. You’re connected to LMT Airways. How may I help you? C Hi, I wonder if you c"
D19-1460,D16-1127,0,0.039641,"2018). There are several dialogue datasets that specialize in a single domain. ATIS (Hemphill et al., 1990) comprises speech data about airlines structured around formal airline flight tables. Similarly, the Google Airlines dataset purportedly contains 400,000 templated dialogues about airline reservations (Wei 4527 et al., 2018).2 The Ubuntu Dialogue Corpus has over a million dialogues about Ubuntu technical support (Lowe et al., 2015). On the other hand, Chit-chat style dialogues without goals have been popular since ELIZA and have been investigated with neural techniques (Weizenbaum, 1966; Li et al., 2016, 2017). However, these datasets cannot be used for modeling goal-oriented tasks. Related dialogue dataset collections used for Sequential Question Answering rely on dialogue to answer questions, but the task is notably different from our use case of modeling goal oriented conversational AI, hence leading to different evaluation considerations (Reddy et al., 2019; Choi et al., 2018). 3 3.1 MultiDoGO Dataset Curation Data Collection Procedure We employ both internal data associates, who we train, and crowd-sourced workers from Mechanical Turk (MTurkers) to generate conversational data using a W"
D19-1460,D17-1230,0,0.0714893,"Missing"
D19-1460,P17-1001,0,0.017317,"absolute F1 scores. Recall that Fastfood had the most diverse dialogues (biases) as per Table 4 and the lowest IAA as per Table 6. sification naturally lends itself to joint training, given agent DAs are shared among all domains. To explore the benefits of multi-domain training, we jointly train an agent DA classification model on all domains and report test results for each domain separately. These results are provided in Table 8. This straightforward technique leads to a consistent but less than one point improvement in F1 scores. We expect that more sophisticated transfer learning methods (Liu et al., 2017; Howard and Ruder, 2018) could generate larger improvements for these domains. Overall, our results demonstrate that there is still headroom for performance improvement, especially for the SL task, across all domains. Consequently, MultiDoGO should be a relevant benchmark for developing new state-of-theart NLU models for the foreseeable future. Sentence vs. Turn Level Annotation Units. Regarding the performance of the LSTM and ELM O models on sentence vs. turn level annotation units, our results suggest that turn level annotations increase the difficulty of the DA classification task. This fi"
D19-1460,W15-4640,0,0.206555,"l., 2016). Frames (Asri et al., 2017) has 1369 dialogues about vacation packages. MultiWOZ contains 10,438 dialogues about Cambridge hotels and restaurants (Budzianowski et al., 2018). There are several dialogue datasets that specialize in a single domain. ATIS (Hemphill et al., 1990) comprises speech data about airlines structured around formal airline flight tables. Similarly, the Google Airlines dataset purportedly contains 400,000 templated dialogues about airline reservations (Wei 4527 et al., 2018).2 The Ubuntu Dialogue Corpus has over a million dialogues about Ubuntu technical support (Lowe et al., 2015). On the other hand, Chit-chat style dialogues without goals have been popular since ELIZA and have been investigated with neural techniques (Weizenbaum, 1966; Li et al., 2016, 2017). However, these datasets cannot be used for modeling goal-oriented tasks. Related dialogue dataset collections used for Sequential Question Answering rely on dialogue to answer questions, but the task is notably different from our use case of modeling goal oriented conversational AI, hence leading to different evaluation considerations (Reddy et al., 2019; Choi et al., 2018). 3 3.1 MultiDoGO Dataset Curation Data"
D19-1460,W17-5506,0,0.0272455,"etween the customer and agent roles creates training data for a bot that explicitly simulates agents. Annotation Unit Granularity: Sentence vs. Turn Level An important decision, which is often under discussed, is the proper semantic DA 0.701 ISAA IC 0.728 SL 0.695 Table 2: Dialogue act (DA), Intent class (IC), and slot labeling (SL) Inter Source Annotation Agreement (ISAA) scores quantifying the agreement of crowd sourced and professional annotations. unit of text to annotate in a dialogue. Commonly, datasets provide annotations at the turn level (Budzianowski et al., 2018; Asri et al., 2017; Mihail et al., 2017). However, turn level annotations can introduce confusion for IC datasets, given multiple intents may be present in different sentences of a single turn. For instance, consider the turn “I would like to book a flight to San Francisco. Also, I want to cancel a flight to Austin.” Here, the first sentence has the BookFlight intent and the second sentence has the CancelFlight intent. An turn level annotation of this utterance would yield the multi-class intent (BookFlight, CancelFlight). In contrast, a sentence level annotation of this utterance identifies that the first sentence corresponds to Bo"
D19-1460,L18-1460,0,0.0291419,"K raw conversations of which 54,818 conversations are annotated at the turn level. We investigate multiple levels of annotation granularity. We annotate a subset of the data on both turn and sentence levels. A turn is defined as a sequence of one or more speech/text sentences by a participant in a conversation. A sentence is a period delimited sequence of words in a turn. A turn may comprise one or more sentences. We do use the term utterance to refer to a unit (turn or sentence, spoken or written by a participant).1 1 We acknowledge that the term utterance is controversial in the literature (Pareti and Lando, 2018) In our devised annotation strategy, we distinguish between dialogue speech acts for agents vs. customers. In MultiDoGO, the agents’ speech acts [DA] are annotated with generic class labels common across all domains, while customer speech acts are labeled with intent classes [IC]. Moreover, we annotate customer utterances with the appropriate slot labels [SL], which consist of the SL span and corresponding tokens with that SL tag. We present the strategies we use to curate and annotate such data given its contextual setting. We furthermore illustrate the efficacy of our devised approaches and"
D19-1460,N18-1202,0,0.0355317,"ularity. Red highlight denotes the strategy with the highest DA F1 score across annotation granularities. split. However, our conversation level splits result in imbalanced intent and slot label distributions. Models: We evaluate the performance of two neural models on each domain. The first is a bi-directional LSTM (Hochreiter and Schmidhuber, 1997) with GloVe word embeddings, a hidden state of size 512, and two fully connected output layers for slot labels and intent classes respectively. The second model, ELM O, is similar to the LSTM architecture but it addiitonally uses pretrained ELM O (Peters et al., 2018) embeddings in addition to GloVe word embeddings, which are kept frozen during training. We combine these ELM O and GloVe embeddings via concatenation. As a sanity check, we also include a most frequent class (MFC) baseline. The MFC baseline assigns the most frequent class label in the training split to every utterance u0 in the test split for both DA and IC tasks. To adapt the MFC baseline to SL, we compute the most frequent slot label MFC(w) for each word type w in the training set. Then given a test utterance u0 , we assign the pre-computed, most frequent slot MFC(w0 ) to each word w0 ∈ u0"
D19-1460,W13-4067,0,0.0130692,"ng NLU and related tasks as they require large, varied, and ideally human-generated datasets. Moreover, recent work (Dong et al., 2015; Devlin et al., 2018) has shown the benefit of applying joint-training and transfer learning techniques to natural language processing tasks. However, these approaches have yet to become widely used in dialogue tasks, due to a lack of largescale datasets. Furthermore, the latest state of the art end-to-end neural approaches benefit from such training data even more so than past work on goal-oriented dialogues structured around slot filling (Lemon et al., 2006; Wang and Lemon, 2013). One way to simulate data—and not risk releasing personally identifying information—for a domain is to use a Wizard-of-Oz data gathering technique, which requires that participants in a conver4526 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4526–4536, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Role Turn A Hey there! Good morning. You’re connected to LMT Airways. How may I help you? C Hi, I wonder if you can confirm my seat assi"
D19-1460,D18-1419,0,0.0922691,"Missing"
D19-5004,P09-2041,0,0.0357382,"is organized as follows: in section 2, we briefly review studies on fake news and satire articles which are the most relevant to our work. In section 3, we present the methods we use to investigate semantic and linguistic differences between fake and satire articles. Next, we evaluate these methods and share insights on nuances between fake news and satire in section 4. Finally, we conclude the paper in section 5 and outline next steps and future work. 2 Related Work Previous work addressed the challenge of identifying fake news (Conroy et al., 2015; Shu et al., 2017), or identifying satire (Burfoot and Baldwin, 2009; Reganti et al., 2016; Rubin et al., 2016), in isolation, compared to real news stories. The most relevant work to ours is that of Golbeck et al. (Golbeck et al., 2018). They introduced a dataset of fake news and satirical articles, which we also employ in this work. The dataset includes the full text of 283 fake news stories and 203 satirical stories, posted between January 2016 and October 2017, with a main focus on American politics. These fake and satirical stories were verified manually such that each fake news article is paired with a rebutting article from a reliable source. This data"
D19-5004,P18-1031,0,0.0138219,"uage representations, meaning that it is pre-trained on a large text corpus and then used for downstream NLP tasks. Word2Vec (Mikolov et al., 2013) showed that we can use vectors to properly represent words in a way that captures semantic or meaning-related relationships. While Word2Vec is a context-free model that generates a single word-embedding for each word in the vocabulary, BERT generates a representation of each word that is based on the other words in the sentence. It was built upon recent work in pre-training contextual representations, such as ELMo (Peters et al., 2018) and ULMFit (Howard and Ruder, 2018), and is deeply bidirectional, representing each word using both its left and right context. We use the pre-trained models of BERT and fine-tune it on the dataset of fake news and satire articles using Adam optimizer with 3 types of decay and 0.01 decay rate. Our BERT-based binary classifier is created by adding a single new layer in BERT’s neural network architecture that will be trained to fine-tune BERT to our task of classifying fake news and satire articles. 3.2 Linguistic Analysis with Coh-Metrix Inspired by previous work on satire detection, and specifically Rubin et al. (Rubin et al.,"
D19-5004,N18-1202,0,0.0259184,"is a method for pre-training language representations, meaning that it is pre-trained on a large text corpus and then used for downstream NLP tasks. Word2Vec (Mikolov et al., 2013) showed that we can use vectors to properly represent words in a way that captures semantic or meaning-related relationships. While Word2Vec is a context-free model that generates a single word-embedding for each word in the vocabulary, BERT generates a representation of each word that is based on the other words in the sentence. It was built upon recent work in pre-training contextual representations, such as ELMo (Peters et al., 2018) and ULMFit (Howard and Ruder, 2018), and is deeply bidirectional, representing each word using both its left and right context. We use the pre-trained models of BERT and fine-tune it on the dataset of fake news and satire articles using Adam optimizer with 3 types of decay and 0.01 decay rate. Our BERT-based binary classifier is created by adding a single new layer in BERT’s neural network architecture that will be trained to fine-tune BERT to our task of classifying fake news and satire articles. 3.2 Linguistic Analysis with Coh-Metrix Inspired by previous work on satire detection, and speci"
D19-5004,W16-0802,0,0.81077,"y review studies on fake news and satire articles which are the most relevant to our work. In section 3, we present the methods we use to investigate semantic and linguistic differences between fake and satire articles. Next, we evaluate these methods and share insights on nuances between fake news and satire in section 4. Finally, we conclude the paper in section 5 and outline next steps and future work. 2 Related Work Previous work addressed the challenge of identifying fake news (Conroy et al., 2015; Shu et al., 2017), or identifying satire (Burfoot and Baldwin, 2009; Reganti et al., 2016; Rubin et al., 2016), in isolation, compared to real news stories. The most relevant work to ours is that of Golbeck et al. (Golbeck et al., 2018). They introduced a dataset of fake news and satirical articles, which we also employ in this work. The dataset includes the full text of 283 fake news stories and 203 satirical stories, posted between January 2016 and October 2017, with a main focus on American politics. These fake and satirical stories were verified manually such that each fake news article is paired with a rebutting article from a reliable source. This data carries two desirable properties. First, th"
diab-etal-2014-tharwa,C12-2029,1,\N,Missing
diab-etal-2014-tharwa,W11-2602,1,\N,Missing
diab-etal-2014-tharwa,2009.mtsummit-caasl.5,1,\N,Missing
diab-etal-2014-tharwa,P02-1040,0,\N,Missing
diab-etal-2014-tharwa,P11-2062,1,\N,Missing
diab-etal-2014-tharwa,N13-1036,1,\N,Missing
diab-etal-2014-tharwa,habash-etal-2012-conventional,1,\N,Missing
diab-etal-2014-tharwa,pasha-etal-2014-madamira,1,\N,Missing
diab-etal-2014-tharwa,P13-2081,1,\N,Missing
diab-etal-2014-tharwa,maamouri-etal-2006-developing,1,\N,Missing
diab-etal-2014-tharwa,W12-2301,1,\N,Missing
diab-etal-2014-tharwa,P00-1056,0,\N,Missing
E06-1047,N04-4038,1,0.632821,"biguous in MSA. For example, the LA words  mn ‘from’ and  myn ‘who’ both are translated into an orthographically ambiguous form in MSA  mn ‘from’ or ‘who’. 5.1 Implementation Each word in the LA sentence is translated into a bag of MSA words, producing a sausage lattice. The lattice is scored and decoded using the SRILM toolkit with a trigram language model trained on 54 million MSA words from Arabic Gigaword (Graff, 2003). The text used for language modeling was tokenized to match the tokenization of the Arabic used in the ATB and LATB. The tokenization was done using the ASVM Toolkit (Diab et al., 2004). The 1-best path in the lattice is passed on to the Bikel parser (Bikel, 2002), which was trained on the MSA training ATB. Finally, the terminal nodes in the resulting parse structure are replaced with the original LA words. 5.2 Experimental Results Table 1 describes the results of the sentence transduction path on the development corpus (DEV) in different settings: using no POS tags in the input versus using gold POS tags in the input, and using SLXUN versus BLXUN. The baseline results are obtained by parsing the LA sentence directly using the MSA parser (with and without gold POS tags). The"
E06-1047,A00-1002,0,0.0265605,"ency trees for the MSA and LA sentences (not shown here for space considerations) are isomorphic. They differ only in the node labels. 5 Sentence Transduction In this approach, we parse an MSA translation of the LA sentence and then link the LA sentence to the MSA parse. Machine translation (MT) is not easy, especially when there are no MT resources available such as naturally occurring parallel text or transfer lexicons. However, for this task we have three encouraging insights. First, for really close languages it is possible to obtain better translation quality by means of simpler methods (Hajic et al., 2000). Second, suboptimal MSA output can still be helpful for the parsing task without necessarily being fluent or accurate (since our goal is parsing LA, not translating it to MSA). And finally, translation from LA to MSA is easier than from MSA to LA. This is a result of the availability of abundant resources for MSA as compared to LA: for example, text corpora and tree banks for 4 Levantine also has other negation markers that precede the verb, as well as the circumfi x m- -$. language modeling and a morphological generation system (Habash, 2004). One disadvantage of this approach is the lack of"
E06-1047,maamouri-etal-2006-developing,1,0.666828,"Missing"
E06-1047,J01-1004,1,0.72461,"can be thought of as a variant of the treebank-transduction approach in which the syntactic transformations are localized to elementary trees. Moreover, because a parsed MSA translation is produced as a byproduct, we can also think of this approach as being related to the sentence-transduction approach. 7.1 Preliminaries The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. In its present form, the formalism is tree-substitution grammar (Schabes, 1990) with an additional operation called sister-adjunction (Rambow et al., 2001). Because of space constraints, we omit discussion of the sister-adjunction operation in this paper. A tree-substitution grammar is a set of elementary trees. A frontier node labeled with a nonterminal label is called a substitution site. If an elementary tree has exactly one terminal symbol, that symbol is called its lexical anchor. A derivation starts with an elementary tree and proceeds by a series of composition operations. In the substitution operation, a substitution site is rewritten with an elementary tree with a matching root label. The final product is a tree with no more substitutio"
E06-1047,P00-1008,0,0.0658914,"Missing"
E06-1047,W04-3207,0,0.0165264,"LA and MSA (Section 4). We then proceed to discuss three approaches: sentence transduction, in which the LA sentence to be parsed is turned into an MSA sentence and then parsed with an MSA parser (Section 5); treebank transduction, in which the MSA treebank is turned into an LA treebank (Section 6); and grammar transduction, in which an MSA grammar is turned into an LA grammar which is then used for parsing LA (Section 7). We summarize and discuss the results in Section 8. 2 Related Work There has been a fair amount of interest in parsing one language using another language, see for example (Smith and Smith, 2004; Hwa et al., 2004) for recent work. Much of this work uses synchronized formalisms as do we in the grammar transduction approach. However, these approaches rely on parallel corpora. For MSA and its dialects, there are no naturally occurring parallel corpora. It is this fact that has led us to investigate the use of explicit linguistic knowledge to complement machine learning. We refer to additional relevant work in the appropriate sections. aries. The resulting development data comprises 1928 sentences and 11151 tokens (DEV). The test data comprises 2051 sentences and 10,644 tokens (TEST). Fo"
E06-1047,W00-1307,0,0.0120082,"stic synchronous TSG, using a straightforward generalization of the CKY and Viterbi algorithms, we obtain the highestprobability paired derivation which includes a parse for S on one side, and a parsed translation of S on the other side. It is also straightforward to calculate inside and outside probabilities for reestimation by Expectation-Maximization (EM). 7.2 An MSA-dialect synchronous grammar We now describe how we build our MSA-dialect synchronous grammar. As mentioned above, the MSA side of the grammar is extracted from the ATB in a process described by Chiang and others (Chiang, 2000; Xia et al., 2000; Chen, 2001). This process also gives us MSA-only substitution probabilities P (α |η). We then apply various transformation rules (described below) to the MSA elementary trees to produce a dialect grammar, at the same time assigning probabilities P (α0 |α). The synchronoussubstitution probabilities can then be estimated as: P (α, α0 |η, η 0 ) ≈ P (α |η)P (α0 |α) ≈ P (α |η)P (w 0 , t0 |w, t) P (¯ α0 |α ¯ , w0 , t0 , w, t) where w and t are the lexical anchor of α and its POS tag, and α ¯ is the equivalence class of α modulo lexical anchors and their POS tags. P (w0 , t0 |w, t) is assigned as d"
E06-1047,P00-1058,1,0.647244,"andwritten rules into dialect elementary trees to yield an MSAdialect synchronous grammar. This synchronous grammar can be used to parse new dialect sentences using statistics gathered from the MSA data. Thus this approach can be thought of as a variant of the treebank-transduction approach in which the syntactic transformations are localized to elementary trees. Moreover, because a parsed MSA translation is produced as a byproduct, we can also think of this approach as being related to the sentence-transduction approach. 7.1 Preliminaries The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. In its present form, the formalism is tree-substitution grammar (Schabes, 1990) with an additional operation called sister-adjunction (Rambow et al., 2001). Because of space constraints, we omit discussion of the sister-adjunction operation in this paper. A tree-substitution grammar is a set of elementary trees. A frontier node labeled with a nonterminal label is called a substitution site. If an elementary tree has exactly one terminal symbol, that symbol is called its lexical anchor. A derivation starts with an elemen"
E06-1047,W97-0119,0,\N,Missing
E06-1047,H05-1107,0,\N,Missing
E06-1047,J03-3002,0,\N,Missing
E06-1047,W02-2026,0,\N,Missing
E06-1047,N01-1020,0,\N,Missing
E06-1047,J90-2002,0,\N,Missing
E06-1047,P99-1067,0,\N,Missing
E06-1047,P95-1032,0,\N,Missing
E06-1047,W01-0713,0,\N,Missing
E06-1047,J00-2004,0,\N,Missing
E06-1047,N04-1034,0,\N,Missing
habash-etal-2012-conventional,I11-1036,1,\N,Missing
habash-etal-2012-conventional,P11-2007,0,\N,Missing
habash-etal-2012-conventional,N09-1045,1,\N,Missing
habash-etal-2012-conventional,elfardy-diab-2012-simplified,1,\N,Missing
I11-1036,2008.amta-papers.9,0,0.0619427,"Missing"
I11-1036,D08-1041,0,0.0290966,"Missing"
I11-1036,H05-1057,0,0.0472695,"Missing"
I11-1036,C90-2036,0,0.0810659,"Missing"
I11-1036,P05-1071,0,0.17942,"Missing"
I11-1036,C10-1045,0,0.0491416,"Missing"
I11-1036,P10-2063,0,0.0216834,"Missing"
I13-1168,W05-0909,0,0.0842614,"ods to get the different experimental conditions listed in Table 4. Here is some example input preprocessing for the same sentence according to different conditions: -Baseline (and all dynamic integration): invading iraqis kurdistan is no longer an easy task . -S_VAA1: invading iraqis kurdistan is no_longer an easy task . -S_NN: invading iraqis_kurdistan is no longer an easy task . -Z_VAA+NN: invading &lt;zone&gt; iraqis kurdistan &lt;/zone&gt; is &lt;zone&gt; no longer &lt;/zone&gt; an easy task . 5 Evaluation Results We used four standard MT metrics2; BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR3 (Banerjee and Lavie, 2005), and TER (Snover et al., 2006), to report and compare performance of different experimental conditions. Table 4, summarizes the results. The results show that, for the three integration methods (S, D and Z), the only conditions that help across all test-sets are S_VAA and D_VAA. S_VAA gives the best results except for METEOR where D_NN and D_NE are outperforming S_VAA. 1 We use the convention: IntegrationMethod_MWEScheme [-IntegrationMethod_MWEScheme]* to label different conditions: e.g “S_VAA-D_NE+NN” refers to a hybrid integration where the “VAA” MWEs are statically integrated and the “NE+N"
I13-1168,P05-1045,0,0.00764901,"ve list from the wide coverage English WordNet database 3.0. (Fellbaum,1998). Table 1 shows the number of MWEs extracted from WordNet 3.0 dictionaries. It is worth noting that the MWE.V list comprises all three types of verbal MWEs (VNC, VPC, LVC), moreover the MWE.N includes NNC and some NEs as listed in WordNet. 1182 MWE list # MWE types MWE.V 3,089 MWE.N 62,244 MWE.AJ 3,358 MWE.AV 826 Table 1: WordNet 3.0 based MWE statistics Named Entities Tagging: We consider Named Entities (NEs) as another type of MWE. To construct our NEs list, we exploit a named entity tagger, the Stanford NER [SNER] (Finkel et al., 2005). SNER tags named entities in a given English text into three categories: 1) Person 2) Organization and 3) Location. We are interested in Multiword NEs only and pay no attention to the different NE categories. The extracted NEs list consists of the 65616 Multiword NEs tagged by SNER in our training corpus. There are some overlaps between the NEs list and the MWE lists extracted from WordNet as shown in table 2. The large overlap is between the NEs list and the MWE.N, which contains NEs as listed in WordNet 3.0. Examples # types MWE.N 1216 abraham lincoln abu dhabi abu sayyaf adam smith addis a"
I13-1168,P07-2045,0,0.0308971,"grouped together with an underscore. While in Dynamic integration, the MWEs are identified in the phrase table and an additional weighted feature, as a soft constraint, is added to the phrase translation table. Carpuat and Diab (2010) focus only on MWEs as identified in WordNet (Fellbaum, 1998) with no explicit distinction between the different types of MWEs. Accordingly, the MWEs are considered a single type with no attention to various POS information. Our work here is taking a much fine grained approach and deeper study and analysis. 4 Approach We adopt a Phrase-based SMT framework, Moses (Koehn et al., 2007). In the following subsections, we address the issue of representation of MWE in our SMT pipeline and then we investigate the manner in which the MWE information is integrated in the SMT framework. 4.1 Data Sets For training the translation models, we use LDC GALE newswire parallel Arabic-English corpus (LDC2007E103) (a total of 474299 sentence pairs / about 10M un-tokenized words / 12M tokenized words). The Log-Linear model features weights are tuned using the newswire part of NIST MT06 (765 sentence pairs) as the tuning dataset and BLEU (Papineni et al., 2002) as the objective function. For"
I13-1168,2005.mtsummit-posters.11,0,0.134085,"e.g. “make a decision”) [LVC], and (3) decomposable idioms (e.g. “sweep under the rug”) [VNC]. 3 Related Work Previous work has focused on automatically learning and integrating translations of very specific MWE categories, such as, for instance, idiomatic Chinese four character expressions (Bai 1181 International Joint Conference on Natural Language Processing, pages 1181–1187, Nagoya, Japan, 14-18 October 2013. et al., 2009.) MWEs have also been defined not from a lexical semantics perspective but from a SMT error reduction perspective, as phrases that are hard to align during SMT training (Lambert and Banchs, 2005). For each of these particular cases, translation quality improved by augmenting the SMT translation lexicon with the learned bilingual MWEs either directly or through improved word alignments. Ren et al. (2009) described a method integrating an in-domain bilingual MWE to Moses by introducing an additional feature that identifies whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010) who replaced the binary feature by a count feature representing the number of MWEs in the source language phrase. They present results on a large data s"
I13-1168,P03-1021,0,0.0191301,"f the training data is also generated for use in alignment. We used SRILM toolkit (Stolcke, 2002) to create a 5-gram Arabic LM modified using Kneser-Ney smoothing. In all our experimental conditions, the parallel corpus is word-aligned using GIZA++ in both translation directions using the lemmatized version of both sides to decreases data sparseness, and phrase translations of up to 10 words are extracted from the tokenized version of both sides using the grow-diag-final-and heuristic (Koehn et al., 2007). We optimized log-linear model feature weights using Minimum Error Rate Training (MERT) (Och, 2003). To account for the instability of MERT, we run the tuning step three times per condition with different random seeds and use the optimized weights that give the median score. Integration Methods: (a) Static Integration (S) Matching Algorithm: In order to identify the MWE in the source English side of the parallel data, we use a Maximum Forward Matching algorithm that finds the longest matching MWE in the text. The algorithm matches over the tokenized version of the data and if no match, it backs-off to the lemmatized version to account for the different inflectional forms of the MWE (e.g. “t"
I13-1168,P02-1040,0,0.0857621,"Phrase-based SMT framework, Moses (Koehn et al., 2007). In the following subsections, we address the issue of representation of MWE in our SMT pipeline and then we investigate the manner in which the MWE information is integrated in the SMT framework. 4.1 Data Sets For training the translation models, we use LDC GALE newswire parallel Arabic-English corpus (LDC2007E103) (a total of 474299 sentence pairs / about 10M un-tokenized words / 12M tokenized words). The Log-Linear model features weights are tuned using the newswire part of NIST MT06 (765 sentence pairs) as the tuning dataset and BLEU (Papineni et al., 2002) as the objective function. For training the language model (LM), we use the LDC Arabic GIGAWORD 4th edition (LDC2009T30) (about 850M un-tokenized words). We use the newswire part of NIST-MT04 (707 sentences) as our development test-set to compare performance and select combinations of different conditions. We report results using two blind test-sets; NIST-MT05 (1056 sentences) and the newswire part of NIST-MT08 (813 sentences). These standard test sets are originally designed to test Arabic to English translation systems thus it consists of one Arabic source set and four English human referen"
I13-1168,W09-2907,0,0.0449398,"ies, such as, for instance, idiomatic Chinese four character expressions (Bai 1181 International Joint Conference on Natural Language Processing, pages 1181–1187, Nagoya, Japan, 14-18 October 2013. et al., 2009.) MWEs have also been defined not from a lexical semantics perspective but from a SMT error reduction perspective, as phrases that are hard to align during SMT training (Lambert and Banchs, 2005). For each of these particular cases, translation quality improved by augmenting the SMT translation lexicon with the learned bilingual MWEs either directly or through improved word alignments. Ren et al. (2009) described a method integrating an in-domain bilingual MWE to Moses by introducing an additional feature that identifies whether or not a bilingual phrase contains bilingual MWEs. This approach was generalized in Carpuat and Diab (2010) who replaced the binary feature by a count feature representing the number of MWEs in the source language phrase. They present results on a large data set of English to Arabic SMT. They introduce two ways of integrating MWE knowledge in the SMT framework: Static and Dynamic integration. For Static integration, MWE tokens in the source data are grouped together"
I13-1168,2006.amta-papers.25,0,0.0154326,"l conditions listed in Table 4. Here is some example input preprocessing for the same sentence according to different conditions: -Baseline (and all dynamic integration): invading iraqis kurdistan is no longer an easy task . -S_VAA1: invading iraqis kurdistan is no_longer an easy task . -S_NN: invading iraqis_kurdistan is no longer an easy task . -Z_VAA+NN: invading &lt;zone&gt; iraqis kurdistan &lt;/zone&gt; is &lt;zone&gt; no longer &lt;/zone&gt; an easy task . 5 Evaluation Results We used four standard MT metrics2; BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR3 (Banerjee and Lavie, 2005), and TER (Snover et al., 2006), to report and compare performance of different experimental conditions. Table 4, summarizes the results. The results show that, for the three integration methods (S, D and Z), the only conditions that help across all test-sets are S_VAA and D_VAA. S_VAA gives the best results except for METEOR where D_NN and D_NE are outperforming S_VAA. 1 We use the convention: IntegrationMethod_MWEScheme [-IntegrationMethod_MWEScheme]* to label different conditions: e.g “S_VAA-D_NE+NN” refers to a hybrid integration where the “VAA” MWEs are statically integrated and the “NE+NN” MWEs are dynamically integra"
I13-1168,D09-1050,0,\N,Missing
I13-1168,W10-3707,0,\N,Missing
I13-1168,bouamor-etal-2012-identifying,0,\N,Missing
I13-1168,D08-1076,0,\N,Missing
I13-2004,habash-etal-2006-design,1,0.651317,"h or h H. We address this problem in the context of natural language processing of Arabic dialect by proposing a conventional orthography for representing dialecIntroduction The Arabic language poses two problems for information retrieval (IR). First, Arabic is morphologically rich, which increases the likelihood of mismatch between words used in queries and words in documents. Much work has been done on addressing this issue in the context of Modern Standard Arabic (MSA), primarily using different methods of stemming and query reformulation (Al-Kharashi and Evens, 1999; Darwish et al., 2005; Habash et al., 2006; Larkey et al., 2007).1 Secondly, the Arabic-speaking world displays diglossia, meaning that a standard language, MSA, co-exists with dialects, such as Egyptian Arabic (EGY). The dialects differ from MSA in many dimensions, which limits the effectiveness of using MSA tools to handle the dialects. Relevant to IR are lexical and morphological differences. Lexically, different words may be used to 2 Arabic transliteration throughout the paper is presented in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007): (in alphabetical order) ˇ AbtθjHxdðrzsšSDTDςγfqklmnhwy and the additional ˇ ¯ sy"
I13-2004,W05-0704,0,0.0320462,"Y may be written as ë h or h H. We address this problem in the context of natural language processing of Arabic dialect by proposing a conventional orthography for representing dialecIntroduction The Arabic language poses two problems for information retrieval (IR). First, Arabic is morphologically rich, which increases the likelihood of mismatch between words used in queries and words in documents. Much work has been done on addressing this issue in the context of Modern Standard Arabic (MSA), primarily using different methods of stemming and query reformulation (Al-Kharashi and Evens, 1999; Darwish et al., 2005; Habash et al., 2006; Larkey et al., 2007).1 Secondly, the Arabic-speaking world displays diglossia, meaning that a standard language, MSA, co-exists with dialects, such as Egyptian Arabic (EGY). The dialects differ from MSA in many dimensions, which limits the effectiveness of using MSA tools to handle the dialects. Relevant to IR are lexical and morphological differences. Lexically, different words may be used to 2 Arabic transliteration throughout the paper is presented in the Habash-Soudi-Buckwalter scheme (Habash et al., 2007): (in alphabetical order) ˇ AbtθjHxdðrzsšSDTDςγfqklmnhwy and t"
I13-2004,W12-2301,1,0.952931,"éËðA£  ð P zawja¯h ék . hawlA’ ZBñë ˆ Table 1: Four examples showing lexical variation among Arabic dialects and MSA. convey the same meaning in different dialects and MSA. Table 1 presents the same set of four words in English, MSA, Egyptian Arabic and Levantine Arabic.2 Morphologically, the dialects may use different forms from MSA, e.g., the short phrase ‘he writes’  JºJ K. appears as I in MSA , but as I . JºK yaktubu . JºK X dayiktib in Iraqi Arabic biyiktib in EGY, I .  and I . JºJ » kayiktib in Moroccan Arabic. The differences between MSA and dialect morphology can be rather large: Habash et al. (2012a) report that over one-third of EGY words cannot be analyzed using an MSA morphological analyzer; and Habash and Rambow (2006) report similar figures for Levantine verbs. Furthermore, while MSA has a standard orthography, the dialects are not orthographically standardized, which leads to the coexistence of multiple spellings for the same word, e.g., the future marker in EGY may be written as ë h or h H. We address this problem in the context of natural language processing of Arabic dialect by proposing a conventional orthography for representing dialecIntroduction The Arabic language poses tw"
I13-2004,habash-etal-2012-conventional,1,0.905655,"éËðA£  ð P zawja¯h ék . hawlA’ ZBñë ˆ Table 1: Four examples showing lexical variation among Arabic dialects and MSA. convey the same meaning in different dialects and MSA. Table 1 presents the same set of four words in English, MSA, Egyptian Arabic and Levantine Arabic.2 Morphologically, the dialects may use different forms from MSA, e.g., the short phrase ‘he writes’  JºJ K. appears as I in MSA , but as I . JºK yaktubu . JºK X dayiktib in Iraqi Arabic biyiktib in EGY, I .  and I . JºJ » kayiktib in Moroccan Arabic. The differences between MSA and dialect morphology can be rather large: Habash et al. (2012a) report that over one-third of EGY words cannot be analyzed using an MSA morphological analyzer; and Habash and Rambow (2006) report similar figures for Levantine verbs. Furthermore, while MSA has a standard orthography, the dialects are not orthographically standardized, which leads to the coexistence of multiple spellings for the same word, e.g., the future marker in EGY may be written as ë h or h H. We address this problem in the context of natural language processing of Arabic dialect by proposing a conventional orthography for representing dialecIntroduction The Arabic language poses tw"
I13-2004,P06-1086,1,0.880511,"vey the same meaning in different dialects and MSA. Table 1 presents the same set of four words in English, MSA, Egyptian Arabic and Levantine Arabic.2 Morphologically, the dialects may use different forms from MSA, e.g., the short phrase ‘he writes’  JºJ K. appears as I in MSA , but as I . JºK yaktubu . JºK X dayiktib in Iraqi Arabic biyiktib in EGY, I .  and I . JºJ » kayiktib in Moroccan Arabic. The differences between MSA and dialect morphology can be rather large: Habash et al. (2012a) report that over one-third of EGY words cannot be analyzed using an MSA morphological analyzer; and Habash and Rambow (2006) report similar figures for Levantine verbs. Furthermore, while MSA has a standard orthography, the dialects are not orthographically standardized, which leads to the coexistence of multiple spellings for the same word, e.g., the future marker in EGY may be written as ë h or h H. We address this problem in the context of natural language processing of Arabic dialect by proposing a conventional orthography for representing dialecIntroduction The Arabic language poses two problems for information retrieval (IR). First, Arabic is morphologically rich, which increases the likelihood of mismatch be"
I17-2003,P15-1039,0,0.554224,"n of the greedy (local) model of Bj¨orkelund et al. (2009) except that we use an averaged perceptron algorithm (Freund and Schapire, 1999) as the learning algorithm. 2.1 Bootstrapping Bootstrapping (or self-training) is a simple but very useful technique that makes use of unlabeled data. A traditional self-training method (McClosky et al., 2006) labels unlabeled data (in our case, fill in missing SRL decisions) and adds that data to the labeled data for further training. We report results for this setting in §3.1 as fill–in. Although fill–in method is shown to be very useful in previous work (Akbik et al., 2015), empirically, we find that it is better to relabel all training instances (including the already labeled data) instead of only labeling unlabeled raw data. Therefore, the classifier is empowered to discover outliers (resulting from erroneous projections) and change their labels during the training process. Figure 1 illustrates our algorithm. It starts with training on the labeled data and uses the trained model to label the unlabeled data and relabel the already labeled data. This process repeats for a certain number of epochs until the model converges, i.e., reaches its maximum performance."
I17-2003,D17-1159,0,0.0204616,"benefit from information obtained from projection sparsity and syntactic similarity to weigh projections. We utilize a bootstrapping algorithm to train a SRL system over projections. We showed that we can get better results if we relabel the entire train data in each iteration as opposed to only labeling instances without projections. For the future work, we consider experimenting with newly published Universal Proposition Bank (Wang et al., 2017) that provides a unified labeling scheme for all languages. Given the recent success in SRL systems with neural networks (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017), we plan to use them for further improvement. We expect a similar trend by applying the same ideas in a neural SRL system. Related Work There have been several studies on transferring SRL systems (Pad´o and Lapata, 2005, 2009; Mukund et al., 2010; van der Plas et al., 2011, 2014; Kozhevnikov and Titov, 2013; Akbik et al., 2015). Pad´o and Lapata (2005), as one of the earliest studies on annotation projection for SRL using parallel resources, apply different heuristics and techniques to improve the quality of their model by focusing on having better word and constituent alignments. van der Pla"
I17-2003,N06-1020,0,0.0276726,"guage (TLang) through parallel data. Our SRL system is formed as a pipeline of classifiers consisting of a predicate identification and disambiguation module, an argument identification module, and an argument classification module. In particular, we use our re-implementation of the greedy (local) model of Bj¨orkelund et al. (2009) except that we use an averaged perceptron algorithm (Freund and Schapire, 1999) as the learning algorithm. 2.1 Bootstrapping Bootstrapping (or self-training) is a simple but very useful technique that makes use of unlabeled data. A traditional self-training method (McClosky et al., 2006) labels unlabeled data (in our case, fill in missing SRL decisions) and adds that data to the labeled data for further training. We report results for this setting in §3.1 as fill–in. Although fill–in method is shown to be very useful in previous work (Akbik et al., 2015), empirically, we find that it is better to relabel all training instances (including the already labeled data) instead of only labeling unlabeled raw data. Therefore, the classifier is empowered to discover outliers (resulting from erroneous projections) and change their labels during the training process. Figure 1 illustrate"
I17-2003,P13-2017,0,0.068304,"Missing"
I17-2003,P03-1068,0,0.451693,"Missing"
I17-2003,C10-1090,0,0.0195982,"ta in each iteration as opposed to only labeling instances without projections. For the future work, we consider experimenting with newly published Universal Proposition Bank (Wang et al., 2017) that provides a unified labeling scheme for all languages. Given the recent success in SRL systems with neural networks (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017), we plan to use them for further improvement. We expect a similar trend by applying the same ideas in a neural SRL system. Related Work There have been several studies on transferring SRL systems (Pad´o and Lapata, 2005, 2009; Mukund et al., 2010; van der Plas et al., 2011, 2014; Kozhevnikov and Titov, 2013; Akbik et al., 2015). Pad´o and Lapata (2005), as one of the earliest studies on annotation projection for SRL using parallel resources, apply different heuristics and techniques to improve the quality of their model by focusing on having better word and constituent alignments. van der Plas et al. (2011) improve an annotation projection model by jointly training a transfer system for parsing and SRL. They solely focus on fully projected annotations and train only on verbs. In this work, we train on all predicates Acknowledgments Th"
I17-2003,J03-1002,0,0.00842071,") and target (TLang) languages. We use the Universal part-of-speech tagset of Petrov et al. (2011) and the Google Universal Treebank (McDonald et al., 2013). We ignore the projection of the AM roles to German since this particular role does not appear in the German dataset. We use the standard data splits in the CoNLL shared task on SRL (Hajiˇc et al., 2009) for evaluation. We replace the POS and dependency information with the predictions from the Yara parser (Rasooli and Tetreault, 2015) trained on the Google Universal Treebank.1 We use the parallel Europarl corpus (Koehn, 2005) and Giza++ (Och and Ney, 2003) for extracting word alignments. Since predicate senses are projected from English to German, comparing projected senses with the gold German predicate sense is impossible. To address this, all evaluations are conducted using the Gold predicate sense. After filtering projections with density criteria of §2.1, 29417 of the sentences are preserved. The number of preserved sentences after filtering sparse alignments is roughly one percent of the 3.1 Results and Discussion Table 1 shows the results of different models on the German evaluation data. As we can see in the table, bootstrapping outperf"
I17-2003,2005.mtsummit-papers.11,0,0.0674489,"or both the source (SLang) and target (TLang) languages. We use the Universal part-of-speech tagset of Petrov et al. (2011) and the Google Universal Treebank (McDonald et al., 2013). We ignore the projection of the AM roles to German since this particular role does not appear in the German dataset. We use the standard data splits in the CoNLL shared task on SRL (Hajiˇc et al., 2009) for evaluation. We replace the POS and dependency information with the predictions from the Yara parser (Rasooli and Tetreault, 2015) trained on the Google Universal Treebank.1 We use the parallel Europarl corpus (Koehn, 2005) and Giza++ (Och and Ney, 2003) for extracting word alignments. Since predicate senses are projected from English to German, comparing projected senses with the gold German predicate sense is impossible. To address this, all evaluations are conducted using the Gold predicate sense. After filtering projections with density criteria of §2.1, 29417 of the sentences are preserved. The number of preserved sentences after filtering sparse alignments is roughly one percent of the 3.1 Results and Discussion Table 1 shows the results of different models on the German evaluation data. As we can see in t"
I17-2003,J05-1004,0,0.259218,"ith shallow semantic labels characterizing “Who did What to Whom, How, When and Where?” (Palmer et al., 2010). These rich semantic representations are useful in many applications such as question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011), hence gaining a lot of attention in recent years (Zhou and Xu, 2015; T¨ackstr¨om et al., 2015; Roth and Lapata, 2016; Marcheggiani et al., 2017). Since the process of creating annotated resources needs significant manual effort, SRL resources are available for a relative small number of languages such as English (Palmer et al., 2005), German (Erk et al., 2003), Arabic (Zaghouani et al., 2010) and Hindi (Vaidya et al., 2011). However, most languages still lack SRL systems. There have been some efforts to use information from a resource-rich language to develop SRL systems for resource-poor languages. Transfer methods address this problem by transferring information from a resource-rich language 13 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 13–19, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP alignments and the SLang supervised SRL system. In order to addres"
I17-2003,P13-1117,0,0.508062,"F–score of the two most frequent semantic dependencies (predicate pos + argument label): VERB+A0, VERB+A1 throughout relabeling iterations. As demonstrated in the graph, both precision and recall improve by cost-sensitive re1 Our ideal setting is to transfer to more languages but because of the semantic label inconsistency across CoNLL datasets, we find it impossible to evaluate our model on more languages. Future work should work on defining a reliable conversion scheme to unify the annotations in different datasets. 16 VERB+A0 0.6 0.5 0.55 0.45 0.5 2 4 as well as exploit partial annotation. Kozhevnikov and Titov (2013) define shared feature representations between the source and target languages in annotation projection. The benefit of using shared representations is complementary to our work encouraging us to use it in future work. Akbik et al. (2015) introduce an iterative selftraining approach using different types of linguistic heuristics and alignment filters to improve the quality of projected roles. Unlike our work that does not use any external resources, Akbik et al. (2015) make use of bilingual dictionaries. Our work also leverages self-training but with a different approach: first of all, ours do"
I17-2003,petrov-etal-2012-universal,0,0.0641501,"Missing"
I17-2003,K17-1041,0,0.216841,"labeled F-score improvement over a standard annotation projection method. 1 Introduction Semantic role labeling (SRL) is the task of automatically labeling predicates and arguments of a sentence with shallow semantic labels characterizing “Who did What to Whom, How, When and Where?” (Palmer et al., 2010). These rich semantic representations are useful in many applications such as question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011), hence gaining a lot of attention in recent years (Zhou and Xu, 2015; T¨ackstr¨om et al., 2015; Roth and Lapata, 2016; Marcheggiani et al., 2017). Since the process of creating annotated resources needs significant manual effort, SRL resources are available for a relative small number of languages such as English (Palmer et al., 2005), German (Erk et al., 2003), Arabic (Zaghouani et al., 2010) and Hindi (Vaidya et al., 2011). However, most languages still lack SRL systems. There have been some efforts to use information from a resource-rich language to develop SRL systems for resource-poor languages. Transfer methods address this problem by transferring information from a resource-rich language 13 Proceedings of the The 8th Internation"
I17-2003,P11-2052,0,0.433717,"Missing"
I17-2003,P16-1113,0,0.0518613,"ts yield a 3.5 absolute labeled F-score improvement over a standard annotation projection method. 1 Introduction Semantic role labeling (SRL) is the task of automatically labeling predicates and arguments of a sentence with shallow semantic labels characterizing “Who did What to Whom, How, When and Where?” (Palmer et al., 2010). These rich semantic representations are useful in many applications such as question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011), hence gaining a lot of attention in recent years (Zhou and Xu, 2015; T¨ackstr¨om et al., 2015; Roth and Lapata, 2016; Marcheggiani et al., 2017). Since the process of creating annotated resources needs significant manual effort, SRL resources are available for a relative small number of languages such as English (Palmer et al., 2005), German (Erk et al., 2003), Arabic (Zaghouani et al., 2010) and Hindi (Vaidya et al., 2011). However, most languages still lack SRL systems. There have been some efforts to use information from a resource-rich language to develop SRL systems for resource-poor languages. Transfer methods address this problem by transferring information from a resource-rich language 13 Proceeding"
I17-2003,D07-1002,0,0.0461135,"mploys information from source and target syntactic dependencies as well as word alignment density to improve the quality of an iterative bootstrapping method. Our experiments yield a 3.5 absolute labeled F-score improvement over a standard annotation projection method. 1 Introduction Semantic role labeling (SRL) is the task of automatically labeling predicates and arguments of a sentence with shallow semantic labels characterizing “Who did What to Whom, How, When and Where?” (Palmer et al., 2010). These rich semantic representations are useful in many applications such as question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011), hence gaining a lot of attention in recent years (Zhou and Xu, 2015; T¨ackstr¨om et al., 2015; Roth and Lapata, 2016; Marcheggiani et al., 2017). Since the process of creating annotated resources needs significant manual effort, SRL resources are available for a relative small number of languages such as English (Palmer et al., 2005), German (Erk et al., 2003), Arabic (Zaghouani et al., 2010) and Hindi (Vaidya et al., 2011). However, most languages still lack SRL systems. There have been some efforts to use information from a resource-ric"
I17-2003,Q15-1003,0,0.0270335,"Missing"
I17-2003,W11-0403,0,0.0264921,"mer et al., 2010). These rich semantic representations are useful in many applications such as question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011), hence gaining a lot of attention in recent years (Zhou and Xu, 2015; T¨ackstr¨om et al., 2015; Roth and Lapata, 2016; Marcheggiani et al., 2017). Since the process of creating annotated resources needs significant manual effort, SRL resources are available for a relative small number of languages such as English (Palmer et al., 2005), German (Erk et al., 2003), Arabic (Zaghouani et al., 2010) and Hindi (Vaidya et al., 2011). However, most languages still lack SRL systems. There have been some efforts to use information from a resource-rich language to develop SRL systems for resource-poor languages. Transfer methods address this problem by transferring information from a resource-rich language 13 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 13–19, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP alignments and the SLang supervised SRL system. In order to address these problems, we apply the following techniques to improve learning from partial and noi"
I17-2003,D17-1205,0,0.0577134,"in the dependencybased SRL task utilizing a data-dependent costsensitive training. Unlinke previous studies that use manually-defined rules to filter projections, we benefit from information obtained from projection sparsity and syntactic similarity to weigh projections. We utilize a bootstrapping algorithm to train a SRL system over projections. We showed that we can get better results if we relabel the entire train data in each iteration as opposed to only labeling instances without projections. For the future work, we consider experimenting with newly published Universal Proposition Bank (Wang et al., 2017) that provides a unified labeling scheme for all languages. Given the recent success in SRL systems with neural networks (Marcheggiani et al., 2017; Marcheggiani and Titov, 2017), we plan to use them for further improvement. We expect a similar trend by applying the same ideas in a neural SRL system. Related Work There have been several studies on transferring SRL systems (Pad´o and Lapata, 2005, 2009; Mukund et al., 2010; van der Plas et al., 2011, 2014; Kozhevnikov and Titov, 2013; Akbik et al., 2015). Pad´o and Lapata (2005), as one of the earliest studies on annotation projection for SRL u"
I17-2003,W10-1836,1,0.803474,"to Whom, How, When and Where?” (Palmer et al., 2010). These rich semantic representations are useful in many applications such as question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011), hence gaining a lot of attention in recent years (Zhou and Xu, 2015; T¨ackstr¨om et al., 2015; Roth and Lapata, 2016; Marcheggiani et al., 2017). Since the process of creating annotated resources needs significant manual effort, SRL resources are available for a relative small number of languages such as English (Palmer et al., 2005), German (Erk et al., 2003), Arabic (Zaghouani et al., 2010) and Hindi (Vaidya et al., 2011). However, most languages still lack SRL systems. There have been some efforts to use information from a resource-rich language to develop SRL systems for resource-poor languages. Transfer methods address this problem by transferring information from a resource-rich language 13 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 13–19, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP alignments and the SLang supervised SRL system. In order to address these problems, we apply the following techniques to impro"
I17-2003,P15-1109,0,0.0391237,"iterative bootstrapping method. Our experiments yield a 3.5 absolute labeled F-score improvement over a standard annotation projection method. 1 Introduction Semantic role labeling (SRL) is the task of automatically labeling predicates and arguments of a sentence with shallow semantic labels characterizing “Who did What to Whom, How, When and Where?” (Palmer et al., 2010). These rich semantic representations are useful in many applications such as question answering (Shen and Lapata, 2007) and information extraction (Christensen et al., 2011), hence gaining a lot of attention in recent years (Zhou and Xu, 2015; T¨ackstr¨om et al., 2015; Roth and Lapata, 2016; Marcheggiani et al., 2017). Since the process of creating annotated resources needs significant manual effort, SRL resources are available for a relative small number of languages such as English (Palmer et al., 2005), German (Erk et al., 2003), Arabic (Zaghouani et al., 2010) and Hindi (Vaidya et al., 2011). However, most languages still lack SRL systems. There have been some efforts to use information from a resource-rich language to develop SRL systems for resource-poor languages. Transfer methods address this problem by transferring inform"
I17-2003,C14-1121,0,\N,Missing
K15-1005,I13-1047,0,0.0302209,"ken (ex. numbers, punctuation, Latin character, emoticons, etc) The fully tagged tokens in the given sentence are then used in addition to some other features to classify the sentence as being mostly MSA or EDA. 3.1 Token Level Identification Identifying the class of a token in a given sentence requires knowledge of its surrounding tokens since 4 44 http://nlp.ldeo.columbia.edu/madamira/ words, they are being tagged as unk. This module also provides a binary flag called “isMixed”. It is “true” only if the LM decisions for the prefix, stem, and suffix are not the same. • Modality List: ModLex (Al-Sabbagh et al., 2013) is a manually compiled lexicon of Arabic modality triggers (i.e. words and phrases that convey modality). It provides the lemma with a context and the class of this lemma (MSA, EDA, or both) in that context. In our approach, we match the lemma of the input word that is provided by MADAMIRA and its surrounding context with an entry in ModLex. Then we assign this word the corresponding class from the lexicon. If we find more than one match, we use the class of the longest matched context. If there is no match, the word takes unk tag. Ex. the word “Sdq” which means ”told the truth” gets the clas"
K15-1005,W09-0807,0,0.0332599,"eatures pertaining to the sentence statistics to decide upon the class of each token in the given sentence. For the sentence level task we resort to a classifier ensemble approach that combines independent decisions made by two classifiers and use their decisions to train a new one. The proposed approaches for both tasks significantly beat the current state of the art performance with a significant margin, while creating a pipelined system. 2 Related Work Dialect Identification in Arabic has recently gained interest among Arabic NLP researchers. Early work on the topic focused on speech data. Biadsy et al. (2009) presented a system that identifies dialectal words in speech through acoustic signals. More recent work targets textual data. The main task for textual data is to decide the class of each word in a given sentence; whether it is MSA, EDA or some other class such as Named-Entity or punctuation and whether the whole sentence is mostly MSA or EDA. The first task is referred to as “Token Level Dialect Identification” while the second is “Sentence Level Dialect Identification”. For sentence level dialect identification in Arabic, the most recent works are (Zaidan and Callison-Burch, 2011), (Elfardy"
K15-1005,cotterell-callison-burch-2014-multi,0,0.4422,"t identifies dialectal words in speech through acoustic signals. More recent work targets textual data. The main task for textual data is to decide the class of each word in a given sentence; whether it is MSA, EDA or some other class such as Named-Entity or punctuation and whether the whole sentence is mostly MSA or EDA. The first task is referred to as “Token Level Dialect Identification” while the second is “Sentence Level Dialect Identification”. For sentence level dialect identification in Arabic, the most recent works are (Zaidan and Callison-Burch, 2011), (Elfardy and Diab, 2013), and (Cotterell and Callison-Burch, 2014a). Zaidan and Callison-Burch (2011) annotate MSA-DA news commentaries on Amazon Mechanical Turk and explore the use of a language-modeling based approach to perform sentence-level dialect identification. They target three Arabic dialects; Egyptian, Levantine and Gulf and develop different models to distinguish each of them against the others and against MSA. They achieve an accuracy of 2 3 43 https://code.google.com/p/cld2/ https://github.com/shuyo/ldig Input Sentence Language Model LM decision Modality List Modality decision NER NER decision CRF++ Ensemble MADAMIRA these surrounding tokens c"
K15-1005,P13-2081,1,0.921001,"(2009) presented a system that identifies dialectal words in speech through acoustic signals. More recent work targets textual data. The main task for textual data is to decide the class of each word in a given sentence; whether it is MSA, EDA or some other class such as Named-Entity or punctuation and whether the whole sentence is mostly MSA or EDA. The first task is referred to as “Token Level Dialect Identification” while the second is “Sentence Level Dialect Identification”. For sentence level dialect identification in Arabic, the most recent works are (Zaidan and Callison-Burch, 2011), (Elfardy and Diab, 2013), and (Cotterell and Callison-Burch, 2014a). Zaidan and Callison-Burch (2011) annotate MSA-DA news commentaries on Amazon Mechanical Turk and explore the use of a language-modeling based approach to perform sentence-level dialect identification. They target three Arabic dialects; Egyptian, Levantine and Gulf and develop different models to distinguish each of them against the others and against MSA. They achieve an accuracy of 2 3 43 https://code.google.com/p/cld2/ https://github.com/shuyo/ldig Input Sentence Language Model LM decision Modality List Modality decision NER NER decision CRF++ Ens"
K15-1005,P14-2125,1,0.911314,"Missing"
K15-1005,N06-2013,0,0.0430131,"combine it with other morphological information, in addition to a named entity gazetteer to decide upon the final class of each word. 3 • MADAMIRA: is a publicly available tool for morphological analysis and disambiguation of EDA and MSA text (Pasha et al., 2014).4 MADAMIRA uses SAMA (Maamouri et al., 2010) to analyze the MSA words and CALIMA (Habash et al., 2012) for the EDA words. We use MADAMIRA to tokenize both the language model and input sentences using D3 tokenization-scheme, the most detailed level of tokenization provided by the tool (ex. bAlfryq, “By the team” becomes “b+ Al+ fryq”)(Habash and Sadat, 2006). This is important in order to maximize the Language Models (LM) coverage. Furthermore, we also use MADAMIRA to tag each token in the input sentence as MSA or EDA by tagging the source of the morphological analysis, if MADAMIRA. analyses the word using SAMA, then the token is tagged MSA while if the analysis comes from CALIMA, the token is tagged EDA. Out of vocabulary words are tagged unk. • Language Model: is a D3-tokenized 5-grams language model. It is built using the 119K manually annotated words of the training data of the shared task ShTk in addition to 8M words from weblogs data (4M fr"
K15-1005,N03-1028,0,0.0925146,"as ne in the training data of ShTk in addition to the named-entities from ANERGazet (Benajiba et al., 2007) to identify the namedentities in the input sentence. This module also checks the POS provided by MADAMIRA for each input word. If a token is tagged as noun prop POS, then the token is classified as ne. Token-‐Level-‐Iden,ﬁca,on Input Surface(Level, Data MSA(LM, Surface(Level, EDA(LM, Comp-‐Cl D3#Tokenized, MSA#LM, D3#Tokenized, EDA#LM, DT Ensemble Output Abs-‐Cl Figure 2: Sentence-level identification pipeline We then use these features to train a CRF classifier using CRF++ toolkit (Sha and Pereira, 2003) and we set the window size to 16.5 Figure 1 illustrates the different components of the token-level system. 3.2 Sentence Level Identification For this level of identification, we rely on a classifier ensemble to generate the class label for each sentence. The underlying classifiers are trained on gold labeled data with sentence level binary decisions of either being MSA or EDA. Figure 2 shows the pipeline of the sentence level identification component. The pipeline consists of two main pathways with some pre-processing components. The first classifier (Comprehensive Classifier/Comp-Cl) is int"
K15-1005,W14-3907,1,0.859164,"for the given sentence. If the input sentence contains out of vocabulary Approach We introduce AIDA2. This is an improved version of our previously published tool AIDA (Elfardy et al., 2014). It tackles the problems of dialect identification in Arabic both on the token and sentence levels in mixed modern standard Arabic MSA and Egyptian dialect EDA text. We first classify each word in the input sentence to be one of the following six tags as defined in the shared task for “Language Identification in Code-Switched Data” in the first workshop on computational approaches to code-switching [ShTk](Solorio et al., 2014): • lang1: If the token is MSA (ex. AlwAqE, “The reality”) • lang2: If the token is EDA (ex. m$, “Not”) • ne: If the token is a named entity (ex. >mrykA, “America”) • ambig: If the given context is not sufficient to identify the token as MSA or EDA (ex. slAm Elykm, “Peace be upon you”) • mixed: If the token is of mixed morphology (ex. b>myT meaning “I’m always removing”) • other: If the token is or is attached to any non Arabic token (ex. numbers, punctuation, Latin character, emoticons, etc) The fully tagged tokens in the given sentence are then used in addition to some other features to clas"
K15-1005,W12-2301,0,0.0308388,"ed system AIDA (Elfardy et al., 2014) we use a weakly supervised rule based approach that relies on a language model to tag each word in the given sentence to be MSA, EDA, or unk. We then use the LM decision for each word in the given sentence/tweet and combine it with other morphological information, in addition to a named entity gazetteer to decide upon the final class of each word. 3 • MADAMIRA: is a publicly available tool for morphological analysis and disambiguation of EDA and MSA text (Pasha et al., 2014).4 MADAMIRA uses SAMA (Maamouri et al., 2010) to analyze the MSA words and CALIMA (Habash et al., 2012) for the EDA words. We use MADAMIRA to tokenize both the language model and input sentences using D3 tokenization-scheme, the most detailed level of tokenization provided by the tool (ex. bAlfryq, “By the team” becomes “b+ Al+ fryq”)(Habash and Sadat, 2006). This is important in order to maximize the Language Models (LM) coverage. Furthermore, we also use MADAMIRA to tag each token in the input sentence as MSA or EDA by tagging the source of the morphological analysis, if MADAMIRA. analyses the word using SAMA, then the token is tagged MSA while if the analysis comes from CALIMA, the token is"
K15-1005,P11-2007,0,0.290754,"cused on speech data. Biadsy et al. (2009) presented a system that identifies dialectal words in speech through acoustic signals. More recent work targets textual data. The main task for textual data is to decide the class of each word in a given sentence; whether it is MSA, EDA or some other class such as Named-Entity or punctuation and whether the whole sentence is mostly MSA or EDA. The first task is referred to as “Token Level Dialect Identification” while the second is “Sentence Level Dialect Identification”. For sentence level dialect identification in Arabic, the most recent works are (Zaidan and Callison-Burch, 2011), (Elfardy and Diab, 2013), and (Cotterell and Callison-Burch, 2014a). Zaidan and Callison-Burch (2011) annotate MSA-DA news commentaries on Amazon Mechanical Turk and explore the use of a language-modeling based approach to perform sentence-level dialect identification. They target three Arabic dialects; Egyptian, Levantine and Gulf and develop different models to distinguish each of them against the others and against MSA. They achieve an accuracy of 2 3 43 https://code.google.com/p/cld2/ https://github.com/shuyo/ldig Input Sentence Language Model LM decision Modality List Modality decision"
K15-1005,W14-3912,0,0.123775,"Missing"
K15-1005,pasha-etal-2014-madamira,1,0.90527,"Missing"
L16-1567,E14-4027,0,0.0360821,"Missing"
L16-1567,P98-1015,0,0.205197,"Missing"
L16-1567,P08-2053,0,0.0720439,"Missing"
L16-1567,P07-1072,0,0.0340964,"ounds, Saxon genitive, Norman genitive, nouns modified by adjectives derived from nouns, and nouns qualified by prepositions), but eventually found that only 26 of these relations have actual representation in their annotated corpus which are (in order of frequency): part-whole, attribute-holder, possession, theme, measure, agent, temporal, location/space, kinship, source, topic, recipient, purpose, depiction-depicted, is-a (hypernymy), make/produce, associated with, result, instrument, cause, 3574 manner, experiencer, means, influence, frequency, and predicate. Later, Girju et al. (2005) and Girju (2007) developed slightly modified versions of this scheme. Tratz and Hovy (2010) developed their own fine-grained taxonomy of 43 semantic types created mainly by breaking down main semantic categories into subcategories. For example the category of topic is divided into 7 other subcategories including topic of communication, e.g. “travel story”; topic of rules, e.g. “loan terms”; topic of emotion, e.g. “jazz fan”. They mapped their taxonomy to six previous taxonomies and concluded that the relations are ‘fairly similar’. They used five notations for mapping: ≈-approximately equivalent; ⊃/⊂-super/su"
L16-1567,C10-1045,0,0.060889,"Missing"
L16-1567,D07-1116,0,0.223958,"Arabic, construct state, Idafa, annotation, Treebank, syntax, semantics 1. Introduction 1.1. Idafa in Arabic Idafa is an Arabic term that means “annexation” or “addition”. In Arabic grammar, Idafa is a construction that is made up of two nominal parts (nouns, adjectives, proper nouns), where the whole construction serves as a single syntactic unit. The first part, in Arabic “mudaf” (MDF), is an indefinite noun and the second one, “mudaaf elayh” (MDFE), could either be definite or indefinite. Typically, in Idafa constructions (IC) the MDFE defines or specifies the MDF (Boujelben et al., 2011). Habash et al. (2007) highlight the role IC plays in syntactic case realization for nominals in Arabic. Syntactic case marking depends on whether the nominal is indefinite, namely marked with the nunation/tanween diacritic typically expressed as a word final an, un, in, or if the nominal is definite through agglutinating the definite article Al+ as a prefix, or is definite through IC. The case on MDF is sometimes referred to as the construct state. In principle, IC could be recursive with no specific bounds on the number of embeddings or nestings. IC in Arabic is practically a wide construction that covers many li"
L16-1567,P09-2056,0,0.123004,"fferent types of ICs. Accordingly, no distinction is made between noun-noun and quantifier-noun constructions. The phrases ِِﻛﺘﺎﺏب ُ ﺍاﻟﻨﱠﺤْ ﻮ kitAbu Al-naHowi 1 “grammar book” and ﻮﺍاﺿﻴﯿﻊ  ُﻛﻞﱡ ﺍاﻟ َﻤkul~u ِ Al-mawADiyEi “all topics” are rendered with the same syntactic realization, i.e. both are simply labeled (NP (NN NP)) where the words for book and all are labeled the same way as NN. The only attempt we know of to explicitly label ICs on a large scale was in the context of the CATiB dependency annotation effort in which they use the IDF tag, as one of 8 grammatical relation labels (Habash and Roth, 2009). Accordingly, in this paper, we present a typification of the various coarse syntactic constructions of (NP (NN NP)) and (NP (JJ NP)) present in the ATB, corresponding to the syntactic IC linguistic phenomenon, into their various IC types based on the true POS categories of their heads and complements. We present an automatic extraction and classification process which yields three main types that are further divided into 10 syntactic IC subtypes. 1 All the transliteration in this paper is presented using the Buckwalter encoding system www.qamus.com Furthermore, we present our semantic framew"
L16-1567,maamouri-etal-2008-enhancing,0,0.0833316,"Missing"
L16-1567,W04-2609,0,0.0299691,"g precise distinctions between types. This overview of the literature on the semantic classification of compound nouns shows the divergence between theoretical and empirical studies. Theoretical studies tend to use coarse-grained types, as they are inclined more towards generalization, while empirical studies tend to use medium- to fine-grained sets of labels to be more descriptive and more exhaustive of the data they target to annotate. 5.2. Our Annotation Scheme In our work on the semantic annotation of compound nouns (specifically Arabic Idafa), we adopt the 26 taxonomy types designated by Moldovan et al (2004). The reasons we adopt this annotation scheme are: 1) its medium granularity which makes a good compromise between generalizability and applicability; 2) it was designed for English noun phrases which are closer to Arabic IC than the more restricted set of noun-noun compounds; and 3) they use purely semantic notation, i.e. without relying on grammatical functions or lexical interpretation. Lauer’s list of 8 prepositional paraphrases for noun compound has gained popularity in the literature the field since its inception. We argue against the use of prepositions for the purpose of semantic class"
L16-1567,P10-1070,0,0.015536,"es derived from nouns, and nouns qualified by prepositions), but eventually found that only 26 of these relations have actual representation in their annotated corpus which are (in order of frequency): part-whole, attribute-holder, possession, theme, measure, agent, temporal, location/space, kinship, source, topic, recipient, purpose, depiction-depicted, is-a (hypernymy), make/produce, associated with, result, instrument, cause, 3574 manner, experiencer, means, influence, frequency, and predicate. Later, Girju et al. (2005) and Girju (2007) developed slightly modified versions of this scheme. Tratz and Hovy (2010) developed their own fine-grained taxonomy of 43 semantic types created mainly by breaking down main semantic categories into subcategories. For example the category of topic is divided into 7 other subcategories including topic of communication, e.g. “travel story”; topic of rules, e.g. “loan terms”; topic of emotion, e.g. “jazz fan”. They mapped their taxonomy to six previous taxonomies and concluded that the relations are ‘fairly similar’. They used five notations for mapping: ≈-approximately equivalent; ⊃/⊂-super/sub set; ∞-some overlap; ∪-union. The problem with this scheme is the obvious"
L16-1567,C94-2125,0,0.0269024,"Missing"
L16-1577,D15-1274,0,0.145302,"entence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter transliteration encoding system to represent Arabic in Romanized script (Buckwalter, 2002) Undiacritized Diacritized Buckwalter Y«ð Y « ð /waEad/ Y«ð Y«ð Y«ð Y«ð  ð Y« ð Y« Y « ð Y « ð English he p"
L16-1577,W15-3209,1,0.859315,"th the diacritics for a variety of Arabic texts covering more than 10 genres. The target size of the annotated corpus is 2 million words. The present work was mainly motivated by the lack of equivalent multi-genres large scale annotated corpus. The creation of manually annotated corpus usually presents many challenges and issues. In order to address those challenges, we created comprehensive and simplified annotation guidelines that were used by a team consisting of five annotators and an annotation manager. The guidelines were defined after an initial pilot annotation experiment described in Bouamor et al. (2015). In order to ensure a high annotation agreement between the annotators, multiple training sessions were held and a regular inter-annotator agreement (IAA) measures were performed to check annotation quality. To the best of our knowledge, this is the first Arabic diacritized multi-genres corpus. The remainder of this paper is organized as follows. We review related work in section 2. Afterwards, we discuss the challenges posed by the complexity of the Arabic diacritization process in section 3. Then, we describe our corpus and the development of the guidelines in sections 4 and 5. 3637 We pres"
L16-1577,2007.mtsummit-papers.20,1,0.926265,"nced, except the last letter diacritization, and b) case and mood diacritization, which exists above or below the last letter in each word, indicating its grammatical function in the sentence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter translite"
L16-1577,palmer-etal-2008-pilot,1,0.881222,"utomatic diacritization system for Arabic using rule-based, statistical and hybrid methods. We refer to the recent literature review in Abandah et al. (2015) for a general overview of these methods and tools. The most relevant resource to our work is the Penn Arabic Treebank (PATB), a large corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another re"
L16-1577,dukes-habash-2010-morphological,0,0.0333421,"corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to fin"
L16-1577,maamouri-etal-2008-enhancing,0,0.0233017,"about the text, the author and the source. In order to use the CCA corpus, a normalization effort was done to produce a consistent XML mark-up format to be used by our annotation tool. 5. Development of the Guidelines We provided the annotators with detailed guidelines, describing our diacritization scheme and specifying when and where to add the diacritics. We describe the annotation procedure and explained how to deal with borderline cases. We also include several annotated examples to illustrate the specified rules. Our guidelines are mostly inspired from the LDC POS annotation guidelines (Maamouri et al., 2008). Since, the LDC guidelines are mainly designed for the POS annotation and not specifically for the diacritization per se, we created a simplified version and added some specific diacritization rules to make the annotation process consistent. Below we provide some examples of diacritization exceptions and specific rules. The Shadda: The shadda mark should be added in all cases specified in the guidelines except the following in the definite artilce, where it should not be added to the letter ÊË@ /Allymwn/ ’lemon’ È /l/ of the definite article (e.g. àñÒJ  ÊË@ /A˜llymwn/). Moreover, the shadda"
L16-1577,maamouri-etal-2010-speech,1,0.874215,"and present our future work in section 8. 2. Related Work Since, our paper is mainly about the creation and evaluation of a large annotated corpus, we will focus mostly on this aspect in the previous works. There have been numerous approaches to build an automatic diacritization system for Arabic using rule-based, statistical and hybrid methods. We refer to the recent literature review in Abandah et al. (2015) for a general overview of these methods and tools. The most relevant resource to our work is the Penn Arabic Treebank (PATB), a large corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacrit"
L16-1577,W14-3605,1,0.918057,"Missing"
L16-1577,pasha-etal-2014-madamira,1,0.898534,"Missing"
L16-1577,W15-3204,1,0.903933,"Missing"
L16-1577,D15-1152,0,0.0742449,"indicating its grammatical function in the sentence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter transliteration encoding system to represent Arabic in Romanized script (Buckwalter, 2002) Undiacritized Diacritized Buckwalter Y«ð Y « ð /waEad/ Y«ð Y"
L16-1577,2006.amta-papers.25,0,0.0982236,"Missing"
L16-1577,W10-1836,1,0.806692,"tion system for Arabic using rule-based, statistical and hybrid methods. We refer to the recent literature review in Abandah et al. (2015) for a general overview of these methods and tools. The most relevant resource to our work is the Penn Arabic Treebank (PATB), a large corpus annotated by the Linguistic Data Consortium (Maamouri et al., 2010). Most of the LDC Treebank corpora are also manually diacritized, but they cover mainly news and weblog text genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to"
L16-1577,W12-2511,1,0.785024,"genres. The PATB served later to build the first Arabic Proposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and"
L16-1577,zaghouani-etal-2014-large,1,0.532947,"roposition Bank (APB) using the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w"
L16-1577,W15-1614,1,0.736149,"ing the fully specified diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w as well as consonants."
L16-1577,L16-1295,1,0.768521,"diacritized lemmas (Diab et al., 2008; Zaghouani et al., 2010). The Tashkeela classical Arabic vocalized corpus (Zerrouki, 2011) is another notable dataset covering six million words. Tashkeela was compiled from various web sources covering Islamic religious heritage (mainly classical Arabic books). Moreover, Dukes and Habash (2010), created the Quranic Arabic Corpus, a fully diacritized annotated linguistic resource which we used later on to build the first Quranic Arabic Proposition Bank Zaghouani et al. (2012). The Qatar Arabic Language Bank (Zaghouani et al., 2014; Zaghouani et al., 2015; Zaghouani et al., 2016) is another relevant work that aims to build a large corpus of manually corrected Arabic text for building automatic correction tools for three Arabic text genres: native, non-native and machine translation post-edited text. Recently, in Bouamor et al. (2015), we conducted various annotation experiments to find the most suitable and efficient annotation procedure in creating a large scale diacritized corpus. 3. Arabic Diacritics Arabic script consists of two classes of symbols: letters and diacritics. Letters comprise long vowels such as A, y, w as well as consonants. Diacritics on the other h"
L16-1577,P06-1073,0,0.603177,"t letter in each word, indicating its grammatical function in the sentence. There are three types of diacritics: vowel, nunation, and shadda (gemination). The lack of diacritics leads usually to considerable lexical and morphological ambiguity as shown in the example in Table 1.1 Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to newswire stories (those distributed by the LDC), religious texts such as the Holy Quran, or educational texts. 1 We use the Buckwalter transliteration encoding system to represent Arabic in Romanized script (Buckwalter, 2002) Undiacritized Diacritized Buckwalter"
L16-1640,K15-1005,1,0.473735,"and SeparatePunc commands. TagSounds detects sound patterns in an input String (e.g., hmmm, ahh, etc.). 5. Case Study As we are a specialized group in Arabic processing with many publicly available projects, the need for having a consistent preprocessing behavior across our projects is a must. Though, it is not necessary to have the same preprocessing pipeline for all projects, having the same implementation of the shared steps is crucial. Figures 5, 6, and 7 show the preprocessing pipelines of our publicly available tools for Arabic; AIDA for Arabic dialect identification and classification (Al-Badrashiny et al., 2015), MADAMIRA for morphological analysis and disambiguation of Egyptian and modern standard Arabic text (Pasha et al., 2014), and 3ARRIB, for converting dialectal Arabic written in Latin characters in social media to normalized Arabic orthography (Al-Badrashiny et al., 2014) and (Eskander et al., 2014). SPLIT has maintained the same system performance for these tools, but it significantly simplified the code by separating the text preprocessing part from the core engines. This enabled us to simply try different preprocessing schemes in a streamlined manner expediting the turn around for the exper"
L16-1640,althobaiti-etal-2014-aranlp,0,0.0202847,"nts to segment text into paragraphs, sentences, words and other kinds of tokens. GetItFull is a tool for downloading and preprocessing full-text journals. It performs various commonly used preprocessing steps and puts the output in a structured XML document for each article with tags identifying the various sections and journal information articles (Natarajan et al., 2006). AraNLP is a preprocessing tool developed specifically for preprocessing Arabic texts. It includes a sentence detector, tokenizer, light stemmer, root stemmer, part-of-speech tagger, and a punctuation and diacritic remover (Althobaiti et al., 2014). In this paper, we introduce SPLIT, the Smart Preprocessing (Quasi) Language Independent Tool. By language independence we mean that the tool is built in allows it to be as flexible as possible and not be restricted to a certain language. The tool consists of a list of commands. Where each of them performs only one task. The users can then build their own preprocessing pipeline using a simple configuration file. This list of commands can easily be expanded just by adding a file that contains the code of the new preprocessing task to the source directory of the tool. SPLIT is then able to use"
L16-1640,W14-3901,1,0.84161,"is not necessary to have the same preprocessing pipeline for all projects, having the same implementation of the shared steps is crucial. Figures 5, 6, and 7 show the preprocessing pipelines of our publicly available tools for Arabic; AIDA for Arabic dialect identification and classification (Al-Badrashiny et al., 2015), MADAMIRA for morphological analysis and disambiguation of Egyptian and modern standard Arabic text (Pasha et al., 2014), and 3ARRIB, for converting dialectal Arabic written in Latin characters in social media to normalized Arabic orthography (Al-Badrashiny et al., 2014) and (Eskander et al., 2014). SPLIT has maintained the same system performance for these tools, but it significantly simplified the code by separating the text preprocessing part from the core engines. This enabled us to simply try different preprocessing schemes in a streamlined manner expediting the turn around for the experimental investigations. 4058 Figure 7: 3ARRIB preprocessing pipeline Figure 5: AIDA preprocessing pipeline Figure 6: MADAMIRA preprocessing pipeline 6. Conclusions In this paper, we introduced version 1.01 of SPLIT. The tool provides the most preprocessing tasks that are needed to clean and prepare"
L16-1640,grover-etal-2000-lt,0,0.0782072,"tivate the need for a simple standard preprocessing framework that has a unified implementation of the most important preprocessing steps taking into consideration the different behaviors of various languages and genres. This enables the researcher to focus more on the research point. Some attempts toward this objective have been introduced. PRETO is a preprocessing tool developed specifically for preprocessing Turkish texts only (Tunali and Bilgin, 2012). JPreText is another tool that focuses on stemming, stopword removal, and term weighting (TF/IDF) for English text (Nogueira et al., 2008). Grover et al. (2000) introduced a tool called “A Flexible Tokenisation Tool” that includes ready-made components to segment text into paragraphs, sentences, words and other kinds of tokens. GetItFull is a tool for downloading and preprocessing full-text journals. It performs various commonly used preprocessing steps and puts the output in a structured XML document for each article with tags identifying the various sections and journal information articles (Natarajan et al., 2006). AraNLP is a preprocessing tool developed specifically for preprocessing Arabic texts. It includes a sentence detector, tokenizer, ligh"
L16-1640,pasha-etal-2014-madamira,1,0.830646,"specialized group in Arabic processing with many publicly available projects, the need for having a consistent preprocessing behavior across our projects is a must. Though, it is not necessary to have the same preprocessing pipeline for all projects, having the same implementation of the shared steps is crucial. Figures 5, 6, and 7 show the preprocessing pipelines of our publicly available tools for Arabic; AIDA for Arabic dialect identification and classification (Al-Badrashiny et al., 2015), MADAMIRA for morphological analysis and disambiguation of Egyptian and modern standard Arabic text (Pasha et al., 2014), and 3ARRIB, for converting dialectal Arabic written in Latin characters in social media to normalized Arabic orthography (Al-Badrashiny et al., 2014) and (Eskander et al., 2014). SPLIT has maintained the same system performance for these tools, but it significantly simplified the code by separating the text preprocessing part from the core engines. This enabled us to simply try different preprocessing schemes in a streamlined manner expediting the turn around for the experimental investigations. 4058 Figure 7: 3ARRIB preprocessing pipeline Figure 5: AIDA preprocessing pipeline Figure 6: MADA"
L16-1669,K15-1005,1,0.56704,"mes mainly from discussion forums. AOC is reader commentaries that were crawled from an Egyptian Newspaper called “Al-Youm Al-Sabe”. TWT data is crawled from some Egyptian public figures’ Twitter accounts. 5.3. Data Preprocessing A preprocessing pipeline is developed to prepare data for annotation. First, raw text data is extracted from sources and different cleaning steps (such as handling non-standard characters) are carried out using the Smart Preprocessing (Quasi) Language Independent tool (SPLIT) (Al-Badrashiny et al. 2016). Then Automatic Identification of Dialectal Arabic (AIDA2) tool (Al-Badrashiny et al. 2015) is used to assign initial automatic tagging for highly confident data categories (label types 9 through 15 in table 2) in addition to named entities (label type 7). Finally, the preprocessing pipeline puts the data in the format acceptable by the annotation application. 5.4.1 Types of Users Three types of users have been considered in the design of the tool: Super-user, Lead Annotator, and Annotator. Each type of users is provided with different kinds of privileges and functionalities in order to fulfill their tasks. Super-user: There is only one super-user account for all dialects/languages."
L16-1669,L16-1640,1,0.809087,"Missing"
L16-1669,dey-fung-2014-hindi,0,0.0601476,"Missing"
L16-1669,E06-1047,1,0.812023,"Missing"
L16-1669,elfardy-diab-2012-simplified,1,0.85978,"Missing"
L16-1669,li-etal-2012-mandarin,0,0.0229301,"Missing"
L16-1669,W15-1608,0,0.0360648,"Missing"
L16-1669,W14-3907,1,0.848153,"ies, including parsing, machine translation (MT), automatic speech recognition (ASR), semantic processing, and information retrieval (IR) and extraction (IE). Techniques trained for one language quickly break down when there is input from another. Being able to predict/identify probable switch points, as well as which dialect/language a speaker is switching to, enables applications to adapt their models. A major barrier to research on CS has been the lack of large, consistently and accurately annotated corpora of CS data. In the shared task for “Language Identification in Code-Switched Data” (Solorio et al., 2014), the first set of annotated data was created which focused on social media and covered four language pairs: Modern Standard Arabic - Dialectal Arabic (MSA-DA), Mandarin - English (MAN-EN), Nepali - English (NEP-EN), and Spanish - English (SPA-EN). In this work we present our effort to build a large repository of CS data that will cover multiple language pairs and dialects. We started by focusing on Arabic Language. Arabic is a Semitic language spoken by over 300M people worldwide. CS between MSA and DA is widespread among native speakers of Arabic. MSA is the language of education used in for"
L16-1669,D08-1110,0,0.180703,"Missing"
L16-1669,P11-2007,0,0.0699664,"Missing"
L18-1173,K15-1005,1,0.823516,"Missing"
L18-1173,L16-1640,1,0.832577,"per, we present our effort in building an annotation system, WASA, that can manage and facilitate large-scale CS data annotation. WASA differs from other annotation systems in several respects. Our system has an option that can provide initial automatic tagging for specific tokens such as Latin words, URL, punctuation, digits, diacritics, emoticons, and speech effect tokens. This option increases the quality and the speed of annotation substantially. Moreover, the system is integrated with language-specific date preprocessing tool Smart Preprocessing (Quasi) Language Independent Tool (SPLIT) (Al-Badrashiny et al., 2016) to streamline raw data cleaning and preparation. The remainder of this paper is organized as follows: Section 2 provides an overview of related work. Section 3 describes the System Architecture. Types of users including permissions and users tasks are introduced in Section 4. The data preprocessing and cleaning are discussed in Section 5. We provide an overview of the database design in Section 6. Inter-annotator agreement, current status, and our conclusion and future work are discussed in sections 2. Related Works Although, many annotation tools, such as (Aziz et al., 2012), (Cunningham et"
L18-1173,aziz-etal-2012-pet,0,0.0155897,"SPLIT) (Al-Badrashiny et al., 2016) to streamline raw data cleaning and preparation. The remainder of this paper is organized as follows: Section 2 provides an overview of related work. Section 3 describes the System Architecture. Types of users including permissions and users tasks are introduced in Section 4. The data preprocessing and cleaning are discussed in Section 5. We provide an overview of the database design in Section 6. Inter-annotator agreement, current status, and our conclusion and future work are discussed in sections 2. Related Works Although, many annotation tools, such as (Aziz et al., 2012), (Cunningham et al., 2009), (Kahan et al., 2002), MnM (Vargas-Vera et al., 2002), GATE ((Cunningham et al., 2009); (Aswani and Gaizauskas, 2009), and (Dickinson and Ledbetter, 2012)), are effective in serving their intended purposes, none of them meets the CS annotation requirements perfectly. We need a tool that can help in sequence annotating in a way that can report the time needed for annotators to get their tasks done, manage number of annotator teams, enable quality control measures and annotation statistics, and assign some initial tags to some tags automatically (e.g. punctuation, URL"
L18-1173,L16-1669,1,0.833398,"Missing"
L18-1173,dickinson-ledbetter-2012-annotating,0,0.0319319,"f related work. Section 3 describes the System Architecture. Types of users including permissions and users tasks are introduced in Section 4. The data preprocessing and cleaning are discussed in Section 5. We provide an overview of the database design in Section 6. Inter-annotator agreement, current status, and our conclusion and future work are discussed in sections 2. Related Works Although, many annotation tools, such as (Aziz et al., 2012), (Cunningham et al., 2009), (Kahan et al., 2002), MnM (Vargas-Vera et al., 2002), GATE ((Cunningham et al., 2009); (Aswani and Gaizauskas, 2009), and (Dickinson and Ledbetter, 2012)), are effective in serving their intended purposes, none of them meets the CS annotation requirements perfectly. We need a tool that can help in sequence annotating in a way that can report the time needed for annotators to get their tasks done, manage number of annotator teams, enable quality control measures and annotation statistics, and assign some initial tags to some tags automatically (e.g. punctuation, URL, emoticon, etc.) Our tool is most similar to the annotation tool for the COLABA project (Diab et al., 2010); (Benajiba and Diab, 2010)(Benajiba and Diab, 2010; Diab et al., 2010). W"
L18-1173,W16-5808,0,0.0837178,"ponent in the COLABA annotation tool. Although, the code switching annotation task and manual diacritization of Standard Arabic text task are completely different tasks, the MANDIAC tool (Obeid et al., 2016), which used for diacritization annotation task, has a similar annotator management component to the WASA management component. However, the technologies used in both management components are different. For instance, WASA uses PostgreSql database to store content, while MANDIAC uses a JSON blob to store content. Two other comparable tools to ours are WebANNO (Yimam et al., 2013) and SWAT (Samih et al., 2016). They both use the latest available technologies to perform a number of linguistic annotation types. The SWAT tool is a web-based interface for annotating tokens in a sequence with a predefined set of labels. The main advantages of this tool are the simplicity of its use and instal1073 lation as it only requires a modern web browser and minimum server-side requirements to get the tool work. The WebANNO tool is also a web-based tool that offers wide range of linguistic annotations tasks, e.g., named entity, dependency parsing, co-reference chain identification, and part-of speech annotation. H"
L18-1173,P13-4001,0,0.0604308,"Missing"
L18-1199,P17-1067,0,0.0641525,"hrase-level. Strapparava and Rada (2007), collected HLN set and labeled it using EK6 tags and valence, which, valence measures the polarity of each data point. HLN is used in SemEval 2007, task 14. Pang and Lee (2005) crawled web to collect MOV dataset to address rating inference problem. Mishne (2005) collected a set of blog posts - online diary entries - which include an indication of the writers’s mood. Yan (2014) expanded the range of automatic emotion detection in microblogging text using three sampling strategies: random sampling, topics and events sampling, and sampling based on users. Abdul-Mageed and Lyle (2017) collected a large set of tweets using hashtags, they used Plutchik’s Wheel of Emotions to create relevant hashtags, and the set is annotated using distant supervision method. To date, sentence-level emotion classification has been studied by a large group of researchers (Aman and Szpakowicz, 2007; Strapparava and Rada, 2007; Mishne, 2005; Yan, 2014; Das and Bandyopadhyay, 2010; Ghazi ¨ et al., 2010; Kim et al., 2010; Mohammad, 2012; Ozbal and Pighin, 2013; Abdul-Mageed and Lyle, 2017), who ad1246 dressed the EDC task on the document and sentence levels, to our knowledge, nobody investigated a"
L18-1199,Y10-1013,0,0.033399,"ation of the writers’s mood. Yan (2014) expanded the range of automatic emotion detection in microblogging text using three sampling strategies: random sampling, topics and events sampling, and sampling based on users. Abdul-Mageed and Lyle (2017) collected a large set of tweets using hashtags, they used Plutchik’s Wheel of Emotions to create relevant hashtags, and the set is annotated using distant supervision method. To date, sentence-level emotion classification has been studied by a large group of researchers (Aman and Szpakowicz, 2007; Strapparava and Rada, 2007; Mishne, 2005; Yan, 2014; Das and Bandyopadhyay, 2010; Ghazi ¨ et al., 2010; Kim et al., 2010; Mohammad, 2012; Ozbal and Pighin, 2013; Abdul-Mageed and Lyle, 2017), who ad1246 dressed the EDC task on the document and sentence levels, to our knowledge, nobody investigated automatic tagging on the clause level and the impact of clause-level on sentence-level emotion classification, and that distinguish our work from previous works. 3. Data Description We aim to create a multigenre corpus annotated with emotion tags on the clause and sentence level. We would like to cater to fine grained emotion detection with the goal of eventually building system"
L18-1199,W10-0217,0,0.518053,"Missing"
L18-1199,W10-0208,0,0.115698,"e range of automatic emotion detection in microblogging text using three sampling strategies: random sampling, topics and events sampling, and sampling based on users. Abdul-Mageed and Lyle (2017) collected a large set of tweets using hashtags, they used Plutchik’s Wheel of Emotions to create relevant hashtags, and the set is annotated using distant supervision method. To date, sentence-level emotion classification has been studied by a large group of researchers (Aman and Szpakowicz, 2007; Strapparava and Rada, 2007; Mishne, 2005; Yan, 2014; Das and Bandyopadhyay, 2010; Ghazi ¨ et al., 2010; Kim et al., 2010; Mohammad, 2012; Ozbal and Pighin, 2013; Abdul-Mageed and Lyle, 2017), who ad1246 dressed the EDC task on the document and sentence levels, to our knowledge, nobody investigated automatic tagging on the clause level and the impact of clause-level on sentence-level emotion classification, and that distinguish our work from previous works. 3. Data Description We aim to create a multigenre corpus annotated with emotion tags on the clause and sentence level. We would like to cater to fine grained emotion detection with the goal of eventually building systems that detect emotion intensity. Toward"
L18-1199,P14-5010,0,0.0045255,"Missing"
L18-1199,J93-2004,0,0.0631949,"Missing"
L18-1199,N12-1071,0,0.114907,"ic emotion detection in microblogging text using three sampling strategies: random sampling, topics and events sampling, and sampling based on users. Abdul-Mageed and Lyle (2017) collected a large set of tweets using hashtags, they used Plutchik’s Wheel of Emotions to create relevant hashtags, and the set is annotated using distant supervision method. To date, sentence-level emotion classification has been studied by a large group of researchers (Aman and Szpakowicz, 2007; Strapparava and Rada, 2007; Mishne, 2005; Yan, 2014; Das and Bandyopadhyay, 2010; Ghazi ¨ et al., 2010; Kim et al., 2010; Mohammad, 2012; Ozbal and Pighin, 2013; Abdul-Mageed and Lyle, 2017), who ad1246 dressed the EDC task on the document and sentence levels, to our knowledge, nobody investigated automatic tagging on the clause level and the impact of clause-level on sentence-level emotion classification, and that distinguish our work from previous works. 3. Data Description We aim to create a multigenre corpus annotated with emotion tags on the clause and sentence level. We would like to cater to fine grained emotion detection with the goal of eventually building systems that detect emotion intensity. Toward that goal, we cr"
L18-1199,E14-3005,0,0.164837,"investigated the problem in various data genres. We present studies most relevant to this paper. Aman and Szpakowicz (2007) collected and labeled BLG corpus using EK6 tags in sentence and phrase-level. Strapparava and Rada (2007), collected HLN set and labeled it using EK6 tags and valence, which, valence measures the polarity of each data point. HLN is used in SemEval 2007, task 14. Pang and Lee (2005) crawled web to collect MOV dataset to address rating inference problem. Mishne (2005) collected a set of blog posts - online diary entries - which include an indication of the writers’s mood. Yan (2014) expanded the range of automatic emotion detection in microblogging text using three sampling strategies: random sampling, topics and events sampling, and sampling based on users. Abdul-Mageed and Lyle (2017) collected a large set of tweets using hashtags, they used Plutchik’s Wheel of Emotions to create relevant hashtags, and the set is annotated using distant supervision method. To date, sentence-level emotion classification has been studied by a large group of researchers (Aman and Szpakowicz, 2007; Strapparava and Rada, 2007; Mishne, 2005; Yan, 2014; Das and Bandyopadhyay, 2010; Ghazi ¨ et"
maamouri-etal-2006-developing,E06-1047,1,\N,Missing
maamouri-etal-2006-developing,W04-1602,1,\N,Missing
maamouri-etal-2006-developing,N04-4038,1,\N,Missing
maamouri-etal-2006-developing,P05-1071,1,\N,Missing
N04-4038,N03-2009,1,0.224384,"ied to the problem of POS tagging and BP Chunking. Such problems are cast as a classification problem where, given a number of features extracted from a predefined linguistic context, the task is to predict the class of a token. Support Vector Machines (SVMs) (Vapnik, 1995) are one class of such model. SVMs are a supervised learning algorithm that has the advantage of being robust where it can handle a large number of (overlapping) features with good generalization performance. Consequently, SVMs have been applied in many NLP tasks with great success (Joachims, 1998; Kudo and Matsumato, 2000; Hacioglu and Ward, 2003). We adopt a tagging perspective for the three tasks. Thereby, we address them using the same SVM experimental setup which comprises a standard SVM as a multiclass classifier (Allwein et al., 2000). The difference for the three tasks lies in the input, context and features. None of the features utilized in our approach is explicitly language dependent. The following subsections illustrate the different tasks and their corresponding features and tag sets. 4.1 Word Tokenization We approach word tokenization (segmenting off clitics) as a one-of-six classification task, in which each letter in a w"
N04-4038,W00-0730,0,0.043484,"arning approaches are applied to the problem of POS tagging and BP Chunking. Such problems are cast as a classification problem where, given a number of features extracted from a predefined linguistic context, the task is to predict the class of a token. Support Vector Machines (SVMs) (Vapnik, 1995) are one class of such model. SVMs are a supervised learning algorithm that has the advantage of being robust where it can handle a large number of (overlapping) features with good generalization performance. Consequently, SVMs have been applied in many NLP tasks with great success (Joachims, 1998; Kudo and Matsumato, 2000; Hacioglu and Ward, 2003). We adopt a tagging perspective for the three tasks. Thereby, we address them using the same SVM experimental setup which comprises a standard SVM as a multiclass classifier (Allwein et al., 2000). The difference for the three tasks lies in the input, context and features. None of the features utilized in our approach is explicitly language dependent. The following subsections illustrate the different tasks and their corresponding features and tag sets. 4.1 Word Tokenization We approach word tokenization (segmenting off clitics) as a one-of-six classification task, i"
N04-4038,W00-0726,0,0.165552,"Missing"
N06-2039,J01-3003,0,0.17889,"Missing"
N06-2039,C00-2108,0,0.0865361,"Missing"
N10-1029,D09-1050,0,0.0421377,"meaning and produce appropriate translations and avoid the generation of unnatural or nonsensical sentences in the target language.” However, statistical machine translation (SMT) typically does not model MWEs explicitly. SMT 1 The research was partially funded by IBM under the DARPA GALE project. As a result, the usefulness of explicitly modeling MWEs in the SMT framework has not yet been studied systematically. Previous work has focused on automatically learning and integrating translations of very specific MWE categories, such as, for instance, idiomatic Chinese four character expressions (Bai et al., 2009) or domain specific MWEs (Ren et al., 2009). MWEs have also been defined not from a lexical semantics perspective but from a SMT error reduction perspective, as phrases that are hard to align during SMT training (Lambert and Banchs, 2005). For each of these particular cases, translation quality improved by augmenting the SMT translation lexicon with the learned bilingual MWEs either directly or through improved word alignments. In this paper, we consider a more general problem: we view SMT as an extrinsic evaluation of the usefulness of monolingual MWEs as used pervasively in natural language"
N10-1029,2005.mtsummit-posters.11,0,0.483322,"e research was partially funded by IBM under the DARPA GALE project. As a result, the usefulness of explicitly modeling MWEs in the SMT framework has not yet been studied systematically. Previous work has focused on automatically learning and integrating translations of very specific MWE categories, such as, for instance, idiomatic Chinese four character expressions (Bai et al., 2009) or domain specific MWEs (Ren et al., 2009). MWEs have also been defined not from a lexical semantics perspective but from a SMT error reduction perspective, as phrases that are hard to align during SMT training (Lambert and Banchs, 2005). For each of these particular cases, translation quality improved by augmenting the SMT translation lexicon with the learned bilingual MWEs either directly or through improved word alignments. In this paper, we consider a more general problem: we view SMT as an extrinsic evaluation of the usefulness of monolingual MWEs as used pervasively in natural language regardless of domain, idiomaticity and compositionality. A MWE is compositional if its meaning as a unit can be predicted from the meaning of its component words such as in make a decision meaning to decide. Some MWEs are more predictable"
N10-1029,P02-1040,0,0.110059,"rge amounts of data available. However, we tackle the less common English to Arabic direction in order to take advantage of the rich lexical resources available for English on the input side. Our test set consists of the 813 newswire sentences of the 2008 NIST Open Machine Translation Evaluation, which is standard evaluation data for Arabic-English translation. The first English reference translation is used as the input to our SMT system, and the single Arabic translation is used as the unique reference2 . Translation quality is evaluated using two automatic evaluation metrics: (1) BLEUr1n4 (Papineni et al., 2002), which is based on n-gram precisions for n = 1..4, and (2) Translation Edit Rate (TER) (Snover et al., 2006), which generalizes edit distance beyond single-word edits. 3 4.1 2 Static integration of MWE in SMT Dynamic integration of MWE in SMT The second strategy attempts to encourage cohesive translations of MWEs without ignoring their components. Word alignment and phrasal translation extraction are conducted without any MWE knowledge, so that the SMT system can learn word-forword translations from consistently translated compositional MWEs. MWE knowledge is integrated as a feature in the tr"
N10-1029,W09-2907,0,0.594536,"Missing"
N10-1029,J93-1007,0,0.481454,"Missing"
N10-1029,2006.amta-papers.25,0,0.0445852,"advantage of the rich lexical resources available for English on the input side. Our test set consists of the 813 newswire sentences of the 2008 NIST Open Machine Translation Evaluation, which is standard evaluation data for Arabic-English translation. The first English reference translation is used as the input to our SMT system, and the single Arabic translation is used as the unique reference2 . Translation quality is evaluated using two automatic evaluation metrics: (1) BLEUr1n4 (Papineni et al., 2002), which is based on n-gram precisions for n = 1..4, and (2) Translation Edit Rate (TER) (Snover et al., 2006), which generalizes edit distance beyond single-word edits. 3 4.1 2 Static integration of MWE in SMT Dynamic integration of MWE in SMT The second strategy attempts to encourage cohesive translations of MWEs without ignoring their components. Word alignment and phrasal translation extraction are conducted without any MWE knowledge, so that the SMT system can learn word-forword translations from consistently translated compositional MWEs. MWE knowledge is integrated as a feature in the translation lexicon. For each entry, in addition to the standard phrasal translation probabilities, we define a"
N10-1029,P07-2045,0,\N,Missing
N10-1029,W09-2903,1,\N,Missing
N12-1057,P11-1078,0,0.123661,"in language use (e.g., (O’Barr, 1982)). Locher (2004) recognizes “restriction of an interactant’s action-environment” (Wartenberg, 1990) as a key element by which exercise of power in interactions can be identified. Through ODP we capture this action-restriction at an utterance level. In the computational field, several studies have used Social Network Analysis (e.g., (Diesner and Carley, 2005)) for extracting social relations from online communication. Only recently have researchers started using NLP to analyze the content of messages to deduce social relations (e.g., (Diehl et al., 2007)). Bramsen et al. (2011) use knowledge of the actual organizational structure to create two sets of messages: messages sent from a superior to a subordinate, and vice versa. Their task is to determine the direction of power (since all their data, by construction of the corpus, has a power relationship). Their reported results cannot be directly compared with ours since their results are on classifying aggregations of messages as being to a superior or to a subordinate, whereas our results are on predicting whether a single utterance has an ODP or not. 518 2012 Conference of the North American Chapter of the Associati"
N12-1057,W09-3953,1,0.708218,"Missing"
N12-1057,prabhakaran-etal-2012-annotations,1,0.762636,"Missing"
N13-1089,S12-1051,1,0.919325,"ry) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of-the-art performance on the LI06 (Li et al., 2006) SS dataset. However, all of these models make harsh simplifying assumptions on how a token is generated: (1) in LSA/WTMF, a token is generated by the inner product of the word latent"
N13-1089,D07-1109,0,0.280057,"Missing"
N13-1089,P10-1156,1,0.821303,"he complete semantic picture of the sentence. In this paper, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) m"
N13-1089,D11-1051,1,0.849791,"sional space (Li et al., 2006; Liu et al., 2007; Islam and Inkpen, 2008; Tsatsaronis et al., 2010; Ho et al., 2010), where co-occurrence information was not efficiently exploited. Researchers (O’Shea et al., 2008) find LSA does not yield good performance. In (Guo and Diab, 2012b; Guo and Diab, 2012c), we show the superiority of the latent space approach in WTMF. In this paper, we improve the WTMF model 743 and achieve state-of-the-art Pearson correlation on two standard SS datasets. There are latent variable models designed for lexical semantics, such as word senses (Boyd-Graber et al., 2007; Guo and Diab, 2011), function words (Griffiths et al., 2005), selectional preference (Ritter et al., 2010), synonyms and antonyms (Yih et al., 2012), etc. However little improvement is shown on document/sentence level semantics: (Ritter et al., 2010) and (Yih et al., 2012) focus on selectional preference and antonym identification, respectively; in (Griffiths et al., 2005) the LDA performance degrades in the text categorization task including the modeling of function words. Rather, we concentrate on nuanced lexical semantics phenomena that could benefit sentential semantics. 9 Conclusion We incorporate corpus-ba"
N13-1089,P12-2028,1,0.868001,"picture of the sentence. In this paper, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large mar"
N13-1089,P12-1091,1,0.921317,"picture of the sentence. In this paper, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large mar"
N13-1089,S12-1086,1,0.933269,"picture of the sentence. In this paper, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large mar"
N13-1089,W97-0209,0,0.146755,"el semantics. However, in the SS setting, it is crucial to make good use of each word, given the limited number of words in a sentence. We believe a reasonable word generation story will avoid introducing noise in sentential semantics, encouraging robust lexical semantics which can further boost the sentential semantics. In this paper, we explicitly encode lexical semantics, both corpus-based and knowledge-based information, in the WTMF model, by which we are able to achieve even better results in SS task. The additional corpus-based information we exploit is selectional preference semantics (Resnik, 1997), a feature already existing in the data yet ignored by most latent variable models. Selectional preference focuses on the admissible arguments for a word, thus capturing more nuanced semantics than the sentence IDs (when applied to a corpus of sentences as opposed to documents). Consider the following example: 739 Proceedings of NAACL-HLT 2013, pages 739–745, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics Figure 1: matrix factorization Many analysts say the global Brent crude oil benchmark price, currently around $111 a barrel ... In WTMF/LSA/LDA, a word wi"
N13-1089,D12-1111,0,0.112072,"Missing"
N13-1089,N06-1057,0,0.0119468,"er, we hypothesize that by better modeling lexical semantics we can obtain better sentential semantics. We incorporate both corpus-based (selectional preference information) and knowledge-based (similar words extracted in a dictionary) lexical semantics into a latent variable model. The experiments show state-of-the-art performance among unsupervised systems on two SS datasets. 1 Introduction Sentence Similarity [SS] is emerging as a crucial step in many NLP tasks that focus on sentence level semantics such as word sense disambiguation (Guo and Diab, 2010; Guo and Diab, 2012a), summarization (Zhou et al., 2006), text coherence (Lapata and Barzilay, 2005), tweet clustering (Sankaranarayanan et al., 2009; Jin et al., 2011), etc. SS operates in a very small context, on average 11 words per sentence in Semeval-2012 dataset (Agirre et al., 2012), resulting in inadequate evidence to generalize to robust sentential semantics. Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b) is a latent variable model that outperforms Latent Semantic Analysis [LSA] (Deerwester et al., 1990) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) models by a large margin in the SS task, yielding state-of"
N13-1089,C10-2048,0,\N,Missing
N13-1089,P10-1044,0,\N,Missing
N19-1391,C18-1226,1,0.850173,"ence, monolingual sentence embeddings are generated first, then mapped to the target space using the sentencelevel transformation matrix.1 4 Experiments We used skip-gram with subword information, i.e FastText (Bojanowski et al., 2017), for the static word embeddings, and ELMo for contextualized word embeddings. Sentence embeddings were calculated from ELMo as the arithmetic average of the contextualized embeddings 2 . For FastText, we applied weighted averaging using smooth inverse frequency (Arora et al., 2017), which works better for sentence similarity compared to other averaging schemes (Aldarmaki and Diab, 2018). 4.1 Data and Processing We trained and aligned all models using the same monolingual and parallel datasets. For monolingual training, we used the 1 Billion Word benchmark (Chelba et al., 2014) for English, and equivalent subsets of ∼400 million tokens from WMT’13 (Bojar et al., 2013) news crawl data. We trained monolingual ELMo and FastText with default parameters. We used the WMT’13 commoncrawl data for cross-lingual mapping, and the WMT’13 test sets for evaluating sentence translation retrieval. For all datasets, the only preprocessing we performed was tokenization. 4.2 Evaluation Framewor"
N19-1391,Q18-1014,1,0.898784,"ransformation matrix on the entries of a bilingual seed dictionary (Mikolov et al., 2013). This approach is versatile and scalable: multilingual embeddings can be obtained by mapping the vector spaces of multiple languages into a shared target language, typically English. In addition, imposing an orthogonality constraint on the mapping ensures that the original pair-wise distances are preserved after the transformation and results in better word translation retrieval (Artetxe et al., 2016; Smith et al., 2017). While word vector spaces tend to be globally consistent across language variations (Aldarmaki et al., 2018), individual words like homographs with unrelated senses (e.g. ‘bank’, ‘coast’) and phrasal verbs (‘stand up’, ‘stand out’) are likely to behave less consistently in multilingual vector spaces due to their different usage distributions. Consequently, using such words in the alignment dictionary may result in suboptimal overall mapping. We propose two approaches to counteract this effect by incorporating sentential context in the mapping process without explicit word sense disambiguation or additional linguistic resources. The first approach is based on the recently proposed contextualized embe"
N19-1391,D16-1250,0,0.273896,"et al., 2016). One of the most common and effective approaches for obtaining bilingual word embeddings is by fitting a linear transformation matrix on the entries of a bilingual seed dictionary (Mikolov et al., 2013). This approach is versatile and scalable: multilingual embeddings can be obtained by mapping the vector spaces of multiple languages into a shared target language, typically English. In addition, imposing an orthogonality constraint on the mapping ensures that the original pair-wise distances are preserved after the transformation and results in better word translation retrieval (Artetxe et al., 2016; Smith et al., 2017). While word vector spaces tend to be globally consistent across language variations (Aldarmaki et al., 2018), individual words like homographs with unrelated senses (e.g. ‘bank’, ‘coast’) and phrasal verbs (‘stand up’, ‘stand out’) are likely to behave less consistently in multilingual vector spaces due to their different usage distributions. Consequently, using such words in the alignment dictionary may result in suboptimal overall mapping. We propose two approaches to counteract this effect by incorporating sentential context in the mapping process without explicit word"
N19-1391,Q17-1010,0,0.0899947,"ligned sentence embeddings. Over a large parallel corpus, the aggregate mapping can yield a more optimal global solution compared to word-level mapping. This approach can be applied using any model capable of generating monolingual sentence embeddings. In this work, we use the average of word vectors in each sentence, where the word vectors are either static or contextualized. For inference, monolingual sentence embeddings are generated first, then mapped to the target space using the sentencelevel transformation matrix.1 4 Experiments We used skip-gram with subword information, i.e FastText (Bojanowski et al., 2017), for the static word embeddings, and ELMo for contextualized word embeddings. Sentence embeddings were calculated from ELMo as the arithmetic average of the contextualized embeddings 2 . For FastText, we applied weighted averaging using smooth inverse frequency (Arora et al., 2017), which works better for sentence similarity compared to other averaging schemes (Aldarmaki and Diab, 2018). 4.1 Data and Processing We trained and aligned all models using the same monolingual and parallel datasets. For monolingual training, we used the 1 Billion Word benchmark (Chelba et al., 2014) for English, an"
N19-1391,J93-2003,0,0.156377,"lated as the hidden states of a bi-LSTM network trained as a language model (Peters et al., 2018). The network can be used in lieu of static word embeddings within other models, which yields better performance in a range of tasks, including word sense disambiguation. Sentence embeddings can be obtained from ELMo by averaging the contextualized word embeddings (Perone et al., 2018). Since ELMo generates dynamic, contextdependent vectors, we cannot use a simple wordlevel dictionary to map the model across languages. Instead, we use a parallel corpus with word alignments, i.e using an IBM Model (Brown et al., 1993), to extract a dynamic dictionary of aligned contextualized word embeddings. Depending on the size of the parallel corpus, a large dictionary can be extracted to learn an orthogonal mapping as described in Section 3.1, which is then applied post-hoc on newly generated contextualized embeddings. 3.3 T Y X = U ΣV T Then, R = UV T (2) The resultant transformation, R, can then be used to transform additional vectors in the source vector space. The quality of the transformation depends on the size and accuracy of the initial Mapping of Contextualized Embeddings Sentence-Level Mapping An alternative"
N19-1391,S17-2002,0,0.157053,"Missing"
N19-1391,N13-1073,0,0.044501,"ighted averaging. requirements of each approach and measure how the various models respond to additional data, we split the training parallel corpus into smaller subsets of increasing sizes, starting from 100 to a million sentences (we double the size at each step). Data splits and evaluation scripts are available at https://github.com/h-aldarmaki/ sent_translation_retrieval. 4.3 Alignment Schemes For ELMo, word embeddings need to be calculated from context, so we extracted a dictionary of contextualized words from the parallel corpora by first applying word-level alignments using Fast Align (Dyer et al., 2013). We then calculated the contextualized embeddings for source and target sentences, and extracted a dictionary from the aligned words that have a one-to-one alignment (i.e. we excluded phrasal alignments). Since this can result in a very large dictionary, we capped the number of dictionary words at 1M for efficiency. For a fair comparison with FastText word-level mapping, we extracted a dictionary from word alignment probabilities using the same parallel sets. For each word in the source language, we extracted its translation as the word with the maximum alignment probability if the maximum wa"
N19-1391,N18-1202,0,0.755153,"with unrelated senses (e.g. ‘bank’, ‘coast’) and phrasal verbs (‘stand up’, ‘stand out’) are likely to behave less consistently in multilingual vector spaces due to their different usage distributions. Consequently, using such words in the alignment dictionary may result in suboptimal overall mapping. We propose two approaches to counteract this effect by incorporating sentential context in the mapping process without explicit word sense disambiguation or additional linguistic resources. The first approach is based on the recently proposed contextualized embeddings from language models, ELMo (Peters et al., 2018). Using a parallel corpus with word-alignments, we extract contextualized embeddings to construct a contextaware dictionary for mapping. The second approach is to learn a transformation between sentence embeddings rather than individual word embeddings. Since these embeddings include context that spans full sentences, we surmise that a mapping learned at this level would be more robust to individual word misalignments. We used a constrained set of parallel sentences ranging from one hundred to a million sentences for alignment. We then evaluated the resultant mappings on sentence translation r"
N19-1391,N19-1162,0,0.0389167,"l word vector spaces using a seed dictionary was originally proposed in Mikolov et al. (2013). In Artetxe et al. (2016) and Smith et al. (2017), it was shown that imposing an orthogonality constraint on the transformation leads to better word translation quality. Recently, contextualized word embeddings were proposed, where a sequential neural network is trained as a language model and then used to extract context-sensitive word representations from the hidden states (Peters et al., 2018). We use parallel text in order to align independently-trained contextualized embeddings across languages. Schuster et al. (2019) independently proposed a cross-lingual alignment approach for contextualized embeddings without the use of parallel text. 3 Approach 3.1 Orthogonal Bilingual Mapping Given a dictionary of source to target pairs hx, yi and matrix representations X and Y whose columns are vector representations of the corresponding dictionary items, we seek to find an orthogonal transformation matrix R that minimizes the distances between the transformed vectors in RX and Y . Formally, ˆ − Y k s. t. R ˆT R ˆ=I R = arg minkRX (1) ˆ R where k.k denotes the Frobenius norm. The orthogonality constraint ensures that"
N19-1391,P16-1157,0,0.0270817,"s. We also implement cross-lingual mapping of deep contextualized word embeddings using parallel sentences with word alignments. In our experiments, both approaches resulted in cross-lingual sentence embeddings that outperformed context-independent word mapping in sentence translation retrieval. Furthermore, the sentence-level transformation could be used for word-level mapping without loss in word translation quality. 1 Introduction Cross-lingual word vector models aim to embed words from multiple languages into a shared vector space to enable cross-lingual transfer and dictionary expansion (Upadhyay et al., 2016). One of the most common and effective approaches for obtaining bilingual word embeddings is by fitting a linear transformation matrix on the entries of a bilingual seed dictionary (Mikolov et al., 2013). This approach is versatile and scalable: multilingual embeddings can be obtained by mapping the vector spaces of multiple languages into a shared target language, typically English. In addition, imposing an orthogonality constraint on the mapping ensures that the original pair-wise distances are preserved after the transformation and results in better word translation retrieval (Artetxe et al"
P02-1033,W99-0512,0,\N,Missing
P02-1033,W00-0801,1,\N,Missing
P02-1033,H91-1025,0,\N,Missing
P02-1033,H94-1047,0,\N,Missing
P02-1033,C92-2070,0,\N,Missing
P02-1033,H93-1052,0,\N,Missing
P02-1033,P95-1026,0,\N,Missing
P02-1033,P91-1048,0,\N,Missing
P02-1033,J94-4003,0,\N,Missing
P02-1033,P99-1068,1,\N,Missing
P02-1033,P00-1056,0,\N,Missing
P04-1039,W04-1609,1,0.833866,"a produced by Diab’s unsupervised system SALAAM (Diab&Resnik, 2002; Diab, 2003). SALAAM is a WSD system that exploits parallel corpora for sense disambiguation of words in running text. To date, SALAAM yields the best scores for an unsupervised system on the SENSEVAL2 English All-Words task (Diab, 2003). SALAAM is an appealing approach as it provides automatically sense annotated data in two languages simultaneously, thereby providing a multilingual framework for solving the data acquisition problem. For instance, SALAAM has been used to bootstrap the WSD process for Arabic as illustrated in (Diab, 2004). In a supervised learning setting, WSD is cast as a classification problem, where a predefined set of sense tags constitutes the classes. The ambiguous words in text are assigned one or more of these classes by a machine learning algorithm based on some extracted features. This algorithm learns parameters from explicit associations between the class and the features, or combination of features, that characterize it. Therefore, such systems are very sensitive to the training data, and those data are, generally, assumed to be as clean as possible. In this paper, we question that assumption. Can"
P04-1039,P02-1033,1,0.895475,"Missing"
P04-1039,1992.tmi-1.9,0,0.368047,"Missing"
P04-1039,P99-1020,0,0.0442121,"Missing"
P04-1039,mihalcea-2002-bootstrapping,0,0.204238,"Missing"
P04-1039,P95-1026,0,0.269165,"Missing"
P04-1039,S01-1014,0,\N,Missing
P06-2102,P05-1071,0,0.0353813,"acted. The verbs and frames are put into a matrix where the row entries are the verbs and the column entries are the frames. The elements of the matrix are the frequency of the row verb occurring in a given frame column entry. There are 2401 verb types and 320 frame types, corresponding to 52167 total verb frame tokens. For the LSA feature, we apply LSA to the AG corpus. AG (GIGAWORD 2) comprises 481 million words of newswire text. The AG corpus is morphologically disambiguated using MADA.5 MADA is an SVM based system that disambiguates among different morphological analyses produced by BAMA.(Habash and Rambow, 2005) We extract the lemma forms of all the words in AG 5 798 http://www.ccls.columbia.edu/cadim/resources and use them for the LSA algorithm. To extract the LSA vectors, first the lemmatized AG data is split into 100 sentence long pseudo-documents. Next, an LSA model is trained using the Infomap software 6 on half of the AG (due to size limitations of Infomap). Infomap constructs a word similarity matrix in document space, then reduces the dimensionality of the data using SVD. LSA reduces AG to 44 dimensions. The 44-dimensional vector is extracted for each verb, which forms the LSA data set for cl"
P06-2102,J01-3003,0,0.127828,"of raw Arabic newswire text. In our ongoing project, we exploit the ATB and AG to determine the best features for the novel task of automatically creating lexical semantic verb classes 1 http://www.ldc.upenn.edu/ 795 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 795–802, c Sydney, July 2006. 2006 Association for Computational Linguistics 2 effects of three more features, and report results using both precision and recall. This project is inspired by previous approaches to automatically induce lexical semantic classes for English verbs, which have met with success (Merlo and Stevenson, 2001; Schulte im Walde, 2000) , comparing their results with manually created Levin verb classes. However, Arabic morphology has well known correlations with the kind of event structure that forms the basis of the Levin classification. (Fassi-Fehri, 2003). This characteristic of the language makes this a particularly interesting task to perform in MSA. Thus, the scientific goal of this project is to determine the features that best aid verb clustering, particularly the language-specific features that are unique to MSA and related languages. Levin Classes The idea that verbs form lexical semantic c"
P06-2102,C00-2108,0,0.0135269,"ngoing project, we exploit the ATB and AG to determine the best features for the novel task of automatically creating lexical semantic verb classes 1 http://www.ldc.upenn.edu/ 795 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 795–802, c Sydney, July 2006. 2006 Association for Computational Linguistics 2 effects of three more features, and report results using both precision and recall. This project is inspired by previous approaches to automatically induce lexical semantic classes for English verbs, which have met with success (Merlo and Stevenson, 2001; Schulte im Walde, 2000) , comparing their results with manually created Levin verb classes. However, Arabic morphology has well known correlations with the kind of event structure that forms the basis of the Levin classification. (Fassi-Fehri, 2003). This characteristic of the language makes this a particularly interesting task to perform in MSA. Thus, the scientific goal of this project is to determine the features that best aid verb clustering, particularly the language-specific features that are unique to MSA and related languages. Levin Classes The idea that verbs form lexical semantic clusters based on their sy"
P06-2102,N06-2039,1,0.554608,"at although the syntactic alternations will differ across languages, the semantic similarities that they signal will hold cross linguistically. For Arabic, a significant test of LH has been the work of Fareh and Hamdan (2000), who argue the existence of the Locative Alternation in Jordanian Arabic. However, to date no general study of MSA verbs and alternations exists. We address this problem by automatically inducing such classes, exploiting explicit syntactic and morphological information in the ATB using unsupervised clustering techniques. This paper is an extension of our previous work in Snider and Diab (2006), which found a preliminary effect of syntactic frames on the precision of MSA verb clustering. In this work, we find We exploit the resources in the Arabic Treebank (ATB) and Arabic Gigaword (AG) to determine the best features for the novel task of automatically creating lexical semantic verb classes for Modern Standard Arabic (MSA). The verbs are classified into groups that share semantic elements of meaning as they exhibit similar syntactic behavior. The results of the clustering experiments are compared with a gold standard set of classes, which is approximated by using the noisy English t"
P08-1091,P98-1013,0,0.0162825,"Missing"
P08-1091,W05-0620,0,0.235943,"Missing"
P08-1091,W03-1006,0,0.0357672,"ts in a sentence has a lot of potential for and is a significant step toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this paper, we present an SRL system for MSA that exploits many aspects of the rich morphological features of the language. It is based on a supervised model that uses support"
P08-1091,P02-1034,0,0.023017,"us to save a lot of time in the design and implementation of features. The basic idea is (a) to design a set of basic value-attribute features and apply polynomial kernels and generate all possible combinations; or (b) to design basic tree structures expressing properties related to the target linguistic objects and use tree kernels to generate all possible tree subparts, which will constitute the feature representation vectors for the learning algorithm. Tree kernels evaluate the similarity between two trees in terms of their overlap, generally measured as the number of common substructures (Collins and Duffy, 2002). For example, Figure 2, shows a small parse tree and some of its fragments. To design a function which computes the number of common substructures between two trees t1 and t2 , let us define the set of fragments F={f1 , f2 , ..} and the indicator function Ii (n), equal to 1 if the target fi is rooted at node n and 0 otherwise. A tree kernel function KT (·) over two trees is defined as: 802 VP VBD  @YK. NP NP NN  KP NP NP NNP NNP ð P ú m.' ðP úæJ Ë@ Z@P PñË@ NN JJ Figure 3: Example of the positive AST structured feature encoding the argument ARG0 in the sentence depicted in Figure 1. P P"
P08-1091,S07-1017,1,0.883154,"Missing"
P08-1091,2007.mtsummit-papers.20,1,0.75554,"st for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this paper, we present an SRL system for MSA that exploits many aspects of the rich morphological features of the language. It is based on a supervised model that uses support vector machines (SVM) technology (Vapnik, 1998) for argument boundary detection and argument classification. It is trained and tested using the pilot Arabic Propbank data released as part of the SemEval 2007 data. Given the lack of a reliable Arabic deep syntactic parser, we 1 2 http://nlp.cs.swarthmore.edu/semeval/ We"
P08-1091,erk-pado-2006-shalmaneser,0,0.0180766,"ea applies to passive constructions, for example. There is a widely held belief in the NLP and computational linguistics communities that identifying and defining roles of predicate arguments in a sentence has a lot of potential for and is a significant step toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on t"
P08-1091,P02-1031,0,0.0280664,"oles of predicate arguments in a sentence has a lot of potential for and is a significant step toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this paper, we present an SRL system for MSA that exploits many aspects of the rich morphological features of the language. It is based on a supervised m"
P08-1091,P05-1071,0,0.031638,"ic language as expressed through morphology. Hence, we explicitly encode new SRL features that capture the richness of Arabic morphology and its role in morpho-syntactic behavior. The set of morphological attributes include: inflectional morphology such as Number, Gender, Definiteness, Mood, Case, Person; derivational morphology such as the Lemma form of the words with all the diacritics explicitly marked; vowelized and fully diacritized form of the surface form; the English gloss6 . It is worth noting that there exists highly accurate morphological taggers for Arabic such as the MADA system (Habash and Rambow, 2005; Roth et al., 2008). MADA tags 6 The gloss is not sense disambiguated, hence they include homonyms. Feature Name Definiteness Number Gender Case Mood Person Lemma Gloss Vocalized word Unvowelized word Description Applies to nominals, values are definite, indefinite or inapplicable Applies to nominals and verbs, values are singular, plural or dual or inapplicable Applies to nominals, values are feminine, masculine or inapplicable Applies to nominals, values are accusative, genitive, nominative or inapplicable Applies to verbs, values are subjunctive, indicative, jussive or inapplicable Applies"
P08-1091,W05-0623,0,0.0215325,"ns such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this paper, we present an SRL system for MSA that exploits many aspects of the rich morphological features of the language. It is based on a supervised model that uses support vector machines (SVM) technology (Vapnik, 1998) for argument boundary detection and argument classification."
P08-1091,maamouri-etal-2006-developing,1,0.898791,"Missing"
P08-1091,W05-0630,1,0.888893,"beyond the previously proposed basic SRL system for Arabic (Diab et al., 2007a; Diab and Moschitti, 2007). We exploit the full morphological potential of the language to verify our hypothesis that taking advantage of the interaction between morphology and syntax can improve on a basic SRL system for morphologically rich languages. Similar to the previous Arabic SRL systems, our adopted SRL models use Support Vector Machines to implement a two step classification approach, i.e. boundary detection and argument classification. Such models have already been investigated in (Pradhan et al., 2005; Moschitti et al., 2005). The two step classification description is as follows. 3.1 Predicate Argument Extraction The extraction of predicative structures is based on the sentence level. Given a sentence, its predicates, as indicated by verbs, have to be identified along with their arguments. This problem is usually divided in two subtasks: (a) the detection of the target argument boundaries, i.e. the span of the argument words in the sentence, and (b) the classification of the argument type, e.g. Arg0 or ArgM for Propbank S NP VP VP ⇒ VBD VP NNP VBD Mary bought NP bought D N a cat Figure 2: NP VBD VP NP VBD D N D N"
P08-1091,P07-1098,1,0.866381,"same. Hence, for the sentence ‘John opened the door’ and ‘the door opened’, though ‘the door’ is the object of the first sentence and the subject of the second, it is the ‘theme’ in both sentences. Same idea applies to passive constructions, for example. There is a widely held belief in the NLP and computational linguistics communities that identifying and defining roles of predicate arguments in a sentence has a lot of potential for and is a significant step toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the"
P08-1091,P04-1043,1,0.937443,"p toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this paper, we present an SRL system for MSA that exploits many aspects of the rich morphological features of the language. It is based on a supervised model that uses support vector machines (SVM) technology (Vapnik, 1998) for argument b"
P08-1091,E06-1015,1,0.886131,"owelized Arabic in the Buckwalter transliteration scheme for deriving the basic features for the AST experimental condition. The data comprises a development set, a test set and a training set of 886, 902 and 8,402 sentences, respectively, where each set contain 1725, 1661 and 21,194 argument instances. These instances are distributed over 26 different role types. The training instances of the boundary detection task also include parse-tree nodes that do not correspond to correct boundaries (we only considered 350K examples). For the experiments, we use SVM-Light-TK toolkit8 (Moschitti, 2004; Moschitti, 2006) and its SVM-Light default parameters. The system performance, i.e. F1 on single boundary and role classifier, accuracy of the role multi-classifier and the F1 of the complete SRL systems, are computed by means of the CoNLL evaluator9 . 4.2 Results Figure 5 reports the F1 of the SVM boundary classifier using Polynomial Kernels with a degree from 1 to 6 (i.e. Polyi), the AST and the EAST kernels and their combinations. We note that as we introduce conjunctions, i.e. a degree larger than 2, the F1 increases by more than 3 percentage points. Thus, not only are the English features meaningful for"
P08-1091,W06-2909,1,0.926303,"Missing"
P08-1091,P08-2030,1,0.812311,"through morphology. Hence, we explicitly encode new SRL features that capture the richness of Arabic morphology and its role in morpho-syntactic behavior. The set of morphological attributes include: inflectional morphology such as Number, Gender, Definiteness, Mood, Case, Person; derivational morphology such as the Lemma form of the words with all the diacritics explicitly marked; vowelized and fully diacritized form of the surface form; the English gloss6 . It is worth noting that there exists highly accurate morphological taggers for Arabic such as the MADA system (Habash and Rambow, 2005; Roth et al., 2008). MADA tags 6 The gloss is not sense disambiguated, hence they include homonyms. Feature Name Definiteness Number Gender Case Mood Person Lemma Gloss Vocalized word Unvowelized word Description Applies to nominals, values are definite, indefinite or inapplicable Applies to nominals and verbs, values are singular, plural or dual or inapplicable Applies to nominals, values are feminine, masculine or inapplicable Applies to nominals, values are accusative, genitive, nominative or inapplicable Applies to verbs, values are subjunctive, indicative, jussive or inapplicable Applies to verbs and pronou"
P08-1091,N04-1032,0,0.0147664,"e constructions, for example. There is a widely held belief in the NLP and computational linguistics communities that identifying and defining roles of predicate arguments in a sentence has a lot of potential for and is a significant step toward improving important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this pape"
P08-1091,W04-3212,0,0.304138,"g important applications such as document retrieval, machine translation, question answering and information extraction (Moschitti et al., 2007). To date, most of the reported SRL systems are for English, and most of the data resources exist for English. We do see some headway for other languages such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004). The systems for the other languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Chen and Rambow, 2003; Thompson et al., 2003; Pradhan et al., 2003; Moschitti, 2004; Xue and Palmer, 2004; Haghighi et al., 2005). In the same spirit and facilitated by the release of the SemEval 2007 Task 18 data1 , based on the Pilot Arabic Propbank, a preliminary SRL system exists for Arabic2 (Diab and Moschitti, 2007; Diab et al., 2007a). However, it did not exploit some special characteristics of the Arabic language on the SRL task. In this paper, we present an SRL system for MSA that exploits many aspects of the rich morphological features of the language. It is based on a supervised model that uses support vector machines (SVM) technology (Vapnik, 1998) for argument boundary detection and"
P08-1091,C98-1013,0,\N,Missing
P08-1091,J02-3001,0,\N,Missing
P08-2030,N07-2014,1,0.69545,"Missing"
P08-2030,A00-2013,0,0.302047,"Missing"
P08-2030,P06-1073,0,0.106827,"Missing"
P08-2030,P05-1071,1,\N,Missing
P09-1048,P98-1013,0,0.0125282,"Missing"
P09-1048,W05-0620,0,0.0201316,"Missing"
P09-1048,P01-1017,0,0.0402229,"Missing"
P09-1048,W03-1006,0,0.020803,"Missing"
P09-1048,erk-pado-2006-shalmaneser,0,0.021266,"Missing"
P09-1048,J02-3001,0,0.10105,"Missing"
P09-1048,P02-1031,0,0.0346015,"Missing"
P09-1048,P07-1098,0,0.019224,"Missing"
P09-1048,N07-1051,0,0.0240934,"Missing"
P09-1048,C04-1127,1,0.822126,"Missing"
P09-1048,N04-1032,0,0.0335295,"Missing"
P09-1048,W05-0623,0,0.0539303,"Missing"
P09-1048,W04-3212,0,0.049316,"Missing"
P09-1048,W01-1511,1,0.848003,"Missing"
P09-1048,D07-1077,0,0.0941353,"Missing"
P09-1048,N10-1005,1,\N,Missing
P09-1048,C98-1013,0,\N,Missing
P10-1156,D07-1007,0,0.0414174,"t using an unsupervised method, we achieve an overall F measure of 64.58 using a voting scheme. 1 Introduction Despite advances in natural language processing (NLP), Word Sense Disambiguation (WSD) is still considered one of the most challenging problems in the field. Ever since the field’s inception, WSD has been perceived as one of the central problems in NLP. WSD is viewed as an enabling technology that could potentially have far reaching impact on NLP applications in general. We are starting to see the beginnings of a positive effect of WSD in NLP applications such as Machine Translation (Carpuat and Wu, 2007; Chan et al., 2007). Advances in WSD research in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as WordNets (Fellbaum, 1998; Miller, 1990), the availability of large scale corpora, the existence and dissemination of standardized data sets over the past 10 years through different testbeds such as SENSEVAL and SEMEVAL competitions,1 devising more robust computing algorithms to handle large scale data sets, and simply advancement in hardware machinery. In this paper, we address the problem of WSD of all conten"
P10-1156,P07-1005,0,0.0443211,"d method, we achieve an overall F measure of 64.58 using a voting scheme. 1 Introduction Despite advances in natural language processing (NLP), Word Sense Disambiguation (WSD) is still considered one of the most challenging problems in the field. Ever since the field’s inception, WSD has been perceived as one of the central problems in NLP. WSD is viewed as an enabling technology that could potentially have far reaching impact on NLP applications in general. We are starting to see the beginnings of a positive effect of WSD in NLP applications such as Machine Translation (Carpuat and Wu, 2007; Chan et al., 2007). Advances in WSD research in the current millennium can be attributed to several key factors: the availability of large scale computational lexical resources such as WordNets (Fellbaum, 1998; Miller, 1990), the availability of large scale corpora, the existence and dissemination of standardized data sets over the past 10 years through different testbeds such as SENSEVAL and SEMEVAL competitions,1 devising more robust computing algorithms to handle large scale data sets, and simply advancement in hardware machinery. In this paper, we address the problem of WSD of all content words in a sentenc"
P10-1156,P02-1033,1,0.911667,"e task is to associate all tokens with their contextually relevant meaning definitions from some computational lexical resource. Our work hinges upon combining two high quality WSD systems that rely on essentially different sources of evidence. The two WSD systems are a monolingual system RelCont and a multilingual system TransCont. RelCont is an enhancement on an existing graph based algorithm, In-Degree, first described in (Navigli and Lapata, 2007). TransCont is an enhancement over an existing approach that leverages multilingual evidence through projection, SALAAM, described in detail in (Diab and Resnik, 2002). Similar to the leveraged systems, the current combined approach is unsupervised, namely it does not rely on training data from the onset. We show that by combining both sources of evidence, our approach yields the highest performance for an unsupervised system to date on standard All-Words data sets. This paper is organized as follows: Section 2 delves into the problem of WSD in more detail; Section 3 explores some of the relevant related work; in Section 4, we describe the two WSD systems in some detail emphasizing the improvements to the basic systems in addition to a description of our co"
P10-1156,W09-2410,1,0.332531,"Missing"
P10-1156,O97-1002,0,0.0563038,"Missing"
P10-1156,H05-1052,0,0.371104,"the words that are mapped to (aligned with) the same orthographic form in a foreign language constitute the context. In the next subsections we describe the two approaches RelCont and TransCont in some detail, then we proceed to describe two combination methods for the two approaches: MERGE and VOTE. 4.1 Monolingual System RelCont RelCont is based on an extension of a stateof-the-art WSD approach by (Sinha and Mihalcea, 2007), henceforth (SM07). In the basic SM07 work, the authors combine different semantic similarity measures with different graph based algorithms as an extension to work in (Mihalcea, 2005). Given a sequence of words W = {w1 , w2 ...wn }, each word wi with several senses {si1 , si2 ...sim }. A graph G = (V,E) is defined such that there exists a vertex v for each sense. Two senses of two different words may be connected by an edge e, depending on their distance. That two senses are connected suggests they should have influence on each other, accordingly a maximum 3 We acknowledge the existence of many research papers that tackled the AW WSD problem using unsupervised approaches, yet for lack of space we will not be able to review most of them. 1543 allowable distance is set. They"
P10-1156,J03-1002,0,0.00428089,"dverbs, and is very shallow for verbs. Essentially SALAAM relies on variability in translation as it is important to have multiple words in a typeset to allow for disambiguation. In the original SALAAM system, the authors automatically translated several balanced corpora in order to render more variable data for the approach to show it’s impact. The corpora that were translated are: the WSJ, the Brown corpus and all the SENSEVAL data. The data were translated to different languages (Arabic, French and Spanish) using state of art MT systems. They employed the automatic alignment system GIZA++ (Och and Ney, 2003) to obtain word alignments in a single direction from L1 to L2. For TransCont we use the basic SALAAM approach with some crucial modifications that lead to better performance. We still rely on parallel corpora, we extract typesets based on the intersection of word alignments in both alignment directions using more advanced GIZA++ machinery. In contrast to DR02, we experiment with all four POS: Verbs (V), Nouns (N), Adjectives (A) and Adverbs (R). Moreover, we modified the underlying disambiguation method on the typesets. We still employ WN similarity, however, we do not use the NounGroupings a"
P10-1156,S01-1005,0,0.406096,"he meaning definition of: sloping land especially the slope beside a body of water, and so on. 3 Related Works Many systems over the years have been proposed for the task. A thorough review of the state of the art through the late 1990s (Ide and Veronis, 1998) and more recently in (Navigli, 2009). Several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning techniques. To date, the best approaches that solve the AW WSD task are supervised as illustrated in the different SenseEval and SEMEVAL AW task (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised combination approach to the AW WSD problem that 2 http://wordnet.princeton.edu relies on WN similarity measures in conjunction with evidence obtained through exploiting multilingual evidence. We will review the closely relevant related work on which this current investigation is based.3 4 Our Approach Our current investigation exploits two basic unsupervised approaches that perform at state-of-theart for the AW WSD task in an unsupervised setting. Crucially the two systems rely on different sources of e"
P10-1156,S07-1016,0,0.156117,"lly the slope beside a body of water, and so on. 3 Related Works Many systems over the years have been proposed for the task. A thorough review of the state of the art through the late 1990s (Ide and Veronis, 1998) and more recently in (Navigli, 2009). Several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning techniques. To date, the best approaches that solve the AW WSD task are supervised as illustrated in the different SenseEval and SEMEVAL AW task (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised combination approach to the AW WSD problem that 2 http://wordnet.princeton.edu relies on WN similarity measures in conjunction with evidence obtained through exploiting multilingual evidence. We will review the closely relevant related work on which this current investigation is based.3 4 Our Approach Our current investigation exploits two basic unsupervised approaches that perform at state-of-theart for the AW WSD task in an unsupervised setting. Crucially the two systems rely on different sources of evidence allowing them to complement each other t"
P10-1156,W04-0811,0,0.262079,"of: sloping land especially the slope beside a body of water, and so on. 3 Related Works Many systems over the years have been proposed for the task. A thorough review of the state of the art through the late 1990s (Ide and Veronis, 1998) and more recently in (Navigli, 2009). Several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning techniques. To date, the best approaches that solve the AW WSD task are supervised as illustrated in the different SenseEval and SEMEVAL AW task (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised combination approach to the AW WSD problem that 2 http://wordnet.princeton.edu relies on WN similarity measures in conjunction with evidence obtained through exploiting multilingual evidence. We will review the closely relevant related work on which this current investigation is based.3 4 Our Approach Our current investigation exploits two basic unsupervised approaches that perform at state-of-theart for the AW WSD task in an unsupervised setting. Crucially the two systems rely on different sources of evidence allowing them to"
P10-1156,J98-1001,0,\N,Missing
P10-2052,D08-1063,1,0.824843,", 2004), the author uses projections from English into Arabic to bootstrap a sense tagging system for Arabic as well as a seed Arabic WordNet through projection. In (Hwa et al., 2002), the authors report promising results of inducing Chinese dependency trees from English. The obtained model outperformed the baseline. More recently, in (Chen and Ji, 2009), the authors report their comparative study between monolingual and cross-lingual bootstrapping. Finally, in Mention Detection (MD), a task which includes NER and adds the identification and classification of nominal and pronominal mentions, (Zitouni and Florian, 2008) show the impact of using a MT system to enhance the performance of an Arabic MD model. The authors report an improvement of up to 1.6F when the baseline system uses lexical features only. Unlike the work we present here, their approach requires the availability of an accurate MT system which is a more expensive process. the all the features except the syntagmatic ones (All-Synt.) contrasted against the system including the semantic features, i.e. All the features, per class All . The baseline results, FreqBaseline, assigns a test token the most frequent tag observed for it in the gold trainin"
P10-2052,W03-2201,0,0.0192083,"highly accurate Arabic NER system. We bootstrap noisy features by projection from an Arabic-English parallel corpus that is automatically tagged with a baseline NER system. The feature space covers lexical, morphological, and syntactic features. The proposed approach yields an improvement of up to 1.64 F-measure (absolute). 1 2 Introduction Named Entity Recognition (NER) has earned an important place in Natural Language Processing (NLP) as an enabling process for other tasks. When explicitly taken into account, research shows that it helps such applications achieve better performance levels (Babych and Hartley, 2003; Thompson and Dozier, 1997). NER is defined as the computational identification and classification of Named Entities (NEs) in running text. For instance, consider the following text: Barack Obama is visiting the Middle East. A NER system should be able to identify Barack Obama and Middle East as NEs and classify them as Person (PER) and Geo-Political Entity (GPE), respectively. The class-set used to tag NEs may vary according to user needs. In this research, we adopt the Automatic Content Extraction (ACE) 2007 nomenclature1 . According to (Nadeau and Sekine, 2007), optimization of the feature"
P10-2052,D08-1030,1,0.902274,"class-set used to tag NEs may vary according to user needs. In this research, we adopt the Automatic Content Extraction (ACE) 2007 nomenclature1 . According to (Nadeau and Sekine, 2007), optimization of the feature set is the key component in enhancing the performance of a global NER system. In this paper we investigate the possibility of building a high performance Arabic NER system by using a large space of available feature sets that go beyond the explored shallow feature sets used to date in the literature for Arabic NER. 1 Our Approach We use our state-of-the-art NER system described in (Benajiba et al., 2008) as our baseline system (BASE) since it yields, to our knowledge, the best performance for Arabic NER . BASE employs Support Vector Machines (SVMs) and Conditional Random Fields (CRFs) as Machine Learning (ML) approaches. BASE uses lexical, syntactic and morphological features extracted using highly accurate automatic Arabic POS-taggers. BASE employs a multi-classifier approach where each classifier is tagging a NE class separately. The feature selection is performed by using an incremental approach selecting the top n features (the features are ranked according to their individual impact) at"
P10-2052,W09-2209,0,0.0116509,"81.42 76.07 54.49 81.82 75.92 55.65 81.31 75.30 57.30 81.73 75.67 58.11 Table 2: Final Results obtained with selected features contrasted against all features combined works, they augment training data from parallel data for training supervised systems. In (Diab, 2004), the author uses projections from English into Arabic to bootstrap a sense tagging system for Arabic as well as a seed Arabic WordNet through projection. In (Hwa et al., 2002), the authors report promising results of inducing Chinese dependency trees from English. The obtained model outperformed the baseline. More recently, in (Chen and Ji, 2009), the authors report their comparative study between monolingual and cross-lingual bootstrapping. Finally, in Mention Detection (MD), a task which includes NER and adds the identification and classification of nominal and pronominal mentions, (Zitouni and Florian, 2008) show the impact of using a MT system to enhance the performance of an Arabic MD model. The authors report an improvement of up to 1.6F when the baseline system uses lexical features only. Unlike the work we present here, their approach requires the availability of an accurate MT system which is a more expensive process. the all"
P10-2052,P02-1033,1,0.803187,"Missing"
P10-2052,P05-1071,0,0.0608239,"ganization NE class (ORG) lexica; 4. POS-tag and Base Phrase Chunk (BPC): automatically tagged using AMIRA (Diab et al., 2007) which yields Fmeasures for both tasks in the high 90’s; 5. Morphological features: automatically tagged using the Morphological Analysis and Disambiguation for Arabic (MADA) tool to extract information about gender, number, person, definiteness and ashttp://www.nist.gov/speech/tests/ace/index.htm 281 Proceedings of the ACL 2010 Conference Short Papers, pages 281–285, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics pect for each word (Habash and Rambow, 2005); 6. Capitalization: derived as a side effect from running MADA. MADA chooses a specific morphological analysis given the context of a given word. As part of the morphological information available in the underlying lexicon that MADA exploits. As part of the information present, the underlying lexicon has an English gloss associated with each entry. More often than not, if the word is a NE in Arabic then the gloss will also be a NE in English and hence capitalized. We devise an extended Arabic NER system (EXTENDED) that uses the same architecture as BASE but employs additional features to thos"
P10-2052,P03-1058,0,0.0362183,"Missing"
P10-2052,W97-0315,0,0.583534,"R system. We bootstrap noisy features by projection from an Arabic-English parallel corpus that is automatically tagged with a baseline NER system. The feature space covers lexical, morphological, and syntactic features. The proposed approach yields an improvement of up to 1.64 F-measure (absolute). 1 2 Introduction Named Entity Recognition (NER) has earned an important place in Natural Language Processing (NLP) as an enabling process for other tasks. When explicitly taken into account, research shows that it helps such applications achieve better performance levels (Babych and Hartley, 2003; Thompson and Dozier, 1997). NER is defined as the computational identification and classification of Named Entities (NEs) in running text. For instance, consider the following text: Barack Obama is visiting the Middle East. A NER system should be able to identify Barack Obama and Middle East as NEs and classify them as Person (PER) and Geo-Political Entity (GPE), respectively. The class-set used to tag NEs may vary according to user needs. In this research, we adopt the Automatic Content Extraction (ACE) 2007 nomenclature1 . According to (Nadeau and Sekine, 2007), optimization of the feature set is the key component in"
P11-2103,C04-1200,0,0.263084,"Missing"
P11-2103,W10-1401,0,0.0195633,"ictator, stepped down). Subjective text is further classified with sentiment or polarity. For sentiment classification, the task refers to identifying whether the subjective text is positive (e.g., What an excellent camera!), negative (e.g., I hate this camera!), neutral (e.g., I believe there will be a meeting.), or, sometimes, mixed (e.g., It is good, but I hate it!) texts. Most of the SSA literature has focused on English and other Indio-European languages. Very few studies have addressed the problem for morphologically rich languages (MRL) such as Arabic, Hebrew, 587 Turkish, Czech, etc. (Tsarfaty et al., 2010). MRL pose significant challenges to NLP systems in general, and the SSA task is expected to be no exception. The problem is even more pronounced in some MRL due to the lack in annotated resources for SSA such as labeled corpora, and polarity lexica. In the current paper, we investigate the task of sentence-level SSA on Modern Standard Arabic (MSA) texts from the newswire genre. We run experiments on three different pre-processing settings based on tokenized text from the Penn Arabic Treebank (PATB) (Maamouri et al., 2004) and employ both language-independent and Arabicspecific, morphology-bas"
P11-2103,P99-1032,0,0.711929,"Missing"
P11-2103,J04-3002,0,0.0583472,"configuration schemes, with each underlined. Features: The features we employed are of two main types: Language-independent features and Morphological features. Language-Independent Features: This group of features has been employed in various SSA studies. Domain: Following (Wilson et al., 2009), we apply a feature indicating the domain of the document to which a sentence belongs. As mentioned earlier, each sentence has a document domain label manually associated with it. 2 A detailed account of issues related to the annotation task will appear in a separate publication. 588 UNIQUE: Following Wiebe et al. (2004) we apply a unique feature. Namely words that occur in our corpus with an absolute frequency &lt; 5, are replaced with the token ”UNIQUE”. N-GRAM: We run experiments with N-grams ≤ 4 and all possible combinations of them. ADJ: For subjectivity classification, we follow Bruce & Wiebe’s (1999) in adding a binary has adjective feature indicating whether or not any of the adjectives in our manually created polarity lexicon exists in a sentence. For sentiment classification, we apply two features, has POS adjective and has NEG adjective, each of these binary features indicate whether a POS or NEG adje"
P11-2103,J94-2004,0,0.0435343,"notated on the sentence level. We also describe an automatic SSA tagging system that exploits the annotated data. We investigate the impact of different levels of preprocessing settings on the SSA classification task. We show that by explicitly accounting for the rich morphology the system is able to achieve significantly higher levels of performance. 1 Introduction Subjectivity and Sentiment Analysis (SSA) is an area that has been witnessing a flurry of novel research. In natural language, subjectivity refers to expression of opinions, evaluations, feelings, and speculations (Banfield, 1982; Wiebe, 1994) and thus incorporates sentiment. The process of subjectivity classification refers to the task of classifying texts into either objective (e.g., Mubarak stepped down) or subjective (e.g., Mubarak, the hateful dictator, stepped down). Subjective text is further classified with sentiment or polarity. For sentiment classification, the task refers to identifying whether the subjective text is positive (e.g., What an excellent camera!), negative (e.g., I hate this camera!), neutral (e.g., I believe there will be a meeting.), or, sometimes, mixed (e.g., It is good, but I hate it!) texts. Most of th"
P11-2103,J09-3003,0,0.0195244,"r lemma citation forms, for instance in case of verbs it is the 3rd person masculine singular perfective form; and (3) Stem, which is the surface form minus inflectional morphemes, it should be noted that this configuration may result in non proper Arabic words (a la IR stemming). Table 1 illustrates examples of the three configuration schemes, with each underlined. Features: The features we employed are of two main types: Language-independent features and Morphological features. Language-Independent Features: This group of features has been employed in various SSA studies. Domain: Following (Wilson et al., 2009), we apply a feature indicating the domain of the document to which a sentence belongs. As mentioned earlier, each sentence has a document domain label manually associated with it. 2 A detailed account of issues related to the annotation task will appear in a separate publication. 588 UNIQUE: Following Wiebe et al. (2004) we apply a unique feature. Namely words that occur in our corpus with an absolute frequency &lt; 5, are replaced with the token ”UNIQUE”. N-GRAM: We run experiments with N-grams ≤ 4 and all possible combinations of them. ADJ: For subjectivity classification, we follow Bruce & Wi"
P12-1042,P11-4021,1,0.804094,"Missing"
P12-1042,W11-1701,0,0.0370505,"lice officer asking you to prove your citizenship is Unconstitutional? As soon as you start trading Constitutional rights for ”security”, then you’ve lost. Table 1: Example posts from the Arizona Immigration Law thread pairing as shown in Section 3 below. 2.3 Community Mining Previous work also studied community mining in social media sites. Somasundaran and Wiebe (2009) presents an unsupervised opinion analysis method for debate-side classification. They mine the web to learn associations that are indicative of opinion stances in debates and combine this knowledge with discourse information. Anand et al. (2011) present a supervised method for stance classification. They use a number of linguistic and structural features such as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. This work is limited to dual sided debates and defines the problem as a classification task where the two debate sides are know beforehand. Our work is characterized by handling multi-side debates and by regarding the problem as a clustering problem where the number of sides is not known by the algorithm. This work also utilizes only discussant-to-topic attitud"
P12-1042,E06-1027,0,0.0057825,"granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to deter400 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and"
P12-1042,banea-etal-2008-bootstrapping,0,0.0134465,"each participant. The rest of this paper is organized as follows. Section 2 examines the previous work. We describe the data used in the paper in Section 2.4. Section 3 presents our approach. Experiments, results and analysis are presented in Section 4. We conclude in Section 5 2 Related Work mine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another"
P12-1042,C08-2004,0,0.153544,"n another work, Kim and Hovy (2007) predict the results of an election by analyzing discussion threads in online forums that discuss the elections. They use a supervised approach that uses unigrams, bigrams, and trigrams as features. In contrast, our work is unsupervised and uses different types information. Moreover, although this work is related to ours at the goal level, it does not involve any opinion analysis. Another related work classifies the speakers side in a corpus of congressional floor debates, using the speakers final vote on the bill as a labeling for side (Thomas et al., 2006; Bansal et al., 2008; 401 Yessenalina et al., 2010). This work infers agreement between speakers based on cases where one speaker mentions another by name, and a simple algorithm for determining the polarity of the sentence in which the mention occurs. This work shows that even with the resulting sparsely connected agreement structure, the MinCut algorithm can improve over stance classification based on textual information alone. This work also requires that the debate sides be known by the algorithm and it only identifies discussant-to-discussant attitude. In our experiments below we show that identifying both d"
P12-1042,H05-1091,0,0.056093,"Stanford Parser (Klein and Manning, 2003) to generate the dependency parse tree of each sentence in the thread. An opinion word and a target form a pair if they stratify at least one of our dependency rules. Table 4 illustrates some 404 of these rules 5 . The rules basically examine the types of the dependencies on the shortest path that connect the opinion word and the target in the dependency parse tree. It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005). If a sentence S in a post written by participant Pi contains an opinion word OPj and a target T Rk , and if the opinion-target pair satisfies one of our dependency rules, we say that Pi expresses an attitude towards T Rk . The polarity of the attitude is determined by the polarity of OPj . We represent this as + − Pi → T Rk if OPj is positive and Pi → T Rk if OPj is negative. It is likely that the same participant Pi express sentiment toward the same target T Rk multiple times in different sentences in different posts. We keep track of the counts of all the instances of positive/negative att"
P12-1042,P05-1045,0,0.00351808,". For example, the noun group Arizona immigration law is mentioned by Discussant 1 and Discussant 2 in snippets 1 and 2 above respectively. Therefore, we replace it with a placehold as illustrated in snippets (4) and (5) below. We only consider as entities noun groups that contain two words or more. We impose this requirement because individual nouns are very common and regarding all of them as entities will introduce significant noise. In addition to this shallow parsing method, we also use named entity recognition (NER) to identify more entities. We use the Stanford Named Entity Recognizer (Finkel et al., 2005) for this purpose. It recognizes three types of entities: person, location, and organization. We impose no restrictions on the entities identified using this method. Again, we replace each distinct entity with a unique placeholder. The final set of entities identified in a thread is the union of the entities identified by the two aforementioned methods. Table 3 Finally, a challenge that always arises when performing text mining tasks at this level of granularity is that entities are usually expressed by anaphorical pronouns. Previous work has shown that For example, the following snippet conta"
P12-1042,grover-etal-2000-lt,0,0.0397916,"s another discussant, either the discussant name is mentioned explicitly or a second person pronoun is used to indicate that the opinion is targeting the recipient of the post. For example, in snippet (2) above the second person pronoun you indicates that the opinion word disagree is targeting Discussant 1, the recipient of the post. The target of opinion can also be an entity mentioned in the discussion. We use two methods to identify such entities. The first method uses shallow parsing to identify noun groups (NG). We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for this purpose. We consider as an entity any noun group that is mentioned by at least two different discussants. We replace each identified entity with a unique placeholder (EN T IT YID ). For example, the noun group Arizona immigration law is mentioned by Discussant 1 and Discussant 2 in snippets 1 and 2 above respectively. Therefore, we replace it with a placehold as illustrated in snippets (4) and (5) below. We only consider as entities noun groups that contain two words or more. We impose this requirement because individual nouns are very common and regarding all of them as entities wil"
P12-1042,P10-1041,1,0.514903,"identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to deter400 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et"
P12-1042,D10-1121,1,0.869877,"prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment analysis. Previous work has studied sentiment in text at different levels of granularity. The first level is identifying the polarity of individual words. Hatziva"
P12-1042,P11-2104,1,0.551222,"n conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to deter400 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008). In this context, opinion targets usually refer to product features (i.e. product compo"
P12-1042,P97-1023,0,0.182692,"(2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment analysis. Previous work has studied sentiment in text at different levels of granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedne"
P12-1042,C00-1044,0,0.151386,"up and the subgroup membership of each participant. The rest of this paper is organized as follows. Section 2 examines the previous work. We describe the data used in the paper in Section 2.4. Section 3 presents our approach. Experiments, results and analysis are presented in Section 4. We conclude in Section 5 2 Related Work mine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant wi"
P12-1042,P10-2049,0,0.326,"components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants. Scaffidi et al. (2007) propose a language model approach to product feature extraction. They assume that product features are mentioned more often in product reviews than they appear in general English text. However, such statistics may not be reliable when the corpus size is small. In another related work, Jakob and Gurevych (2010) showed that resolving the anaphoric links in the text significantly improves opinion target extraction. In our work, we use anaphora resolution to improve opinion-target Participant A posted: I support Arizona because they have every right to do so. They are just upholding well-established federal law. All states should enact such a law. Participant B commented on A’s post: I support the law because the federal government is either afraid or indifferent to the issue. Arizona has the right and the responsibility to protect the people of the State of Arizona. If this requires a possible slight"
P12-1042,kamps-etal-2004-using,0,0.0841726,"Missing"
P12-1042,C04-1200,0,0.0270863,"different levels of granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to deter400 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context o"
P12-1042,P03-1054,0,0.00465615,"ult of applying this step to snippet (6) is: (6) It doesn’t matter whether you vote for Obama. Obama is unbeatable. Now, both mentions of Obama will be recognized by the Stanford NER system and will be identified as one entity. 3.4 Opinion-Target Pairing At this point, we have all the opinion words and the potential targets identified separately. The next step is to determine which opinion word is targeting which target. We propose a rule based approach for opinion-target pairing. Our rules are based on the dependency relations that connect the words in a sentence. We use the Stanford Parser (Klein and Manning, 2003) to generate the dependency parse tree of each sentence in the thread. An opinion word and a target form a pair if they stratify at least one of our dependency rules. Table 4 illustrates some 404 of these rules 5 . The rules basically examine the types of the dependencies on the shortest path that connect the opinion word and the target in the dependency parse tree. It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005). If a sentence"
P12-1042,D07-1114,0,0.0193424,"Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to deter400 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008). In this context, opinion targets usually refer to product features (i.e. product components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants. Scaffidi et al. (2007) propose a language model approach to product feature extraction. They assume that product features are mentioned more often in product reviews than they appea"
P12-1042,D07-1113,0,0.0393162,"d punctuation, and opinion dependencies to build a stance classification model. This work is limited to dual sided debates and defines the problem as a classification task where the two debate sides are know beforehand. Our work is characterized by handling multi-side debates and by regarding the problem as a clustering problem where the number of sides is not known by the algorithm. This work also utilizes only discussant-to-topic attitude predictions for debate-side classification. Out work utilizes both discussant-to-topic and discussant-to-discussant attitude predictions. In another work, Kim and Hovy (2007) predict the results of an election by analyzing discussion threads in online forums that discuss the elections. They use a supervised approach that uses unigrams, bigrams, and trigrams as features. In contrast, our work is unsupervised and uses different types information. Moreover, although this work is related to ours at the goal level, it does not involve any opinion analysis. Another related work classifies the speakers side in a corpus of congressional floor debates, using the speakers final vote on the bill as a labeling for side (Thomas et al., 2006; Bansal et al., 2008; 401 Yessenalin"
P12-1042,H05-1043,0,0.291514,"en word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment analysis. Previous work has studied sentiment in text at different levels of granular"
P12-1042,W03-1014,0,0.0342432,"ented in Section 4. We conclude in Section 5 2 Related Work mine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment ana"
P12-1042,P09-1026,0,0.657995,"t the people of the State of Arizona. If this requires a possible slight inconvenience to any citizen so be it. Participant C commented on B’s post: That is such a sad thing to say. You do realize that under the 14th Amendment, the very interaction of a police officer asking you to prove your citizenship is Unconstitutional? As soon as you start trading Constitutional rights for ”security”, then you’ve lost. Table 1: Example posts from the Arizona Immigration Law thread pairing as shown in Section 3 below. 2.3 Community Mining Previous work also studied community mining in social media sites. Somasundaran and Wiebe (2009) presents an unsupervised opinion analysis method for debate-side classification. They mine the web to learn associations that are indicative of opinion stances in debates and combine this knowledge with discourse information. Anand et al. (2011) present a supervised method for stance classification. They use a number of linguistic and structural features such as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. This work is limited to dual sided debates and defines the problem as a classification task where the two debate side"
P12-1042,C08-1103,0,0.0207489,"lt on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to deter400 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008). In this context, opinion targets usually refer to product features (i.e. product components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants. Scaffidi et al. (2007) propose a language model approach to product feature extraction. They assume that product features are mentioned more often in product reviews than they appear in general English text. However, such stati"
P12-1042,P05-1017,0,0.0277497,"ning could be found in (Pang and Lee, 2008). 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment analysis. Previous work has studied sentiment in text at different levels of granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work i"
P12-1042,W06-1639,0,0.413715,"titude predictions. In another work, Kim and Hovy (2007) predict the results of an election by analyzing discussion threads in online forums that discuss the elections. They use a supervised approach that uses unigrams, bigrams, and trigrams as features. In contrast, our work is unsupervised and uses different types information. Moreover, although this work is related to ours at the goal level, it does not involve any opinion analysis. Another related work classifies the speakers side in a corpus of congressional floor debates, using the speakers final vote on the bill as a labeling for side (Thomas et al., 2006; Bansal et al., 2008; 401 Yessenalina et al., 2010). This work infers agreement between speakers based on cases where one speaker mentions another by name, and a simple algorithm for determining the polarity of the sentence in which the mention occurs. This work shows that even with the resulting sparsely connected agreement structure, the MinCut algorithm can improve over stance classification based on textual information alone. This work also requires that the debate sides be known by the algorithm and it only identifies discussant-to-discussant attitude. In our experiments below we show th"
P12-1042,P08-4003,0,0.0119738,"Split posts into sentences • Anaphora resolution • Identify named entities • Identify Frequent noun phrases. • Identify mentions of other discussants • Identify polarized words • Identify the contextual polarity of each word Subgroups Discussant Attitude Profiles (DAPs) Opinion-Target Pairing • Dependency Rules Clustering Figure 1: An overview of the subgroups detection system He is unbeatable. Jakob and Gurevych (2010) showed experimentally that resolving the anaphoric links in the text significantly improves opinion target extraction. We use the Beautiful Anaphora Resolution Toolkit (BART) (Versley et al., 2008) to resolve all the anaphoric links within the text of each post separately. The result of applying this step to snippet (6) is: (6) It doesn’t matter whether you vote for Obama. Obama is unbeatable. Now, both mentions of Obama will be recognized by the Stanford NER system and will be identified as one entity. 3.4 Opinion-Target Pairing At this point, we have all the opinion words and the potential targets identified separately. The next step is to determine which opinion word is targeting which target. We propose a rule based approach for opinion-target pairing. Our rules are based on the dep"
P12-1042,H05-2018,0,0.0556596,"rsing We start by parsing the thread to identify posts, participants, and the reply structure of the thread (i.e. who replies to whom). In the datasets described in Section 2.4, all this information was explicitly available in the thread. We tokenize the text of each post and split it into sentences using CLAIRLib (AbuJbara and Radev, 2011). 3.2 Opinion Word Identification The next step is to identify the words that express opinion and determine their polarity (positive or negative). Lehrer (1974) defines word polarity as the direction the word deviates to from the norm. We use OpinionFinder (Wilson et al., 2005a) to identify polarized words and their polarities. The polarity of a word is usally affected by the context in which it appears. For example, the word fine is positive when used as an adjective and negative when used as a noun. For another example, a positive word that appears in a negated context becomes negative. OpinionFinder uses a large set of features to identify the contextual polarity of a given polarized word given its isolated polarity and the sentence in which it appears (Wilson et al., 2005b). Snippet (3) below shows the result of applying this step to snippet (1) above (O means"
P12-1042,H05-1044,0,0.0436636,"rsing We start by parsing the thread to identify posts, participants, and the reply structure of the thread (i.e. who replies to whom). In the datasets described in Section 2.4, all this information was explicitly available in the thread. We tokenize the text of each post and split it into sentences using CLAIRLib (AbuJbara and Radev, 2011). 3.2 Opinion Word Identification The next step is to identify the words that express opinion and determine their polarity (positive or negative). Lehrer (1974) defines word polarity as the direction the word deviates to from the norm. We use OpinionFinder (Wilson et al., 2005a) to identify polarized words and their polarities. The polarity of a word is usally affected by the context in which it appears. For example, the word fine is positive when used as an adjective and negative when used as a noun. For another example, a positive word that appears in a negated context becomes negative. OpinionFinder uses a large set of features to identify the contextual polarity of a given polarized word given its isolated polarity and the sentence in which it appears (Wilson et al., 2005b). Snippet (3) below shows the result of applying this step to snippet (1) above (O means"
P12-1042,D10-1102,0,0.011553,"ovy (2007) predict the results of an election by analyzing discussion threads in online forums that discuss the elections. They use a supervised approach that uses unigrams, bigrams, and trigrams as features. In contrast, our work is unsupervised and uses different types information. Moreover, although this work is related to ours at the goal level, it does not involve any opinion analysis. Another related work classifies the speakers side in a corpus of congressional floor debates, using the speakers final vote on the bill as a labeling for side (Thomas et al., 2006; Bansal et al., 2008; 401 Yessenalina et al., 2010). This work infers agreement between speakers based on cases where one speaker mentions another by name, and a simple algorithm for determining the polarity of the sentence in which the mention occurs. This work shows that even with the resulting sparsely connected agreement structure, the MinCut algorithm can improve over stance classification based on textual information alone. This work also requires that the debate sides be known by the algorithm and it only identifies discussant-to-discussant attitude. In our experiments below we show that identifying both discussant-to-discussant and dis"
P12-1042,W03-1017,0,0.09027,"onclude in Section 5 2 Related Work mine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment analysis. Previous work has studie"
P12-1042,H05-2017,0,\N,Missing
P12-1091,C04-1051,0,0.139337,"are unknown, hence R is not complete. In the RS setting, they are interested in predicting individual ratings, while we are interested in the sentence 1 An efficient way to compute equation 4 is proposed in (Steck, 2010). 4 Evaluation for SS We need to show the impact of our proposed model WTMF on the SS task. However we are faced with a problem, the lack of a suitable large evaluation set from which we can derive robust observations. The two data sets we know of for SS are: 1. human-rated sentence pair similarity data set (Li et al., 2006) [LI06]; 2. the Microsoft Research Paraphrase Corpus (Dolan et al., 2004) [MSR04]. The LI06 data set consists of 65 pairs of noun definitions selected from the Collin Cobuild Dictionary. A subset of 30 pairs is further selected by LI06 to render the similarity scores evenly distributed. While this is the ideal data set for SS, the small size makes it impossible for tuning SS algorithms or deriving significant performance conclusions. On the other hand, the MSR04 data set comprises a much larger set of sentence pairs: 4,076 training and 1,725 test pairs. The ratings on the pairs are binary labels: similar/not similar. This is not a problem per se, however the issue"
P12-1091,N06-2015,0,0.0149933,"initions but on a large scale. 4.1 Concept Definition Retrieval We define a new framework for evaluating SS and project it as a Concept Definition Retrieval (CDR) task where the data points are dictionary definitions. The intuition is that two definitions in different dic2 http://www.grouplens.org/node/73, with 1M data set being the most widely used. tionaries referring to the same concept should be assigned large similarity. In this setting, we design the CDR task in a search engine style. The SS algorithm has access to all the definitions in WordNet (WN). Given an OntoNotes (ON) definition (Hovy et al., 2006), the SS algorithm should rank the equivalent WN definition as high as possible based on sentence similarity. The manual mapping already exists for ON to WN. One ON definition can be mapped to several WN definitions. After preprocessing we obtain 13669 ON definitions mapped to 19655 WN definitions. The data set has the advantage of being very large and it doesn’t require further human scrutiny. After the SS model learns the co-occurrence of words from WN definitions, in the testing phase, given an ON definition d, the SS algorithm needs to identify the equivalent WN definitions by computing th"
P12-1091,N06-1058,0,0.0319368,"on Retrieval. The new framework allows for large scale tuning and testing of Sentence Similarity models. Experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models. Our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is at the core of many NLP applications that focus on sentence level semantics such as Machine Translation (Kauchak and Barzilay, 2006), Summarization (Zhou et al., 2006), Text Coherence Detection (Lapata and Barzilay, 2005), etc.To date, almost all Sentence Similarity [SS] approaches work in the high-dimensional word space and rely mainly on word similarity. There are two main (not unrelated) disadvantages to word similarity based approaches: 1. lexical ambiguity as the pairwise word similarity ignores the semantic interaction between the word and its sentential context; Mona Diab Center for Computational Learning Systems, Columbia University, mdiab@ccls.columbia.edu 2. word co-occurrence information is not sufficiently expl"
P12-1091,N06-1057,0,0.00971178,"arge scale tuning and testing of Sentence Similarity models. Experiments on the new task and previous data sets show significant improvement of our model over baselines and other traditional latent variable models. Our results indicate comparable and even better performance than current state of the art systems addressing the problem of sentence similarity. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is at the core of many NLP applications that focus on sentence level semantics such as Machine Translation (Kauchak and Barzilay, 2006), Summarization (Zhou et al., 2006), Text Coherence Detection (Lapata and Barzilay, 2005), etc.To date, almost all Sentence Similarity [SS] approaches work in the high-dimensional word space and rely mainly on word similarity. There are two main (not unrelated) disadvantages to word similarity based approaches: 1. lexical ambiguity as the pairwise word similarity ignores the semantic interaction between the word and its sentential context; Mona Diab Center for Computational Learning Systems, Columbia University, mdiab@ccls.columbia.edu 2. word co-occurrence information is not sufficiently exploited. Latent variable models, such"
P12-1091,C10-2048,0,\N,Missing
P12-2013,P12-1042,1,0.761334,"oblem in that we aim at identifying multiple subgroups. 3 Approach We tackle the problem using Vector Space Modeling techniques to represent the discussion threads. Each vector represents a discussant in the thread creating an Attitude Profile (AP). We use a clustering algorithm to partition the vector space of APs into multiple sub-groups. The idea is that resulting clusters would comprise sub-groups of discussants with 66 similar attitudes. 3.1 Basic Features We use two basic features, namely Negative and Positive sentiment towards specific discussants and entities like in the work done by (Abu-Jbara et al., 2012). We start off by determining sentences that express attitude in the thread, attitude sentences (AS). We use OpinionFinder (Wilson et al., 2005) which employs negative and positive polarity cues. For determining discussant sentiment, we need to first identify who the target of their sentiment is: another discussant, or an entity, where an entity could be a topic or a person not participating in the discussion. Sentiment toward another discussant: This is quite challenging since explicit sentiment expressed in a post is not necessarily directed towards another discussant to whom it is a reply."
P12-2013,P05-1045,0,0.00732939,"iscussant. However as a simplifying assumption, similar to the work of (Hassan et al., 2010), we adopt the view that replies in the sentences that are determined to be attitudinal and contain secondperson pronouns (you, your, yourself) are assumed to be directed towards the recipients of the replies. Sentiment toward an entity: We again adopt a simplifying view by modeling all the named entities in a sentence without heeding the roles these entities play, i.e. whether they are targets or not. Accordingly, we extract all the named entities in a sentence using Stanford’s Name Entity Recognizer (Finkel et al., 2005). We only focus on Person and Organization named entities. 3.2 Extracting Implicit Attitudes We define implicit attitudes as the semantic similarity between texts comprising discussant utterances or posts in a thread. We cannot find enough overlapping words between posts, since some posts are very short. Hence we apply LDA (Blei et al., 2003) on texts to extract latent semantics of texts. We split text into sentences, i.e., each sentence is treated as a single document. Accordingly, each sentence is represented as a K-dimension vector. By computing the similarity on these vectors, we obtain a"
P12-2013,C08-1031,0,0.0770835,"tude yields better results. This may be due to the fact that in informal data, strong subjective opinions about entities/events or towards other discussants are expressed more explicitly. This is generally not the case in the formal genre where ideas do not have as much sentiment associated with them, and hence the opinions are more “implicit”. Finally, we observe that combining both kinds of features improves performance of our systems for both genres. 2 Related Work Substantial research exists in the fields of Opinion Identification and Community Mining that is related to our current work. (Ganapathibhotla and Liu, 2008) deal with the problem of finding opinions from comparative sentences. Many previous research efforts related to Opinion Target Identification (Hu and Liu, 2004; Kobayashi et al., 2007; Jakob and Gurevych, 2010), focus on the domain of product reviews where they exploit the genre in multiple ways. Somasundaran and Wiebe (2009) used unsupervised methods to identify stances in online debates. They mine the web to find associations indicative of opinions and combine them with discourse information. Their problem essentially deals with the debate genre and finding the stance of an individual given"
P12-2013,D10-1121,0,0.0352859,"s. For determining discussant sentiment, we need to first identify who the target of their sentiment is: another discussant, or an entity, where an entity could be a topic or a person not participating in the discussion. Sentiment toward another discussant: This is quite challenging since explicit sentiment expressed in a post is not necessarily directed towards another discussant to whom it is a reply. It is possible that a discussant may be replying to another poster but expressing an attitude towards a third entity or discussant. However as a simplifying assumption, similar to the work of (Hassan et al., 2010), we adopt the view that replies in the sentences that are determined to be attitudinal and contain secondperson pronouns (you, your, yourself) are assumed to be directed towards the recipients of the replies. Sentiment toward an entity: We again adopt a simplifying view by modeling all the named entities in a sentence without heeding the roles these entities play, i.e. whether they are targets or not. Accordingly, we extract all the named entities in a sentence using Stanford’s Name Entity Recognizer (Finkel et al., 2005). We only focus on Person and Organization named entities. 3.2 Extractin"
P12-2013,P10-2049,0,0.0213359,"se in the formal genre where ideas do not have as much sentiment associated with them, and hence the opinions are more “implicit”. Finally, we observe that combining both kinds of features improves performance of our systems for both genres. 2 Related Work Substantial research exists in the fields of Opinion Identification and Community Mining that is related to our current work. (Ganapathibhotla and Liu, 2008) deal with the problem of finding opinions from comparative sentences. Many previous research efforts related to Opinion Target Identification (Hu and Liu, 2004; Kobayashi et al., 2007; Jakob and Gurevych, 2010), focus on the domain of product reviews where they exploit the genre in multiple ways. Somasundaran and Wiebe (2009) used unsupervised methods to identify stances in online debates. They mine the web to find associations indicative of opinions and combine them with discourse information. Their problem essentially deals with the debate genre and finding the stance of an individual given two options. Ours is a more general problem since we deal with discussion data in general and not debates on specific topics. Hence our aim is to identify multiple groups, not just two. In terms of Sentiment An"
P12-2013,D07-1114,0,0.0144185,"is generally not the case in the formal genre where ideas do not have as much sentiment associated with them, and hence the opinions are more “implicit”. Finally, we observe that combining both kinds of features improves performance of our systems for both genres. 2 Related Work Substantial research exists in the fields of Opinion Identification and Community Mining that is related to our current work. (Ganapathibhotla and Liu, 2008) deal with the problem of finding opinions from comparative sentences. Many previous research efforts related to Opinion Target Identification (Hu and Liu, 2004; Kobayashi et al., 2007; Jakob and Gurevych, 2010), focus on the domain of product reviews where they exploit the genre in multiple ways. Somasundaran and Wiebe (2009) used unsupervised methods to identify stances in online debates. They mine the web to find associations indicative of opinions and combine them with discourse information. Their problem essentially deals with the debate genre and finding the stance of an individual given two options. Ours is a more general problem since we deal with discussion data in general and not debates on specific topics. Hence our aim is to identify multiple groups, not just tw"
P12-2013,P09-1026,0,0.161145,"more “implicit”. Finally, we observe that combining both kinds of features improves performance of our systems for both genres. 2 Related Work Substantial research exists in the fields of Opinion Identification and Community Mining that is related to our current work. (Ganapathibhotla and Liu, 2008) deal with the problem of finding opinions from comparative sentences. Many previous research efforts related to Opinion Target Identification (Hu and Liu, 2004; Kobayashi et al., 2007; Jakob and Gurevych, 2010), focus on the domain of product reviews where they exploit the genre in multiple ways. Somasundaran and Wiebe (2009) used unsupervised methods to identify stances in online debates. They mine the web to find associations indicative of opinions and combine them with discourse information. Their problem essentially deals with the debate genre and finding the stance of an individual given two options. Ours is a more general problem since we deal with discussion data in general and not debates on specific topics. Hence our aim is to identify multiple groups, not just two. In terms of Sentiment Analysis, the work done by Hassan et al.(2010) in using part-of-speech and dependency structures to identify polarities"
P12-2013,H05-2018,0,0.156184,"Missing"
P12-2028,E09-1005,0,0.0444634,"Missing"
P12-2028,P06-1013,0,0.022249,"a gene called p53 could transform normal cells into cancerous ones... elesk returns the wrong sense computer device, due to the sparsity of overlapping words between definitions of animal mouse and the context words. wmfvec chooses the correct sense animal mouse, by recognizing the biology element of animal mouse and related context words gene, cell, cancerous. 5 Related Work Sense similarity measures have been the core component in many unsupervised WSD systems and lexical semantics research/applications. To date, elesk is the most popular such measure (McCarthy et al., 2004; Mihalcea, 2005; Brody et al., 2006). Sometimes people use jcn to obtain similarity of noun-noun and verb-verb pairs (Sinha and Mihalcea, 2007; Guo and Diab, 2010). Our similarity measure wmfvec exploits the same information (sense definitions) elesk and ldavec use, and outperforms them significantly on four standardized data sets. To our best knowledge, we are the first to construct a sense similarity by latent semantics of sense definitions. 6 Conclusions We construct a sense similarity wmfvec from the latent semantics of sense definitions. Experiment results show wmfvec significantly outperforms previous definition-based simi"
P12-2028,D07-1108,0,0.0459077,"Missing"
P12-2028,P10-1156,1,0.928521,"o not link the senses across dictionaries, hence Wik is only used as augmented data for WMF to better learn the semantics of words. All data is tokenized, POS tagged (Toutanova et al., 2003) and lemmatized, resulting in 341,557 sense definitions and 3,563,649 words. WSD Algorithm: To perform WSD we need two components: (1) a sense similarity measure that returns a similarity score given two senses; (2) a disambiguation algorithm that determines which senses to choose as final answers based on the sense pair similarity scores. We choose the Indegree algorithm used in (Sinha and Mihalcea, 2007; Guo and Diab, 2010) as our disambiguation algorithm. It is a graphbased algorithm, where nodes are senses, and edge weight equals to the sense pair similarity. The final answer is chosen as the sense with maximum indegree. Using the Indegree algorithm allows us to easily replace the sense similarity with wmfvec. In Indegree, two senses are connected if their words are within a local window. We use the optimal window size of 6 tested in (Sinha and Mihalcea, 2007; Guo and Diab, 2010). Baselines: We compare with (1) elesk, the most widely used sense similarity. We use the implementation in (Pedersen et al., 2004)."
P12-2028,D11-1051,1,0.909907,"Missing"
P12-2028,P12-1091,1,0.64044,"lies in that sense definitions are typically too short/sparse for latent variable models to learn accurate semantics, since these models are designed for long documents. For example, topic models such as LDA (Blei et al., 2003), can only find the dominant topic based on the observed words in a definition (f inancial topic in bank#n#1 and stock#n#1) without further discernibility. In this case, many senses will share the same latent semantics profile, as long as they are in the same topic/domain. To solve the sparsity issue we use missing words as negative evidence of latent semantics, as in (Guo and Diab, 2012). We define missing words of a sense definition as the whole vocabulary in a corpus minus the observed words in the sense definition. Since observed words in definitions are too few to reveal the semantics of senses, missing words can be used to tell the model what the definition is not about. Therefore, we want to find a latent semantics profile that is related to observed words in a definition, but also not related to missing words, so that the induced latent semantics is unique for the sense. Finally we also show how to use WN neighbor sense definitions to construct a nuanced sense similari"
P12-2028,P10-1116,0,0.0346657,"Missing"
P12-2028,P04-1036,0,0.0352306,"ext: ... in experiments with mice that a gene called p53 could transform normal cells into cancerous ones... elesk returns the wrong sense computer device, due to the sparsity of overlapping words between definitions of animal mouse and the context words. wmfvec chooses the correct sense animal mouse, by recognizing the biology element of animal mouse and related context words gene, cell, cancerous. 5 Related Work Sense similarity measures have been the core component in many unsupervised WSD systems and lexical semantics research/applications. To date, elesk is the most popular such measure (McCarthy et al., 2004; Mihalcea, 2005; Brody et al., 2006). Sometimes people use jcn to obtain similarity of noun-noun and verb-verb pairs (Sinha and Mihalcea, 2007; Guo and Diab, 2010). Our similarity measure wmfvec exploits the same information (sense definitions) elesk and ldavec use, and outperforms them significantly on four standardized data sets. To our best knowledge, we are the first to construct a sense similarity by latent semantics of sense definitions. 6 Conclusions We construct a sense similarity wmfvec from the latent semantics of sense definitions. Experiment results show wmfvec significantly outpe"
P12-2028,H05-1052,0,0.0254271,"with mice that a gene called p53 could transform normal cells into cancerous ones... elesk returns the wrong sense computer device, due to the sparsity of overlapping words between definitions of animal mouse and the context words. wmfvec chooses the correct sense animal mouse, by recognizing the biology element of animal mouse and related context words gene, cell, cancerous. 5 Related Work Sense similarity measures have been the core component in many unsupervised WSD systems and lexical semantics research/applications. To date, elesk is the most popular such measure (McCarthy et al., 2004; Mihalcea, 2005; Brody et al., 2006). Sometimes people use jcn to obtain similarity of noun-noun and verb-verb pairs (Sinha and Mihalcea, 2007; Guo and Diab, 2010). Our similarity measure wmfvec exploits the same information (sense definitions) elesk and ldavec use, and outperforms them significantly on four standardized data sets. To our best knowledge, we are the first to construct a sense similarity by latent semantics of sense definitions. 6 Conclusions We construct a sense similarity wmfvec from the latent semantics of sense definitions. Experiment results show wmfvec significantly outperforms previous"
P12-2028,N04-3012,0,0.126523,"007; Guo and Diab, 2010) as our disambiguation algorithm. It is a graphbased algorithm, where nodes are senses, and edge weight equals to the sense pair similarity. The final answer is chosen as the sense with maximum indegree. Using the Indegree algorithm allows us to easily replace the sense similarity with wmfvec. In Indegree, two senses are connected if their words are within a local window. We use the optimal window size of 6 tested in (Sinha and Mihalcea, 2007; Guo and Diab, 2010). Baselines: We compare with (1) elesk, the most widely used sense similarity. We use the implementation in (Pedersen et al., 2004). We believe WMF is a better approach to model latent semantics than LDA, hence the second baseline (2) LDA using Gibbs sampling (Griffiths and Steyvers, 2004). However, we cannot directly use estimated topic distribution P (z|d) to represent the definition since it only has non-zero values on one or two topics. Instead, we calculate the latent vecModel random elesk ldavec wmfvec jcn+elesk jcn+wmfvec SE3 random elesk ldavec wmfvec jcn+elesk jcn+wmfvec SE07 random elesk ldavec wmfvec jcn+elesk jcn+wmfvec Semcor random elesk ldavec wmfvec jcn+elesk jcn+wmfvec Noun 43.9 63.5 68.6 69.7 69.3 70.8 3"
P12-2028,N03-1033,0,0.00480448,"b) in documents. The data sets we use are all-words tasks in SENSEVAL2 [SE2], SENSEVAL3 [SE3], SEMEVAL-2007 [SE07], and Semcor. We tune the parameters in wmfvec and other baselines based on SE2, and then directly apply the tuned models on other three data sets. Data: The sense inventory is WN3.0 for the four WSD data sets. WMF and LDA are built on the corpus of sense definitions of two dictionaries: WN and Wiktionary [Wik].2 We do not link the senses across dictionaries, hence Wik is only used as augmented data for WMF to better learn the semantics of words. All data is tokenized, POS tagged (Toutanova et al., 2003) and lemmatized, resulting in 341,557 sense definitions and 3,563,649 words. WSD Algorithm: To perform WSD we need two components: (1) a sense similarity measure that returns a similarity score given two senses; (2) a disambiguation algorithm that determines which senses to choose as final answers based on the sense pair similarity scores. We choose the Indegree algorithm used in (Sinha and Mihalcea, 2007; Guo and Diab, 2010) as our disambiguation algorithm. It is a graphbased algorithm, where nodes are senses, and edge weight equals to the sense pair similarity. The final answer is chosen as"
P13-1024,S12-1051,1,0.377412,"Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics believe many NLP tasks will benefit from this task. In fact, in the topic modeling research, previous work (Jin et al., 2011) already showed that by incorporating webpages whose urls are contained in tweets, the tweet clustering purity score was boosted from 0.280 to 0.392. Given the few number of words in a tweet (14 words on average in our dataset), the traditional high dimensional surface word matching is lossy and fails to pinpoint the news article. This constitutes a classic short text semantics impediment (Agirre et al., 2012). Latent variable models are powerful by going beyond the surface word level and mapping short texts into a low dimensional dense vector (Socher et al., 2011; Guo and Diab, 2012b). Accordingly, we apply a latent variable model, namely, the Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b; Guo and Diab, 2012c) to both the tweets and the news articles. WTMF is a state-of-the-art unsupervised model that was tested on two short text similarity datasets: (Li et al., 2006) and (Agirre et al., 2012), which outperforms Latent Semantic Analysis [LSA] (Landauer et al., 1998) and Latent"
P13-1024,P11-1040,0,0.0373227,"vated by the observation that a tweet usually only covers one aspect of an event. We show that using tweet specific feature (hashtag) and news specific feature (named entities) as well as temporal constraints, we are able to extract text-to-text correlations, and thus completes the semantic picture of a short text. Our experiments show significant improvement of our new model over baselines with three evaluation metrics in the new task. 1 Pray for Mali... A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al., 2011), and thus fails to shed light on the users focus/interests. To enable the NLP tools to better understand Twitter feeds, we propose the task of linking a tweet to a news article that is relevant to the tweet, thereby augmenting the context of the tweet. For example, we want to supplement the implicit context of the above tweet with a news article such as the following entitled: State of emergency declared in Mali where abundant evidence can be fed into an offthe-shelf event extraction/discovery system. To create a gold standard dataset, we download tweets spanning over 18 days, each with a url"
P13-1024,C10-1034,0,0.0180773,"Missing"
P13-1024,P12-2028,1,0.697599,"ious work (Jin et al., 2011) already showed that by incorporating webpages whose urls are contained in tweets, the tweet clustering purity score was boosted from 0.280 to 0.392. Given the few number of words in a tweet (14 words on average in our dataset), the traditional high dimensional surface word matching is lossy and fails to pinpoint the news article. This constitutes a classic short text semantics impediment (Agirre et al., 2012). Latent variable models are powerful by going beyond the surface word level and mapping short texts into a low dimensional dense vector (Socher et al., 2011; Guo and Diab, 2012b). Accordingly, we apply a latent variable model, namely, the Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b; Guo and Diab, 2012c) to both the tweets and the news articles. WTMF is a state-of-the-art unsupervised model that was tested on two short text similarity datasets: (Li et al., 2006) and (Agirre et al., 2012), which outperforms Latent Semantic Analysis [LSA] (Landauer et al., 1998) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) by a large margin. We employ it as a strong baseline in this task as it exploits and effectively models the missing words in a tw"
P13-1024,P12-1091,1,0.822339,"ious work (Jin et al., 2011) already showed that by incorporating webpages whose urls are contained in tweets, the tweet clustering purity score was boosted from 0.280 to 0.392. Given the few number of words in a tweet (14 words on average in our dataset), the traditional high dimensional surface word matching is lossy and fails to pinpoint the news article. This constitutes a classic short text semantics impediment (Agirre et al., 2012). Latent variable models are powerful by going beyond the surface word level and mapping short texts into a low dimensional dense vector (Socher et al., 2011; Guo and Diab, 2012b). Accordingly, we apply a latent variable model, namely, the Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b; Guo and Diab, 2012c) to both the tweets and the news articles. WTMF is a state-of-the-art unsupervised model that was tested on two short text similarity datasets: (Li et al., 2006) and (Agirre et al., 2012), which outperforms Latent Semantic Analysis [LSA] (Landauer et al., 1998) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) by a large margin. We employ it as a strong baseline in this task as it exploits and effectively models the missing words in a tw"
P13-1024,S12-1086,1,0.625346,"ious work (Jin et al., 2011) already showed that by incorporating webpages whose urls are contained in tweets, the tweet clustering purity score was boosted from 0.280 to 0.392. Given the few number of words in a tweet (14 words on average in our dataset), the traditional high dimensional surface word matching is lossy and fails to pinpoint the news article. This constitutes a classic short text semantics impediment (Agirre et al., 2012). Latent variable models are powerful by going beyond the surface word level and mapping short texts into a low dimensional dense vector (Socher et al., 2011; Guo and Diab, 2012b). Accordingly, we apply a latent variable model, namely, the Weighted Textual Matrix Factorization [WTMF] (Guo and Diab, 2012b; Guo and Diab, 2012c) to both the tweets and the news articles. WTMF is a state-of-the-art unsupervised model that was tested on two short text similarity datasets: (Li et al., 2006) and (Agirre et al., 2012), which outperforms Latent Semantic Analysis [LSA] (Landauer et al., 1998) and Latent Dirichelet Allocation [LDA] (Blei et al., 2003) by a large margin. We employ it as a strong baseline in this task as it exploits and effectively models the missing words in a tw"
P13-1024,N13-1089,1,0.859083,"Missing"
P13-1024,P04-1035,0,0.00450094,"Missing"
P13-1024,P05-1015,0,0.0718262,"Missing"
P13-1024,D09-1026,0,0.0355149,"on cannot be efficiently exploited. Guo and Diab (2012b; 2012a; 2013) show the superiority of the latent space approach in the WTMF model achieving state-of-the-art performance on two datasets. However, all of them only reply on text-to-word information. In this paper, we focus on modeling inter-text relations induced by Twitter/news features. We extend the WTMF model and adapt it into tweets modeling, achieving significantly better results. Modeling Tweets in a Latent Space: Ramage et al. (2010) also use hashtags to improve the latent representation of tweets in a LDA framework, Labeled-LDA (Ramage et al., 2009), treating each hashtag as a label. Similar to the experiments presented in this paper, the result of using LabeledLDA alone is worse than the IR model, due to the sparseness in the induced LDA latent vector. Jin et al. (2011) apply an LDA based model on clustering by incorporating url referred documents. The semantics of long documents are transferred to the topic distribution of tweets. News recommendation: A news recommendation system aims to recommend news articles to a user based on the features (e.g., key words, tags, category) in the documents that the user likes (hence these documents"
P13-1024,P08-1030,1,0.806816,"le model that models the inter short text correlations (text-to-text information). This is motivated by the observation that a tweet usually only covers one aspect of an event. We show that using tweet specific feature (hashtag) and news specific feature (named entities) as well as temporal constraints, we are able to extract text-to-text correlations, and thus completes the semantic picture of a short text. Our experiments show significant improvement of our new model over baselines with three evaluation metrics in the new task. 1 Pray for Mali... A typical event extraction/discovery system (Ji and Grishman, 2008) fails to discover the war event due to the lack of context information (Benson et al., 2011), and thus fails to shed light on the users focus/interests. To enable the NLP tools to better understand Twitter feeds, we propose the task of linking a tweet to a news article that is relevant to the tweet, thereby augmenting the context of the tweet. For example, we want to supplement the implicit context of the above tweet with a news article such as the following entitled: State of emergency declared in Mali where abundant evidence can be fed into an offthe-shelf event extraction/discovery system."
P13-1024,P11-1016,0,0.0154431,"State of emergency declared in Mali where abundant evidence can be fed into an offthe-shelf event extraction/discovery system. To create a gold standard dataset, we download tweets spanning over 18 days, each with a url linking to a news article of CNN or NYTIMES, as well as all the news of CNN and NYTIMES published during the period. The goal is to predict the url referred news article based on the text in each tweet.1 We Introduction Recently there has been an increasing interest in language understanding of Twitter messages. Researchers (Speriosui et al., 2011; Brody and Diakopoulos, 2011; Jiang et al., 2011) were in1 The data and code is publicly available at www.cs. 239 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 239–249, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics believe many NLP tasks will benefit from this task. In fact, in the topic modeling research, previous work (Jin et al., 2011) already showed that by incorporating webpages whose urls are contained in tweets, the tweet clustering purity score was boosted from 0.280 to 0.392. Given the few number of words in a tweet (14 words on average in our data"
P13-1024,W11-2207,0,0.0239471,"with a news article such as the following entitled: State of emergency declared in Mali where abundant evidence can be fed into an offthe-shelf event extraction/discovery system. To create a gold standard dataset, we download tweets spanning over 18 days, each with a url linking to a news article of CNN or NYTIMES, as well as all the news of CNN and NYTIMES published during the period. The goal is to predict the url referred news article based on the text in each tweet.1 We Introduction Recently there has been an increasing interest in language understanding of Twitter messages. Researchers (Speriosui et al., 2011; Brody and Diakopoulos, 2011; Jiang et al., 2011) were in1 The data and code is publicly available at www.cs. 239 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 239–249, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics believe many NLP tasks will benefit from this task. In fact, in the topic modeling research, previous work (Jin et al., 2011) already showed that by incorporating webpages whose urls are contained in tweets, the tweet clustering purity score was boosted from 0.280 to 0.392. Given the few number of"
P13-1024,Y12-1013,1,0.865303,"Missing"
P13-1024,P11-1015,0,0.108427,"Missing"
P13-1024,N03-1033,0,0.0334268,"uring automatic NER accumulates much faster given the large number of named entities in news data. Therefore we only extract temporal relations for news articles. 2 Note that there are some false positive named entities detected such as apple. We plan to address removing noisy named entities and hashtags in future work 242 5 WTMF on Graphs treated as a document), sense definitions of Wiktionary and Wordnet (Fellbaum, 1998). The tweets and news articles are also included in the corpus, generating 441,258 short texts and 5,149,122 words. The data is tokenized, POS-tagged by Stanford POS tagger (Toutanova et al., 2003), and lemmatized by WordNet::QueryData.pm. The value of each word in matrix X is its TF-IDF value in the short text. Baselines: We present 4 baselines: 1. Information Retrieval model [IR], which simply treats a tweet as a document, and performs traditional surface word matching. 2. LDA-θ with Gibbs Sampling as inference method. We use the inferred topic distribution θ as a latent vector to represent the tweet/news. 3. LDA-wvec. The problem with LDA-θ is the inferred topic distribution latent vector is very sparse with only a few non-zero values, resulting in many tweet/news pairs receiving a h"
P13-1024,P12-1054,0,0.0172256,"imilar than those are not chronologically close (Wang and McCallum, 2006). However, we cannot simply assume any two tweets are similar only based on the timestamp. Therefore, for a tweet we link it to the k most similar tweets whose published time is within 24 hours of the target tweet’s timestamp. We use the similarity score returned by WTMF model to measure the similarity of two tweets. We experimented with other features such as authorship. We note that it was not a helpful feature. While authorship information helps in the task of news/tweets recommendation for a user (Corso et al., 2005; Yan et al., 2012), the authorship information is too general for this task where we target on “recommending” a news article for a tweet. where λ is a regularization term. 4 Creating Text-to-text Relations via Twitter/News Features WTMF exploits the text-to-word information in a very nuanced way, while the dependency between texts is ignored. In this Section, we introduce how to create text-to-text relations. 4.1 Temporal Relations Hashtags and Named Entities Hashtags highlight the topics in tweets, e.g., The #flu season has started. We believe two tweets sharing the same hashtag should be related, hence we pla"
P13-1024,P11-1037,0,\N,Missing
P13-1024,D11-1052,0,\N,Missing
P13-1024,C12-1076,1,\N,Missing
P13-2081,W09-0807,0,0.0289315,"these dialects did not originally exist in a written form, they are pervasively present in social media text (normally mixed with MSA) nowadays. DA does not have a standard orthography leading to many spelling variations and inconsistencies. Linguistic Code switching (LCS) between MSA and DA happens both intra-sententially and intersententially. LCS in Arabic poses a serious challenge for almost all NLP tasks since MSA and DA Related Work Dialect Identification in Arabic is crucial for almost all NLP tasks, yet most of the research in Arabic NLP, with few exceptions, is targeted towards MSA. Biadsy et al. (2009) present a system that identifies dialectal words in speech and their dialect of origin through the acoustic signals. Salloum and Habash (2011) tackle the problem of DA to English Machine Translation (MT) by pivoting through MSA. The authors present a system that applies transfer rules from DA to MSA then uses state of the art MSA to English MT system. Habash et al. (2012) present CODA, a Conventional Orthography for Dialectal Arabic that aims to standardize the orthography of all the variants of DA while Dasigi and Diab (2011) present an unsupervised clustering approach to identify orthograph"
P13-2081,I11-1036,1,0.828186,"esearch in Arabic NLP, with few exceptions, is targeted towards MSA. Biadsy et al. (2009) present a system that identifies dialectal words in speech and their dialect of origin through the acoustic signals. Salloum and Habash (2011) tackle the problem of DA to English Machine Translation (MT) by pivoting through MSA. The authors present a system that applies transfer rules from DA to MSA then uses state of the art MSA to English MT system. Habash et al. (2012) present CODA, a Conventional Orthography for Dialectal Arabic that aims to standardize the orthography of all the variants of DA while Dasigi and Diab (2011) present an unsupervised clustering approach to identify orthographic variants in DA. Zaidan and CallisonBurch (2011) crawl a large dataset of MSA-DA news’ commentaries. The authors annotate part of the dataset for sentence-level dialectalness on 456 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 456–461, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Amazon Mechanical Turk and try a language modeling (LM) approach to solve the problem. In Elfardy and Diab (2012a), we present a set of guidelines for token-level"
P13-2081,C12-2029,1,0.867263,"e the orthography of all the variants of DA while Dasigi and Diab (2011) present an unsupervised clustering approach to identify orthographic variants in DA. Zaidan and CallisonBurch (2011) crawl a large dataset of MSA-DA news’ commentaries. The authors annotate part of the dataset for sentence-level dialectalness on 456 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 456–461, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Amazon Mechanical Turk and try a language modeling (LM) approach to solve the problem. In Elfardy and Diab (2012a), we present a set of guidelines for token-level identification of dialectalness while in (Elfardy and Diab, 2012b), (Elfardy et al., 2013) we tackle the problem of tokenlevel dialect-identification by casting it as a codeswitching problem. 3 a standardized form, eg. the elongated form of the word Q J» ktyr1 ‘a lot’ which could be rendered in the text as Q J J J JJJJ» kttttyyyyr is reduced to Q  J  J J  J  J » ktttyyyr (specifically three repeated letters instead of an unpredictable number of repetitions, to maintain the signal that there is a speech effect which could b"
P13-2081,N13-1066,0,0.0100066,"effect which could be a DA indicator). ex. AJJ Ê« Q J»ð Ð@Qk èY» kdh HrAm wktyr ElynA Approach to Sentence-Level Dialect Identification We present a supervised system that uses a Naive Bayes classifier trained on gold labeled data with sentence level binary decisions of either being MSA or DA. 2. Orthography Normalized (CODAfied) LM: since DA is not originally a written form of Arabic, no standard orthography exists for it. Habash et al. (2012) attempt to solve this problem by presenting CODA, a conventional orthography for writing DA. We use the implementation of CODA presented in CODAfy (Eskander et al., 2013), to build an orthography-normalized LM. While CODA and its applied version using CODAfy solve the spelling inconsistency problem in DA, special care must be taken when using it for our task since it removes valuable dialectalness cues. For example, the  (v in Buckwalter (BW) Transliteraletter H  (t in BW) tion) is converted into the letter H in a DA context. CODA suggests that such cases get mapped to the original MSA phonological variant which might make the dialect identification problem more challenging. On the other hand, CODA solves the sparseness issue by mapping multiple spelling-var"
P13-2081,habash-etal-2012-conventional,1,0.523815,"ous challenge for almost all NLP tasks since MSA and DA Related Work Dialect Identification in Arabic is crucial for almost all NLP tasks, yet most of the research in Arabic NLP, with few exceptions, is targeted towards MSA. Biadsy et al. (2009) present a system that identifies dialectal words in speech and their dialect of origin through the acoustic signals. Salloum and Habash (2011) tackle the problem of DA to English Machine Translation (MT) by pivoting through MSA. The authors present a system that applies transfer rules from DA to MSA then uses state of the art MSA to English MT system. Habash et al. (2012) present CODA, a Conventional Orthography for Dialectal Arabic that aims to standardize the orthography of all the variants of DA while Dasigi and Diab (2011) present an unsupervised clustering approach to identify orthographic variants in DA. Zaidan and CallisonBurch (2011) crawl a large dataset of MSA-DA news’ commentaries. The authors annotate part of the dataset for sentence-level dialectalness on 456 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 456–461, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Amaz"
P13-2081,N13-1044,0,0.0517584,"en used as features for the proposed model. The following variants of the underlying token-level system are built to assess the effect of varying the level of preprocessing on the underlying LM on the performance of the overall sentence level dialect identification process: (1) Surface, (2) Tokenized, (3) CODAfied, and (4) Tokenized-CODA. We use the following sentence to show the different techniques: AJJ Ê« Q J»ð Ð@Qk èY» kdh HrAm wktyr ElynA 3. Tokenized LM: D3 tokenization-scheme is applied to all data using MADA (Habash et al., 2009) (an MSA Tokenizer) for the MSA corpora, and MADA-ARZ (Habash et al., 2013) (an EDA tokenizer) for the EDA corpora. For building the tokenized LM, we maintain clitics and lexemes. Some clitics are unique to MSA while others are unique to EDA so maintaining them in the LM is helpful, eg. the negation enclitic  $ is only used in EDA but it could be seen with an MSA/EDA homograph, maintaining the enclitic in the LM facilitates the identification 1. Surface LMs: No significant preprocessing is applied apart from the regular initial clean up of the text which includes removal of URLs, normalization of speech effects such as reducing all redundant letters in a word to 1"
P13-2081,W11-2602,0,0.140264,"wadays. DA does not have a standard orthography leading to many spelling variations and inconsistencies. Linguistic Code switching (LCS) between MSA and DA happens both intra-sententially and intersententially. LCS in Arabic poses a serious challenge for almost all NLP tasks since MSA and DA Related Work Dialect Identification in Arabic is crucial for almost all NLP tasks, yet most of the research in Arabic NLP, with few exceptions, is targeted towards MSA. Biadsy et al. (2009) present a system that identifies dialectal words in speech and their dialect of origin through the acoustic signals. Salloum and Habash (2011) tackle the problem of DA to English Machine Translation (MT) by pivoting through MSA. The authors present a system that applies transfer rules from DA to MSA then uses state of the art MSA to English MT system. Habash et al. (2012) present CODA, a Conventional Orthography for Dialectal Arabic that aims to standardize the orthography of all the variants of DA while Dasigi and Diab (2011) present an unsupervised clustering approach to identify orthographic variants in DA. Zaidan and CallisonBurch (2011) crawl a large dataset of MSA-DA news’ commentaries. The authors annotate part of the dataset"
P13-2081,P11-2007,0,0.548342,"okenized & Tokenized-CODA). We conduct two sets of experiments. In the first one, Experiment Set A, we split the data into a training set and a held-out test set. In the second set, Experiment Set B, we use the whole dataset for training without further splitting. For both sets of experiments, we apply 10-fold cross validation on the training data. While using a held-out testset for evaluation (in the first set of experiments) is a better indicator of how well our approach performs on unseen data, only the results from the second set of experiments are directly comparable to those produced by Zaidan and Callison-Burch (2011). In addition to the underlying token-level system, we use the following token-level features: 1. Percentage of words in the sentence that is analyzable by an MSA morphological analyzer. 2. Percentage of words in the sentence that is analyzable by an EDA morphological analyzer. 3. Percentage of words in the sentence that exists in a precompiled EDA lexicon. 3.1.1.2 Perplexity-based Features: We run each sentence through each of the MSA and EDA LMs and record the perplexity for each of them. The perplexity of a language model on a given test sentence; S(w1 , .., wn ) is defined as: perplexity ="
P13-2081,elfardy-diab-2012-simplified,1,\N,Missing
P13-2098,W09-4613,0,0.0166207,"ome form of relation between w1 and w2 . Whereas Lin (1997) and Lin (1998) used dependency relation between words, we use distance. Given a sentence, the distance between w1 and w2 is one plus the number of words that are seen after w1 and before w2 in that sentence. Hence, f (w1 , d, w2 ) is the number of times w1 occurs before w2 at a distance d in all the sentences in a corpus. ∗ is a placeholder for any word, i.e., f (∗, d, ∗) is the frequency of all word pairs occurring at distance d. The distances are directional and not absolute values. A similar measure of relatedness was also used by Kolb (2009). We estimate the frequencies from the Arabic Gigaword. We set the window size to 3 and calculate IC values of all pairs of words occurring at distance within the window size. Since the distances are directional, it has to be noted that given a word, its relations with three words before it and three words after it are modeled. During testing, for each phrase in our test set, we measure semantic relatedness of pairs of words using the IC values estimated from the Arabic Gigaword, and normalize their sum by the number of pairs in the phrase to obtain a measure of Semantic Coherence (SC) of the"
P13-2098,P97-1009,0,0.0657713,"training data for the meta-ranker. We choose RankSVM4 as the metaranker since it performed well as a base-ranker. 2.3 lexical semantic information, simple bag-of-words models usually have a lot of noise; while more sophisticated models considering positional information have sparsity issues. To strike a balance between these two extremes, we introduce a novel model of semantic coherence that is based on a measure of semantic relatedness between pairs of words. We model semantic relatedness between two words using the Information Content (IC) of the pair in a method similar to the one used by Lin (1997) and Lin (1998). Features Our features fall into five families. Base features include the HMM and LM scores produced by the OCR system. These features are used by the baseline system5 as well as by the various reranking methods. Simple features (“simple”) include the baseline rank of the hypothesis and a 0-to-1 range normalized version of it. We also use a hypothesis confidence feature which corresponds to the average of the confidence of individual words in the hypothesis; “confidence” for a given word is computed as the fraction of hypotheses in the n-best list that contain the word (Habash"
P13-2098,P98-2127,0,0.313254,"for the meta-ranker. We choose RankSVM4 as the metaranker since it performed well as a base-ranker. 2.3 lexical semantic information, simple bag-of-words models usually have a lot of noise; while more sophisticated models considering positional information have sparsity issues. To strike a balance between these two extremes, we introduce a novel model of semantic coherence that is based on a measure of semantic relatedness between pairs of words. We model semantic relatedness between two words using the Information Content (IC) of the pair in a method similar to the one used by Lin (1997) and Lin (1998). Features Our features fall into five families. Base features include the HMM and LM scores produced by the OCR system. These features are used by the baseline system5 as well as by the various reranking methods. Simple features (“simple”) include the baseline rank of the hypothesis and a 0-to-1 range normalized version of it. We also use a hypothesis confidence feature which corresponds to the average of the confidence of individual words in the hypothesis; “confidence” for a given word is computed as the fraction of hypotheses in the n-best list that contain the word (Habash and Roth, 2011)"
P13-2098,P06-2034,0,0.0673404,"Missing"
P13-2098,P05-1071,1,0.82405,"Missing"
P13-2098,P11-1088,1,0.849138,"ave been developed (M¨argner and Abed, 2009). The BBN Byblos OCR system (Natajan et al., 2002; Prasad et al., 2008; Saleem et al., 2009), which we use in this paper, relies on a hidden Markov model (HMM) to recover the sequence of characters from the image, and uses an n-gram language model (LM) to emphasize the fluency of the output. For an input image, the OCR decoder generates an nbest list of hypotheses each of which is associated with HMM and LM scores. In addition to fluency as evaluated by LMs, other information potentially helps in discriminating good from bad hypotheses. For example, Habash and Roth (2011) use a variety of linguistic (morphological and syntactic) and non-linguistic features to automatically identify errors in OCR 2 Discriminative Reranking for OCR Each hypothesis in an n-best list {hi }ni=1 is represented by a d-dimensional feature vector xi ∈ Rd . Each xi is associated with a loss li to generate a labeled n-best list H = {(xi , li )}ni=1 . The loss is computed as the Word Error Rate (WER) of the 549 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 549–555, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Lingu"
P13-2098,N04-1023,0,0.352104,"Missing"
P13-2098,J05-1003,0,\N,Missing
P13-2098,W12-3412,0,\N,Missing
P13-2098,C98-2122,0,\N,Missing
P13-2144,W12-3705,1,0.423233,"Missing"
P13-2144,P12-1042,1,0.79734,"They mine the web to learn associations that are indicative of opinion stances in debates and combine this knowledge with discourse information. Anand et al. (2011) present a supervised method for stance classification. They use a number of linguistic and structural features such as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. In previous work, we proposed a method that uses participantto-participant and participant-to-topic attitudes to identify subgroups in ideological discussions using attitude vector space clustering (Abu-Jbara and Radev, 2012). In this paper, we extend this method by adding latent similarity features to the attitude vectors and applying it to Arabic discussions. In another previous work, our group proposed a supervised method for extracting signed social networks from text (Hassan et al., 2012a). The signed networks constructed using this method were based only on participant-to-participant attitudes that are expressed explicitly in discussions. We used this method to extract signed networks from discussions and used a partitioning algorithm to detect opinion subgroups (Hassan et al., 2012b). In this paper, we exte"
P13-2144,P12-1091,1,0.0945698,"HTML file to identify the posts, the discussants, and the thread structure. We transform the Arabic content of the posts and the discussant names that are written in Arabic to the Buckwalter encoding (Buckwalter, 2004). We use AMIRAN (Diab, 2009), a system for processing Arabic text, to tokenize the text and identify noun phrases. 3.2 Identifying Opinion Targets 3.4 Latent Textual Similarity If two participants share the same opinion, they tend to focus on similar aspects of the discussion topic and emphasize similar points that support their opinion. To capture this, we follow previous work (Guo and Diab, 2012; Dasigi et al., 2012) and apply Latent Dirichelet Allocation (LDA) topic models to the text written by the different participants. We use an LDA model with 100 topics. So, we represent all the text written in the discussion by each participant as a vector of 100 dimensions. The vector of each participant contains the topic distribution of the participant, as produced by the LDA model. Identifying Opinionated Text To identify opinion-bearing text, we start from the word level. We identify the polarized words that appear in text by looking each word up in a lexicon of Arabic polarized words. In"
P13-2144,W11-1701,0,0.0227731,"gned network (the second representation). We evaluate this system using a data set of Arabic discussions collected from an Arabic debating site. We experiment with several variations of the system. The results show that the clustering the vector space representation achieves better results than partitioning the signed network representation. 2 Somasundaran and Wiebe (2009) present an unsupervised opinion analysis method for debateside classification. They mine the web to learn associations that are indicative of opinion stances in debates and combine this knowledge with discourse information. Anand et al. (2011) present a supervised method for stance classification. They use a number of linguistic and structural features such as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. In previous work, we proposed a method that uses participantto-participant and participant-to-topic attitudes to identify subgroups in ideological discussions using attitude vector space clustering (Abu-Jbara and Radev, 2012). In this paper, we extend this method by adding latent similarity features to the attitude vectors and applying it to Arabic discussions."
P13-2144,P10-1041,1,0.806507,"siloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying the holders and the targets of opinion (Zhai et al., 2010; Popescu and Etzioni, 2007; Bethard et al., 2004). Opinion mining and sentiment analysis techniques have been used in various applications. One example of such applications is identifying perspectives (Grefenstette et al., 2004; Lin et al., 2006) which is most similar to the topic of this paper. For example, in (Lin et al., 2006), the authors experiment with several supervised and statistical models to capture how perspectives are expressed at the document and the sentence levels. Unfortun"
P13-2144,D10-1121,1,0.0599058,"Pang & Lee (2008) and Liu & Zhang (2012) wrote two recent comprehensive surveys about sentiment analysis and opinion mining techniques and applications. Previous work has proposed methods for identifying subjective text that expresses opinion and distinguishing it from objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying the holders and the targets of opinion (Zhai et al., 2010; Popescu and Etzioni, 2007; Bethard et al., 2004). Opinion mining and sentiment analysis techniques have been used in various applications. One examp"
P13-2144,banea-etal-2008-bootstrapping,0,0.0115159,"n et al., 2012b). In this paper, we extend this method by using participant-to-topic attitudes to construct the signed network. Previous Work Our work is related to a large body of research on opinion mining and sentiment analysis. Pang & Lee (2008) and Liu & Zhang (2012) wrote two recent comprehensive surveys about sentiment analysis and opinion mining techniques and applications. Previous work has proposed methods for identifying subjective text that expresses opinion and distinguishing it from objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research effor"
P13-2144,P11-2104,1,0.701137,"s. One example of such applications is identifying perspectives (Grefenstette et al., 2004; Lin et al., 2006) which is most similar to the topic of this paper. For example, in (Lin et al., 2006), the authors experiment with several supervised and statistical models to capture how perspectives are expressed at the document and the sentence levels. Unfortunately, not much work has been done on Arabic sentiment analysis and opinion mining. Abbasi et al. (2008) applies sentiment analysis techniques to identify and classify documentlevel opinions in text crawled from English and Arabic web forums. Hassan et al. (2011) proposed a method for identifying the polarity of nonEnglish words using multilingual semantic graphs. They applied their method to Arabic and Hindi. Abdul-Mageed and Diab (2011) annotated a corpus of Modern Standard Arabic (MSA) news text for subjectivity at the sentence level. In a later work (2012a), they expanded their corpus by la830 beling data from more genres using Amazon Mechanical Turk. Abdul-Mageed et al. (2012a) developed SAMAR, a system for subjectivity and Sentiment Analysis for Arabic social media genres. We use this system as a component in our approach. 3 accuracy of SAMAR on"
P13-2144,W04-1606,0,0.012904,"s of five components. The input to the pipeline is a discussion thread in Arabic language crawled from a discussion forum. The output is the list of participants in the discussion and the subgroup membership of each discussant. We describe the components of the pipeline in the following subsections. 3.1 Preprocessing The input to this component is a discussion thread in HTML format. We parse the HTML file to identify the posts, the discussants, and the thread structure. We transform the Arabic content of the posts and the discussant names that are written in Arabic to the Buckwalter encoding (Buckwalter, 2004). We use AMIRAN (Diab, 2009), a system for processing Arabic text, to tokenize the text and identify noun phrases. 3.2 Identifying Opinion Targets 3.4 Latent Textual Similarity If two participants share the same opinion, they tend to focus on similar aspects of the discussion topic and emphasize similar points that support their opinion. To capture this, we follow previous work (Guo and Diab, 2012; Dasigi et al., 2012) and apply Latent Dirichelet Allocation (LDA) topic models to the text written by the different participants. We use an LDA model with 100 topics. So, we represent all the text w"
P13-2144,W12-4102,1,0.820433,"h as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. In previous work, we proposed a method that uses participantto-participant and participant-to-topic attitudes to identify subgroups in ideological discussions using attitude vector space clustering (Abu-Jbara and Radev, 2012). In this paper, we extend this method by adding latent similarity features to the attitude vectors and applying it to Arabic discussions. In another previous work, our group proposed a supervised method for extracting signed social networks from text (Hassan et al., 2012a). The signed networks constructed using this method were based only on participant-to-participant attitudes that are expressed explicitly in discussions. We used this method to extract signed networks from discussions and used a partitioning algorithm to detect opinion subgroups (Hassan et al., 2012b). In this paper, we extend this method by using participant-to-topic attitudes to construct the signed network. Previous Work Our work is related to a large body of research on opinion mining and sentiment analysis. Pang & Lee (2008) and Liu & Zhang (2012) wrote two recent comprehensive surveys"
P13-2144,W03-1014,0,0.0200975,"this paper, we extend this method by using participant-to-topic attitudes to construct the signed network. Previous Work Our work is related to a large body of research on opinion mining and sentiment analysis. Pang & Lee (2008) and Liu & Zhang (2012) wrote two recent comprehensive surveys about sentiment analysis and opinion mining techniques and applications. Previous work has proposed methods for identifying subjective text that expresses opinion and distinguishing it from objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying"
P13-2144,W06-1652,0,0.0303638,"Liu & Zhang (2012) wrote two recent comprehensive surveys about sentiment analysis and opinion mining techniques and applications. Previous work has proposed methods for identifying subjective text that expresses opinion and distinguishing it from objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying the holders and the targets of opinion (Zhai et al., 2010; Popescu and Etzioni, 2007; Bethard et al., 2004). Opinion mining and sentiment analysis techniques have been used in various applications. One example of such application"
P13-2144,C00-1044,0,0.411253,"to detect opinion subgroups (Hassan et al., 2012b). In this paper, we extend this method by using participant-to-topic attitudes to construct the signed network. Previous Work Our work is related to a large body of research on opinion mining and sentiment analysis. Pang & Lee (2008) and Liu & Zhang (2012) wrote two recent comprehensive surveys about sentiment analysis and opinion mining techniques and applications. Previous work has proposed methods for identifying subjective text that expresses opinion and distinguishing it from objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010)."
P13-2144,P09-1026,0,0.499168,"used their method to estimate the policy positions of political parties in Britain and Ireland, on both economic and social policy dimensions. tive. To identify opinion subgroups, we cluster the vector space (the first representation) or partition the signed network (the second representation). We evaluate this system using a data set of Arabic discussions collected from an Arabic debating site. We experiment with several variations of the system. The results show that the clustering the vector space representation achieves better results than partitioning the signed network representation. 2 Somasundaran and Wiebe (2009) present an unsupervised opinion analysis method for debateside classification. They mine the web to learn associations that are indicative of opinion stances in debates and combine this knowledge with discourse information. Anand et al. (2011) present a supervised method for stance classification. They use a number of linguistic and structural features such as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. In previous work, we proposed a method that uses participantto-participant and participant-to-topic attitudes to identi"
P13-2144,P05-1017,0,0.0115434,"(Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying the holders and the targets of opinion (Zhai et al., 2010; Popescu and Etzioni, 2007; Bethard et al., 2004). Opinion mining and sentiment analysis techniques have been used in various applications. One example of such applications is identifying perspectives (Grefenstette et al., 2004; Lin et al., 2006) which is most similar to the topic of this paper. For example, in (Lin et al., 2006), the authors experiment with several supervised and statistical models to capture how perspectives are expressed at the document and the"
P13-2144,W06-1642,0,0.0162619,"presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying the holders and the targets of opinion (Zhai et al., 2010; Popescu and Etzioni, 2007; Bethard et al., 2004). Opinion mining and sentiment analysis techniques have been used in various applications. One example of such applications is identifying perspectives (Grefenstette et al., 2004; Lin et al., 2006) which is most similar to the topic of this paper. For example, in (Lin et al., 2006), the authors experiment with several supervised and statistical models to capture how perspectives are expressed a"
P13-2144,W06-2915,0,0.0277713,"ation depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying the holders and the targets of opinion (Zhai et al., 2010; Popescu and Etzioni, 2007; Bethard et al., 2004). Opinion mining and sentiment analysis techniques have been used in various applications. One example of such applications is identifying perspectives (Grefenstette et al., 2004; Lin et al., 2006) which is most similar to the topic of this paper. For example, in (Lin et al., 2006), the authors experiment with several supervised and statistical models to capture how perspectives are expressed at the document and the sentence levels. Unfortunately, not much work has been done on Arabic sentiment analysis and opinion mining. Abbasi et al. (2008) applies sentiment analysis techniques to identify and classify documentlevel opinions in text crawled from English and Arabic web forums. Hassan et al. (2011) proposed a method for identifying the polarity of nonEnglish words using multilingual se"
P13-2144,H05-1044,0,0.0140321,"written in the discussion by each participant as a vector of 100 dimensions. The vector of each participant contains the topic distribution of the participant, as produced by the LDA model. Identifying Opinionated Text To identify opinion-bearing text, we start from the word level. We identify the polarized words that appear in text by looking each word up in a lexicon of Arabic polarized words. In our experiments, we use Sifat (Abdul-Mageed and Diab, 2012b), a lexicon of 3982 Arabic adjectives labeled as positive, negative, or neutral. The polarity of a word may be dependant on its context (Wilson et al., 2005). For example, a positive word that appears in a negated context should be treated as expressing negative opinion rather than positive. To identify the polarity of a word given the sentence it appears in, we use SAMAR (Abdul-Mageed et al., 2012b), a system for Subjectivity and Sentiment Analysis for Arabic social media genres. SAMAR labels a sentence that contains an opinion expression as positive, negative, or neutral taking into account the context of the opinion expression. The reported 3.5 Subgroup Detection At this point, we have for every discussant the targets towards which he/she expre"
P13-2144,C10-1143,0,0.00641845,"Missing"
P13-2144,W11-0413,1,\N,Missing
P13-2144,H05-2017,0,\N,Missing
P13-2144,H05-1043,0,\N,Missing
P13-2144,P12-2013,1,\N,Missing
P13-2144,abdul-mageed-diab-2012-awatif,1,\N,Missing
P14-2125,P07-2045,0,0.0127241,"mental setup and the four baseline systems we built, and we evaluate their performance and the potential of their combination. In the next section we present and evaluate the system selection approach. MT Systems. We build four MT systems. (1) DA-Only. This system is trained on the DAEnglish data and tuned on EgyDevV3. (2) MSA-Only. This system is trained on the MSA-English data and tuned on MT08. (3) DA+MSA. This system is trained on the combination of both corpora (resulting in 62M tokenized2 words on the Arabic side) and tuned on MT Tools and Settings. We use the open-source Moses toolkit (Koehn et al., 2007) to build four Arabic-English phrase-based statistical machine translation systems (SMT). Our systems use a standard phrase-based architecture. The parallel corpora are word-aligned using GIZA++ (Och and Ney, 2003). The language model for our systems is trained on English Gigaword (Graff and Cieri, 2003). We use SRILM Toolkit (Stolcke, 2002) to build a 5-gram language model with modified 2 Since the DA+MSA system is intended for DA data and DA morphology, as far as tokenization is concerned, is more complex, we tokenized the training data with dialect awareness (DA with M ADA -A RZ and MSA wit"
P14-2125,W09-0807,1,0.834439,"lect identification together with various linguistic features in optimizing the selection of outputs of four different MT systems on input text that includes a mix of dialects. We test our approach on Arabic, a prototypical diglossic language (Ferguson, 1959) where the standard form of the language, Modern Standard Arabic (MSA) and the regional dialects (DA) live side-by-side and are closely related. MSA is the language used in education, scripted speech and official settings while DA is the primarily spoken Dialect Identification. There has been a number of efforts on dialect identification (Biadsy et al., 2009; Zaidan and Callison-Burch, 2011; Akbacak et al., 2011; Elfardy et al., 2013; Elfardy and Diab, 2013). Elfardy et al. (2013) performed token-level dialect ID by casting the problem as a code-switching problem and treating MSA and Egyptian as two different languages. They later 1 This paper presents work supported by the Defense Advanced Research Projects Agency (DARPA) contract No. HR0011-12-C-0014. Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA. 772 Proceedings of the 52nd Annual Mee"
P14-2125,W04-3250,0,0.425605,"Missing"
P14-2125,P13-2081,1,0.908394,"ts of four different MT systems on input text that includes a mix of dialects. We test our approach on Arabic, a prototypical diglossic language (Ferguson, 1959) where the standard form of the language, Modern Standard Arabic (MSA) and the regional dialects (DA) live side-by-side and are closely related. MSA is the language used in education, scripted speech and official settings while DA is the primarily spoken Dialect Identification. There has been a number of efforts on dialect identification (Biadsy et al., 2009; Zaidan and Callison-Burch, 2011; Akbacak et al., 2011; Elfardy et al., 2013; Elfardy and Diab, 2013). Elfardy et al. (2013) performed token-level dialect ID by casting the problem as a code-switching problem and treating MSA and Egyptian as two different languages. They later 1 This paper presents work supported by the Defense Advanced Research Projects Agency (DARPA) contract No. HR0011-12-C-0014. Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA. 772 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 772–778, c Baltimore, Mary"
P14-2125,N13-1045,0,0.0117847,"(Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers explored the idea of re-ranking the n-best output of MT systems using different types of syntactic models (Och et al., 2004; Hasan et al., 2006; Ma and McKeown, 2013). While most researchers use target language features in training their rerankers, others considered source language features (Ma and McKeown, 2013). Most MT system combination work uses MT systems employing different techniques to train on the same data. However, in this paper, we use the same MT algorithms for training, tuning, and testing, but vary the training data, specifically in terms of the degree of source language dialectness. Our approach runs a classifier trained only on source language features to decide which system should translate each sentence in the test set, which means that"
P14-2125,J03-1002,0,0.00634686,"Missing"
P14-2125,N13-1044,1,0.910277,"eser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the M ADA +T OKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with M ADA -A RZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers explored the idea of re-ranking the n-best output of MT systems using different types of syntactic models (Och et al., 2004; Hasan et al., 2006; Ma and McKeown, 2013)"
P14-2125,P03-1021,0,0.0147561,"Linguistics (Short Papers), pages 772–778, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics used features from their token-level system to train a classifier that performs sentence-level dialect ID (Elfardy and Diab, 2013). In this paper, we use A IDA, the system of Elfardy and Diab (2013), to provide a variety of dialect ID features to train classifiers that select, for a given sentence, the MT system that produces the best translation. Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the M ADA +T OKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with M ADA -A RZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010)"
P14-2125,P02-1040,0,0.0896015,"re, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics used features from their token-level system to train a classifier that performs sentence-level dialect ID (Elfardy and Diab, 2013). In this paper, we use A IDA, the system of Elfardy and Diab (2013), to provide a variety of dialect ID features to train classifiers that select, for a given sentence, the MT system that produces the best translation. Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the M ADA +T OKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with M ADA -A RZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). System Selection and Combination in Machine Translation. The m"
P14-2125,W06-2606,0,0.0138725,"ed with M ADA -A RZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers explored the idea of re-ranking the n-best output of MT systems using different types of syntactic models (Och et al., 2004; Hasan et al., 2006; Ma and McKeown, 2013). While most researchers use target language features in training their rerankers, others considered source language features (Ma and McKeown, 2013). Most MT system combination work uses MT systems employing different techniques to train on the same data. However, in this paper, we use the same MT algorithms for training, tuning, and testing, but vary the training data, specifically in terms of the degree of source language dialectness. Our approach runs a classifier trained only on source language features to decide which system should translate each sentence in the tes"
P14-2125,P07-1040,0,0.0245416,"to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the M ADA +T OKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with M ADA -A RZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers explored the idea of re-ranking the n-best output of MT systems using different types of syntactic models (Och et al., 2004; Hasan et al., 2006; Ma and McKeown, 2013). While most researchers use target language features in training their rerankers, others considered source language features (Ma and McKeown, 2013). Most MT system combination work uses MT systems employing different techniques to train on the same data. However, in this paper, we use the same MT algorithms for training, tuning, and testing, but vary the"
P14-2125,D08-1011,0,0.0160327,"scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the M ADA +T OKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with M ADA -A RZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers explored the idea of re-ranking the n-best output of MT systems using different types of syntactic models (Och et al., 2004; Hasan et al., 2006; Ma and McKeown, 2013). While most researchers use target language features in training their rerankers, others considered source language features (Ma and McKeown, 2013). Most MT system combination work uses MT systems employing different techniques to train on the same data. However, in this paper, we use the same MT algorithms for training, tuning, and testing, but vary the training data, specifically in terms o"
P14-2125,P08-2030,1,0.805052,", for a given sentence, the MT system that produces the best translation. Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the M ADA +T OKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with M ADA -A RZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers explored the idea of re-ranking the n-best output of MT systems using different types of"
P14-2125,P08-2021,0,0.0247599,"ank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the M ADA +T OKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with M ADA -A RZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers explored the idea of re-ranking the n-best output of MT systems using different types of syntactic models (Och et al., 2004; Hasan et al., 2006; Ma and McKeown, 2013). While most researchers use target language features in training their rerankers, others considered source language features (Ma and McKeown, 2013). Most MT system combination work uses MT systems employing different techniques to train on the same data. However, in this paper, we use the same MT algorithms for training, tuning, and testing, but vary the training data, specif"
P14-2125,P06-1001,1,0.782279,"d Diab (2013), to provide a variety of dialect ID features to train classifiers that select, for a given sentence, the MT system that produces the best translation. Kneser-Ney smoothing. Feature weights are tuned to maximize BLEU on tuning sets using Minimum Error Rate Training (Och, 2003). Results are presented in terms of BLEU (Papineni et al., 2002). All evaluation results are case insensitive. The English data is tokenized using simple punctuation-based rules. The MSA portion of the Arabic side is segmented according to the Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004; Sadat and Habash, 2006) using the M ADA +T OKAN morphological analyzer and tokenizer v3.1 (Roth et al., 2008), while the DA portion is ATB-tokenized with M ADA -A RZ (Habash et al., 2013). The Arabic text is also Alif/Ya normalized. For more details on processing Arabic, see (Habash, 2010). System Selection and Combination in Machine Translation. The most popular approach to MT system combination involves building confusion networks from the outputs of different MT systems and decoding them to generate new translations (Rosti et al., 2007; Karakos et al., 2008; He et al., 2008; Xu et al., 2011). Other researchers ex"
P14-2125,P13-2001,0,0.122008,"Missing"
P14-2125,W11-2602,1,0.874594,"Missing"
P14-2125,N13-1036,1,0.902589,"aword (Graff and Cieri, 2003). We use SRILM Toolkit (Stolcke, 2002) to build a 5-gram language model with modified 2 Since the DA+MSA system is intended for DA data and DA morphology, as far as tokenization is concerned, is more complex, we tokenized the training data with dialect awareness (DA with M ADA -A RZ and MSA with M ADA) since M ADA -A RZ does a lot better than M ADA on DA (Habash et al., 2013). Tuning and Test data, however, are tokenized by M ADA -A RZ since we do not assume any knowledge of the dialect of a test sentence. 773 EgyDevV3. (4) MSA-Pivot. This MSA-pivoting system uses Salloum and Habash (2013)’s DA-MSA MT system followed by an Arabic-English SMT system which is trained on both corpora augmented with the DA-English where the DA side is preprocessed with the same DA-MSA MT system then tokenized with M ADA -A RZ. The result is 67M tokenized words on the Arabic side. EgyDevV3 was similarly preprocessed with the DA-MSA MT system and M ADA -A RZ and used for tuning the system parameters. Test sets are similarly preprocessed before decoding with the SMT system. is that these systems complement each other in interesting ways where the combination of their selections could lead to better ov"
P14-2125,2010.amta-papers.5,0,0.358145,"Missing"
P14-2125,W11-2121,0,0.0486787,"Missing"
P14-2125,P11-2007,0,0.0759012,"ogether with various linguistic features in optimizing the selection of outputs of four different MT systems on input text that includes a mix of dialects. We test our approach on Arabic, a prototypical diglossic language (Ferguson, 1959) where the standard form of the language, Modern Standard Arabic (MSA) and the regional dialects (DA) live side-by-side and are closely related. MSA is the language used in education, scripted speech and official settings while DA is the primarily spoken Dialect Identification. There has been a number of efforts on dialect identification (Biadsy et al., 2009; Zaidan and Callison-Burch, 2011; Akbacak et al., 2011; Elfardy et al., 2013; Elfardy and Diab, 2013). Elfardy et al. (2013) performed token-level dialect ID by casting the problem as a code-switching problem and treating MSA and Egyptian as two different languages. They later 1 This paper presents work supported by the Defense Advanced Research Projects Agency (DARPA) contract No. HR0011-12-C-0014. Any opinions, findings and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of DARPA. 772 Proceedings of the 52nd Annual Meeting of the Association for Compu"
P14-2125,N12-1006,0,0.066245,"o avoid confusion. 3 MT Train/Tune/Test Data. We have two parallel corpora. The first is a DA-English corpus of 5M tokenized words of Egyptian (∼3.5M) and Levantine (∼1.5M). This corpus is part of BOLT data. The second is an MSA-English corpus of 57M tokenized words obtained from several LDC corpora (10 times the size of the DAEnglish data). We work with eight standard MT test sets: three MSA sets from NIST MTEval with four references (MT06, MT08, and MT09), four Egyptian sets from LDC BOLT data with two references (EgyDevV1, EgyDevV2, EgyDevV3, and EgyTestV2), and one Levantine set from BBN (Zbib et al., 2012) with one reference which we split into LevDev and LevTest. We used MT08 and EgyDevV3 to tune SMT systems while we divided the remaining sets among classifier training data (5,562 sentences), dev (1,802 sentences) and blind test (1,804 sentences) sets to ensure each of these new sets has a variety of dialects and genres (weblog and newswire). Machine Translation Experiments In this section, we present our MT experimental setup and the four baseline systems we built, and we evaluate their performance and the potential of their combination. In the next section we present and evaluate the system"
P14-2125,N04-1021,0,\N,Missing
P14-2125,D08-1076,0,\N,Missing
palmer-etal-2008-pilot,W04-3212,1,\N,Missing
palmer-etal-2008-pilot,W05-0630,0,\N,Missing
palmer-etal-2008-pilot,W03-1006,0,\N,Missing
palmer-etal-2008-pilot,N07-2014,0,\N,Missing
palmer-etal-2008-pilot,P98-1013,0,\N,Missing
palmer-etal-2008-pilot,C98-1013,0,\N,Missing
palmer-etal-2008-pilot,P04-1043,0,\N,Missing
palmer-etal-2008-pilot,S07-1026,1,\N,Missing
palmer-etal-2008-pilot,W05-0620,0,\N,Missing
palmer-etal-2008-pilot,J02-3001,0,\N,Missing
palmer-etal-2008-pilot,P02-1031,1,\N,Missing
palmer-etal-2008-pilot,N04-1030,0,\N,Missing
palmer-etal-2008-pilot,N04-1032,0,\N,Missing
pasha-etal-2014-madamira,P05-1071,1,\N,Missing
pasha-etal-2014-madamira,P08-2030,1,\N,Missing
pasha-etal-2014-madamira,P09-2056,1,\N,Missing
pasha-etal-2014-madamira,P06-1086,1,\N,Missing
pasha-etal-2014-madamira,mohamed-etal-2012-annotating,0,\N,Missing
pasha-etal-2014-madamira,N13-1044,1,\N,Missing
pasha-etal-2014-madamira,W12-2301,1,\N,Missing
prabhakaran-etal-2012-annotations,W09-3953,1,\N,Missing
prabhakaran-etal-2012-annotations,P11-1078,0,\N,Missing
prabhakaran-etal-2012-annotations,N12-1057,1,\N,Missing
Q18-1014,W16-1614,0,0.0997443,"cs separately, and an iterative training algorithm was then used to retrieve the mapping of English words to themselves. Only punctuation marks were used to seed the learning and high accuracy results were reported. However, the method was not evaluated cross-lingually. We observed experimentally that punctuation marks—and function words in general—are insufficient to map words cross-lingually since they have different distributional profiles in different languages due to their predominant syntactic role. Another unsupervised approach has been recently proposed using adversarial autoencoders (Barone, 2016) where a transformation is learned without a seed by matching the distribution of the source word embeddings with the target distribution. Preliminary investigation showed some correct mappings but the results were not comparable to supervised methods. Recent efforts using carefully-tuned adversarial methods report encouraging results comparable to supervised methods (Zhang et al., 2017; Conneau et al., 2017). In Kiela et al. (2015), bilingual lexicon induction is achieved by matching visual features extracted from images that correspond to each word using a convolutional neural network. The i"
Q18-1014,Q17-1010,0,0.272914,"imize the probability of all words within a fixed window around a given word. Formally, given a word w in a vocabulary W , the objective of the skip-gram model is to maximize the following loglikelihood: ∑ log p(c|w) c∈Cw where Cw is the set of words in the context of w. The words are represented as one-hot vectors of size |W |that are projected into dense vectors of size d. Over a large corpus, the d-dimensional word projections encode semantic and syntactic features that are not only useful for maximizing the above probability, but also serve as general-purpose representations for words. In Bojanowski et al. (2017), a word vector is represented as the sum of its character n-grams which helps account for inflectional variations within a language, especially for morphologically rich languages where less frequent inflections are less likely to have good representations using only word-level features. Using n-grams helps account for lexical similarities among words within the same language; independently-learned embeddings with no explicit alignment would still have unrelated n-gram representations even if the languages share lexical similarities. We will refer to this model as the subword skip-gram. 187 2."
Q18-1014,elkateb-etal-2006-building,0,0.0402698,"erative mapping (IM) method. The loss function L in equation 2 was used to guide the tuning of model parameters. We tuned k = [10, 20, 30, 40, 50] for the spectral initialization, and due to randomness in IM, we repeated each experiment 10 times and used the mapping that resulted in the smallest loss. For the final linear transformation T , we used the most frequent 50K words in both source and target languages, and we used the hubness reduction method described in Dinu et al. (2015) with c=5000. We extracted dictionary pairs from the Multilingual WordNet (Miller, 1995; Sagot and Fišer, 2008; Elkateb et al., 2006; Abouenour et al., 2013) where the source words are within the top 15K words in all datasets. From these pairs, we extracted a random sample of 2K unique (source, target) pairs for training the supervised method, and the remaining source words and all their translations were used for testing. This resulted in a total of 977 French words and 473 Arabic words for evaluation. 4.3 Analysis and Results The unsupervised word mapping method proposed in this paper consists of three parts: given a subset of source and target words with a viable mapping, we extract tentative correspondences using spect"
Q18-1014,S17-1001,0,0.0140648,"armaki;mahesh_mohan;mtdiab}@gwu.edu Abstract to perform arithmetic on word vectors for analogical reasoning and semantic composition (Mikolov et al., 2013b). For example, in a vector space V where f = V (‘‘f rance”), p = V (‘‘paris”), and g = V (‘‘germany”), the distance f − p reflects the country-capital relationship, and g + f − p results in a vector closest to V (‘‘berlin”). Named entities and inflectional morphemes are particularly amenable to vector arithmetic, while derivational morphology, polysemy, and other nuanced semantic categories result in lower performance in analogy questions (Finley et al., 2017). Most existing methods for automatic bilingual dictionary induction rely on prior alignments between the source and target languages, such as parallel corpora or seed dictionaries. For many language pairs, such supervised alignments are not readily available. We propose an unsupervised approach for learning a bilingual dictionary for a pair of languages given their independently-learned monolingual word embeddings. The proposed method exploits local and global structures in monolingual vector spaces to align them such that similar words are mapped to each other. We show empirically that the p"
Q18-1014,P08-1088,0,0.190412,"f supervision, ranging from word-level alignment using bilingual dictionaries (Ammar et al., 2016), sentence-level alignment using parallel corpora (Gouws et al., 2015; Klementiev et al., 2012), or document alignment using crosslingual topic models (Vulić and Moens, 2015; Vulić and Moens, 2012). Using such alignments, especially large parallel corpora or sizable dictionaries, high-quality bilingual embeddings can be obtained (Upadhyay et al., 2016). In addition, a number of methods have been proposed for expanding dictionaries using a small initial dictionary with as few as a hundred entries (Haghighi et al., 2008). However, such alignments are not available for all languages and dialects, and while a small dictionary might be feasible to acquire, discovering word mappings with no prior knowledge whatsoever is valuable. Intuitively, if the monolingual corpora express similar aspects of the world, there should be enough structure within the vector space of each language to recover the mappings in a completely unsupervised manner. In this paper, we propose a novel approach for learning a transformation between monolingual word embeddings without the use of prior alignments. We show empirically that we can"
Q18-1014,E17-2098,0,0.0199928,"zes the differences in global pair-wise distances among word vectors. The retrieved mappings are then used to fit a linear projection matrix to transform word embeddings from the source to the target language. 1.1 Related Work Few models have been proposed for extracting dictionaries or learning bilingual embeddings without the use of any prior alignment. For languages that 186 share orthographic similarities, lexical features such as the normalized edit distance between source and target words can be used to extract a seed lexicon for bootstrapping the bilingual dictionary induction process (Hauer et al., 2017). In (Diab and Finch, 2000), unsupervised mappings were extracted by preserving pairwise distances between word co-occurrence representations from two comparable corpora. The model was only evaluated mono-lingually, where two sections of a corpus were used for collecting co-occurrence statistics separately, and an iterative training algorithm was then used to retrieve the mapping of English words to themselves. Only punctuation marks were used to seed the learning and high accuracy results were reported. However, the method was not evaluated cross-lingually. We observed experimentally that pun"
Q18-1014,D15-1015,0,0.0245495,"utional profiles in different languages due to their predominant syntactic role. Another unsupervised approach has been recently proposed using adversarial autoencoders (Barone, 2016) where a transformation is learned without a seed by matching the distribution of the source word embeddings with the target distribution. Preliminary investigation showed some correct mappings but the results were not comparable to supervised methods. Recent efforts using carefully-tuned adversarial methods report encouraging results comparable to supervised methods (Zhang et al., 2017; Conneau et al., 2017). In Kiela et al. (2015), bilingual lexicon induction is achieved by matching visual features extracted from images that correspond to each word using a convolutional neural network. The imagebased approach performs particularly well for words that express concrete rather than abstract concepts, and provides a convenient alternative to linguistic supervision when corresponding images are available. The unsupervised mapping problem arises in other contexts where an optimal alignment between two isomorphic point sets is sought. In image registration and shape recognition, various efficient methods can be used to find a"
Q18-1014,C12-1089,0,0.0304864,"Action Editor: Sebastian Pad´o . Submission batch: 10/2017; Revision batch: 12/2017; Published 3/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. tic space. Such cross-lingual word embeddings can be used to expand dictionaries or to learn languageindependent classifiers. A number of methods have been proposed recently for learning cross-lingual word embeddings with various degrees of supervision, ranging from word-level alignment using bilingual dictionaries (Ammar et al., 2016), sentence-level alignment using parallel corpora (Gouws et al., 2015; Klementiev et al., 2012), or document alignment using crosslingual topic models (Vulić and Moens, 2015; Vulić and Moens, 2012). Using such alignments, especially large parallel corpora or sizable dictionaries, high-quality bilingual embeddings can be obtained (Upadhyay et al., 2016). In addition, a number of methods have been proposed for expanding dictionaries using a small initial dictionary with as few as a hundred entries (Haghighi et al., 2008). However, such alignments are not available for all languages and dialects, and while a small dictionary might be feasible to acquire, discovering word mappings with no p"
Q18-1014,W16-2503,0,0.0166768,"rge text corpora (Bengio et al., 2003; Mikolov et al., 2013b). The qualitative nature of these embeddings can be demonstrated through empirical evidence of regularities that reflect certain semantic relationships. Words in the vector space are generally clustered by meaning, and the distances between words and clusters reflect semantic or syntactic relationships, which makes it possible The extent of these semantic and syntactic regularities is difficult to assess intrinsically, and the performance in analogical reasoning can be partially attributed to the clustering of the words in question (Linzen, 2016). If meaning is encoded in the relative distances among word vectors, then the structure within vector spaces should be consistent across different languages given that the datasets used to build them express similar content. In Rapp (1995), a simulation study showed that similarity in word co-occurrence patterns within unrelated German and English texts is correlated with the number of corresponding word positions in the monolingual cooccurrence matrices. More recently, Mikolov et al. (2013a) showed that a linear projection can be learned to transform word embeddings from one language into th"
Q18-1014,P14-5010,0,0.00365795,"P Agence France Presse corpora from Gigaword datasets for English (Parker et al., 2011b), French (Mendonça et al., 2009), and Arabic (Parker et al., 2011a). APW The Associated Press corpora from Gigaword datasets. UN Parallel Arabic-English corpus from UN proceedings (Ma, 2004). We randomly extracted 5M sentences from each corpus to create the datasets in Table 1, which are either parallel (suffix:p), similar (suffix:s), or dissimilar (suffix:d). All datasets are within-genre to ensure that they share a common vocabulary. We tokenized the English and French datasets using the CoreNLP toolkit (Manning et al., 2014). We also converted all characters to lower case and normalized numeric sequences to a single token. Arabic text was tokenized using the Madamira toolkit (Pasha et al., 2014). We used the D3 tokenization scheme, and we further processed the data by separating punctuation and normalizing digits. Note that Arabic tokenization is non-deterministic due to clitic affixation, so the processed datasets still contained untokenized phrases. 4.2 Experimental Set-up For each of the datasets described above, we generated 100-dimensional word embeddings using the subword skip-gram model (Bojanowski et al.,"
Q18-1014,N13-1090,0,0.680487,"We show empirically that the performance of bilingual correspondents that are learned using our proposed unsupervised method is comparable to that of using supervised bilingual correspondents from a seed dictionary. 1 Introduction The working hypothesis in distributional semantics is that the meaning of a word can be inferred by its distribution, or co-occurrence, around other words. The validity of this hypothesis is most evident in the performance of distributed vector representations of words, i.e word embeddings, that are automatically induced from large text corpora (Bengio et al., 2003; Mikolov et al., 2013b). The qualitative nature of these embeddings can be demonstrated through empirical evidence of regularities that reflect certain semantic relationships. Words in the vector space are generally clustered by meaning, and the distances between words and clusters reflect semantic or syntactic relationships, which makes it possible The extent of these semantic and syntactic regularities is difficult to assess intrinsically, and the performance in analogical reasoning can be partially attributed to the clustering of the words in question (Linzen, 2016). If meaning is encoded in the relative distan"
Q18-1014,pasha-etal-2014-madamira,1,0.765356,"Missing"
Q18-1014,P95-1050,0,0.456712,"e generally clustered by meaning, and the distances between words and clusters reflect semantic or syntactic relationships, which makes it possible The extent of these semantic and syntactic regularities is difficult to assess intrinsically, and the performance in analogical reasoning can be partially attributed to the clustering of the words in question (Linzen, 2016). If meaning is encoded in the relative distances among word vectors, then the structure within vector spaces should be consistent across different languages given that the datasets used to build them express similar content. In Rapp (1995), a simulation study showed that similarity in word co-occurrence patterns within unrelated German and English texts is correlated with the number of corresponding word positions in the monolingual cooccurrence matrices. More recently, Mikolov et al. (2013a) showed that a linear projection can be learned to transform word embeddings from one language into the vector space of another using a medium-size seed dictionary, which demonstrates that the multilingual vector spaces are at least related by a linear transform. This makes it possible to align word embeddings of different languages in orde"
Q18-1014,P16-1157,0,0.0559876,"ictionaries or to learn languageindependent classifiers. A number of methods have been proposed recently for learning cross-lingual word embeddings with various degrees of supervision, ranging from word-level alignment using bilingual dictionaries (Ammar et al., 2016), sentence-level alignment using parallel corpora (Gouws et al., 2015; Klementiev et al., 2012), or document alignment using crosslingual topic models (Vulić and Moens, 2015; Vulić and Moens, 2012). Using such alignments, especially large parallel corpora or sizable dictionaries, high-quality bilingual embeddings can be obtained (Upadhyay et al., 2016). In addition, a number of methods have been proposed for expanding dictionaries using a small initial dictionary with as few as a hundred entries (Haghighi et al., 2008). However, such alignments are not available for all languages and dialects, and while a small dictionary might be feasible to acquire, discovering word mappings with no prior knowledge whatsoever is valuable. Intuitively, if the monolingual corpora express similar aspects of the world, there should be enough structure within the vector space of each language to recover the mappings in a completely unsupervised manner. In this"
Q18-1014,E12-1046,0,0.0249936,"2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. tic space. Such cross-lingual word embeddings can be used to expand dictionaries or to learn languageindependent classifiers. A number of methods have been proposed recently for learning cross-lingual word embeddings with various degrees of supervision, ranging from word-level alignment using bilingual dictionaries (Ammar et al., 2016), sentence-level alignment using parallel corpora (Gouws et al., 2015; Klementiev et al., 2012), or document alignment using crosslingual topic models (Vulić and Moens, 2015; Vulić and Moens, 2012). Using such alignments, especially large parallel corpora or sizable dictionaries, high-quality bilingual embeddings can be obtained (Upadhyay et al., 2016). In addition, a number of methods have been proposed for expanding dictionaries using a small initial dictionary with as few as a hundred entries (Haghighi et al., 2008). However, such alignments are not available for all languages and dialects, and while a small dictionary might be feasible to acquire, discovering word mappings with no prior knowledge whatsoever is valuable. Intuitively, if the monolingual corpora express similar aspects"
Q18-1014,P15-2118,0,0.0249329,"17; Published 3/2018. c 2018 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. tic space. Such cross-lingual word embeddings can be used to expand dictionaries or to learn languageindependent classifiers. A number of methods have been proposed recently for learning cross-lingual word embeddings with various degrees of supervision, ranging from word-level alignment using bilingual dictionaries (Ammar et al., 2016), sentence-level alignment using parallel corpora (Gouws et al., 2015; Klementiev et al., 2012), or document alignment using crosslingual topic models (Vulić and Moens, 2015; Vulić and Moens, 2012). Using such alignments, especially large parallel corpora or sizable dictionaries, high-quality bilingual embeddings can be obtained (Upadhyay et al., 2016). In addition, a number of methods have been proposed for expanding dictionaries using a small initial dictionary with as few as a hundred entries (Haghighi et al., 2008). However, such alignments are not available for all languages and dialects, and while a small dictionary might be feasible to acquire, discovering word mappings with no prior knowledge whatsoever is valuable. Intuitively, if the monolingual corpora"
Q18-1014,P17-1179,0,0.0573717,"ss-lingually since they have different distributional profiles in different languages due to their predominant syntactic role. Another unsupervised approach has been recently proposed using adversarial autoencoders (Barone, 2016) where a transformation is learned without a seed by matching the distribution of the source word embeddings with the target distribution. Preliminary investigation showed some correct mappings but the results were not comparable to supervised methods. Recent efforts using carefully-tuned adversarial methods report encouraging results comparable to supervised methods (Zhang et al., 2017; Conneau et al., 2017). In Kiela et al. (2015), bilingual lexicon induction is achieved by matching visual features extracted from images that correspond to each word using a convolutional neural network. The imagebased approach performs particularly well for words that express concrete rather than abstract concepts, and provides a convenient alternative to linguistic supervision when corresponding images are available. The unsupervised mapping problem arises in other contexts where an optimal alignment between two isomorphic point sets is sought. In image registration and shape recognition,"
R13-1001,A00-2013,0,0.435736,"Missing"
R13-1001,W96-0102,0,0.0395104,"earning method that does not abstract rules from the data, but rather keeps all training data. During training, the learner stores the training instances without abstraction. Given a new instance, the classifier finds the k nearest neighbors in the training set and chooses their most frequent class for the new instance. MBL has been shown to have a suitable bias for NLP problems (Daelemans et al., 1999; Daelemans and van den Bosch, 2005) since it does not abstract over irregularities or subregularities. For each of the two classification tasks (i.e., segmentation and POS tagging), we use MBT (Daelemans et al., 1996), a memory-based POS tagger that has access to previous tagging decisions in addition to an expressive feature set. 4.2 5 Segmentation 5.1 Setup We define segmentation as an IOB classification task, where each letter in a word is tagged with a label indicating its place in a segment. The tagset is {B-SEG, I-SEG, O}, where B is a tag assigned to the beginning of a segment, I denotes the inside of a segment, and O spaces between surface form words. Data Sets and Splits We use segmentation and POS data from the Penn Arabic Treebank (PATB) (Maamouri et al., 2004), specifically, we use the followin"
R13-1001,N10-1105,1,0.843769,"Missing"
R13-1001,W95-0107,0,0.361934,"Missing"
R13-1001,N04-4038,1,0.820658,"Missing"
R13-1001,P08-2030,1,0.909449,"Missing"
R13-1001,W10-1401,0,0.122729,"Missing"
R13-1001,P05-1071,0,0.0971224,"Missing"
R13-1001,P09-2056,0,0.0503576,"Missing"
S07-1017,P98-1013,0,0.00879976,"over 8,000 synsets with over 15,000 words; about 1,400 synsets refer to Named Entities. Shallow approaches to text processing have been garnering a lot of attention recently. Specifically, shallow approaches to semantic processing are making large strides in the direction of efficiently and effectively deriving tacit semantic information from text. Semantic Role Labeling (SRL) is one such approach. With the advent of faster and powerful computers, more effective machine learning algorithms, and importantly, large data resources annotated with relevant levels of semantic information FrameNet (Baker et al., 1998) and ProbBank corpora (Palmer et al., 2005), we are seeing a surge in efficient approaches to SRL (Carreras and M`arquez, 2005). SRL is the process by which predicates and their arguments are identified and their roles defined in a sentence. To date, most of the reported SRL systems are for English. We do see some headway for other languages such as German and Chinese. The systems for the other languages follow the successful models devised for English, (Gildea and Jurafsky, 2002; Task design With the release of the AWN, we set out to design a sub-task on Arabic WSD. The 95 5 Task: Semantic Ro"
S07-1017,W05-0620,0,0.320515,"Missing"
S07-1017,J02-3001,0,0.231356,"arning algorithms, and importantly, large data resources annotated with relevant levels of semantic information FrameNet (Baker et al., 1998) and ProbBank corpora (Palmer et al., 2005), we are seeing a surge in efficient approaches to SRL (Carreras and M`arquez, 2005). SRL is the process by which predicates and their arguments are identified and their roles defined in a sentence. To date, most of the reported SRL systems are for English. We do see some headway for other languages such as German and Chinese. The systems for the other languages follow the successful models devised for English, (Gildea and Jurafsky, 2002; Task design With the release of the AWN, we set out to design a sub-task on Arabic WSD. The 95 5 Task: Semantic Role Labeling (SRL) Xue and Palmer, 2004; Pradhan et al., 2003). However, no SRL systems exist for Arabic. Challenges of Arabic for SRL Given the deep difference between such languages, this method may not be straightforward. To clarify this point, let us consider Figure 1. It illustrates a sample Arabic syntactic tree with the relevant part of speech tags and arguments defined. The sentence is                     .      ("
S07-1017,J05-1004,1,0.380781,"; about 1,400 synsets refer to Named Entities. Shallow approaches to text processing have been garnering a lot of attention recently. Specifically, shallow approaches to semantic processing are making large strides in the direction of efficiently and effectively deriving tacit semantic information from text. Semantic Role Labeling (SRL) is one such approach. With the advent of faster and powerful computers, more effective machine learning algorithms, and importantly, large data resources annotated with relevant levels of semantic information FrameNet (Baker et al., 1998) and ProbBank corpora (Palmer et al., 2005), we are seeing a surge in efficient approaches to SRL (Carreras and M`arquez, 2005). SRL is the process by which predicates and their arguments are identified and their roles defined in a sentence. To date, most of the reported SRL systems are for English. We do see some headway for other languages such as German and Chinese. The systems for the other languages follow the successful models devised for English, (Gildea and Jurafsky, 2002; Task design With the release of the AWN, we set out to design a sub-task on Arabic WSD. The 95 5 Task: Semantic Role Labeling (SRL) Xue and Palmer, 2004; Pra"
S07-1017,W04-3212,1,0.868328,"ora (Palmer et al., 2005), we are seeing a surge in efficient approaches to SRL (Carreras and M`arquez, 2005). SRL is the process by which predicates and their arguments are identified and their roles defined in a sentence. To date, most of the reported SRL systems are for English. We do see some headway for other languages such as German and Chinese. The systems for the other languages follow the successful models devised for English, (Gildea and Jurafsky, 2002; Task design With the release of the AWN, we set out to design a sub-task on Arabic WSD. The 95 5 Task: Semantic Role Labeling (SRL) Xue and Palmer, 2004; Pradhan et al., 2003). However, no SRL systems exist for Arabic. Challenges of Arabic for SRL Given the deep difference between such languages, this method may not be straightforward. To clarify this point, let us consider Figure 1. It illustrates a sample Arabic syntactic tree with the relevant part of speech tags and arguments defined. The sentence is                     .      (                  m$rwE AlAmm AlmtHdp frD mhlp nhAyp l AtAHp AlfrSp AmAm qbrS. meaning ‘The United Nations’ project imposed a final grace period as an o"
S07-1017,J98-1001,0,\N,Missing
S07-1017,C98-1013,0,\N,Missing
S07-1026,W05-0620,0,0.120609,"Missing"
S07-1026,J02-3001,0,0.666067,"t applications such as document retrieval, machine translation, question answering and information extraction. However, effective ways for seeing this belief come to fruition require a lot more research investment. Since most of the available data resources are for the English language, most of the reported SRL systems to date only deal with English. Nevertheless, we do see some headway for other languages, such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004; Xue and Palmer, 2005). The systems for non-English languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2003). However, no SRL system exists for Arabic. In this paper, we present a system for semantic role labeling for modern standard Arabic. To our knowledge, it is the first SRL system for a semitic 2 SRL system for Arabic The design of an optimal model for an Arabic SRL systems should take into account specific linguistic aspects of the language. However, a remarkable amount of research has already been done in SRL and we can capitalize from it to design a basic and effective SRL system. The idea is to use the technology developed for English and verify"
S07-1026,W05-0630,1,0.808058,"ic The design of an optimal model for an Arabic SRL systems should take into account specific linguistic aspects of the language. However, a remarkable amount of research has already been done in SRL and we can capitalize from it to design a basic and effective SRL system. The idea is to use the technology developed for English and verify if it is suitable for Arabic. Our adopted SRL models use Support Vector Machines (SVM) to implement a two steps classification approach, i.e. boundary detection and argument classification. Such models have already been investigated in (Pradhan et al., 2003; Moschitti et al., 2005) and their description is hereafter reported. 2.1 Predicate Argument Extraction The extraction of predicative structures is carried out at the sentence level. Given a predicate within a natural language sentence, its arguments have to be properly labeled. This problem is usually divided in two subtasks: (a) the detection of the boundaries, i.e. the word spans of the arguments, and (b) the classification of their type, e.g. Arg0 and ArgM in 133 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 133–136, c Prague, June 2007. 2007 Association for Computati"
S07-1026,P04-1043,1,0.911905,"75.00 86.83 In this paper, we presented a first system for Arabic SRL system. The system yields results that are very promising, 94.06 for argument boundary detection and 81.43 on argument classification. For future work, we would like to experiment with explicit morphological features and different POS tag sets that are tailored to Arabic. The results presented here are based on gold parses. We would like to experiment with automatic parses and shallower representations such as chunked data. Finally, we would like to experiment with more sophisticated kernels, the tree kernels described in (Moschitti, 2004), i.e. models that have shown a lot of promise for the English SRL process. Acknowledgements The first author is funded by DARPA Contract No. HR001106-C-0023. References Table 3: Argument classification results on the test set. more difficult classify in Arabic than it is in English. In our current experiments, the F1 for ARG1 is only 89.83 (compared to 95.42 for ARG0). This may be attributed to two main factors. Arabic allows for different types of syntactic configurations, subject-verb-object, object-verb-subject, verb-subject-object, hence the logical object of a predicate is highly confusa"
S07-1026,N04-1032,0,0.0873263,"guments, semantic role labeling (SRL), in a sentence has a lot of potential for and is a significant step towards the improvement of important applications such as document retrieval, machine translation, question answering and information extraction. However, effective ways for seeing this belief come to fruition require a lot more research investment. Since most of the available data resources are for the English language, most of the reported SRL systems to date only deal with English. Nevertheless, we do see some headway for other languages, such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004; Xue and Palmer, 2005). The systems for non-English languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2003). However, no SRL system exists for Arabic. In this paper, we present a system for semantic role labeling for modern standard Arabic. To our knowledge, it is the first SRL system for a semitic 2 SRL system for Arabic The design of an optimal model for an Arabic SRL systems should take into account specific linguistic aspects of the language. However, a remarkable amount of research has already been done in S"
S07-1026,W04-3212,0,0.079245,"ment retrieval, machine translation, question answering and information extraction. However, effective ways for seeing this belief come to fruition require a lot more research investment. Since most of the available data resources are for the English language, most of the reported SRL systems to date only deal with English. Nevertheless, we do see some headway for other languages, such as German and Chinese (Erk and Pado, 2006; Sun and Jurafsky, 2004; Xue and Palmer, 2005). The systems for non-English languages follow the successful models devised for English, e.g. (Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Pradhan et al., 2003). However, no SRL system exists for Arabic. In this paper, we present a system for semantic role labeling for modern standard Arabic. To our knowledge, it is the first SRL system for a semitic 2 SRL system for Arabic The design of an optimal model for an Arabic SRL systems should take into account specific linguistic aspects of the language. However, a remarkable amount of research has already been done in SRL and we can capitalize from it to design a basic and effective SRL system. The idea is to use the technology developed for English and verify if it is suitable for"
S10-1026,S07-1066,0,0.0328139,"Missing"
S10-1026,S07-1029,0,0.0716957,"Missing"
S10-1026,O97-1002,0,0.060904,"Missing"
S10-1026,W09-2413,0,0.277525,"Missing"
S10-1026,S07-1050,0,0.0496559,"Missing"
S10-1026,S01-1005,0,0.0977757,"Missing"
S10-1026,P10-1156,1,0.684844,"Missing"
S10-1026,H05-1052,0,0.0633796,"Missing"
S10-1026,W09-2410,1,0.781389,"Missing"
S10-1026,J03-1002,0,0.00704674,"Missing"
S10-1026,S07-1016,0,0.0308597,"Missing"
S10-1026,W09-2412,0,0.0847926,"Missing"
S10-1026,W04-0811,0,0.0820942,"Missing"
S10-1026,S07-1044,0,0.037649,"Missing"
S10-1026,S07-1091,0,\N,Missing
S10-1026,D07-1007,0,\N,Missing
S10-1026,P07-1005,0,\N,Missing
S10-1026,S10-1002,0,\N,Missing
S12-1051,W07-0718,0,0.194874,"Missing"
S12-1051,W08-0309,0,0.0142237,"Missing"
S12-1051,P11-1020,0,0.0767679,"Missing"
S12-1051,C04-1051,0,0.824876,"Missing"
S12-1051,N06-2015,0,0.158425,"Missing"
S12-1086,S12-1051,1,0.303682,"Missing"
S12-1086,P11-1020,0,0.0224554,"Missing"
S12-1086,C04-1051,0,0.110731,"Missing"
S12-1086,P12-2028,1,0.650652,"for the task, Pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is helpful for many NLP topics. In Machine Translation (Kauchak and Barzilay, 2006) and Text Summarization (Zhou et al., 2006), results are automatically evaluated based on sentence comparison. In Text Coherence Detection (Lapata and Barzilay, 2005), sentences are linked together by similar or related words. For Word Sense Disambiguation, researchers (Banerjee and Pedersen, 2003; Guo and Diab, 2012a) construct a sense similarity measure from the sentence similarity of the sense definitions. Almost all SS approaches decompose the task into word pairwise similarity problems. For example, Is1 Mona Diab, co-author of this paper, is one of the task organizers lam and Inkpen (2008) create a matrix for each sentence pair, where columns are the words in the first sentence and rows are the words in the second sentence, and each cell stores the distributional similarity of the two words. Then they create an alignment between words in two sentences, and sentence similarity is calculated based on t"
S12-1086,P12-1091,1,0.542446,"for the task, Pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is helpful for many NLP topics. In Machine Translation (Kauchak and Barzilay, 2006) and Text Summarization (Zhou et al., 2006), results are automatically evaluated based on sentence comparison. In Text Coherence Detection (Lapata and Barzilay, 2005), sentences are linked together by similar or related words. For Word Sense Disambiguation, researchers (Banerjee and Pedersen, 2003; Guo and Diab, 2012a) construct a sense similarity measure from the sentence similarity of the sense definitions. Almost all SS approaches decompose the task into word pairwise similarity problems. For example, Is1 Mona Diab, co-author of this paper, is one of the task organizers lam and Inkpen (2008) create a matrix for each sentence pair, where columns are the words in the first sentence and rows are the words in the second sentence, and each cell stores the distributional similarity of the two words. Then they create an alignment between words in two sentences, and sentence similarity is calculated based on t"
S12-1086,N06-2015,0,0.00796031,"et al., 2004), 2. MSR video data (Chen and Dolan, 2011), 3. SMT europarl data, 2 http://en.wiktionary.org/wiki/Wiktionary:Main Page http://nlp.stanford.edu/software/tagger.shtml 4 http://wn-similarity.sourceforge.net, WordNet::QueryData 3 models LDA WTMF MSRpar 0.274 0.411(67/89) MSRvid 0.7682 0.835(11/89) SMT-eur 0.452 0.513(10/89) ON-WN 0.619 0.727(1/89) SMT-news 0.366 0.438(28/89) Table 2: Performance of LDA and WTMF on each individual test set of Task 6 STS data ALL 0.695(20/89) ALLnrm 0.830(10/89) Mean 0.608(19/89) Table 3: Performance of WTMF on all test sets 4. OntoNotes-WordNet data (Hovy et al., 2006), 5. SMT news data. Evaluation Metrics: Since the systems are required to assigned a similarity score to each sentence pair, Pearson’s correlation is used to measure the performance of systems on each of the 5 data sets. However, measuring the overall performance on the concatenation of 5 data sets is rarely discussed in previous work. Accordingly the organizers of STS task provide three evaluation metrics: 1. ALL: Pearson correlation with the gold standard for the combined 5 data sets. 2. ALLnrm: Pearson correlation after the system outputs for each data set are fitted to the gold standard us"
S12-1086,N06-1058,0,0.0108303,"atures, can outperform most systems for this task. The key to the approach is to carefully handle missing words that are not in the sentence, and thus rendering it superior to Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Our system ranks 20 out of 89 systems according to the official evaluation metric for the task, Pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is helpful for many NLP topics. In Machine Translation (Kauchak and Barzilay, 2006) and Text Summarization (Zhou et al., 2006), results are automatically evaluated based on sentence comparison. In Text Coherence Detection (Lapata and Barzilay, 2005), sentences are linked together by similar or related words. For Word Sense Disambiguation, researchers (Banerjee and Pedersen, 2003; Guo and Diab, 2012a) construct a sense similarity measure from the sentence similarity of the sense definitions. Almost all SS approaches decompose the task into word pairwise similarity problems. For example, Is1 Mona Diab, co-author of this paper, is one of the task organizers lam and Inkpen (2008"
S12-1086,N06-1057,0,0.0125877,"he key to the approach is to carefully handle missing words that are not in the sentence, and thus rendering it superior to Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Our system ranks 20 out of 89 systems according to the official evaluation metric for the task, Pearson correlation, and it ranks 10/89 and 19/89 in the other two evaluation metrics employed by the organizers. 1 Introduction Identifying the degree of semantic similarity [SS] between two sentences is helpful for many NLP topics. In Machine Translation (Kauchak and Barzilay, 2006) and Text Summarization (Zhou et al., 2006), results are automatically evaluated based on sentence comparison. In Text Coherence Detection (Lapata and Barzilay, 2005), sentences are linked together by similar or related words. For Word Sense Disambiguation, researchers (Banerjee and Pedersen, 2003; Guo and Diab, 2012a) construct a sense similarity measure from the sentence similarity of the sense definitions. Almost all SS approaches decompose the task into word pairwise similarity problems. For example, Is1 Mona Diab, co-author of this paper, is one of the task organizers lam and Inkpen (2008) create a matrix for each sentence pair, w"
S12-1086,C10-2048,0,\N,Missing
S13-1004,S12-1051,1,0.661393,"t together. • (1) The two sentences are not equivalent, but are on the same topic. The woman is playing the violin. The young lady enjoys listening to the guitar. • (0) The two sentences are on different topics. John went horse back riding at dawn with a whole group of friends. Sunrise at dawn is a magnificent view to take in if you wake up early enough for it. Figure 1: Annotation values with explanations and examples for the core STS task. In 2012 we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success: 35 teams participated with 88 system runs (Agirre et al., 2012). In addition, we held a DARPA sponsored workshop at Columbia University1 . In 2013, STS was selected as the official Shared Task of the *SEM 2013 conference. Accordingly, in STS 2013, we set up two tasks: The core task CORE, which is similar to the 2012 task; and a pilot task on typed-similarity TYPED between semi-structured records. For CORE, we provided all the STS 2012 data as training data, and the test data was drawn from related but different datasets. This is in contrast to the STS 2012 task where the train/test data were drawn from the same datasets. The 2012 datasets comprised the fo"
S13-1004,P98-1013,0,0.04578,"(OnWN) and FrameNet-WordNet (FnWN). These pairs are sampled based on the string similarity ranging from 0.4 to 0.9. String similarity is used to measure the similarity between a pair of glosses. The OnWN subset comprises 561 gloss pairs from OntoNotes 4.0 (Hovy et al., 2006) and WordNet 3.0 (Fellbaum, 1998). 370 out of the 561 pairs are sampled from the 110K sense-mapped pairs as made available from the authors. The rest, 291 pairs, are sampled from unmapped sense pairs with a string similarity ranging from 0.5 to 0.9. The FnWN subset has 189 manually mapped pairs of senses from FrameNet 1.5 (Baker et al., 1998) to WordNet 3.1. They are ran35 domly selected from 426 mapped pairs. In combination, both datasets comprise 750 pairs of glosses. 2.2 Typed-similarity TYPED task This task is devised in the context of the PATHS project,3 which aims to assist users in accessing digital libraries looking for items. The project tests methods that offer suggestions about items that might be useful to recommend, to assist in the interpretation of the items, and to support the user in the discovery and exploration of the collections. Hence the task is about comparing pairs of items. The pairs are generated in the E"
S13-1004,S12-1059,0,0.0590663,"_coefficient# Calculating_a_weighted_correlation a one-tailed parametric test based on Fisher’s ztransformation (Press et al., 2002, equation 14.5.10). 4.2 The Baseline Systems For the CORE dataset, we produce scores using a simple word overlap baseline system. We tokenize the input sentences splitting at white spaces, and then represent each sentence as a vector in the multidimensional token space. Each dimension has 1 if the token is present in the sentence, 0 otherwise. Vector similarity is computed using the cosine similarity metric. We also run two freely available sysˇ c et tems, DKPro (Bar et al., 2012) and TakeLab (Sari´ 5 al., 2012) from STS 2012, and evaluate them on the CORE dataset. They serve as two strong contenders since they ranked 1st (DKPro) and 2nd (TakeLab) in last year’s STS task. For the TYPED dataset, we first produce XML files for each of the items, using the fields as provided to participants. Then we run named entity recognition and classification (NERC) and date detection using Stanford CoreNLP. This is followed by calculating the similarity score for each of the types as follows. • General: cosine similarity of TF-IDF vectors of tokens from all fields. • Author: cosine s"
S13-1004,N12-1017,0,0.00854843,"ies on several (1-4) reference translations. HYTER, on the other hand, leverages millions of translations. The HTER set comprises 150 pairs, where one sentence is machine translation output and the corresponding sentence is a human post-edited translation. We sample the data from the dataset used in the DARPA GALE project with an HTER score ranging from 0 to 120. The HYTER set has 600 pairs from 3 subsets (each subset contains 200 pairs): a. reference Figure 3: Annotation instructions for TYPED task vs. machine translation. b. reference vs. Finite State Transducer (FST) generated translation (Dreyer and Marcu, 2012). c. machine translation vs. FST generated translation. The HYTER data set is used in (Dreyer and Marcu, 2012). The OnWN/FnWN dataset contains gloss pairs from two sources: OntoNotes-WordNet (OnWN) and FrameNet-WordNet (FnWN). These pairs are sampled based on the string similarity ranging from 0.4 to 0.9. String similarity is used to measure the similarity between a pair of glosses. The OnWN subset comprises 561 gloss pairs from OntoNotes 4.0 (Hovy et al., 2006) and WordNet 3.0 (Fellbaum, 1998). 370 out of the 561 pairs are sampled from the 110K sense-mapped pairs as made available from the au"
S13-1004,N06-2015,0,0.104937,"ation instructions for TYPED task vs. machine translation. b. reference vs. Finite State Transducer (FST) generated translation (Dreyer and Marcu, 2012). c. machine translation vs. FST generated translation. The HYTER data set is used in (Dreyer and Marcu, 2012). The OnWN/FnWN dataset contains gloss pairs from two sources: OntoNotes-WordNet (OnWN) and FrameNet-WordNet (FnWN). These pairs are sampled based on the string similarity ranging from 0.4 to 0.9. String similarity is used to measure the similarity between a pair of glosses. The OnWN subset comprises 561 gloss pairs from OntoNotes 4.0 (Hovy et al., 2006) and WordNet 3.0 (Fellbaum, 1998). 370 out of the 561 pairs are sampled from the 110K sense-mapped pairs as made available from the authors. The rest, 291 pairs, are sampled from unmapped sense pairs with a string similarity ranging from 0.5 to 0.9. The FnWN subset has 189 manually mapped pairs of senses from FrameNet 1.5 (Baker et al., 1998) to WordNet 3.1. They are ran35 domly selected from 426 mapped pairs. In combination, both datasets comprise 750 pairs of glosses. 2.2 Typed-similarity TYPED task This task is devised in the context of the PATHS project,3 which aims to assist users in acce"
S13-1004,2006.amta-papers.25,0,0.0305115,"the headlines come from a different EMM cluster, then we computed the string similarity between those pairs. Accordingly, we sampled 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sample another 375 pairs from the different EMM cluster in the same manner. The SMT dataset comprises pairs of sentences used in machine translation evaluation. We have two different sets based on the evaluation metric used: an HTER set, and a HYTER set. Both metrics use the TER metric (Snover et al., 2006) to measure the similarity of pairs. HTER typically relies on several (1-4) reference translations. HYTER, on the other hand, leverages millions of translations. The HTER set comprises 150 pairs, where one sentence is machine translation output and the corresponding sentence is a human post-edited translation. We sample the data from the dataset used in the DARPA GALE project with an HTER score ranging from 0 to 120. The HYTER set has 600 pairs from 3 subsets (each subset contains 200 pairs): a. reference Figure 3: Annotation instructions for TYPED task vs. machine translation. b. reference vs"
S13-1004,S12-1060,0,0.328958,"Missing"
S13-1004,C98-1013,0,\N,Missing
S14-2010,S14-2085,0,0.0392393,"Missing"
S14-2010,S14-2069,0,0.0326403,"Missing"
S14-2010,S14-2128,0,0.0328229,"Missing"
S14-2010,P13-1024,1,0.0618966,"data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs for Deft-forum and 300 pairs for Deft-news. They are sampled evenly from string similarities falling in the interval 0.6 to 1. The Tweets data set contains tweet-news pairs selected from the corpus released in (Guo et al., 2013), where each pair contains a sentence that pertains to the news title, while the other one represents a Twitter comment on that particular news. They are evenly sampled from string similarity values between 0.5 and 1. Table 1 shows the explanations and values associated with each score between 5 and 0. As in prior years, we used Amazon Mechanical Turk (AMT)3 to crowdsource the annotation of the English pairs.4 Annotators are presented with the Table 2: English subtask: Summary of train (2012 and 2013) and test (2014) datasets. a DARPA sponsored workshop at Columbia University.1 In 2013, STS wa"
S14-2010,S14-2112,0,0.0317369,"Missing"
S14-2010,N06-2015,0,0.0715746,"ferent similarity ranges, hence we built two sets of headline pairs: (i) a set where the pairs come from the same EMM cluster, (ii) and another set where the headlines come from a different EMM cluster, then we computed the string similarity between those pairs. Accordingly, we sampled 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sampled other 375 pairs from the different EMM cluster in the same manner. For OnWN, we used the sense definition pairs of OntoNotes (Hovy et al., 2006) and WordNet (Fellbaum, 1998). Different from previous tasks, the two definition sentences in a pair belong to different senses. We sampled 750 pairs based on a string similarity ranging from 0.5 to 1. The Images data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs"
S14-2010,S12-1051,1,0.623306,"r as both tasks have been defined to date in the literature) in that, rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). A quantifiable graded bidirectional notion of textual similarity is useful for a myriad of NLP tasks such as MT evaluation, information extraction, question answering, summarization, etc. In 2012 we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success: 35 teams participated with 88 system runs (Agirre et al., 2012). In addition, we held In Semantic Textual Similarity, systems rate the degree of semantic equivalence between two text snippets. This year, the participants were challenged with new data sets for English, as well as the introduction of Spanish, as a new language in which to assess semantic similarity. For the English subtask, we exposed the systems to a diversity of testing scenarios, by preparing additional OntoNotesWordNet sense mappings and news headlines, as well as introducing new genres, including image descriptions, DEFT discussion forums, DEFT newswire, and tweet-newswire headline map"
S14-2010,S14-2131,0,0.0336314,"Missing"
S14-2010,S14-2072,0,0.0817436,"Missing"
S14-2010,Q14-1018,0,0.0731,"Missing"
S14-2010,S14-2039,0,0.0993932,"Missing"
S14-2010,S14-2078,0,0.101731,"Missing"
S14-2010,S14-2022,0,0.0306687,"Missing"
S14-2010,S14-2046,0,0.022257,"Missing"
S14-2010,D13-1179,0,0.0175763,"Missing"
S14-2010,S12-1060,0,0.0199826,"Missing"
S14-2010,S14-2093,0,0.0281526,"Missing"
S14-2010,W10-0721,0,0.050451,"d 375 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity. We sampled other 375 pairs from the different EMM cluster in the same manner. For OnWN, we used the sense definition pairs of OntoNotes (Hovy et al., 2006) and WordNet (Fellbaum, 1998). Different from previous tasks, the two definition sentences in a pair belong to different senses. We sampled 750 pairs based on a string similarity ranging from 0.5 to 1. The Images data set is a subset of the PASCAL VOC-2008 data set (Rashtchian et al., 2010), which consists of 1,000 images and has been used by a number of image description systems. It was also sampled from string similarity values between 0.6 and 1. Deft-forum and Deft-news are from DEFT data.2 Deft-forum contains the forum post sentences, and Deft-news are news summaries. We selected 450 pairs for Deft-forum and 300 pairs for Deft-news. They are sampled evenly from string similarities falling in the interval 0.6 to 1. The Tweets data set contains tweet-news pairs selected from the corpus released in (Guo et al., 2013), where each pair contains a sentence that pertains to the new"
S14-2010,W10-0707,0,\N,Missing
S14-2010,P94-1019,0,\N,Missing
S14-2010,Q14-1017,0,\N,Missing
S14-2010,S14-2138,0,\N,Missing
S15-1009,P98-1013,0,0.149614,"the syntactic head of the text passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals"
S15-1009,baker-etal-2010-modality,0,0.0482113,"Missing"
S15-1009,W13-2322,0,0.0158937,"xt passage describing the proposition). We do not propose to develop our own semantic representation, but instead we will look to using existing relation and event representations based on the ACE program (Doddington et al., 2004). These have the advantage that there are offthe-shelf computational tools available for detecting ACE relations and events; they have the disadvantage that they do not cover all propositions we may be interested in. An alternative would be the use of a shallower semantic representation such as PropBank (Kingsbury et al., 2002), FrameNet (Baker et al., 1998), or AMR (Banarescu et al., 2013). 7.3 Entities as Targets In Section 6, we discussed an initial evaluation of a belief being about an entity. In this section we discuss further guidelines for identifying belief targets, i.e., when one can say that someone’s belief is about 89 a certain entity. In general, the notion of belief “aboutness” is fairly fuzzy and it may be difficult to circumscribe precisely without some additional constraints. Suppose then that one of the ultimate objectives of belief extraction is to populate a knowledge base with beliefs held about specific entities: individuals, groups, artifacts, etc., which"
S15-1009,W09-3012,1,0.690346,"space we do not provide an overview over all definitions. While at first the terms “belief” and “factuality” appear to relate to rather different things (a subjective state versus truth), in the NLP community they in fact refer to the same phenomenon, while having rather different connotations. The phenomenon is the communicative intention of a writer1 to present propositional content as something that she firmly believes is true, weakly believes is true, or has some other attitude towards, namely a wish or a reported belief. The term “belief” here describes the cognitive state of the writer (Diab et al., 2009), and comes from artificial intelligence and cognitive science, as in the Belief-Desire-Intention model of Bratman (1999 1987). The term “factuality” describes the communicative intention of the writer (Saur´ı and Pustejovsky, 2012, p. 263) (our emphasis): The fact that an eventuality is depicted as holding or not does not mean that this is the case in the world, but that this is how it is characterized by its informant. Similarly, it does not mean that this is the real knowledge that informant has (his true cognitive state regarding that event) but what he wants us to believe it is. We would"
S15-1009,doddington-etal-2004-automatic,1,0.834711,"sity/George Washington University, the Florida Institute for Human and Machine Cognition, and the University of Albany. The goal of our research project is not linguistic annotation, but the identification of meaning which is expressed in a non-linguistic manner. Such a meaning representation is useful for many applications; in our project we are specifically interested in knowledge base population. A different part of the DEFT program is concerned with the representation of propositional meaning, following the tradition of the ACE program in representing entities, relations and events (ERE) (Doddington et al., 2004). The work presented here is concerned with the attitude of agents towards propositional content: do the agents express a committed belief or a non-committed belief in the propositional content? Our work has several characteristics that set it apart from other work: we are interested in annotation which can be done fairly quickly; we are not interested in annotating linguistic elements (such as trigger words); and we are planning an integration with sentiment annotation. The structure of the paper is as follows: we start out by situating our notion of “belief” with respect to other notions of"
S15-1009,W10-3001,0,0.293703,"Missing"
S15-1009,P11-2102,0,0.0364363,"rotates around the earth, as was his (presumably) honest communicative intention. Therefore, to us as researchers interested in describing how language 2 Sarcasm and irony differ from lying in that the communicative intention and the cognitive state are aligned, but they do not align with the standard interpretation of the utterance. Here, the intention is that the reader recognizes that the form of the utterance does not literally express the cognitive state. We leave aside sarcasm and irony in this paper; for current computational work on sarcasm detection, see for example (Gonz´alez-Ib´an˜ ez et al., 2011). is used to communicate, it does not matter that astronomers now believe that Ptolemy was wrong, it does not change our account of communication and it does not change the communication that happened two millennia ago. And since we do not need to make the assumption that the writer knows what she is talking about, we choose not to make this assumption. In the case of Ptolemy, we leave this determination – what is actually true – to astronomers. In other cases, we typically have models of trustworthiness: if a writer sends her spouse a text message saying she is hungry, the spouse has no reaso"
S15-1009,P09-2078,0,0.0256017,"d, we could assume that the writer knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumptio"
S15-1009,P11-1032,0,0.0145471,"er knows what is true (assumption of truth). In this paper, we do not make this second assumption. We discuss these two assumptions in turn. We start with the assumption of truthfulness. In the quote above, Saur´ı and Pustejovsky (2012) (apart from distinguishing factuality from truth) also make the point that the writer’s communicative intention of making the reader believe she has a specific belief state does not mean that she actually has that cognitive state, since she may be lying. Lying is clearly an important phenomenon that researchers have looked into (Mihalcea and Strapparava, 2009; Ott et al., 2011).2 However, we (as linguists interested in understanding how language enables communication) feel that assuming the writer is truthful is a standard assumption about communication which we should in general make. This is because if we do not make this assumption, we cannot explain why communication is possible at all, since discourse participants would have no motivation to ever adopt another discourse participant’s belief as their own. We therefore do claim that we can infer belief from utterances, while assuming that the writer is not lying, and knowing that this assumption may be false in c"
S15-1009,C10-2117,1,0.747479,"on-committed belief in the annotations, the heuristic rules (mainly based on the presence of modal auxiliaries) that we added for the purpose of classifying the beliefs (CB, NCB, ROB, NA) did not work reliably in all cases. 4.3 System C System C uses a supervised learning approach to identify tokens denoting the heads of propositions that denote author’s expressed beliefs. It approaches this problem as a 5-way (CB, NCB, ROB, NA, nil) multi-class classification task at the word level. System C is adapted from a previous system which uses an earlier, simpler definition and annotation of belief (Prabhakaran et al., 2010). The system uses lexical and syntactic features for this task, which are extracted using the part-of-speech tags and dependency parses obtained from the Stanford CoreNLP system. In addition to the features described in (Prabhakaran et al., 2010), System C uses a set of new features including features based on a dictionary of hedge-words (Prokofieva and Hirschberg, 2014). The hedge features improved the NCB Fmeasure by around 2.2 percentage points (an overall F-measure improvement of 0.25 percentage points) in experiments conducted on a separate development set. It uses a quadratic kernel SVM"
S15-1009,W12-3807,1,0.915772,"Missing"
S15-1009,J12-2002,0,0.157236,"Missing"
S15-1009,W15-1304,1,0.70443,"ore, the FactBank annotation is basically compatible with ours. Our annotation is much simpler than that of FactBank in order to allow for a quicker annotation. We summarize the main points of simplification here. • We have taken the source always to be the writer. As we will discuss in Section 7.1, we will adopt the FactBank annotation in the next iteration of our annotation. • We do not distinguish between possible and probable; this distinction may be hard to annotate and not too valuable. • We ignore negation. If present, we simply assume it is part of the proposition which is the target. Werner et al. (2015) study the relation between belief and factuality in more detail. They provide an automatic way of mapping the annotations in FactBank to the 4-way distinction of speaker/writer’s belief that we present in this paper. 3.3 Corpus and Annotation Results The annotation effort for this phase of belief annotation for DEFT produced a training corpus of 852,836 words and an evaluation corpus of 100,037 words. All annotated data consisted of English text from discussion forum threads. The discussion forum threads were originally collected for the DARPA BOLT program, and were harvested from a wide vari"
S15-1009,C98-1013,0,\N,Missing
S15-1015,P12-1042,1,0.906269,"Missing"
S15-1015,D10-1111,0,0.0321599,"discussion fora, there has been a significant increase in documented political and ideological discussions. Automatically predicting the perspective or stance of users in such media is a challenging research 137 Chris Callison-Burch University of Pennsylvania Philadelphia, PA ccb@cis.upenn.edu problem that has a wide variety of applications including recommendation systems, targeted advertising, political polling, product reviews and even predicting possible future events. Ideology refers to the beliefs that influence an individual’s goals, expectations and views of the world (Van Dijk, 1998; Ahmed and Xing, 2010). The ideological perspective of a person is often expressed in his/her choice of discussed topics. People with opposing perspectives will choose to make different topics more salient. (Entman, 1993). From a social-science viewpoint, the notion of “perspective” is related to the concept of “framing”. Framing involves making some topics (or some aspects of the discussed topics) more prominent in order to promote the views and interpretations of the writer (communicator). The communicator makes these framing decisions either consciously or unconsciously (Entman, 1993). These decisions are often"
S15-1015,C12-1003,0,0.039367,"Missing"
S15-1015,P12-2013,1,0.889844,"Missing"
S15-1015,P12-1091,1,0.793103,"“support#v#1” whose Synset is support#v#1’, back_up#v#1. 4.3 Latent Semantics The next set of features relies on “Latent Semantics” which maps text from a high-dimensional space such as unigrams to a low-dimensional one such as topics. Most of these models assign a semantic profile to each given sentence (or document) by considering the observed words and assuming that each given document has a distribution over “K” topics. We apply (1) Latent Dirichlet Allocation (LDA) (Blei et al., 2003) as implemented in MALLET toolkit (McCallum, 2002), and (2) Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012) to each post. In addition to observed words, WTMF also models missing ones namely explicitly modeling what the post is not about. WTMF defines missing words as the whole vocabulary of the training data minus the ones observed in the given document. 4.3.1 Number of Topics We vary the number of topics (K) between 100 and 500 (with a step-size of 100) and use the best “K” for each dataset. We define the best K, for each of LDA and WTMF, as the one that yields the best cross-validation results when combined with unigram features. The best K value for LDA is 400 for PCC and Abortion, 500 for Creat"
S15-1015,C12-2045,0,0.252418,"Missing"
S15-1015,P13-2142,0,0.302685,"Missing"
S15-1015,P10-2047,0,0.412334,"Missing"
S15-1015,W06-2915,0,0.668037,"Missing"
S15-1015,P05-3019,0,0.0119828,"hows the size of the training and test data in the ANES and Ideological-Debates datasets. 141 4.1 Approach Preprocessing We apply basic preprocessing to the text by separating punctuation and numbers from words. All punctuation and numbers are then ignored when training the classifier for all of our systems including the unigram baseline. The intuition behind this is that punctuation and numbers do not capture the perspective of a person but rather the writing style. Moreover, by ignoring them, we avoid overfitting the training data. 4.2 Word Sense Disambiguation (WSD) We use WN-Sense-Relate (Patwardhan et al., 2005) to perform word sense disambiguation. SenseRelate uses WordNet (Miller, 1995) to tag each word with the part-of-speech and sense-id. The only parts of speech that are handled by WN-Sense-Relate are adjectives (a), adverbs (r), verbs (v) and nouns (n). In addition to the part-of-speech and sense-id, WNSense-Relate also identifies and tags compounds. The word sense tagging process can be either contextual or can rely on the most frequent sense. We experiment with both variants. 4.2.1 Contextual WSD (WSD-CXT) In this variant of WSD, in addition to tagging compounds, we contextually disambiguate"
S15-1015,W10-0214,0,0.0791204,"0 106 7,142 159 88 84 67 2,734 2,160 2,842 1,588 24 18 14 15 Table 5: Statistics of the training and test sets for both the ANES and the four domains of the IdeologicalDebates datasets. debt.”, we replace “They” with “Republicans”. 4 3.2 Our goal is to determine whether semantic features help in identifying a person’s ideological perspective as determined by his/her answer to the PCC constrained question in the “ANES” dataset and his/her stance towards the ideological-topics discussed in the “Ideological-Debates” dataset independently. Ideological Debates Dataset This dataset was collected by Somasundaran and Wiebe (2010) . It contains debate posts from six domains; (a) Abortion, (b) Creationism, (c) GayRights, (d) Gun-Rights, (e) Healthcare and (f) Existence of God. Each domain represents an ideological topic with two possible perspectives, pro and against. Similar to the work of (Somasundaran and Wiebe, 2010), we use the first four domains to evaluate our approach. Table 2 shows the class distribution in each of these four domains while table 4 lists some sample posts. It should be noted that our results are not comparable to those obtained by (Somasundaran and Wiebe, 2010), since they used a subset of the p"
S15-1015,W10-0723,0,0.199649,"Missing"
S15-2045,agerri-etal-2014-ixa,1,0.57453,"onal and knowledge-based similarity are widely used, and also syntactic analysis and named entity recognition. Most teams add a machine learning algorithm to learn the output scores, but note that Samsung team did not use it in their best run. 4 3.6 The baseline system used for the interpretable subtask consists of a cascade concatenation of several procedures. First, we undertake a brief NLP step in which input sentences are tokenized using simple regular expressions. Additionally, this step collects chunk regions coming either from gold standard or from the chunking done by ixa-pipes-chunk (Agerri et al., 2014). This is followed by a lowercased token aligning phase, which consists of aligning (or linking) identical tokens across the input sentences. Then we use chunk boundaries as token regions to group individual tokens into groups, and compute all links across groups. The weight of the link across groups is proportional to the number of links counted between within-group tokens. The next phase consists of an optimization step in which groups x,y that have the highest link weight are identified, as well as the chunks that are linked to either x or y but not with a maximum alignment weight (thus ena"
S15-2045,S12-1051,1,0.519741,"TS also differs from both TE and paraphrasing (in as far as both tasks have been defined to date in the literature) in that rather than being a binary yes/no decision (e.g. a vehicle is not a car), we define STS to be a graded similarity notion (e.g. a vehicle and a car are more similar than a wave and a car). A quantifiable graded bidirectional notion of textual similarity is useful for many NLP tasks such as MT evaluation, information extraction, question answering, summarization. In 2012, we held the first pilot task at SemEval 2012, as part of the *SEM 2012 conference, with great success (Agirre et al., 2012). In addition, we 252 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252–263, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics held a DARPA sponsored workshop at Columbia University.1 In 2013, STS was selected as the official shared task of the *SEM 2013 conference, with two subtasks: a core task, which was similar to the 2012 task, and a pilot task on typed-similarity between semi-structured records. In 2014, new datasets including new genres were used, and we expanded the evaluations to address sentence similarity"
S15-2045,S14-2010,1,0.800447,"f the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 252–263, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics held a DARPA sponsored workshop at Columbia University.1 In 2013, STS was selected as the official shared task of the *SEM 2013 conference, with two subtasks: a core task, which was similar to the 2012 task, and a pilot task on typed-similarity between semi-structured records. In 2014, new datasets including new genres were used, and we expanded the evaluations to address sentence similarity in a new language, namely Spanish (Agirre et al., 2014). This year we presented three subtasks: the English subtask, the Spanish subtask and the interpretable pilot subtask. The English subtask comprised pairs from headlines and image descriptions, and it also introduced new genres, including answer pairs from a tutorial dialogue system and from Q&A websites, and pairs from a dataset tagged with committed belief annotations. For the Spanish subtask, additional pairs from news and Wikipedia articles were selected. The annotations for both tasks leveraged crowdsourcing. Finally, with the interpretable STS pilot subtask, we wanted to start exploring"
S15-2045,P14-1023,0,0.0115737,"7070 0.7251 0.7311 0.7250 0.7422 0.6364 0.7775 0.7032 0.7130 0.7189 0.4616 0.7533 0.6111 0.5379 0.5424 0.5672 0.6558 0.4919 0.5912 0.6964 0.7114 0.6364 Table 3: Task 2a: English evaluation results in terms of Pearson correlation. 259 Rank 61 42 29 63 56 57 70 69 71 62 44 49 43 74 72 73 34 28 26 1 3 5 19 18 16 8 9 2 12 13 23 41 59 20 47 22 11 15 33 45 55 24 10 17 50 51 52 4 7 6 46 36 27 39 31 30 32 25 53 14 40 37 35 68 21 58 66 65 64 48 67 60 42 38 54 approach for the top three participants (DLS@CU, ExBThemis, Samsung). They use WordNet (Miller, 1995), Mikolov Embeddings (Mikolov et al., 2013; Baroni et al., 2014) and PPDB (Ganitkevitch et al., 2013). In general, generic NLP tools such as lemmatization, PoS tagging, distributional word embeddings, distributional and knowledge-based similarity are widely used, and also syntactic analysis and named entity recognition. Most teams add a machine learning algorithm to learn the output scores, but note that Samsung team did not use it in their best run. 4 3.6 The baseline system used for the interpretable subtask consists of a cascade concatenation of several procedures. First, we undertake a brief NLP step in which input sentences are tokenized using simple"
S15-2045,N13-1092,0,0.0796576,"the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. For interpretable STS the similarity scores range from 0 to 5, as in the English subtask. With respect to the relation between the aligned chunks, the present pilot only allowed 1:1 alignments. As a consequence, we had to include a special alignment context tag (ALIC) to simulate those chunks which had some semantic similarity or relatedness in the other sentence, but could not have been aligned because of"
S15-2045,P13-2080,0,0.0179598,"Contrary to the mentioned works, we first identified the segments (chunks in our case) in each sentence separately, and then aligned them. In a different strand of work, Nielsen et al. (2009) defined a textual entailment model where the “facets” (words under some syntactic/semantic relation) in the response of a student were linked to the concepts in the reference answer. The link would signal whether each facet in the response was entailed by the reference answer or not, but would not explicitly mark which parts of the reference answer caused the entailment. This model was later followed by Levy et al. (2013). Our task was different in that we identified the corresponding chunks in both sentences. We think that, in the future, the aligned facets could provide complementary information to chunks. For interpretable STS the similarity scores range from 0 to 5, as in the English subtask. With respect to the relation between the aligned chunks, the present pilot only allowed 1:1 alignments. As a consequence, we had to include a special alignment context tag (ALIC) to simulate those chunks which had some semantic similarity or relatedness in the other sentence, but could not have been aligned because of"
S15-2045,W10-0721,0,0.0157602,"ns student answers Q&A forum answers commited belief Table 2: English subtask: Summary of train (2012, 2013, 2014) and test (2015) datasets. lines come from a different EMM cluster. Then, we computed the string similarity between those pairs. Accordingly, we sampled 1000 headline pairs of headlines that occur in the same EMM cluster, aiming for pairs equally distributed between minimal and maximal similarity using simple string similarity as a metric. We sampled another 1000 pairs from the different EMM cluster in the same manner. The Images dataset is a subset of the PASCAL VOC-2008 dataset (Rashtchian et al., 2010), which consists of 1000 images with around 10 descriptions each, and has been used by a number of image description systems. It was also sampled using string similarity, discarding those that had been used in previous years. We organized two bins with 1000 pairs each: one with pairs of descriptions from the same image, and the other one with pairs of descriptions from different images. The source of the Answers-student pairs is the BEETLE corpus (Dzikovska et al., 2010), which is a question-answer dataset collected and annotated during the evaluation of the BEETLE II tutorial dialogue system."
S15-2045,W00-0726,0,0.313421,"Missing"
S15-2045,S12-1060,0,0.211502,"Missing"
S15-2045,W10-0707,0,\N,Missing
S16-1070,P98-1013,0,0.0466645,"Missing"
S16-1070,S10-1059,0,0.0273706,"Favor” and “Against” classes) Abortion Atheism Climate Clinton Feminism All-Targets Majority-BL 40.3 42.11 6.11 36.83 39.1 36.41 n-grams 57.43 54.98 36.71 51.59 48.37 60.41 WTMF 48.08 57.73 39.83 45.81 52.25 62.27 Semantic 57.25 48.77 39.48 39.89 54.29 63.1 All 60.52 56.39 38.89 55.82 51.93 63.6 Table 4: Held-Out Test Set Results (measured in average Fβ=1 score of “Favor” and “Against” classes) frame is called the “frame-target”. For example, the target for the frame “Killing” can be any kill verb and the frame elements will include the killer and the victim. We use SEMAFOR (Das et al., 2010; Chen et al., 2010), a publicly available frame-semantic parser, to identify all the semantic frames in each given tweet. For example, in the tweet “Because I want young American women to be able to be proud of the 1st woman president.”, SEMAFOR identifies the following frames, “Leadership Target:president”, “Capability Target:able”, “Origin Target:American”,“Desiring Target:want”, “People Target:women” and “Age Target:young” . We create a list of all the frames that occur in the training data and use binary features to indicate the presence/absence of each of them in each given tweet. This set of features provi"
S16-1070,S15-1015,1,0.748097,"can either make these framing decisions consciously or unconsciously (Entman, 1993). In this paper, we present a system that employs lexical and semantic features to identify the stance of a person – “Favor”, “Against” or “None”– on topics that are often backed up by one’s ideology or belief system such as abortion, climate change and feminism. We evaluate the performance of the proposed system through the participation in Sub-Task A “Supervised Framework” of SemEval Task 6 “Detecting Stance in Tweets”. (Mohammad et al., 2016) We build on our previous work on automatic perspective detection (Elfardy et al., 2015) by exploring the use of sentiment, Linguistic Inquiry and Word Count (LIWC) dictionaries, frame and latent semantics in addition to standard lexical features to automatically classify a given tweet according to its stance on the topic of interest. Most current computational linguistics research on supervised stance detection performs document (or post) level stance classification, whether binary or multiway and use a variety of lexical, syntactic and semantic features to identify the stance of a post towards a specific contentious topic/target such as the Israeli-Palestinian conflict, abortio"
S16-1070,P12-1091,1,0.801318,"verting all words to lower case. Converting the text to lower case is intended to reduce the sparseness of the data while excluding the punctuation and number is meant to avoid overfitting the training data. We use n-grams having a length between 1 and 3 and exclude the ones that occur in only one training instance. 3.2 Latent Semantics such as topics. Most of these models assign a semantic profile to each given sentence (or document) by considering the observed words and assuming that each given document has a distribution over K topics. We apply Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012) to each tweet. The advantage WTMF offers –over standard topic models– is that in addition to modeling observed words, it models missing ones. WTMF defines missing words as the whole vocabulary of the training data minus the ones observed in the given tweet/post. Modeling missing words is particularly useful when the input is very short in length because in such case, only a limited context of observed words is present. We use the distributable version of WTMF which is trained on WordNet (Fellbaum, 2010), Wiktionary definitions1 and the Brown Corpus2 and sets the number of topics (K) to 100. 3"
S16-1070,C12-2045,0,0.0197242,"ard lexical features to automatically classify a given tweet according to its stance on the topic of interest. Most current computational linguistics research on supervised stance detection performs document (or post) level stance classification, whether binary or multiway and use a variety of lexical, syntactic and semantic features to identify the stance of a post towards a specific contentious topic/target such as the Israeli-Palestinian conflict, abortion, creationism, gun-rights, gay-rights, healthcare, legalization of marijuana and death penalty (Lin et al., 2006; Klebanov et al., 2010; Hasan and NG, 2012; Hasan and Ng, 2013; Somasundaran and Wiebe, 2010; Elfardy et al., 2015). For a detailed literature review, we direct the reader to our *Sem paper. (Elfardy et al., 2015) 2 Shared Task Description SemEval Task 6 “Detecting Stance in Tweets” (Mohammad et al., 2016) aims at evaluating how well an automated system can identify the stance of a 434 Proceedings of SemEval-2016, pages 434–439, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Training Held-Out Test Abortion Atheism Climate Clinton Feminism Abortion Atheism Climate Clinton Feminism Favor 105 92"
S16-1070,P13-2142,0,0.0187227,"to automatically classify a given tweet according to its stance on the topic of interest. Most current computational linguistics research on supervised stance detection performs document (or post) level stance classification, whether binary or multiway and use a variety of lexical, syntactic and semantic features to identify the stance of a post towards a specific contentious topic/target such as the Israeli-Palestinian conflict, abortion, creationism, gun-rights, gay-rights, healthcare, legalization of marijuana and death penalty (Lin et al., 2006; Klebanov et al., 2010; Hasan and NG, 2012; Hasan and Ng, 2013; Somasundaran and Wiebe, 2010; Elfardy et al., 2015). For a detailed literature review, we direct the reader to our *Sem paper. (Elfardy et al., 2015) 2 Shared Task Description SemEval Task 6 “Detecting Stance in Tweets” (Mohammad et al., 2016) aims at evaluating how well an automated system can identify the stance of a 434 Proceedings of SemEval-2016, pages 434–439, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Training Held-Out Test Abortion Atheism Climate Clinton Feminism Abortion Atheism Climate Clinton Feminism Favor 105 92 212 112 210 46 32 1"
S16-1070,P10-2047,0,0.0235391,"cs in addition to standard lexical features to automatically classify a given tweet according to its stance on the topic of interest. Most current computational linguistics research on supervised stance detection performs document (or post) level stance classification, whether binary or multiway and use a variety of lexical, syntactic and semantic features to identify the stance of a post towards a specific contentious topic/target such as the Israeli-Palestinian conflict, abortion, creationism, gun-rights, gay-rights, healthcare, legalization of marijuana and death penalty (Lin et al., 2006; Klebanov et al., 2010; Hasan and NG, 2012; Hasan and Ng, 2013; Somasundaran and Wiebe, 2010; Elfardy et al., 2015). For a detailed literature review, we direct the reader to our *Sem paper. (Elfardy et al., 2015) 2 Shared Task Description SemEval Task 6 “Detecting Stance in Tweets” (Mohammad et al., 2016) aims at evaluating how well an automated system can identify the stance of a 434 Proceedings of SemEval-2016, pages 434–439, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Training Held-Out Test Abortion Atheism Climate Clinton Feminism Abortion Atheism Climate Clinton F"
S16-1070,W06-2915,0,0.0416949,"and latent semantics in addition to standard lexical features to automatically classify a given tweet according to its stance on the topic of interest. Most current computational linguistics research on supervised stance detection performs document (or post) level stance classification, whether binary or multiway and use a variety of lexical, syntactic and semantic features to identify the stance of a post towards a specific contentious topic/target such as the Israeli-Palestinian conflict, abortion, creationism, gun-rights, gay-rights, healthcare, legalization of marijuana and death penalty (Lin et al., 2006; Klebanov et al., 2010; Hasan and NG, 2012; Hasan and Ng, 2013; Somasundaran and Wiebe, 2010; Elfardy et al., 2015). For a detailed literature review, we direct the reader to our *Sem paper. (Elfardy et al., 2015) 2 Shared Task Description SemEval Task 6 “Detecting Stance in Tweets” (Mohammad et al., 2016) aims at evaluating how well an automated system can identify the stance of a 434 Proceedings of SemEval-2016, pages 434–439, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Training Held-Out Test Abortion Atheism Climate Clinton Feminism Abortion At"
S16-1070,S16-1003,0,0.0617522,"Missing"
S16-1070,D13-1170,0,0.00349465,"e number of topics (K) to 100. 3.3 As mentioned earlier, sentiment analysis is closely related to stance detection. The ideological stance of a person normally influences his/her sentiment towards different ideological topics. Accordingly we decide to identify the overall sentiment of each sentence in the given tweet and use this sentiment as a possible indicator of a tweeter’s stance. Our assumption is that since tweets are inherently short in length, we can assume that the expressed sentiment targets the ideological topic the tweet is discussing. We use Stanford’s sentiment analysis system (Socher et al., 2013) to identify the number of positive, negative and neutral sentences in a given tweet and use these three counts as features for our 1 Latent Semantics map text from a high-dimensional space such as n-grams to a low-dimensional one 435 Sentiment http://en.wiktionary.org http://en.wikipedia.org/wiki/Brown _Corpus 2 Abortion Feeling 8.93 Biological Pr. 2.81 Body 49.59 Health 12.07 Sexual 35.37 Ingestion 30.74 Relativity 1.32 Motion 73.39 Relativity 15.7 Space 48.93 Time 45.62 Work 16.2 Achievement 20.66 Leisure 6.61 Home 3.14 Money 7.27 Religion 15.7 Death 17.36 Assent 3.97 Atheism 5.83 4.08 27."
S16-1070,W10-0214,0,0.10195,"assify a given tweet according to its stance on the topic of interest. Most current computational linguistics research on supervised stance detection performs document (or post) level stance classification, whether binary or multiway and use a variety of lexical, syntactic and semantic features to identify the stance of a post towards a specific contentious topic/target such as the Israeli-Palestinian conflict, abortion, creationism, gun-rights, gay-rights, healthcare, legalization of marijuana and death penalty (Lin et al., 2006; Klebanov et al., 2010; Hasan and NG, 2012; Hasan and Ng, 2013; Somasundaran and Wiebe, 2010; Elfardy et al., 2015). For a detailed literature review, we direct the reader to our *Sem paper. (Elfardy et al., 2015) 2 Shared Task Description SemEval Task 6 “Detecting Stance in Tweets” (Mohammad et al., 2016) aims at evaluating how well an automated system can identify the stance of a 434 Proceedings of SemEval-2016, pages 434–439, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics Training Held-Out Test Abortion Atheism Climate Clinton Feminism Abortion Atheism Climate Clinton Feminism Favor 105 92 212 112 210 46 32 123 45 58 Against 334 304 15 36"
S16-1070,C98-1013,0,\N,Missing
S16-1081,S16-1103,0,0.0432732,"n of Sultan et al. (2015)’s very successful STS model enhanced with additional features found to work well in the literature. The team in second place overall, UWB, combines a large number of diverse similarity models and features (Brychcin and Svoboda, 2016). Similar to Samsung, UWB includes both manually engineered NLP features (e.g., character n-gram overlap) with sophisticated models from deep learning (e.g., Tree LSTMs). The third place team, MayoNLPTeam, also achieves their best results using a combination of a more traditionally engineered NLP pipeline with a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The nex"
S16-1081,S12-1051,1,0.454212,"for replicating human judgements regarding the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translat"
S16-1081,S13-1004,1,0.536348,"n judgements regarding the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techni"
S16-1081,S14-2010,1,0.564132,"g the degree to which a translation generated by an machine translation system corresponds to a reference translation produced by a human translator. STS systems plausibly could be used as a drop-in replacement for existing translation evaluation metrics (e.g., BLEU, MEANT, ME498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fl"
S16-1081,S16-1101,1,0.859309,"Missing"
S16-1081,S16-1086,0,0.0343004,"Missing"
S16-1081,P98-1013,0,0.0704238,"E498 TEOR, TER).1 The cross-lingual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fluency independent from adequacy, it is possible that the deeper semantic assessment being performed by STS systems could benefit from being paired with a separate fluency module. evaluation adds new data sets drawn from plagiarism detection and post-edited machine translation"
S16-1081,S16-1117,0,0.0317832,"Missing"
S16-1081,S16-1089,0,0.463664,"STS. The overall winner, Samsung Poland NLP Team, proposes a textual similarity model that is a novel hybrid of recursive auto-encoders from deep learning with penalty and reward signals extracted from WordNet (Rychalska et al., 2016). To obtain even better performance, this model is combined in an ensemble with a number of other similarity models including a version of Sultan et al. (2015)’s very successful STS model enhanced with additional features found to work well in the literature. The team in second place overall, UWB, combines a large number of diverse similarity models and features (Brychcin and Svoboda, 2016). Similar to Samsung, UWB includes both manually engineered NLP features (e.g., character n-gram overlap) with sophisticated models from deep learning (e.g., Tree LSTMs). The third place team, MayoNLPTeam, also achieves their best results using a combination of a more traditionally engineered NLP pipeline with a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structure"
S16-1081,D15-1181,0,0.0283646,"nik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engineering. Finally, the best performing system on the postediting data, RICOH’s Run-n, introduces a novel IRbased approach for textual similarity that incorporates word alignment information (Itoh, 2016). 6"
S16-1081,S16-1170,0,0.023392,"s such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engineering. Finally, the best performing system on the postediting data, RICOH’s Run-n, introduces a novel IRbased"
S16-1081,N06-2015,0,0.0113871,"ual STS subtask that is newly introduced this year is similarly related to machine translation quality estimation. The STS shared task has been held annually since 2012, providing a venue for the evaluation of state-of-the-art algorithms and models (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015). During this time, a diverse set of genres and data sources have been explored (i.a., news headlines, video and image descriptions, glosses from lexical resources including WordNet (Miller, 1995; Christiane Fellbaum, 1998), FrameNet (Baker et al., 1998), OntoNotes (Hovy et al., 2006), web discussion forums, and Q&A data sets). This year’s 1 Both monolingual and cross-lingual STS score what is referred to in the machine translation literature as adequacy and ignore fluency unless it obscures meaning. While popular machine translation evaluation techniques do not assess fluency independent from adequacy, it is possible that the deeper semantic assessment being performed by STS systems could benefit from being paired with a separate fluency module. evaluation adds new data sets drawn from plagiarism detection and post-edited machine translations. We also introduce an evaluat"
S16-1081,S16-1106,0,0.0956972,"ween the best and median scores to highlight the extent to which top scoring systems outperformed the typical level of performance achieved on each data set. The best overall performance is obtained by Samsung Poland NLP Team’s EN1 system, which achieves an overall correlation of 0.778 (Rychalska et al., 2016). This system also performs best on three out of the five individual evaluation sets: answer-answer, headlines, plagiarism. The EN1 system achieves competitive performance on the postediting data with a correlation score of 0.83516. The best system on the postediting data, RICOH’s Run-n (Itoh, 2016), obtains a score of 0.867. Like all systems, EN1 struggles on the question-question data, achieving a correlation of 0.687. Another system submitted by the Samsung Poland NLP Team named 15 The median scores reported here do not include late or corrected systems. The median scores for the on-time systems without corrections are: ALL 0.68923; plagiarism 0.78949; answer-answer 0.48018; postediting 0.81241; headlines 0.76439; question-question 0.57140. Team Run ALL Ans.-Ans. HDL Plagiarism Postediting Ques.-Ques. Samsung Poland NLP Team UWB MayoNLPTeam Samsung Poland NLP Team NaCTeM ECNU UMD-TTIC"
S16-1081,P14-2124,0,0.0245172,"roximately 0.25 drop in correlation on the news data as compare to the multi-source setting; 2) systems performing evenly on both data sets. 6.5.1 Methods In terms of approaches, most runs rely on a monolingual framework. They automatically translate the Spanish member of a sentence pair into English and then compute monolingual semantic similarity using a system developed for English. In contrast, the CNRC team (Lo et al., 2016) provides a true crosslingual system that makes use of embedding space phrase similarity, the score from XMEANT, a crosslingual machine translation evaluation metric (Lo et al., 2014), and precision and recall features for material filling aligned cross-lingual semantic roles (e.g., action, agent, patient). The FBK HLT team (Ataman et al., 2016) proposes a model combining cross-lingual word embeddings with features from QuEst (Specia et al., 2013), a tool for machine translation quality estimation. The RTM system (Bic¸ici, 2016) also builds on methods developed for machine translation quality estimation and is applicable to both cross-lingual and monolingual similarity. The GWU NLP team (Aldarmaki and Diab, 2016) uses a shared cross-lingual vector space to directly assess"
S16-1081,S16-1102,0,0.0363344,"Missing"
S16-1081,P14-5010,0,0.0120217,"data sources we use for the evaluation sets. 3.1.1 Selection Heuristics Unless otherwise noted, pairs are heuristically selected using a combination of lexical surface form and word embedding similarity between a candidate pair of text snippets. The heuristics are used to find pairs sharing some minimal level of either surface or embedding space similarity. An approximately equal number of candidate sentence pairs are produced using our lexical surface form and word embedding selection heuristics. Both heuristics make use of a Penn Treebank style tokenization of the text provided by CoreNLP (Manning et al., 2014). 500 year 2016 2016 2016 dataset Trial News Multi-source pairs 103 301 294 source Sampled ≤ 2015 STS en-es news articles en news headlines, short-answer plag., MT postedits, Q&A forum answers, Q&A forum questions Table 3: Spanish-English subtask: Trial and test data sets. Surface Lexical Similarity Our surface form selection heuristic uses an information theoretic measure based on unigram overlap (Lin, 1998). As shown in equation (1), surface level lexical similarity between two snippets s1 and s2 is computed as a log probability weighted sum of the words common to both snippets divided by a"
S16-1081,D14-1162,0,0.109685,"Missing"
S16-1081,S16-1093,0,0.0144173,"ased model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without any manual feature engin"
S16-1081,S16-1091,0,0.0291873,"representations of the two snippets. 6.4 English Subtask The rankings for the English STS subtask are given in Tables 4 and 5. The baseline system ranked 100th. Table 6 provides the best and median scores for each of the individual evaluation sets as well as overall.15 The table also provides the difference between the best and median scores to highlight the extent to which top scoring systems outperformed the typical level of performance achieved on each data set. The best overall performance is obtained by Samsung Poland NLP Team’s EN1 system, which achieves an overall correlation of 0.778 (Rychalska et al., 2016). This system also performs best on three out of the five individual evaluation sets: answer-answer, headlines, plagiarism. The EN1 system achieves competitive performance on the postediting data with a correlation score of 0.83516. The best system on the postediting data, RICOH’s Run-n (Itoh, 2016), obtains a score of 0.867. Like all systems, EN1 struggles on the question-question data, achieving a correlation of 0.687. Another system submitted by the Samsung Poland NLP Team named 15 The median scores reported here do not include late or corrected systems. The median scores for the on-time sy"
S16-1081,P13-4014,0,0.0272301,"Missing"
S16-1081,2011.eamt-1.12,0,0.00782041,"swers. This corpus provides a collection of short answers to computer science questions that exhibit varying degrees of plagiarism from related Wikipedia articles.4 The short answers include text that was constructed by each of the following four strategies: 1) copying and pasting individual sentences from Wikipedia; 2) light revision of material copied from Wikipedia; 3) heavy revision of material from Wikipedia; 4) non-plagiarised answers produced without even looking at Wikipedia. This corpus is segmented into individual sentences using CoreNLP (Manning et al., 2014). 3.1.4 Postediting The Specia (2011) EAMT 2011 corpus provides machine translations of French news data using the Moses machine translation system (Koehn et al., 2007) paired with postedited corrections of those translations.5 The corrections were provided by human translators instructed to perform the minimum useful for finding semantically similar text snippets that differ in surface form. 4 Questions: A. What is inheritance in object orientated programming?, B. Explain the PageRank algorithm that is used by the Google search engine, C. Explain the Vector Space Model that is used for Information Retrieval., D. Explain Bayes Th"
S16-1081,S15-2027,0,0.0111195,"r to be significantly worse than the monolingual submissions even though the systems are being asked to perform the more challenging problem of evaluating crosslingual sentence pairs. While the correlations are not directly comparable, they do seem to motivate a more direct comparison between cross-lingual and monolingual STS systems. In terms of performance on the manually culled news data set, the highest overall rank is achieved by an unsupervised system submitted by team UWB (Brychcin and Svoboda, 2016). The unsupervised UWB system builds on the word alignment based STS method proposed by Sultan et al. (2015). However, when calculating the final similarity score, it weights both the aligned and unaligned words by their inverse document frequency. This system is able to attain a 0.912 correlation on the news data, while ranking second on the multi-source data set. For the multi-source test set, the highest scoring submission is a supervised system from the UWB team that combines multiple signals originating from lexical, syntactic and semantic similarity approaches in a regression-based model, achieving a 0.819 correlation. This is modestly better than the second place unsupervised approach that ac"
S16-1081,S16-1094,0,0.00995156,"th a deep learning based model (Afzal et al., 2016). Specifically, MayoNLPTeam combines a pipeline that makes use of linguistic resources such as WordNet and well understood concepts such as the information content of a word (Resnik, 1995) with a deep learning method known as Deep Structured Semantic Model (DSSM) (Huang et al., 2013). The next two teams in overall performance, ECNU and NaCTeM, make use of large feature sets, including features based on word embeddings. However, they did not incorporate the more sophisticated deep learning based models explored by Samsung, UWB and MayoNLPTeam (Tian and Lan, 2016; Przybyła et al., 2016). The next team in the rankings, UMD-TTIC-UW, only makes use of a single deep learning model (He et al., 2016). The team extends a multi-perspective convolutional neural network (MPCNN) (He et al., 2015) with a simple word level attentional mecha17 To see how much of this is related to the Q&A domain in particular, we will investigate including difficult non-Q&A evaluation data in future STS competitions. nism based on the aggregate cosine similarity of a word in one text with all of the words in a paired text. The submission is notable for how well it performs without"
S16-1081,C98-1013,0,\N,Missing
S16-1081,P07-2045,0,\N,Missing
S16-1101,eisele-chen-2010-multiun,0,0.0302902,"of 700K sentences derived from various resources. We extract and combine the following sets: a random set of 150K sentences from LDC’s English Gigaword fifth edition (Parker et al., 2011), a random set of 150K sentences from the English Wikipedia3 , the Brown Corpus (Francis, 1964), Wordnet (Miller, 1995) and Wiktionary4 definitions appended with examples. Bilingual Data: We extract training data for the bilingual models from WMT13 (Mach´acˇ ek and Bojar, 2013) sentence-aligned parallel corpora, specifically version 7 of the EuroParl parallel corpus (Koehn, 2005), the multiUN parallel corpus (Eisele and Chen, 2010), and news commentary data. We train the bilingual model using a sample of 1M sentence pairs from these datasets. All sentences in our data are tokenized and stemmed, and number sequences are replaced with a special token as a normalization step. We use the Stanford CoreNLP toolkit (Manning et al., 2014) for English preprocessing, and Treetagger tools (Schmid, 1995) for Spanish. Words that appear less than 5 times in the training set are discarded from the vocabulary. 4.2 Parameter Settings We train our bilingual b-WMF models strictly using the bilingual parallel data. On the other hand, we tr"
S16-1101,P12-1091,1,0.928344,"ng between a pair of variablelength textual snippets, such as sentences. Using unsupervised vector space models, words and sentences can be mapped into dense vector representations that capture implicit syntactic and semantic information. These representations can then be directly compared using well-known distance or similairty measures, such as the Euclidean distance or cosine similarity, which reflect their overall semantic relatedness. Such distributed representations of words, or word embeddings, can be learned using global word co-occurrence statistics as in matrix factorization models (Guo and Diab, 2012; Pennington et al., 2014), or using local context as in neural probabilistic language models (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2013). A variable-length sentence can be mapped into a fixedlength vector either by combining word embeddings In crosslingual STS, the challenge is to compare sentences from two different languages. We address this problem by directly learning crosslingual vector representations for words and sentences, which allows us to calculate the STS scores without the need for explicit translation or mapping. Several models can be used for learnin"
S16-1101,C12-1089,0,0.0771158,"Missing"
S16-1101,2005.mtsummit-papers.11,0,0.0443117,"l English model, the training set consists of 700K sentences derived from various resources. We extract and combine the following sets: a random set of 150K sentences from LDC’s English Gigaword fifth edition (Parker et al., 2011), a random set of 150K sentences from the English Wikipedia3 , the Brown Corpus (Francis, 1964), Wordnet (Miller, 1995) and Wiktionary4 definitions appended with examples. Bilingual Data: We extract training data for the bilingual models from WMT13 (Mach´acˇ ek and Bojar, 2013) sentence-aligned parallel corpora, specifically version 7 of the EuroParl parallel corpus (Koehn, 2005), the multiUN parallel corpus (Eisele and Chen, 2010), and news commentary data. We train the bilingual model using a sample of 1M sentence pairs from these datasets. All sentences in our data are tokenized and stemmed, and number sequences are replaced with a special token as a normalization step. We use the Stanford CoreNLP toolkit (Manning et al., 2014) for English preprocessing, and Treetagger tools (Schmid, 1995) for Spanish. Words that appear less than 5 times in the training set are discarded from the vocabulary. 4.2 Parameter Settings We train our bilingual b-WMF models strictly using"
S16-1101,W13-2202,0,0.024851,"ntence-aligned data. 4 Model b-WMF x-WMF UWB Empirical Evaluation 4.1 Data Monolingual Data: For the monolingual English model, the training set consists of 700K sentences derived from various resources. We extract and combine the following sets: a random set of 150K sentences from LDC’s English Gigaword fifth edition (Parker et al., 2011), a random set of 150K sentences from the English Wikipedia3 , the Brown Corpus (Francis, 1964), Wordnet (Miller, 1995) and Wiktionary4 definitions appended with examples. Bilingual Data: We extract training data for the bilingual models from WMT13 (Mach´acˇ ek and Bojar, 2013) sentence-aligned parallel corpora, specifically version 7 of the EuroParl parallel corpus (Koehn, 2005), the multiUN parallel corpus (Eisele and Chen, 2010), and news commentary data. We train the bilingual model using a sample of 1M sentence pairs from these datasets. All sentences in our data are tokenized and stemmed, and number sequences are replaced with a special token as a normalization step. We use the Stanford CoreNLP toolkit (Manning et al., 2014) for English preprocessing, and Treetagger tools (Schmid, 1995) for Spanish. Words that appear less than 5 times in the training set are d"
S16-1101,P14-5010,0,0.00535645,"and Wiktionary4 definitions appended with examples. Bilingual Data: We extract training data for the bilingual models from WMT13 (Mach´acˇ ek and Bojar, 2013) sentence-aligned parallel corpora, specifically version 7 of the EuroParl parallel corpus (Koehn, 2005), the multiUN parallel corpus (Eisele and Chen, 2010), and news commentary data. We train the bilingual model using a sample of 1M sentence pairs from these datasets. All sentences in our data are tokenized and stemmed, and number sequences are replaced with a special token as a normalization step. We use the Stanford CoreNLP toolkit (Manning et al., 2014) for English preprocessing, and Treetagger tools (Schmid, 1995) for Spanish. Words that appear less than 5 times in the training set are discarded from the vocabulary. 4.2 Parameter Settings We train our bilingual b-WMF models strictly using the bilingual parallel data. On the other hand, we train the English pivot model used in x-WMF strictly using the English monolingual data, while the parallel corpora are only used for training the Spanish component of the x-WMF models. For the b-WMF models and the English monolingual model, we run the ALS algorithm for 20 iterations. We use the following"
S16-1101,D14-1162,0,0.0804913,"Missing"
S16-1101,W15-1512,0,0.0137515,"tence can be mapped into a fixedlength vector either by combining word embeddings In crosslingual STS, the challenge is to compare sentences from two different languages. We address this problem by directly learning crosslingual vector representations for words and sentences, which allows us to calculate the STS scores without the need for explicit translation or mapping. Several models can be used for learning cross-lingual compositional representations (Klementieva et al., 2012; Shi et al., 2015; Pennington et al., 2014; Cavallanti et al., 2010; Mikolov et al., 2013; Coulmance et al., 2015; Pham et al., 2015). We propose a relatively simple and nuanced unsupervised model inspired by the monolingual weighted matrix factorization (WMF) model proposed in (Guo and Diab, 2012), which we extend to the cross-lingual setting. The WMF model learns word representations by decomposing a sparse tf-idf matrix into two lowrank factor matrices representing words and sentences. The weights are adjusted to reflect the confidence levels in reconstructing observed vs. missing words in the original matrix. Representations for variable-length sequences can be calculated by minimizing the reconstruction error as descri"
S16-1101,P15-2093,0,0.0725073,"stic language models (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2013). A variable-length sentence can be mapped into a fixedlength vector either by combining word embeddings In crosslingual STS, the challenge is to compare sentences from two different languages. We address this problem by directly learning crosslingual vector representations for words and sentences, which allows us to calculate the STS scores without the need for explicit translation or mapping. Several models can be used for learning cross-lingual compositional representations (Klementieva et al., 2012; Shi et al., 2015; Pennington et al., 2014; Cavallanti et al., 2010; Mikolov et al., 2013; Coulmance et al., 2015; Pham et al., 2015). We propose a relatively simple and nuanced unsupervised model inspired by the monolingual weighted matrix factorization (WMF) model proposed in (Guo and Diab, 2012), which we extend to the cross-lingual setting. The WMF model learns word representations by decomposing a sparse tf-idf matrix into two lowrank factor matrices representing words and sentences. The weights are adjusted to reflect the confidence levels in reconstructing observed vs. missing words in the original matr"
S16-1101,P13-1045,0,0.0202899,"or representations that capture implicit syntactic and semantic information. These representations can then be directly compared using well-known distance or similairty measures, such as the Euclidean distance or cosine similarity, which reflect their overall semantic relatedness. Such distributed representations of words, or word embeddings, can be learned using global word co-occurrence statistics as in matrix factorization models (Guo and Diab, 2012; Pennington et al., 2014), or using local context as in neural probabilistic language models (Bengio et al., 2003; Collobert and Weston, 2008; Socher et al., 2013). A variable-length sentence can be mapped into a fixedlength vector either by combining word embeddings In crosslingual STS, the challenge is to compare sentences from two different languages. We address this problem by directly learning crosslingual vector representations for words and sentences, which allows us to calculate the STS scores without the need for explicit translation or mapping. Several models can be used for learning cross-lingual compositional representations (Klementieva et al., 2012; Shi et al., 2015; Pennington et al., 2014; Cavallanti et al., 2010; Mikolov et al., 2013; C"
S17-1028,W15-1201,1,0.727747,"73.88 74.28 74.78 Random Forest Precision Recall 69.48 68.92 60.94 60.63 79.38 78.77 74.80 73.76 80.00 79.50 67.72 67.47 75.00 74.72 78.30 78.02 75.89 75.52 82.01 81.30 78.81 78.40 F-Score 69.20 60.78 79.07 74.28 79.75 67.59 74.86 78.16 75.70 81.65 78.60 sistent with sentiment categorical analysis. In general, neutral sentiment is the most common for a given text and for patients, we would expect to see more negative sentiment and this was confirmed by this analysis. However, negative sentiment could also be prominent in other psychiatric diseases such as post-traumatic stress disorder (PTSD)(Coppersmith et al., 2015), as such, by itself, it may not be a discriminatory feature for schizophrenia patients. For classification purposes, sentiment intensity features performed better than sentiment features. This could be due to the fact that intensity values are more specific and collected at word/phrase level in contrast to sentence level. bels are Quantity and Social Event. More specific labels are Morality Evaluation, Catastrophe, Manipulate into Doing and Being Obligated. Words that are labeled as such are listed in Table 4. These two different sets of labels could be due to the type of questions asked to t"
S17-1028,P11-2008,0,0.0546266,"Missing"
S17-1028,D13-1170,0,0.00505229,"Missing"
S17-1028,D14-1108,0,0.0909395,"Higher ups keep their jobs while the worker ants get disposed of. How about people who take advantage of others and build an Empire off it like insurance or drug companies. All these good decent people not getting what they deserved. Yup that makes me angry. 2.2.1 Syntactic Features To capture the syntactic information from writings, we produce the POS tags and dependency parse trees using Stanford Core NLP (Manning et al., 2014). To use these as features to the classifier, we calculate the frequency of each POS tag and dependencies from parse trees. For the Twitter dataset, we use a parser (Kong et al., 2014) and POS tagger (Gimpel et al., 2011) that are specifically trained for social media data. 242 2.2.2 Semantic Features sities are produced at the phrase level. Rather than categorical values, this intensity encodes the magnitude of the sentiment more explicitly. As such, we calculate the total intensity for each document as sum of its phrases’ intensities at each level. For Twitter dataset, we use a sentiment classifier that was trained for social media data (Radeva et al., 2016). Its output includes three levels of sentiment negative, neutral, and positive without intensity information. To an"
S17-1028,W15-1202,1,0.768019,"es of schizophrenia using the Twitter API. This dataset includes 174 users with apparently genuine self-stated diagnosis of a schizophrenia-related condition and 174 age and gender matched controls. Schizophrenia users were selected via regular expression on schizo for a close phonetic approximation. Each diagnosis was examined by a human annotator to verify that it seems genuine. For each schizophrenia user, a control that had the same gender label and was closest in age was selected. The average number of tweets per user is around 2,800. Detailed information on this dataset can be found in (Mitchell et al., 2015). Below are some tweets from this dataset (they have been rephrased to preserve anonymity): Predictive Linguistic Features of Schizophrenia Dataset The first dataset called LabWriting consists of 93 patients with schizophrenia who were recruited from sites in both Washington, D.C. and New York City. This includes patients that have a diagnosis of schizophreniform disorder or firstepisode or early-course patients with a psychotic disorder not otherwise specified. All patients were native English-speaking patients, aged 18-50 years and cognitively intact enough to understand and participate in t"
S17-1028,D14-1162,0,0.0873198,"as a distribution over the words of the vocabulary. We use the MALLET tool (McCallum, 2002) to train the topic model and empirically choose number of topics based on best classification performance on a validation set. The best performing number of topics is 20 for LabWriting dataset and 40 for Twitter dataset. Finally, we compute dense semantic features by computing clusters based on global word vectors. Specifically, for LabWriting dataset, we use word vectors trained on Wikipedia 2014 dump and Gigaword 5 (Parker, 2011) which are generated based on global word-word co-occurrence statistics (Pennington et al., 2014). For Twitter dataset, we use Twitter models trained on 2 billion tweets.1 We, then, create clusters of these word vectors using the K-means algorithm (K= 100, empirically chosen) for both datasets. Then, for each writing, we calculate the frequency of each cluster by checking the existence of each word of the document in the cluster. With this cluster based representation, we aim to capture the effect of semantically related words on the classification. 2.2.3 2.2.4 Feature Analysis To be able to better evaluate best performing features, we analyze them based on two feature selection algorithm"
S17-2001,S17-2013,0,0.019824,"Missing"
S17-2001,S17-2031,0,0.0137096,"Missing"
S17-2001,P98-1013,0,0.169413,"Missing"
S17-2001,S15-2045,1,0.888131,"elated but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversati"
S17-2001,L16-1662,0,0.0083775,"ubmission uses sentence IC exclusively. Another ensembles IC with Sultan et al. (2015)’s alignment method, while a third ensembles IC with cosine similarity of summed word embeddings with an IDF weighting scheme. Sentence IC in isolation outperforms all systems except those from ECNU. Combining sentence IC with word embedding similarity performs best. CompiLIG (Ferrero et al., 2017) The best Spanish-English performance on SNLI sentences was achieved by CompiLIG using features including: cross-lingual conceptual similarity using DBNary (Serasset, 2015), cross-language MultiVec word embeddings (Berard et al., 2016), and Brychcin and Svoboda (2016)’s improvements to Sultan et al. (2015)’s method. LIM-LIG (Nagoudi et al., 2017) Using only weighted word embeddings, LIM-LIG took second place on Arabic.17 Arabic word embeddings are summed into sentence embeddings using uniform, POS and IDF weighting schemes. Sentence similarity is computed by cosine similarity. POS and IDF outperform uniform weighting. Combining the IDF and POS weights by multiplication is reported by LIM-LIG to achieve r 0.7667, higher than all submitted Arabic (track 1) systems. HCTI (Shao, 2017) Third place overall is obtained by HCTI wit"
S17-2001,S17-2030,0,0.0302147,"Missing"
S17-2001,S17-2021,0,0.0336317,"Missing"
S17-2001,S16-1081,1,0.903281,"n an English sentence and its Arabic machine translation5 where they perform post-editing to correct errors. Spanish translation is completed by a University of Sheffield graduate student who is a native Spanish speaker and fluent in English. Turkish translations are obtained from SDL.6 3.4 Crowdsourced Annotations Crowdsourced annotation is performed on Amazon Mechanical Turk.8 Annotators examine the STS pairings of English SNLI sentences. STS labels are then transferred to the translated pairs for crosslingual and non-English tracks. The annotation instructions and template are identical to Agirre et al. (2016). Labels are collected in batches of 20 pairs with annotators paid $1 USD per batch. Five annotations are collected per pair. The MTurk master9 qualification is required to perform the task. Gold scores average the five individual annotations. This section describes the preparation of the evaluation data. For SNLI data, this includes the selection of sentence pairs, annotation of pairs with STS labels and the translation of the original English sentences. WMT quality estimation data is directly annotated with STS labels. 3.3 Annotation 4.1 Table 2 summarizes the evaluation data by track. The s"
S17-2001,W14-3302,1,0.742822,"Missing"
S17-2001,S12-1051,1,0.784858,"nd paraphrase detection in that it captures gradations of meaning overlap rather than making binary classifications of particular relationships. While semantic relatedness expresses a graded semantic relationship as well, it is non-specific about the nature of the relationship with contradictory material still being a candidate for a high score (e.g., “night” and “day” are highly related but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep lear"
S17-2001,S17-2012,0,0.0153726,"Missing"
S17-2001,S17-2032,0,0.0142311,"Missing"
S17-2001,D15-1075,0,0.136451,"s/missing. John said he is considered a witness but not a suspect. “He is not a suspect anymore.” John said. The two sentences are not equivalent, but share some details. They flew out of the nest in groups. They flew into the nest together. The two sentences are not equivalent, but are on the same topic. The woman is playing the violin. The young lady enjoys listening to the guitar. The two sentences are completely dissimilar. The black dog is running through the snow. A race car driver is driving his car through the mud. Evaluation Data The Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015) is the primary evaluation data source with the exception that one of the 3 Previous years of the STS shared task include more data sources. This year the task draws from two data sources and includes a diverse set of languages and language-pairs. 4 HTER is the minimal number of edits required for correction of a translation divided by its length after correction. pilot track on cross-lingual Spanish-English STS. The English tracks attracted the most participation and have the largest use of the evaluation data in ongoing research. 2 Track 1 2 3 4a 4b 5 6 Language(s) Arabic (ar-ar) Arabic-Engl"
S17-2001,S16-1089,0,0.0248981,"Missing"
S17-2001,S17-2015,0,0.0563979,"fication of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://translate.google.com 5 Team ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) BIT (Wu et al., 2017)* BIT (Wu et al., 2017)* BIT (Wu et al., 2017) HCTI (Shao, 2017) MITRE (Henderson et al., 2017) MITRE (Henderson et al., 2017) FCICU (Hassan et al., 2017) neobility (Zhuang and Chang, 2017) FCICU (Hassan et al., 2017) STS-UHH (Kohail et al., 2017) RTV HCTI (Shao, 2017) RTV MatrusriIndia STS-UHH (Kohail et al., 2017) SEF@UHH (Duma and Menzel, 2017) SEF@UHH (Duma and Menzel, 2017) RTV SEF@UHH (Duma and Menzel, 2017) neobility (Zhuang and Chang, 2017) neobility (Zhuang and Chang, 2017) MatrusriIndia NLPProxem UMDeep (Barrow and Peskov, 2017) NLPProxem UMDeep (Barrow and Peskov, 2017) Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* NLPProxem RTM (Bic¸ici"
S17-2001,D15-1181,0,0.0245131,"Missing"
S17-2001,N16-1108,0,0.0260604,"Missing"
S17-2001,S16-1170,0,0.0147665,"Missing"
S17-2001,D17-1070,0,0.281621,"Missing"
S17-2001,N16-1162,0,0.0181244,"Missing"
S17-2001,C04-1051,0,0.847641,"Missing"
S17-2001,N06-2015,0,0.0472258,"Missing"
S17-2001,S17-2024,0,0.0294688,"Missing"
S17-2001,S17-2019,0,0.0294083,"Missing"
S17-2001,P15-1162,0,0.0488426,"Missing"
S17-2001,S12-1061,0,0.675418,"ationship with contradictory material still being a candidate for a high score (e.g., “night” and “day” are highly related but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarizat"
S17-2001,marelli-etal-2014-sick,0,0.0431263,"from English STS shared tasks (2012-2017). 2 5 4 3 2 1 0 Table 1: Similarity scores with explanations and English examples from Agirre et al. (2013). cross-lingual tracks explores data from the WMT 2014 quality estimation task (Bojar et al., 2014).3 Sentences pairs in SNLI derive from Flickr30k image captions (Young et al., 2014) and are labeled with the entailment relations: entailment, neutral, and contradiction. Drawing from SNLI allows STS models to be evaluated on the type of data used to assess textual entailment methods. However, since entailment strongly cues for semantic relatedness (Marelli et al., 2014), we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores. Track 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences from a variety of methods including RBMT, SMT, hybrid-MT and human translation. Translations are annotated with the time required for human correction by post-editing and Human-targeted Translation Error Rate (HTER) (Snover et al., 2006).4 Participants are not allowed to use the gold qualit"
S17-2001,P16-1089,0,0.0231566,"Missing"
S17-2001,S17-2025,0,0.0295389,"Missing"
S17-2001,H92-1116,0,0.16712,"Missing"
S17-2001,W16-1609,0,0.0245746,"Arabic, Spanish and Turkish. The primary evaluation criteria combines performance on all of the different language conditions except English-Turkish, which was run as a surprise language track. Even with this departure from prior years, the task attracted 31 teams producing 84 submissions. STS shared task data sets have been used extensively for research on sentence level similarity and semantic representations (i.a., Arora et al. (2017); Conneau et al. (2017); Mu et al. (2017); Pagliardini et al. (2017); Wieting and Gimpel (2017); He and Lin (2016); Hill et al. (2016); Kenter et al. (2016); Lau and Baldwin (2016); Wieting et al. (2016b,a); He et al. (2015); Pham et al. (2015)). To encourage the use of a common evaluation set for assessing new methods, we present the STS Benchmark, a publicly available selection of data from English STS shared tasks (2012-2017). 2 5 4 3 2 1 0 Table 1: Similarity scores with explanations and English examples from Agirre et al. (2013). cross-lingual tracks explores data from the WMT 2014 quality estimation task (Bojar et al., 2014).3 Sentences pairs in SNLI derive from Flickr30k image captions (Young et al., 2014) and are labeled with the entailment relations: entailment"
S17-2001,P17-2099,0,0.0191097,"Missing"
S17-2001,S17-2029,0,0.0284965,"Missing"
S17-2001,S17-2017,0,0.0940842,"jerva and Ostling, 2017) ¨ ResSim (Bjerva and Ostling, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) hjpwhu hjpwhu compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) FCICU (Hassan et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* L2F/INESC-ID (Fialho et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) MatrusriIndia NRC* NRC OkadaNaoya ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017) QLUT (Meng et al., 2017)* QLUT (Meng et al., 2017) QLUT (Meng et al., 2017)* SIGMA SIGMA SIGMA SIGMA PKU 2 SIGMA PKU 2 SIGMA PKU 2 STS-UHH (Kohail et al., 2017) UCSC-NLP UdL (Al-Natsheh et al., 2017) UdL (Al-Natsheh et al., 2017)* UdL (Al-Natsheh et al., 2017) Primary 73.16 70.44 69.40 67.89 67.03 66.62 65.98 65.90"
S17-2001,P10-1023,0,0.039078,"2017) Fourth place overall is MITRE that, like ECNU, takes an ambitious feature engineering approach complemented by deep learning. Ensembled components inˇ c clude: alignment similarity; TakeLab STS (Sari´ et al., 2012b); string similarity measures such as matching n-grams, summarization and MT metrics (BLEU, WER, PER, ROUGE); a RNN and recurrent convolutional neural networks (RCNN) over word alignments; and a BiLSTM that is state-ofthe-art for textual entailment (Chen et al., 2016). FCICU (Hassan et al., 2017) Fifth place overall is FCICU that computes a sense-base alignment using BabelNet (Navigli and Ponzetto, 2010). BabelNet synsets are multilingual allowing non-English and cross-lingual pairs to be processed similarly to English pairs. Alignment similarity scores are used with two runs: one that combines the scores within a string kernel and another that uses them with a weighted variant of Sultan et al. (2015)’s method. Both runs average the Babelnet based scores with soft-cardinality (Jimenez et al., 2012b). BIT (Wu et al., 2017) Second place overall is achieved by BIT primarily using sentence information content (IC) informed by WordNet and BNC word frequencies. One submission uses sentence IC exclu"
S17-2001,S17-2022,0,0.0368865,"Missing"
S17-2001,N18-1049,0,0.148448,"Missing"
S17-2001,S17-2014,0,0.250083,"ow and Peskov, 2017) Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* Lump (Espa˜na Bonet and Barr´on-Cede˜no, 2017)* NLPProxem RTM (Bic¸ici, 2017)* UMDeep (Barrow and Peskov, 2017) RTM (Bic¸ici, 2017)* RTM (Bic¸ici, 2017)* ¨ ResSim (Bjerva and Ostling, 2017) ¨ ResSim (Bjerva and Ostling, 2017) ¨ ResSim (Bjerva and Ostling, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) LIPN-IIMAS (Arroyo-Fern´andez and Meza Ruiz, 2017) hjpwhu hjpwhu compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) compiLIG (Ferrero et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) DT TEAM (Maharjan et al., 2017) FCICU (Hassan et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) ITNLPAiKF (Liu et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* L2F/INESC-ID (Fialho et al., 2017) L2F/INESC-ID (Fialho et al., 2017)* LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) LIM-LIG (Nagoudi et al., 2017) MatrusriIndia NRC* NRC OkadaNaoya ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) ´ OPI-JSA (Spiewak et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017) PurdueNLP (Lee et al., 2017)"
S17-2001,P14-5010,0,0.0119011,"Missing"
S17-2001,D14-1162,0,0.109043,"Missing"
S17-2001,P15-1094,0,0.0153938,"Missing"
S17-2001,S12-1060,0,0.0229108,"Missing"
S17-2001,C16-1009,0,0.0254292,"lect methods are highlighted below. As directed by the SemEval workshop organizers, the CodaLab research platform hosts the task.11 6.4 Rankings Baseline The baseline is the cosine of binary sentence vectors with each dimension representing whether an individual word appears in a sentence.12 For crosslingual pairs, non-English sentences are translated into English using state-of-the-art machine translation.13 The baseline achieves an average correlation of 53.7 with human judgment on tracks 1-5 and would rank 23rd overall out the 44 system submissions that participated in all tracks. 14 e.g., Reimers et al. (2016) report success using STS labels with alternative metrics such as normalized Cumulative Gain (nCG), normalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://transla"
S17-2001,S16-1091,0,0.00933941,"2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversational systems. The STS shared task is a venue for assessing the current state-of-the-art. The 2017 task focuses on multilingual and cross-lingual pairs with one sub-track exploring MT quality estimation (MTQE) data. The task obtained strong participation from 31 tea"
S17-2001,Q15-1025,0,0.02311,"Missing"
S17-2001,D16-1157,0,0.0328236,"Missing"
S17-2001,P16-2068,0,0.0331584,"Missing"
S17-2001,P17-1190,0,0.0172847,"Missing"
S17-2001,S17-2007,0,0.0512581,"ormalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://translate.google.com 5 Team ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) BIT (Wu et al., 2017)* BIT (Wu et al., 2017)* BIT (Wu et al., 2017) HCTI (Shao, 2017) MITRE (Henderson et al., 2017) MITRE (Henderson et al., 2017) FCICU (Hassan et al., 2017) neobility (Zhuang and Chang, 2017) FCICU (Hassan et al., 2017) STS-UHH (Kohail et al., 2017) RTV HCTI (Shao, 2017) RTV MatrusriIndia STS-UHH (Kohail et al., 2017) SEF@UHH (Duma and Menzel, 2017) SEF@UHH (Duma and Menzel, 2017) RTV SEF@UHH (Duma and Menzel, 2017) neobility (Zhuang and Chang, 2017) neobility (Zhuang and Chang, 2017) MatrusriIndia NLPProxem UMDeep (Barrow and Peskov, 2017) NLPProxem UMDeep (Barrow and Peskov, 2017) Lump (Espa˜n"
S17-2001,S17-2016,0,0.204045,"Missing"
S17-2001,S15-2001,0,0.0578438,"elated but not particularly similar). To encourage and support research in this area, the STS shared task has been held annually since 2012, providing a venue for evaluation of state-ofthe-art algorithms and models (Agirre et al., 2012, 2013, 2014, 2015, 2016). During this time, diverse similarity methods and data sets1 have been explored. Early methods focused on lexical semantics, surface form matching and basic syntacˇ c et al., 2012a; tic similarity (B¨ar et al., 2012; Sari´ Jimenez et al., 2012a). During subsequent evaluations, strong new similarity signals emerged, such as Sultan et al. (2015)’s alignment based method. More recently, deep learning became competitive with top performing feature engineered systems (He et al., 2016). The best performance tends to be obtained by ensembling feature engineered and deep learning models (Rychalska et al., 2016). Significant research effort has focused on STS over English sentence pairs.2 English STS is a Semantic Textual Similarity (STS) measures the meaning similarity of sentences. Applications include machine translation (MT), summarization, generation, question answering (QA), short answer grading, semantic search, dialog and conversati"
S17-2001,Q14-1006,0,0.0320307,"Lin (2016); Hill et al. (2016); Kenter et al. (2016); Lau and Baldwin (2016); Wieting et al. (2016b,a); He et al. (2015); Pham et al. (2015)). To encourage the use of a common evaluation set for assessing new methods, we present the STS Benchmark, a publicly available selection of data from English STS shared tasks (2012-2017). 2 5 4 3 2 1 0 Table 1: Similarity scores with explanations and English examples from Agirre et al. (2013). cross-lingual tracks explores data from the WMT 2014 quality estimation task (Bojar et al., 2014).3 Sentences pairs in SNLI derive from Flickr30k image captions (Young et al., 2014) and are labeled with the entailment relations: entailment, neutral, and contradiction. Drawing from SNLI allows STS models to be evaluated on the type of data used to assess textual entailment methods. However, since entailment strongly cues for semantic relatedness (Marelli et al., 2014), we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores. Track 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences f"
S17-2001,2006.amta-papers.25,0,0.0602904,"e entailment strongly cues for semantic relatedness (Marelli et al., 2014), we construct our own sentence pairings to deter gold entailment labels from informing evaluation set STS scores. Track 4b investigates the relationship between STS and MT quality estimation by providing STS labels for WMT quality estimation data. The data includes Spanish translations of English sentences from a variety of methods including RBMT, SMT, hybrid-MT and human translation. Translations are annotated with the time required for human correction by post-editing and Human-targeted Translation Error Rate (HTER) (Snover et al., 2006).4 Participants are not allowed to use the gold quality estimation annotations to inform STS scores. Task Overview STS is the assessment of pairs of sentences according to their degree of semantic similarity. The task involves producing real-valued similarity scores for sentence pairs. Performance is measured by the Pearson correlation of machine scores with human judgments. The ordinal scale in Table 1 guides human annotation, ranging from 0 for no meaning overlap to 5 for meaning equivalence. Intermediate values reflect interpretable levels of partial overlap in meaning. The annotation scale"
S17-2001,S17-2023,0,0.0172414,"Missing"
S17-2001,S17-2018,0,0.0148193,"Missing"
S17-2001,S15-2027,0,0.00981403,"Missing"
S17-2001,S17-2028,0,0.624183,"abels with alternative metrics such as normalized Cumulative Gain (nCG), normalized Discounted Cumulative Gain (nDCG) and F1 to more accurately predict performance on the downstream tasks: text reuse detection, binary classification of document relatedness and document relatedness within a corpus. 15 Within the highlighted submissions, the following use a monolingual English system fed by MT: ECNU, BIT, HCTI 11 https://competitions.codalab.org/ competitions/16051 12 Words obtained using Arabic (ar), Spanish (es) and English (en) Treebank tokenizers. 13 http://translate.google.com 5 Team ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) ECNU (Tian et al., 2017) BIT (Wu et al., 2017)* BIT (Wu et al., 2017)* BIT (Wu et al., 2017) HCTI (Shao, 2017) MITRE (Henderson et al., 2017) MITRE (Henderson et al., 2017) FCICU (Hassan et al., 2017) neobility (Zhuang and Chang, 2017) FCICU (Hassan et al., 2017) STS-UHH (Kohail et al., 2017) RTV HCTI (Shao, 2017) RTV MatrusriIndia STS-UHH (Kohail et al., 2017) SEF@UHH (Duma and Menzel, 2017) SEF@UHH (Duma and Menzel, 2017) RTV SEF@UHH (Duma and Menzel, 2017) neobility (Zhuang and Chang, 2017) neobility (Zhuang and Chang, 2017) MatrusriIndia NLPProxem UMDeep (Barrow a"
S17-2001,S14-2010,1,\N,Missing
S17-2001,S17-2027,0,\N,Missing
S17-2001,W17-4759,0,\N,Missing
S17-2056,L16-1640,1,0.821429,"about medical concerns and the answers are generally from doctors. The dataset contains: a training of 1,031 questions and 30,411 potentially related QA pairs, a development set of 250 questions and 7,385 potentially related QA pairs, and a test set of 1400 questions associated with 8 to 9 potentially related QA pairs for each.3 3 ted model. 3.1 Preprocessing and Features 3.1.1 Text Preprocessing Text preprocessing is especially important for this CQA dataset. Therefore, in this section we briefly outline the preprocessing we applied before the feature extraction. First of all, we used SPLIT (Al-Badrashiny et al., 2016) to check if a token is a number, date, URL, or punctuation. All URLs and punctuation are removed and numbers and dates are normalized to Num and Date, respectively. Alef and Yaa characters are normalized each to a single form which is typical in large scale Arabic NLP applications to overcome and avoid writing variations. For tokenization, lemmatization and stemming we used MADAMIRA (Pasha et al., 2014) (a D3 tokenization scheme which segments determiners as well as proclitics and enclitics). Finally, we removed stop words based on a list.4 3.1.2 Features 1 . Latent Semantics Features: a late"
S17-2056,pasha-etal-2014-madamira,1,0.871911,"Missing"
S17-2056,S15-2047,0,0.0608057,"Missing"
S17-2056,P12-1091,1,0.799651,"of standard bag-of-words representation by assigning a semantic profile to the text, which captures implicit syntactic and semantic information. There are various models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), which rely on observed words to find text distribution over ”K” topics. These models in general are applied to relatively lengthy pieces of text or documents. However, texts such as question and answer pairs found in CQA are relatively short pieces of text with two to three sentences on average. Therefore, we used the Weighted Textual Matrix Factorization (WTMF) (Guo and Diab, 2012) latent model, which is more appropriate for semantic profiling of a short text. Approach In this work, we are interested in studying the effect of using semantic textual similarity (STS) based on latent semantic representations and surface level similarity features derived from the given triple: User new Question Qu , and the retrieved Question Answer (QA) pairs which we will refer to as RQ and RA , respectively. Therefore, we casted the problem as a ranking problem that orders the QA pairs according to their relatedness to a given query Qu . We used a supervised framework SV Mrank (Manning e"
S17-2056,S17-2003,0,0.0504976,"Missing"
S17-2056,S16-1083,0,0.0522214,"Missing"
S19-1006,W16-1201,1,0.850087,"single encoder and decoder that handles several input languages (Johnson et al., 2017), but the latter has not been evaluated as a general-purpose sentence representation model. According to Hill et al. (2016), the quality of the representations induced using a machine translation objective is lower than other neural models trained with different compositional objectives, such as Denoising Auto-Encoders and Skip-Thought (Kiros et al., 2015). Mono-lingual evaluation of sentence representation models can be found in Hill et al. (2016), Aldarmaki and Diab (2018), and Conneau and Kiela (2018). In Aldarmaki and Diab (2016), a modular training objective has been proposed for cross-lingual sentence embedding. However, their application was limited to the specific matrix factorization model they discussed. More recently, Conneau et al. (2018) proposed a modular transfer learning objective and evaluated it on neural sentence encoders using cross-lingual natural language inference classification. Our representation transfer framework is very similar to their approach, although we use a simpler loss function. In addition, we evaluate the framework as a general-purpose sentence encoder and compare it to other framewor"
S19-1006,D18-1269,0,0.366671,"the representations induced using a machine translation objective is lower than other neural models trained with different compositional objectives, such as Denoising Auto-Encoders and Skip-Thought (Kiros et al., 2015). Mono-lingual evaluation of sentence representation models can be found in Hill et al. (2016), Aldarmaki and Diab (2018), and Conneau and Kiela (2018). In Aldarmaki and Diab (2016), a modular training objective has been proposed for cross-lingual sentence embedding. However, their application was limited to the specific matrix factorization model they discussed. More recently, Conneau et al. (2018) proposed a modular transfer learning objective and evaluated it on neural sentence encoders using cross-lingual natural language inference classification. Our representation transfer framework is very similar to their approach, although we use a simpler loss function. In addition, we evaluate the framework as a general-purpose sentence encoder and compare it to other frameworks. sources, particularly for sequential models like the bi-directional LSTM networks typically used to encode sentences. In addition, the joint framework does not allow post-hoc or modular training, where new languages c"
S19-1006,C18-1226,1,0.917689,"pus. Neural machine translation can also be achieved with a single encoder and decoder that handles several input languages (Johnson et al., 2017), but the latter has not been evaluated as a general-purpose sentence representation model. According to Hill et al. (2016), the quality of the representations induced using a machine translation objective is lower than other neural models trained with different compositional objectives, such as Denoising Auto-Encoders and Skip-Thought (Kiros et al., 2015). Mono-lingual evaluation of sentence representation models can be found in Hill et al. (2016), Aldarmaki and Diab (2018), and Conneau and Kiela (2018). In Aldarmaki and Diab (2016), a modular training objective has been proposed for cross-lingual sentence embedding. However, their application was limited to the specific matrix factorization model they discussed. More recently, Conneau et al. (2018) proposed a modular transfer learning objective and evaluated it on neural sentence encoders using cross-lingual natural language inference classification. Our representation transfer framework is very similar to their approach, although we use a simpler loss function. In addition, we evaluate the framework as a gener"
S19-1006,Q18-1014,1,0.853639,"s often desired to generalize word and sentence embeddings across several languages to facilitate cross-lingual transfer learning (Zhou et al., 2016) and mining of parallel sentences (Guo et al., 2018). For word embeddings, cross-lingual learning can be achieved in various ways (Upadhyay et al., 2016), such as learning directly with a cross-lingual objective (Shi et al., 2015) or post-hoc alignment of monolingual word embeddings using dictionaries (Ammar et al., 2016), parallel corpora (Gouws et al., 2015; Klementiev et al., 2012), or even with no bilingual supervision (Conneau et al., 2017b; Aldarmaki et al., 2018). For bottom-up composition like vector averaging, word-level alignment is sufficient to yield cross-lingual sentence embeddings. For top-down sentence embeddings, the efforts in cross-lingual learning are more limited. Typically, a multi-faceted cross-lingual learning objective is used to align the sentence models while training them, as in Soyer et al. (2014). Cross-lingual sentence embeddings can also be learned via a neural machine translation framework trained jointly for multiple languages (Schwenk and Douze, 2017). While they indeed yield cross-lingual embeddings, the joint training mod"
S19-1006,W18-6317,0,0.0303472,"yntax, such as topic categorization, neural models often yield representations that encode syntactic and positional features, which results in superior performance in tasks that rely on sentence structure (Aldarmaki and Diab, 2018). General-purpose sentence embeddings can be used as features in various classification tasks, or to directly assess the similarity of a pair of sentences using the cosine measure. It is often desired to generalize word and sentence embeddings across several languages to facilitate cross-lingual transfer learning (Zhou et al., 2016) and mining of parallel sentences (Guo et al., 2018). For word embeddings, cross-lingual learning can be achieved in various ways (Upadhyay et al., 2016), such as learning directly with a cross-lingual objective (Shi et al., 2015) or post-hoc alignment of monolingual word embeddings using dictionaries (Ammar et al., 2016), parallel corpora (Gouws et al., 2015; Klementiev et al., 2012), or even with no bilingual supervision (Conneau et al., 2017b; Aldarmaki et al., 2018). For bottom-up composition like vector averaging, word-level alignment is sufficient to yield cross-lingual sentence embeddings. For top-down sentence embeddings, the efforts in"
S19-1006,Q17-1010,0,0.0287801,"eriments were conducted before their release. 54 P –CO Y– Target Vector L1 Loss Sentence Vector Sentence Vector –UPDATE– Pre-trained English Encoder German Encoder All birds fly Alle V¨ogel fliegen –FIXED– Figure 4: Representation transfer model, with pre-trained English encoder and L1 loss. tween the source and target representations, or to maximize the cosine of the angle between them. Empirically, we observed no notable difference between these alternatives.2 The transfer learning approach is illustrated in Figure 4. 3.5 other bottom-up approaches. We use skipgram with subword information (Bojanowski et al., 2017) , i.e. FastText, for the word embeddings, which are also used as input to the neural models. We applied static dictionary alignment using the approach and dictionaries in Smith et al. (2017), in addition to sentence mapping using the parallel corpora. We trained the monolingual FastText word embeddings and SDAE models using the 1 Billion Word benchmark (Chelba et al., 2014) for English, and WMT’12 News Crawl data for Spanish and German (Callison-Burch et al., 2012). We used WMT’12 Common Crawl data for cross-lingual alignment, and WMT’12 test sets for evaluations. We used the augmented SNLI d"
S19-1006,W12-3102,0,0.0340419,"Missing"
S19-1006,C12-1089,0,0.239033,"to directly assess the similarity of a pair of sentences using the cosine measure. It is often desired to generalize word and sentence embeddings across several languages to facilitate cross-lingual transfer learning (Zhou et al., 2016) and mining of parallel sentences (Guo et al., 2018). For word embeddings, cross-lingual learning can be achieved in various ways (Upadhyay et al., 2016), such as learning directly with a cross-lingual objective (Shi et al., 2015) or post-hoc alignment of monolingual word embeddings using dictionaries (Ammar et al., 2016), parallel corpora (Gouws et al., 2015; Klementiev et al., 2012), or even with no bilingual supervision (Conneau et al., 2017b; Aldarmaki et al., 2018). For bottom-up composition like vector averaging, word-level alignment is sufficient to yield cross-lingual sentence embeddings. For top-down sentence embeddings, the efforts in cross-lingual learning are more limited. Typically, a multi-faceted cross-lingual learning objective is used to align the sentence models while training them, as in Soyer et al. (2014). Cross-lingual sentence embeddings can also be learned via a neural machine translation framework trained jointly for multiple languages (Schwenk and"
S19-1006,W15-1512,0,0.0252994,"nd the sequential denoising auto-encoder, SDAE (Hill et al., 2016). For most approaches, we rely on parallel sentences as sentence-level dictionaries for cross-lingual supervision. We report the performance on sentence translation retrieval and crosslingual document classification. Our results support representation transfer as a scalable approach for modular cross-lingual alignment that works well across different neural models and evaluation benchmarks. 2 Related Work Learning bilingual compositional representations can be achieved by optimizing a bilingual objective on parallel corpora. In Pham et al. (2015), distributed representations for bilingual phrases and sentences are learned using an extended version of the paragraph vector model (Le and Mikolov, 2014) by forcing parallel sentences to share one vector. In Soyer et al. (2014), cross-lingual compositional embeddings are learned by optimizing a joint bilingual objective that aligns parallel source and target representations by minimizing the Euclidean distances between them, and a monolingual objective that maximizes the similarity between similar phrases. The monolingual objective was implemented by maximizing the similarity between random"
S19-1006,W17-2619,0,0.544493,"al., 2012), or even with no bilingual supervision (Conneau et al., 2017b; Aldarmaki et al., 2018). For bottom-up composition like vector averaging, word-level alignment is sufficient to yield cross-lingual sentence embeddings. For top-down sentence embeddings, the efforts in cross-lingual learning are more limited. Typically, a multi-faceted cross-lingual learning objective is used to align the sentence models while training them, as in Soyer et al. (2014). Cross-lingual sentence embeddings can also be learned via a neural machine translation framework trained jointly for multiple languages (Schwenk and Douze, 2017). While they indeed yield cross-lingual embeddings, the joint training models in existing literature pose some practical limitations: simultaneous training requires massive computational reWe develop and investigate several crosslingual alignment approaches for neural sentence embedding models, such as the supervised inference classifier, InferSent, and sequential encoder-decoder models. We evaluate three alignment frameworks applied to these models: joint modeling, representation transfer learning, and sentence mapping, using parallel text to guide the alignment. Our results support represent"
S19-1006,L18-1560,0,0.0329226,"Missing"
S19-1006,P15-2093,0,0.0258591,"rely on sentence structure (Aldarmaki and Diab, 2018). General-purpose sentence embeddings can be used as features in various classification tasks, or to directly assess the similarity of a pair of sentences using the cosine measure. It is often desired to generalize word and sentence embeddings across several languages to facilitate cross-lingual transfer learning (Zhou et al., 2016) and mining of parallel sentences (Guo et al., 2018). For word embeddings, cross-lingual learning can be achieved in various ways (Upadhyay et al., 2016), such as learning directly with a cross-lingual objective (Shi et al., 2015) or post-hoc alignment of monolingual word embeddings using dictionaries (Ammar et al., 2016), parallel corpora (Gouws et al., 2015; Klementiev et al., 2012), or even with no bilingual supervision (Conneau et al., 2017b; Aldarmaki et al., 2018). For bottom-up composition like vector averaging, word-level alignment is sufficient to yield cross-lingual sentence embeddings. For top-down sentence embeddings, the efforts in cross-lingual learning are more limited. Typically, a multi-faceted cross-lingual learning objective is used to align the sentence models while training them, as in Soyer et al."
S19-1006,P16-1157,0,0.0250081,"ctic and positional features, which results in superior performance in tasks that rely on sentence structure (Aldarmaki and Diab, 2018). General-purpose sentence embeddings can be used as features in various classification tasks, or to directly assess the similarity of a pair of sentences using the cosine measure. It is often desired to generalize word and sentence embeddings across several languages to facilitate cross-lingual transfer learning (Zhou et al., 2016) and mining of parallel sentences (Guo et al., 2018). For word embeddings, cross-lingual learning can be achieved in various ways (Upadhyay et al., 2016), such as learning directly with a cross-lingual objective (Shi et al., 2015) or post-hoc alignment of monolingual word embeddings using dictionaries (Ammar et al., 2016), parallel corpora (Gouws et al., 2015; Klementiev et al., 2012), or even with no bilingual supervision (Conneau et al., 2017b; Aldarmaki et al., 2018). For bottom-up composition like vector averaging, word-level alignment is sufficient to yield cross-lingual sentence embeddings. For top-down sentence embeddings, the efforts in cross-lingual learning are more limited. Typically, a multi-faceted cross-lingual learning objective"
S19-1006,P16-1133,0,0.0188386,"perior performance in tasks that are independent of syntax, such as topic categorization, neural models often yield representations that encode syntactic and positional features, which results in superior performance in tasks that rely on sentence structure (Aldarmaki and Diab, 2018). General-purpose sentence embeddings can be used as features in various classification tasks, or to directly assess the similarity of a pair of sentences using the cosine measure. It is often desired to generalize word and sentence embeddings across several languages to facilitate cross-lingual transfer learning (Zhou et al., 2016) and mining of parallel sentences (Guo et al., 2018). For word embeddings, cross-lingual learning can be achieved in various ways (Upadhyay et al., 2016), such as learning directly with a cross-lingual objective (Shi et al., 2015) or post-hoc alignment of monolingual word embeddings using dictionaries (Ammar et al., 2016), parallel corpora (Gouws et al., 2015; Klementiev et al., 2012), or even with no bilingual supervision (Conneau et al., 2017b; Aldarmaki et al., 2018). For bottom-up composition like vector averaging, word-level alignment is sufficient to yield cross-lingual sentence embeddin"
S19-1006,Q17-1024,0,\N,Missing
S19-2038,baccianella-etal-2010-sentiwordnet,0,0.00559094,"different word embedding methods such as word2vec, GloVe (Pennington et al., 2014), fasttext (Mikolov et al., 2018), and ELMo. Among these methods fasttext and ELMo create better results. 2.1 2.2 Word Embedding Textual Information Contextual Information Objective Emotion NLP is fun fasttext fasttext fasttext 300 300 300 GRU GRU GRU 148 300 Softmax Sentiment Perceptron Attention Layer 8 Concatinate 70 Sentiment Encoding Sentiment and objective Information (SOI)relativity of subjectivity and sentiment with emotion are well studied in the literature. To craft these features we use SentiwordNet (Baccianella et al., 2010), we create sentiment and subjective score per word in each sentences. SentiwordNet is the result of the automatic annotation of all the synsets of WORDNET according to the notions of positivity, negativity, and neutrality. Each synset s in WORDNET is associated to three numerical scores Pos(s), Neg(s), and Obj(s) which indicate how positive, negative, and objective (i.e., neutral) the terms contained in the synset are. Different senses of the same term may thus have different opinion-related properties. These scores are presented per sentence and their lengths are equal to the length of each"
S19-2038,W17-5206,0,0.0510433,"Missing"
S19-2038,S18-1037,0,0.0465913,"Missing"
S19-2038,S18-1043,0,0.0194446,".5 0.6 0.7 train 0.8 dev 0.9 test 1 1.1 1.2 1.3 1.4 1.5 ·104 Figure 4: EmoContext data set - train, dev, test data statistic 6 https://www.rottentomatoes.com/ 232 Methods/Data set Baseline GRU-att-fasttext GRU-att-fasttext+F GRU-att-ELMo GRU-att-ELMo+F Context Results(emotion only) pr. 88.12 88.27 88.50 88.61 54.28 re. 81.24 84.47 82.65 84.34 57.93 EmoContext f. acc. 46.20 46.20 83.44 80.84 85.27 82.07 83.05 82.65 85.54 83.62 56.04 - resources, and emotion lexicons. Among these works (Baziotis et al., 2018) proposed a Bi-LSTM architecture equipped with a multi-layer self attention mechanism, (Meisheri and Dey, 2018) their model learned the representation of each tweet using mixture of different embedding. in WASSA 2017 Shared Task on Emotion Intensity (Mohammad and Bravo-Marquez, 2017), among the proposed approaches, we can recognize teams who used different word embeddings: GloVe or word2vec (He et al., 2017; Duppada and Hiray, 2017) and exploit a neural net architecture such as LSTM (Goel et al., 2017; Akhtar et al., 2017), LSTM-CNN combinations (K¨oper et al., 2017; Zhang et al., 2017) and bi-directional versions (He et al., 2017) to predict emotion intensity. Similar approach is developed by (Gupta e"
S19-2038,L18-1008,0,0.0252493,"ion layers (explained in section 2.1) are concatenated with GRU layer as auxiliary layer. We utilize a dropout (Graves et al., 2013) layer after the first perceptron layer for regularization. Using different word embedding or end to end models where word representation learned from local context create different results in emotion detection. We noted that pre-trained word embeddings need to be tuned with local context during our experiments or it causes the model to not converge. We experimented with different word embedding methods such as word2vec, GloVe (Pennington et al., 2014), fasttext (Mikolov et al., 2018), and ELMo. Among these methods fasttext and ELMo create better results. 2.1 2.2 Word Embedding Textual Information Contextual Information Objective Emotion NLP is fun fasttext fasttext fasttext 300 300 300 GRU GRU GRU 148 300 Softmax Sentiment Perceptron Attention Layer 8 Concatinate 70 Sentiment Encoding Sentiment and objective Information (SOI)relativity of subjectivity and sentiment with emotion are well studied in the literature. To craft these features we use SentiwordNet (Baccianella et al., 2010), we create sentiment and subjective score per word in each sentences. SentiwordNet is the"
S19-2038,S18-1001,0,0.0185734,"at 0 is not a good initial value in neural net. 5 231 https://keras.io/ https://www.tensorflow.org/ Data set MULTI AIT EmoContext A multigenre corpus created by (Tafreshi and Diab, 2018) with following genres: emotional blog posts, collected by (Aman and Szpakowicz, 2007), headlines data set from SemEval 2007-task 14 (Strapparava and Mihalcea, 2007), movie review data set (Pang and Lee, 2005) originally collected from Rotten tomatoes 6 for sentiment analysis and it is among the benchmark sets for this task. We refer to this multigenre set as (MULTI), b) SemEval-2018 Affect in Tweets data set (Mohammad et al., 2018) (AIT) with most popular emotion tags: anger, fear, joy, and sadness, c) the data set that is given for this task, which is 3-turn conversation data. From these data sets we only used the emotion tags happy, sad, and angry. We used tag no-emotion from MULTI data set as others tag. Data statistics are shown in figures 2, 3, 4 . #train 13776 6839 30160 #dev 1722 887 2755 #test 1722 4072 5510 total 17220 11798 38425 Table 1: Data statistics illustrating the distributions of the train, dev, and test sets across different data sets. baseline- in each sentence we tagged every emotional word using NR"
S19-2038,W17-5205,0,0.044216,"Missing"
S19-2038,W17-5228,0,0.0224183,"24 84.47 82.65 84.34 57.93 EmoContext f. acc. 46.20 46.20 83.44 80.84 85.27 82.07 83.05 82.65 85.54 83.62 56.04 - resources, and emotion lexicons. Among these works (Baziotis et al., 2018) proposed a Bi-LSTM architecture equipped with a multi-layer self attention mechanism, (Meisheri and Dey, 2018) their model learned the representation of each tweet using mixture of different embedding. in WASSA 2017 Shared Task on Emotion Intensity (Mohammad and Bravo-Marquez, 2017), among the proposed approaches, we can recognize teams who used different word embeddings: GloVe or word2vec (He et al., 2017; Duppada and Hiray, 2017) and exploit a neural net architecture such as LSTM (Goel et al., 2017; Akhtar et al., 2017), LSTM-CNN combinations (K¨oper et al., 2017; Zhang et al., 2017) and bi-directional versions (He et al., 2017) to predict emotion intensity. Similar approach is developed by (Gupta et al., 2017) using sentiment and LSTM architecture. Proper word embedding for emotion task is key, choosing the most efficient distance between vectors is crucial, the following studies explore solution sparsity related properties possibly including uniqueness (Shen and Mousavi, 2018; Mousavi and Shen, 2017) . sp./#epo. n.a"
S19-2038,W17-5207,0,0.0205255,"2.07 83.05 82.65 85.54 83.62 56.04 - resources, and emotion lexicons. Among these works (Baziotis et al., 2018) proposed a Bi-LSTM architecture equipped with a multi-layer self attention mechanism, (Meisheri and Dey, 2018) their model learned the representation of each tweet using mixture of different embedding. in WASSA 2017 Shared Task on Emotion Intensity (Mohammad and Bravo-Marquez, 2017), among the proposed approaches, we can recognize teams who used different word embeddings: GloVe or word2vec (He et al., 2017; Duppada and Hiray, 2017) and exploit a neural net architecture such as LSTM (Goel et al., 2017; Akhtar et al., 2017), LSTM-CNN combinations (K¨oper et al., 2017; Zhang et al., 2017) and bi-directional versions (He et al., 2017) to predict emotion intensity. Similar approach is developed by (Gupta et al., 2017) using sentiment and LSTM architecture. Proper word embedding for emotion task is key, choosing the most efficient distance between vectors is crucial, the following studies explore solution sparsity related properties possibly including uniqueness (Shen and Mousavi, 2018; Mousavi and Shen, 2017) . sp./#epo. n.a. 103/14 321/8 310/20 960/28 960/28 Table 2: Results on the EmoContext"
S19-2038,P05-1015,0,0.0441986,"RC lexicon. Each feature represent one emotion category, where 0.001 3 indicates of absent of 3.1 Data We used three different emotion corpora in our experiments. Our corpora are as follows: a) 4 3 empirically we observed that 0 is not a good initial value in neural net. 5 231 https://keras.io/ https://www.tensorflow.org/ Data set MULTI AIT EmoContext A multigenre corpus created by (Tafreshi and Diab, 2018) with following genres: emotional blog posts, collected by (Aman and Szpakowicz, 2007), headlines data set from SemEval 2007-task 14 (Strapparava and Mihalcea, 2007), movie review data set (Pang and Lee, 2005) originally collected from Rotten tomatoes 6 for sentiment analysis and it is among the benchmark sets for this task. We refer to this multigenre set as (MULTI), b) SemEval-2018 Affect in Tweets data set (Mohammad et al., 2018) (AIT) with most popular emotion tags: anger, fear, joy, and sadness, c) the data set that is given for this task, which is 3-turn conversation data. From these data sets we only used the emotion tags happy, sad, and angry. We used tag no-emotion from MULTI data set as others tag. Data statistics are shown in figures 2, 3, 4 . #train 13776 6839 30160 #dev 1722 887 2755 #"
S19-2038,S07-1013,0,0.0911027,"on feature, presenting all 8 emotion categories of the NRC lexicon. Each feature represent one emotion category, where 0.001 3 indicates of absent of 3.1 Data We used three different emotion corpora in our experiments. Our corpora are as follows: a) 4 3 empirically we observed that 0 is not a good initial value in neural net. 5 231 https://keras.io/ https://www.tensorflow.org/ Data set MULTI AIT EmoContext A multigenre corpus created by (Tafreshi and Diab, 2018) with following genres: emotional blog posts, collected by (Aman and Szpakowicz, 2007), headlines data set from SemEval 2007-task 14 (Strapparava and Mihalcea, 2007), movie review data set (Pang and Lee, 2005) originally collected from Rotten tomatoes 6 for sentiment analysis and it is among the benchmark sets for this task. We refer to this multigenre set as (MULTI), b) SemEval-2018 Affect in Tweets data set (Mohammad et al., 2018) (AIT) with most popular emotion tags: anger, fear, joy, and sadness, c) the data set that is given for this task, which is 3-turn conversation data. From these data sets we only used the emotion tags happy, sad, and angry. We used tag no-emotion from MULTI data set as others tag. Data statistics are shown in figures 2, 3, 4 ."
S19-2038,L18-1199,1,0.838978,"13) with 8 emotion tags (e.i. joy, trust, anticipation, surprise, anger, fear, sadness, disgust). We demonstrate the presence of emotion words as an 8 dimension feature, presenting all 8 emotion categories of the NRC lexicon. Each feature represent one emotion category, where 0.001 3 indicates of absent of 3.1 Data We used three different emotion corpora in our experiments. Our corpora are as follows: a) 4 3 empirically we observed that 0 is not a good initial value in neural net. 5 231 https://keras.io/ https://www.tensorflow.org/ Data set MULTI AIT EmoContext A multigenre corpus created by (Tafreshi and Diab, 2018) with following genres: emotional blog posts, collected by (Aman and Szpakowicz, 2007), headlines data set from SemEval 2007-task 14 (Strapparava and Mihalcea, 2007), movie review data set (Pang and Lee, 2005) originally collected from Rotten tomatoes 6 for sentiment analysis and it is among the benchmark sets for this task. We refer to this multigenre set as (MULTI), b) SemEval-2018 Affect in Tweets data set (Mohammad et al., 2018) (AIT) with most popular emotion tags: anger, fear, joy, and sadness, c) the data set that is given for this task, which is 3-turn conversation data. From these dat"
S19-2038,W17-5227,0,0.0209597,"ks (Baziotis et al., 2018) proposed a Bi-LSTM architecture equipped with a multi-layer self attention mechanism, (Meisheri and Dey, 2018) their model learned the representation of each tweet using mixture of different embedding. in WASSA 2017 Shared Task on Emotion Intensity (Mohammad and Bravo-Marquez, 2017), among the proposed approaches, we can recognize teams who used different word embeddings: GloVe or word2vec (He et al., 2017; Duppada and Hiray, 2017) and exploit a neural net architecture such as LSTM (Goel et al., 2017; Akhtar et al., 2017), LSTM-CNN combinations (K¨oper et al., 2017; Zhang et al., 2017) and bi-directional versions (He et al., 2017) to predict emotion intensity. Similar approach is developed by (Gupta et al., 2017) using sentiment and LSTM architecture. Proper word embedding for emotion task is key, choosing the most efficient distance between vectors is crucial, the following studies explore solution sparsity related properties possibly including uniqueness (Shen and Mousavi, 2018; Mousavi and Shen, 2017) . sp./#epo. n.a. 103/14 321/8 310/20 960/28 960/28 Table 2: Results on the EmoContext test sets. We report the mean score over 5 runs. Standard deviations in score are arou"
S19-2195,S19-2147,0,0.214605,"lenging task as the new emerging rumor could be entirely new regarding the event, propagation pattern, and also the provenance. Despite these challenges, many researchers have been studying the generalizable metrics that could be aggregated from the source, replied posts, or network information (Vosoughi et al., 2018b; Kochkina et al., 2018; Grinberg et al., 2019). Our first mission in this paper is to automatically determine the veracity of rumors as part of the SemEval task. SemEval is an ongoing shared task for evaluations of computational sentiment analysis systems. Task 7 (RumourEval19) (Gorrell et al., 2019) is one of the twelve tasks, consisting of two subtasks. Task A is about stance orientation of people as supporting, denying, querying or commenting (SDQC) in a rumor discourse and Task B is about the verification of a given rumor. We propose a hybrid model with rules and a neural network machine learning scheme for both tasks. For task A we rely on the text content of the post, and its parent. In Task B not only do we aggregate contextual information of the source of the rumor but also using the veracity orientation of the others in the same conversation. We devise some rules to improve the p"
S19-2195,C18-1288,0,0.0580925,"et al., 2011; Hamidian and Diab, 2015). However, the performance suffers when it comes to real life application for dealing with the emerging rumors which are priorly unknown. Identifying the emerging rumor and veracity of the rumor by relying on previous observations is a challenging task as the new emerging rumor could be entirely new regarding the event, propagation pattern, and also the provenance. Despite these challenges, many researchers have been studying the generalizable metrics that could be aggregated from the source, replied posts, or network information (Vosoughi et al., 2018b; Kochkina et al., 2018; Grinberg et al., 2019). Our first mission in this paper is to automatically determine the veracity of rumors as part of the SemEval task. SemEval is an ongoing shared task for evaluations of computational sentiment analysis systems. Task 7 (RumourEval19) (Gorrell et al., 2019) is one of the twelve tasks, consisting of two subtasks. Task A is about stance orientation of people as supporting, denying, querying or commenting (SDQC) in a rumor discourse and Task B is about the verification of a given rumor. We propose a hybrid model with rules and a neural network machine learning scheme for bot"
S19-2195,D14-1162,0,0.0837054,"ation to the underlying posts and create a new set of veracity tags including Source True, Source False, and Source Unverified (Six-way classification). We first apply a sequential neural network-based apFigure 2: Illustration of the hybrid network comprising the rule-based and Bi-LSTM-Softmax network on Task A and Task B. 4.1.1 Input Representations Recent studies on NLP applications are reported to have good performance applying the pretrained word embedding (Socher et al., 2013). We adopted two widely-used methods including the character embedding and pre-trained word vectors, i.e., GloVe (Pennington et al., 2014). We use a Bi-LSTM network to encode the morphology and word embeddings from characters. Intuitively the concatenated fixed size vectors WCharacter capture word morphology. W ordCharacter is concatenated with a pre-trained word embedding from GloVe Wpre−trained−Glove to get the final word representation. For Contextual Encoding, once the word embedding is created we use another Bi-LSTM layer to encode the contextual meaning from the sequence of word vectors W1 , W2 , ..., Wt 1117 Dev Test Accuracy 0.802 0.796 Macro-F 0.487 0.435 Task A Support Query 0.420 0.586 0.446 0.408 Deny 0.058 0.0 Comme"
S19-2195,D11-1147,0,0.0844465,"n, and 0.262 F-macro and 0.801 RMSE in rumor verification tasks in Task7 of SemEval 2019. 1 Introduction The number of users who rely on social media to seek daily news and information rises daily, but not all information online is trustworthy. The unmoderated feature of social media makes the emergence and diffusion of misinformation even more intense. Consequently, the propagation of misinformation online could harm an individual or even a society. Most of the current approaches on verifying credibility perform well for the unfold topics which are already verified by a trustworthy resource (Qazvinian et al., 2011; Hamidian and Diab, 2015). However, the performance suffers when it comes to real life application for dealing with the emerging rumors which are priorly unknown. Identifying the emerging rumor and veracity of the rumor by relying on previous observations is a challenging task as the new emerging rumor could be entirely new regarding the event, propagation pattern, and also the provenance. Despite these challenges, many researchers have been studying the generalizable metrics that could be aggregated from the source, replied posts, or network information (Vosoughi et al., 2018b; Kochkina et a"
S19-2195,D13-1170,0,0.00288402,"e, and Unverified) in which we rely on both source and conversation content. We expand the veracity tags for the source of the conversation to the underlying posts and create a new set of veracity tags including Source True, Source False, and Source Unverified (Six-way classification). We first apply a sequential neural network-based apFigure 2: Illustration of the hybrid network comprising the rule-based and Bi-LSTM-Softmax network on Task A and Task B. 4.1.1 Input Representations Recent studies on NLP applications are reported to have good performance applying the pretrained word embedding (Socher et al., 2013). We adopted two widely-used methods including the character embedding and pre-trained word vectors, i.e., GloVe (Pennington et al., 2014). We use a Bi-LSTM network to encode the morphology and word embeddings from characters. Intuitively the concatenated fixed size vectors WCharacter capture word morphology. W ordCharacter is concatenated with a pre-trained word embedding from GloVe Wpre−trained−Glove to get the final word representation. For Contextual Encoding, once the word embedding is created we use another Bi-LSTM layer to encode the contextual meaning from the sequence of word vectors"
W00-0801,P94-1020,0,0.0658889,"Missing"
W00-0801,J94-4003,0,0.0953873,"Missing"
W00-0801,J00-2004,0,0.00862499,"2 for the noun bank; instances where rive is aLigned with shore is assigned sense #1 for shore. Furthermore, we created linEq automatically in WordNet for the French word rive. Our approach is described as follows: • Preprocessing o f corpora Tokenizc both corpora Align the sentences of the corpora such that each sentence in the source corpus is aligned with one corresponding sentence in the target corpus. • For each source and corresponding target sentence, find the best token level alignments. Methods for automating this process have been proposed in the Literature. [A10naizan et al., 1999; Melamed, 2000; etc.] • For each source language token, create a List of its alignments to target language tokens, target set • Using the taxonomy, calculate the distance between the senses of the tokens in the target set; assign the appropriate sense(s) to each of the tokens in the target set based on an optimiTation function over the entire set of target token senses * Propagate the assigned senses back to both target and source corpora tokens, effectively, creating two tag sets, one for each the target and source corpus • Evaluate the resnlting tag sets against a hand tagged test set. 3. Preliminary Eval"
W00-0801,C00-2163,0,0.0243464,"Missing"
W00-0801,W97-0209,0,0.0684707,"Missing"
W00-0801,C92-2070,0,0.0698864,"Missing"
W00-0801,J93-2003,0,\N,Missing
W00-0801,H94-1046,0,\N,Missing
W00-0801,P95-1026,0,\N,Missing
W00-0801,P99-1068,0,\N,Missing
W04-1609,W02-0805,0,0.0301657,"ionaries list the entries in terms of roots rather than surface forms. In this paper, we present an approach, SALAAM (Sense Annotations Leveraging Alignments And Multilinguality), to bootstrap WSD for Arabic text presented in surface form. The approach of SALAAM is based on work by (Diab and Resnik, 2002) but it goes beyond it in the sense of extending the approach to the tagging of Arabic as a target language.(Diab, 2003) SALAAM uses cross-linguistic correspondences for characterizing word meanings in natural language. This idea is explored by several researchers, (Resnik and Yarowsky, 1998; Chugur et al., 2002; Ide, 2000; Dyvik, 1998). Basically, a word meaning or a word sense is quantifiable as much as it is uniquely translated in some language or set of languages. SALAAM is an empirical validation of this very notion of characterizing word meaning using cross-linguistic correspondences. Since automated lexical resources are virtually non-existent for Arabic, SALAAM leverages sense ambiguity resolution for Arabic off of existing English lexical resources and an Arabic English parallel corpus, thereby providing a bilingual solution to the WSD problem. The paper is organized as follows: Section 2 de"
W04-1609,P02-1033,1,0.881443,"iners, possessive pronouns and pronouns. The stems consist of an underlying consonantal root and a template. The root could be anywhere from two to four consonants devoid of vocalization. Typically text in Modern Standard Arabic is written in the stem surface form with the various affixes. However, most Arabic dictionaries list the entries in terms of roots rather than surface forms. In this paper, we present an approach, SALAAM (Sense Annotations Leveraging Alignments And Multilinguality), to bootstrap WSD for Arabic text presented in surface form. The approach of SALAAM is based on work by (Diab and Resnik, 2002) but it goes beyond it in the sense of extending the approach to the tagging of Arabic as a target language.(Diab, 2003) SALAAM uses cross-linguistic correspondences for characterizing word meanings in natural language. This idea is explored by several researchers, (Resnik and Yarowsky, 1998; Chugur et al., 2002; Ide, 2000; Dyvik, 1998). Basically, a word meaning or a word sense is quantifiable as much as it is uniquely translated in some language or set of languages. SALAAM is an empirical validation of this very notion of characterizing word meaning using cross-linguistic correspondences. Si"
W04-1609,W00-0801,1,0.752906,"ds task requires the WSD system to sense tag every content word in an English language text. 3.2 Token Aligned Parallel Corpora The gold standard set corresponds to the test set in an unsupervised setting. Therefore the test set corpus is the SENSEVAL2 English All Words test corpus which comprises three articles from the Wall Street Journal discussing religious practice, medicine and education. The test corpus does not exist in Arabic. Due to the high expense of manually creating a parallel corpus, i.e. using human translators, we opt for automatic translation systems in a fashion similar to (Diab, 2000). To our knowledge there exist two off the shelf English Arabic Machine Translation (MT) systems: Tarjim and Almisbar.3 We use both MT systems to translate the test corpus into Arabic. We merge the outputs of both in an attempt to achieve more variability in translation as an approximation to human quality translation. The merging process is based on the assumption that the MT systems rely on different sources of knowledge, different dictionaries in the least, in their translation process. Fortunately, the MT systems produce sentence aligned parallel corpora.4 However, SALAAM expects token ali"
W04-1609,W99-0512,0,0.030417,"xample, tea in English has the following metonymic sense from WN1.7pre: – ceiling: maximum altitude at which a plane can fly (under specified conditions) ; ! , 3 – a reception or party at which tea is served; ""we met at the Dean’s tea for newcomers""  .! This sense of tea does not have a correspondent in the Arabic $Ay (  ). Yet, the English lamb has the metonymic sense of MEAT which exists in Arabic. Researchers building EuroWordNet have been able to devise a number of consistent metonymic relations that hold cross linguistically such as fabric/material, animal/food, building/organization (Vossen et al., 1999; Wim Peters and Wilks, 2001). In general, in Arabic, these defined classes seem to hold, however, the specific case of tea and party does not exist. In Arabic, the English sense would be expressed as a compound tea  ). party or Hflp $Ay (  .! 8: Arabic word equivalent to specific English sense(s) The correct sense tag assigned by SALAAM to ceiling in English is the first sense, which is correct for the Arabic translation sqf ( ). Yet, the other 3 senses are not correct translations for the Arabic word. For instance, the second sense definition would be translated  as rtfAE (  ) and t"
W07-0812,N04-4038,1,0.790675,"s from the beginning and end of a word in focus. The context for YAMCHA is defined as +/2 words around the focus word. The words before the focus word are considered with their ERTS tags. The kernel is a polynomial degree 2 kernel. We adopt the one-vs-all approach for classification, where the tagged examples for one class are considered positive training examples and instances for other classes are considered negative examples. We present results and a brief discussion of the POS tagging performance in Section 4. 3.2 In this task, we use a setup similar to that of Kudo & Matsumato (2000) and Diab et al. (2004 & 2007), with the IOB annotation representation: Inside I a phrase, Outside O a phrase, and Beginning B of a phrase. However, we designate 10 types of chunked phrases. The chunk phrases identified for Arabic are ADJP, ADVP, CONJP, INTJP, NP, PP, PREDP, PRTP, SBARP, VP. Thus the task is a one of 21 classification task (since there are I and B tags for each chunk phrase type, and a single O tag). The 21 IOB tags are listed: {O, I-ADJP, B-ADJP, I-ADVP, 3 2 Base Phrase Chunking All the romanized Arabic is presented in the Buckwalter transliteration scheme (Buckwalter, 2002). Dan Bikel, personal c"
W07-0812,P05-1071,0,0.251213,"Missing"
W07-0812,N03-2009,0,0.0173652,"ne learning approaches have been applied to POS and BPC tagging, by casting them as classification tasks. Given a set of features extracted from the linguistic context, a classifier predicts the POS or BPC class of a token. SVMs (Vapnik, 1995) are one such supervised machine learning algorithm, with the advantages of: discriminative training, robustness and a capability to handle a large number of (overlapping) features with good generalization per1 http://www.chasen.org/ taku/software/yamcha/ formance. Consequently, SVMs have been applied in many NLP tasks with great success (Joachims, 1998; Hacioglu & Ward, 2003). We adopt a unified tagging perspective for both the POS tagging and the BPC tasks. We address them using the same SVM experimental setup which comprises a standard SVM as a multi-class classifier (Allwein et al., 2000). A la KM00, we use the YAMCHA sequence model on the SVMs to take advantage of the context of the items being compared in a vertical manner in addition to the encoded features in the horizontal input of the vectors. Accordingly, in our different tasks, we define the notion of context to be a window of fixed size around the segment in focus for learning and tagging. 3.1 POS Tagg"
W07-0812,W00-0730,0,0.159301,"by Ramshaw and Marcus (1995), as a problem of NP chunking. Then it was extended to include other types of base phrases by Sang and Buchholz (2000) in the CoNLL 2000 shared task. Most successful approaches are based on machine learning techniques and sequence modeling of the different labels associated with the chunks. Both generative algorithms such as HMMs and multilevel Markov models, as well as, discriminative methods such as Support Vector Machines (SVM) and conditional random fields have been used for the BPC task. The closest relevant approach to the current investigation is the work of Kudo and Matsumato (2000) (KM00) on using SVMs and a sequence model for chunking. A la Ramshaw and Marcus (1995), they represent the words as a sequence of labeled words with IOB annotations, where the B marks a word at the beginning of a chunk, I marks a word inside a chunk, and O marks those words (and punctuation) that are outside chunks. The IOB annotation scheme for the English example described earlier is illustrated in Table 1. 90 word I would eat red luscious apples on Sundays . IOB label B-NP B-VP I-VP B-NP I-NP I-NP B-PP I-PP O Table 1: IOB annotation example KM00 develop a sequence model, YAMCHA, over the l"
W07-0812,W95-0107,0,0.104413,"Missing"
W07-0812,E06-1047,1,\N,Missing
W07-0812,J04-4004,0,\N,Missing
W07-0812,W00-0726,0,\N,Missing
W09-0213,W03-1812,0,0.176946,"hoc, non-fixed expressions such as break new ground, speak of the devil. Semi-idiomatic: This class includes expressions that seem semantically non-compositional, yet their semantics are more or less transparent. This category consists of Light Verb Constructions (LVC) such as make a living and Verb Particle Constructions (VPC) such as write-up, call-up. Non-Idiomatic: This category includes expressions that are semantically compositional such as prime minister, proper nouns such as New York Yankees. 3 Previous Related Work Several researchers have addressed the problem of MWE classification (Baldwin et al., 2003; Katz and Giesbrecht, 2006; Schone and Juraksfy, 2001; Hashimoto et al., 2006; Hashimoto and Kawahara, 2008). The majority of the proposed research has been using unsupervised approaches and have addressed the problem of MWE type classification irrespective of usage in context. Only, the work by Hashimoto et al. (2006) and Hashimoto and Kawahara (2008) addressed token classification in Japanese using supervised learning. The most comparable work to ours is the research by (Cook et al., 2007) and (Fazly and Stevenson, 2007). On the other hand, (Cook et al., 2007) develop an unsupervised techni"
W09-0213,W07-1106,0,0.724733,"nkees. 3 Previous Related Work Several researchers have addressed the problem of MWE classification (Baldwin et al., 2003; Katz and Giesbrecht, 2006; Schone and Juraksfy, 2001; Hashimoto et al., 2006; Hashimoto and Kawahara, 2008). The majority of the proposed research has been using unsupervised approaches and have addressed the problem of MWE type classification irrespective of usage in context. Only, the work by Hashimoto et al. (2006) and Hashimoto and Kawahara (2008) addressed token classification in Japanese using supervised learning. The most comparable work to ours is the research by (Cook et al., 2007) and (Fazly and Stevenson, 2007). On the other hand, (Cook et al., 2007) develop an unsupervised technique that classifies a VNC expression as idiomatic or literal. They examine if the similarity between the context vector of the MWE, in this case the VNC, and that of its idiomatic usage is higher than the similarity between its context vector and that of its literal usage. They define the vector dimensions in terms of the co-occurrence frequencies of 1000 most frequent content bearing words (nouns, 1 We note that the use of accuracy as a measure for this work is not the most appropriate since"
W09-0213,W07-1102,0,0.0721062,"d Work Several researchers have addressed the problem of MWE classification (Baldwin et al., 2003; Katz and Giesbrecht, 2006; Schone and Juraksfy, 2001; Hashimoto et al., 2006; Hashimoto and Kawahara, 2008). The majority of the proposed research has been using unsupervised approaches and have addressed the problem of MWE type classification irrespective of usage in context. Only, the work by Hashimoto et al. (2006) and Hashimoto and Kawahara (2008) addressed token classification in Japanese using supervised learning. The most comparable work to ours is the research by (Cook et al., 2007) and (Fazly and Stevenson, 2007). On the other hand, (Cook et al., 2007) develop an unsupervised technique that classifies a VNC expression as idiomatic or literal. They examine if the similarity between the context vector of the MWE, in this case the VNC, and that of its idiomatic usage is higher than the similarity between its context vector and that of its literal usage. They define the vector dimensions in terms of the co-occurrence frequencies of 1000 most frequent content bearing words (nouns, 1 We note that the use of accuracy as a measure for this work is not the most appropriate since accuracy is a measure of error"
W09-0213,P06-2046,0,0.0687809,"i-idiomatic: This class includes expressions that seem semantically non-compositional, yet their semantics are more or less transparent. This category consists of Light Verb Constructions (LVC) such as make a living and Verb Particle Constructions (VPC) such as write-up, call-up. Non-Idiomatic: This category includes expressions that are semantically compositional such as prime minister, proper nouns such as New York Yankees. 3 Previous Related Work Several researchers have addressed the problem of MWE classification (Baldwin et al., 2003; Katz and Giesbrecht, 2006; Schone and Juraksfy, 2001; Hashimoto et al., 2006; Hashimoto and Kawahara, 2008). The majority of the proposed research has been using unsupervised approaches and have addressed the problem of MWE type classification irrespective of usage in context. Only, the work by Hashimoto et al. (2006) and Hashimoto and Kawahara (2008) addressed token classification in Japanese using supervised learning. The most comparable work to ours is the research by (Cook et al., 2007) and (Fazly and Stevenson, 2007). On the other hand, (Cook et al., 2007) develop an unsupervised technique that classifies a VNC expression as idiomatic or literal. They examine if"
W09-0213,W06-1203,0,0.210117,"ions such as break new ground, speak of the devil. Semi-idiomatic: This class includes expressions that seem semantically non-compositional, yet their semantics are more or less transparent. This category consists of Light Verb Constructions (LVC) such as make a living and Verb Particle Constructions (VPC) such as write-up, call-up. Non-Idiomatic: This category includes expressions that are semantically compositional such as prime minister, proper nouns such as New York Yankees. 3 Previous Related Work Several researchers have addressed the problem of MWE classification (Baldwin et al., 2003; Katz and Giesbrecht, 2006; Schone and Juraksfy, 2001; Hashimoto et al., 2006; Hashimoto and Kawahara, 2008). The majority of the proposed research has been using unsupervised approaches and have addressed the problem of MWE type classification irrespective of usage in context. Only, the work by Hashimoto et al. (2006) and Hashimoto and Kawahara (2008) addressed token classification in Japanese using supervised learning. The most comparable work to ours is the research by (Cook et al., 2007) and (Fazly and Stevenson, 2007). On the other hand, (Cook et al., 2007) develop an unsupervised technique that classifies a VNC e"
W09-0213,P99-1041,0,0.0953177,"Missing"
W09-0213,D07-1039,0,0.0377015,"Missing"
W09-0213,W97-0311,0,0.121585,"Missing"
W09-0213,P08-1028,0,0.0343573,"N contexts, a suitable threshold, which is indepen98 type (V) and the noun type (N). After combining the word types in the vector dimensions, we need to handle their co-occurrence frequency values. Hence we have two methods: addition where we simply add the frequencies in the cases of the shared dimensions which amounts to a union where the co-occurrence frequencies are added; or multiplication which amounts to an intersection of the vector dimensions where the cooccurrence frequencies are multiplied, hence giving more weight to the shared dimensions than in the addition case. In a study by (Mitchell and Lapata, 2008) on a sentence similarity task, a multiplicative combination model performs better than the additive one. dent of data size, is determined on this ratio in order to prune context words. The latter two pruning techniques, DimensionF req and DimensionRatio , are not performed for a VNC token’s context, hence, all the words in the VNC token’s contextual window are included. These thresholding methods are only applied to V-N composed vectors obtained from the combination of the verb and noun vectors. Context-Content This parameter had two settings: words as they occur in the corpus, Context − Cont"
W09-0213,W06-2405,0,0.0744467,"Missing"
W09-0213,W01-0513,0,0.0135529,"nd, speak of the devil. Semi-idiomatic: This class includes expressions that seem semantically non-compositional, yet their semantics are more or less transparent. This category consists of Light Verb Constructions (LVC) such as make a living and Verb Particle Constructions (VPC) such as write-up, call-up. Non-Idiomatic: This category includes expressions that are semantically compositional such as prime minister, proper nouns such as New York Yankees. 3 Previous Related Work Several researchers have addressed the problem of MWE classification (Baldwin et al., 2003; Katz and Giesbrecht, 2006; Schone and Juraksfy, 2001; Hashimoto et al., 2006; Hashimoto and Kawahara, 2008). The majority of the proposed research has been using unsupervised approaches and have addressed the problem of MWE type classification irrespective of usage in context. Only, the work by Hashimoto et al. (2006) and Hashimoto and Kawahara (2008) addressed token classification in Japanese using supervised learning. The most comparable work to ours is the research by (Cook et al., 2007) and (Fazly and Stevenson, 2007). On the other hand, (Cook et al., 2007) develop an unsupervised technique that classifies a VNC expression as idiomatic or l"
W09-0213,W07-1104,0,0.0254108,"Missing"
W09-0213,D08-1104,0,\N,Missing
W09-2410,D07-1007,0,0.215341,"Missing"
W09-2410,P07-1005,0,0.233831,"Missing"
W09-2410,O97-1002,0,0.189047,"Missing"
W09-2410,S01-1005,0,0.643856,"Missing"
W09-2410,H05-1052,0,0.604899,"Missing"
W09-2410,S07-1016,0,0.440375,"signed the meaning sloping land especially the slope beside a body of water and so on. 3 Related Works Many systems over the years have been used for the task. A thorough review of the current state of the art is in (Navigli, 2009). Several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning approaches. To date, the best approaches that solve the all words WSD task are supervised as illustrated in the different SenseEval and SEMEVAL All Words tasks (M. Palmer and Dang, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised approach to the all words WSD problem relying on WordNet similarity measures. We will review only three of the most relevant related research due to space limitations. We acknowledge the existence of many research papers that tackled the problem using unsupervised approaches. Firstly, in work by (Pedersen and Patwardhan, 2005), the authors investigate different word similarity measures as a means of disambiguating words in context. They compare among different similarity measures. They show that using an extension on the Lesk similarity measure (Lesk"
W09-2410,W04-0811,0,0.675374,"the noun bank will be assigned the meaning sloping land especially the slope beside a body of water and so on. 3 Related Works Many systems over the years have been used for the task. A thorough review of the current state of the art is in (Navigli, 2009). Several techniques have been used to tackle the problem ranging from rule based/knowledge based approaches to unsupervised and supervised machine learning approaches. To date, the best approaches that solve the all words WSD task are supervised as illustrated in the different SenseEval and SEMEVAL All Words tasks (M. Palmer and Dang, 2001; Snyder and Palmer, 2004; Pradhan et al., 2007). In this paper, we present an unsupervised approach to the all words WSD problem relying on WordNet similarity measures. We will review only three of the most relevant related research due to space limitations. We acknowledge the existence of many research papers that tackled the problem using unsupervised approaches. Firstly, in work by (Pedersen and Patwardhan, 2005), the authors investigate different word similarity measures as a means of disambiguating words in context. They compare among different similarity measures. They show that using an extension on the Lesk s"
W09-2903,W03-1812,0,0.686797,"a decision, etc. An MWE typically has an idiosyncratic meaning that is more or different from the meaning of its component words. An MWE meaning is transparent, i.e. predictable, in as much as the component words in the expression relay the meaning portended by the speaker compositionally. Accordingly, MWEs vary in their degree of meaning compositionality; compositionality is correlated with the level of idiomaticity. An MWE is compositional if the meaning of an To date, most research has addressed the problem of MWE type classiﬁcation for VNC expressions in English (Melamed, 1997; Lin, 1999; Baldwin et al., 2003; na Villada Moir´on and Tiedemann, 2006; Fazly and Stevenson, 2007; Van de Cruys and Villada Moir´on, 2007; McCarthy et al., 2007), not token classiﬁcation. For example: he spilt the beans on the kitchen counter is most likely a literal usage. This is given away by the use of the prepositional phrase on the kitchen counter, as it is plausable that beans could have literally been spilt on a location such as a kitchen counter. Most previous research would classify spilt the beans as idiomatic irrespective of contextual usage. In a recent study by (Cook et al., 2008) of 53 idiom MWE types used i"
W09-2903,W07-1102,0,0.22082,"Missing"
W09-2903,E06-1042,0,0.0366118,"e translation. 17 Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 17–22, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP In this paper, we address the problem of MWE classiﬁcation for verb-noun (VNC) token constructions in running text. We investigate the binary classiﬁcation of an unseen VNC token expression as being either Idiomatic (IDM) or Literal (LIT). An IDM expression is certainly an MWE, however, the converse is not necessarily true. To date most approaches to the problem of idiomaticity classiﬁcation on the token level have been unsupervised (Birke and Sarkar, 2006; Diab and Krishna, 2009b; Diab and Krishna, 2009a; Sporleder and Li, 2009). In this study we carry out a supervised learning investigation using support vector machines that uses some of the features which have been shown to help in unsupervised approaches to the problem. This paper is organized as follows: In Section 2 we describe our understanding of the various classes of MWEs in general. Section 3 is a summary of previous related research. Section 4 describes our approach. In Section 5 we present the details of our experiments. We discuss the results in Section 6. Finally, we conclude in"
W09-2903,D08-1104,0,0.221896,"Missing"
W09-2903,W07-1106,0,0.792101,"Missing"
W09-2903,P06-2046,0,0.146622,"Missing"
W09-2903,W06-1203,0,0.644878,"ng of the various classes of MWEs in general. Section 3 is a summary of previous related research. Section 4 describes our approach. In Section 5 we present the details of our experiments. We discuss the results in Section 6. Finally, we conclude in Section 7. 2 Hashimoto et al., 2006; Hashimoto and Kawahara, 2008). The majority of the proposed research has been using unsupervised approaches and have addressed the problem of MWE type classiﬁcation irrespective of usage in context (Fazly and Stevenson, 2007; Cook et al., 2007). We are aware of two supervised approaches to the problem: work by (Katz and Giesbrecht, 2006) and work by (Hashimoto and Kawahara, 2008). In Katz and Giesbrecht (2006) (KG06) the authors carried out a vector similarity comparison between the context of an MWE and that of the constituent words using LSA to determine if the expression is idiomatic or not. The KG06 is similar in intuition to work proposed by (Fazly and Stevenson, 2007), however the latter work was unsupervised. KG06 experimented with a tiny data set of only 108 sentences corresponding to one MWE idiomatic expression. Hashimoto and Kawahara (2008) (HK08) is the ﬁrst large scale study to our knowledge that addressed token"
W09-2903,P99-1041,0,0.143853,"Missing"
W09-2903,D07-1039,0,0.0984648,"Missing"
W09-2903,W97-0311,0,0.533006,"Missing"
W09-2903,W06-2405,0,0.282827,"Missing"
W09-2903,W01-0513,0,0.0860213,"ach We apply a supervised learning framework to the problem of both identifying and classifying a MWE expression token in context. We speciﬁcally focus on VNC MWE expressions. We use the annotated data by (Cook et al., 2008). We adopt a chunking approach to the problem using an Inside Outside Beginning (IOB) tagging framework for performing the identiﬁcation of MWE VNC tokens and classifying them as idiomatic or literal in context. For chunk tagging, we use the YamRelated Work Several researchers have addressed the problem of MWE classiﬁcation (Baldwin et al., 2003; Katz and Giesbrecht, 2006; Schone and Juraksfy, 2001; 18 Cha sequence labeling system.1 YamCha is based on Support Vector Machines technology using degree 2 polynomial kernels. We label each sentence with standard IOB tags. Since this is a binary classiﬁcation task, we have 5 different tags: B-L (Beginning of a literal chunk), I-L (Inside of a literal chunk), B-I (Beginning an Idiomatic chunk), I-I (Inside an Idiomatic chunk), O (Outside a chunk). As an example a sentence such as John kicked the bucket last Friday will be annotated as follows: John O, kicked B-I, the I-I, bucket I-I, last O, Friday O. We experiment with some basic features and"
W09-2903,E09-1086,0,0.075731,"s, ACL-IJCNLP 2009, pages 17–22, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP In this paper, we address the problem of MWE classiﬁcation for verb-noun (VNC) token constructions in running text. We investigate the binary classiﬁcation of an unseen VNC token expression as being either Idiomatic (IDM) or Literal (LIT). An IDM expression is certainly an MWE, however, the converse is not necessarily true. To date most approaches to the problem of idiomaticity classiﬁcation on the token level have been unsupervised (Birke and Sarkar, 2006; Diab and Krishna, 2009b; Diab and Krishna, 2009a; Sporleder and Li, 2009). In this study we carry out a supervised learning investigation using support vector machines that uses some of the features which have been shown to help in unsupervised approaches to the problem. This paper is organized as follows: In Section 2 we describe our understanding of the various classes of MWEs in general. Section 3 is a summary of previous related research. Section 4 describes our approach. In Section 5 we present the details of our experiments. We discuss the results in Section 6. Finally, we conclude in Section 7. 2 Hashimoto et al., 2006; Hashimoto and Kawahara, 2008). The maj"
W09-2903,W07-1104,0,0.153687,"Missing"
W09-3012,J04-3002,0,0.199612,"Missing"
W09-3012,krestel-etal-2008-minding,0,0.0374878,"Missing"
W09-3012,C08-1101,0,0.0165575,"is is not because we think this is the only interesting information in text, but we do this in order to obtain a manageable annotation in our pilot study. Specifically, we annotate whether the writer intends the reader to interpret a stated proposition as the writer’s strongly held belief, as a proposition which the writer does not believe strongly (but could), or as a proposition towards which the writer has an entirely different cognitive attitude, such as desire or intention. We do not annotate subjectivity (Janyce Wiebe and Martin, 2004; Wilson and Wiebe, 2005), nor opinion (for example: (Somasundaran et al., 2008)): the nature of the proposition (opinion and type of opinion, statement about interior world, external world) is not of interest. Thus, this work is orthogonal to the extensive literature on opinion detection. And we do not annotate truth: real-world (encyclopedic) truth is not relevant. We have three categories: • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. A subcase of committed belief concerns propositions about the future, such as GM wil"
W09-3012,W05-0308,0,0.0144516,"d we are only interested in the writer’s beliefs. This is not because we think this is the only interesting information in text, but we do this in order to obtain a manageable annotation in our pilot study. Specifically, we annotate whether the writer intends the reader to interpret a stated proposition as the writer’s strongly held belief, as a proposition which the writer does not believe strongly (but could), or as a proposition towards which the writer has an entirely different cognitive attitude, such as desire or intention. We do not annotate subjectivity (Janyce Wiebe and Martin, 2004; Wilson and Wiebe, 2005), nor opinion (for example: (Somasundaran et al., 2008)): the nature of the proposition (opinion and type of opinion, statement about interior world, external world) is not of interest. Thus, this work is orthogonal to the extensive literature on opinion detection. And we do not annotate truth: real-world (encyclopedic) truth is not relevant. We have three categories: • Committed belief (CB): the writer indicates in this utterance that he or she believes the proposition. For example, GM has laid off workers, or, even stronger, We know that GM has laid off workers. A subcase of committed belief"
W10-1836,W03-1006,0,0.060505,". 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument,"
W10-1836,choi-etal-2010-propbank-instance,1,0.897928,"integrated manner, how to identify one construction from the other, figuring out a language specific reliable diagnostic test, and whether we deal with these constructions as a whole unit or as separate parts; and how? (Hwang, et al., 2010) 4.2 Tools Frameset files are created in an XML format. During the Pilot Propbank project these files were created manually by editing the XML file related to a particular predicate. This proved to be time consuming and prone to many formatting errors. The Frame File creation for the revised APB is now performed with the recently developed Cornerstone tool (Choi et al., 2010a), which is a PropBank frameset editor that allows the creation and editing of Propbank framesets without requiring any prior knowledge of XML. Moreover, the annotation is now performed by Jubilee, a new annotation tool, which has improved the annotation process by displaying several types of relevant syntactic and semantic information at the same time. Having everything displayed helps the annotator quickly absorb and apply the necessary syntactic and semantic information pertinent to each predicate for consistent and efficient annotation (Choi et al., 20010b). Both tools are available as Op"
W10-1836,P08-1091,1,0.8821,"e Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be labeled as the agent/experiencer and ‘movies’ w"
W10-1836,J02-3001,0,0.0350371,"the data tagging and also simplify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the"
W10-1836,P02-1031,1,0.76542,"d multi-word expressions. New tools facilitate the data tagging and also simplify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the"
W10-1836,N07-2014,0,0.0683748,"Missing"
W10-1836,N06-2015,1,0.851769,"Missing"
W10-1836,W10-1810,1,0.834857,"ing of cohesion in the group. The APB has decided to thoroughly tackle light verb constructions and multi-word expressions as part of an effort to facilitate mapping between the different languages that are being PropBanked. In the process of setting this up a number of challenges have surfaced which include: how can we cross-linguistically approach these phenomena in a (semi) integrated manner, how to identify one construction from the other, figuring out a language specific reliable diagnostic test, and whether we deal with these constructions as a whole unit or as separate parts; and how? (Hwang, et al., 2010) 4.2 Tools Frameset files are created in an XML format. During the Pilot Propbank project these files were created manually by editing the XML file related to a particular predicate. This proved to be time consuming and prone to many formatting errors. The Frame File creation for the revised APB is now performed with the recently developed Cornerstone tool (Choi et al., 2010a), which is a PropBank frameset editor that allows the creation and editing of Propbank framesets without requiring any prior knowledge of XML. Moreover, the annotation is now performed by Jubilee, a new annotation tool, w"
W10-1836,P04-1043,0,0.0648245,"e in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be lab"
W10-1836,W05-0630,0,0.0204675,"tomated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be labeled as the agent/experi"
W10-1836,J05-1004,1,0.599345,"aper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, and the second argument, the object, is ‘movies’. ‘John’ would be labeled as the agent/experiencer and ‘movies’ would be the theme/content. According to PropBank, ‘John’ is labeled Arg0 (or enjoyer) and ‘movies’ is labeled Arg1 (or thing enjoyed). Crucially, that independent of the l"
W10-1836,N04-1030,1,0.800833,"New tools facilitate the data tagging and also simplify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’"
W10-1836,W04-3212,1,0.799084,"implify frame creation. 1 Introduction Recent years have witnessed a surge in available automated resources for the Arabic language. 1 These resources can now be exploited by the computational linguistics community with the aim of improving the automatic processing of Arabic. This paper discusses semantic labeling. Shallow approaches to semantic processing are making large advances in the direction of efficiently and effectively deriving application relevant explicit semantic information from text (Pradhan et al., 2003; Gildea and Palmer, 2002; Pradhan et al., 2004; Gildea and Jurafsky, 2002; Xue and Palmer, 2004; Chen and Rambow, 2003; Carreras and Marquez, 2005; Moschitti, 2004; Moschitti et al., 2005; Diab et al., 2008). Indeed, the existence of semantically annotated resources in English such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury and Palmer, 2003; Palmer et al., 2005) corpora have marked a surge in efficient approaches to automatic se1 In this paper, we use Arabic to refer to Modern Standard Arabic (MSA). mantic labeling of the English language. For example, in the English sentence, ‘John enjoys movies’, the predicate is ‘enjoys’ and the first argument, the subject, is ‘John’, a"
W10-1836,P80-1024,0,0.623039,"Missing"
W10-1836,P09-5003,0,\N,Missing
W10-1836,W05-0620,0,\N,Missing
W10-1836,palmer-etal-2008-pilot,1,\N,Missing
W11-0413,W09-3012,1,0.485063,"es and provide examples from our corpus exemplifying the different phenomena. Throughout the paper, we discuss expression of subjectivity in natural language, combining various previously scattered insights belonging to many branches of linguistics. 1 Introduction As the volume of web data continues to phenomenally increase, researchers are becoming more interested in mining that data and making the information therein accessible to end-users in various innovative ways. As a result, searches and processing of data beyond the limiting level of surface words are becoming increasingly important (Diab et al., 2009). The sentiment expressed in Web data specifically continues to be of high interest and value to internet users, businesses, and governmental bodies. Thus, the area of Subjectivity and sentiment analysis (SSA) has been witnessing a flurry of novel research. Subjectivity in natural language refers to aspects of language used to express opinions, feelings, evaluations, and speculations (Banfield, 1982; Wiebe, In spite of the great interest in SSA, only few studies have been conducted on morphologicallyrich languages (MRL) (i.e., languages in which significant information concerning syntactic uni"
W11-0413,C04-1200,0,0.359557,"Missing"
W11-0413,W06-2915,0,0.119935,"lad” which is a private state (i.e., a state that is not subject to direct verification) (Quirk et al., 1974). 3.2 Good & Bad News News can be good or bad. For example, whereas ”Five persons were killed in a car accident” is bad news, ”It is sunny and warm today in Chicago” is good news. Our coders were instructed not to consider good news positive nor bad news negative if they think the sentences expressing them are objectively reporting information. Thus, bad news and good news can be OBJ as is the case in both examples. 3.3 Perspective Some sentences are written from a certain perspective (Lin et al., 2006) or point of view. Consider the two sentences (1) ”Israeli soldiers, our heroes, are keen on protecting settlers” and (2) ”Palestinian freedom fighters are willing to attack these Israeli targets”. Sentence (1) is written from an Israeli perspective, while sentence (2) is written from a Palestinian perspective. The perspective from which a sentence is written interplays with how sentiment is assigned. Sentence (1) is considered positive from an Israeli perspective, yet the act of protecting settlers is considered negative from a Palestinian perspective. Similarly, attacking Israeli targets may"
W11-0413,2007.sigdial-1.5,0,0.00828931,"teration: w ybdw An Altktm Al*y AHAT bzyArp byryz AlY AndwnysyA kAn yhdf AlY tfAdy AvArp rdwd fEl mEAdyp fy AlblAd. English: It seems that the secrecy surrounding Peres’s visit to Indonesia was aimed at avoiding negative reactions in the country.   k QÓ B@ ù¢« @ é@ ñªË@ àA¢J.¯ à @ l . P B@ úÎ«ð (21) K  B@ É¿ ZA®£A .AîD JÓ úÎ« HB . Transliteration: wElY AlArjH An qbTAn AlgwASp AETY AlAmr bATfA’ kl AlAlAt ElY mtnhA. English: Most likely the submarine’s captain ordered turning off all the machines on board. Some S-NEUT cases are examples of arguing that something is true or should be done (Somasundaran et al., 2007). (22) is an illustrative example:  Ö Ï A¯ ,AëPQ» @ð ,AîDÊ ¯ (22)  Ë éÊ ÐAm Ì '@ ¡® JË@ ú ¯ I  ® JÖ Ï @ ú¯ AÖß @ ð . éJ ¢® JË@ HA Transliteration: qlthA, wAkrrhA, fAlm$klp lyst fy AlnfT AlxAm wInmA fy Alm$tqAt AlnfTyp. English: I said, and I repeat it, the problem is not in crude oil but rather in oil derivatives. Example 22 was, however, initially tagged as OBJ. Later, the two annotators agreed to assign it an S-NEUT tag. 6 Related Work There are a number of datasets annotated for SSA. Most relevant to us is work on the news genre. (Wiebe et al., 2005) describe a fine-grained n"
W11-0413,W10-1401,0,0.0354738,"continues to be of high interest and value to internet users, businesses, and governmental bodies. Thus, the area of Subjectivity and sentiment analysis (SSA) has been witnessing a flurry of novel research. Subjectivity in natural language refers to aspects of language used to express opinions, feelings, evaluations, and speculations (Banfield, 1982; Wiebe, In spite of the great interest in SSA, only few studies have been conducted on morphologicallyrich languages (MRL) (i.e., languages in which significant information concerning syntactic units and relations are expressed at the word-level (Tsarfaty et al., 2010)). Arabic, Hebrew, Turkish, Czech, and Basque are examples of MRLs. SSA work on MRLs has been hampered by lack of annotated data. In the current paper we report efforts to manually annotate a corpus of Modern Standard Arabic (MSA), a morphologically-rich variety of Arabic, e.g., (Diab et al., 2007; Habash et al., 2009). The corpus is a collection of documents from the newswire genre covering several domains such as politics and sports. We label the data at the sentence level. Our annotation guidelines explicitly incorporate linguisticallymotivated information. 110 Proceedings of the Fifth Law"
W11-0413,P99-1032,0,0.496906,"Missing"
W11-0413,J94-2004,0,0.362688,"Missing"
W11-3217,D11-1128,0,\N,Missing
W11-3217,W10-2408,0,\N,Missing
W11-3217,P07-2045,0,\N,Missing
W11-3217,W09-3510,0,\N,Missing
W11-3217,W09-3525,0,\N,Missing
W11-3217,W09-3519,0,\N,Missing
W11-3217,W10-2406,0,\N,Missing
W11-3217,P00-1056,0,\N,Missing
W11-3217,W09-3527,0,\N,Missing
W11-3407,D08-1102,0,0.431567,"f Leveraging Crowd Sourcing for the Creation of a Large Scale Annotated Resource for Hindi English Code Switched Data: A Pilot Annotation Mona Diab Center for Computational Learning Systems Columbia University, New York mdiab@ccls.columbia.edu Abstract LCS occurs in all genres of communication for such speakers, including spoken conversation, email, online chat rooms, blogs and newsgroups. Thus, it seriously impacts attempts to process these exchanges computationally, for the purposes of automatic translation, speech recognition, and information extraction, inter alia (Solorio and Liu, 2008a; Solorio and Liu, 2008b). With increasing interest in LCS, there is need for large annotated LCS corpora which can support the needs of computational as well as theoretical research. This paper presents one experiment where a corpus of code switched sentences is annotated for identifying code switch points using crowd sourcing methods. The data collection serves as the first attempt at creating a repository for LCS data. Also the annotations of LCS points will shed light into the nature of this phenomenon and will be an initial building block for the development of interesting analytical and predictive models for a"
W11-3407,D08-1110,0,0.187591,"f Leveraging Crowd Sourcing for the Creation of a Large Scale Annotated Resource for Hindi English Code Switched Data: A Pilot Annotation Mona Diab Center for Computational Learning Systems Columbia University, New York mdiab@ccls.columbia.edu Abstract LCS occurs in all genres of communication for such speakers, including spoken conversation, email, online chat rooms, blogs and newsgroups. Thus, it seriously impacts attempts to process these exchanges computationally, for the purposes of automatic translation, speech recognition, and information extraction, inter alia (Solorio and Liu, 2008a; Solorio and Liu, 2008b). With increasing interest in LCS, there is need for large annotated LCS corpora which can support the needs of computational as well as theoretical research. This paper presents one experiment where a corpus of code switched sentences is annotated for identifying code switch points using crowd sourcing methods. The data collection serves as the first attempt at creating a repository for LCS data. Also the annotations of LCS points will shed light into the nature of this phenomenon and will be an initial building block for the development of interesting analytical and predictive models for a"
W11-3407,D08-1027,0,\N,Missing
W11-3407,C82-1023,0,\N,Missing
W12-2511,W11-0403,0,0.199417,"ization and information extraction. In order to build robust SRL systems there is a need for significant resources the most important of which are semantically annotated resources such as proposition banks. Several such resources exist now for different languages including FrameNet (Baker et al., 1998), VerbNet (Kipper et al. 2000) and PropBank (Palmer et al., 2005). These resources have marked a surge in efficient approaches to automatic SRL of the English language. Apart from English, there exist various PropBank projects in Chinese (Xue et al., 2009), Korean (Palmer et al. 2006) and Hindi (Ashwini et al., 2011). These resources exist on a large scale spearheading the SRL research in the associated languages (Carreras and Marquez, 2005), Surdeanu et al. (2008). However, resources created for Arabic are significantly more modest. The only Arabic Propank [APB] project (Zaghouani et al., 2010; Diab et al., 2008) based on the phrase structure syntactic Arabic Treebank (Maamouri et al. 2010) comprises a little over 4.5K verbs of newswire modern standard Arabic. Apart from the modesty in size, the Arabic language genre used in the APB does not represent the full scope of the Arabic language. The Arabic cul"
W12-2511,P98-1013,0,0.0204143,"he correlate of this characterization in natural language processing literature (Gildea and Jurafsky 2002). In SRL, the system automatically identifies predicates and their arguments and tags the identified arguments with meaningful semantic information. SRL has been successfully used in machine translation, summarization and information extraction. In order to build robust SRL systems there is a need for significant resources the most important of which are semantically annotated resources such as proposition banks. Several such resources exist now for different languages including FrameNet (Baker et al., 1998), VerbNet (Kipper et al. 2000) and PropBank (Palmer et al., 2005). These resources have marked a surge in efficient approaches to automatic SRL of the English language. Apart from English, there exist various PropBank projects in Chinese (Xue et al., 2009), Korean (Palmer et al. 2006) and Hindi (Ashwini et al., 2011). These resources exist on a large scale spearheading the SRL research in the associated languages (Carreras and Marquez, 2005), Surdeanu et al. (2008). However, resources created for Arabic are significantly more modest. The only Arabic Propank [APB] project (Zaghouani et al., 201"
W12-2511,choi-etal-2010-propbank-instance,0,0.0278624,"Missing"
W12-2511,palmer-etal-2008-pilot,1,0.935487,"erbNet (Kipper et al. 2000) and PropBank (Palmer et al., 2005). These resources have marked a surge in efficient approaches to automatic SRL of the English language. Apart from English, there exist various PropBank projects in Chinese (Xue et al., 2009), Korean (Palmer et al. 2006) and Hindi (Ashwini et al., 2011). These resources exist on a large scale spearheading the SRL research in the associated languages (Carreras and Marquez, 2005), Surdeanu et al. (2008). However, resources created for Arabic are significantly more modest. The only Arabic Propank [APB] project (Zaghouani et al., 2010; Diab et al., 2008) based on the phrase structure syntactic Arabic Treebank (Maamouri et al. 2010) comprises a little over 4.5K verbs of newswire modern standard Arabic. Apart from the modesty in size, the Arabic language genre used in the APB does not represent the full scope of the Arabic language. The Arabic culture has a long history of literary writing and a rich linguistic heritage in classical Arabic. In fact all historical religious non-religious texts are written in Classical Arabic. The ultimate source on classical Arabic language is the Quran. It is considered the Arabic language reference point for a"
W12-2511,W08-2121,0,0.0372406,"antically annotated resources such as proposition banks. Several such resources exist now for different languages including FrameNet (Baker et al., 1998), VerbNet (Kipper et al. 2000) and PropBank (Palmer et al., 2005). These resources have marked a surge in efficient approaches to automatic SRL of the English language. Apart from English, there exist various PropBank projects in Chinese (Xue et al., 2009), Korean (Palmer et al. 2006) and Hindi (Ashwini et al., 2011). These resources exist on a large scale spearheading the SRL research in the associated languages (Carreras and Marquez, 2005), Surdeanu et al. (2008). However, resources created for Arabic are significantly more modest. The only Arabic Propank [APB] project (Zaghouani et al., 2010; Diab et al., 2008) based on the phrase structure syntactic Arabic Treebank (Maamouri et al. 2010) comprises a little over 4.5K verbs of newswire modern standard Arabic. Apart from the modesty in size, the Arabic language genre used in the APB does not represent the full scope of the Arabic language. The Arabic culture has a long history of literary writing and a rich linguistic heritage in classical Arabic. In fact all historical religious non-religious texts ar"
W12-2511,W04-3212,0,0.0870691,"Missing"
W12-2511,W10-1836,1,0.844528,"Missing"
W12-2511,C98-1013,0,\N,Missing
W12-2511,W05-0620,0,\N,Missing
W12-2511,J02-3001,0,\N,Missing
W12-3403,W10-3704,0,0.080759,"Missing"
W12-3403,R13-1005,0,0.0732877,"Missing"
W12-3403,N10-1029,1,0.825431,"of almost 3000 English sentences annotated with VNC usage at the token level. Katz and Giesbrecht (2006) carried out a vector similarity comparison between the context of an English MWE and that of the constituent words using Latent Semantic Analysis to determine if the expression is idiomatic or not. In work by Hashimoto and Kawahara (2008), they addressed token classification into idiomatic versus literal for Japanese MWEs of all types. They annotated a corpus of 102K sentences, and used it to train a supervised classifier for MWEs. Using MWEs in machine translation is another application. Carpuat and Diab (2010) studied the effect of integrating English MWEs with a statistical translation system. They used the WordNet 3.0 lexical database (Fellbaum, 1998) as the main source for MWEs. Attia et al., in 2010, extracted Arabic MWEs from various resources. They focused only on nominal MWEs and used diverse techniques for automatic MWE extraction from cross-lingual parallel Wikipedia titles, machine-translated English MWEs taken from the English WordNet and the Arabic Gigaword 4.0 corpus. They found a large number of MWEs, however only a few of them were evaluated. In this paper, we describe the process of"
W12-3403,W09-2903,1,0.881598,"s for each word to assist an automated identification of MWEs in a large corpus of text. Part of the Arabic Gigaword 4.0 (Parker, 2009) is processed accordingly and the MWEs are annotated based on a deterministic algorithm considering different variants of every MWE in our list. There are diverse tasks that require a corpus with annotated MWEs, which have not been addressed in Arabic due to the lack of such a resource. However, a lot of attention is put on those tasks when implemented in English and other languages. Among those tasks, classifying MWEs in a running text is the most common one. Diab and Bhutada (2009) applied a supervised learning framework to the problem of classifying token level English MWEs in context. They used the annotated corpus provided by Cook (2008), a resource of almost 3000 English sentences annotated with VNC usage at the token level. Katz and Giesbrecht (2006) carried out a vector similarity comparison between the context of an English MWE and that of the constituent words using Latent Semantic Analysis to determine if the expression is idiomatic or not. In work by Hashimoto and Kawahara (2008), they addressed token classification into idiomatic versus literal for Japanese M"
W12-3403,P05-1071,0,0.0237699,"tagging of MWEs in running text, based on pattern matching. Sections 4 and 5 summarize the results of applying the pattern-matching algorithm on a corpus. Finally, we conclude in Section 6. 2 Arabic MWE List Our Arabic MWE list is created based on a collection of about 5,000 expressions, which is manually extracted from various Arabic dictionaries (Abou Saad, 1987; Seeny et al., 1996; Dawod, 2003; Fayed, 2007). Each MWE is preprocessed by the following steps: 1) cleaning punctuations and unnecessary characters, 2) breaking alternative expressions into individual entries, and 3) running MADA (Habash and Rambow, 2005; Roth et al, 2008) on each MWE individually for finding the context-sensitive morphological analysis for every word. Some of the extracted MWEs are originally enriched with placeholder generic words that play the same semantic role in the context of the MWE. That set of generic words is manually normalized and reduced to a group of types, as shown in Table 2. Generic Type flAn “so-and-so” a person k*A “something“ an object &lt;mr “something” an issue Semantic Role Agent/Patient Goal Source Example qr flAn EynA “pleased someone“ ElY HsAb k*A “at the expense of that/this” &lt;mr Abn ywmh “something v"
W12-3403,D08-1104,0,0.0537497,"Missing"
W12-3403,W06-1203,0,0.0348384,"st. There are diverse tasks that require a corpus with annotated MWEs, which have not been addressed in Arabic due to the lack of such a resource. However, a lot of attention is put on those tasks when implemented in English and other languages. Among those tasks, classifying MWEs in a running text is the most common one. Diab and Bhutada (2009) applied a supervised learning framework to the problem of classifying token level English MWEs in context. They used the annotated corpus provided by Cook (2008), a resource of almost 3000 English sentences annotated with VNC usage at the token level. Katz and Giesbrecht (2006) carried out a vector similarity comparison between the context of an English MWE and that of the constituent words using Latent Semantic Analysis to determine if the expression is idiomatic or not. In work by Hashimoto and Kawahara (2008), they addressed token classification into idiomatic versus literal for Japanese MWEs of all types. They annotated a corpus of 102K sentences, and used it to train a supervised classifier for MWEs. Using MWEs in machine translation is another application. Carpuat and Diab (2010) studied the effect of integrating English MWEs with a statistical translation sys"
W12-3403,P08-2030,1,\N,Missing
W12-3705,abdul-mageed-diab-2012-awatif,1,0.518069,"lexicon. For sentiment classification, we apply two features, has POS adjective and has NEG adjective. These binary features indicate whether a POS or NEG adjective from the lexicon occurs in a sentence. 4.4 Dialectal Arabic Features Dialect: We apply the two gold language variety features, {MSA, DA}, on the Twitter data set to represent whether the tweet is in MSA or in a dialect. 4.5 Genre Specific Features Gender: Inspired by gender variation research exploiting social media data (e.g., (Herring, 1996)), we apply three gender (GEN) features corresponding to the set {MALE, FEMALE, UNKNOWN}. Abdul-Mageed and Diab (2012a) suggest that there is a relationship between politeness strategies and sentiment expression. And gender variation research in social media shows that expression of linguistic politeness (Brown and Levinson, 1987) differs based on the gender of the user. User ID: The user ID (UID) labels are inspired by research on Arabic Twitter showing that a considerable share of tweets is produced by organizations such as news agencies (Abdul-Mageed et al., 2011a) as opposed to lay users. We hence employ two features from the set {PERSON, ORGANIZATION} to classification of the Twitter data set. The assum"
W12-3705,P11-2103,1,0.514872,"dbaths in Syria are horrifying!), neutral (e.g., Obama may sign the bill.), or, sometimes, mixed (e.g., The iPad is cool, but way too expensive). In this work, we address two main issues in Subjectivity and Sentiment Analysis (SSA): First, SSA has mainly been conducted on a small number of genres such as newspaper text, customer reports, 19 and blogs. This excludes, for example, social media genres (such as Wikipedia Talk Pages). Second, despite increased interest in the area of SSA, only few attempts have been made to build SSA systems for morphologically-rich languages (Abbasi et al., 2008; Abdul-Mageed et al., 2011b), i.e. languages in which a significant amount of information concerning syntactic units and relations is expressed at the word-level, such as Finnish or Arabic. We thus aim at partially bridging these two gaps in research by developing an SSA system for Arabic, a morphologically highly complex languages (Diab et al., 2007; Habash et al., 2009). We present SAMAR, a sentence-level SSA system for Arabic social media texts. We explore the SSA task on four different genres: chat, Twitter, Web forums, and Wikipedia Talk Pages. These genres vary considerably in terms of their functions and the lan"
W12-3705,R11-1096,1,0.934465,"dbaths in Syria are horrifying!), neutral (e.g., Obama may sign the bill.), or, sometimes, mixed (e.g., The iPad is cool, but way too expensive). In this work, we address two main issues in Subjectivity and Sentiment Analysis (SSA): First, SSA has mainly been conducted on a small number of genres such as newspaper text, customer reports, 19 and blogs. This excludes, for example, social media genres (such as Wikipedia Talk Pages). Second, despite increased interest in the area of SSA, only few attempts have been made to build SSA systems for morphologically-rich languages (Abbasi et al., 2008; Abdul-Mageed et al., 2011b), i.e. languages in which a significant amount of information concerning syntactic units and relations is expressed at the word-level, such as Finnish or Arabic. We thus aim at partially bridging these two gaps in research by developing an SSA system for Arabic, a morphologically highly complex languages (Diab et al., 2007; Habash et al., 2009). We present SAMAR, a sentence-level SSA system for Arabic social media texts. We explore the SSA task on four different genres: chat, Twitter, Web forums, and Wikipedia Talk Pages. These genres vary considerably in terms of their functions and the lan"
W12-3705,W07-0812,1,0.795461,"HsnAt in the TOK setting. POS tagging: Since we use only the base forms of words, the question arises whether we lose meaningful morphological information and consequently whether we could represent this information in the POS tags instead. Thus, we use two sets of POS features that are specific to Arabic: the reduced tag set (RTS) and the extended reduced tag set (ERTS) (Diab, 2009). The RTS is composed of 42 tags and reflects only number for nouns and some tense information for verbs whereas the ERTS comprises 115 tags and enriches the RTS with gender, number, and definiteness information. Diab (2007b; 2007a) shows that using the ERTS improves results for higher processing tasks such as base phrase chunking of Arabic. 4.3 Standard Features This group includes two features that have been employed in various SSA studies. 22 Unique: Following Wiebe et al. (2004), we apply a UNIQUE (Q) feature: We replace low frequency words with the token ”UNIQUE”. Experiments showed that setting the frequency threshold to 3 yields the best results. Polarity Lexicon (PL): The lexicon (cf. section 3) is used in two different forms for the two tasks: For subjectivity classification, we follow Bruce and Wiebe ("
W12-3705,C04-1200,0,0.213208,"Missing"
W12-3705,P02-1053,0,0.00892885,"nt for SSA. More specifically, we concentrate on two questions: Since we need to reduce word forms to base forms to combat data sparseness, is it more useful to use tokenization or lemmatization? And given that the part-of-speech (POS) tagset for Arabic contains a fair amount of morphological information, how much of this information is useful for SSA? More specifically, we investigate two different reduced tagsets, the RTS and the ERTS. For more detailed information see section 4. RQ2 addresses the impact of using two standard features, frequently employed in SSA studies (Wiebe et al., 2004; Turney, 2002), on social media data, which exhibit DA usage and text length variations, e.g. in twitter data. First, we investigate the utility of applying a UNIQUE feature (Wiebe et al., 2004) where low frequency words below a threshold are replaced with the token ”UNIQUE”. Given that our data includes very short posts (e.g., twitter data has a limit of only 140 characters per tweet), it is questionable whether the UNIQUE feature will be useful or whether it replaces too many content words. Second, we test whether a polarity lexicon extracted in a standard domain using Modern Standard Arabic (MSA) transfe"
W12-3705,P99-1032,0,0.404161,"Missing"
W12-3705,J04-3002,0,0.0593286,"tion that is important for SSA. More specifically, we concentrate on two questions: Since we need to reduce word forms to base forms to combat data sparseness, is it more useful to use tokenization or lemmatization? And given that the part-of-speech (POS) tagset for Arabic contains a fair amount of morphological information, how much of this information is useful for SSA? More specifically, we investigate two different reduced tagsets, the RTS and the ERTS. For more detailed information see section 4. RQ2 addresses the impact of using two standard features, frequently employed in SSA studies (Wiebe et al., 2004; Turney, 2002), on social media data, which exhibit DA usage and text length variations, e.g. in twitter data. First, we investigate the utility of applying a UNIQUE feature (Wiebe et al., 2004) where low frequency words below a threshold are replaced with the token ”UNIQUE”. Given that our data includes very short posts (e.g., twitter data has a limit of only 140 characters per tweet), it is questionable whether the UNIQUE feature will be useful or whether it replaces too many content words. Second, we test whether a polarity lexicon extracted in a standard domain using Modern Standard Arabi"
W12-3705,W03-1017,0,0.249658,"Missing"
W12-3807,P11-2049,0,0.0155415,"tion and automatic tagging of the belief modality (i.e., factivity) is described in more detail in (Diab et al., 2009b; Prabhakaran et al., 2010). There has been a considerable amount of interest in modality in the biomedical domain. Negation, uncertainty, and hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical const"
W12-3807,W09-1324,0,0.0193045,"nd hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they often render the prese"
W12-3807,baker-etal-2010-modality,1,0.685998,"Missing"
W12-3807,J12-2006,1,0.846788,"features used to train our modality tagger and presents experiments and results. Section 5 concludes and discusses future work. 58 2 Related Work Previous related work includes TimeML (Sauri et al., 2006), which involves modality annotation on events, and Factbank (Sauri and Pustejovsky, 2009), where event mentions are marked with degree of factuality. Modality is also important in the detection of uncertainty and hedging. The CoNLL shared task in 2010 (Farkas et al., 2010) deals with automatic detection of uncertainty and hedging in Wikipedia and biomedical sentences. Baker et al. (2010) and Baker et al. (2012) analyze a set of eight modalities which include belief, require and permit, in addition to the five modalities we focus on in this paper. They built a rule-based modality tagger using a semi-automatic approach to create rules. This earlier work differs from the work described in this paper in that the our emphasis is on the creation of an automatic modality tagger using machine learning techniques. Note that the annotation and automatic tagging of the belief modality (i.e., factivity) is described in more detail in (Diab et al., 2009b; Prabhakaran et al., 2010). There has been a considerable"
W12-3807,W09-3012,1,0.821042,"y of the information. Did the speaker 57 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 57–64, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics have firsthand knowledge of what he or she is reporting, or was it hearsay or inferred from indirect evidence? Sentiment deals with a speaker’s positive or negative feelings toward an event, state, or proposition. In this paper, we focus on the following five modalities; we have investigated the belief/factivity modality previously (Diab et al., 2009b; Prabhakaran et al., 2010), and we leave other modalities to future work. • Ability: can H do P? • Effort: does H try to do P? • Intention: does H intend P? • Success: does H succeed in P? • Want: does H want P? We investigate automatically training a modality tagger by using multi-class Support Vector Machines (SVMs). One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a modality tagger because modality triggers are sparse for the overwhelming majority of the sentences. Baker et al. (2010) created a modality tagg"
W12-3807,P03-1004,0,0.0266465,"removed the ones which did not in fact have a modality. In the remaining sentences (94 sentences), our expert annotated the target predicate. We refer to this as the Gold dataset in this paper. The MTurk and Gold datasets differ in terms of genres as well as annotators (Turker vs. Expert). The distribution of modalities in both MTurk and Gold annotations are given in Table 2. 4.2 Gold Ability Table 1: For each modality, the number of sentences returned by the simple tagger that we posted on MTurk. 4 MTurk Table 2: Frequency of Modalities modalities in context. For tagging, we used the Yamcha (Kudo and Matsumoto, 2003) sequence labeling system which uses the SVMlight (Joachims, 1999) package for classification. We used One versus All method for multi-class classification on a quadratic kernel with a C value of 1. We report recall and precision on word tokens in our corpus for each modality. We also report Fβ=1 (F)-measure as the harmonic mean between (P)recision and (R)ecall. 4.3 Features We used lexical features at the token level which can be extracted without any parsing with relatively high accuracy. We use the term context width to denote the window of tokens whose features are considered for predictin"
W12-3807,W09-1304,0,0.0189282,"ain. Negation, uncertainty, and hedging are annotated in the Bioscope corpus (Vincze et al., 2008), along with information about which words are in the scope of negation/uncertainty. The i2b2 NLP Shared Task in 2010 included a track for detecting assertion status (e.g. present, absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they"
W12-3807,Y05-1014,0,0.0194818,", absent, possible, conditional, hypothetical etc.) of medical problems in clinical records.1 Apostolova et al. (2011) presents a rule-based system for the detection of negation and speculation scopes using the Bioscope corpus. Other studies emphasize the importance of detecting uncertainty in medical text summarization (Morante and Daelemans, 2009; Aramaki et al., 2009). Modality has also received some attention in the context of certain applications. Earlier work describing the difficulty of correctly translating modality using machine translation includes (Sigurd and Gawr´onska, 1994) and (Murata et al., 2005). Sigurd et al. (1994) write about rule based frameworks and how using alternate grammatical constructions such as the passive can improve the rendering of the modal in the target language. Murata et al. (2005) 1 https://www.i2b2.org/NLP/Relations/ analyze the translation of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to specifically handle modal constructions, while our modal annotation approach is a part of a full translation system. The textual entailment literature includes moda"
W12-3807,W06-3907,0,0.0243548,"of Japanese into English by several systems, showing they often render the present incorrectly as the progressive. The authors trained a support vector machine to specifically handle modal constructions, while our modal annotation approach is a part of a full translation system. The textual entailment literature includes modality annotation schemes. Identifying modalities is important to determine whether a text entails a hypothesis. Bar-Haim et al. (2007) include polarity based rules and negation and modality annotation rules. The polarity rules are based on an independent polarity lexicon (Nairn et al., 2006). The annotation rules for negation and modality of predicates are based on identifying modal verbs, as well as conditional sentences and modal adverbials. The authors read the modality off parse trees directly using simple structural rules for modifiers. 3 Constructing Modality Training Data In this section, we will discuss the procedure we followed to construct the training data for building the automatic modality tagger. In a pilot study, we obtained and ran the modality tagger described in (Baker et al., 2010) on the English side of the Urdu-English LDC language pack.2 We randomly selected"
W12-3807,C10-2117,1,0.929409,". Did the speaker 57 Proceedings of the ACL-2012 Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics (ExProM-2012), c pages 57–64, Jeju, Republic of Korea, 13 July 2012. 2012 Association for Computational Linguistics have firsthand knowledge of what he or she is reporting, or was it hearsay or inferred from indirect evidence? Sentiment deals with a speaker’s positive or negative feelings toward an event, state, or proposition. In this paper, we focus on the following five modalities; we have investigated the belief/factivity modality previously (Diab et al., 2009b; Prabhakaran et al., 2010), and we leave other modalities to future work. • Ability: can H do P? • Effort: does H try to do P? • Intention: does H intend P? • Success: does H succeed in P? • Want: does H want P? We investigate automatically training a modality tagger by using multi-class Support Vector Machines (SVMs). One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a modality tagger because modality triggers are sparse for the overwhelming majority of the sentences. Baker et al. (2010) created a modality tagger by using a semiautomatic"
W12-3807,C94-1018,0,0.029939,"Missing"
W12-3807,W08-0606,0,\N,Missing
W14-3606,W10-3700,0,0.171412,"Missing"
W14-3606,W11-3806,0,0.0377276,"Missing"
W14-3606,I13-1168,1,0.759475,"Missing"
W14-3606,calzolari-etal-2002-towards,0,0.243725,"ed data, construction of lexicons for specific languages, integration in NLP applications, and the construction of guidelines and best practices. A significant amount of research has focused on the identification and extraction of MWEs (Ramisch et al., 2010; Dubremetz and Nivre, 2014; Attia et al., 2010; Weller and Heid, 2010; Schneider et al., 2014). Description and specifications of MWE lexical resources have been presented for Japanese (Shudo et al. 2011), Italian (Zaninello and Nissim, 2010), Dutch (Grégoire, 2010; Odijk, 2013), and Modern Standard Arabic (Hawwari et al., 2012). Moreover, Calzolari et al. (2002) presented a project that attempted to introduce best practice recommendations for the treatment of MWE in mono- and multi-lingual computational lexicons that incorporate both syntactic and semantic information, but the limitation of their work is that they focus on only two types of MWEs, namely, support verbs and noun compounds. Apart from Schneider et al. (2014), who focused on the language of the social web, none of these projects dealt with informal or dialectal languages, which are rampant in user-generated content (UGC). With the explosion of social media, the language of Web 2.0 is und"
W14-3606,C86-1001,0,0.78221,"Missing"
W14-3606,habash-etal-2012-conventional,1,0.821172,"searchers proposed different typology for this phenomena. Fillmore et al. (1988) proposed three types based on lexical and syntactic familiarity: a) unfamiliar pieces familiarly combined, b) familiar pieces unfamiliarly combined, and c) familiar pieces familiarly combined. Mel&apos;čuk (1989), on the other hand, introduced three different classes: a) complete phraseme, b) semiphraseme, c) and quasi-phraseme. Sag et al. introduced two classes: institutionalized phrases and lexicalized phrases, with lexicalized phrases subdivided into fixed, semi-fixed and syntactically flexible expressions. Ramisch (2012) introduced yet another set of classes: nominal, verbal and adverbial expressions. 4 Annotation of Linguistic Features in MWE In this section, we provide a comprehensive specification of MWE types and the detailed linguistic information, including the phonological, orthographical, syntactic, semantic and pragmatic features. 4.1 From the lexicographic point of view, the legacy three-way division of MWEs proved to be too coarse-grained to cater for the needs of lexicographers who need to identify the large array of sub-types that fall under the umbrella of ‘MWEs’. Atkins and Rundell (2008) empha"
W14-3606,W12-3403,1,0.771196,"searchers proposed different typology for this phenomena. Fillmore et al. (1988) proposed three types based on lexical and syntactic familiarity: a) unfamiliar pieces familiarly combined, b) familiar pieces unfamiliarly combined, and c) familiar pieces familiarly combined. Mel&apos;čuk (1989), on the other hand, introduced three different classes: a) complete phraseme, b) semiphraseme, c) and quasi-phraseme. Sag et al. introduced two classes: institutionalized phrases and lexicalized phrases, with lexicalized phrases subdivided into fixed, semi-fixed and syntactically flexible expressions. Ramisch (2012) introduced yet another set of classes: nominal, verbal and adverbial expressions. 4 Annotation of Linguistic Features in MWE In this section, we provide a comprehensive specification of MWE types and the detailed linguistic information, including the phonological, orthographical, syntactic, semantic and pragmatic features. 4.1 From the lexicographic point of view, the legacy three-way division of MWEs proved to be too coarse-grained to cater for the needs of lexicographers who need to identify the large array of sub-types that fall under the umbrella of ‘MWEs’. Atkins and Rundell (2008) empha"
W14-3606,deksne-etal-2008-dictionary,0,0.0577743,"Missing"
W14-3606,N10-1089,0,0.0248821,"Missing"
W14-3606,zaninello-nissim-2010-creation,0,0.170937,"a paraphrase otherwise. Previous Work There are four main areas of research on MWEs: extraction from structured and unstructured data, construction of lexicons for specific languages, integration in NLP applications, and the construction of guidelines and best practices. A significant amount of research has focused on the identification and extraction of MWEs (Ramisch et al., 2010; Dubremetz and Nivre, 2014; Attia et al., 2010; Weller and Heid, 2010; Schneider et al., 2014). Description and specifications of MWE lexical resources have been presented for Japanese (Shudo et al. 2011), Italian (Zaninello and Nissim, 2010), Dutch (Grégoire, 2010; Odijk, 2013), and Modern Standard Arabic (Hawwari et al., 2012). Moreover, Calzolari et al. (2002) presented a project that attempted to introduce best practice recommendations for the treatment of MWE in mono- and multi-lingual computational lexicons that incorporate both syntactic and semantic information, but the limitation of their work is that they focus on only two types of MWEs, namely, support verbs and noun compounds. Apart from Schneider et al. (2014), who focused on the language of the social web, none of these projects dealt with informal or dialectal langu"
W14-3606,ramisch-etal-2010-mwetoolkit,0,0.0572166,"Missing"
W14-3606,schneider-etal-2014-comprehensive,0,0.0231046,"MWEs where applicable; i) 2 Translation, which includes the MSA and English equivalents, either as an MWE in MSA and English if available or as a paraphrase otherwise. Previous Work There are four main areas of research on MWEs: extraction from structured and unstructured data, construction of lexicons for specific languages, integration in NLP applications, and the construction of guidelines and best practices. A significant amount of research has focused on the identification and extraction of MWEs (Ramisch et al., 2010; Dubremetz and Nivre, 2014; Attia et al., 2010; Weller and Heid, 2010; Schneider et al., 2014). Description and specifications of MWE lexical resources have been presented for Japanese (Shudo et al. 2011), Italian (Zaninello and Nissim, 2010), Dutch (Grégoire, 2010; Odijk, 2013), and Modern Standard Arabic (Hawwari et al., 2012). Moreover, Calzolari et al. (2002) presented a project that attempted to introduce best practice recommendations for the treatment of MWE in mono- and multi-lingual computational lexicons that incorporate both syntactic and semantic information, but the limitation of their work is that they focus on only two types of MWEs, namely, support verbs and noun compoun"
W14-3606,P11-1017,0,0.017754,"nd English if available or as a paraphrase otherwise. Previous Work There are four main areas of research on MWEs: extraction from structured and unstructured data, construction of lexicons for specific languages, integration in NLP applications, and the construction of guidelines and best practices. A significant amount of research has focused on the identification and extraction of MWEs (Ramisch et al., 2010; Dubremetz and Nivre, 2014; Attia et al., 2010; Weller and Heid, 2010; Schneider et al., 2014). Description and specifications of MWE lexical resources have been presented for Japanese (Shudo et al. 2011), Italian (Zaninello and Nissim, 2010), Dutch (Grégoire, 2010; Odijk, 2013), and Modern Standard Arabic (Hawwari et al., 2012). Moreover, Calzolari et al. (2002) presented a project that attempted to introduce best practice recommendations for the treatment of MWE in mono- and multi-lingual computational lexicons that incorporate both syntactic and semantic information, but the limitation of their work is that they focus on only two types of MWEs, namely, support verbs and noun compounds. Apart from Schneider et al. (2014), who focused on the language of the social web, none of these projects"
W14-3606,weller-heid-2010-extraction,0,\N,Missing
W14-3606,W14-0812,0,\N,Missing
W14-3606,W10-3704,1,\N,Missing
W14-3606,N10-1029,1,\N,Missing
W14-3606,J05-1004,0,\N,Missing
W14-3606,W11-0815,0,\N,Missing
W14-3610,habash-etal-2012-conventional,1,0.779358,"Missing"
W14-3610,W10-2417,0,0.279122,"Missing"
W14-3610,N13-1044,0,0.0421637,"wish, 2010) suggest a number of features, that we incorporate a subset of in our DA NER attention, yielding a more urgent need for DA NER systems. Furthermore, applying NLP tools, such as NER, that are designed for MSA on DA results in considerably low performance, thus the need to build resources and tools that specifically target DA (Habash et al., 2012). In addition to the afore mentioned challenges for Arabic NER in general compared to Latin based languages, DA NER faces additional issues: • Lack of annotated data for supervised NER; • Lack of standard orthographies or language academics (Habash et al., 2013): Unlike MSA, the same word in DA can be rewritten in so many forms, e.g. mAtEyT$, mtEyt$, mA tEyT$ ‘do not cry’ are all acceptable variants since there is no one standard; • Lack of comprehensive enough Gazetteers: this is a problem facing all NER systems for all languages addressing NER in social media text, since by definition such media has a ubiquitous presence of highly productive names exemplified by the usage of nick names, hence the PERSON class in social media NER will always have a coverage problem. In this paper, we propose a DA NER system – using Egyptian Arabic (EGY) as an exampl"
W14-3610,D08-1030,1,0.912841,"Missing"
W14-3610,pasha-etal-2014-madamira,1,0.888346,"Missing"
W14-3610,P10-2052,1,0.940052,"Missing"
W14-3610,J92-4003,0,0.510007,"Missing"
W14-3610,darwish-gao-2014-simple,0,0.272998,"word has an adjacent dot, capitalization binary feature which is dependent on the English gloss generated by MADAMIRA, nominal binary feature that is set to true if the POS tag is noun or proper noun, and binary features to represent whether the current, previous, or next word belong to the gazetteers. We omit Rule-based features in our baseline since we do not have access to the exact rules used and their rules specifically targeted MSA, hence would not be directly applicable to DA. 3.2 NER Features In our approach, we propose the following NER features: • Lexical Features: Similar to BAS1 (Darwish and Gao, 2014) character n-gram features, the head and trailing bigrams (L2), trigrams (L3), and 4-grams (L4) characters; Approach In this paper, we use a supervised machine learning approach since it has been shown in the literature that supervised typically outperform unsupervised approaches for the NER task (Nadeau et al., 2006). We use Conditional Random Field (CRF) sequence labeling as described in (Lafferty et al., 2001). Moreover, (Benajiba and Rosso, 2008) demonstrates that CRF yields better results over other supervised machine learning techniques. • Contextual Features (CTX): The surrounding undia"
W14-3610,J14-2008,0,0.241971,"Missing"
W14-3610,W97-0315,0,0.109627,"Missing"
W14-3620,W11-4417,1,0.897784,"Missing"
W14-3620,C12-2011,1,0.889852,"Missing"
W14-3620,W98-1007,0,0.126018,"d correction problem with a focus on non-word errors. Our work is different from previous work on Arabic in that we cover punctuation errors as well. Furthermore, we fine-tune a Language Model (LM) disambiguator by adding probability scores for candidates using forwardbackward tracking, which yielded better results than the default Viterbi. We also develop a new and more efficient splitting algorithm for merged words. 1.2 Arabic Morphology, Orthography and Punctuation Arabic has a rich and complex morphology as it applies both concatenative and nonconcatenative morphotactics (Ratcliffe, 1998; Beesley, 1998; Habash, 2010), yielding a wealth of morphemes that express various morpho148 Proceedings of the EMNLP 2014 Workshop on Arabic Natural Langauge Processing (ANLP), pages 148–154, c October 25, 2014, Doha, Qatar. 2014 Association for Computational Linguistics syntactic features, such as tense, person, number, gender, voice and mood. Arabic has a large array of orthographic variations, leading to what is called ‘typographic errors’ or ‘orthographic variations’ (Buckwalter, 2004a), and sometimes referred to as substandard spellings, or spelling soft errors. These errors are basically related to t"
W14-3620,P00-1037,0,0.189707,"phasis. HASP in its current stage only handles types (a), (b), and (e) errors. We assume that the various error types are too distinct to be treated with the same computational technique. Therefore, we treat each problem separately, and for each problem we select the approach that seems most efficient, and ultimately all components are integrated in a single framework. 1.1 Previous Work Detecting spelling errors in typing is one of the earliest NLP applications, and it has been researched extensively over the years, particularly for English (Damerau, 1964; Church and Gale, 1991; Kukich, 1992; Brill and Moore, 2000; Van Delden et al., 2004; Golding, 1995; Golding and Roth, 1996; Fossati and Di Eugenio, 2007; Islam in Inkpen, 2009; Han and Baldwin, 2011; Wu et al., 2013). The problem of Arabic spelling error correction has been investigated in a number of papers (Haddad and Yaseen, 2007; Alfaifi and Atwell, 2012; Hassan et al., 2008; Shaalan et al., 2012; Attia et al., 2012; Alkanhal et al., 2012). In our research, we address the spelling error detection and correction problem with a focus on non-word errors. Our work is different from previous work on Arabic in that we cover punctuation errors as well."
W14-3620,J92-4003,0,0.249131,"Missing"
W14-3620,W04-1606,0,0.0949575,"Missing"
W14-3620,N12-1067,0,0.0869502,"Missing"
W14-3620,C10-1041,0,0.0666082,"Missing"
W14-3620,P11-1038,0,0.0356615,"treated with the same computational technique. Therefore, we treat each problem separately, and for each problem we select the approach that seems most efficient, and ultimately all components are integrated in a single framework. 1.1 Previous Work Detecting spelling errors in typing is one of the earliest NLP applications, and it has been researched extensively over the years, particularly for English (Damerau, 1964; Church and Gale, 1991; Kukich, 1992; Brill and Moore, 2000; Van Delden et al., 2004; Golding, 1995; Golding and Roth, 1996; Fossati and Di Eugenio, 2007; Islam in Inkpen, 2009; Han and Baldwin, 2011; Wu et al., 2013). The problem of Arabic spelling error correction has been investigated in a number of papers (Haddad and Yaseen, 2007; Alfaifi and Atwell, 2012; Hassan et al., 2008; Shaalan et al., 2012; Attia et al., 2012; Alkanhal et al., 2012). In our research, we address the spelling error detection and correction problem with a focus on non-word errors. Our work is different from previous work on Arabic in that we cover punctuation errors as well. Furthermore, we fine-tune a Language Model (LM) disambiguator by adding probability scores for candidates using forwardbackward tracking, wh"
W14-3620,E09-2008,0,0.0856422,"c reported in the literature to date. We enhance the AraComLex Extended dictionary by utilizing the annotated data in the shared task’s training data. We add 776 new valid words to the dictionary and remove 4,810 misspelt words, leading to significant improvement in the dictionary’s ability to make decisions on words. Table 6 shows the dictionary’s performance on the training and development set in the shared task as applied only to non-words and excluding grammatical, semantic and punctuation errors. P F Training 98.84 96.34 97.57 b. Candidate Generation For candidate generation we use Foma (Hulden, 2009), a finite state compiler that is capable of producing candidates from a wordlist (compiled as an FST network) within a certain edit distance from an error word. Foma allows the ranking of candidates according to customizable transformation rules. # 1. 2. 3. 4. 5. 6. 7. This is more akin to the typical spelling correction problem where a word has the wrong letters, rendering it a non-word. We address this problem using two approaches: Dictionary-LM Correction, and Alignment Based Correction. Spelling error detection and correction mainly consists of three phases: a) error detection; b) candida"
W14-3620,P03-1004,0,0.0221444,"k (PATB) tokenization (e.g., ﻝل+  ﺍاﻟﺘﺸﺎﻭوﺭرl+Alt$Awr); (3) Kulick POS tag (e.g., IN+DT+NN). (4) Buckwalter POS tag (e.g., PREP+DET+ NOUN+CASE_DEF_GN) as produced by MADAMIRA; (5) Classes to be predicted: colon_after, comma_after, exclmark_after, period_after, qmark_after, semicolon_after and NA (when no punctuation marks are used); Window Recall Precision F-measure Size 4 36.24 54.09 43.40 5 37.95 59.61 46.37 6 36.65 59.99 45.50 7 34.50 59.53 43.68 Table 3. Yamcha results on the development set For classification, we experiment with Support Vector Machines (SVM) as implemented in Yamcha (Kudo and Matsumoto, 2003) and Conditional Random Field (CRF++) classifiers (Lafferty et al. 2001). In our investigation, we vary the context window size from 4 to 8 and we use all 5 features listed for every word in the window. As Tables 3 and 4 show, we found that window size 5 gives the best f-score by both Yamcha and CRF. When we strip clitics from tokenized tag, reducing it to stems only, the performance of the system improved. Overall CRF yields significantly higher results using the same experimental setup. We assume that the performance advantage of CRF is a result of the way words in the context and their feat"
W14-3620,W06-1648,0,0.0727139,"Missing"
W14-3620,J03-1002,0,0.00635933,"tained by System 1, which is ranked 5th among the 9 systems participating in the shared task. # Experiment R P F 1 System 1 52.98 75.47 62.25 2 System 2 52.99 75.34 62.22 Table 9. Final official results on the test set provided by the Shared Task 2.3.3.2. Alignment-Based Correction We formatted the data for alignment using a window of 4 words: one word to each side (forming the contextual boundary) and two words in the middle. The two words in the middle are split into characters so that character transformations can be observed and learned by the aligner. The alignment tool we use is Giza++ (Och and Ney, 2003). Results are reported in Table 10. # Experiment R P F 1 for all error types 36.05 45.13 37.99 2 excluding punc 32.37 54.65 40.66 3 2 + CRF_punc+norm 46.11 62.02 52.90 Table 10. Results of character-based alignment # Experiment R P F 1 LM+split-1 33.32 73.71 45.89 2 +CRF_punc+split-1 49.74 65.38 56.50 3 + norm+split-1 +CRF_punc+norm +split-1 +CRF_punc+norm +orig_punc+split-1 +CRF_punc+norm +orig_punc+split-2 38.81 69.08 49.70 Although these preliminary results from Alignment are significantly below results yielded from the Dictionary-LM approach, we believe that there are several potential imp"
W14-3620,pasha-etal-2014-madamira,1,0.881036,"Missing"
W14-3620,P08-2030,1,0.874397,"Missing"
W14-3620,O13-5002,0,\N,Missing
W14-3620,shaalan-etal-2012-arabic,1,\N,Missing
W14-3620,W14-3605,0,\N,Missing
W14-3620,zaghouani-etal-2014-large,0,\N,Missing
W14-3620,I08-2131,0,\N,Missing
W14-3620,W13-3601,0,\N,Missing
W14-3907,li-etal-2012-mandarin,1,0.84418,"Missing"
W14-3907,W14-3917,0,0.103569,"Missing"
W14-3907,W14-3909,0,0.0388692,"Missing"
W14-3907,W14-3915,0,0.0710818,"Missing"
W14-3907,D13-1084,0,0.207232,"Missing"
W14-3907,W14-3911,1,0.921109,"t exploiting. For instance, the NE lexicons might account for the best results in the NE class in both the Twitter data and the Surprise genre (see Table 4 last row for SPA-EN and second to last for SPAEN Surprise). Most systems showed considerable 67 F-measure 1 0.9 Baseline 0.894 0.892 0.888 0.838 0.8 0.7 0.6 (Jain and Bhat, 2014) (Lin et al., 2014) (Chittaranjan et al., 2014) (King et al., 2014) F-measure (a) MAN-EN Baseline Test1 0.4 Baseline Test2 0.3 0.196 0.2 0.152 0.118 0.1 (Chittaranjan et al., 2014) 0.417 0.360 0.338 0.260 (King et al., 2014) 0.095 0.048 0.044 (Jain and Bhat, 2014) (Elfardy et al., 2014) (Lin et al., 2014) F-measure (b) MSA-DA. Dark gray bars show performance on Test1 and light gray bars show performance for Test2 Baseline 1 0.952 0.962 (King et al., 2014) (Lin et al., 2014) 0.975 0.974 0.972 0.977 0.9 0.8 (Jain and Bhat, 2014) (Shrestha, 2014) (Chittaranjan et al., 2014) (Barman et al., 2014) (c) NEP-EN F-measure 1 0.9 Baseline 0.8 0.7 0.6 0.703 0.754 0.753 0.783 0.793 0.822 0.634 (Shrestha, 2014) (King et al., 2014) (Chittaranjan et al., 2014) (Barman et al., 2014) (Jain and Bhat, 2014) (Lin et al., 2014) (Bar and Dershowitz, 2014) (d) SPA-EN Figure 1: Prediction results on"
W14-3907,W14-3916,0,0.102327,"Missing"
W14-3907,W14-3910,0,0.0496265,"Missing"
W14-3907,C82-1023,0,0.500666,"olumbia.edu pascale@ece.ust.hk ayc2135@columbia.edu Abstract this direction. We define CS broadly as a communication act, whether spoken or written, where two or more languages are being used interchangeably. In its spoken form, CS has probably been around ever since different languages first came in contact. Linguists have studied this phenomenon since the mid 1900s. In contrast, the Natural Language Processing (NLP) community has only recently started to pay attention to CS, with the earliest work in this area dating back to Joshi’s theoretical work proposing an approach to parsing CS data (Joshi, 1982) based on the Matrix and Embedded language framework. With the wide-spread use of social media, CS is now being used more and more in written language and thus we are seeing an increase in published papers dealing with CS. We are specifically interested in intrasentential code switched phenomena. As a result of this task, we have successfully created the first set of annotated data for several language pairs with a coherent set of labels across the languages. As the shared task results show, CS poses new research questions that warrant new NLP approaches, and thus we expect to see a significan"
W14-3907,P11-2007,0,0.0996179,"Missing"
W14-3907,N13-1131,0,0.144834,"Missing"
W14-3907,W14-3912,0,0.116613,"Missing"
W14-3907,W15-3116,0,\N,Missing
W14-3907,E14-1001,0,\N,Missing
W14-3907,W15-2902,0,\N,Missing
W14-3907,N15-1109,0,\N,Missing
W14-3907,W15-5936,0,\N,Missing
W14-3911,W09-0807,0,0.07503,"Missing"
W14-3911,bouamor-etal-2014-multidialectal,0,0.0218022,"he early works is that of (Biadsy et al., 2009) where the authors present a system that identifies dialectal words in speech through acoustic signals. Zaidan and Callison-Burch (2011) crawled a large dataset of MSA-DA news commentaries and annotated part of the dataset for sentence-level dialectalness employing Amazon Mechanical Turk. Cotterell and Callison-Burch (2014) extended the previous work by handling more dialects. In (Cotterell et al., 2014), the same authors collect and annotate on Amazon Mechanical Turk a large set of tweets and user commentaries pertaining to five Arabic dialects. Bouamor et al. (2014) select a set of 2,000 Egyptian Arabic sentences and have them translated into four other Arabic dialects to present the first multidialectal Arabic parallel corpus. Eskander et al. (2014) present a system for handling Arabic written in Roman script “Arabizi”. Using decision trees; the system identifies whether each word in the given text is a foreign word or not and further divides non foreign words into four Introduction Most languages exist in some standard form while also being associated with informal regional varieties. Some languages exist in a state of diglossia (Ferguson, 1959). Arabi"
W14-3911,cotterell-callison-burch-2014-multi,0,0.131391,"Missing"
W14-3911,W14-3907,1,0.801534,"state of the art MSA to English MT system to produce an English translation. In (Elfardy and Diab, 2012a), we present a set of guidelines for token-level identification of DA while in (Elfardy and Diab, 2012b), (Elfardy et al., 2013) we tackle the problem of token-level dialect-identification by casting it as a code-switching problem. Elfardy and Diab (2013) presents our solution for the sentence-level dialect identification problem. 3 4.1 Shared Task Description Preprocessing We experiment with two preprocessing techniques: The shared task for “Language Identification in Code-Switched Data” (Solorio et al., 2014) aims at allowing participants to perform wordlevel language identification in code-switched Spanish-English, MSA-DA, Chinese-English and Nepalese-English data. In this work, we only focus on MSA-DA data. The dataset has six tags: 1. Basic: In this scheme, we only perform a basic clean-up of the text by separating punctuation and numbers from words, normalizing word-lengthening effects, and replacing all punctuation, URLs, numbers and nonArabic words with PUNC, URL, NUM, and LAT keywords, respectively 1. lang1: corresponds to an MSA word, ex. QË@, AlrAhn 2 meaning “the current”; áë@ 2. Tokeniz"
W14-3911,elfardy-diab-2012-simplified,1,0.909906,"vantage of using MADAMIRA over using a morphological analyzer is that MADAMIRA performs contextual disambiguation of the analyses produced by the morphological analyzer, hence reducing the possible options for analyses per word. Figures 1 illustrates the pipeline of the proposed system. In the context of machine-translation, Salloum and Habash (2011) tackle the problem of DA to English Machine Translation (MT) by pivoting through MSA. The authors present a system that uses DA to MSA transfer rules before applying state of the art MSA to English MT system to produce an English translation. In (Elfardy and Diab, 2012a), we present a set of guidelines for token-level identification of DA while in (Elfardy and Diab, 2012b), (Elfardy et al., 2013) we tackle the problem of token-level dialect-identification by casting it as a code-switching problem. Elfardy and Diab (2013) presents our solution for the sentence-level dialect identification problem. 3 4.1 Shared Task Description Preprocessing We experiment with two preprocessing techniques: The shared task for “Language Identification in Code-Switched Data” (Solorio et al., 2014) aims at allowing participants to perform wordlevel language identification in cod"
W14-3911,C12-2029,1,0.896956,"vantage of using MADAMIRA over using a morphological analyzer is that MADAMIRA performs contextual disambiguation of the analyses produced by the morphological analyzer, hence reducing the possible options for analyses per word. Figures 1 illustrates the pipeline of the proposed system. In the context of machine-translation, Salloum and Habash (2011) tackle the problem of DA to English Machine Translation (MT) by pivoting through MSA. The authors present a system that uses DA to MSA transfer rules before applying state of the art MSA to English MT system to produce an English translation. In (Elfardy and Diab, 2012a), we present a set of guidelines for token-level identification of DA while in (Elfardy and Diab, 2012b), (Elfardy et al., 2013) we tackle the problem of token-level dialect-identification by casting it as a code-switching problem. Elfardy and Diab (2013) presents our solution for the sentence-level dialect identification problem. 3 4.1 Shared Task Description Preprocessing We experiment with two preprocessing techniques: The shared task for “Language Identification in Code-Switched Data” (Solorio et al., 2014) aims at allowing participants to perform wordlevel language identification in cod"
W14-3911,P13-2081,1,0.712887,"ipeline of the proposed system. In the context of machine-translation, Salloum and Habash (2011) tackle the problem of DA to English Machine Translation (MT) by pivoting through MSA. The authors present a system that uses DA to MSA transfer rules before applying state of the art MSA to English MT system to produce an English translation. In (Elfardy and Diab, 2012a), we present a set of guidelines for token-level identification of DA while in (Elfardy and Diab, 2012b), (Elfardy et al., 2013) we tackle the problem of token-level dialect-identification by casting it as a code-switching problem. Elfardy and Diab (2013) presents our solution for the sentence-level dialect identification problem. 3 4.1 Shared Task Description Preprocessing We experiment with two preprocessing techniques: The shared task for “Language Identification in Code-Switched Data” (Solorio et al., 2014) aims at allowing participants to perform wordlevel language identification in code-switched Spanish-English, MSA-DA, Chinese-English and Nepalese-English data. In this work, we only focus on MSA-DA data. The dataset has six tags: 1. Basic: In this scheme, we only perform a basic clean-up of the text by separating punctuation and numbers"
W14-3911,P11-2007,0,0.25413,"Missing"
W14-3911,W14-3901,1,0.821936,"wled a large dataset of MSA-DA news commentaries and annotated part of the dataset for sentence-level dialectalness employing Amazon Mechanical Turk. Cotterell and Callison-Burch (2014) extended the previous work by handling more dialects. In (Cotterell et al., 2014), the same authors collect and annotate on Amazon Mechanical Turk a large set of tweets and user commentaries pertaining to five Arabic dialects. Bouamor et al. (2014) select a set of 2,000 Egyptian Arabic sentences and have them translated into four other Arabic dialects to present the first multidialectal Arabic parallel corpus. Eskander et al. (2014) present a system for handling Arabic written in Roman script “Arabizi”. Using decision trees; the system identifies whether each word in the given text is a foreign word or not and further divides non foreign words into four Introduction Most languages exist in some standard form while also being associated with informal regional varieties. Some languages exist in a state of diglossia (Ferguson, 1959). Arabic is one of those languages comprising a standard form known as Modern Standard Arabic (MSA), that is used in education, formal settings, and official scripts; and dialectal variants (DA)"
W14-3911,N06-2013,0,0.0443336,"sh data. In this work, we only focus on MSA-DA data. The dataset has six tags: 1. Basic: In this scheme, we only perform a basic clean-up of the text by separating punctuation and numbers from words, normalizing word-lengthening effects, and replacing all punctuation, URLs, numbers and nonArabic words with PUNC, URL, NUM, and LAT keywords, respectively 1. lang1: corresponds to an MSA word, ex. QË@, AlrAhn 2 meaning “the current”; áë@ 2. Tokenized: In this scheme, in addition to basic preprocessing, we use MADAMIRA toolkit to tokenize clitics and affixes by applying the D3-tokenization scheme (Habash and Sadat, 2006). For example, the word Ym.&apos;., bjdwhich means “with seriousness” becomes “ Yg. + H . ”, “b+ jd” after tokenization. 2. lang2: corresponds to a DA word, ex. K P@ , ezyk meaning “how are you”; 3. mixed: corresponds to a word with mixed ñ  Ë A ÖÏ @ , Alm&gt;lw$wn morphology, ex. àñ meaning “the ones that were excluded or rejected”; 4. other: corresponds to punctuation, numbers and words having punctuation or numbers attached to them; 5. ambig: corresponds to a word where the class cannot be determined given the current context, could either be lang1 or lang2; ex.  the phrase ÐAÖß éÊ¿, klh tmAm"
W14-3911,W12-2301,0,0.0404863,"ered an out of vocabulary word by the LM) and: • If MADAMIRA retrieved the glosses from SAMA, the word is assigned a lang1 tag; • Else if MADAMIRA outputs that the glosses were retrieved from CALIMA, then the word is assigned a lang2 tag • Else if the word is still untagged (i.e. non-analyzable), the word is assigned lang2 tag. MADAMIRA Using MADAMIRA, each word in a given untagged sentence is tokenized, lemmatized, and POS-tagged. Moreover, the MSA and English glosses for each morpheme of the given word are provided. Since MADAMIRA uses two possible underlying morphological analyzers CALIMA (Habash et al., 2012) and SAMA (Maamouri et al., 2010), as part of the output, MADAMIRA indicates which of them is used to retrieve the glosses. 4.4 Named Entities List We use the ANERGazet (Benajiba et al., 2007) to identify named-entities. ANERGazet consists of the following Gazetteers: • Locations: 1,545 entries corresponding to names of continents, countries, cities, etc. (ex. H . Q ª ÖÏ @ , Almgrb ) which means “Morocco”; • People: 2,100 entries corresponding to names of people. (ex. Yê¯, fhd); • Organizations: 318 entries corresponding to names of Organizations such as companies  and football teams. (ex. úæ"
W14-3911,pasha-etal-2014-madamira,1,0.0840603,"Missing"
W14-3911,W11-2602,0,\N,Missing
W14-4213,P07-2045,0,0.0165466,"Missing"
W14-4213,E06-1047,1,0.835421,"Missing"
W14-4213,J03-1002,0,0.00436082,"from three corpora sets: a) The English Gigaword 5 (Graff and Cieri, 2003); b) The English side of the BOLT Phase1 parallel data; and, c) different LDC English corpora collected from discussion forums (LDC2012E04, LDC2012E16, LDC2012E21, LDC2012E54). We use SRILM (Stolcke., 2002) to build 5-gram language models with modified Kneser-Ney smoothing. 3.3 Experimental Results SMT System We use the open-source Moses toolkit (Koehn et al., 2007) to build a standard phrase-based SMT system which extracts up to 8 words phrases in the Moses phrase table. The parallel corpus is wordaligned using GIZA++ (Och and Ney, 2003). Feature weights are tuned to maximize BLEU on the dev set using Minimum Error Rate Training (MERT) (Och, 2003). To account for the instability of MERT, we run the tuning step three times per condition with different random seeds and use the optimized weights that give the median score on the development set. As all our DA identification resources (MADAMIRA, AIDA and THARWA) are lemma-based, we adopt a factored translation model setup to introduce the extra information in the form of a lemma factor. As lemma only is not enough to generate appropriate inflected surface (lexeme) forms, we add a"
W14-4213,P02-1040,0,0.087698,"Missing"
W14-4213,P13-2081,1,0.859617,"te an equivalent EN gloss. However, for EGY words, MADAMIRA does not generate the equivalent MSA lemma (Pasha et al., 2014); • lemma+POS-to-lexeme translation (lem+POS-to-lex): In this path source lemma and POS are translated into the appropriate target lexeme. We expect this path provides plausible translations for DA words that are not observed in the phrase tables. • AIDA: A full-fledged DA identification tool which is able to identify and classify DA words on the token and sentence levels. AIDA exploits MADAMIRA internally in addition to more information from context to identify DA words (Elfardy and Diab, 2013). AIDA provides both the MSA equivalent lemma(s) and corresponding EN gloss(es) for the identified DA words; • lexeme-to-lexeme;lemma+POS-to-lexeme translation (lex-to-lex;lem+POS-to-lex): The first path translates directly from a source lexeme to the target lexeme. So it provides appropriate lexeme translations for the words (MSA or DA) which have been observed in the trained model. It is worth noting that lex-to-lex translation path does not contain any replacement or normalization. Therefore, it is different from the first path (Glex-to-lex). The second path is similar to the lem+POS-to-lex"
W14-4213,N13-1036,0,0.0218179,"e research on DA. MSA has a wealth of resources such as parallel corpora and tools like morphological analyzers, disambiguation systems, etc. On the other hand, DA still lacks such tools and resources. As an example, parallel DA to English (EN) corpora are still very few and there are almost no MSA-DA parallel corpora. Similar to MSA, DA has the problem of writing with optional diacritics. It also lacks orthographic standards. Hence, translating from DA to EN is challenging as there are impediments posed by the nature of the language coupled with the lack of resources and tools to process DA (Salloum and Habash, 2013). MSA and DA are significantly different on all levels of linguistic representation: phonologically, morphologically, lexically, syntactically, semantically and pragmatically. The morphological differences between MSA and DA are most noticeably expressed by using some clitics and affixes that do not exist in MSA. For instance, the DA (Egyptian and Levantine) future marker clitic H 1 is expressed as the clitic s in MSA (Salloum and Habash, 2013). On a lexical level, MSA and DA share a considerable number of faux amis where the lexical tokens are homographs but have different meanings. For insta"
W14-4213,C12-3048,0,0.0282465,"Missing"
W14-4213,P05-1071,0,0.135464,"Missing"
W14-4213,P08-2015,0,0.0458151,"Missing"
W14-4213,2006.amta-papers.25,0,0.0546528,"Missing"
W14-4213,N12-1006,0,\N,Missing
W14-4213,diab-etal-2014-tharwa,1,\N,Missing
W14-4213,N13-1044,0,\N,Missing
W14-4213,pasha-etal-2014-madamira,1,\N,Missing
W14-4213,D08-1076,0,\N,Missing
W15-1005,W14-4213,1,0.903249,"ork in two aspects: First, we try to improve SMT lexical choice by identifying false friends and replacing them with the most adequate equivalent from standard language. Unlike previous work, all these steps are done on a given input sentence and we can see them as a preprocessing phase, thereby, there is no need to change the SMT model. Second, our approach does not assume that the in-domain parallel data is available. Hence, it is not constrained by the domain and can be extended to any other language variants. The main difference between this approach and our previous work as described in (Aminian et al., 2014) lies in the fact that we try to improve SMT lexical choice by enhancing FF translation. Rather than blindly replacing all dialectal words with their standard equivalent as we did in (Aminian et al., 2014), here we try to automatically identify FF as one of the important sources of translation degradation across language variants and leverage knowledge acquired from monolingual standard data to predict the best equivalent for FF based on the context. 3 Approach We describe our model in this section. We use two modules in our model: 1) a FF identifier (henceforth PARL) and, 2) a disambiguator ("
W15-1005,W05-0909,0,0.103628,"Missing"
W15-1005,P05-1048,0,0.0884726,"Missing"
W15-1005,D07-1007,0,0.20289,"Missing"
W15-1005,P07-1005,0,0.267095,"Missing"
W15-1005,P11-2031,0,0.017999,"on results (BLEU, METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), WER, PER) on the Bolt-arz test set compared to the baselines. SMT System We use Moses decoder (Koehn et al., 2007) to build a standard phrase-based SMT system. Feature weights are tuned to maximize BLEU score on the tuning set using Minimum Error Rate Training (MERT) (Och, 2003) algorithm. Final results are reported by averaging over three tuning sessions with random initialization. Significant test is also performed to make sure that gains in the results are statistically significant. We use the implementation of Clark et al. (2011) to compute the p-value via approximate randomization algorithms. Since AIDA generates MSA equivalents in the lemma form, we use a factored translation model with lemma and POS factors. We use GIZA++ (Och and Ney, 2003) to word align the parallel corpus. We use SRILM (Stolcke and others, 2002) to build 5-gram language models with modified Kneser-Ney smoothing (Kneser and Ney, 1995). Our language modeling data consists of three data sets: a) The English Gigaword 5 (Graff and Cieri, 2003); b) The English side of the BOLT Phase 1 parallel data; and, c) different LDC English corpora collected from"
W15-1005,P13-2081,1,0.841669,"l., 2004). We used Tree Tagger (Schmid, 1994) to tokenize English data. Tools We use GIZA++ (Och and Ney, 2003) for word alignment. We obtain word clusters from word2vec (Mikolov et al., 2013) K-means word clustering tool. We use the continuous bag of word model to build word vectors of size 200 using a word window of size 8 for both left and right. The number of negative samples for logistic regression is set to 25 and threshold used for sub-sampling of frequent words is set to 10−5 in the model with 15 iterations. We also use full softmax to obtain the probability distribution. We use AIDA (Elfardy and Diab, 2013) as the dialect identification tool. AIDA also provides a list of MSA equivalents for identified DA words in context. 1 41 LDC catalogs including data prepared for GALE and BOLT projects. BASELINE1 BASELINE2 BASELINE3 PARL WCcor PARL+WC PARL+WCcor BLEU METEOR TER WER PER 20.6 20.1 21.3 20.7 20.9 21.0 21.3 27.5 27.2 28.0 27.1 27.7 27.7 27.9 65.9 68.3 65.2 67.5 65.4 66.2 65.5 69.2 71.6 68.6 69.6 68.7 68.5 68.0 45.3 46.6 44.6 45.5 44.8 45.3 44.5 Table 1: Evaluation results (BLEU, METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), WER, PER) on the Bolt-arz test set compared to the basel"
W15-1005,P06-1056,0,0.0777946,"Missing"
W15-1005,P07-2045,0,0.0091832,"DA also provides a list of MSA equivalents for identified DA words in context. 1 41 LDC catalogs including data prepared for GALE and BOLT projects. BASELINE1 BASELINE2 BASELINE3 PARL WCcor PARL+WC PARL+WCcor BLEU METEOR TER WER PER 20.6 20.1 21.3 20.7 20.9 21.0 21.3 27.5 27.2 28.0 27.1 27.7 27.7 27.9 65.9 68.3 65.2 67.5 65.4 66.2 65.5 69.2 71.6 68.6 69.6 68.7 68.5 68.0 45.3 46.6 44.6 45.5 44.8 45.3 44.5 Table 1: Evaluation results (BLEU, METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), WER, PER) on the Bolt-arz test set compared to the baselines. SMT System We use Moses decoder (Koehn et al., 2007) to build a standard phrase-based SMT system. Feature weights are tuned to maximize BLEU score on the tuning set using Minimum Error Rate Training (MERT) (Och, 2003) algorithm. Final results are reported by averaging over three tuning sessions with random initialization. Significant test is also performed to make sure that gains in the results are statistically significant. We use the implementation of Clark et al. (2011) to compute the p-value via approximate randomization algorithms. Since AIDA generates MSA equivalents in the lemma form, we use a factored translation model with lemma and PO"
W15-1005,N01-1014,0,0.158479,"Missing"
W15-1005,R09-1054,0,0.36838,"Missing"
W15-1005,J03-1002,0,0.0271093,"aining 848M tokenized MSA words. To train the model described in § 3.2, we exclude punctuation as well as clitics from the target word local context. These words usually do not provide much information about the target word and will increase model sparsity. All data sets used in our experiments have undergone the following preprocessing steps: all Arabic data is Alef/Ya normalized and tokenized using MADAMIRA v1. (Pasha et al., 2014) according to Arabic Treebank (ATB) tokenization scheme (Maamouri et al., 2004). We used Tree Tagger (Schmid, 1994) to tokenize English data. Tools We use GIZA++ (Och and Ney, 2003) for word alignment. We obtain word clusters from word2vec (Mikolov et al., 2013) K-means word clustering tool. We use the continuous bag of word model to build word vectors of size 200 using a word window of size 8 for both left and right. The number of negative samples for logistic regression is set to 25 and threshold used for sub-sampling of frequent words is set to 10−5 in the model with 15 iterations. We also use full softmax to obtain the probability distribution. We use AIDA (Elfardy and Diab, 2013) as the dialect identification tool. AIDA also provides a list of MSA equivalents for id"
W15-1005,P03-1021,0,0.00975834,"INE3 PARL WCcor PARL+WC PARL+WCcor BLEU METEOR TER WER PER 20.6 20.1 21.3 20.7 20.9 21.0 21.3 27.5 27.2 28.0 27.1 27.7 27.7 27.9 65.9 68.3 65.2 67.5 65.4 66.2 65.5 69.2 71.6 68.6 69.6 68.7 68.5 68.0 45.3 46.6 44.6 45.5 44.8 45.3 44.5 Table 1: Evaluation results (BLEU, METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), WER, PER) on the Bolt-arz test set compared to the baselines. SMT System We use Moses decoder (Koehn et al., 2007) to build a standard phrase-based SMT system. Feature weights are tuned to maximize BLEU score on the tuning set using Minimum Error Rate Training (MERT) (Och, 2003) algorithm. Final results are reported by averaging over three tuning sessions with random initialization. Significant test is also performed to make sure that gains in the results are statistically significant. We use the implementation of Clark et al. (2011) to compute the p-value via approximate randomization algorithms. Since AIDA generates MSA equivalents in the lemma form, we use a factored translation model with lemma and POS factors. We use GIZA++ (Och and Ney, 2003) to word align the parallel corpus. We use SRILM (Stolcke and others, 2002) to build 5-gram language models with modified"
W15-1005,P02-1040,0,0.0917197,"Missing"
W15-1005,pasha-etal-2014-madamira,1,0.881195,"Missing"
W15-1005,C04-1117,0,0.0961854,"Missing"
W15-1005,2006.amta-papers.25,0,0.108255,"Missing"
W15-1005,H05-1097,0,0.10207,"Missing"
W15-1005,P94-1019,0,0.163482,"nguage which bears the same meaning. By doing this, we benefit from availability of standard parallel data to choose a more accurate target translation for the false friends. We aim to identify false friends without any labeled training data. We then try to choose equivalents from the standard language for the identified false friends. We exploit a classifier for identifying false friends and designing a word sense disambiguator for finding the best equivalent from the standard language. We employ unsupervised word alignment from parallel text and a taxonomy-based semantic similarity measure (Wu and Palmer, 1994) to automatically acquire training data for the FF identifier. Our word sense disambiguator benefits from unsupervised word clusters to model the context. We obtain word clusters from a large monolingual text in the standard language. Training the model only involves counting the coocurrences of 39 Proceedings of SSST-9, Ninth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39–48, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics each word with word clusters for different context definitions. During decoding (disambiguation), for a w"
W15-1005,2012.amta-papers.29,0,0.711306,"Missing"
W15-1304,W09-3012,1,0.804462,"(SW) level of commitment to a given proposition, which could be their own or a reported proposition. Modeling this type of knowledge explicitly is useful in determining an SWs cognitive state, also referred to as person’s private state (Wiebe et al., 2005). Wiebe et al. (2005) use the definition of (Quirk et al., 1985), who defines a private state to be an “internal (state) Initial work addressed the task of automatically identifying LCB of the SW. Approaches to date have relied on supervised models dependent on manually annotated data. There are two standard annotated corpora, the LU corpus (Diab et al., 2009) and FactBank (Saur´ı and Pustejovsky, 2009). Though in effect aiming for the same objective, both corpora use different terminology, different annotation standards, and they cover different genres. Previous studies performed on these corpora were conducted independently. In this work, we explore both corpora systematically and investigate their respective proposed tag sets. We experiment with multiple machine learning algorithms, varying the tag sets as we go along. Our goal is to build an automatic LCB tagger that is robust in a multi-genre context. Eventually we aim to adapt this tagger to"
W15-1304,P14-5010,0,0.0128785,"Missing"
W15-1304,W09-1501,0,0.0661361,"Missing"
W15-1304,C10-2117,1,0.948009,"s from the LU corpus in two major respects (other than the granularity in which they capture annotations): 1) FactBank is roughly four times the size of the LU corpus, and 2) FactBank is more homogeneous in terms of genre than the LU corpus as it consists primarily of newswire. In this paper, we unify the factuality annotations in Factback and the level of committed belief annotations present in the LU corpus to a 4-way committed-belief distinction.2 2 For an additional discussion of the relation between factuality and belief, see (Prabhakaran et al., 2015) 3 Approach Following previous work (Prabhakaran et al., 2010), we adopt a supervised approach to the LCB problem. We experiment with the two available manually annotated corpora, the LU and FB corpora. Going beyond previous approaches to the problem reported in the literature, our goal is to create a robust LCB system while gaining a deeper understanding of the phenomenon of LCB as an expressed modality by systematically teasing apart the different factors that affect performance. 3.1 Annotation Transformations The NCB category of the LU tagging scheme captures two different notions: that of uncertainty of the speaker/writer and that of belief being att"
W15-1304,S15-1009,1,0.70443,"o model LCB. From a computational perspective, FactBank differs from the LU corpus in two major respects (other than the granularity in which they capture annotations): 1) FactBank is roughly four times the size of the LU corpus, and 2) FactBank is more homogeneous in terms of genre than the LU corpus as it consists primarily of newswire. In this paper, we unify the factuality annotations in Factback and the level of committed belief annotations present in the LU corpus to a 4-way committed-belief distinction.2 2 For an additional discussion of the relation between factuality and belief, see (Prabhakaran et al., 2015) 3 Approach Following previous work (Prabhakaran et al., 2010), we adopt a supervised approach to the LCB problem. We experiment with the two available manually annotated corpora, the LU and FB corpora. Going beyond previous approaches to the problem reported in the literature, our goal is to create a robust LCB system while gaining a deeper understanding of the phenomenon of LCB as an expressed modality by systematically teasing apart the different factors that affect performance. 3.1 Annotation Transformations The NCB category of the LU tagging scheme captures two different notions: that of"
W15-1304,bethard-etal-2014-cleartk,0,\N,Missing
W15-1524,W10-2417,0,0.0449415,"Missing"
W15-1524,D08-1030,1,0.185923,"entry. This is based on the observation that social media data might contain non-standard spelling of words since it contains the DA variety. • Morphological Features: The morphological features that we employ in our feature set are generated by MADAMIRA (Pasha et al., 2014). The set comprises the following: – Part of Speech (POS) tags: We use POS tags generated from MADAMIRA within a window of ±1 (POS-1, POS0, POS1); – Capitalization (CAPS): In order to circumvent the lack of capitalization in Arabic, we check the capitalization of the translated NE which could indicate that a word is an NE (Benajiba et al., 2008). This feature is dependent on the English gloss generated by MADAMIRA. This feature is set to true when the gloss starts with a capital letter; – Aspect (ASP), person (PERS), proclitics0 (PROC0), proclitics1 (PROC1), proclitics2 (PROC2), proclitics3 (PROC3), enclitics0 (ENC0); detailed description for these features is provided in Table 1; Feature Aspect Person Proclitic3 Proclitic2 Proclitic1 Proclitic Enclitics Feature Values Verb aspect: Command, Imperfective, Perfective, Not applicable Person Information: 1st, 2nd, 3rd, Not applicable Question proclitic: No proclitic, Not applicable, Inte"
W15-1524,P10-2052,1,0.87216,"Missing"
W15-1524,J92-4003,0,0.141202,", 2nd person feminine plural/singular, 2nd person masculine plural/singular, 3rd person dual/plural, 3rd person feminine plural/singular, 3rd person masculine plural/singular, Vocative particle, Negative particle lA, Interrogative pronoun mA, Interrogative pronoun mA, Interrogative pronoun mn, Relative pronoun mn, m, mA, Subordinating conjunction m, mA. Table 1: Morphological Features – isNum: Binary feature that is set to true if the token is a number; – isNoun: Binary feature that is set to true if the token is proper noun (i.e. POS=noun prop). • Brown Clustering IDs (BC): Brown clustering (Brown et al., 1992) is a hierarchical clustering approach that maximizes the mutual information of word bigrams. Word representations, especially Brown Clustering, have been shown to improve the performance of NER system when added as a feature (Turian et al., 2010). In this work, we use Brown Clustering IDs (BC) of variable prefix lengths (4,5,6,7,10,13 and the full length of the cluster ID) as features resulting in the following set of features BC4, BC5, BC6, BC7, BC10, BC13, and BC, respectively. For example if AmrykA ’America’ has the brown cluster ID BC=11110010 then BC7=1111001, whereas BC10 and BC13 are e"
W15-1524,darwish-gao-2014-simple,0,0.494441,"bedding and word representations on the NER performance, with the potential of these features substituting the use of extensive large gazetteers. 3.1 Baseline For our baseline system (BL), we reimplemented the features proposed in (Abdul-Hamid and Darwish, 2010) that produced the best results: previous and next word, and leading and trailing +/(2-4) character ngrams. These features are chosen as a preferred set for the baseline since they are directly applicable to DA, as none of the features rely on the availability of morphological or syntactic analyzers. This baseline is also adapted from (Darwish and Gao, 2014). We opted for this baseline as opposed to (Darwish and Gao, 2014)’s NER system since they use Wikipedia gazetteers (WikiGaz) for their NER system.2 However, we report the results of applying their features using exact match against the gazetteers’ entries in Section 4.3. 3.2 Our NER Features In addition to the features employed in the baseline BL, we introduce the following additional features in our NER system: • Lexical Features: character n-gram features, the leading and trailing character bigrams (L2), trigrams (L3), and quadrigrams (L4); • Contextual Features (CTX): The surrounding undia"
W15-1524,P13-1153,0,0.0578942,"will be grouped together in the same cluster and will have a common prefix; • Word2vec Cluster IDs : Word2vec is an algorithm for learning embeddings using a neural network model (Mikolov et al., 2013). Embeddings are represented by a set of latent variables, where each word is represented by a specific instantiation of these variables. In our system, we apply K-means clustering on the word vectors and use the clusters IDs as features. 180 3.3 Datasets We use Microblogs and Dialectal weblogs datasets for our experiments: • Twitter dataset: We use the training and test data split proposed in (Darwish, 2013), where the training dataset contains 3,646 tweets which were randomly selected from tweets that were authored in the period of May 3-12, 2012. The tweets were scraped from Twitter using the query lang:ar. The testing data contains 1,423 tweets that were randomly selected from tweets authored between November 23, 2011 and November 27, 2011. This dataset has also been used in (Darwish and Gao, 2014) for testing. Both datasets are annotated using the Linguistics Data Consortium ACE tagging guidelines; • Dialectal Arabic dataset (DA-EGY): The annotated data was chosen from a set of web blogs that"
W15-1524,W10-2608,0,0.0142374,"Missing"
W15-1524,habash-etal-2012-conventional,1,0.740395,"s: this is a problem facing all NER systems for all languages addressing NER in social media text, 1 We use the Buckwalter encoding system to render Arabic. For a reference listing please see http://www.qamus.org/transliteration.htm 177 since by definition such media has a ubiquitous presence of highly productive names exemplified by the usage of nick names, hence the PERSON class in social media NER will always have a coverage problem; • Applying NLP tools designed for MSA to DA results in considerably lower performance, thus the need to build resources and tools that specifically target DA (Habash et al., 2012). The majority of existing NER systems rely on the use of gazetteers to improve the system accuracy (Kazama and Torisawa, 2007), however, large external resources are correlated with higher performance cost. In this paper, we study the impact of word representation and embedding features on Arabic NER performance for Twitter and Dialectal Arabic, and demonstrate that our proposed features show comparable and superior results to other NER systems that use large gazetteers. Our contributions are as follows: • Show the impact of using word representations and embedding on NER performance; • Propo"
W15-1524,N13-1044,0,0.0307212,"ka undiacritized, which results in higher ambiguity that can only be resolved using contextual information (Benajiba et al., 2009). Examples of ambiguity are: mSr, may be miSor as in ’Egypt’ or muSir as in ’insistent’; qTr may be the name of the country ’Qatar’ if vowelized/diacritized as qaTar, qaTor for ’sugar syrup’, quTor for ’diameter’. In addition to the afore mentioned challenges, in general, for Arabic NER in general compared to Latin-based languages, DA NER faces additional issues: • Lack of annotated data for supervised DA NER; • Lack of standard orthographies or language academics (Habash et al., 2013): Unlike MSA, the same word in DA can be rewritten in so many forms, e.g. mAtEyT$, mtEyt$, mA tEyT$, ’do not cry’, are all acceptable variants since there is no one standard; • Lack of comprehensive Gazetteers: this is a problem facing all NER systems for all languages addressing NER in social media text, 1 We use the Buckwalter encoding system to render Arabic. For a reference listing please see http://www.qamus.org/transliteration.htm 177 since by definition such media has a ubiquitous presence of highly productive names exemplified by the usage of nick names, hence the PERSON class in socia"
W15-1524,D07-1073,0,0.0221048,"r encoding system to render Arabic. For a reference listing please see http://www.qamus.org/transliteration.htm 177 since by definition such media has a ubiquitous presence of highly productive names exemplified by the usage of nick names, hence the PERSON class in social media NER will always have a coverage problem; • Applying NLP tools designed for MSA to DA results in considerably lower performance, thus the need to build resources and tools that specifically target DA (Habash et al., 2012). The majority of existing NER systems rely on the use of gazetteers to improve the system accuracy (Kazama and Torisawa, 2007), however, large external resources are correlated with higher performance cost. In this paper, we study the impact of word representation and embedding features on Arabic NER performance for Twitter and Dialectal Arabic, and demonstrate that our proposed features show comparable and superior results to other NER systems that use large gazetteers. Our contributions are as follows: • Show the impact of using word representations and embedding on NER performance; • Propose a set of features that does not include the use of external resources; • Produce comparable NER performance to other systems"
W15-1524,pasha-etal-2014-madamira,1,0.0353354,"Missing"
W15-1524,D11-1141,0,0.124433,"Missing"
W15-1524,J14-2008,0,0.0560144,"Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics proper nouns are not capitalized, which renders the identification of NEs more complicated; • Nominal Confusability: Some words can be proper nouns, nouns, or adjectives. For instance, jamiyolap1 which means ’beautiful’ can be a proper noun or an adjective. Another example, jamAl, which means ’beauty’, is a noun but can be a common noun or a proper name; • Agglutination: Since Arabic exhibits concatenative morphology, we note the pervasive presence of affixes agglutinating to proper nouns as prefixes and suffixes (Shaalan, 2014). For instance, determiners appear as prefixes Al as in (AlqAhrp, ’Cairo’), likewise with affixival prepositions such as l, ’for’ (ldm$q ’for/to/from Damascus’), as well as prefixed conjunctions such as w, ’and’, as in (wAlqds ’and Jerusalem’); • Absence of Short Vowels (Diacritic Markers): Written MSA, even in newswire, is underspecified for short vowels, aka undiacritized, which results in higher ambiguity that can only be resolved using contextual information (Benajiba et al., 2009). Examples of ambiguity are: mSr, may be miSor as in ’Egypt’ or muSir as in ’insistent’; qTr may be the name o"
W15-1524,W97-0315,0,0.693517,"Missing"
W15-1524,P10-1040,0,0.0312586,"errogative pronoun mA, Interrogative pronoun mn, Relative pronoun mn, m, mA, Subordinating conjunction m, mA. Table 1: Morphological Features – isNum: Binary feature that is set to true if the token is a number; – isNoun: Binary feature that is set to true if the token is proper noun (i.e. POS=noun prop). • Brown Clustering IDs (BC): Brown clustering (Brown et al., 1992) is a hierarchical clustering approach that maximizes the mutual information of word bigrams. Word representations, especially Brown Clustering, have been shown to improve the performance of NER system when added as a feature (Turian et al., 2010). In this work, we use Brown Clustering IDs (BC) of variable prefix lengths (4,5,6,7,10,13 and the full length of the cluster ID) as features resulting in the following set of features BC4, BC5, BC6, BC7, BC10, BC13, and BC, respectively. For example if AmrykA ’America’ has the brown cluster ID BC=11110010 then BC7=1111001, whereas BC10 and BC13 are empty strings. This feature is based on the observation that semantically similar words will be grouped together in the same cluster and will have a common prefix; • Word2vec Cluster IDs : Word2vec is an algorithm for learning embeddings using a ne"
W15-1524,W14-3610,1,0.51051,"ed between November 23, 2011 and November 27, 2011. This dataset has also been used in (Darwish and Gao, 2014) for testing. Both datasets are annotated using the Linguistics Data Consortium ACE tagging guidelines; • Dialectal Arabic dataset (DA-EGY): The annotated data was chosen from a set of web blogs that are manually identified by LDC as Egyptian dialect and contains nearly 40k tokens. The data was annotated by one native Arabic speaker annotator who followed the Linguistics Data Consortium guidelines for tagging. We use the same 80/20 train/test 5-fold cross validation split proposed in (Zirikly and Diab, 2014) Table 2 shows dataset statistics, namely number of tokens, and the named entity types: PER, LOC, and ORG. Brown Clustering and word2vec Data In our work, we run brown clustering and word2vec three times based on the data genre: i) Newswire Twitter-Train Twitter-Test DA-EGY #Tokens 55k 26k 24k #PER 788 464 311 #LOC 713 587 155 #ORG 449 316 19 4 Experiments & Discussion 4.1 Table 2: Twitter and DA-EGY Evaluation data statistics (NW): Arabic Gigaword, ANERCorp, and NW data of ACE2005 and ACE2006; ii) Broadcast News (BN): BN data of ACE2005 and ACE2006; iii) Weblogs (WL): Twitter data (training a"
W15-3209,2007.mtsummit-papers.20,1,0.567467,"stances of a word type are observed in a corpus, and (2) ambiguity where a word has multiple readings or interpretations. Undiacritized surface forms of an Arabic word might have as many as 200 readings depending on the complexity of its morphology. The lack of diacritics usually leads to considerable lexical ambiguity, as shown in the example in Table 1, a reason for which diacritization, aka vowel/diacritic restoration, has been shown to improve state-of-the-art Arabic automatic systems such as speech recognition (ASR) (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications. In general, building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available diacritized MSA corpora are generally limited to the newswire genres (as distributed by the LDC) or religion related texts such as the Quran or the Tashkeela corpus.2 In this paper we present a pilot study where we annotate a sample of non-diacritized text extracted from fiv"
W15-3209,P06-1073,0,0.583986,"lts show that readers benefited from the disambiguating diacritics. This study was a MIN scheme exploration focused on heterophonic-homographic target verbs that have different pronunciations in active and Related Work The task of diacritization is about adding diacritics to the canonical underspecified written form. This task has been discussed in several research works in various NLP areas addressing various applications. Automatic Arabic Diacritization Much work has been done on recovery of diacritics over the past two decades by developing automatic methods yielding acceptable accuracies. Zitouni et al. (2006) built a diacritization framework based on such as number, gender, aspect, voice, etc. Whereas a lemma is a conventionalized citation form. 82 ATB News ATB BN ATB WebLog Tashkeela Wikipedia Total Size in words 2,478 3,093 3,177 5,172 2,850 16,770 GOLD annotation Yes Yes Yes Yes No - Table 2: The size of the data for annotation per corpus genre passive. classical Arabic books). This corpus contains over 6 million words fully diacritized. For our study we include a subset of 5k words from this corpus. In this work we are interested in two components: annotating large amounts of varied genres typ"
W15-3209,P05-1071,0,0.0659168,"n between Form I and Form II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorized and un-factorized textual features to automatically diacritize raw Arabic text. Emam and Fischer (2011) introduced a hierarchical approach for diacritization based on a search method in a set of dictionaries of sentences, phrases and words, using a top down strategy. More"
W15-3209,N07-2014,0,0.648924,"acritic is the distinction between Form I and Form II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorized and un-factorized textual features to automatically diacritize raw Arabic text. Emam and Fischer (2011) introduced a hierarchical approach for diacritization based on a search method in a set of dictionaries of sentences, phrases and words, using a"
W15-3209,maamouri-etal-2008-enhancing,0,0.396831,"very text genre, two annotators were asked to annotate independently a sample of 100 words. We measured the IAA between two annotators by averaging WER (Word Error Rate) over all pairs of words. The higher the WER between two annotations, the lower their agreement. The results given in Table 5, show clearly that the Advanced mode is the best strategy to adopt for this diacritization task. It is the less confusing method on all text genres (with WER between 1.56 and 5.58). We note that Wiki annotations in Advanced mode garner the highest IAA with a very low WER. We extended the LDC guidelines (Maamouri et al., 2008) by adding some diacritization rules: The shadda mark should not be added to the definite article (e.g., àñÒJ ÊË@/’lemon’ and not àñÒJ ÊË@); The sukuun sign should not be indicated at the end /’from’); The letters folof silent words (e.g., áÓ lowed by a long Alif, should not be diacritized  as it is a deterministic diacritization ( Y« @ñ ®Ë@/’the rules’); Abbreviations are not diacritized ( Õ»/’km’, /’kg’). Ñª» We also added an appendix that sumWe measure the reliability of the annotations by comparing them against gold standard annotations. In order to build the gold Wiki annotations, we hi"
W15-3209,pasha-etal-2014-madamira,1,0.916558,"Missing"
W15-3209,P08-2030,1,0.710351,"II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorized and un-factorized textual features to automatically diacritize raw Arabic text. Emam and Fischer (2011) introduced a hierarchical approach for diacritization based on a search method in a set of dictionaries of sentences, phrases and words, using a top down strategy. More recently, Abandah et"
W15-3209,W04-1612,0,0.67965,"kAtib/’writer’ and I.KA¿/kAtab/’to correspond’ distinguishes between the meanings of the word (lexical disambiguation) rather than their inflections. Any of diacritics may be used to mark lexical variation. A common example with the shadda (gemination) diacritic is the distinction between Form I and Form II of Arabic verb derivations. Form II, indicates, in most cases, added causativity to the Form I meaning. Form II is marked by doubling the second rad ical of the root used in Form I: É¿ @/Akal/’ate’ maximum entropy classification to restore missing diacritics on each letter in a given word. Vergyri and Kirchhoff (2004) worked on automatic diacritization with the goal of improving automatic speech recognition (ASR). Different algorithms for diacritization based mainly on morphological analysis and lexeme-based language models were developed (Habash and Rambow, 2007; Habash and Rambow, 2005; Roth et al., 2008). Various approaches combining morphological analysis and/or Hidden Markov Models for automatic diacritization are found in the literature (Bebah et al., 2014; Alghamdi and Muzaffar, 2007; Rashwan et al., 2009). Rashwan et al. (2009) designed a stochastic Arabic diacritizer based on a hybrid of factorize"
W15-3216,W14-3620,1,0.803001,"Missing"
W15-3216,C12-2011,1,0.927158,"Missing"
W15-3216,P00-1037,0,0.304702,"Missing"
W15-3216,N12-1067,0,0.038977,". Our tri-gram language model is trained on the Arabic Gigaword Corpus, 5th edition (Parker et al., 2011) and a corpus crawled from Al-Jazeera (Attia et al.; 2012). For the LM disambiguation we use the ‘-fb’ option (forward-backward tracking), and we provide candidates with probability scores collected from the QALB training data. Both of the forward-backward tracking and the probability scores in tandem yield better results than the default values. We evaluate the performance of our system against the gold standard using the MaxMatch (M2) method for evaluating grammatical error correction by Dahlmeier and Ng (2012). Our best f-score is obtained by priming candidates from the training data, adding Al-Jazeera corpus to Gigaword 5, and using the two-pass CRF punctuation prediction. Table 3 and 4 show the results on Alj and L2 development sets respectively. Table 5 shows the results on Alj and L2 test sets. Alhm#20; Allhm#17 ElY#818; Ely#318 # Experiment R P F 1 Baseline (HASP’14) Prime non-word candidates from the training set Include real-word candidates from the training data Prime merge errors from the training set Post-processing Two-pass punctuation correction 3 gram LM and adding Al-Jazeera corpus to"
W15-3216,I08-2131,0,0.199114,"rns punctuation placement, and then it learns to identify punctuation types. 1 Introduction In this paper1 we describe our system for Arabic spelling error detection and correction, HASP2015 (Hybrid Arabic Spelling and Punctuation Corrector). We introduce significant improvements to our previous version HASP2014 (Attia et al., 2014). We participate with HASP-2015 in the QALB-2015 Second Shared Task on Arabic Error Correction (Rozovskaya et al., 2015). The problem of Arabic spelling error correction has been investigated in a number of papers (Haddad and Yaseen, 2007; Alfaifi and Atwell, 2012; Hassan et al., 2008; Attia et al., 2012; Alkanhal et al., 2012). Significant contributions were also introduced in the 2014 Shared Task on Arabic Error Correction (Mohit et al., 2014) including (Rozovskaya et al., 2014; Nawar and Ragheb, 2014; Jeblee et al., 2014; and Mubarak and Darwish, 2014). The QALB-‐2015 shared task is an extension of the first QALB shared task (Mohit et al., 2014) that took place in 2014. QALB-‐2014 addressed errors in comments written to Aljazeera articles by native Arabic speakers (Zaghouani et al., 2014). This year&apos;s competition includes two tracks, and, in addition to errors produce"
W15-3216,E09-2008,0,0.0132126,"tionary. 140 4.2 Candidate Generation Correcting spelling errors is ideally treated as a probabilistic problem formulated as (Kernigan, 1990; Norvig, 2009; Brill, and Moore, 2000): ??????! ?(? |?) ?(?) Here ?(?) is the probability that ? is the correct word (or the language model), and ?(? |?) is the probability that ? is typed when ? is intended (the error model or noisy channel model), ??????! is the scoring mechanism that computes the correction c that maximizes the probability. In HASP-2014, we ranked candidates according to their edit distance score using the finite state compiler, foma (Hulden, 2009), but in HASP-2015, we rank candidates according to their probability, (? |?) , as derived from the training data, and we pass candidates along with their probability scores to the language model. Again, the edit distance candidates and their ranking are used when no probability information is available from the training data. The following are some illustrative examples of the statistical information extracted from the training data for the various error types. Non-word errors: An “ ﺍاﻥنthat” &gt;n#7781; <n#1485; |n#29 AlA “ ﺍاﻻbut” <lA#1442; &gt;lA#225 Semantic errors: Alhm “ ﺍاﻟﻬﮭﻢworry” El"
W15-3216,C90-2036,0,0.765073,"Missing"
W15-3216,W14-3617,0,0.0123264,"mprovements to our previous version HASP2014 (Attia et al., 2014). We participate with HASP-2015 in the QALB-2015 Second Shared Task on Arabic Error Correction (Rozovskaya et al., 2015). The problem of Arabic spelling error correction has been investigated in a number of papers (Haddad and Yaseen, 2007; Alfaifi and Atwell, 2012; Hassan et al., 2008; Attia et al., 2012; Alkanhal et al., 2012). Significant contributions were also introduced in the 2014 Shared Task on Arabic Error Correction (Mohit et al., 2014) including (Rozovskaya et al., 2014; Nawar and Ragheb, 2014; Jeblee et al., 2014; and Mubarak and Darwish, 2014). The QALB-‐2015 shared task is an extension of the first QALB shared task (Mohit et al., 2014) that took place in 2014. QALB-‐2014 addressed errors in comments written to Aljazeera articles by native Arabic speakers (Zaghouani et al., 2014). This year&apos;s competition includes two tracks, and, in addition to errors produced by native speakers, also includes correction of texts written by learners of Arabic as a foreign language (L2) (Zaghouani et al., 2015). The native track includes Alj-‐train-‐2014, Alj-‐dev-‐2014, Alj-‐ test-‐2014 texts from QALB-‐2014. The L2 track includes L2-‐tra"
W15-3216,W14-3619,0,0.0142221,"unctuation Corrector). We introduce significant improvements to our previous version HASP2014 (Attia et al., 2014). We participate with HASP-2015 in the QALB-2015 Second Shared Task on Arabic Error Correction (Rozovskaya et al., 2015). The problem of Arabic spelling error correction has been investigated in a number of papers (Haddad and Yaseen, 2007; Alfaifi and Atwell, 2012; Hassan et al., 2008; Attia et al., 2012; Alkanhal et al., 2012). Significant contributions were also introduced in the 2014 Shared Task on Arabic Error Correction (Mohit et al., 2014) including (Rozovskaya et al., 2014; Nawar and Ragheb, 2014; Jeblee et al., 2014; and Mubarak and Darwish, 2014). The QALB-‐2015 shared task is an extension of the first QALB shared task (Mohit et al., 2014) that took place in 2014. QALB-‐2014 addressed errors in comments written to Aljazeera articles by native Arabic speakers (Zaghouani et al., 2014). This year&apos;s competition includes two tracks, and, in addition to errors produced by native speakers, also includes correction of texts written by learners of Arabic as a foreign language (L2) (Zaghouani et al., 2015). The native track includes Alj-‐train-‐2014, Alj-‐dev-‐2014, Alj-‐ test-‐2014 t"
W15-3216,pasha-etal-2014-madamira,1,0.836583,"Missing"
W15-3216,W14-3622,0,0.0116626,"rid Arabic Spelling and Punctuation Corrector). We introduce significant improvements to our previous version HASP2014 (Attia et al., 2014). We participate with HASP-2015 in the QALB-2015 Second Shared Task on Arabic Error Correction (Rozovskaya et al., 2015). The problem of Arabic spelling error correction has been investigated in a number of papers (Haddad and Yaseen, 2007; Alfaifi and Atwell, 2012; Hassan et al., 2008; Attia et al., 2012; Alkanhal et al., 2012). Significant contributions were also introduced in the 2014 Shared Task on Arabic Error Correction (Mohit et al., 2014) including (Rozovskaya et al., 2014; Nawar and Ragheb, 2014; Jeblee et al., 2014; and Mubarak and Darwish, 2014). The QALB-‐2015 shared task is an extension of the first QALB shared task (Mohit et al., 2014) that took place in 2014. QALB-‐2014 addressed errors in comments written to Aljazeera articles by native Arabic speakers (Zaghouani et al., 2014). This year&apos;s competition includes two tracks, and, in addition to errors produced by native speakers, also includes correction of texts written by learners of Arabic as a foreign language (L2) (Zaghouani et al., 2015). The native track includes Alj-‐train-‐2014, Alj-‐dev-‐20"
W15-3216,W15-1614,0,0.233893,"sk on Arabic Error Correction (Mohit et al., 2014) including (Rozovskaya et al., 2014; Nawar and Ragheb, 2014; Jeblee et al., 2014; and Mubarak and Darwish, 2014). The QALB-‐2015 shared task is an extension of the first QALB shared task (Mohit et al., 2014) that took place in 2014. QALB-‐2014 addressed errors in comments written to Aljazeera articles by native Arabic speakers (Zaghouani et al., 2014). This year&apos;s competition includes two tracks, and, in addition to errors produced by native speakers, also includes correction of texts written by learners of Arabic as a foreign language (L2) (Zaghouani et al., 2015). The native track includes Alj-‐train-‐2014, Alj-‐dev-‐2014, Alj-‐ test-‐2014 texts from QALB-‐2014. The L2 track includes L2-‐train-‐2015 and L2-‐dev-‐2015. This data was released for the development of the systems. The systems are scored on blind test sets Alj-‐test-‐2015 and L2-‐test-‐2015. Our system is ranked third and fourth on the Alj and L2, respectively. The shared task data deals with “errors” in the general sense which comprise: a) punctuation errors; b) non-word errors; c) real-word spelling errors; d) grammatical errors (related to case, number and gender); and, e)"
W15-3216,J03-1002,0,0.00924074,"Missing"
W15-3216,W14-3605,0,\N,Missing
W15-3216,zaghouani-etal-2014-large,0,\N,Missing
W15-3216,W13-3601,0,\N,Missing
W15-3222,P05-1071,0,0.138546,"Missing"
W15-3222,N06-2013,0,0.103124,"Missing"
W15-3222,J92-4003,0,0.215706,"r. The input to this classification problem consists of a list of digit-normalized tokens with explicit enclitic and proclitic affixes marked with ’+’ at segmentation points. The feature vector consists of [-2,+2] tokens in context, character N-grams, N≤4, for the current token, the type of the current token {alpha, numeric}, and the previous 2 tag decisions. We add two more components (constraints and features) over the MD-AMIRA POS tagging pipeline as follows. 4.1 Clustering features: We add cluster IDs retrieved from a large unlabeled dataset using two clustering methods: Brown clustering (Brown et al., 1992), and word2vec K-means clustering (Mikolov et. al., 2013). ALMOR for Constrained Tagging ALMOR (Habah, 2007) is a morphological analysis and generation system for MSA and dialectal Arabic. Given a word, ALMOR retrieves all possible analyses for that word and a list of characteristics, including part-of-speech tags, for each analysis. ALMOR constructs the analyses by generating all possible segmentations and verifying the validity and compatibility of the segments on an underlying database of valid stems and affixes. In our POS tagging model, ALMOR is used as a source of external knowledge to c"
W15-3222,N04-4038,1,0.857086,"c role of their stems and are not segmented here. A word, in this context, refers to a stem and its inflectional and derivational affixes, and clitic segmentation is the process of separating clitics from words. Since clitics have their own meaning and part-of-speech tags, separating them reduces sparsity in the input space. In MSA, a word can have up to three proclitics and one enclitic. Table 1 shows some of the word classes that serve as clitics in MSA. Related Work The sequential NLP process presented in this paper is adapted from the AMIRA toolkit, which is described in (Diab, 2009) and (Diab et al., 2004), and is publicly available. Another data-driven part-of-speech tagger for Arabic was presented in (Kopru, 2011), which uses an HMM to learn an efficient classifier using surface features. The alternative approach is MADA, which relies on deep morphological analysis and disambiguation, as described in (Habah et al., 2005) and (Roth et al, 2008). Several SVM classifiers are trained to predict morphological features in the first stage. These features are then used to rank the morphological analyses retrieved from a dictionary, and the analysis with the highest score is taken as the final analysi"
W15-3222,W07-0812,1,0.834372,"Missing"
W15-3222,pasha-etal-2014-madamira,1,0.880489,"Missing"
W15-3222,D10-1107,0,0.0181589,"p, which is described in Section 3.1.2. We evaluate the performance of the separate steps and the whole pipeline from tokenization to part-of-speech tagging in Sections 6 to 8. We show that this approach is robust and efficient as it compares to state of the art accuracy while exhibiting robust performance on unseen words. 2 in (Punyakanok et al, 2005). This is related to the constrained POS tagging attempted here, where external inference is used to maintain consistency after learning. A related example of incorporating external resources to constrain the learned classifiers is presented in (Do and Roth, 2010) 3 Approach We adapt the AMIRA tagger approach by using linear support vector machines (SVM) as our basic classification machinery for both MD-AMIRA and MD-AMIRA+EX. We approach both Tokenization and POS tagging as classification problems. The basic models directly follow the implementation details described in (Diab, 2009). 3.1 Tokenization 3.1.1 Clitic Segmentation Clitics are independent meaning-bearing units that are phonologically and orthographically merged with words, either as prefixes (proclitics) or suffixes (enclitics). Clitics are different from derivational or inflectional affixes"
W15-3222,P08-2030,1,0.80249,"ave up to three proclitics and one enclitic. Table 1 shows some of the word classes that serve as clitics in MSA. Related Work The sequential NLP process presented in this paper is adapted from the AMIRA toolkit, which is described in (Diab, 2009) and (Diab et al., 2004), and is publicly available. Another data-driven part-of-speech tagger for Arabic was presented in (Kopru, 2011), which uses an HMM to learn an efficient classifier using surface features. The alternative approach is MADA, which relies on deep morphological analysis and disambiguation, as described in (Habah et al., 2005) and (Roth et al, 2008). Several SVM classifiers are trained to predict morphological features in the first stage. These features are then used to rank the morphological analyses retrieved from a dictionary, and the analysis with the highest score is taken as the final analysis for the given word. This deep analysis results in accurate and detailed tagging albeit slower than simple SVM methods. Finally, the problem of classification and incorporating structural constraints on the output is studied Type Category Examples Proclitic Proclitic Proclitic Proclitic Enclitics Definite article Prepositions Conjunctions Futu"
W16-0403,P12-1091,1,0.230079,"ID * Time * Hashtag Hashtag Content URL Re-Tweet *Reply User ID Content Unigram Content Bigram Pos Unigram Pos Bigram *NER *Event *Sentiment *Emoticon Value Binary Binary String Binary Binary Binary Binary String String String String String String String Binary Table 6: List of S15 features used for RR Experiment . to tell us what the sentence is about. We assume that the semantic space of both the observed and missing words make up the complete semantic profile of a sentence. We propose the Tweet Latent Vector (TLV) feature by applying the Semantic Textual Similarity (STS) model proposed by (Guo and Diab, 2012) (Guo et al., 2014), which built on the WordNet+Wiktionary+Brown+training data set. STS preprocess each short text by tokenization and stemming, then changes the preprocessed data by removing infrequent words and TF-IDF weighting, and finally uses the model to extract the latent semantics, which is represented as a 100-dimension vector. 3.3.2 Committed Belief For the belief feature we investigate the level of committed belief for each tweet, which is a modality in natural language, and indicates the author’s belief in a proposition. We relied on the Werner et al. (Werner et al., 2015) belief t"
W16-0403,C14-1047,1,0.851887,"ashtag Content URL Re-Tweet *Reply User ID Content Unigram Content Bigram Pos Unigram Pos Bigram *NER *Event *Sentiment *Emoticon Value Binary Binary String Binary Binary Binary Binary String String String String String String String Binary Table 6: List of S15 features used for RR Experiment . to tell us what the sentence is about. We assume that the semantic space of both the observed and missing words make up the complete semantic profile of a sentence. We propose the Tweet Latent Vector (TLV) feature by applying the Semantic Textual Similarity (STS) model proposed by (Guo and Diab, 2012) (Guo et al., 2014), which built on the WordNet+Wiktionary+Brown+training data set. STS preprocess each short text by tokenization and stemming, then changes the preprocessed data by removing infrequent words and TF-IDF weighting, and finally uses the model to extract the latent semantics, which is represented as a 100-dimension vector. 3.3.2 Committed Belief For the belief feature we investigate the level of committed belief for each tweet, which is a modality in natural language, and indicates the author’s belief in a proposition. We relied on the Werner et al. (Werner et al., 2015) belief tagger to tag the Co"
W16-0403,P04-1043,0,0.0479606,"and the V11 model. The Majority baseline assigns the majority label from the training data set to all the test data. In the S15 baseline we perform the RR experiment by relying on the features that are proposed in S15 and shown in Table 6. We performed the RR experiment with different models in Weka platform and chose the SMO, which yield to the highest result in this experiment. We also compared our results with V11, which reported the results as Mean Average Precision. 4.3 Machine Learning Tools For the experiments we employ SVM Tree Kernel model, which was proposed by Alessandro Moschitti (Moschitti, 2004). In another experiment, we perform the RR task by applying S15 features, which are illustrated at Table 6 by hiring the SMO classifier on Weka (Hall et al., 2009). 4.4 Experiments and Evaluations We implement two main experimental pipelines: Rumor Retrieval (RR) and Belief investigation. Content and TLV features are employed for the RR task and then we conduct our experiment in two different phases. In the development phase we utilized development data for tuning. Then the model, which could reach the highest performance, is used on the test data set. Evaluating the performance of the propose"
W16-0403,D11-1147,0,0.827677,"ciety. There are many information seekers who do not rely on a single source to get information, but this is not always a good solution since even other news outlets sometime rely on social media when it comes to novel breaking news. Smart phones enable everyone to capture and tweet every single moment hours before TV cameras arrive. Considering that, social media is an appealing option for those who crave novel tempting news but on the other hand, could deceive anyone by well-structured and formatted rumors. In this study we work on a standard dataset of rumors collected by Qazvinian et al. (Qazvinian et al., 2011). In their work, the definition of rumor is defined as a statement whose truth value is unverifiable or deliberately false. We are using the same definition and not investigating the stimulus behind rumors creation. We investigate the problem of detecting rumors in Twitter data. We start with the motivation behind this research, and then the history of similar studies about rumors is overviewed. Then the overall pipeline is exposed, in which we adopt a supervised machine learning framework, and then we investigate the belief change for president Obama rumors in three years, and finally, we com"
W16-0403,W15-1304,1,0.824063,"posed by (Guo and Diab, 2012) (Guo et al., 2014), which built on the WordNet+Wiktionary+Brown+training data set. STS preprocess each short text by tokenization and stemming, then changes the preprocessed data by removing infrequent words and TF-IDF weighting, and finally uses the model to extract the latent semantics, which is represented as a 100-dimension vector. 3.3.2 Committed Belief For the belief feature we investigate the level of committed belief for each tweet, which is a modality in natural language, and indicates the author’s belief in a proposition. We relied on the Werner et al. (Werner et al., 2015) belief tagger to tag the Committed Belief as(CB) where someone(SW) strongly believes in the proposition, Non-committed belief (NCB) where SW reflects a weak belief in the proposition, and Non Attributable Belief (NA) where SW is not (or could not be) expressing a belief in the proposition (e.g., desires, questions etc.) There is also the ROB tag where SW’s intention is to report on SW else’s stated belief, whether or not they themselves believe it. The feature values are set to a binary 0 or 1 for each CB, NCB, NA, and ROB corresponding to unseen or observed. The following example illustrates"
W16-1201,S14-2010,1,0.825943,"oss-lingual models, so the results are comparable. WMF*: We train another Spanish WMF model with a more varied training set, similar in construction to the English monolingual model. This training set includes Wikipedia and newswire articles, so it’s more similar to the test set. This set consists of about 400K sentences extracted from the second edition of Spanish Gigawords (Mendona et al., 2009) and the Spanish Wikipedia Corpus (Reese et al., 2010). We use the same values for all the parameters, and we run ALS for 20 iterations. Table 3 shows the results on Semeval Spanish STS 2014 dataset (Agirre et al., 2014), which includes sentence pairs extracted from Spanish Wikipedia and news articles. We also show the results on the harder 2015 dataset (Agirre et al., 2015), which intentionally includes sentence pairs with higher degree of difficulty, such as sentences with shared vocabulary but different compositional meaning. The first row depicts the results obtained by the top system participating in the Semeval task, Semeval Best. While none of our models outperforms the official Semeval top ranking system, Semeval Best, we show that the Spanish models trained using the BMF and CMF models actually outpe"
W16-1201,W12-3102,0,0.175794,"Missing"
W16-1201,eisele-chen-2010-multiun,0,0.0473353,"Missing"
W16-1201,P12-1091,1,0.939731,"hey can also be used to learn classifiers that generalize to languages beyond the ones used in training. Introduction A large body of NLP research in recent years has focused on representing natural language words and phrases in high-dimensional continuous vector spaces. Such representations can be integrated with various NLP applications as they can be easily learned, processed, and compared, often in an unsupervised or semi-supervised manner. Distributed representations of words, or word embeddings, can be learned using global word co-occurrence statistics as in matrix factorization models (Guo and Diab, 2012; Pennington et al., 2014), or using local context as in neural probabilistic language models (Bengio et al., 2003; Collobert and Weston, 2008; Socher A number of models have recently been proposed for learning cross-lingual compositional representations (Klementiev et al., 2012; Shi et al., 2015; Pennington et al., 2014; Cavallanti et al., 2010; Mikolov et al., 2013; Coulmance et al., 2015; Pham et al., 2015). We propose a relatively simple and nuanced model inspired by the monolingual weighted matrix factorization (WMF) model proposed in (Guo and Diab, 2012), which we extend to the cross-lin"
W16-1201,C12-1089,0,0.616363,"ch representations can be integrated with various NLP applications as they can be easily learned, processed, and compared, often in an unsupervised or semi-supervised manner. Distributed representations of words, or word embeddings, can be learned using global word co-occurrence statistics as in matrix factorization models (Guo and Diab, 2012; Pennington et al., 2014), or using local context as in neural probabilistic language models (Bengio et al., 2003; Collobert and Weston, 2008; Socher A number of models have recently been proposed for learning cross-lingual compositional representations (Klementiev et al., 2012; Shi et al., 2015; Pennington et al., 2014; Cavallanti et al., 2010; Mikolov et al., 2013; Coulmance et al., 2015; Pham et al., 2015). We propose a relatively simple and nuanced model inspired by the monolingual weighted matrix factorization (WMF) model proposed in (Guo and Diab, 2012), which we extend to the cross-lingual setting. The WMF model learns word representations by decomposing a sparse tf-idf matrix into two lowrank factor matrices representing words and sen1 Proceedings of the Workshop on Multilingual and Cross-lingual Methods in NLP, pages 1–9, c San Diego, California, June 17, 2"
W16-1201,2005.mtsummit-papers.11,0,0.0455172,"l English model, the training set consists of 700K sentences derived from various resources. We extract and combine the following sets: a random set of 150K sentences from LDC’s English Gigaword fifth edition (Parker et al., 2011), a random set of 150K sentences from the English Wikipedia2 , the Brown Corpus (Francis, 1964), Wordnet (Miller, 1995) and Wiktionary3 definitions appended with examples. Bilingual Data: We extract training data for the bilingual models from WMT13 (Mach´acˇ ek and Bojar, 2013) sentence-aligned parallel corpora, specifically version 7 of the EuroParl parallel corpus (Koehn, 2005), the multiUN parallel corpus (Eisele 2 3 http://en.wikipedia.org http://www.wiktionary.org and Chen, 2010), and news commentary data for two language pairs: English-Spanish (en-es) and English-German (en-de). We train each bilingual model using a sample of 1M sentence pairs from these datasets. All sentences in our data are tokenized and stemmed, and number sequences are replaced with a special token as a normalization step. We use the Stanford CoreNLP toolkit (Manning et al., 2014) for English preprocessing, and Treetagger tools (Schmid, 1995) for both Spanish and German data. Words that app"
W16-1201,W13-2202,0,0.0239707,"milarity (STS), and Cross-lingual Document Classification (CLDC). 4.1 Data Monolingual Data: For the monolingual English model, the training set consists of 700K sentences derived from various resources. We extract and combine the following sets: a random set of 150K sentences from LDC’s English Gigaword fifth edition (Parker et al., 2011), a random set of 150K sentences from the English Wikipedia2 , the Brown Corpus (Francis, 1964), Wordnet (Miller, 1995) and Wiktionary3 definitions appended with examples. Bilingual Data: We extract training data for the bilingual models from WMT13 (Mach´acˇ ek and Bojar, 2013) sentence-aligned parallel corpora, specifically version 7 of the EuroParl parallel corpus (Koehn, 2005), the multiUN parallel corpus (Eisele 2 3 http://en.wikipedia.org http://www.wiktionary.org and Chen, 2010), and news commentary data for two language pairs: English-Spanish (en-es) and English-German (en-de). We train each bilingual model using a sample of 1M sentence pairs from these datasets. All sentences in our data are tokenized and stemmed, and number sequences are replaced with a special token as a normalization step. We use the Stanford CoreNLP toolkit (Manning et al., 2014) for Eng"
W16-1201,P14-5010,0,0.00325756,"(Mach´acˇ ek and Bojar, 2013) sentence-aligned parallel corpora, specifically version 7 of the EuroParl parallel corpus (Koehn, 2005), the multiUN parallel corpus (Eisele 2 3 http://en.wikipedia.org http://www.wiktionary.org and Chen, 2010), and news commentary data for two language pairs: English-Spanish (en-es) and English-German (en-de). We train each bilingual model using a sample of 1M sentence pairs from these datasets. All sentences in our data are tokenized and stemmed, and number sequences are replaced with a special token as a normalization step. We use the Stanford CoreNLP toolkit (Manning et al., 2014) for English preprocessing, and Treetagger tools (Schmid, 1995) for both Spanish and German data. Words that appear less than 5 times in the training set are discarded from the vocabulary. The final vocabulary sizes for each model are shown below: Monolingual English English from the en-es set English from the en-de set Spanish German 4.2 55,881 25,057 21,879 30,411 57,431 Parameter settings for empirical tasks Using English as a Pivot: Cross-lingual STS Validation One of the advantages of the CMF model is that it can be readily used to learn representations for several languages. We test this"
W16-1201,D14-1162,0,0.103869,"Missing"
W16-1201,W15-1512,0,0.507422,"pervised or semi-supervised manner. Distributed representations of words, or word embeddings, can be learned using global word co-occurrence statistics as in matrix factorization models (Guo and Diab, 2012; Pennington et al., 2014), or using local context as in neural probabilistic language models (Bengio et al., 2003; Collobert and Weston, 2008; Socher A number of models have recently been proposed for learning cross-lingual compositional representations (Klementiev et al., 2012; Shi et al., 2015; Pennington et al., 2014; Cavallanti et al., 2010; Mikolov et al., 2013; Coulmance et al., 2015; Pham et al., 2015). We propose a relatively simple and nuanced model inspired by the monolingual weighted matrix factorization (WMF) model proposed in (Guo and Diab, 2012), which we extend to the cross-lingual setting. The WMF model learns word representations by decomposing a sparse tf-idf matrix into two lowrank factor matrices representing words and sen1 Proceedings of the Workshop on Multilingual and Cross-lingual Methods in NLP, pages 1–9, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics tences. The weights are adjusted to reflect the confidence levels in reconstructin"
W16-1201,P15-2093,0,0.516459,"integrated with various NLP applications as they can be easily learned, processed, and compared, often in an unsupervised or semi-supervised manner. Distributed representations of words, or word embeddings, can be learned using global word co-occurrence statistics as in matrix factorization models (Guo and Diab, 2012; Pennington et al., 2014), or using local context as in neural probabilistic language models (Bengio et al., 2003; Collobert and Weston, 2008; Socher A number of models have recently been proposed for learning cross-lingual compositional representations (Klementiev et al., 2012; Shi et al., 2015; Pennington et al., 2014; Cavallanti et al., 2010; Mikolov et al., 2013; Coulmance et al., 2015; Pham et al., 2015). We propose a relatively simple and nuanced model inspired by the monolingual weighted matrix factorization (WMF) model proposed in (Guo and Diab, 2012), which we extend to the cross-lingual setting. The WMF model learns word representations by decomposing a sparse tf-idf matrix into two lowrank factor matrices representing words and sen1 Proceedings of the Workshop on Multilingual and Cross-lingual Methods in NLP, pages 1–9, c San Diego, California, June 17, 2016. 2016 Associat"
W16-1201,P13-1045,0,0.126454,"Missing"
W16-1710,P13-2142,0,0.015633,"g of President Mohamed Morsi Jun. 30 - Jul. 6, 2013 9. Army calls for mandate to crack down on terrorism Jul. 24 - Jul. 30, 2013 10. Rabia (Pro-Muslim Brotherhood) camp dismantling Aug. 14 - Aug. 20, 2013 Table 1: List of events and their associated dates for which the data was selected. to uncover the different underlying elements of a person’s belief system. Most of the currently available datasets that are annotated for Ideological Perspective are in English (Lin et al., 2006; Somasundaran and Wiebe, 2010; Abu-Jbara et al., 2012; Yano et al., 2010; Elfardy et al., 2015; Hasan and NG, 2012; Hasan and Ng, 2013). The only Arabic Ideological Perspective datasets that we are aware of are those of Abu-Jbara et al. (2013), Siegel (2014) and Borge-Holthoefer et al. (2015). AbuJbara et al. (2013)’s dataset is self annotated, and the annotations are more abstract –only provide the binary stance of each post toward the debate question. Siegel (2014) study whether Egyptian twitter users who are exposed to a more diverse twitter network become more tolerant toward people having different political and ideological leanings. On the other hand, Borge-Holthoefer et al. (2015) collect Arabic tweets posted between J"
W16-1710,W06-2915,0,0.0210852,"residential election results Jun. 24 - Jun. 30, 2012 7. Presidential decree and associated protests Nov. 22 - Nov. 28, 2012 8. Ousting of President Mohamed Morsi Jun. 30 - Jul. 6, 2013 9. Army calls for mandate to crack down on terrorism Jul. 24 - Jul. 30, 2013 10. Rabia (Pro-Muslim Brotherhood) camp dismantling Aug. 14 - Aug. 20, 2013 Table 1: List of events and their associated dates for which the data was selected. to uncover the different underlying elements of a person’s belief system. Most of the currently available datasets that are annotated for Ideological Perspective are in English (Lin et al., 2006; Somasundaran and Wiebe, 2010; Abu-Jbara et al., 2012; Yano et al., 2010; Elfardy et al., 2015; Hasan and NG, 2012; Hasan and Ng, 2013). The only Arabic Ideological Perspective datasets that we are aware of are those of Abu-Jbara et al. (2013), Siegel (2014) and Borge-Holthoefer et al. (2015). AbuJbara et al. (2013)’s dataset is self annotated, and the annotations are more abstract –only provide the binary stance of each post toward the debate question. Siegel (2014) study whether Egyptian twitter users who are exposed to a more diverse twitter network become more tolerant toward people havin"
W16-1710,W10-0214,0,0.0262589,"on results Jun. 24 - Jun. 30, 2012 7. Presidential decree and associated protests Nov. 22 - Nov. 28, 2012 8. Ousting of President Mohamed Morsi Jun. 30 - Jul. 6, 2013 9. Army calls for mandate to crack down on terrorism Jul. 24 - Jul. 30, 2013 10. Rabia (Pro-Muslim Brotherhood) camp dismantling Aug. 14 - Aug. 20, 2013 Table 1: List of events and their associated dates for which the data was selected. to uncover the different underlying elements of a person’s belief system. Most of the currently available datasets that are annotated for Ideological Perspective are in English (Lin et al., 2006; Somasundaran and Wiebe, 2010; Abu-Jbara et al., 2012; Yano et al., 2010; Elfardy et al., 2015; Hasan and NG, 2012; Hasan and Ng, 2013). The only Arabic Ideological Perspective datasets that we are aware of are those of Abu-Jbara et al. (2013), Siegel (2014) and Borge-Holthoefer et al. (2015). AbuJbara et al. (2013)’s dataset is self annotated, and the annotations are more abstract –only provide the binary stance of each post toward the debate question. Siegel (2014) study whether Egyptian twitter users who are exposed to a more diverse twitter network become more tolerant toward people having different political and ideo"
W16-1710,W10-0723,0,0.046381,"Missing"
W16-1710,P12-1042,1,0.819895,"2012 7. Presidential decree and associated protests Nov. 22 - Nov. 28, 2012 8. Ousting of President Mohamed Morsi Jun. 30 - Jul. 6, 2013 9. Army calls for mandate to crack down on terrorism Jul. 24 - Jul. 30, 2013 10. Rabia (Pro-Muslim Brotherhood) camp dismantling Aug. 14 - Aug. 20, 2013 Table 1: List of events and their associated dates for which the data was selected. to uncover the different underlying elements of a person’s belief system. Most of the currently available datasets that are annotated for Ideological Perspective are in English (Lin et al., 2006; Somasundaran and Wiebe, 2010; Abu-Jbara et al., 2012; Yano et al., 2010; Elfardy et al., 2015; Hasan and NG, 2012; Hasan and Ng, 2013). The only Arabic Ideological Perspective datasets that we are aware of are those of Abu-Jbara et al. (2013), Siegel (2014) and Borge-Holthoefer et al. (2015). AbuJbara et al. (2013)’s dataset is self annotated, and the annotations are more abstract –only provide the binary stance of each post toward the debate question. Siegel (2014) study whether Egyptian twitter users who are exposed to a more diverse twitter network become more tolerant toward people having different political and ideological leanings. On the"
W16-1710,P13-2144,1,0.853512,"Jul. 24 - Jul. 30, 2013 10. Rabia (Pro-Muslim Brotherhood) camp dismantling Aug. 14 - Aug. 20, 2013 Table 1: List of events and their associated dates for which the data was selected. to uncover the different underlying elements of a person’s belief system. Most of the currently available datasets that are annotated for Ideological Perspective are in English (Lin et al., 2006; Somasundaran and Wiebe, 2010; Abu-Jbara et al., 2012; Yano et al., 2010; Elfardy et al., 2015; Hasan and NG, 2012; Hasan and Ng, 2013). The only Arabic Ideological Perspective datasets that we are aware of are those of Abu-Jbara et al. (2013), Siegel (2014) and Borge-Holthoefer et al. (2015). AbuJbara et al. (2013)’s dataset is self annotated, and the annotations are more abstract –only provide the binary stance of each post toward the debate question. Siegel (2014) study whether Egyptian twitter users who are exposed to a more diverse twitter network become more tolerant toward people having different political and ideological leanings. On the other hand, Borge-Holthoefer et al. (2015) collect Arabic tweets posted between June and September 2013. They manually annotate a subset of 1000 tweets as either supporting, opposing or bei"
W16-1710,S15-1015,1,0.763298,"protests Nov. 22 - Nov. 28, 2012 8. Ousting of President Mohamed Morsi Jun. 30 - Jul. 6, 2013 9. Army calls for mandate to crack down on terrorism Jul. 24 - Jul. 30, 2013 10. Rabia (Pro-Muslim Brotherhood) camp dismantling Aug. 14 - Aug. 20, 2013 Table 1: List of events and their associated dates for which the data was selected. to uncover the different underlying elements of a person’s belief system. Most of the currently available datasets that are annotated for Ideological Perspective are in English (Lin et al., 2006; Somasundaran and Wiebe, 2010; Abu-Jbara et al., 2012; Yano et al., 2010; Elfardy et al., 2015; Hasan and NG, 2012; Hasan and Ng, 2013). The only Arabic Ideological Perspective datasets that we are aware of are those of Abu-Jbara et al. (2013), Siegel (2014) and Borge-Holthoefer et al. (2015). AbuJbara et al. (2013)’s dataset is self annotated, and the annotations are more abstract –only provide the binary stance of each post toward the debate question. Siegel (2014) study whether Egyptian twitter users who are exposed to a more diverse twitter network become more tolerant toward people having different political and ideological leanings. On the other hand, Borge-Holthoefer et al. (201"
W16-1710,N09-1057,0,0.0135102,"concept of “Framing”. Framing involves making some topics –or some aspects of the discussed topics– more prominent in order to promote the views and interpretations of the writer (communicator). (Entman, 1993). At the most basic level, these decisions are expressed in lexical choice. For example, a person who opposes gun rights is more likely to use words that emphasize “death” while a supporter is more likely to use ones that promote “self defense”. As the saying goes, “One man’s terrorist is another man’s freedom fighter”. Perspective is also expressed on the syntactic and semantic levels. Greene and Resnik (2009) showed that the syntactic structure can be a strong indicator of a specific perspective, or bias. For example, using the passive voice puts less emphasis on the doer than using an active one. This is particularly important when the verb is sentiment bearing. In such case, the passive voice is less likely to associate the sentiment with the doer. Sentiment in itself serves as another important cue for identifying a person’s perspective since it expresses one’s opinion on different topics. In fact, from a computational point of view, the work on perspectivedetection is closely related to subjec"
W16-1710,C12-2045,0,0.0152606,". 28, 2012 8. Ousting of President Mohamed Morsi Jun. 30 - Jul. 6, 2013 9. Army calls for mandate to crack down on terrorism Jul. 24 - Jul. 30, 2013 10. Rabia (Pro-Muslim Brotherhood) camp dismantling Aug. 14 - Aug. 20, 2013 Table 1: List of events and their associated dates for which the data was selected. to uncover the different underlying elements of a person’s belief system. Most of the currently available datasets that are annotated for Ideological Perspective are in English (Lin et al., 2006; Somasundaran and Wiebe, 2010; Abu-Jbara et al., 2012; Yano et al., 2010; Elfardy et al., 2015; Hasan and NG, 2012; Hasan and Ng, 2013). The only Arabic Ideological Perspective datasets that we are aware of are those of Abu-Jbara et al. (2013), Siegel (2014) and Borge-Holthoefer et al. (2015). AbuJbara et al. (2013)’s dataset is self annotated, and the annotations are more abstract –only provide the binary stance of each post toward the debate question. Siegel (2014) study whether Egyptian twitter users who are exposed to a more diverse twitter network become more tolerant toward people having different political and ideological leanings. On the other hand, Borge-Holthoefer et al. (2015) collect Arabic tw"
W16-4115,J11-4004,0,0.0319415,"ding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity There are many studies that evaluate the language complexity in addition to the quality of manual annotation and also allow the identification of many factors causing lower inter-annotator agreements. For example, Bayerl and Paul (2011) showed that there is a correlation between the inter-annotator agreement and the complexity of the annotation task; for instance, the larger the number of categories is, the lower the inter-annotator agreement is. Moreover, the categories prone to confusions are generally limited. This brings out two complexity issues related to the number of categories and to the existence of ambiguity between some the categories as explained in (Popescu-Belis, 2007). Furthermore, there are some annotation tasks for which the choice of a label is entirely left to the annotator, which can lead to even more co"
W16-4115,D15-1274,0,0.0235161,"ng ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation (OptDiac) project (Zaghouani et al., 2016a; Bouamor et al., 2015). The o"
W16-4115,W15-3209,1,0.748269,"5; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation (OptDiac) project (Zaghouani et al., 2016a; Bouamor et al., 2015). The overreaching goal of our project is to manually create a large-scale annotated corpus with the diacritics for a variety of Arabic texts. The creation of manually annotated corpora presents many challenges and issues related to the linguistic complexity of the Arabic language. In order to streamline the annotation process, we designed various annotation experimental conditions in order to answer the following questions: Can we automatically detect linguistic difficulties such as linguistic ambiguity? To what extent is there agreement between machines and human annotators when it comes to"
W16-4115,2007.mtsummit-papers.20,1,0.914733,"owels and diacritics rendering a mostly consonantal orthography (Schulz, 2004). Arabic diacritization is an orthographic way to describe Arabic word pronunciation, and avoid word reading ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents"
W16-4115,palmer-etal-2008-pilot,1,0.782763,"raining of the annotators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2."
W16-4115,2012.eamt-1.18,0,0.135091,"e analyses contain more than one possibility, the word is marked as ambiguous; otherwise, it is believed to be not ambiguous. Words that have no analysis generated using MADAMIRA are also considered ambiguous. For each sentence, we count the number of words that are marked as ambiguous using our approach, and then calculate the percentage of ambiguity. We sort the sentences according to their ambiguity percentages in descending order so that we give annotators ranked sentences for annotation. Because we are concerned with MSA dataset only, we further filter out dialectal sentences using AIDA (Elfardy and Diab, 2012), a tool that classifies words and sentences as MSA (formal Arabic) or DA (Dialectal Arabic). 5 Evaluation For the evaluation, we used a sample of 10K-Words from the CCA corpus representing 4 domains with approximately 2.5K-words per domain (children stories, economics,sports and politics). We have three experimental conditions for three evaluations carried over a period of six weeks. 1. The first condition (COND1): In the first experimental condition (COND1), four annotators were given raw undiacritized sentences and were asked to add the missing diacritics as per the guidelines. They either"
W16-4115,W07-1522,0,0.0406205,"l reasons that may cause disagreement in annotation decisions including human errors, lack of precision in the guidelines, and the lack of expertise and training of the annotators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally as"
W16-4115,W11-0405,0,0.0272132,"in annotation decisions including human errors, lack of precision in the guidelines, and the lack of expertise and training of the annotators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous ca"
W16-4115,maamouri-etal-2010-speech,1,0.827854,"ral ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity There are many studies that evaluate the language complexity in addition"
W16-4115,W12-2015,1,0.900794,"Missing"
W16-4115,W14-3605,1,0.871251,"Missing"
W16-4115,P06-1031,0,0.094128,"Missing"
W16-4115,I13-2001,1,0.828094,"or: the Shadda gemination mark, the Soukoun (absence of a vowel) and the Nunation marks at the end of a word. Moreover, in some cases, the letters followed by a long Alif  letter @, should not be diacritized as it is considered a deterministic diacritization as in AJJ Ó /miyvAqu/  ’Treaty’ and not AJJ Ó /miyvaAqu/.2 A summary of the most common Arabic diacritization rules is also added as a reference in the guidelines. 3.2 Annotation Tool We designed and implemented MANDIAC, a web-based annotation tool and a work-flow management interface (Obeid et al., 2016), the tool is based on QAWI (Obeid et al., 2013) a token-based editor, used to annotate and correct spelling errors in Arabic text for the Qatar Arabic Language Bank (QALB) project.3 The basic interface of the annotation tool is shown in Figure 1, apart from the surface controls, the interface allows annotators to select from an automatically generated diacritized words list and/or edit words manually as shown. The annotation interface allows users to undo/redo actions, and the history is kept over multiple sessions. The interface includes a timer to keep track of how long each sentence annotation has taken. We used the timer feature to mea"
W16-4115,pasha-etal-2014-madamira,1,0.873559,"Missing"
W16-4115,W10-1004,0,0.0710577,"Missing"
W16-4115,W15-3204,1,0.861846,"Missing"
W16-4115,D15-1152,0,0.0130569,"abic word pronunciation, and avoid word reading ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation (OptDiac) project (Zagh"
W16-4115,W10-1836,1,0.835215,"tators. This disagreement rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity"
W16-4115,W12-2511,1,0.86205,"t rate further increases due to the inherent natural ambiguity in the human language itself where various interpretations for a word are possible. Such linguistic ambiguity has been reported in many annotation projects involving various linguistic phenomenon, such as the coreference relations, the predicate-argument structure, the semantic roles and the L2 language errors (Versley and Tbingen, 2006; Iida et al., 2007), prosodic breaks (Jung and Kwon, 2011; Ruppenhofer et al., 2013; Rosen et al., 2013), as well as the various Arabic PropBank projects (Diab et al., 2008; Zaghouani et al., 2010; Zaghouani et al., 2012) and the Arabic TreeBank (Maamouri et al., 2010). Poesio and Artstein (2005) classify ambiguity into explicit and implicit types. The explicit ambiguity refers to the individuals’ understanding of the annotation task. On the other hand, implicit ambiguity refers to those revealed after observing and contrasting the annotation done in the same task by other annotators. Annotators are generally asked to detect and resolve ambiguous cases, which can be a difficult task to accomplish. This leads to a lower inter-annotator agreement in such tasks. 2.3 Annotation Complexity There are many studies th"
W16-4115,L16-1577,1,0.81745,"Missing"
W16-4115,L16-1295,1,0.886376,"Missing"
W16-4115,P06-1073,0,0.0303502,"hic way to describe Arabic word pronunciation, and avoid word reading ambiguity. In Arabic, diacritics are marks that reflect the phonological, morphological and grammatical rules. The lack of diacritics leads usually to considerable lexical and morphological ambiguity. Full diacritization has been shown to improve state-of-the-art Arabic automatic systems such as automatic speech recognition (ASR) systems (Kirchhoff and Vergyri, 2005) and statistical machine translation (SMT) (Diab et al., 2007). Hence, diacritization has been receiving increased attention in several Arabic NLP applications (Zitouni et al., 2006; Shahrour et al., 2015; Abandah et al., 2015; Belinkov and Glass, 2015). Building models to assign diacritics to each letter in a word requires a large amount of annotated training corpora covering different topics and domains to overcome the sparseness problem. The currently available MSA diacritized corpora are generally limited to religious texts such as the Holy Quran, educational texts or newswire stories distributed by the Linguistic Data Consortium. This paper presents a work carried out within a project to create an optimal diacritization scheme for Arabic orthographic representation"
W16-4804,W15-5413,0,0.315352,"Missing"
W16-4804,W14-5316,0,0.0856605,"Missing"
W16-4804,W15-5407,0,0.0423266,"lated language identification rely heavily on word and character ngram representations. Other features include the use of blacklists and whitelists, language models, POS tag distributions and language-specific orthographical conventions (Bali, 2006; Zampieri and Gebre, 2012). For systems, a wide range of machine learning algorithms have been applied (Naive Bayes and SVM classifiers in particular), with work on optimization and dimensionality reduction (Goutte et al., 2014), and on ensembling and cascading, which yielded the best-performing systems in the 2015 edition (Goutte and L´eger, 2015; Malmasi and Dras, 2015). Previous approaches for Arabic dialect detection, a new task introduced in this shared task edition, use similar approaches. Sadat et al. (2014) argue that character n-gram models are well suited for dialect identification tasks because most of the variation is based on affixation, which can be easily modeled at the character level. Also new to this edition of the shared task is the evaluation on social media data. In 2014, the TweetLID shared task specifically addressed the problem of language identification in very short texts (Zubiaga et al., 2014). This brought to light some of the chall"
W16-4804,W16-4801,0,0.0670715,"Missing"
W16-4804,W14-3601,0,0.0219094,"tal of 7619) • Test data: ASR transcripts containing 315 EGY, 256 GLF, 344 LAV, 274 MSA, and 351 NOR instances (total of 1540) External datasets For the open submission, we used dialect dictionaries to make in-vocabulary frequency count features (as explained in 3). For MSA, we used the Arabic Gigaword vocabulary, whereas for other dialects we built dictionaries based on data collected from Twitter. We are aware that using social media data invariably introduces noise, both in terms of misspelled vocabulary entries and with relation to incorrect geographical information. However, as argued by Mubarak and Darwish (2014), such information still provides acceptable dialectal corpora. We filtered the collected tweets based on the countries of interest that map to the targeted dialects of the shared task (e.g. Syria → LAV ). Before creating the dictionaries, we apply normalization (hamza normalization, emoji and URL removal, . . . ). The resulting dictionary sizes were 76,721 for GLF, 22,003 for EGY, 10,000 for LAV, 286,559 for MSA and 6,343 for NOR. 5.2 Preprocessing We tested applying letter normalization during the train/dev phase, where we normalized the different shapes of hamza (0 , |, >, &, &lt;, ,) to Alif"
W16-4804,W14-5904,0,0.062552,"anguage models, POS tag distributions and language-specific orthographical conventions (Bali, 2006; Zampieri and Gebre, 2012). For systems, a wide range of machine learning algorithms have been applied (Naive Bayes and SVM classifiers in particular), with work on optimization and dimensionality reduction (Goutte et al., 2014), and on ensembling and cascading, which yielded the best-performing systems in the 2015 edition (Goutte and L´eger, 2015; Malmasi and Dras, 2015). Previous approaches for Arabic dialect detection, a new task introduced in this shared task edition, use similar approaches. Sadat et al. (2014) argue that character n-gram models are well suited for dialect identification tasks because most of the variation is based on affixation, which can be easily modeled at the character level. Also new to this edition of the shared task is the evaluation on social media data. In 2014, the TweetLID shared task specifically addressed the problem of language identification in very short texts (Zubiaga et al., 2014). This brought to light some of the challenges inherent to the genre: a need for a better external resources to train systems, low accuracy on underrepresented languages and the inability"
W16-4804,J14-1006,0,0.120827,"Missing"
W16-4804,W14-5307,0,0.161732,"Missing"
W16-4804,W15-5401,0,0.116026,"Missing"
W16-4810,E14-1049,0,0.0419739,"r that the correspondent belongs to. We also use POS tags to skip irrelevant words. This can be done for any language in our lexicon conditioned on the fact that the language has enough monolingual data to induce word clusters. We acknowledge, however, that induced clusters do not necessarily contain exclusively semantically similar synonym words. There might be related and irrelevant words altogether. 2.1.3 Leveraging Cross Lingual Resources Cross-lingual embedding We further incorporate multilingual evidence into monolingual vectorsspace word embeddings. Cross-lingual CCA model proposed by (Faruqui and Dyer, 2014) projects vectors of two different languages into a shared space where they are maximally correlated. Correlation is inferred from an existing bilingual dictionary for the languages. Having projected vectors of a particular language, we expect the synonyms of a word to be found amongst the most similar words in the projected space. Each word is then expanded with the k most similar words acquired from the projected vector-space model. 2.2 Automatic Verification and Augmentation We compare the set of multilingual correspondents acquired in Section 2.1 (T 0 ) with set of correspondents in L. Thi"
W16-4810,P00-1056,0,0.0662142,"eps in order to clean, lemmatize and diacritize the Arabic side of both parallel data sets and render the resources compatible. For the sake of consistency, the lemmatization step is replicated on the English data. The tool we use for processing Arabic is MADAMIRA v1.0 (Pasha et al., 2014), and for English we use TreeTagger (Schmid, 1995). Hence, all the entries in our resources are rendered in lemma form, with the Arabic components being additionally fully diacritized. 3.2 Data Processing The lemmatized-diacritized corpora with the corresponding EN translations are word aligned using GIZA++ (Och and Ney, 2000) producing pairwise EGY-EN and MSA-EN lemma word type alignment files, respectively. We intersected the word alignments on the token level to the type level resulting in a cleaner list of lemma word type alignments per parallel corpus. All correspondents in the form of EGY-EN-MSA are extracted from both alignment files by pivoting on the EN correspondent following Eq. 1 and 2. We refer to this set of tuples as TransDict. We obtain monolingual vector space models using word2vec (Mikolov et al., 2013). We use the Skip-gram model to build word vectors of size 300 from EGYmono and MSAmono corpora"
W16-4810,pasha-etal-2014-madamira,1,0.855682,"Missing"
W16-4810,2009.mtsummit-caasl.5,0,0.0741368,"Missing"
W16-5306,R13-1001,1,0.898062,"Missing"
W16-5306,W07-0812,1,0.84659,"Missing"
W16-5306,A00-2013,0,0.253792,"Missing"
W16-5306,W02-0509,0,0.259406,"nt clustering (Korenius et al., 2004), keyphrase extraction (El-Shishtawy and Al-Sammak, 2012), and text indexing and classification (Hammouda and Almarimi, 2010). From a lexical point of view, normalization can be conducted at the level of the root, stem or lemma. Lemmatization relates surface forms to their canonical base representations (or dictionary lookup form) (Attia and van Genabith, 2013). It is the inverse of inflection (Plisson et al., 2004), as it renders words to a default and uninflected form, or as is the case with Arabic, a least marked form. A lemma is the common denominator (Kamir et al., 2002) of a set of forms that share the same semantic, morphological and syntactic composition, where it represents the least marked word form without any inflectional affixes. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 40 Proceedings of the Workshop on Cognitive Aspects of the Lexicon, pages 40–50, Osaka, Japan, December 11-17 2016. License details: http:// In Arabic, a verb lemma is chosen to be the perfective, indicative, 3rd person, masculine, and singular such as Q º $akara1 “to thank”. Whereas a nominal lemma (na"
W16-5306,P16-2090,0,0.0222331,"Missing"
W16-5306,W05-0711,0,0.0438503,"o be trained on a model for vowelization. Vowelization is an important aspect in the Arabic morphological patterns (which are sometimes referred to as vocalic scheme). Our list of 377 unique patterns is reduced to 175 patterns when vowel marks are removed. Automatic vowelization, or diacritic restoration, has been discussed in a number of papers. For example, Bebah et al. (2014) describe a hybrid method for automatic vowelization using the Al-Khalil morphological analyzer and a hidden Markov model (HMM) to disambiguate. Some researchers use purely statistical methods for restoring diacritics (Nelken and Shieber, 2005; Elshafei et al., 2006; Ameur et al., 2015; Rashwan et al., 2011). 2 Related Work Lemmatization has been discussed for morphologically rich languages, such as Setswana (Brits et al., 2005), Croatian (Tadi´c, 2006), Slovene, Serbian, Hungarian, Estonian, Bulgarian and Romanian (in addition to other languages) (Juršiˇc et al., 2007), French (Seddah et al., 2010), Portuguese (da Silva, 2007), Finnish (Korenius et al., 2004), Turkish (Ozturkmenoglu and Alpkocak, 2012) and even English (Balakrishnan and Lloyd-Yemoh, 2014). Plisson et al. (2004) and Juršiˇc et al. (2007) treat lemmatization as a ma"
W16-5306,P08-2030,1,0.808025,"on sets of example pairs (stem and inflected form) with their feature vectors. El-Shishtawy and El-Ghannam (2012) build a rule-based system that exploits Arabic language knowledge in terms of roots, patterns, affixes, and a set of morpho-syntactic rules to generate lemmas for surface word forms. Hybrid normalization. (Hajiˇc, 2000) argues for the use of a dictionary as a source of morphological analyses for training a statistical POS and morphological tagger for inflectionally rich languages, such as Romanian, Czech, or Hungarian. The method was later applied to Arabic (Hajic et al., 2005). (Roth et al., 2008) develop a system (MADA) that uses statistical methods (SVM classifiers) to perform full morpho-syntactic tagging, along with lemmatization (LexChoice), by selecting the best candidate from the list of competing analyses generated by BAMA (Buckwalter, 2004). Statistical normalization. The Stanford Tagger (Toutanova and Manning, 2000) is a Maximum Entropy POS tagger that has been extended for Arabic, but the problem with this tagger is that it does not perform segmentation of Arabic clitics. AMIRA 2.1 (Diab, 2007; Diab, 2009) uses a supervised SVM-based machine learning method for POS tagging,"
W16-5306,W10-1410,0,0.0432904,"Missing"
W16-5306,2006.jeptalnrecital-long.29,0,0.0682393,"eir function as an instrument for decomposing word forms. Roots and patterns are the hidden layers through which Arabic speakers organize, memorize and access the Arabic lexicon. In many NLP tasks, using surface word forms is found to be inefficient as it significantly adds to sparsity, especially in highly inflected languages; thus, some form of normalization is necessary. Normalization in general, and lemmatization in particular, are meant to reduce the variability in word forms by collapsing related words. This has been shown to be beneficial for information retrieval (Larkey et al., 2002; Semmar et al., 2006), parsing (Seddah et al., 2010), summarization (Skorkovská, 2012; ElShishtawy and El-Ghannam, 2014), document clustering (Korenius et al., 2004), keyphrase extraction (El-Shishtawy and Al-Sammak, 2012), and text indexing and classification (Hammouda and Almarimi, 2010). From a lexical point of view, normalization can be conducted at the level of the root, stem or lemma. Lemmatization relates surface forms to their canonical base representations (or dictionary lookup form) (Attia and van Genabith, 2013). It is the inverse of inflection (Plisson et al., 2004), as it renders words to a default an"
W16-5306,W00-1308,0,0.279852,"Missing"
W16-5414,I13-1047,0,0.0290106,"entation of MWEs. The goal was to integrate system that can morphologically process Hebrew multiword expressions of various types, in spite of the complexity of Hebrew morphology and orthography. Zaninello and Nissim (2010) present three electronic lexical resources for Italian MWE. They created a series of example corpora and a database of MWE modeled around morphosyntactic patterns. Nissim and Zaninello (2013) employed variation patterns to deal with morphological variation in order to create a lexicon and a repository of variation patterns for MWE in morphologically-rich Romance languages. Al-Sabbagh et al. (2013) describe the construction of a lexicon of Arabic Modal Multiword Expressions and a repository for their variation patterns. They used an unsupervised approach to build a lexicon for Arabic Modal Multiword Expressions and a repository for their variation patterns. The lexicon contains 10,664 entries of MSA and Egyptian modal MWE and collocation, linked to the repository. The closest work to ours is that of (Hawwari et al., 2012). They created a list of different types of Arabic MWE collected from various dictionaries which were manually annotated and grouped based on their syntactic type. The"
W16-5414,W03-1812,0,0.0127884,"mething/somebody) Table 1: Example for the entities we are considering within a MWE The main objective of our work is to automatically acquire all observed morphological variants and syntactic pattern alternations of a MWE using large scale corpora, using an empirical method to identify the morphological and syntactic flexibility of AVMWE. 2 Related Work A considerable amount of literature has been published on the morphosyntactic characteristics of MWE . These studies focused on various morphological aspects, within different contexts on different languages. Gurrutxaga and Alegria (2012) and Baldwin et al. (2003) applied latent semantic analysis to build a model of multiword expression decomposability. This model measures the similarity between a multiword expression and its elements words, and considers the constructions with higher similarities are greater decomposability. Diab and Bhutada (2009) present a supervised learning approach to classify the idiomaticity of the Verb-Noun Constructions (VNC) depending on context in running text. Savary (2008) presents a comparative survey of eleven lexical representation approaches to the inflectional aspects in MWE in different languages, including English,"
W16-5414,calzolari-etal-2002-towards,0,0.089054,"an exhibit flexibility especially in verbal MWE. Such morphosyntactic flexibility increases difficulties in computational processing of MWE as they are harder to detect. Characterizing the internal structure of MWE is considered very important for many natural language processing tasks such as syntactic parsing and applications such as machine translation (Ghoneim and Diab, 2013; Carpuat and Diab, 2010). In lexicography, entries for MWE in a lexicon should provide a description of the syntactic behavior of the MWE constructions, such as syntactic peculiarities and morphosyntactic constraints (Calzolari et al., 2002). Automatically identifying the syntactic patterns and listing/detecting their possible variations would help in lexicographic representation of MWE, as the manual annotation of MWE variants suffer from many disadvantages such as time and effort consuming, subjectivity and limited coverage. The problem is exacerbated for morphologically rich languages, where an average word could have up to 12 morphological analyses such as the case for the Arabic language which is highly inflectional. This work is licenced under a Creative Commons Attribution 4.0 International Licence. http://creativecommons."
W16-5414,N10-1029,1,0.83869,"1 Introduction Multiword expressions (MWE) are complex lexemes that contain at least two words reflecting a single concept. They can be morphologically and syntactically fixed expressions but also we note that they can exhibit flexibility especially in verbal MWE. Such morphosyntactic flexibility increases difficulties in computational processing of MWE as they are harder to detect. Characterizing the internal structure of MWE is considered very important for many natural language processing tasks such as syntactic parsing and applications such as machine translation (Ghoneim and Diab, 2013; Carpuat and Diab, 2010). In lexicography, entries for MWE in a lexicon should provide a description of the syntactic behavior of the MWE constructions, such as syntactic peculiarities and morphosyntactic constraints (Calzolari et al., 2002). Automatically identifying the syntactic patterns and listing/detecting their possible variations would help in lexicographic representation of MWE, as the manual annotation of MWE variants suffer from many disadvantages such as time and effort consuming, subjectivity and limited coverage. The problem is exacerbated for morphologically rich languages, where an average word could"
W16-5414,W09-2903,1,0.827173,"he morphological and syntactic flexibility of AVMWE. 2 Related Work A considerable amount of literature has been published on the morphosyntactic characteristics of MWE . These studies focused on various morphological aspects, within different contexts on different languages. Gurrutxaga and Alegria (2012) and Baldwin et al. (2003) applied latent semantic analysis to build a model of multiword expression decomposability. This model measures the similarity between a multiword expression and its elements words, and considers the constructions with higher similarities are greater decomposability. Diab and Bhutada (2009) present a supervised learning approach to classify the idiomaticity of the Verb-Noun Constructions (VNC) depending on context in running text. Savary (2008) presents a comparative survey of eleven lexical representation approaches to the inflectional aspects in MWE in different languages, including English, French, Polish Serbian German, Turkish and Basque. Al-Haj et al. (2013) applied to Modern Hebrew an architecture for lexical representation of MWEs. The goal was to integrate system that can morphologically process Hebrew multiword expressions of various types, in spite of the complexity o"
W16-5414,I13-1168,1,0.901075,"Missing"
W16-5414,gurrutxaga-alegria-2012-measuring,0,0.021074,"g) Syntactic variable (FulAnK) (something/somebody) Table 1: Example for the entities we are considering within a MWE The main objective of our work is to automatically acquire all observed morphological variants and syntactic pattern alternations of a MWE using large scale corpora, using an empirical method to identify the morphological and syntactic flexibility of AVMWE. 2 Related Work A considerable amount of literature has been published on the morphosyntactic characteristics of MWE . These studies focused on various morphological aspects, within different contexts on different languages. Gurrutxaga and Alegria (2012) and Baldwin et al. (2003) applied latent semantic analysis to build a model of multiword expression decomposability. This model measures the similarity between a multiword expression and its elements words, and considers the constructions with higher similarities are greater decomposability. Diab and Bhutada (2009) present a supervised learning approach to classify the idiomaticity of the Verb-Noun Constructions (VNC) depending on context in running text. Savary (2008) presents a comparative survey of eleven lexical representation approaches to the inflectional aspects in MWE in different lan"
W16-5414,W12-3403,1,0.809156,"patterns to deal with morphological variation in order to create a lexicon and a repository of variation patterns for MWE in morphologically-rich Romance languages. Al-Sabbagh et al. (2013) describe the construction of a lexicon of Arabic Modal Multiword Expressions and a repository for their variation patterns. They used an unsupervised approach to build a lexicon for Arabic Modal Multiword Expressions and a repository for their variation patterns. The lexicon contains 10,664 entries of MSA and Egyptian modal MWE and collocation, linked to the repository. The closest work to ours is that of (Hawwari et al., 2012). They created a list of different types of Arabic MWE collected from various dictionaries which were manually annotated and grouped based on their syntactic type. The main goal was to tag a large scale corpus of Arabic text using a pattern1 We use Buckwalter transliteration encoding for Arabic: http://www.qamus.org/transliteration.htm 114 matching algorithm and automatically annotated to enrich and syntactically classify the given MWE list. Their work didn’t approach the derivational or lexical aspects. To the best of our knowledge, to date, none of the previous addressed the systematic inves"
W16-5414,W14-3606,1,0.88371,"Missing"
W16-5414,pasha-etal-2014-madamira,1,0.864945,"Missing"
W16-5414,zaninello-nissim-2010-creation,0,0.0223049,"ch to classify the idiomaticity of the Verb-Noun Constructions (VNC) depending on context in running text. Savary (2008) presents a comparative survey of eleven lexical representation approaches to the inflectional aspects in MWE in different languages, including English, French, Polish Serbian German, Turkish and Basque. Al-Haj et al. (2013) applied to Modern Hebrew an architecture for lexical representation of MWEs. The goal was to integrate system that can morphologically process Hebrew multiword expressions of various types, in spite of the complexity of Hebrew morphology and orthography. Zaninello and Nissim (2010) present three electronic lexical resources for Italian MWE. They created a series of example corpora and a database of MWE modeled around morphosyntactic patterns. Nissim and Zaninello (2013) employed variation patterns to deal with morphological variation in order to create a lexicon and a repository of variation patterns for MWE in morphologically-rich Romance languages. Al-Sabbagh et al. (2013) describe the construction of a lexicon of Arabic Modal Multiword Expressions and a repository for their variation patterns. They used an unsupervised approach to build a lexicon for Arabic Modal Mul"
W16-5805,K15-1005,1,0.864973,"Missing"
W16-5805,L16-1260,0,0.0396233,"ta (Samih and Maier, 2016), a ¨ collection of Turkish-German CS tweets (Ozlem C¸etino˘glu, 2016), a large collection of Modern Standard Arabic and Egyptian Dialectal Arabic CS data (Diab et al., 2016) and a collection of sentiment annotated Spanish-English tweets (Vilares et al., 2016). Other work includes improving word alignment and MT models using CS data (Huang and Yates, 2014), improving OCR in historical documents that contain code-switched text (Garrette et al., 2015), the definition of an objective measure of corpus level complexity of code-switched texts (Gamb¨ack and Das, 2016) and (Begum et al., 2016) presented an annotation scheme for annotating the pragmatic functions of CS in Hindi-English codeswitched tweets. There is still more research to be done involving CS data, and we hope this second edition of the shared task will help motivate further research. we show in parenthesis. 4.1 SPA-ENG For SPA-ENG, we used the training and test corpora from the EMNLP 2014 shared task as this year’s training corpus and development corpus, respectively. However, the previous shared task did not have the same labels we are using this year. We had in-lab annotators follow a simple mapping process: they"
W16-5805,W16-5814,0,0.137774,"ams also participated in the MSA-DA task. There was a wide variety of system architectures ranging from simple rule based systems all the way to more complex machine learning implementations. Most of the systems submitted did not change anything in the implementation to tackle one language pair or the other, which implies that the participants were highly interested in building language independent systems that could be easily scaled to multiple language pairs. In Table 5 we show a summary of the the architectures of the systems submitted by the participants. All teams, with the exception of (Chanda et al., 2016), used some sort of machine learning algorithm in their systems. The algorithm of choice by most participants was the Conditional Random Fields (CRF). This is no surprise since CRFs fit the problem nicely due to the sequence labeling nature of the task as it was evidenced in the high performance by CRFs achieved in the previous shared task. A new addition this year is the use of deep learning algorithms by two of the participants. Deep learning is now much more prevalent in NLP than it was two years ago when the previous shared task was held. (Jaech et al., 2016) used a convolutional neural ne"
W16-5805,L16-1292,0,0.188047,"Missing"
W16-5805,N15-1109,0,0.0305685,"this Egyptian twitter community. We also had contributions of new CS corpora, such as a collection of Arabic-Moroccan Darija social media CS data (Samih and Maier, 2016), a ¨ collection of Turkish-German CS tweets (Ozlem C¸etino˘glu, 2016), a large collection of Modern Standard Arabic and Egyptian Dialectal Arabic CS data (Diab et al., 2016) and a collection of sentiment annotated Spanish-English tweets (Vilares et al., 2016). Other work includes improving word alignment and MT models using CS data (Huang and Yates, 2014), improving OCR in historical documents that contain code-switched text (Garrette et al., 2015), the definition of an objective measure of corpus level complexity of code-switched texts (Gamb¨ack and Das, 2016) and (Begum et al., 2016) presented an annotation scheme for annotating the pragmatic functions of CS in Hindi-English codeswitched tweets. There is still more research to be done involving CS data, and we hope this second edition of the shared task will help motivate further research. we show in parenthesis. 4.1 SPA-ENG For SPA-ENG, we used the training and test corpora from the EMNLP 2014 shared task as this year’s training corpus and development corpus, respectively. However, t"
W16-5805,E14-1001,0,0.0310435,"uial Arabic. The research goal was to describe the code switching phenomena situation found in this Egyptian twitter community. We also had contributions of new CS corpora, such as a collection of Arabic-Moroccan Darija social media CS data (Samih and Maier, 2016), a ¨ collection of Turkish-German CS tweets (Ozlem C¸etino˘glu, 2016), a large collection of Modern Standard Arabic and Egyptian Dialectal Arabic CS data (Diab et al., 2016) and a collection of sentiment annotated Spanish-English tweets (Vilares et al., 2016). Other work includes improving word alignment and MT models using CS data (Huang and Yates, 2014), improving OCR in historical documents that contain code-switched text (Garrette et al., 2015), the definition of an objective measure of corpus level complexity of code-switched texts (Gamb¨ack and Das, 2016) and (Begum et al., 2016) presented an annotation scheme for annotating the pragmatic functions of CS in Hindi-English codeswitched tweets. There is still more research to be done involving CS data, and we hope this second edition of the shared task will help motivate further research. we show in parenthesis. 4.1 SPA-ENG For SPA-ENG, we used the training and test corpora from the EMNLP 2"
W16-5805,W16-5807,0,0.185235,"teams, with the exception of (Chanda et al., 2016), used some sort of machine learning algorithm in their systems. The algorithm of choice by most participants was the Conditional Random Fields (CRF). This is no surprise since CRFs fit the problem nicely due to the sequence labeling nature of the task as it was evidenced in the high performance by CRFs achieved in the previous shared task. A new addition this year is the use of deep learning algorithms by two of the participants. Deep learning is now much more prevalent in NLP than it was two years ago when the previous shared task was held. (Jaech et al., 2016) used a convolutional neural network (CNN) to obtain word vectors which are then fed as a sequence to a bidirectional long short term memory recurrent neural network (LSTM) to map the sequence to a label. The system submitted by (Samih et al., 2016) used the output of a pair of LSTMs along with a CRF and postprocessing to obtain the final label mapping. These systems are perhaps more complex than traditional machine learning algorithms, but the trade off for performance is evident in the results. Most of the participants included some sort of external resource in their system. Among them we ca"
W16-5805,R15-1033,0,0.120597,"Missing"
W16-5805,C82-1023,0,0.832126,"or annotation. In Table 1 we show examples of code-switched tweets that are found in our data. We have posted the annotation guidelines for SPA-ENG, but it can be generalized to the MSA-DA language pair as well. This is possible because we want to have a universal set of annotation labels that can be used to correctly annotate new data with the least amount of error possible. We keep improving the guidelines to accommodate findings from the previous shared task as well as new relevant research. 3 Related Work The earliest work on CS data within the NLP community dates back to research done by Joshi (1982) on an approach to parsing CS data. Following work has been described in the First Shared Task on Language Identification in Code-Switched Data held at EMNLP 2014 (Solorio et al., 2014). Since the first edition of the task, new research has come to light involving CS data. There has been work on language identification of different language pairs in CS text, such as improvements on dialect identification in Arabic (AlBadrashiny et al., 2015) and detection of intraword CS in Dutch and dialect varieties (Nguyen and Cornips, 2016). There has also been work on POS tagging and parsing such as parsi"
W16-5805,W15-3116,0,0.0386905,"ialect identification in Arabic (AlBadrashiny et al., 2015) and detection of intraword CS in Dutch and dialect varieties (Nguyen and Cornips, 2016). There has also been work on POS tagging and parsing such as parsing of bilingual code-switched text (Vilares et al., 2015a), POS tagging of Hindi-English CS social media text (Sequiera et al., 2015; Jamatia et al., 2015) and shallow parsing of Hindi-English CS social media text (Sharma et al., 2016). Another area where there has been some new research work is in sentiment analysis, such as emotion detection in Chinese-English code-switched texts (Lee and Wang, 2015) and sentiment analysis on Spanish-English Twitter posts (Vilares et al., 2015b). (Kosoff, 2014) carried out a sociolinguistic investigation focused on the use of code-switching in the complex speech community of Egyptian Twitter users. It studies the combinations of Modern Standard Arabic(MSA), Egyptian Colloquial Arabic, English, and Arabizi; whether it is a Modern Standard Arabic or Egyptian Colloquial Arabic. The research goal was to describe the code switching phenomena situation found in this Egyptian twitter community. We also had contributions of new CS corpora, such as a collection of"
W16-5805,W16-2013,0,0.0526771,"arliest work on CS data within the NLP community dates back to research done by Joshi (1982) on an approach to parsing CS data. Following work has been described in the First Shared Task on Language Identification in Code-Switched Data held at EMNLP 2014 (Solorio et al., 2014). Since the first edition of the task, new research has come to light involving CS data. There has been work on language identification of different language pairs in CS text, such as improvements on dialect identification in Arabic (AlBadrashiny et al., 2015) and detection of intraword CS in Dutch and dialect varieties (Nguyen and Cornips, 2016). There has also been work on POS tagging and parsing such as parsing of bilingual code-switched text (Vilares et al., 2015a), POS tagging of Hindi-English CS social media text (Sequiera et al., 2015; Jamatia et al., 2015) and shallow parsing of Hindi-English CS social media text (Sharma et al., 2016). Another area where there has been some new research work is in sentiment analysis, such as emotion detection in Chinese-English code-switched texts (Lee and Wang, 2015) and sentiment analysis on Spanish-English Twitter posts (Vilares et al., 2015b). (Kosoff, 2014) carried out a sociolinguistic i"
W16-5805,L16-1667,0,0.125616,"Missing"
W16-5805,L16-1658,0,0.0258703,"tter posts (Vilares et al., 2015b). (Kosoff, 2014) carried out a sociolinguistic investigation focused on the use of code-switching in the complex speech community of Egyptian Twitter users. It studies the combinations of Modern Standard Arabic(MSA), Egyptian Colloquial Arabic, English, and Arabizi; whether it is a Modern Standard Arabic or Egyptian Colloquial Arabic. The research goal was to describe the code switching phenomena situation found in this Egyptian twitter community. We also had contributions of new CS corpora, such as a collection of Arabic-Moroccan Darija social media CS data (Samih and Maier, 2016), a ¨ collection of Turkish-German CS tweets (Ozlem C¸etino˘glu, 2016), a large collection of Modern Standard Arabic and Egyptian Dialectal Arabic CS data (Diab et al., 2016) and a collection of sentiment annotated Spanish-English tweets (Vilares et al., 2016). Other work includes improving word alignment and MT models using CS data (Huang and Yates, 2014), improving OCR in historical documents that contain code-switched text (Garrette et al., 2015), the definition of an objective measure of corpus level complexity of code-switched texts (Gamb¨ack and Das, 2016) and (Begum et al., 2016) presen"
W16-5805,W15-5936,0,0.0808228,"tification in Code-Switched Data held at EMNLP 2014 (Solorio et al., 2014). Since the first edition of the task, new research has come to light involving CS data. There has been work on language identification of different language pairs in CS text, such as improvements on dialect identification in Arabic (AlBadrashiny et al., 2015) and detection of intraword CS in Dutch and dialect varieties (Nguyen and Cornips, 2016). There has also been work on POS tagging and parsing such as parsing of bilingual code-switched text (Vilares et al., 2015a), POS tagging of Hindi-English CS social media text (Sequiera et al., 2015; Jamatia et al., 2015) and shallow parsing of Hindi-English CS social media text (Sharma et al., 2016). Another area where there has been some new research work is in sentiment analysis, such as emotion detection in Chinese-English code-switched texts (Lee and Wang, 2015) and sentiment analysis on Spanish-English Twitter posts (Vilares et al., 2015b). (Kosoff, 2014) carried out a sociolinguistic investigation focused on the use of code-switching in the complex speech community of Egyptian Twitter users. It studies the combinations of Modern Standard Arabic(MSA), Egyptian Colloquial Arabic, En"
W16-5805,N16-1159,0,0.0350113,"he task, new research has come to light involving CS data. There has been work on language identification of different language pairs in CS text, such as improvements on dialect identification in Arabic (AlBadrashiny et al., 2015) and detection of intraword CS in Dutch and dialect varieties (Nguyen and Cornips, 2016). There has also been work on POS tagging and parsing such as parsing of bilingual code-switched text (Vilares et al., 2015a), POS tagging of Hindi-English CS social media text (Sequiera et al., 2015; Jamatia et al., 2015) and shallow parsing of Hindi-English CS social media text (Sharma et al., 2016). Another area where there has been some new research work is in sentiment analysis, such as emotion detection in Chinese-English code-switched texts (Lee and Wang, 2015) and sentiment analysis on Spanish-English Twitter posts (Vilares et al., 2015b). (Kosoff, 2014) carried out a sociolinguistic investigation focused on the use of code-switching in the complex speech community of Egyptian Twitter users. It studies the combinations of Modern Standard Arabic(MSA), Egyptian Colloquial Arabic, English, and Arabizi; whether it is a Modern Standard Arabic or Egyptian Colloquial Arabic. The research"
W16-5805,W16-5816,0,0.0489599,"Missing"
W16-5805,W16-5817,0,0.054738,"Missing"
W16-5805,W14-3907,1,0.730185,"o the MSA-DA language pair as well. This is possible because we want to have a universal set of annotation labels that can be used to correctly annotate new data with the least amount of error possible. We keep improving the guidelines to accommodate findings from the previous shared task as well as new relevant research. 3 Related Work The earliest work on CS data within the NLP community dates back to research done by Joshi (1982) on an approach to parsing CS data. Following work has been described in the First Shared Task on Language Identification in Code-Switched Data held at EMNLP 2014 (Solorio et al., 2014). Since the first edition of the task, new research has come to light involving CS data. There has been work on language identification of different language pairs in CS text, such as improvements on dialect identification in Arabic (AlBadrashiny et al., 2015) and detection of intraword CS in Dutch and dialect varieties (Nguyen and Cornips, 2016). There has also been work on POS tagging and parsing such as parsing of bilingual code-switched text (Vilares et al., 2015a), POS tagging of Hindi-English CS social media text (Sequiera et al., 2015; Jamatia et al., 2015) and shallow parsing of Hindi-"
W16-5805,W15-2902,0,0.148151,"Missing"
W16-5805,L16-1655,0,0.125494,"Missing"
W16-5805,W16-5818,0,0.0581395,"Missing"
W16-5812,K15-1005,1,0.840177,"to carry out our experiments. 100 Figure 1: Graphic representation of the COMB1:LID-MonoLT approach. process to the words in the sentence. The chunks of words identified as lang1 are processed by the monolingual lang1 POS tagger and chunks of words identified as lang2 are processed by the monolingual lang2 POS tagger. Finally we integrate the POS tags from both monolingual taggers creating the POS tag sequence for the sentence. Figure-1 shows a diagram representing this approach for the MSA-EGY language pair. For MSA-EGY, we used the Automatic Identification of Dialectal Arabic (AIDA2) tool (Al-Badrashiny et al., 2015) to perform token level language identification for the EGY and MSA tokens in context. It takes plain Arabic text in Arabic UTF8 encoding or Buckwalter encoding as input and outputs: 1) Class Identification (CI) of the input text to specify whether the tokens are MSA, EGY, as well as other information such as name entity, foreign word, or unknown labels per token. Furthermore, it provides the results with a confidence score; 2)Dialect Classification (DC) of the input text to specify whether it is Egyptian. For SPA-ENG, we trained language models (LM) on English and Spanish data to assign Langu"
W16-5812,N13-1049,0,0.0602647,"Missing"
W16-5812,W14-3902,0,0.166421,"Missing"
W16-5812,W13-2301,0,0.0179214,"notated using the Buckwalter (BW) POS tag set. The BW POS 102 tag set is considered one of the most popular Arabic POS tagsets. It gains its popularity from its use in the Penn Arabic Treebank (PATB) (Maamouri et al., 2004; Alkuhlani et al., 2013). It can be used for tokenized and untokenized Arabic text. The tokenized tags that are used in the PATB are extracted from the untokenized tags. The number of untokenized tags is 485 tags and generated by BAMA (Buckwalter, 2004). Both tokenized and untokenized tags use the same 70 tags and sub-tags such as nominal suffix, ADJ, CONJ, DET, and, NSUFF (Eskander et al., 2013) (Alkuhlani et al., 2013). Combining the sub-tags can form almost 170 morpheme sub-tags such NSUFF FEM SG. This is a very detailed tagset for our purposes and also for cross CS language pair comparison, i.e. in order to compare between trends in the MSA-EGY setting and the SPA-ENG setting. Accordingly, we map the BW tagset which is the output of the MADAMIRA tools to the universal tagset (Petrov et al., 2011). We apply the mapping as follows: 1) Personal, relative, demonstrative, interrogative, and indefinite pronouns are mapped to Pronoun; 2)Acronyms are mapped to Proper Nouns; 3) Complementi"
W16-5812,R15-1033,0,0.132072,"Missing"
W16-5812,J93-2004,0,0.0788385,"rsion of MADAMIRAMSA strictly on pure MSA sentences identified in the EGY Treebank ARZ1-5. Likewise we created a MADAMIRA-EGY tagger trained specifically on the pure EGY sentences extracted from the same ARZ1-5 Treebank.2 For the SPA-ENG language pair we created models using the TreeTagger (Schmid, 1994) monolingual systems for Spanish and English respectively as their performance has been shown to be competitive. Moreover, as pointed out in (Solorio and Liu, 2008) TreeTagger has attractive features for our CS scenario. The data used to train TreeTagger for English was the Penn Treebank data (Marcus et al., 1993), sections 0-22. For the Spanish model, we used Ancora-ES (Taul´e et al., 2008). 3.2 Combined Experimental Conditions COMB1:LID-MonoLT: Language identification followed by monolingual tagging Given a sentence, we apply a token level language identification 2 We are grateful to the MADAMIRA team for providing us with the MADAMIRA training code to carry out our experiments. 100 Figure 1: Graphic representation of the COMB1:LID-MonoLT approach. process to the words in the sentence. The chunks of words identified as lang1 are processed by the monolingual lang1 POS tagger and chunks of words identi"
W16-5812,pasha-etal-2014-madamira,1,0.874077,"Missing"
W16-5812,W15-5936,0,0.176006,"Missing"
W16-5812,D08-1110,1,0.873698,"a relatively pure monolingual tagger per language variety (MSA or EGY), trained on informal genres for both MSA and EGY. Therefore, we retrained a new version of MADAMIRAMSA strictly on pure MSA sentences identified in the EGY Treebank ARZ1-5. Likewise we created a MADAMIRA-EGY tagger trained specifically on the pure EGY sentences extracted from the same ARZ1-5 Treebank.2 For the SPA-ENG language pair we created models using the TreeTagger (Schmid, 1994) monolingual systems for Spanish and English respectively as their performance has been shown to be competitive. Moreover, as pointed out in (Solorio and Liu, 2008) TreeTagger has attractive features for our CS scenario. The data used to train TreeTagger for English was the Penn Treebank data (Marcus et al., 1993), sections 0-22. For the Spanish model, we used Ancora-ES (Taul´e et al., 2008). 3.2 Combined Experimental Conditions COMB1:LID-MonoLT: Language identification followed by monolingual tagging Given a sentence, we apply a token level language identification 2 We are grateful to the MADAMIRA team for providing us with the MADAMIRA training code to carry out our experiments. 100 Figure 1: Graphic representation of the COMB1:LID-MonoLT approach. pro"
W16-5812,taule-etal-2008-ancora,0,0.108198,"Missing"
W16-5812,D14-1105,0,0.331016,"m. Typically people who code switch master two (or more) languages: a common first language (lang1) and another prevalent language as a second language (lang2). The languages could be completely distinct such as Mandarin and English, or Hindi and English, or they can be variants of one another such as in the case of Modern Standard Arabic (MSA) and Arabic regional dialects (e.g. Egyptian dialect– EGY). CS is traditionally prevalent in spoken language but with the proliferation of social media such as Facebook, Instagram, and Twitter, CS is becoming ubiquitous in written modalities and genres (Vyas et al., 2014; Danet and Herring, 2007; C´ardenas-Claros and Isharyanti, 2009) CS can be observed in different linguistic levels of representation for different language pairs: phonological, morphological, lexical, syntactic, semantic, and discourse/pragmatic. It may occur within (intra-sentential) or across utterances (inter-sentential). For example, the following Arabic excerpt exhibits both lexical and syntactic CS. The speaker alternates between two variants of Arabic MSA and EGY. Arabic Intra-sentential CS:1 wlkn AjhztnA AljnA}yp lAnhA m$ xyAl Elmy lm tjd wlw mElwmp wAHdp. English Translation: Since o"
W16-5813,C16-1115,1,0.752448,"using Arabic script. It uses MADAMIRA(Pasha et al., 2014) to find the POS tag, prefix, lemma, suffix, for each word in the input text. Then it models these features together with other features including word level language model probabilities in a series of classifiers where it combines them in a classifier ensemble approach to find the best tag for each word. In this paper we address this challenge using a generic simple language independent approach. We illustrate our approach on both language pair tracks. 2 Approach The presented system in this paper is based on the idea we presented in (Al-Badrashiny and Diab, 2016). It is based on the assumption that each language has its own character pattern behaviors and combinations relating to the underlying phonology, phonetics, and morphology of each language independently. Accordingly, the manner of articulation constrains the possible phonemic/morphemic combinations in a language. Accordingly, we use a supervised learning framework to address the challenge of LCS. We assume the presence of annotated code switched training data where each token is annotated as either Lang1 or Lang2. We create a sequence model using Conditional Random Fields (CRF++) tool(Sha and"
W16-5813,W14-1604,1,0.816847,"his shared task(Molina et al., 2016), the participants are asked to identify the language type of each word in a large set of tweets. The shared task has two language pair tracks; MSA-DA and Spanish• fw: if the word is related to any other language than the targeted language pair • ne: if the word is named entity; • other: if the word is number, punctuation, emoticons, url, date, starts with #, @, or contains underscore; • unk: if can not be determined to by any of the above tags. Relevant work on thhe LCS problem among different language pairs can be summarized in the following work. 3ARRIB (Al-Badrashiny et al., 2014; Eskander et al., 2014) addresses the challenge of how to distinguish between Arabic words written using Roman script (Arabizi) and actual English words in the same context/utterance. The assumption in this framework is the script is Latin for all words. It trains 108 Proceedings of the Second Workshop on Computational Approaches to Code Switching, pages 108–111, c Austin, TX, November 1, 2016. 2016 Association for Computational Linguistics a finite state transducer (FST) to learn the mapping between the Roman form of the Arabizi words and their Arabic form. It uses the resulting FST to find"
W16-5813,K15-1005,1,0.851363,"everal experiments using different approaches including dictionary-based methods, linear kernel SVMs, and a k-nearest neighbor approach. The best setup they found is the SVM-based one that uses character n-gram, binary features indicates whether the word is in a language specific dictionary of the most frequent 5000 words they have constructed, length of the word, previous and next words, 3 boolean features for capitalization to check if the first letter is capitalized, if any letter is capitalized, or if all the letters are capitalized. On the other hand, for within language varieties, AIDA2(Al-Badrashiny et al., 2015) is the best published system attacking this problem in Arabic for the Arabic varieties mix problem. In this context, the problem of LCS is more complicated than mixing two very different languages since in the case of varieties of the same language, the two varieties typically share a common space of cognates and often faux amis, where there are homographs but the 109 words have very different semantic meanings, hence adding another layer of complexity to the problem. In this set up the assumed script is Arabic script. AIDA2 uses a complex system that is based on a mix of language dependent a"
W16-5813,L16-1640,1,0.813239,"nce between the feature vectors of the words related to “lang1” and those related to “lang2”, we use a word level unigram LM for one of the two languages in the training data. In practice, we pick the language where large corpora exist in order to build the LM. Then we apply the unigram LM to each word in the training data to find their word level probability. For the “ne” feature, we use the tagged named entities words from the training data as a lookup table. Then we put one in this feature if the word in the input tweet can be found in that lookup table, otherwise it is zero. We use SPLIT (Al-Badrashiny et al., 2016) to check if the word is numbers, dates, urls, emoticons, sounds, or punctuation. Then if the word is found to be any of these types, we put one the “is other” feature, otherwise it is zero. 3 Experimental Setup Table 1 shows the labels distribution of each language in the training and dev sets. The lang1, lang2 labels refer to the two languages addressed in the dataset name, for example for the language pair English-Spanish, lang1 is English and lang2 is Spanish, in that order. We also used the English Gigaword (LDC, 2003b) to build the unigram word level LM for the English part in English-Sp"
W16-5813,W14-3917,0,0.0154184,"te state transducer (FST) to learn the mapping between the Roman form of the Arabizi words and their Arabic form. It uses the resulting FST to find all possible Arabic candidates for each word in the input text. These candidates are filtered using MADAMIRA (Pasha et al., 2014), a state of the art morphological analyzer and POS disambiguation tool, to filter out non-Arabic solutions. Finally, it leverages a decision tree that is trained on language model probabilities of both the Arabic and Romanized forms to render the final decision for each word in context as either being Arabic or English. Bar and Dershowitz (2014) addresses the challenge for Spanish-English LCS. The authors use several features to train a sequential Support Vector Machines (SVM) classifier. The used features include previous and following two words, substrings of 13 character ngrams from the beginning and end of each word thereby modeling prefix and suffix information, a boolean feature indicating whether the first letter is capitalized or not, and 3-gram character and word n-gram language models trained over large corpora of English and Spanish, respectively. Barman et al. (2014) present systems for both Nepali-English and Spanish-Eng"
W16-5813,W14-3915,0,0.0304279,"word in context as either being Arabic or English. Bar and Dershowitz (2014) addresses the challenge for Spanish-English LCS. The authors use several features to train a sequential Support Vector Machines (SVM) classifier. The used features include previous and following two words, substrings of 13 character ngrams from the beginning and end of each word thereby modeling prefix and suffix information, a boolean feature indicating whether the first letter is capitalized or not, and 3-gram character and word n-gram language models trained over large corpora of English and Spanish, respectively. Barman et al. (2014) present systems for both Nepali-English and Spanish-English LCS. The script for both language pairs is Latin based, i.e. Nepali-English is written in Latin script, and Spanish-English is written in Latin script. The authors carry out several experiments using different approaches including dictionary-based methods, linear kernel SVMs, and a k-nearest neighbor approach. The best setup they found is the SVM-based one that uses character n-gram, binary features indicates whether the word is in a language specific dictionary of the most frequent 5000 words they have constructed, length of the wor"
W16-5813,W14-3901,1,0.857653,"Missing"
W16-5813,W16-5805,1,0.707322,"mmon languages in written and spoken communication. In Spanish-English for example: “She told me that mi esposo looks like un buen hombre.” (“She told me that my husband looks like a good man”). In this work we care about detecting LCS points as they occur intra-sententially where words from more than one language is mixed in the same utterance. LCS is observed on all levels of linguistic representation. It is pervasive especially in social media. LCS poses a significant challenge to NLP, hence detecting LCS points is a very important task for many downstream applications. In this shared task(Molina et al., 2016), the participants are asked to identify the language type of each word in a large set of tweets. The shared task has two language pair tracks; MSA-DA and Spanish• fw: if the word is related to any other language than the targeted language pair • ne: if the word is named entity; • other: if the word is number, punctuation, emoticons, url, date, starts with #, @, or contains underscore; • unk: if can not be determined to by any of the above tags. Relevant work on thhe LCS problem among different language pairs can be summarized in the following work. 3ARRIB (Al-Badrashiny et al., 2014; Eskander"
W16-5813,pasha-etal-2014-madamira,1,0.868667,"Missing"
W16-5813,N03-1028,0,0.128262,"b, 2016). It is based on the assumption that each language has its own character pattern behaviors and combinations relating to the underlying phonology, phonetics, and morphology of each language independently. Accordingly, the manner of articulation constrains the possible phonemic/morphemic combinations in a language. Accordingly, we use a supervised learning framework to address the challenge of LCS. We assume the presence of annotated code switched training data where each token is annotated as either Lang1 or Lang2. We create a sequence model using Conditional Random Fields (CRF++) tool(Sha and Pereira, 2003). For each word in the training data, we create a feature vector comprising character sequence level probabilities, unigram word level probabilities, and two binary features to identify if the word is named entity or not and is other or not . Once we derive the learning model, we apply to input text to identify the tokens in context. For the character sequence level probabilities, we built a 5-gram character language model (CLM) using the SRILM tool(Stolcke, 2002) for each of the two languages presented in the training data using the annotated words. For example, if the training data contains"
W17-1321,W02-0504,0,0.204953,"Missing"
W17-1321,N07-2014,0,0.212282,"are of sizes 4, 8, 12, 16, and 20 grams. At test time, the undiacritized input text goes through the following pipeline: c) Character-Level Diacritization: If there are still some UNK words after steps (a) and (b), the CLM is used to find a plausible solution for them. 4 Experimental Setup 4.1 Data Several studies have been carried out on the problem of full automatic diacritization for MSA. Five of these studies, that also yield the most competitive results despite approaching the problem in different ways, use and report on the same exact data sets. These studies are Zitouni et al. (2006), Habash and Rambow (2007), Rashwan et al. (2011), Abandah et al. (2015), and Belinkov and Glass (2015). We will use the same data which is LDC’s Arabic Treebank of diacritized news stories-Part 3 v1.0: catalog number LDC2004T11 and ISBN 1-58563-298-8. The corpus includes complete Full diacritization comprising both morphological and syntactic diacritization. This corpus includes 600 documents from the Annahar News Text. There are a total of 340,281 words. The data is split as follows into two sets: • Training data comprising approximately 288K words; • Test data (TEST): comprises 90 documents selected by taking the la"
W17-1321,pasha-etal-2014-madamira,1,0.862403,"Missing"
W17-1321,W09-0804,0,0.812795,"Missing"
W17-1321,L16-1577,1,0.741058,"Missing"
W17-1321,P06-1073,0,0.536478,"for the character level are of sizes 4, 8, 12, 16, and 20 grams. At test time, the undiacritized input text goes through the following pipeline: c) Character-Level Diacritization: If there are still some UNK words after steps (a) and (b), the CLM is used to find a plausible solution for them. 4 Experimental Setup 4.1 Data Several studies have been carried out on the problem of full automatic diacritization for MSA. Five of these studies, that also yield the most competitive results despite approaching the problem in different ways, use and report on the same exact data sets. These studies are Zitouni et al. (2006), Habash and Rambow (2007), Rashwan et al. (2011), Abandah et al. (2015), and Belinkov and Glass (2015). We will use the same data which is LDC’s Arabic Treebank of diacritized news stories-Part 3 v1.0: catalog number LDC2004T11 and ISBN 1-58563-298-8. The corpus includes complete Full diacritization comprising both morphological and syntactic diacritization. This corpus includes 600 documents from the Annahar News Text. There are a total of 340,281 words. The data is split as follows into two sets: • Training data comprising approximately 288K words; • Test data (TEST): comprises 90 documents"
W17-1321,D15-1274,0,\N,Missing
W17-1322,L16-1640,1,0.830839,"IDF) scores as dimension values for the input matrix to the w2v. For each TH pair, we obtain the following features: 1. The cosine distance between the T and H vectors which consider both matched and unmatched words; and, b) The cosine distance between the T-H and H-T vectors which consider unmatched words only; specifically, they represent the words in T that are not in H and vice versa, respectively. The latter provides for additional evidence for the distance between T and H. Each vector is calculated as follows: V ector = n X the same manner using the following preprocessing steps: SPLIT (Al-Badrashiny et al., 2016) is used to check if the word is a number, date, URL, or punctuation. Then all URLs and punctuation are removed and numbers and dates are normalized to Num and Date, respectively. Next, Alef and Yaa characters are normalized each to a single form which is typical in large scale Arabic NLP applications. For tokenization, lemmatization and stemming we use MADAMIRA (Pasha et al., 2014). We apply the D3 tokenization scheme which segments determiners as well as proclitics and enclitics; i.e. the D3 tokenization scheme is similar to the ATB scheme with the extra tokenization of the determiner Al. Fi"
W17-1322,R11-2007,0,0.0572427,"Missing"
W17-1322,W04-3205,0,0.151429,"Missing"
W17-1322,maamouri-etal-2008-enhancing,0,0.0462741,"Missing"
W17-1322,P06-1051,0,0.0234737,"es have been proposed. The 185 Proceedings of The Third Arabic Natural Language Processing Workshop (WANLP), pages 185–190, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics 3.1 entailment judgment is typically cast as a classification decision: true entailment if the relation holds and false otherwise. Therefore, most of the proposed systems have been based on machine learning approaches which model the entailment relation over a variety of conventional features varying from basic lexical features to deep semantic features (Inkpen et al., ; Pakray et al., 2011; Zanzotto and Moschitti, 2006; Malakasiotis and Androutsopoulos, 2007). External semantic resources such as WordNet and VerbOcean have been extensively used to capture the semantic relationships between words in the H and T, and also to further enhance the entailment recognition system (Iftene and Moruz, 2009; Mehdad et al., 2009). Using such resources, the authors explicitly model lexical and semantic features (Zanzotto et al., 2009; Sammons et al., 2009; Clinchant et al., 2006; Mehdad et al., 2009; Wang and Neumann, 2008; Moschitti, 2006). Other methods rely on dependency tree representations using different computation"
W17-1322,W07-1407,0,0.0144367,"Proceedings of The Third Arabic Natural Language Processing Workshop (WANLP), pages 185–190, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics 3.1 entailment judgment is typically cast as a classification decision: true entailment if the relation holds and false otherwise. Therefore, most of the proposed systems have been based on machine learning approaches which model the entailment relation over a variety of conventional features varying from basic lexical features to deep semantic features (Inkpen et al., ; Pakray et al., 2011; Zanzotto and Moschitti, 2006; Malakasiotis and Androutsopoulos, 2007). External semantic resources such as WordNet and VerbOcean have been extensively used to capture the semantic relationships between words in the H and T, and also to further enhance the entailment recognition system (Iftene and Moruz, 2009; Mehdad et al., 2009). Using such resources, the authors explicitly model lexical and semantic features (Zanzotto et al., 2009; Sammons et al., 2009; Clinchant et al., 2006; Mehdad et al., 2009; Wang and Neumann, 2008; Moschitti, 2006). Other methods rely on dependency tree representations using different computations ranging from basic common edge count (M"
W17-1322,pasha-etal-2014-madamira,1,0.871036,"Missing"
W18-3219,N18-1127,1,0.877746,"Missing"
W18-3219,L16-1669,1,0.92618,"Missing"
W18-3219,W17-4419,1,0.850073,"e NE tokens in MSA-EGY dataset is higher than the percentage of the NE tokens in ESP-ENG dataset. 4 Approaches In this section, we briefly describe the systems of the participants and discuss their results as well as the final scores. • IIT BHU (Trivedi et al., 2018). They proposed a “new architecture based on gating of character- and word-based representation of a token”. They captured the character and the word representations using a CNN and a bidirectional LSTM, respectively. They also used the Multi-Task Learning on the output layer and transfer the learning to a CRF classifier following Aguilar et al. (2017). Moreover, they fed a gazetteers representation to their model. Figure 3: MSA-EGY Data Annotation (i.e., URL, Punctuation, Number, etc) in addition to named entities. Then, we extracted and prepared all the tweets that contained “ne” for annotation. As we mentioned earlier, the IOB scheme is used as an annotation scheme to identify multiple words as a single named entity. All the URLs, Punctuation and Numbers tags are deterministically converted to “O” tag, while the tweets that include “ne” tags were given to our in-lab annotators for validation and re-annotation if needed. . Quality checks"
W18-3219,W18-3217,0,0.0703829,"are the ones described in Table 3. As stated by (Derczynski et al., 2014), the idea of the Surface Form F1-score is to capture the novel and emerging aspects that are usually encountered in social media data. Those aspects describe a fast-moving language that constantly produces new entities challenging more the recall capabilities of state-of-the-art models than the precision side. They fed the CRF with features from both external and internal resources. Additionally, they incorporated the language identification labels of the datasets from the previous versions of this workshop. • semantic (Geetha et al., 2018). They jointly trained a Bidirectional LSTM with a Conditional Random Fields on the output layer. • BATs (Janke et al., 2018). They used a Conditional Random Fields with multiple features. Some of those features were also used for neural network, but they got better results with the CRF approach. • Fraunhofer FKIE (Claeser et al., 2018). They used a Support Vector Machine (SVM) classifier with a Radial Basis kernel. They handcrafted a lot of features and also included gazetteers. 5.2 Although all the scores reported by the participants outperformed the baselines in both ENGSPA and MSA-EGY lang"
W18-3219,K15-1005,1,0.918003,"Missing"
W18-3219,W18-3213,0,0.0614809,"Missing"
W18-3219,W16-5812,1,0.89568,"Missing"
W18-3219,W18-3212,0,0.0472178,"Missing"
W18-3219,N16-1030,0,0.203476,"Missing"
W18-3219,W15-3116,0,0.0485213,"Missing"
W18-3219,P16-1101,0,0.120177,"Missing"
W18-3219,W16-5805,1,0.914454,"Missing"
W18-3219,W16-5803,0,0.0287502,"Missing"
W18-3219,W18-3220,0,0.0799552,"Missing"
W18-3219,D11-1141,0,0.0941713,"nts as a title. This is an example of what we refer to heterogeneous entity type, mean• Clitic attachment can obscure tokens, e.g. ¢l ¤ wAllh “and-God” or ”swear”. • Clitic attachment can obscure tokens, e.g. Yn¤ wmnY “and-Mona” or ”swear”. 144 N 1 MSA-EGY Samples Buckwalter Encoding:[wAllh]PER OnA HAss bqhr In [ElA’ Ebd AlftAH]PER [wmnY]PER [syf ]PER bytHAkmwA wfy AlqfS Arabic: º®  rhq HFA A ¢l ¤ years (Sang and Meulder, 2003). More recently, however, the focus has drastically moved to social media data due to the great incidence that social networks have in our daily communication (Ritter et al., 2011; Augenstein et al., 2017). The workshop on Noisy User-generated Text (W-NUT) has been a great effort towards the study of named entity recognition on noisy data. In 2016, the organizers focused on named entities from different topics to evaluate the adaptation of models from one topic to another (Strauss et al., 2016). In 2017, the organizers introduced the Surface Form F1-score metric and collected data from multiple social media platforms (Derczynski et al., 2014). The challenge not only lies on the entity types and the social media noisy but also in the distribution of the datasets and the"
W18-3219,W15-2902,0,0.15871,"Missing"
W18-3219,W03-0419,0,0.626032,"Missing"
W18-3219,L16-1655,0,0.0740953,"Missing"
W18-3219,W18-3215,0,0.0617212,"Missing"
W18-3219,W18-3221,0,0.175264,"Missing"
W18-3219,W14-3907,1,0.913831,"Missing"
W18-3219,W18-3214,0,0.187561,"Missing"
W18-3219,D08-1102,1,0.830788,"Missing"
W18-3219,D08-1110,1,0.798454,"Missing"
W18-5525,C16-1091,0,0.0248271,"Missing"
W18-5525,N18-1074,0,0.178011,"idence for that position. As these tasks are dependent on each other, an ideal model would consider the veracity of the claim when finding evidence and also find only the evidence that is relevant. We thus jointly model sentence extraction and verification on the FEVER shared task. Among all participants, we ranked 5th on the blind test set (prior to any additional human evaluation of the evidence). 1 Introduction Verifying claims using textual sources is a difficult problem, requiring natural language inference as well as information retrieval if the sources are not provided. The FEVER task (Thorne et al., 2018) provides a large annotated resource for extraction of sentences from Wikipedia and verification of the extracted evidence against a claim. A system that extracts and verifies statements in this framework must consist of three components: 1) Retrieving Wikipedia articles. 2) Identifying the sentences from Wikipedia that support or refute the claim. 3) Predicting supporting, refuting, or not enough info. We combine these components into two stages: 1) identifying relevant documents (Wikipedia ar∗ 2 Document Retrieval For the baseline system, the document retrieval component from DrQA (Chen et a"
W18-5525,P17-1171,0,0.24027,"lumbia University New York, NY 10027 chidey@cs.columbia.edu Mona Diab Amazon AI Lab diabmona@amazon.com Abstract ticles) for a claim and 2) jointly extracting sentences from the top-ranked articles and predicting a relation for whether the claim is supported, refuted, or if there is not enough information in Wikipedia. We first identify relevant documents by ranking Wikipedia articles according to a model using lexical and syntactic features. Then, we derive contextual sentence representations for the claim paired with each evidence sentence in the extracted documents. We use the ESIM module (Chen et al., 2017b) to create embeddings for each claim/evidence pair and use a pointer network (Vinyals et al., 2015) to recurrently extract only relevant evidence sentences while predicting the relation using the entire set of evidence sentences. Finally, given these components, which are pre-trained using multi-task learning (Le et al., 2016), we tune the parameters of the entailment component given the extracted sentences. Our experiments reveal that jointly training the model provides a significant boost in performance, suggesting that the contextual representations learn information about which sentences"
W18-5525,P17-1152,0,0.204762,"lumbia University New York, NY 10027 chidey@cs.columbia.edu Mona Diab Amazon AI Lab diabmona@amazon.com Abstract ticles) for a claim and 2) jointly extracting sentences from the top-ranked articles and predicting a relation for whether the claim is supported, refuted, or if there is not enough information in Wikipedia. We first identify relevant documents by ranking Wikipedia articles according to a model using lexical and syntactic features. Then, we derive contextual sentence representations for the claim paired with each evidence sentence in the extracted documents. We use the ESIM module (Chen et al., 2017b) to create embeddings for each claim/evidence pair and use a pointer network (Vinyals et al., 2015) to recurrently extract only relevant evidence sentences while predicting the relation using the entire set of evidence sentences. Finally, given these components, which are pre-trained using multi-task learning (Le et al., 2016), we tune the parameters of the entailment component given the extracted sentences. Our experiments reveal that jointly training the model provides a significant boost in performance, suggesting that the contextual representations learn information about which sentences"
W18-5525,P18-1063,0,0.314247,"recall at k = 5 articles is presented in Figure 1. We present results on the paper development and test sets as the evidence for the shared task blind test set was unavailable as of this writing. The model with lexical and syntactic features obtains around 25 points absolute improvement over the DrQA baseline and the title features when added provide an additional 8 points improvement on the development set and 10.8 points on test. 3 Joint Sentence Extraction and Relation Prediction Recurrent neural networks have been shown to be effective for extractive summarization (Nallapati et al., 2017; Chen and Bansal, 2018). However, in this context we want to extract sentences that also help predict whether the claim is supported or refuted so we jointly model extraction and relation prediction and train in a multi-task setting. • Average/max/minimum of GloVe (Pennington et al., 2014) embeddings in the claim and the first and second sentence of Wikipedia. This feature captures the type of sentence. If it is a person, it may have words like ’born’, 151 Figure 2: The contextual claim and evidence sentence representations obtained with ESIM. Figure 3: Our multi-task learning architecture with contextual ESIM repre"
W18-5525,P14-5010,0,0.00455016,"Missing"
W18-5525,D14-1162,0,0.0840372,"s absolute improvement over the DrQA baseline and the title features when added provide an additional 8 points improvement on the development set and 10.8 points on test. 3 Joint Sentence Extraction and Relation Prediction Recurrent neural networks have been shown to be effective for extractive summarization (Nallapati et al., 2017; Chen and Bansal, 2018). However, in this context we want to extract sentences that also help predict whether the claim is supported or refuted so we jointly model extraction and relation prediction and train in a multi-task setting. • Average/max/minimum of GloVe (Pennington et al., 2014) embeddings in the claim and the first and second sentence of Wikipedia. This feature captures the type of sentence. If it is a person, it may have words like ’born’, 151 Figure 2: The contextual claim and evidence sentence representations obtained with ESIM. Figure 3: Our multi-task learning architecture with contextual ESIM representations used for evidence sentence extraction via a pointer network and relation prediction with max/average pooling and dense layers. First, given the top k extracted documents from our retrieval component, we select the top l sentences using a weighted combinati"
W19-0417,P15-1039,0,0.503736,"has led to a growing interest in transfer methods for developing semantic role labeling systems. The ultimate goal of transfer methods is to transfer supervised linguistic information from a rich-resource language to a target language of interest. Amongst transfer methods, annotation projection is a method that projects supervised annotation from a rich-resource language to a low-resource language through automatic word alignments in parallel data (Hwa et al., 2002; Pad´o and Lapata, 2009). Recent work on annotation projection for SRL (Kozhevnikov and Titov, 2013a; van der Plas et al., 2014; Akbik et al., 2015; Aminian et al., 2017) presumes the availability of accurate supervised features such as lemmas, part-of-speech (POS) tags and syntactic parse trees. However, this is not a realistic assumption for truly low-resource languages, for which (accurate) supervised features are hardly available. This paper considers the problem of annotation projection of dependency-based SRL in a scenario for which only parallel data is available for the target language. Recent state-of-the-art SRL systems have shown a significant reliance on the predicate lemma information while in a low-resource language, a lemm"
W19-0417,D16-1102,0,0.492539,"e structure of a state-of-the-art deep SRL system (Marcheggiani et al., 2017) to make it independent of supervised features. Our model solely rely on word and character level features in the target language. The main contribution of this work is on applying annotation projection without relying on supervised features in the target language of interest. To the best of our knowledge, this is the first study that builds a cross-lingual SRL transfer model in the absence of any explicit linguistic information in the target language. We make use of the recently released Universal Proposition Banks (Akbik et al., 2016)1 , 1 https://github.com/System-T/UniversalPropositions AM-ADV A0 A0 A1 AM-ADJ congratulate.01 I congratulate report.01 him on his excellent report Ich begl¨uckw¨unsche ihn zu seinem ausgezeichneten Bericht congratulate.01 A0 report.01 A1 AM-ADJ A0 Figure 1: An example of annotation projection for an English-German sentence pair from the Europarl corpus (Koehn, 2005). Supervised predicate-argument structure of the English sentence (edges on top) is generated using our supervised SRL system trained on PropBank 3 (Palmer et al., 2005). Dashed lines in the middle show intersected word alignments"
W19-0417,I17-2003,1,0.775686,"g interest in transfer methods for developing semantic role labeling systems. The ultimate goal of transfer methods is to transfer supervised linguistic information from a rich-resource language to a target language of interest. Amongst transfer methods, annotation projection is a method that projects supervised annotation from a rich-resource language to a low-resource language through automatic word alignments in parallel data (Hwa et al., 2002; Pad´o and Lapata, 2009). Recent work on annotation projection for SRL (Kozhevnikov and Titov, 2013a; van der Plas et al., 2014; Akbik et al., 2015; Aminian et al., 2017) presumes the availability of accurate supervised features such as lemmas, part-of-speech (POS) tags and syntactic parse trees. However, this is not a realistic assumption for truly low-resource languages, for which (accurate) supervised features are hardly available. This paper considers the problem of annotation projection of dependency-based SRL in a scenario for which only parallel data is available for the target language. Recent state-of-the-art SRL systems have shown a significant reliance on the predicate lemma information while in a low-resource language, a lemmatizer might not be ava"
W19-0417,C18-1233,0,0.0730152,"SRL (Kozhevnikov and Titov, 2013a; Wang et al., 2017). Our work makes use of the recently released Universal Proposition Bank (Akbik et al., 2016). This dataset maps every predicate lemma in every language to its corresponding English lemma following the frame and role label schemes of the English Proposition Bank 3.0 (Palmer et al., 2005) In the realm of supervised SRL methods, however, there have been several efforts to build SRL models that do not need a wide range of linguistic features (specifically syntactic features) (Marcheggiani et al., 2017; Zhou and Xu, 2015; He et al., 2017, 2018; Cai et al., 2018; Mulcaire et al., 2018). In a more recent study, Mulcaire et al. (2018) proposed a polyglot SRL system that benefits from the similarities between the semantic structures of different languages to improve monolingual SRL. All those studies, however, assume the availability of semantically annotated datasets for the target language, thus making them non-applicable to low-resource languages. 6 Conclusion We have described a method for cross-lingual transfer of dependency-based SRL systems via annotation projection. Our model is agnostic to linguistic features leading to a robust model that can"
W19-0417,W02-1001,0,0.109934,"esentation (obtained from a character BiLSTM) for every token in the sentence. We use a deep BiLSTM to get the final representation for each token. The ultimate predictions are made by performing an affine transform on the BiLSTM hidden output. 4.1 Projection Experiments Our supervised SRL system is a reimplementation of the model of Marcheggiani et al. (2017). We generate automatic English predicate senses using a system similar to the predicate disambiguation module of Bj¨orkelund et al. (2009) except that we replace the logistic regression classifier with the averaged Perceptron algorithm (Collins, 2002). In order to comply with the Universal Proposition Bank annotation scheme, we convert the argument spans in the English PropBank v3 (Palmer et al., 2005) to dependencybased arguments by labeling the syntactic head of each span. For annotation projection, we define density of alignments to find sentences with relatively-dense alignments: Plj0 (i) j=1 I(aj &gt; 0) density(i) = li0 (i) where li0 is the length of the ith target sentence in parallel data, aj is the alignment index for the jth (i) word in the target sentence, and I(aj &gt; 0) is an indicator for a non-NULL alignment. We prune the target"
W19-0417,eisele-chen-2010-multiun,0,0.0331973,"a case for which lemmas are represented by character BiLSTMs is shown in figure 2. As shown in the figure, we use two different character BiLSTMs in order to represent lemmas: one for the input representation and the other for the decoder representation. 4 Experiments Datasets and Tools We use English as the source language and project SRL annotations to the following languages: German, Spanish, Finnish, French, Italian, Portuguese, and Chinese. We use the Europarl parallel corpus (Koehn, 2005) for the European languages and a random sample of 2 million sentence pairs from the MultiUN corpus (Eisele and Chen, 2010) for Chinese. We use the Giza++ tool (Och and Ney, 2003) with its default setting for word alignment. We run Giza++ in source-to-target and the reverse Figure 2: A graphical depiction of our joint argument identification and classification model without using part-ofspeech tags, lemmas, and syntax. In this example, the predicate-specific encoder considers word eats as the sentence predicate and the goal is to score the assignment of argument apple with label A0 . Our model contains three different character BiLSTMs; at the bottom, a character BiLSTM is run to acquire a character-based represen"
W19-0417,P03-1068,0,0.0834082,"Missing"
W19-0417,C14-1111,0,0.0748444,"Missing"
W19-0417,P17-1044,0,0.0978626,"rov et al., 2018), and SRL (Kozhevnikov and Titov, 2013a; Wang et al., 2017). Our work makes use of the recently released Universal Proposition Bank (Akbik et al., 2016). This dataset maps every predicate lemma in every language to its corresponding English lemma following the frame and role label schemes of the English Proposition Bank 3.0 (Palmer et al., 2005) In the realm of supervised SRL methods, however, there have been several efforts to build SRL models that do not need a wide range of linguistic features (specifically syntactic features) (Marcheggiani et al., 2017; Zhou and Xu, 2015; He et al., 2017, 2018; Cai et al., 2018; Mulcaire et al., 2018). In a more recent study, Mulcaire et al. (2018) proposed a polyglot SRL system that benefits from the similarities between the semantic structures of different languages to improve monolingual SRL. All those studies, however, assume the availability of semantically annotated datasets for the target language, thus making them non-applicable to low-resource languages. 6 Conclusion We have described a method for cross-lingual transfer of dependency-based SRL systems via annotation projection. Our model is agnostic to linguistic features leading to"
W19-0417,P18-1192,0,0.0598239,"Missing"
W19-0417,P02-1050,0,0.176023,"05; Erk et al., 2003; Zaghouani et al., 2010), majority of languages do not have such annotated resources. The lack of annotated resources for SRL has led to a growing interest in transfer methods for developing semantic role labeling systems. The ultimate goal of transfer methods is to transfer supervised linguistic information from a rich-resource language to a target language of interest. Amongst transfer methods, annotation projection is a method that projects supervised annotation from a rich-resource language to a low-resource language through automatic word alignments in parallel data (Hwa et al., 2002; Pad´o and Lapata, 2009). Recent work on annotation projection for SRL (Kozhevnikov and Titov, 2013a; van der Plas et al., 2014; Akbik et al., 2015; Aminian et al., 2017) presumes the availability of accurate supervised features such as lemmas, part-of-speech (POS) tags and syntactic parse trees. However, this is not a realistic assumption for truly low-resource languages, for which (accurate) supervised features are hardly available. This paper considers the problem of annotation projection of dependency-based SRL in a scenario for which only parallel data is available for the target languag"
W19-0417,L18-1293,0,0.0614361,"Missing"
W19-0417,2005.mtsummit-papers.11,0,0.583439,"owledge, this is the first study that builds a cross-lingual SRL transfer model in the absence of any explicit linguistic information in the target language. We make use of the recently released Universal Proposition Banks (Akbik et al., 2016)1 , 1 https://github.com/System-T/UniversalPropositions AM-ADV A0 A0 A1 AM-ADJ congratulate.01 I congratulate report.01 him on his excellent report Ich begl¨uckw¨unsche ihn zu seinem ausgezeichneten Bericht congratulate.01 A0 report.01 A1 AM-ADJ A0 Figure 1: An example of annotation projection for an English-German sentence pair from the Europarl corpus (Koehn, 2005). Supervised predicate-argument structure of the English sentence (edges on top) is generated using our supervised SRL system trained on PropBank 3 (Palmer et al., 2005). Dashed lines in the middle show intersected word alignments from Giza++ (Och and Ney, 2003). Dashed edges at the bottom show the projected predicate-arguments. a semi-automatically annotated data that unifies the annotation scheme for all languages. We show the effectiveness of our method on a range of languages, namely German, Spanish, Finnish, French, Italian, Portuguese, and Chinese. We compare our model to a state-of-the-"
W19-0417,S13-1044,0,0.0473665,"Missing"
W19-0417,P13-1117,0,0.0574142,"Missing"
W19-0417,N15-1142,0,0.0118585,"3 1 hb4 1 1 hf4 concat xre 2 1 x pe 2 0 1 concat xre 3 x pe 3 #» 0 xre 4 0 an eats Mary vapple van concat xre 1 0 concat x pe 4 #» 0 apple concat µ f 1 µb 1 c p M p ca f 1 ⇣ f 2 ⇣ ⇣b 1 ⇣b 2 ⇣b 3 ⇣ concat p cy p cr f 3 ⇣ ⇣b 4 f 4 p ce p ca c f 1 ⇣ f 2 ⇣ ⇣b 1 ⇣b 2 ⇣b 3 ⇣ concat f 3 p t p cs ⇣ ⇣b 4 f 4 µ f 2 µb 2 µ f 3 µb 3 µ f 4 µb 4 p ca p cn p ca f 2 ⇣ f 1 ⇣ ⇣b 1 ⇣b 2 ⇣ concat p cp p cp f 1 ⇣ f 2 ⇣ f 3 ⇣b 1 ⇣b 2 ⇣b 3 c ⇣ f 4 ⇣b 4 concat direction and get the intersection of alignment links. For English, we use the pre-trained embedding vectors generated using the structured skip-gram model of Ling et al. (2015). For the target languages, we train Word2vec (Mikolov et al., 2013) on Wikipedia data to generate embedding vectors. We implement our deep network using the Dynet library (Neubig et al., 2017). We use the dimension of 100 for word embeddings, 50 for characters, 512 for LSTM encoders, 128 for role and lemma embeddings in the decoder, and 100 for decoder lemma embedding. We pick random minibatches of size 1000 with a fixed learning rate of 0.001 for learning the parameter values with the Adam optimizer (Kingma and Ba, 2014). The depth of BiLSTM network is set to one for character representation"
W19-0417,K17-1041,0,0.140864,"Missing"
W19-0417,P18-2106,0,0.118441,"nd Titov, 2013a; Wang et al., 2017). Our work makes use of the recently released Universal Proposition Bank (Akbik et al., 2016). This dataset maps every predicate lemma in every language to its corresponding English lemma following the frame and role label schemes of the English Proposition Bank 3.0 (Palmer et al., 2005) In the realm of supervised SRL methods, however, there have been several efforts to build SRL models that do not need a wide range of linguistic features (specifically syntactic features) (Marcheggiani et al., 2017; Zhou and Xu, 2015; He et al., 2017, 2018; Cai et al., 2018; Mulcaire et al., 2018). In a more recent study, Mulcaire et al. (2018) proposed a polyglot SRL system that benefits from the similarities between the semantic structures of different languages to improve monolingual SRL. All those studies, however, assume the availability of semantically annotated datasets for the target language, thus making them non-applicable to low-resource languages. 6 Conclusion We have described a method for cross-lingual transfer of dependency-based SRL systems via annotation projection. Our model is agnostic to linguistic features leading to a robust model that can be trained on projected"
W19-0417,J03-1002,0,0.0180547,"/github.com/System-T/UniversalPropositions AM-ADV A0 A0 A1 AM-ADJ congratulate.01 I congratulate report.01 him on his excellent report Ich begl¨uckw¨unsche ihn zu seinem ausgezeichneten Bericht congratulate.01 A0 report.01 A1 AM-ADJ A0 Figure 1: An example of annotation projection for an English-German sentence pair from the Europarl corpus (Koehn, 2005). Supervised predicate-argument structure of the English sentence (edges on top) is generated using our supervised SRL system trained on PropBank 3 (Palmer et al., 2005). Dashed lines in the middle show intersected word alignments from Giza++ (Och and Ney, 2003). Dashed edges at the bottom show the projected predicate-arguments. a semi-automatically annotated data that unifies the annotation scheme for all languages. We show the effectiveness of our method on a range of languages, namely German, Spanish, Finnish, French, Italian, Portuguese, and Chinese. We compare our model to a state-of-the-art baseline that uses a rich set of supervised features and show that our model outperforms on six out of seven languages in the Universal Proposition Banks. Furthermore, for Finnish, a morphologically rich language, our model with unsupervised features improve"
W19-0417,H05-1108,0,0.333478,"Missing"
W19-0417,J05-1004,0,0.95149,"at presumes the availability of supervised features such as lemmas, part-of-speech tags, and dependency parse trees, we only make use of word and character features. Our deep model considers using character-based representations as well as unsupervised stem embeddings to alleviate the need for supervised features. Our experiments outperform a state-of-the-art method that uses supervised lexico-syntactic features on 6 out of 7 languages in the Universal Proposition Bank. 1 Introduction Despite considerable efforts on developing semantically annotated resources for semantic role labeling (SRL) (Palmer et al., 2005; Erk et al., 2003; Zaghouani et al., 2010), majority of languages do not have such annotated resources. The lack of annotated resources for SRL has led to a growing interest in transfer methods for developing semantic role labeling systems. The ultimate goal of transfer methods is to transfer supervised linguistic information from a rich-resource language to a target language of interest. Amongst transfer methods, annotation projection is a method that projects supervised annotation from a rich-resource language to a low-resource language through automatic word alignments in parallel data (Hw"
W19-0417,petrov-etal-2012-universal,0,0.161524,"Missing"
W19-0417,P14-1127,1,0.850361,"Missing"
W19-0417,P16-1113,0,0.0433502,"pi in the sentence, we find the semantic dependencies between each word in the sentence with respect to each predicate: r Lx = [(pi → − j|ψi ); 1 ≤ j ≤ n, pi ∈ P] where r is the role of the jth word as an argument for the predicate word xpi . In case that a word is not an argument, r is NULL. Evaluation of the system output is conducted on semantic dependencies r (pi → − j|ψi ); thus the SRL system should find predicate senses as well as argument roles. During training, these dependencies are used as training instances for a machine learning algorithm. Previous work (Bj¨orkelund et al., 2009; Roth and Lapata, 2016; Marcheggiani et al., 2017) factorized this task into predicate sense disambiguation, argument identification, and argument classification. Annotation Projection In annotation projection, we assume that we have a parallel data P = [(s(1) , t(1) ), · · · , (s(k) , t(k) )] such that each sentence s(i) is a translation of sentence t(i) . Here, we assume that s(i) belongs to a rich-resource language in which annotated resources are available. In contrast, t(i) belongs to a low-resource target language where annotated data and tools such as semantic roles, dependency trees, part-of-speech tags, wo"
W19-0417,K17-3009,0,0.0596556,"Missing"
W19-0417,C14-1121,0,0.0499527,"Missing"
W19-0417,P11-2052,0,0.771441,"Missing"
W19-0417,D17-1205,0,0.0187756,"sion for languages that do not have accurate syntactic parsers such as Arabic and Hindi. In contrary to the previous studies, our work builds a cross-lingual SRL system without having any supervised features for the target language. One obstacle for developing transfer models is the absence of a unified annotation scheme for all languages. There has been a great deal of work in developing universal annotation schemes for a variety of tasks such as POS tagging (Petrov et al., 2011), dependency parsing (Nivre et al., 2017), morphology (Kirov et al., 2018), and SRL (Kozhevnikov and Titov, 2013a; Wang et al., 2017). Our work makes use of the recently released Universal Proposition Bank (Akbik et al., 2016). This dataset maps every predicate lemma in every language to its corresponding English lemma following the frame and role label schemes of the English Proposition Bank 3.0 (Palmer et al., 2005) In the realm of supervised SRL methods, however, there have been several efforts to build SRL models that do not need a wide range of linguistic features (specifically syntactic features) (Marcheggiani et al., 2017; Zhou and Xu, 2015; He et al., 2017, 2018; Cai et al., 2018; Mulcaire et al., 2018). In a more r"
W19-0417,W10-1836,1,0.629842,"sed features such as lemmas, part-of-speech tags, and dependency parse trees, we only make use of word and character features. Our deep model considers using character-based representations as well as unsupervised stem embeddings to alleviate the need for supervised features. Our experiments outperform a state-of-the-art method that uses supervised lexico-syntactic features on 6 out of 7 languages in the Universal Proposition Bank. 1 Introduction Despite considerable efforts on developing semantically annotated resources for semantic role labeling (SRL) (Palmer et al., 2005; Erk et al., 2003; Zaghouani et al., 2010), majority of languages do not have such annotated resources. The lack of annotated resources for SRL has led to a growing interest in transfer methods for developing semantic role labeling systems. The ultimate goal of transfer methods is to transfer supervised linguistic information from a rich-resource language to a target language of interest. Amongst transfer methods, annotation projection is a method that projects supervised annotation from a rich-resource language to a low-resource language through automatic word alignments in parallel data (Hwa et al., 2002; Pad´o and Lapata, 2009). Re"
W19-0417,P15-1109,0,0.0269554,"17), morphology (Kirov et al., 2018), and SRL (Kozhevnikov and Titov, 2013a; Wang et al., 2017). Our work makes use of the recently released Universal Proposition Bank (Akbik et al., 2016). This dataset maps every predicate lemma in every language to its corresponding English lemma following the frame and role label schemes of the English Proposition Bank 3.0 (Palmer et al., 2005) In the realm of supervised SRL methods, however, there have been several efforts to build SRL models that do not need a wide range of linguistic features (specifically syntactic features) (Marcheggiani et al., 2017; Zhou and Xu, 2015; He et al., 2017, 2018; Cai et al., 2018; Mulcaire et al., 2018). In a more recent study, Mulcaire et al. (2018) proposed a polyglot SRL system that benefits from the similarities between the semantic structures of different languages to improve monolingual SRL. All those studies, however, assume the availability of semantically annotated datasets for the target language, thus making them non-applicable to low-resource languages. 6 Conclusion We have described a method for cross-lingual transfer of dependency-based SRL systems via annotation projection. Our model is agnostic to linguistic fea"
W19-1410,W16-5804,0,0.566838,"Missing"
W19-1410,E17-2052,0,0.280359,"Missing"
W19-1410,N18-1090,0,0.0352724,"Missing"
W19-1410,E17-1088,0,0.0494345,"Missing"
W19-1410,Q17-1010,0,0.0896431,"Missing"
W19-1410,W18-3219,1,0.897048,"Missing"
W19-1410,W18-3211,0,0.0515501,"Missing"
W19-1410,K15-1005,1,0.891493,"Missing"
W19-1410,D16-1070,0,0.0513194,"Missing"
W19-1410,L18-1173,1,0.853184,"Curras Corpus of Palestinian Arabic as the MSA-LEV textual CS data (Jarrar et al., 2017). Palestinian Arabic is a subdialect of Levantine Arabic. The corpus comprises 57,000 words, half of which come from transcripts of a TV show and the rest of which comes from various sources such as Facebook, discussing forums, Twitter, and blogs. The corpus is morphologically annotated by Eskander et al. (2016) using the same guidelines utilized for annotating the Egyptian ARZ corpus. We annotate the MSALEV evaluation data set with language id using the guidelines and tool proposed by (Diab et al., 2016; AlGhamdi and Diab, 2018). To train pre-trained embeddings, we crawl tweets from Levantine public figures. Moreover, we compile Levantine and MSA raw textual data from multiple resources: online news commentary corpus from (Zaidan and Callison-Burch, 2011), 103 Table-2 shows the performance of the different baseline POS tagging systems on the test data. For each language pair, there are two baseline systems. For MSA-EGY, the baseline accuracies are 85.40 when the baseline system utilizes the MSA pre-trained embeddings, and 81.06 when BiLSTM-CRF uses the EGY pre-trained embeddings. Similarly, we report the baseline res"
W19-1410,W16-5812,1,0.871664,"identification; d) We present the first empirical evaluation on POS tagging with four different language pairs. All of the previous work focused on a single or two language pair combinations. 2 Related Work Developing CS text processing NLP techniques for analyzing user generated content as well as cater for needs of multilingual societies is vital (Vyas et al., 2014b). Previous studies that address the problem of POS tagging of CS data first attempt to identify the correct language of a word before feeding it into an appropriate monolingual tagger (Solorio and Liu, 2008; Vyas et al., 2014a; AlGhamdi et al., 2016). As is typically the case in NLP, such pipelines suffer from the problem of error propagation; e.g., failure of the language identification will cause problems in the POS tag prediction. Other approaches have trained supervised models on POS-annotated, CS data resources which are expensive to create and unavailable for most language pairs. (AlGhamdi et al., 2016; Solorio and Liu, 2008; Jamatia et al., 2015; Barman et al., 2016) Solorio and Liu (2008), proposed the first statistical approach to POS tagging 100 3 fiers. For the Boolean features, a bi-directional Long short-term memory (BiLSTM)"
W19-1410,cotterell-callison-burch-2014-multi,0,0.0659779,"Missing"
W19-1410,Y15-1004,0,0.0287469,"and Callison-Burch, 2011), 103 Table-2 shows the performance of the different baseline POS tagging systems on the test data. For each language pair, there are two baseline systems. For MSA-EGY, the baseline accuracies are 85.40 when the baseline system utilizes the MSA pre-trained embeddings, and 81.06 when BiLSTM-CRF uses the EGY pre-trained embeddings. Similarly, we report the baseline results for MSA-LEV, SPA-ENG and HIN-ENG language pairs. weblogs from COLABA (Diab et al., 2010), commentaries and tweets from Cotterell and CallisonBurch (2014), the Levantine portion of the PADIC data set (Meftouh et al., 2015), portion of MSA Gigaword (Parker et al., 2011), and LDC Arabic tree bank corpora (MSA) (Maamouri et al., 2004; Diab et al., 2013). SPA-ENG The Miami Bangor (MB) corpus is a conversational speech corpus recorded from bilingual Spanish-English speakers living in Miami, FL. It includes 56 conversations recorded from 84 speakers (Soto and Hirschberg, 2017). The corpus consists of 242,475 words (333,069 including punctuation tokens) and 35 hours of recorded conversation. The language markers in the corpus were manually annotated. To train pre-trained embeddings for Spanish and English language, we"
W19-1410,L16-1669,1,0.8632,"MSA-LEV We use the Curras Corpus of Palestinian Arabic as the MSA-LEV textual CS data (Jarrar et al., 2017). Palestinian Arabic is a subdialect of Levantine Arabic. The corpus comprises 57,000 words, half of which come from transcripts of a TV show and the rest of which comes from various sources such as Facebook, discussing forums, Twitter, and blogs. The corpus is morphologically annotated by Eskander et al. (2016) using the same guidelines utilized for annotating the Egyptian ARZ corpus. We annotate the MSALEV evaluation data set with language id using the guidelines and tool proposed by (Diab et al., 2016; AlGhamdi and Diab, 2018). To train pre-trained embeddings, we crawl tweets from Levantine public figures. Moreover, we compile Levantine and MSA raw textual data from multiple resources: online news commentary corpus from (Zaidan and Callison-Burch, 2011), 103 Table-2 shows the performance of the different baseline POS tagging systems on the test data. For each language pair, there are two baseline systems. For MSA-EGY, the baseline accuracies are 85.40 when the baseline system utilizes the MSA pre-trained embeddings, and 81.06 when BiLSTM-CRF uses the EGY pre-trained embeddings. Similarly,"
W19-1410,C16-1326,0,0.0261702,"hat uses the monolingual adaptation technique to create the shared space embedding. MUSE is equipped to learn either via supervision or no supervision. In our study, we utilize the unsupervised version MSA-LEV We use the Curras Corpus of Palestinian Arabic as the MSA-LEV textual CS data (Jarrar et al., 2017). Palestinian Arabic is a subdialect of Levantine Arabic. The corpus comprises 57,000 words, half of which come from transcripts of a TV show and the rest of which comes from various sources such as Facebook, discussing forums, Twitter, and blogs. The corpus is morphologically annotated by Eskander et al. (2016) using the same guidelines utilized for annotating the Egyptian ARZ corpus. We annotate the MSALEV evaluation data set with language id using the guidelines and tool proposed by (Diab et al., 2016; AlGhamdi and Diab, 2018). To train pre-trained embeddings, we crawl tweets from Levantine public figures. Moreover, we compile Levantine and MSA raw textual data from multiple resources: online news commentary corpus from (Zaidan and Callison-Burch, 2011), 103 Table-2 shows the performance of the different baseline POS tagging systems on the test data. For each language pair, there are two baseline"
W19-1410,W16-5805,1,0.779574,"Egyptian Arabic Treebanks 1-5 (ARZ1-5) (Maamouri et al., 2012). The ARZ1-5 data is from the discussion forums genre mostly in the Egyptian Arabic dialect (EGY). The total number of sentences in the corpus is 13,698 while the number of words is 174,967 words. To train pre-trained embeddings, we crawl Egyptian tweets from some of the Egyptian public figures’ Twitter accounts. The rest of the Egyptian raw textual data comes from the following sources: (Zaidan and Callison-Burch, 2011)’s Egyptian online news commentary corpus, the Egyptian tweets used in the CS shared tasks (Solorio et al., 2014; Molina et al., 2016; Aguilar et al., 2018), portion of MSA Gigaword (Parker et al., 2011), and LDC Arabic tree bank corpora (MSA) (Maamouri et al., 2004; Diab et al., 2013). To identify the language id of MSA-EGY tokens, we use the Automatic Identification of Dialectal Arabic (AIDA2) tool (Al-Badrashiny et al., 2015) to perform token level language identification for the EGY and MSA tokens in context. Merged multilingual embeddings: We train a multilingual embedding for language pairs that have one language in common (pivot language). To do so, we combine the corpora used to train the word embeddings of the lang"
W19-1410,E14-1049,0,0.103496,"Missing"
W19-1410,R15-1033,0,0.293952,"Missing"
W19-1410,D14-1162,0,0.0821908,"f the potential applications of bilingual and multilingual embeddings is in the processing of code switching language. In this section, we compare leveraging multiple neural network architectures for POS tagging CS data. Moreover, we explore different embedding setups to investigate the optimal way of tackling the POS tagging of CS data. First, we illustrate the tool used for training the embeddings layer. Second, we present the neural network models. Then we list the embedding setups. Pretrained Word Embedding Model Most successful methods for learning word embeddings (Mikolov et al., 2013c; Pennington et al., 2014; Bojanowski et al., 2017) rely on the distributional hypothesis (Mikolov et al., 2013b), i.e., words occurring in similar contexts tend to have similar meanings. Among all word embedding techniques, we choose the FastText tool developed by Facebook (Bojanowski et al., 2017). Our choice of FastText is motivated by the fact that social media networks are the primary source of our unlabeled data. This type of data exhibits huge variations of spelling and misspellings. FastText takes advantage of subword (i.e., n-gram) information. It creates vector representations for misspelling replacement can"
W19-1410,D18-1344,0,0.0365515,"Missing"
W19-1410,D14-1105,0,0.599144,"ngton University 2 AWS, Amazon AI {fghamdi, mtdiab}@gwu.edu Abstract guages: a common first language (lang1) and another prevalent language as a second language (lang2). The languages could be completely distinct such as Mandarin and English, or Hindi and English, or they can be variants of one another such as in the case of Modern Standard Arabic (MSA) and Arabic regional dialects (e.g. Egyptian dialect-EGY). CS is traditionally prevalent in the spoken modality but with the ubiquity of the Internet and proliferation of social media, CS is becoming ubiquitous in written modalities and genres (Vyas et al., 2014a; Danet and Herring, 2007; C´ardenas-Claros and Isharyanti, 2009). This new situation has created an unusual deluge of CS textual data on the Internet. This data brings in its wake new opportunities, but it poses serious challenges for different NLP tasks; traditional techniques trained for one language tend to break down when the input text happens to include two or more languages. The performance of NLP models that are currently expected to yield good results (e.g., Part-of-Speech Tagging) would degrade at a rate proportional to the amount and level of mixed-language present. This is a resu"
W19-1410,D17-1035,0,0.0806725,"gton et al., 2014). • The third model is a multi-task learning model. The model learns simultaneously POS and language id tags with the aim of boosting the performance of POS tagging task. The architecture of this model follows MTL-POS Tagger architecture. The difference is that one output layer is for predicting the POS tagging, and the other output layer is for predicting language id task. This model is referred to as MTL-POS+LID Tagger. A Model for Neural POS Tagging For our experiments we use three neural networks architectures: a) a BiLSTM-CRF architecture similar to the one proposed by (Reimers and Gurevych, 2017) for POS tagging; b) a multi-task learning model for learning jointly POS tagging for related language pairs; and, c) a multi-task learning model for learning jointly POS and Language ID tagging. Experimental Conditions Monolingual embedding (baseline): We train word embeddings using the monolingual corpora for each language involved in the four language pairs. The results of this approach are six separate pre-trained embeddings, MSA, EGY, LEV, ENG, SPA, and HIN pre-trained embeddings. For each language, we train a BiLSTM-CRF model using one of the six pre-trained embeddings. We consider these"
W19-1410,P11-2007,0,0.181612,"er. Table-1 shows the distribution of the evaluation data sets. Table 1: Datasets distribution for the four language pairs MSA-EGY We use the LDC Egyptian Arabic Treebanks 1-5 (ARZ1-5) (Maamouri et al., 2012). The ARZ1-5 data is from the discussion forums genre mostly in the Egyptian Arabic dialect (EGY). The total number of sentences in the corpus is 13,698 while the number of words is 174,967 words. To train pre-trained embeddings, we crawl Egyptian tweets from some of the Egyptian public figures’ Twitter accounts. The rest of the Egyptian raw textual data comes from the following sources: (Zaidan and Callison-Burch, 2011)’s Egyptian online news commentary corpus, the Egyptian tweets used in the CS shared tasks (Solorio et al., 2014; Molina et al., 2016; Aguilar et al., 2018), portion of MSA Gigaword (Parker et al., 2011), and LDC Arabic tree bank corpora (MSA) (Maamouri et al., 2004; Diab et al., 2013). To identify the language id of MSA-EGY tokens, we use the Automatic Identification of Dialectal Arabic (AIDA2) tool (Al-Badrashiny et al., 2015) to perform token level language identification for the EGY and MSA tokens in context. Merged multilingual embeddings: We train a multilingual embedding for language pa"
W19-1410,W15-5936,0,0.654338,"Missing"
W19-1410,P16-1147,0,0.0528799,"Missing"
W19-1410,silveira-etal-2014-gold,0,0.0584863,"Missing"
W19-1410,D13-1141,0,0.0308701,"Missing"
W19-1410,W14-3907,1,0.851051,"SA-EGY We use the LDC Egyptian Arabic Treebanks 1-5 (ARZ1-5) (Maamouri et al., 2012). The ARZ1-5 data is from the discussion forums genre mostly in the Egyptian Arabic dialect (EGY). The total number of sentences in the corpus is 13,698 while the number of words is 174,967 words. To train pre-trained embeddings, we crawl Egyptian tweets from some of the Egyptian public figures’ Twitter accounts. The rest of the Egyptian raw textual data comes from the following sources: (Zaidan and Callison-Burch, 2011)’s Egyptian online news commentary corpus, the Egyptian tweets used in the CS shared tasks (Solorio et al., 2014; Molina et al., 2016; Aguilar et al., 2018), portion of MSA Gigaword (Parker et al., 2011), and LDC Arabic tree bank corpora (MSA) (Maamouri et al., 2004; Diab et al., 2013). To identify the language id of MSA-EGY tokens, we use the Automatic Identification of Dialectal Arabic (AIDA2) tool (Al-Badrashiny et al., 2015) to perform token level language identification for the EGY and MSA tokens in context. Merged multilingual embeddings: We train a multilingual embedding for language pairs that have one language in common (pivot language). To do so, we combine the corpora used to train the word e"
W19-1410,D08-1110,0,0.266355,"ntifiers for joint POS tagging and language identification; d) We present the first empirical evaluation on POS tagging with four different language pairs. All of the previous work focused on a single or two language pair combinations. 2 Related Work Developing CS text processing NLP techniques for analyzing user generated content as well as cater for needs of multilingual societies is vital (Vyas et al., 2014b). Previous studies that address the problem of POS tagging of CS data first attempt to identify the correct language of a word before feeding it into an appropriate monolingual tagger (Solorio and Liu, 2008; Vyas et al., 2014a; AlGhamdi et al., 2016). As is typically the case in NLP, such pipelines suffer from the problem of error propagation; e.g., failure of the language identification will cause problems in the POS tag prediction. Other approaches have trained supervised models on POS-annotated, CS data resources which are expensive to create and unavailable for most language pairs. (AlGhamdi et al., 2016; Solorio and Liu, 2008; Jamatia et al., 2015; Barman et al., 2016) Solorio and Liu (2008), proposed the first statistical approach to POS tagging 100 3 fiers. For the Boolean features, a bi-"
W19-1410,W18-3201,0,0.309463,"r and combining the outputs in a single multilingual prediction. The fourth combined condition involved training an SVM model using the output of the monolingual taggers. The three integrated approaches trained a monolingual state-of-the-art POS tagger on a) CS corpus; b) the union of two monolingual corpora of the languages included in the CS; c) the union of the corpora used in a and b. Both combined and integrated conditions outperformed the baseline systems. The SVM approach consistently outperformed the integrated approaches achieving the highest accuracy results for both language pairs. Soto and Hirschberg (2018) propose a new approach to POS tagging for code switching SPA-ENG language pair based on recurrent neural network (RNN) as a way of providing better tools for better code switching data processing, including POS taggers. The authors use an experimental approach of POS tagging for CS utterances entailing use of state-of-the-art bi-directional RNN to extensively study effects of language identiIn this paper, we address the problem of Partof-Speech tagging (POS) for CS data on the intrasentential level for multiple language pairs. We explore the effect of using various embeddings setups and multi"
W19-1410,petrov-etal-2012-universal,0,\N,Missing
W19-4606,J93-2003,0,0.0811821,"to be translated differently. We leverage an English MSA parallel corpus, where the MSA is diacritized in the Full-CM scheme using MADAMIRA (the same preprocessing step for CL described above). In this approach, diacritized variants that share the same English translations are considered unambiguous, whereas those that are typically translated to different English words are considered ambiguous. To that end, we first align the sentences at the token level and generate word translation probabilities using fast-align (Dyer et al., 2013), which is a log-linear reparameterization of IBM Model 2 (Brown et al., 1993). If a word shares any translation with its diacritized variant in the top N most likely translations, we consider it unambiguous (e.g. Ealam ÕÎ« ‘flag” 4.2 and Ealima ÕÎ « ‘learned” are unambiguous since they do not share top translations). Otherwise, the word is tagged as ambiguous. We tune N to include 1, 5, 10, and all translations. 4 Datasets Extrinsic Evaluation We evaluate the effectiveness of the proposed approaches using three applications: Semantic Textual Similarity (STS), Neural Machine Translation (NMT), and Part-of-Speech (POS) tagging. We used different significance testing meth"
W19-4606,L16-1640,1,0.85844,"for different number of clusters. For MULTI, SENSE, CR, we use a combination of four Modern Standard Arabic (MSA) datasets that vary in genre and domain and add up to ∼50M tokens: Gigaword 5th edition, distributed by Linguistic Data Consortium (LDC), Wikipedia dump 2016, Corpus of Contemporary Arabic (CCA) (Zaghouani et al., 2016a; Al-Sulaiti and Atwell, 2006), and LDC Arabic Tree Bank (ATB).10 For TR, we use an Arabic-English parallel dataset which includes ∼60M tokens and is created from 53 LDC catalogs. For data cleaning, we replace e-mails and URLs with a unified token and use SPLIT tool (Al-Badrashiny et al., 2016) to clean UTF8 characters (e.g. Latin and Chinese), remove diacritics in the original data, and separate punctuation, symbols, and numbers in the text, and replace them with separate unified tokens. We split long sentences (more than 150 words) by punctuation and then remove all sentences that are still longer than 150 words. We use D3 style (i.e. all affixes are separated) (Pasha et al., 2014) for Arabic tokenization without normalizing characters. For English, we lower case all characters and use TreeTagger (Schmid, 1999) for tokenization. We used SkipGram word embeddings (Mikolov et al., 20"
W19-4606,S17-2001,1,0.730386,"mbigDictDIAC), we evaluate their efficacy extrinsically on downstream applications. For all downstream applications, training and test data are preprocessed using MADAMIRA (Pasha et al., 2014) with the FULL-CM diacritization scheme where we only keep lexical diacritics.9 Then the data is filtered based on the AmbigDict of choice; namely, only word tokens in the text deemed ambiguous according to each AmbigDict maintain their full diacritics (as generated by MADAMIRA) while the unambiguous words are kept undiacritized. 4.2.1 Semantic Textual Similarity (STS) STS is a benchmark evaluation task (Cer et al., 2017), where the objective is to predict the similarity score between a pair of sentences. Performance is typically evaluated using the Pearson correlation coefficient against human judgments. We used the William test (Graham and Baldwin, 2014) for significance testing. We experiment with an unsupervised system based on matrix factorization developed by (Guo and Diab, 2012; Guo et al., 2014), which generates sentence embeddings from a word-sentence co-occurrence matrix, then compare them using cosine similarity.We use a dimension size of 700. To train the model, we use the 9 Full diacritics are inc"
W19-4606,P11-2031,0,0.0568651,"Missing"
W19-4606,2007.mtsummit-papers.20,1,0.871685,"he same spelling but may have different pronunciation and meaning. A diacritic is a mark that is added above, below, or within letters to indicate pronunciation, vowels, 1 We adopt Buckwalter Transliteration encoding into Latin script for rendering Arabic text http://www.qamus.org/transliteration.htm. 49 Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 49–59 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics full diacritic restoration results in increased sparsity and out-of-vocabulary words, which leads to degradation in performance (Diab et al., 2007; Alqahtani et al., 2016). The main objective of this work is to find a sweet spot between zero and full diacritization in order to reduce lexical ambiguity without increasing sparsity. We propose selective diacritization, a process of restoring diacritics to a subset of the words in a sentence sufficient to disambiguate homographs without significantly increasing sparsity. Selective diacritization can be viewed as a relaxed variant of word sense disambiguation since only homographs that arise from missing diacritics are disambiguated.2 Intrinsically evaluating the quality of a devised selecti"
W19-4606,2016.amta-researchers.15,1,0.771022,"t may have different pronunciation and meaning. A diacritic is a mark that is added above, below, or within letters to indicate pronunciation, vowels, 1 We adopt Buckwalter Transliteration encoding into Latin script for rendering Arabic text http://www.qamus.org/transliteration.htm. 49 Proceedings of the Fourth Arabic Natural Language Processing Workshop, pages 49–59 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics full diacritic restoration results in increased sparsity and out-of-vocabulary words, which leads to degradation in performance (Diab et al., 2007; Alqahtani et al., 2016). The main objective of this work is to find a sweet spot between zero and full diacritization in order to reduce lexical ambiguity without increasing sparsity. We propose selective diacritization, a process of restoring diacritics to a subset of the words in a sentence sufficient to disambiguate homographs without significantly increasing sparsity. Selective diacritization can be viewed as a relaxed variant of word sense disambiguation since only homographs that arise from missing diacritics are disambiguated.2 Intrinsically evaluating the quality of a devised selective diacritization scheme"
W19-4606,P02-1033,1,0.486698,"ymbols, and numbers in the text, and replace them with separate unified tokens. We split long sentences (more than 150 words) by punctuation and then remove all sentences that are still longer than 150 words. We use D3 style (i.e. all affixes are separated) (Pasha et al., 2014) for Arabic tokenization without normalizing characters. For English, we lower case all characters and use TreeTagger (Schmid, 1999) for tokenization. We used SkipGram word embeddings (Mikolov et al., 2013), where applicable. Translation-based Approaches (TR): Translation can be used as a basis for word sense induction (Diab and Resnik, 2002; Ng et al., 2003) since words across different languages tend to have disparate senses. Following a similar intuition, we use English translations from a parallel corpus as a trigger to divide the set of diacritic possibilities of a word into multiple subsets. The intuition here is that homographs worth disambiguating are those that are likely to be translated differently. We leverage an English MSA parallel corpus, where the MSA is diacritized in the Full-CM scheme using MADAMIRA (the same preprocessing step for CL described above). In this approach, diacritized variants that share the same"
W19-4606,P18-1128,0,0.037742,"Missing"
W19-4606,W06-3812,0,0.0343689,"ot. If an undiacritized word has more than one possible diacritic variant, we consider it ambiguous. We use this contextindependent approach as a baseline. Sense Induction Based Approach (SENSE): Selective diacritization is related to word sense disambiguation, however we only target disambiguation through diacritic restoration. Techniques used in automatic word sense induction can be used as a basis for identifying ambiguous words. Using undiacritized text, we apply an offthe-shelf system for word sense induction developed by Pelevina et al. (2017), which uses the Chinese Whispers algorithm (Biemann, 2006) to identify senses of a graph constructed by computing the word similarities (highest cosine similarities) through using word as well as context embeddings. We apply the first three steps described in Pelevina et al. (2017) but we do not use the generated sense-based embeddings; we only use the system to identify the words with multiplw senses. We set the three parameters as follows: the graph size N to 200, the inventory granularity n to 400, and the minimum number of clusters (senses) k to 5.5 A word type is deemed ambiguous if it appears 5 AmbigDict-DIAC Generation tic diacritics and are m"
W19-4606,N13-1073,0,0.0112134,"uition here is that homographs worth disambiguating are those that are likely to be translated differently. We leverage an English MSA parallel corpus, where the MSA is diacritized in the Full-CM scheme using MADAMIRA (the same preprocessing step for CL described above). In this approach, diacritized variants that share the same English translations are considered unambiguous, whereas those that are typically translated to different English words are considered ambiguous. To that end, we first align the sentences at the token level and generate word translation probabilities using fast-align (Dyer et al., 2013), which is a log-linear reparameterization of IBM Model 2 (Brown et al., 1993). If a word shares any translation with its diacritized variant in the top N most likely translations, we consider it unambiguous (e.g. Ealam ÕÎ« ‘flag” 4.2 and Ealima ÕÎ « ‘learned” are unambiguous since they do not share top translations). Otherwise, the word is tagged as ambiguous. We tune N to include 1, 5, 10, and all translations. 4 Datasets Extrinsic Evaluation We evaluate the effectiveness of the proposed approaches using three applications: Semantic Textual Similarity (STS), Neural Machine Translation (NMT),"
W19-4606,Q17-1010,0,0.0824416,"Missing"
W19-4606,W17-1907,0,0.11706,"sed data-driven methods for the automatic identification of ambiguous words; 3. We evaluate and analyze the impact of partial sense disambiguation (i.e. selective diacritic restoration of identified homographs) in downstream applications for MSA. 2 Related Work We are concerned mainly with studies that target word disambiguation through the use of diacritics/accents restoration. Homograph disambiguation through accents has been explored previously in several studies with the use of different rulebased and machine-learning approaches for languages such as Arabic, Spanish, Igbo, and Vietnamese (Ezeani et al., 2017; Nguyen et al., 2012; Nivre et al., 2017; Said et al., 2013; Tufis¸ and Chit¸u, 1999). Bouamor et al. (2015) conducted a pilot study where they asked human annotators to add the minimum number of diacritics sufficient to disambiguate homographs. However, attempts to provide human annotation for selective diacritization resulted in low inter-annotator agreement due to the annotators’ subjectivity and different linguistic understanding of the words and contexts (Bouamor et al., 2015; Zaghouani et al., 2016b). To address this issue, Zaghouani et al. (2016b) used a morphological disambiguation to"
W19-4606,I17-1070,0,0.0543563,"Missing"
W19-4606,D14-1020,0,0.0167228,"where we only keep lexical diacritics.9 Then the data is filtered based on the AmbigDict of choice; namely, only word tokens in the text deemed ambiguous according to each AmbigDict maintain their full diacritics (as generated by MADAMIRA) while the unambiguous words are kept undiacritized. 4.2.1 Semantic Textual Similarity (STS) STS is a benchmark evaluation task (Cer et al., 2017), where the objective is to predict the similarity score between a pair of sentences. Performance is typically evaluated using the Pearson correlation coefficient against human judgments. We used the William test (Graham and Baldwin, 2014) for significance testing. We experiment with an unsupervised system based on matrix factorization developed by (Guo and Diab, 2012; Guo et al., 2014), which generates sentence embeddings from a word-sentence co-occurrence matrix, then compare them using cosine similarity.We use a dimension size of 700. To train the model, we use the 9 Full diacritics are included except inflectional diacritics that reflect the syntactic positions of words within sentences but do not alter meaning. 10 53 Parts 1, 2, 3, 5, 6, 7, 10, 11, and 12 Arabic dataset released for SemEval-2017 task 1 (Cer et al., 2017)."
W19-4606,P12-1091,1,0.736915,"deemed ambiguous according to each AmbigDict maintain their full diacritics (as generated by MADAMIRA) while the unambiguous words are kept undiacritized. 4.2.1 Semantic Textual Similarity (STS) STS is a benchmark evaluation task (Cer et al., 2017), where the objective is to predict the similarity score between a pair of sentences. Performance is typically evaluated using the Pearson correlation coefficient against human judgments. We used the William test (Graham and Baldwin, 2014) for significance testing. We experiment with an unsupervised system based on matrix factorization developed by (Guo and Diab, 2012; Guo et al., 2014), which generates sentence embeddings from a word-sentence co-occurrence matrix, then compare them using cosine similarity.We use a dimension size of 700. To train the model, we use the 9 Full diacritics are included except inflectional diacritics that reflect the syntactic positions of words within sentences but do not alter meaning. 10 53 Parts 1, 2, 3, 5, 6, 7, 10, 11, and 12 Arabic dataset released for SemEval-2017 task 1 (Cer et al., 2017). Since the training dataset is small, we augment it by randomly selecting sentences from the dataset (∼1,655,922) described in Secti"
W19-4606,C14-1047,1,0.802019,"ording to each AmbigDict maintain their full diacritics (as generated by MADAMIRA) while the unambiguous words are kept undiacritized. 4.2.1 Semantic Textual Similarity (STS) STS is a benchmark evaluation task (Cer et al., 2017), where the objective is to predict the similarity score between a pair of sentences. Performance is typically evaluated using the Pearson correlation coefficient against human judgments. We used the William test (Graham and Baldwin, 2014) for significance testing. We experiment with an unsupervised system based on matrix factorization developed by (Guo and Diab, 2012; Guo et al., 2014), which generates sentence embeddings from a word-sentence co-occurrence matrix, then compare them using cosine similarity.We use a dimension size of 700. To train the model, we use the 9 Full diacritics are included except inflectional diacritics that reflect the syntactic positions of words within sentences but do not alter meaning. 10 53 Parts 1, 2, 3, 5, 6, 7, 10, 11, and 12 Arabic dataset released for SemEval-2017 task 1 (Cer et al., 2017). Since the training dataset is small, we augment it by randomly selecting sentences from the dataset (∼1,655,922) described in Section 4.1 where the ch"
W19-4606,P02-1040,0,0.104296,"Missing"
W19-4606,pasha-etal-2014-madamira,1,0.921225,"Missing"
W19-4606,D17-1035,0,0.0675274,"Missing"
W19-4606,2012.eamt-1.60,0,0.0707683,"Missing"
W19-4606,P03-1058,0,0.0571857,"the text, and replace them with separate unified tokens. We split long sentences (more than 150 words) by punctuation and then remove all sentences that are still longer than 150 words. We use D3 style (i.e. all affixes are separated) (Pasha et al., 2014) for Arabic tokenization without normalizing characters. For English, we lower case all characters and use TreeTagger (Schmid, 1999) for tokenization. We used SkipGram word embeddings (Mikolov et al., 2013), where applicable. Translation-based Approaches (TR): Translation can be used as a basis for word sense induction (Diab and Resnik, 2002; Ng et al., 2003) since words across different languages tend to have disparate senses. Following a similar intuition, we use English translations from a parallel corpus as a trigger to divide the set of diacritic possibilities of a word into multiple subsets. The intuition here is that homographs worth disambiguating are those that are likely to be translated differently. We leverage an English MSA parallel corpus, where the MSA is diacritized in the Full-CM scheme using MADAMIRA (the same preprocessing step for CL described above). In this approach, diacritized variants that share the same English translatio"
W19-4606,P94-1013,0,0.243087,"schemes based on linguistic understanding; crucially these partial diacritic schemes are applied to all words in a sentence.4 Our devised strategies differ in that we apply full diacritization to a select set of tokens in the text. Our work is related to these previous studies in the sense that we reduce the search space of candidate words that could benefit from full or partial diacritization without increasing sparsity. Furthermore, the novelty of this work lies in utilizing automatic unsupervised methods to identify such words. context, which makes it harder to generalize to new datasets. Yarowsky (1994) developed an accent restoration algorithm for Spanish and French that specifies the accent patterns for ambiguous words (i.e. multiple accent patterns). Our intuition is different than that of Yarowsky (1994) in two ways. First, they added diacritics to all words that have more than one diacritic pattern while we add the diacritics for only a subset of candidate words. Second, they used context for adding diacritics, while we use context to isolate words that require diacritics, for which we apply an off-the-shelf diacritic restoration model. Rather than restoring all diacritics in the writte"
W19-4606,L16-1577,1,0.907837,"Missing"
W19-4606,W16-4115,1,0.910877,"Missing"
W19-4606,J92-4003,0,\N,Missing
W19-4606,W15-3209,1,\N,Missing
W19-4606,W16-1620,0,\N,Missing
W19-4606,P17-4012,0,\N,Missing
W19-4606,L16-1262,0,\N,Missing
