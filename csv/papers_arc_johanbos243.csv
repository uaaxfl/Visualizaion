2021.gem-1.8,Evaluating Text Generation from Discourse Representation Structures,2021,-1,-1,4,0,6243,chunliu wang,"Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)",0,"We present an end-to-end neural approach to generate English sentences from formal meaning representations, Discourse Representation Structures (DRSs). We use a rather standard bi-LSTM sequence-to-sequence model, work with a linearized DRS input representation, and evaluate character-level and word-level decoders. We obtain very encouraging results in terms of reference-based automatic metrics such as BLEU. But because such metrics only evaluate the surface level of generated output, we develop a new metric, ROSE, that targets specific semantic phenomena. We do this with five DRS generation challenge sets focusing on tense, grammatical number, polarity, named entities and quantities. The aim of these challenge sets is to assess the neural generator{'}s systematicity and generalization to unseen inputs."
2021.acl-short.97,Input Representations for Parsing Discourse Representation Structures: Comparing {E}nglish with {C}hinese,2021,-1,-1,4,0,6243,chunliu wang,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Neural semantic parsers have obtained acceptable results in the context of parsing DRSs (Discourse Representation Structures). In particular models with character sequences as input showed remarkable performance for English. But how does this approach perform on languages with a different writing system, like Chinese, a language with a large vocabulary of characters? Does rule-based tokenisation of the input help, and which granularity is preferred: characters, or words? The results are promising. Even with DRSs based on English, good results for Chinese are obtained. Tokenisation offers a small advantage for English, but not for Chinese. Overall, characters are preferred as input, both for English and Chinese."
2020.lrec-1.35,{MAGPIE}: A Large Corpus of Potentially Idiomatic Expressions,2020,-1,-1,2,1,16657,hessel haagsma,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Given the limited size of existing idiom corpora, we aim to enable progress in automatic idiom processing and linguistic analysis by creating the largest-to-date corpus of idioms for English. Using a fixed idiom list, automatic pre-extraction, and a strictly controlled crowdsourced annotation procedure, we show that it is feasible to build a high-quality corpus comprising more than 50K instances, an order of a magnitude larger than previous resources. Crucial ingredients of crowdsourcing were the selection of crowdworkers, clear and comprehensive instructions, and an interface that breaks down the task in small, manageable steps. Analysis of the resulting corpus revealed strong effects of genre on idiom distribution, providing new evidence for existing theories on what influences idiom usage. The corpus also contains rich metadata, and is made publicly available."
2020.emnlp-main.371,Character-level Representations Improve {DRS}-based Semantic Parsing Even in the Age of {BERT},2020,-1,-1,3,1,6244,rik noord,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We combine character-level and contextual language model representations to improve performance on Discourse Representation Structure parsing. Character representations can easily be added in a sequence-to-sequence model in either one encoder or as a fully separate encoder, with improvements that are robust to different language models, languages and data sets. For English, these improvements are larger than adding individual sources of linguistic information or adding non-contextual embeddings. A new method of analysis based on semantic tags demonstrates that the character-level representations improve performance across a subset of selected semantic phenomena."
2020.dmr-1.2,Separating Argument Structure from Logical Structure in {AMR},2020,-1,-1,1,1,6245,johan bos,Proceedings of the Second International Workshop on Designing Meaning Representations,0,"The AMR (Abstract Meaning Representation) formalism for representing meaning of natural language sentences puts emphasis on predicate-argument structure and was not designed to deal with scope and quantifiers. By extending AMR with indices for contexts and formulating constraints on these contexts, a formalism is derived that makes correct predictions for inferences involving negation and bound variables. The attractive core predicate-argument structure of AMR is preserved. The resulting framework is similar to the meaning representations of Discourse Representation Theory employed in the Parallel Meaning Bank."
2020.conll-shared.1,{MRP} 2020: The Second Shared Task on Cross-Framework and Cross-Lingual Meaning Representation Parsing,2020,-1,-1,4,0,2623,stephan oepen,Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing,0,"The 2020 Shared Task at the Conference for Computational Language Learning (CoNLL) was devoted to Meaning Representation Parsing (MRP) across frameworks and languages. Extending a similar setup from the previous year, five distinct approaches to the representation of sentence meaning in the form of directed graphs were represented in the English training and evaluation data for the task, packaged in a uniform graph abstraction and serialization; for four of these representation frameworks, additional training and evaluation data was provided for one additional language per framework. The task received submissions from eight teams, of which two do not participate in the official ranking because they arrived after the closing deadline or made use of additional training data. All technical information regarding the task, including system submissions, official results, and links to supporting resources and software are available from the task web site at: http://mrp.nlpl.eu"
2020.conll-shared.2,{DRS} at {MRP} 2020: Dressing up Discourse Representation Structures as Graphs,2020,-1,-1,2,1,14527,lasha abzianidze,Proceedings of the CoNLL 2020 Shared Task: Cross-Framework Meaning Representation Parsing,0,"Discourse Representation Theory (DRT) is a formal account for representing the meaning of natural language discourse. Meaning in DRT is modeled via a Discourse Representation Structure (DRS), a meaning representation with a model-theoretic interpretation, which is usually depicted as nested boxes. In contrast, a directed labeled graph is a common data structure used to encode semantics of natural language texts. The paper describes the procedure of dressing up DRSs as directed labeled graphs to include DRT as a new framework in the 2020 shared task on Cross-Framework and Cross-Lingual Meaning Representation Parsing. Since one of the goals of the shared task is to encourage unified models for several semantic graph frameworks, the conversion procedure was biased towards making the DRT graph framework somewhat similar to other graph-based meaning representation frameworks."
W19-4804,Can Neural Networks Understand Monotonicity Reasoning?,2019,33,1,7,0,7510,hitomi yanaka,Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,0,"Monotonicity reasoning is one of the important reasoning skills for any intelligent natural language inference (NLI) model in that it requires the ability to capture the interaction between lexical and syntactic structures. Since no test set has been developed for monotonicity reasoning with wide coverage, it is still unclear whether neural models can perform monotonicity reasoning in a proper way. To investigate this issue, we introduce the Monotonicity Entailment Dataset (MED). Performance by state-of-the-art NLI models on the new test set is substantially worse, under 55{\%}, especially on downward reasoning. In addition, analysis using a monotonicity-driven data augmentation method showed that these models might be limited in their generalization ability in upward and downward reasoning."
W19-4005,{CCG}web: a New Annotation Tool and a First Quadrilingual {CCG} Treebank,2019,0,0,3,1,14274,kilian evang,Proceedings of the 13th Linguistic Annotation Workshop,0,"We present the first open-source graphical annotation tool for combinatory categorial grammar (CCG), and the first set of detailed guidelines for syntactic annotation with CCG, for four languages: English, German, Italian, and Dutch. We also release a parallel pilot CCG treebank based on these guidelines, with 4x100 adjudicated sentences, 10K single-annotator fully corrected sentences, and 82K single-annotator partially corrected sentences."
W19-3302,Thirty Musts for Meaning Banking,2019,-1,-1,2,1,14527,lasha abzianidze,Proceedings of the First International Workshop on Designing Meaning Representations,0,"Meaning banking{---}creating a semantically annotated corpus for the purpose of semantic parsing or generation{---}is a challenging task. It is quite simple to come up with a complex meaning representation, but it is hard to design a simple meaning representation that captures many nuances of meaning. This paper lists some lessons learned in nearly ten years of meaning annotation during the development of the Groningen Meaning Bank (Bos et al., 2017) and the Parallel Meaning Bank (Abzianidze et al., 2017). The paper{'}s format is rather unconventional: there is no explicit related work, no methodology section, no results, and no discussion (and the current snippet is not an abstract but actually an introductory preface). Instead, its structure is inspired by work of Traum (2000) and Bender (2013). The list starts with a brief overview of the existing meaning banks (Section 1) and the rest of the items are roughly divided into three groups: corpus collection (Section 2 and 3, annotation methods (Section 4{--}11), and design of meaning representations (Section 12{--}30). We hope this overview will give inspiration and guidance in creating improved meaning banks in the future"
W19-1201,The First Shared Task on Discourse Representation Structure Parsing,2019,0,2,4,1,14527,lasha abzianidze,Proceedings of the {IWCS} Shared Task on Semantic Parsing,0,"The paper presents the IWCS 2019 shared task on semantic parsing where the goal is to produce Discourse Representation Structures (DRSs) for English sentences. DRSs originate from Discourse Representation Theory and represent scoped meaning representations that capture the semantics of negation, modals, quantification, and presupposition triggers. Additionally, concepts and event-participants in DRSs are described with WordNet synsets and the thematic roles from VerbNet. To measure similarity between two DRSs, they are represented in a clausal form, i.e. as a set of tuples. Participant systems were expected to produce DRSs in this clausal form. Taking into account the rich lexical information, explicit scope marking, a high number of shared variables among clauses, and highly-constrained format of valid DRSs, all these makes the DRS parsing a challenging NLP task. The results of the shared task displayed improvements over the existing state-of-the-art parser."
W19-0504,Linguistic Information in Neural Semantic Parsing with Multiple Encoders,2019,0,0,3,1,6244,rik noord,Proceedings of the 13th International Conference on Computational Semantics - Short Papers,0,"Recently, sequence-to-sequence models have achieved impressive performance on a number of semantic parsing tasks. However, they often do not exploit available linguistic resources, while these, when employed correctly, are likely to increase performance even further. Research in neural machine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders."
S19-1027,{HELP}: A Dataset for Identifying Shortcomings of Neural Models in Monotonicity Reasoning,2019,24,3,7,0,7510,hitomi yanaka,Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*{SEM} 2019),0,"Large crowdsourced datasets are widely used for training and evaluating neural models on natural language inference (NLI). Despite these efforts, neural models have a hard time capturing logical inferences, including those licensed by phrase replacements, so-called monotonicity reasoning. Since no large dataset has been developed for monotonicity reasoning, it is still unclear whether the main obstacle is the size of datasets or the model architectures themselves. To investigate this issue, we introduce a new dataset, called HELP, for handling entailments with lexical and logical phenomena. We add it to training data for the state-of-the-art neural models and evaluate them on test sets for monotonicity phenomena. The results showed that our data augmentation improved the overall accuracy. We also find that the improvement is better on monotonicity inferences with lexical replacements than on downward inferences with disjunction and modification. This suggests that some types of inferences can be improved by our data augmentation while others are immune to it."
W18-4919,The Other Side of the Coin: Unsupervised Disambiguation of Potentially Idiomatic Expressions by Contrasting Senses,2018,0,0,3,1,16657,hessel haagsma,"Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions ({LAW}-{MWE}-{C}x{G}-2018)",0,"Disambiguation of potentially idiomatic expressions involves determining the sense of a potentially idiomatic expression in a given context, e.g. determining that make hay in {`}Investment banks made hay while takeovers shone.{'} is used in a figurative sense. This enables automatic interpretation of idiomatic expressions, which is important for applications like machine translation and sentiment analysis. In this work, we present an unsupervised approach for English that makes use of literalisations of idiom senses to improve disambiguation, which is based on the lexical cohesion graph-based method by Sporleder and Li (2009). Experimental results show that, while literalisation carries novel information, its performance falls short of that of state-of-the-art unsupervised methods."
Q18-1043,Exploring Neural Methods for Parsing Discourse Representation Structures,2018,0,7,4,1,6244,rik noord,Transactions of the Association for Computational Linguistics,0,"Neural methods have had several recent successes in semantic parsing, though they have yet to face the challenge of producing meaning representations based on formal semantics. We present a sequence-to-sequence neural semantic parser that is able to produce Discourse Representation Structures (DRSs) for English sentences with high accuracy, outperforming traditional DRS parsers. To facilitate the learning of the output, we represent DRSs as a sequence of flat clauses and introduce a method to verify that produced DRSs are well-formed and interpretable. We compare models using characters and words as input and see (somewhat surprisingly) that the former performs better than the latter. We show that eliminating variable names from the output using De Bruijn indices increases parser performance. Adding silver training data boosts performance even further."
L18-1267,Evaluating Scoped Meaning Representations,2018,0,2,4,1,6244,rik noord,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Semantic parsing offers many opportunities to improve natural language understanding. We present a semantically annotated parallel corpus for English, German, Italian, and Dutch where sentences are aligned with scoped meaning representations in order to capture the semantics of negation, modals, quantification, and presupposition triggers. The semantic formalism is based on Discourse Representation Theory, but concepts are represented by WordNet synsets and thematic roles by VerbNet relations. Translating scoped meaning representations to sets of clauses enables us to compare them for the purpose of semantic parser evaluation and checking translations. This is done by computing precision and recall on matching clauses, in a similar way as is done for Abstract Meaning Representations. We show that our matching tool for evaluating scoped meaning representations is both accurate and efficient. Applying this matching tool to three baseline semantic parsers yields F-scores between 43% and 54%. A pilot study is performed to automatically find changes in meaning by comparing meaning representations of translations. This comparison turns out to be an additional way of (i) finding annotation mistakes and (ii) finding instances where our semantic analysis needs to be improved."
D18-1526,What can we learn from Semantic Tagging?,2018,26,1,5,0,10900,mostafa abdou,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We investigate the effects of multi-task learning using the recently introduced task of semantic tagging. We employ semantic tagging as an auxiliary task for three different NLP tasks: part-of-speech tagging, Universal Dependency parsing, and Natural Language Inference. We compare full neural network sharing, partial neural network sharing, and what we term the learning what to share setting where negative transfer between tasks is less likely. Our findings show considerable improvements for all tasks, particularly in the learning what to share setting which shows consistent gains across all tasks."
W17-7306,Dealing with Co-reference in Neural Semantic Parsing,2017,8,5,2,1,6244,rik noord,Proceedings of the 2nd Workshop on Semantic Deep Learning ({S}em{D}eep-2),0,"Linguistic phenomena like pronouns, control constructions, or co-reference give rise to co-indexed variables in meaning representations. We review three different methods for dealing with co-indexed variables in the output of neural semantic parsing of abstract meaning representations: (a) copying concepts during training and restoring co-indexation in a post-processing step; (b) explicit indexing of co-indexation; and (c) using absolute paths to designate co-indexing. The second method gives the best results and outperforms the baseline by 2.9 F-score points."
W17-6901,Towards Universal Semantic Tagging,2017,0,8,2,1,14527,lasha abzianidze,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,"The paper proposes the task of universal semantic tagging---tagging word tokens with language-neutral, semantically informative tags. We argue that the task, with its independent nature, contributes to better semantic analysis for wide-coverage multilingual text. We present the initial version of the semantic tagset and show that (a) the tags provide semantically fine-grained information, and (b) they are suitable for cross-lingual semantic parsing. An application of the semantic tagging in the Parallel Meaning Bank supports both of these points as the tags contribute to formal lexical semantics and their cross-lingual projection. As a part of the application, we annotate a small corpus with the semantic tags and present new baseline result for universal semantic tagging."
W17-6905,Indexicals and Compositionality: Inside-Out or Outside-In?,2017,5,1,1,1,6245,johan bos,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
W17-1805,Meaning Banking beyond Events and Roles,2017,-1,-1,1,1,6245,johan bos,Proceedings of the Workshop Computational Semantics Beyond Events and Roles,0,"In this talk I will discuss the analysis of several semantic phenomena that need meaning representations that can describe attributes of propositional contexts. I will do this in a version of Discourse Representation Theory, using a universal semantic tagset developed as part of a project that aims to produce a large meaning bank (a semantically-annotated corpus) for four languages (English, Dutch, German and Italian)."
S17-2160,The Meaning Factory at {S}em{E}val-2017 Task 9: Producing {AMR}s with Neural Semantic Parsing,2017,0,2,2,1,6244,rik noord,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"We evaluate a semantic parser based on a character-based sequence-to-sequence model in the context of the SemEval-2017 shared task on semantic parsing for AMRs. With data augmentation, super characters, and POS-tagging we gain major improvements in performance compared to a baseline character-level model. Although we improve on previous character-based neural semantic parsing models, the overall accuracy is still lower than a state-of-the-art AMR parser. An ensemble combining our neural semantic parser with an existing, traditional parser, yields a small gain in performance."
E17-2039,The {P}arallel {M}eaning {B}ank: Towards a Multilingual Corpus of Translations Annotated with Compositional Meaning Representations,2017,0,25,8,1,14527,lasha abzianidze,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"The Parallel Meaning Bank is a corpus of translations annotated with shared, formal meaning representations comprising over 11 million words divided over four languages (English, German, Italian, and Dutch). Our approach is based on cross-lingual projection: automatically produced (and manually corrected) semantic annotations for English sentences are mapped onto their word-aligned translations, assuming that the translations are meaning-preserving. The semantic annotation consists of five main steps: (i) segmentation of the text in sentences and lexical items; (ii) syntactic parsing with Combinatory Categorial Grammar; (iii) universal semantic tagging; (iv) symbolization; and (v) compositional semantic analysis based on Discourse Representation Theory. These steps are performed using statistical models trained in a semi-supervised manner. The employed annotation models are all language-neutral. Our first results are promising."
W16-3202,Combining Lexical and Spatial Knowledge to Predict Spatial Relations between Objects in Images,2016,0,3,2,0,14261,manuela hurlimann,Proceedings of the 5th Workshop on Vision and Language,0,The first author was supported by the Erasmus Mundus Programme in Language and Communication Technologies (EM LCT) and partly by the SSIX Horizon 2020 project (grant agreement No 645425) and Science Foundation Ireland (SFI) under Grant Number SFI/12/RC/2289.
S16-1182,The Meaning Factory at {S}em{E}val-2016 Task 8: Producing {AMR}s with Boxer,2016,11,7,2,0.666667,996,johannes bjerva,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,"We participated in the shared task on meaning representation parsing (Task 8 at SemEval-2016) with the aim of investigating whether we could use Boxer, an existing open-domain semantic parser, for this task. However, the meaning representations produced by Boxer, Discourse Representation Structures, are considerably different from Abstract Meaning Representations, AMRs, the target meaning representations of the shared task. Our hybrid conversion method (involving lexical adaptation as well as post-processing of the output) failed to produce state-of-the-art results. Nonetheless, F-scores of 53% on development and 47% on test data (50% unofficially) were obtained."
J16-3006,{S}quib: Expressive Power of {A}bstract {M}eaning {R}epresentations,2016,16,14,1,1,6245,johan bos,Computational Linguistics,0,"The syntax of abstract meaning representations AMRs can be defined recursively, and a systematic translation to first-order logic FOL can be specified, including a proper treatment of negation. AMRs without recurrent variables are in the decidable two-variable fragment of FOL. The current definition of AMRs has limited expressive power for universal quantification up to one universal quantifier per sentence. A simple extension of the AMR syntax and translation to FOL provides the means to represent projection and scope phenomena."
C16-1056,Cross-lingual Learning of an Open-domain Semantic Parser,2016,43,5,2,1,14274,kilian evang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We propose a method for learning semantic CCG parsers by projecting annotations via a parallel corpus. The method opens an avenue towards cheaply creating multilingual semantic parsers mapping open-domain text to formal meaning representations. A first cross-lingually learned Dutch (from English) semantic parser obtains f-scores ranging from 42.99{\%} to 69.22{\%} depending on the level of label informativity taken into account, compared to 58.40{\%} to 78.88{\%} for the underlying source-language system. These are promising numbers compared to state-of-the-art semantic parsing in open domains."
C16-1333,Semantic Tagging with Deep Residual Networks,2016,18,31,3,0.666667,996,johannes bjerva,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"We propose a novel semantic tagging task, semtagging, tailored for the purpose of multilingual semantic parsing, and present the first tagger using deep residual networks (ResNets). Our tagger uses both word and character representations, and includes a novel residual bypass architecture. We evaluate the tagset both intrinsically on the new task of semantic tagging, as well as on Part-of-Speech (POS) tagging. Our system, consisting of a ResNet and an auxiliary loss function predicting our semantic tags, significantly outperforms prior results on English Universal Dependencies POS tagging (95.71{\%} accuracy on UD v1.2 and 95.67{\%} accuracy on UD v1.3)."
W15-1832,Uncovering Noun-Noun Compound Relations by Gamification,2015,7,4,1,1,6245,johan bos,Proceedings of the 20th Nordic Conference of Computational Linguistics ({NODALIDA} 2015),0,"Can relations described by English nounnoun compounds be adequately captured by prepositions? We attempt to answer this question in a data-driven way, using gamification to annotate a set of about a thousand noun-noun compound examples. Annotators could make a choice out of five prepositions generated with the help of paraphrases found in the Google ngram corpus. We show that there is substantial agreement among the players of our linguistic annotation game, and that their answers differ in about 50% of raw frequency counts of the Google n-gram corpus. Prepositions can be used to describe the majority of the implicit relations present in noun-noun compounds, but not all relations are captured by natural prepositions and some compounds are not easy to paraphrase with the use of a preposition."
W15-1841,Open-Domain Semantic Parsing with Boxer,2015,17,17,1,1,6245,johan bos,Proceedings of the 20th Nordic Conference of Computational Linguistics ({NODALIDA} 2015),0,"Boxer is a semantic parser for English texts with many input and output possibilities, and various ways to perform meaning analysis based on Discourse Representation Theory. This involves the various ways that meaning representations can be computed, as well as their possible semantic ingredients."
P15-1146,Adding Semantics to Data-Driven Paraphrasing,2015,39,24,2,0,7335,ellie pavlick,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We add an interpretable semantics to the paraphrase database (PPDB). To date, the relationship between phrase pairs in the database has been weakly defined as approximately equivalent. We show that these pairs represent a variety of relations, including directed entailment (little girl/girl) and exclusion (nobody/someone). We automatically assign semantic entailment relations to entries in PPDB using features derived from past work on discovering inference rules from text and semantic taxonomy induction. We demonstrate that our model assigns these relations with high accuracy. In a downstream RTE task, our labels rival relations from WordNet and improve the coverage of a proof-based RTE system by 17%."
S14-2084,{R}o{B}ox: {CCG} with Structured Perceptron for Supervised Semantic Parsing of Robotic Spatial Commands,2014,5,4,2,1,14274,kilian evang,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"We use a Combinatory Categorial Grammar (CCG) parser with a structured perceptron learner to address Shared Task 6 of SemEval-2014, Supervised Semantic Parsing of Robotic Spatial Commands. Our system reaches an accuracy of 79% ignoring spatial context and 87% using the spatial planner, showing that CCG can successfully be applied to the task."
S14-2114,The Meaning Factory: Formal Semantics for Recognizing Textual Entailment and Determining Semantic Similarity,2014,19,54,2,0.666667,996,johannes bjerva,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"Shared Task 1 of SemEval-2014 comprised two subtasks on the same dataset of sentence pairs: recognizing textual entailment and determining textual similarity. We used an existing system based on formal semantics and logical inference to participate in the first subtask, reaching an accuracy of 82%, ranking in the top 5 of more than twenty participating systems. For determining semantic similarity we took a supervised approach using a variety of features, the majority of which was produced by our system for recognizing textual entailment. In this subtask our system achieved a mean squared error of 0.322, the best of all participating systems."
2014.lilt-9.3,Is there a place for logic in recognizing textual entailment,2014,-1,-1,1,1,6245,johan bos,"Linguistic Issues in Language Technology, Volume 9, 2014 - Perspectives on Semantic Representations for Textual Inference",0,"From a purely theoretical point of view, it makes sense to approach recognizing textual entailment (RTE) with the help of logic. After all, entailment matters are all about logic. In practice, only few RTE systems follow the bumpy road from words to logic. This is probably because it requires a combination of robust, deep semantic analysis and logical inference{---}and why develop something with this complexity if you perhaps can get away with something simpler? In this article, with the help of an RTE system based on Combinatory Categorial Grammar, Discourse Representation Theory, and first-order theorem proving, we make an empirical assessment of the logic-based approach. High precision paired with low recall is a key characteristic of this system. The bottleneck in achieving high recall is the lack of a systematic way to produce relevant background knowledge. There is a place for logic in RTE, but it is (still) overshadowed by the knowledge acquisition problem."
W13-3802,The {G}roningen Meaning Bank,2013,-1,-1,1,1,6245,johan bos,Proceedings of the Joint Symposium on Semantic Processing. Textual Inference and Structures in Corpora,0,None
W13-2101,Aligning Formal Meaning Representations with Surface Strings for Wide-Coverage Text Generation,2013,20,5,2,1,7,valerio basile,Proceedings of the 14th {E}uropean Workshop on Natural Language Generation,0,"Statistical natural language generation from abstract meaning representations presupposes large corpora consisting of textxe2x80x90meaning pairs. Even though such corpora exist nowadays, or could be constructed using robust semantic parsing, the simple alignment between text and meaning representation is too coarse for developing robust (statistical) NLG systems. By reformatting semantic representations as graphs, fine-grained alignment can be obtained. Given a precise alignment at the word level, the complete surface form of a meaning representations can be deduced using a simple declarative rule."
W13-0203,Scope Disambiguation as a Tagging Task,2013,15,1,2,1,14274,kilian evang,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Short Papers,0,"In this paper we present a pragmatic account of scope alternation involving universal quantifiers in a lexicalist framework based on CCG and DRT. This account can derive the desired reading for 96% of all cases of scope interaction involving universal quantification mediated by prepositions in a real corpus. We show how this account allows for recasting scope resolution as a simple token classification task, providing a simpler handle for statistical approaches to scope resolution than previous accounts."
W13-0215,Gamification for Word Sense Labeling,2013,9,33,4,0,41161,noortje venhuizen,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Short Papers,0,"Obtaining gold standard data for word sense disambiguation is important but costly. We show how it can be done using a xe2x80x9cGame with a Purposexe2x80x9d (GWAP) called Wordrobe. This game consists of a large set of multiple-choice questions on word senses generated from the Groningen Meaning Bank. The players need to answer these questions, scoring points depending on the agreement with fellow players. The working assumption is that the right sense for a word can be determined by the answers given by the players. To evaluate our method, we gold-standard tagged a portion of the data that was also used in the GWAP. A comparison yielded promising results, ranging from a precision of 0.88 and recall of 0.83 for relative majority agreement, to a precision of 0.98 and recall of 0.35 for questions that were answered unanimously."
W13-0122,Parsimonious Semantic Representations with Projection Pointers,2013,-1,-1,2,0,41161,noortje venhuizen,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,None
D13-1146,{E}lephant: Sequence Labeling for Word and Sentence Segmentation,2013,19,22,4,1,14274,kilian evang,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Tokenization is widely regarded as a solved problem due to the high accuracy that rulebased tokenizers achieve. But rule-based tokenizers are hard to maintain and their rules language specific. We show that highaccuracy word and sentence segmentation can be achieved by using supervised sequence labeling on the character level combined with unsupervised feature learning. We evaluated our method on three languages and obtained error rates of 0.27 xe2x80xb0 (English), 0.35 xe2x80xb0 (Dutch) and 0.76 xe2x80xb0 (Italian) for our best models."
W12-0607,Predicting the 2011 {D}utch Senate Election Results with {T}witter,2012,-1,-1,2,0,16268,erik sang,Proceedings of the Workshop on Semantic Analysis in Social Media,0,None
S12-1040,{UG}roningen: Negation detection with Discourse Representation Structures,2012,10,7,2,1,7,valerio basile,"*{SEM} 2012: The First Joint Conference on Lexical and Computational Semantics {--} Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation ({S}em{E}val 2012)",0,"We use the NLP toolchain that is used to construct the Groningen Meaning Bank to address the task of detecting negation cue and scope, as defined in the shared task Resolving the Scope and Focus of Negation. This toolchain applies the C&C tools for parsing, using the formalism of Combinatory Categorial Grammar, and applies Boxer to produce semantic representations in the form of Discourse Representation Structures (DRSs). For negation cue detection, the DRSs are converted to flat, non-recursive structures, called Discourse Representation Graphs (DRGs). DRGs simplify cue detection by means of edge labels representing relations. Scope detection is done by gathering the tokens that occur within the scope of a negated DRS. The result is a system that is fairly reliable for cue detection and scope detection. Furthermore, it provides a fairly robust algorithm for detecting the negated event or property within the scope."
basile-etal-2012-developing,Developing a large semantically annotated corpus,2012,19,71,2,1,7,valerio basile,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"What would be a good method to provide a large collection of semantically annotated texts with formal, deep semantics rather than shallow? We argue that a bootstrapping approach comprising state-of-the-art NLP tools for parsing and semantic interpretation, in combination with a wiki-like interface for collaborative annotation of experts, and a game with a purpose for crowdsourcing, are the starting ingredients for fulfilling this enterprise. The result is a semantic resource that anyone can edit and that integrates various phenomena, including predicate-argument structure, scope, tense, thematic roles, rhetorical relations and presuppositions, into a single semantic formalism: Discourse Representation Theory. Taking texts rather than sentences as the units of annotation results in deep semantic representations that incorporate discourse structure and dependencies. To manage the various (possibly conflicting) annotations provided by experts and non-experts, we introduce a method that stores ``Bits of Wisdom'' in a database as stand-off annotations."
E12-2019,A platform for collaborative semantic annotation,2012,11,14,2,1,7,valerio basile,Proceedings of the Demonstrations at the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Data-driven approaches in computational semantics are not common because there are only few semantically annotated resources available. We are building a large corpus of public-domain English texts and annotate them semi-automatically with syntactic structures (derivations in Combinatory Categorial Grammar) and semantic representations (Discourse Representation Structures), including events, thematic roles, named entities, anaphora, scope, and rhetorical structure. We have created a wiki-like Web-based platform on which a crowd of expert annotators (i.e. linguists) can log in and adjust linguistic analyses in real time, at various levels of analysis, such as boundaries (tokens, sentences) and tags (part of speech, lexical categories). The demo will illustrate the different features of the platform, including navigation, visualization and editing."
W11-2819,Towards Generating Text from Discourse Representation Structures,2011,23,8,2,1,7,valerio basile,Proceedings of the 13th {E}uropean Workshop on Natural Language Generation,0,"We argue that Discourse Representation Structures form a suitable level of language-neutral meaning representation for micro planning and surface realisation. DRSs can be viewed as the output of macro planning, and form the rough plan and structure for generating a text. We present the first ideas of building a large DRS corpus that enables the development of broad-coverage, robust text generators. A DRS-based generator imposes various challenges on micro-planning and surface realisation, including generating referring expressions, lexicalisation and aggregation."
P10-1022,Rebanking {CCG}bank for Improved {NP} Interpretation,2010,26,17,3,0,37818,matthew honnibal,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"Once released, treebanks tend to remain unchanged despite any shortcomings in their depth of linguistic analysis or coverage of specific phenomena. Instead, separate resources are created to address such problems. In this paper we show how to improve the quality of a treebank, by integrating resources and implementing improved analyses for specific constructions.n n We demonstrate this rebanking process by creating an updated version of CCG-bank that includes the predicate-argument structure of both verbs and nouns, base-NP brackets, verb-particle constructions, and restrictive and non-restrictive nominal modifiers; and evaluate the impact of these changes on a statistical parser."
W09-3705,Computing Genitive Superlatives,2009,-1,-1,1,1,6245,johan bos,Proceedings of the Eight International Conference on Computational Semantics,0,None
W08-2220,Introduction to the Shared Task on Comparing Semantic Representations,2008,8,15,1,1,6245,johan bos,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"Seven groups participated in the STEP 2008 shared task on comparing semantic representations as output by practical wide-coverage NLP systems. Each of this groups developed their own system for producing semantic representations for texts, each in their own semantic formalism. Each group was requested to provide a short sample text, producing a shared task set of seven texts, allowing participants to challenge each other. Following this, each group was asked to provide the raw system output for all texts, which are made available on http://www.sigsem.org. Two groups were extremely inspired by the shared task and also provided gold-standard semantic representations for the seven texts, together with evaluation measures. The STEP 2008 workshop itself will continue the discussion, focusing on the feasibility of a theory-neutral gold standard for deep semantic representations."
W08-2222,Wide-Coverage Semantic Analysis with {B}oxer,2008,16,213,1,1,6245,johan bos,Semantics in Text Processing. {STEP} 2008 Conference Proceedings,0,"Boxer is an open-domain software component for semantic analysis of text, based on Combinatory Categorial Grammar (CCG) and Discourse Representation Theory (DRT). Used together with the CC (b) discourse structure triggered by conditionals, negation or discourse adverbs was overall correctly computed; (c) some measure and time expressions are correctly analysed, others aren't; (d) several shallow analyses are given for lexical phrases that require deep analysis; (e) bridging references and pronouns are not resolved in most cases. Boxer is distributed with the C&C tools and freely available for research purposes."
bos-2008-lets,Let{'}s not Argue about Semantics,2008,23,20,1,1,6245,johan bos,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"WhatÂs the best way to assess the performance of a semantic component in an NLP system? Tradition in NLP evaluation tells us that comparing output against a gold standard is a good idea. To define a gold standard, one first needs to decide on the representation language, and in many cases a first-order language seems a good compromise between expressive power and efficiency. Secondly, one needs to decide how to represent the various semantic phenomena, in particular the depth of analysis of quantification, plurals, eventualities, thematic roles, scope, anaphora, presupposition, ellipsis, comparatives, superlatives, tense, aspect, and time-expressions. Hence it will be hard to come up with an annotation scheme unless one permits different level of semantic granularity. The alternative is a theory-neutral black-box type evaluation where we just look at how systems react on various inputs. For this approach, we can consider the well-known task of recognising textual entailment, or the lesser-known task of textual model checking. The disadvantage of black-box methods is that it is difficult to come up with natural data that cover specific semantic phenomena."
P07-2009,Linguistically Motivated Large-Scale {NLP} with {C}{\\&}{C} and Boxer,2007,18,201,3,0.038052,25856,james curran,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"The statistical modelling of language, together with advances in wide-coverage grammar development, have led to high levels of robustness and efficiency in NLP systems and made linguistically motivated large-scale language processing a possibility (Matsuzaki et al., 2007; Kaplan et al., 2004). This paper describes an NLP system which is based on syntactic and semantic formalisms from theoretical linguistics, and which we have used to analyse the entire Gigaword corpus (1 billion words) in less than 5 days using only 18 processors. This combination of detail and speed of analysis represents a break-through in NLP technology."
W06-1602,An Empirical Approach to the Interpretation of Superlatives,2006,14,19,1,1,6245,johan bos,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"In this paper we introduce an empirical approach to the semantic interpretation of superlative adjectives. We present a corpus annotated for superlatives and propose an interpretation algorithm that uses a wide-coverage parser and produces semantic representations. We achieve F-scores between 0.84 and 0.91 for detecting attributive superlatives and an accuracy in the range of 0.69--0.84 for determining the correct comparison set. As far as we are aware, this is the first automated approach to superlatives for open-domain texts and questions."
H05-1079,Recognising Textual Entailment with Logical Inference,2005,17,181,1,1,6245,johan bos,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We use logical inference techniques for recognising textual entailment. As the performance of theorem proving turns out to be highly dependent on not readily available background knowledge, we incorporate model building, a technique borrowed from automated reasoning, and show that it is a useful robust method to approximate entailment. Finally, we use machine learning to combine these deep semantic analysis techniques with simple shallow word overlap; the resulting hybrid model achieves high accuracy on the RTE testset, given the state of the art. Our results also show that the different techniques that we employ perform very differently on some of the subsets of the RTE corpus and as a result, it is useful to use the nature of the dataset as a feature."
C04-1043,Unificational {C}ombinatory {C}ategorial {G}rammar. Combining Information Structure and Discourse Representations,2004,7,4,2,0,52359,maarika traat,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper we present a grammar formalism that combines the insights from Combinatory Categorial Grammar with feature structure unification. We show how information structure can be incorporated with syntactic and semantic representations in a principled way. We focus on the way theme, rheme, and focus are integrated in the compositional semantics, using Discourse Representation Theory as first-order semantic theory. UCCG can be used for parsing and generating prosodically annotated text, and therefore has the potential to advance spoken dialogue systems."
C04-1180,Wide-Coverage Semantic Representations from a {CCG} Parser,2004,21,172,1,1,6245,johan bos,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper shows how to construct semantic representations from the derivations produced by a wide-coverage CCG parser. Unlike the dependency structures returned by the parser itself, these can be used directly for semantic interpretation. We demonstrate that well-formed semantic representations can be produced for over 97% of the sentences in unseen WSJ text. We believe this is a major step towards widecoverage semantic interpretation, one of the key objectives of the field of NLP."
W03-2406,Automatic Multi-Layer Corpus Annotation for Evaluation Question Answering Methods: {CBC}4{K}ids,2003,16,3,4,0,29355,jochen leidner,Proceedings of 4th International Workshop on Linguistically Interpreted Corpora ({LINC}-03) at {EACL} 2003,0,None
W03-2123,{DIPPER}: Description and Formalisation of an Information-State Update Dialogue System Architecture,2003,17,122,1,1,6245,johan bos,Proceedings of the Fourth {SIG}dial Workshop of Discourse and Dialogue,0,None
J03-2002,Implementing the Binding and Accommodation Theory for Anaphora Resolution and Presupposition Projection,2003,28,33,1,1,6245,johan bos,Computational Linguistics,0,"Computational aspects of Van der Sandt's binding and accommodation theory (BAT) for presupposition projection and anaphora resolution are presented and discussed in this article. BAT is reformulated to meet requirements for computational implementation, which include operations on discourse representation structures (renaming and merging), the representation of presuppositions (allowing for selective binding and determining free and bound variables), and a formulation of the acceptability constraints imposed by BAT. An efficient presupposition resolution algorithm is presented, and several further improvements such as preferences for binding and accommodation are discussed and integrated in this algorithm. Finally, innovative use of first-order theorem provers to carry out consistency checking of discourse representations is investigated."
E03-1042,Meaningful Conversation with a Mobile Robot,2003,2,31,1,1,6245,johan bos,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We describe an implementation integrating a spoken dialogue system with a mobile robot, which the user can direct to specific locations, ask for information about its status, and supply information about its environment. The robot uses an internal map for navigation, and communicates its current orientation and accessible locations to the dialogue system using a topological map as interface."
C02-1067,An Inference-based Approach to Dialogue System Design,2002,6,21,1,1,6245,johan bos,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We present an architecture for spoken dialogue systems where first-order inference (both theorem proving and model building) plays a crucial role in interpreting utterances of dialogue participants and deciding how the system should respond and carry out instructions. The dialogue itself is represented as a DRS which is translated into first-order logic for inference tasks. The system is implemented as a society of OAA-agents, and evaluated against a specific application (home automation)."
C02-1095,Compilation of Unification Grammars with Compositional Semantics to Speech Recognition Packages,2002,7,22,1,1,6245,johan bos,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"In this paper a method to compile unification grammars into speech recognition packages is presented, and in particular, rules are specified to transfer the compositional semantics stated in unification grammars into speech recognition grammars. The resulting compiler creates a context-free backbone of the unification grammar, eliminates left-recursive productions and removes redundant grammar rules. The method was tested on a medium-sized unification grammar for English using Nuance speech recognition software on a corpus of 131 utterances of 12 different speakers. Results showed no significant computational overhead with respect to speech recognition performances for speech recognition grammar with compositional semantics compared to grammars without."
P98-1024,Managing Information at Linguistic Interfaces,1998,10,14,1,1,6245,johan bos,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"A large spoken dialogue translation system imposes both engineering and linguistic constraints on the way in which linguistic information is communicated between modules. We describe the design and use of interface terms, whose formal, functional and communicative role has been tested in a sequence of integrated systems and which have proven adequate to these constraints."
P98-1072,Semantic-Head Based Resolution of Scopal Ambiguities,1998,10,5,2,0,15137,bjorn gamback,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"We introduce an algorithm for scope resolution in underspecified semantic representations. Scope preferences are suggested on the basis of semantic argument structure. The major novelty of this approach is that, while maintaining an (scopally) underspecified semantic representation, we at the same time suggest a resolution possibility. The algorithm has been implemented and tested in a large-scale system and fared quite well: 28% of the utterances were ambiguous, 80% of these were correctly interpreted, leaving errors in only 5.7% of the utterance set."
C98-1024,Managing information at linguistic interfaces,1998,10,14,1,1,6245,johan bos,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"A large spoken dialogue translation system imposes both engineering and linguistic constraints on the way in which linguistic information is communicated between modules. We describe the design and use of interface terms, whose formal, functional and communicative role has been tested in a sequence of integrated systems and which have proven adequate to these constraints."
C98-1069,Semantic-Head Based Resolution of Scopal Ambiguities,1998,10,5,2,0,15137,bjorn gamback,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"We introduce an algorithm for scope resolution in underspecified semantic representations. Scope preferences are suggested on the basis of semantic argument structure. The major novelty of this approach is that, while maintaining an (scopally) underspecified semantic representation, we at the same time suggest a resolution possibility. The algorithm has been implemented and tested in a large-scale system and fared quite well: 28% of the utterances were ambiguous, 80% of these were correctly interpreted, leaving errors in only 5.7% of the utterance set."
C96-1024,Compositional Semantics in Verbmobil,1996,6,40,1,1,6245,johan bos,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"The paper discusses how compositional semantics is implemented in the Verbmobil speech-to-speech translation system using LUD, a description language for underspecified discourse representation structures. The description language and its formal interpretation in DRT are described as well as its implementation together with the architecture of the system's entire syntactic-semantic processing module. We show that a linguistically sound theory and formalism can be properly implemented in a system with (near) real-time requirements."
C94-2193,Presupposition {\\&} {VP}-Ellipsis,1994,11,3,1,1,6245,johan bos,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"We discuss a treatment of VP-ellipsis resolution in DRT in general, and particularly cases where the source clause of the elliptical VP contains presupposition triggers. We propose to restrain VP-ellipsis resolution by presupposition neutralization. We view presupposition as a kind of anaphora, with the ability to accommodate an antecedent if not provided by discourse."
