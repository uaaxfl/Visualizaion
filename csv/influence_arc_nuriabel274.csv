2001.mtsummit-papers.13,bel-etal-2000-simple,1,0.791392,"towards de facto standards has already allowed the field of Language Resources to establish broad consensus on key issues for some well-established areas — and will allow similar consensus to be achieved for other important areas through the ISLE project — providing thus a key opportunity for further consolidation and a basis for technological advance. EAGLES previous results have already become de facto standards. To mention several key examples: the LE PAROLE/SIMPLE resources (morphological/syntactic/semantic lexicons and corpora for 12 EU languages, Ruimy et al., 1998, Lenci et al., 2000, Bel et al., 2000) rely on EAGLES results (Sanfilippo, A. et al., 1996 and 1999), and are now being enlarged at the national level through many National Projects; the ELRA Validation Manuals for Lexicons (Underwood and Navarretta, 1997) and Corpora (Burnard et al., 1997) are based on EAGLES guidelines; morphosyntactic tagging of corpora in a very large number of EU, international and national projects – and for more than 20 languages — is conformant to EAGLES recommendations (Leech and Wilson, 1996). The ISLE objective is more ambitious both in geographic scope , involving European, American and now Asian group"
2020.lrec-1.407,gavrilidou-etal-2012-meta,1,0.919419,"Missing"
2020.lrec-1.407,2020.lrec-1.420,1,0.860379,"Missing"
2020.lrec-1.407,L18-1213,1,0.894888,"Missing"
2020.lrec-1.407,piperidis-etal-2014-meta,1,0.824391,"ween 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META-NET results (Rehm and Uszkoreit, 2012), funded a"
2020.lrec-1.407,piperidis-2012-meta,1,0.92358,"n 34 European countries. META-NET was, between 2010 and 2017, supported through the EU projects T4ME, CESAR, METANET4U, META-NORD and CRACKER. One of its main goals is technology support for all European languages as well as fostering innovative research by providing strategic recommendations with regard to key research topics (Rehm and Uszkoreit, 2013). META-SHARE3 is an infrastructure that brings together providers and consumers of language data, tools and services. It is a network of repositories that store resources, documented with high-quality metadata aggregated in central inventories (Piperidis, 2012; Gavrilidou et al., 2012; Piperidis et al., 2014). CLARIN ERIC The CLARIN European Research Infrastructure for Language Resources and Technology is a legal entity set up in 2012, with 20 member countries at present.4 CLARIN makes language resources available to scholars, researchers, and students from all disciplines with a focus on the humanities and social sciences. CLARIN offers solutions and services for deploying, connecting, analyzing and sustaining digital language data and tools. Call ICT-17-2014 – “Cracking the Language Barrier” The EU call ICT-17-2014, which was informed by key META"
2020.lrec-1.407,L16-1251,1,0.865781,"Missing"
2020.lrec-1.407,2020.lrec-1.413,1,0.785764,"Missing"
alonso-etal-2012-voting,W99-0512,0,\N,Missing
alonso-etal-2012-voting,markert-nissim-2002-towards,0,\N,Missing
alonso-etal-2012-voting,bel-etal-2000-simple,1,\N,Missing
alonso-etal-2012-voting,N01-1010,0,\N,Missing
alonso-etal-2012-voting,N07-2002,1,\N,Missing
alonso-etal-2012-voting,E09-1005,0,\N,Missing
alonso-etal-2012-voting,E09-1045,0,\N,Missing
alonso-etal-2012-voting,W04-1908,0,\N,Missing
alonso-etal-2012-voting,C04-1133,0,\N,Missing
alonso-etal-2012-voting,W09-3716,0,\N,Missing
alonso-etal-2012-voting,J03-2004,0,\N,Missing
alonso-etal-2012-voting,H92-1045,0,\N,Missing
alonso-etal-2012-voting,S10-1005,0,\N,Missing
alonso-etal-2012-voting,pustejovsky-etal-2006-towards,0,\N,Missing
alonso-etal-2012-voting,jezek-quochi-2010-capturing,0,\N,Missing
arias-etal-2014-boosting,E12-2012,0,\N,Missing
arias-etal-2014-boosting,W08-1301,0,\N,Missing
arias-etal-2014-boosting,W13-3732,0,\N,Missing
arias-etal-2014-boosting,I08-3008,0,\N,Missing
arias-etal-2014-boosting,marimon-etal-2012-iula,1,\N,Missing
arias-etal-2014-boosting,I13-1123,1,\N,Missing
arias-etal-2014-boosting,P13-2017,0,\N,Missing
arias-etal-2014-boosting,taule-etal-2008-ancora,0,\N,Missing
arias-etal-2014-boosting,padro-stanilovsky-2012-freeling,0,\N,Missing
atkins-etal-2002-resources,2001.mtsummit-papers.39,1,\N,Missing
atkins-etal-2002-resources,bel-etal-2000-simple,1,\N,Missing
atkins-etal-2002-resources,calzolari-etal-2002-towards,1,\N,Missing
bel-2010-handling,J96-2001,0,\N,Missing
bel-2010-handling,E03-1040,0,\N,Missing
bel-2010-handling,P97-1003,0,\N,Missing
bel-2010-handling,C08-1057,0,\N,Missing
bel-2010-handling,C00-2108,0,\N,Missing
bel-2010-handling,N07-2002,1,\N,Missing
bel-2010-handling,J93-2002,0,\N,Missing
bel-2010-handling,P03-1059,0,\N,Missing
bel-2010-handling,P05-1076,0,\N,Missing
bel-2010-handling,J01-3003,0,\N,Missing
bel-2010-handling,baldwin-etal-2004-road,0,\N,Missing
bel-2010-handling,W02-2014,0,\N,Missing
bel-etal-2006-new,bel-etal-2000-simple,1,\N,Missing
bel-etal-2006-new,P04-3012,1,\N,Missing
bel-etal-2006-new,marimon-bel-2004-lexical,1,\N,Missing
bel-etal-2008-automatic,W07-1214,1,\N,Missing
bel-etal-2008-automatic,N07-2002,1,\N,Missing
bel-etal-2008-automatic,A97-1052,0,\N,Missing
bel-etal-2008-automatic,J93-2002,0,\N,Missing
bel-etal-2008-automatic,J93-1002,0,\N,Missing
bel-etal-2008-automatic,P03-1059,0,\N,Missing
bel-etal-2008-automatic,J01-3003,0,\N,Missing
bel-etal-2008-automatic,P07-1115,0,\N,Missing
bel-etal-2008-automatic,baldwin-etal-2004-road,0,\N,Missing
bel-etal-2008-automatic,chesley-salmon-alt-2006-automatic,0,\N,Missing
bel-etal-2008-coldic,bel-etal-2000-simple,1,\N,Missing
bel-etal-2008-coldic,francopoulo-etal-2006-lexical,1,\N,Missing
bel-etal-2012-automatic,C10-1006,1,\N,Missing
bel-etal-2012-automatic,N07-2002,1,\N,Missing
bel-etal-2012-automatic,P90-1034,0,\N,Missing
bel-etal-2012-automatic,J10-4006,0,\N,Missing
bel-etal-2012-automatic,P03-1059,0,\N,Missing
bel-etal-2012-automatic,P06-1014,0,\N,Missing
bel-etal-2012-automatic,W09-2501,0,\N,Missing
bel-etal-2012-automatic,P11-2123,0,\N,Missing
bel-etal-2012-automatic,P96-1004,0,\N,Missing
bel-etal-2012-automatic,J01-3003,0,\N,Missing
bel-etal-2012-automatic,P98-2127,0,\N,Missing
bel-etal-2012-automatic,C98-2122,0,\N,Missing
bel-etal-2012-automatic,bel-2010-handling,1,\N,Missing
C10-1006,P03-1059,0,0.074135,"ut characteristics of the contexts where words of the same class occur. The idea behind this is that differences in the distribution of the contexts will separate words in different classes, e.g. the class of transitive verbs will show up in passive constructions, while the intransitive verbs will not. Thus, the whole set of occurrences (tokens) of a word are taken as cues for defining its class (the class of the type), either because the word is observed in a number of particular contexts or because it is not. Selected references for this approach are: Brent, 1993; Merlo and Stevenson, 2001; Baldwin and Bond, 2003; Baldwin, 2005; Joanis and Stevenson, 2003; Joanis et al. 2007. Different supervised Machine Learning (ML) techniques have been applied to cue-based lexical acquisition. A learner is supplied with classified examples of words represented by numerical information about matched and not matched cues. The final exercise is to confirm that the data characterized by the linguistically motivated cues support indeed the division into the proposed classes. This was the approach taken by Merlo and Stevenson (2001), who worked with a Decision Tree and selected linguistic cues to classify English verbs i"
C10-1006,J93-2002,0,0.123967,"proposes to classify words taking as input characteristics of the contexts where words of the same class occur. The idea behind this is that differences in the distribution of the contexts will separate words in different classes, e.g. the class of transitive verbs will show up in passive constructions, while the intransitive verbs will not. Thus, the whole set of occurrences (tokens) of a word are taken as cues for defining its class (the class of the type), either because the word is observed in a number of particular contexts or because it is not. Selected references for this approach are: Brent, 1993; Merlo and Stevenson, 2001; Baldwin and Bond, 2003; Baldwin, 2005; Joanis and Stevenson, 2003; Joanis et al. 2007. Different supervised Machine Learning (ML) techniques have been applied to cue-based lexical acquisition. A learner is supplied with classified examples of words represented by numerical information about matched and not matched cues. The final exercise is to confirm that the data characterized by the linguistically motivated cues support indeed the division into the proposed classes. This was the approach taken by Merlo and Stevenson (2001), who worked with a Decision Tree and s"
C10-1006,H05-1088,0,0.0121138,"those which are indeed nondeverbal event nouns and those which are not. Because deverbal result nouns are easily identifiable by the nominal suffix they bear (for instance, ‘-tion’ for English and ‘-ción’ for Spanish), our experiment has been centered in separating non-deverbal event nouns like guerra/war from non event nouns like tren/train. Some work related to our experiments can be found in the literature dealing with the identification of new events for broadcast news and semantic annotation of texts, which are two possible applications of automatic event detection (Allan et al. 1998 and Saurí et al. 2005, respectively, for example). For these systems, however, it would be difficult to find non-deverbal event nouns because of the absence of morphological suffixes, and therefore they could benefit from our learner. 3.1 Cue-based Lexical Information Acquisition According to the linguistic tradition, words that can be inserted in the same contexts can be said to belong to the same class. Thus, lexical classes are linguistic generalizations drawn from the characteristics of the contexts where a number of words tend to appear. Consequently, one of the approaches to lexical acquisition proposes to c"
C10-1006,E06-1025,0,\N,Missing
C10-1006,J01-3003,0,\N,Missing
C12-2100,C10-1006,1,0.925331,"ectively, describe and discuss results. Section 7 reflects upon lexical classes and logical polysemy and is followed by final remarks. 2 Background and Motivation Mainstream approaches to lexical semantic class acquisition classify words according to occurrences, i.e. they use the entire set of occurrences of a word to determine class membership. Yet, this approach has some limitations. Blind-theory distributional approaches have been shown to fail to account for the wide range of linguistic behavior displayed by words in language data (see Pustejovsky and Ježek (2008)), while authors such as Bel et al. (2010) reported problems caused by sparse data, or lack of evidence, and noise, or information obtained though not aimed at. Concerning sparse data in classification tasks, nouns that appear only once or twice in a corpus, and not in sought contexts, can render ineffective any classifier or clustering algorithm by not providing sufficient information for classification. We aim to soften effects of sparse data in the context of a clustering task by using a bootstrapping technique reliant on natural language inference properties (see Section 4.1). Noise, another pervasive issue in lexical semantic cla"
C12-2100,bel-etal-2012-automatic,1,0.83691,"exts, can render ineffective any classifier or clustering algorithm by not providing sufficient information for classification. We aim to soften effects of sparse data in the context of a clustering task by using a bootstrapping technique reliant on natural language inference properties (see Section 4.1). Noise, another pervasive issue in lexical semantic class acquisition, can be due to different factors: the occurrence of very general nominal expressions (e.g. “kind of”), which do not provide distinguishing lexical information; misleading corpus features; and the use of low-level tools (see Bel et al. (2012)). We assume noise resulting from errors generated by NLP tools to be typically characterized by unique occurrences and we employ a filtering strategy to overcome its possible effects (see Section 4.1). Concerning misleading corpus features, these are often caused by ambiguity of lexical items, resulting in nouns occurring in contexts not corresponding to their assumed lexical class. This presents challenging problems in classification tasks, as most authors do not distinguish among related senses of the same word, i.e. they either consider it as part of the class 1030 or not (Hindle, 1990; Bu"
C12-2100,W09-3707,0,0.0309846,"ilable annotated with such information, we needed to obtain it automatically. Automatically extracting qualia roles with lexico-syntactic patterns has been receiving considerable attention for its success: Hearst (1992) identified lexico-syntactic patterns to acquire noun hyponyms, corresponding to the FORMAL role, whereas Cimiano and Wenderoth (2007) identified lexico-syntactic patterns to obtain information regarding semantic relations that correspond to each qualia role. As we needed information regarding the FORMAL role, not full lexical entries, in order for clusters to emerge, following Celli and Nissim (2009), we bypassed the representation of the entire QS, assuming semantic relations can be induced by matching lexico-syntactic patterns that convey a relation of interest. 4 Methodology Given the unavailability of lexica annotated with FORMAL role information, and considering our basic goal of evaluating whether this information is enough to cluster together nouns of the same class, we extracted it from a corpus using lexico-syntactic patterns, following Cimiano and Wenderoth (2007), and then used it as features for a clustering task. In the experiment performed, we employed two steps: the extract"
C12-2100,P07-1112,0,0.100112,"he context of our work as it provides a formal explanation for words belonging to more than one type, and essentially to more than one class. Our experiment uses FORMAL role information as features for identifying lexical class membership. However, as there are no lexica available annotated with such information, we needed to obtain it automatically. Automatically extracting qualia roles with lexico-syntactic patterns has been receiving considerable attention for its success: Hearst (1992) identified lexico-syntactic patterns to acquire noun hyponyms, corresponding to the FORMAL role, whereas Cimiano and Wenderoth (2007) identified lexico-syntactic patterns to obtain information regarding semantic relations that correspond to each qualia role. As we needed information regarding the FORMAL role, not full lexical entries, in order for clusters to emerge, following Celli and Nissim (2009), we bypassed the representation of the entire QS, assuming semantic relations can be induced by matching lexico-syntactic patterns that convey a relation of interest. 4 Methodology Given the unavailability of lexica annotated with FORMAL role information, and considering our basic goal of evaluating whether this information is"
C12-2100,C92-2082,0,0.482563,"ally allowing for multiple selection (see (1)). Being able to represent lexical items as complex objects is useful in the context of our work as it provides a formal explanation for words belonging to more than one type, and essentially to more than one class. Our experiment uses FORMAL role information as features for identifying lexical class membership. However, as there are no lexica available annotated with such information, we needed to obtain it automatically. Automatically extracting qualia roles with lexico-syntactic patterns has been receiving considerable attention for its success: Hearst (1992) identified lexico-syntactic patterns to acquire noun hyponyms, corresponding to the FORMAL role, whereas Cimiano and Wenderoth (2007) identified lexico-syntactic patterns to obtain information regarding semantic relations that correspond to each qualia role. As we needed information regarding the FORMAL role, not full lexical entries, in order for clusters to emerge, following Celli and Nissim (2009), we bypassed the representation of the entire QS, assuming semantic relations can be induced by matching lexico-syntactic patterns that convey a relation of interest. 4 Methodology Given the unav"
C12-2100,P90-1034,0,0.517633,"el et al. (2012)). We assume noise resulting from errors generated by NLP tools to be typically characterized by unique occurrences and we employ a filtering strategy to overcome its possible effects (see Section 4.1). Concerning misleading corpus features, these are often caused by ambiguity of lexical items, resulting in nouns occurring in contexts not corresponding to their assumed lexical class. This presents challenging problems in classification tasks, as most authors do not distinguish among related senses of the same word, i.e. they either consider it as part of the class 1030 or not (Hindle, 1990; Bullinaria, 2008; Bel et al., 2012). This is particularly problematic when words allow for multiple selection, i.e. when different senses of the same lexical item can be simultaneously selected for in one sentence (see (1)). Known as logical polysemy, this type of ambiguity has been shown to have well-defined properties (see Pustejovsky (1995) and Buitelaar (1998)) and has been consistently reported as a factor in lexical semantic acquisition tasks. The newly constructed (LOCATION) bank offers special conditions (ORGANIZATION) to new clients. (1) Approaches in this domain have usually tried"
C12-2100,J91-4003,0,0.0626729,"Lexicon theory GL models the internal structure of lexical items in a computational perspective (Pustejovsky, 1995), proposing various levels of representation to semantically represent words, while allowing for the computation of meaning in context. Qualia Structure (QS) is one of these levels, consisting of 4 roles (FORMAL: what an object is; CONSTITUTIVE: what it is composed of; TELIC: its purpose; AGENTIVE: its origin), which model the predicative potential of lexical items. Here, we focus on the FORMAL role, defined as the role that distinguishes a lexical object within a larger domain (Pustejovsky, 1991). QS also models phenomena such as polysemy of lexical items inherently complex in their meaning. These instances, dot objects, are the logical pairing of senses denoted by individual types in a complex type (Pustejovsky, 1995), which can pick up distinct aspects of the object, as well as properties of more than one class (Pustejovsky and Ježek, 2008), typically allowing for multiple selection (see (1)). Being able to represent lexical items as complex objects is useful in the context of our work as it provides a formal explanation for words belonging to more than one type, and essentially to"
C12-2124,baccianella-etal-2010-sentiwordnet,0,0.100662,"tilizes external language resources as lexicons and thesaurus which, although not collecting polarity relations, can help to increase the number of a set of opinion seeds by different methods. The majority of the works that follow this procedure make use of WordNet (Miller, 1995) to carry out this task. In the work of (Hu & Liu, 2004) the authors hypothesized that synonyms of a seed adjective have the same semantic orientation while the antonymous would have the opposite one, employing WordNet synsets to find out these relations. Lexical resources like SentiWordNet (Esuli & Sebastiani, 2005) (Baccianella, Esuli, & Sebastiani, 2010) classified polarity elements into Positive, Negative or Objective by analyzing the similarity between the glosses or definitions of the words and also by studying the relations established among them in the thesaurus. Valitutti (Valitutti, Strapparava, & Stock, 2004) tried to adapt WordNet to Sentiment Analysis purposes through the identification and subsequent annotation of all the elements having a high load of emotion or affective content. Although the dictionary based approach achieved great results, it has two main shortcomings. On the one hand, it does not take into account the polarit"
C12-2124,W06-1642,0,0.0403374,"Missing"
C12-2124,vazquez-bel-2012-classification,1,0.620372,"ive by analyzing the similarity between the glosses or definitions of the words and also by studying the relations established among them in the thesaurus. Valitutti (Valitutti, Strapparava, & Stock, 2004) tried to adapt WordNet to Sentiment Analysis purposes through the identification and subsequent annotation of all the elements having a high load of emotion or affective content. Although the dictionary based approach achieved great results, it has two main shortcomings. On the one hand, it does not take into account the polarity changes due to different domains. As some works demonstrated (Vázquez & Bel, 2012), a great majority of the adjectives are domain dependent: they could be positive in one domain but negative or even neutral in another. On the other hand, this approach suffers from a lack of scalability since it does not take into account words not appearing in the language resources used. Actually, it falls down on the analysis of colloquial words or different kinds of slang expressions that are not collected in WordNet or any thesaurus. Corpus based approach starts, as dictionary based one, with a manually built list of seed words but unlike it, this approach does not rely on the availabil"
C12-2124,padro-etal-2010-freeling,0,\N,Missing
C12-2124,H05-1116,0,\N,Missing
C12-2124,W03-0404,0,\N,Missing
C12-2124,D07-1115,0,\N,Missing
C12-2124,P97-1023,0,\N,Missing
C14-1049,P11-2123,0,0.0234169,"class-indicatory, or marked, contexts. Its aim is to evaluate how unmarked contexts can be used to improve the accuracy and reliability of lexical semantic classifiers. Results demonstrate that the combined use of both types of distributional information (marked and unmarked) is crucial to improve classification. This result was replicated using two different corpora, demonstrating the robustness of the method proposed. 1 Introduction Lexical resources annotated with lexical semantic classes have been successfully incorporated into a wide range of NLP applications, such as grammar induction (Agirre et al., 2011) and the building and extending of semantic ontologies (Abbès et al., 2011). However, lexical semantic tagging in large lexica is mostly done by hand, implying high costs with regard to maintenance and domain tuning. As the use of an inadequate lexicon is one of the causes of poor performance of NLP applications, current research to improve the automatic production of rich language resources, and of class-annotated lexica, in particular, is critical. One way to approach this task is through supervised cue-based lexical semantic classification. Based on the distributional hypothesis (Harris, 19"
C14-1049,bel-2010-handling,1,0.928269,"ded in usage-based theories of grammar (Goldberg, 2006), a lexical class is seen as a generalization of the systematic co-distribution of a number of words and contexts. Construction-based grammar hypotheses allow us to predict there are sets of word occurrences that, together, constitute a class mark, indicating a particular semantic class, in line with the structuralist notion of markedness (Jakobson, 1971). The identification of relevant cues for machine learning classification is problematic as low frequency evidence is typically disregarded by automatic systems. To overcome this problem, Bel (2010) applied smoothing methods, demonstrating increases in accuracy, though low frequency words remained problematic for classifiers when evidence was scarce, and thus not considered as a positive cue for the class, although it was indicative. In Bel et al. (2012) we built on this hypothesis, assuming only frequently occurring contexts would be efficient for classification tasks, and considering only frequent predicates, prepositions, affixes, etc., as well as negative cases (i.e. marked cues for other classes), as indicators for a class membership decision. Thus, we ignored all information that,"
C14-1049,bel-etal-2012-automatic,1,0.664513,"se specific words might be observed in a number of class-indicative contexts but not always are. This type of marked, or class-indicative, context (e.g. co-occurrence with specific prepositions, predicate selectional restrictions, and grammatical information, such as indirect objects) are sparse in any corpus as, being so specific, they do not occur often with each target noun. Using only exclusive class-indicative contexts as features in nominal lexical semantic classification has been shown to not always provide sufficient information to make a decision regarding class membership of a noun (Bel et al., 2012), especially when the data does not contain relevant co-occurrences or when those cooccurrences are too disperse to be correlated. Recent work on the use of distributional models for nominal classification tasks (Romeo et al., 2014) discusses potential bottlenecks of models using data extracted with lexico-syntactic patterns as features, identifying data sparsity as one of the major issues affecting the performance of these systems. In fact, the selection of class-indicative information, in an attempt to provide relevant information to classifiers and thus reduce noise, naturally limits the am"
C14-1049,2011.eamt-1.40,0,0.0303118,"rent aforementioned strategies for encoding distributional information regarding marked and unmarked contexts, respectively, as detailed further below in this section. Once all of the information was compiled, the vectors were provided to classifiers. As previously mentioned, our experiments covered English nouns of the classes: INF, ORG, HUM, EVT and LOC (see Section 4.2). For the purpose of the work presented here, we experimented with two corpora to determine the transferability and robustness of our method, independently of specific corpus data. We first used a general web-crawled corpus (Pecina et al., 2011) consisting of 30 million PoStagged English tokens (henceforth Corpus A) to identify unmarked contexts (see Section 3.1) as well as to train our classifiers. We also employed an excerpt of the web-crawled UkWaC corpus (Baroni et al., 2009), consisting of 60 million PoS-tagged English tokens (henceforth Corpus B) to test our approach on unknown data, in this way ensuring that our approach and classifiers are not over-fitted to any specific corpus, instead confirming that the method we propose can be generalized, and the results obtained are replicable given any dataset. Regular expressions over"
C14-1049,romeo-etal-2014-choosing,1,0.883025,"and grammatical information, such as indirect objects) are sparse in any corpus as, being so specific, they do not occur often with each target noun. Using only exclusive class-indicative contexts as features in nominal lexical semantic classification has been shown to not always provide sufficient information to make a decision regarding class membership of a noun (Bel et al., 2012), especially when the data does not contain relevant co-occurrences or when those cooccurrences are too disperse to be correlated. Recent work on the use of distributional models for nominal classification tasks (Romeo et al., 2014) discusses potential bottlenecks of models using data extracted with lexico-syntactic patterns as features, identifying data sparsity as one of the major issues affecting the performance of these systems. In fact, the selection of class-indicative information, in an attempt to provide relevant information to classifiers and thus reduce noise, naturally limits the amount of data available to the system, often resulting in sparse vectors. Resulting from the necessity of selecting the information provided to classifiers, in an attempt to improve the accuracy of classification decisions, the spars"
de-smedt-etal-2014-clara,Y12-1015,0,\N,Missing
de-smedt-etal-2014-clara,W11-2153,0,\N,Missing
de-smedt-etal-2014-clara,W12-3903,0,\N,Missing
de-smedt-etal-2014-clara,W11-2605,0,\N,Missing
de-smedt-etal-2014-clara,W13-1728,0,\N,Missing
de-smedt-etal-2014-clara,R11-1041,1,\N,Missing
de-smedt-etal-2014-clara,W11-4647,0,\N,Missing
de-smedt-etal-2014-clara,W13-2907,1,\N,Missing
de-smedt-etal-2014-clara,W11-4604,1,\N,Missing
de-smedt-etal-2014-clara,P13-1054,1,\N,Missing
de-smedt-etal-2014-clara,Y12-1014,0,\N,Missing
de-smedt-etal-2014-clara,P11-3013,0,\N,Missing
de-smedt-etal-2014-clara,ramasamy-zabokrtsky-2012-prague,0,\N,Missing
de-smedt-etal-2014-clara,W13-2805,0,\N,Missing
de-smedt-etal-2014-clara,larasati-2012-identic,0,\N,Missing
de-smedt-etal-2014-clara,alonso-etal-2012-voting,1,\N,Missing
de-smedt-etal-2014-clara,W12-3410,0,\N,Missing
de-smedt-etal-2014-clara,drobac-etal-2014-heuristic,1,\N,Missing
de-smedt-etal-2014-clara,P13-2127,1,\N,Missing
de-smedt-etal-2014-clara,W11-4406,0,\N,Missing
de-smedt-etal-2014-clara,dione-2014-pruning,0,\N,Missing
de-smedt-etal-2014-clara,R11-2019,0,\N,Missing
de-smedt-etal-2014-clara,W12-6304,0,\N,Missing
de-smedt-etal-2014-clara,W14-1203,1,\N,Missing
de-smedt-etal-2014-clara,W12-5017,0,\N,Missing
de-smedt-etal-2014-clara,lis-2012-polish,0,\N,Missing
de-smedt-etal-2014-clara,W14-0808,0,\N,Missing
de-smedt-etal-2014-clara,schumann-2012-knowledge,0,\N,Missing
de-smedt-etal-2014-clara,C12-1065,1,\N,Missing
de-smedt-etal-2014-clara,dione-2012-morphological,0,\N,Missing
de-smedt-etal-2014-clara,escartin-2012-design,0,\N,Missing
de-smedt-etal-2014-clara,W12-2019,1,\N,Missing
de-smedt-etal-2014-clara,lenkiewicz-etal-2012-avatech,1,\N,Missing
de-smedt-etal-2014-clara,W11-3302,1,\N,Missing
de-smedt-etal-2014-clara,escartin-2014-chasing,0,\N,Missing
de-smedt-etal-2014-clara,W12-0503,0,\N,Missing
de-smedt-etal-2014-clara,W13-5411,1,\N,Missing
de-smedt-etal-2014-clara,gebre-etal-2012-towards,1,\N,Missing
E12-2001,W10-1840,0,0.0132734,"s been used successfully to build workflows for PoS tagging and alignment. Some WSs, e.g. dependency parsers, require a more complex representation that cannot be handled by the TO. Therefore, a more expressive format has been adopted for these. The Graph Annotation Format (GrAF) (Ide and Suderman, 2007) is a XML representation of a graph that allows different levels of annotation using a “feature– value” paradigm. This system allows different in-house formats to be easily encapsulated in this container-based format. On the other hand, GrAF can be used as a pivot format between other formats (Ide and Bunt, 2010), e.g. there is software to convert GrAF to UIMA and GATE formats (Ide and Suderman, 2009) and it can be used to merge data represented in a graph. Interoperability Interoperability plays a crucial role in a platform of distributed WSs. Soaplab deploys SOAP10 WSs and handles automatically most of the issues involved in this process, while Taverna can combine SOAP and REST11 WSs. Hence, we can say that communication protocols are being handled by the tools. However, parameters and data interoperability need to be addressed. 5.1 Travelling Object Common Interface Both TO and GrAF address syntact"
E12-2001,W07-1501,0,0.0136085,"connect the different tools in the platform regardless of their original input/output formats. We have adopted for TO the XML Corpus Encoding Standard (XCES) format (Ide et al., 2000) because it was the already existing format that required the minimum transduction effort from the in-house formats. The XCES format has been used successfully to build workflows for PoS tagging and alignment. Some WSs, e.g. dependency parsers, require a more complex representation that cannot be handled by the TO. Therefore, a more expressive format has been adopted for these. The Graph Annotation Format (GrAF) (Ide and Suderman, 2007) is a XML representation of a graph that allows different levels of annotation using a “feature– value” paradigm. This system allows different in-house formats to be easily encapsulated in this container-based format. On the other hand, GrAF can be used as a pivot format between other formats (Ide and Bunt, 2010), e.g. there is software to convert GrAF to UIMA and GATE formats (Ide and Suderman, 2009) and it can be used to merge data represented in a graph. Interoperability Interoperability plays a crucial role in a platform of distributed WSs. Soaplab deploys SOAP10 WSs and handles automatica"
E12-2001,W09-3004,0,0.142124,".g. dependency parsers, require a more complex representation that cannot be handled by the TO. Therefore, a more expressive format has been adopted for these. The Graph Annotation Format (GrAF) (Ide and Suderman, 2007) is a XML representation of a graph that allows different levels of annotation using a “feature– value” paradigm. This system allows different in-house formats to be easily encapsulated in this container-based format. On the other hand, GrAF can be used as a pivot format between other formats (Ide and Bunt, 2010), e.g. there is software to convert GrAF to UIMA and GATE formats (Ide and Suderman, 2009) and it can be used to merge data represented in a graph. Interoperability Interoperability plays a crucial role in a platform of distributed WSs. Soaplab deploys SOAP10 WSs and handles automatically most of the issues involved in this process, while Taverna can combine SOAP and REST11 WSs. Hence, we can say that communication protocols are being handled by the tools. However, parameters and data interoperability need to be addressed. 5.1 Travelling Object Common Interface Both TO and GrAF address syntactic interoperability while semantic interoperability is still an open topic. To facilitate"
E12-2001,ide-etal-2000-xces,0,0.0258492,"g there are more than 100 WSs and 30 workflows registered. 5 5.2 A goal of the project is to facilitate the deployment of as many tools as possible in the form of WSs. In many cases, tools performing the same task use in-house formats. We have designed a container, called “Travelling Object” (TO), as the data object that is being transfered between WSs. Any tool that is deployed needs to be adapted to the TO, this way we can interconnect the different tools in the platform regardless of their original input/output formats. We have adopted for TO the XML Corpus Encoding Standard (XCES) format (Ide et al., 2000) because it was the already existing format that required the minimum transduction effort from the in-house formats. The XCES format has been used successfully to build workflows for PoS tagging and alignment. Some WSs, e.g. dependency parsers, require a more complex representation that cannot be handled by the TO. Therefore, a more expressive format has been adopted for these. The Graph Annotation Format (GrAF) (Ide and Suderman, 2007) is a XML representation of a graph that allows different levels of annotation using a “feature– value” paradigm. This system allows different in-house formats"
J14-3001,W03-0403,0,0.127579,"his makes the task even slower and more expensive. 2 The IULA Spanish LSP Treebank contains 43,000 annotated sentences, distributed among different domains (Law, Economy, Computing Science, Medicine, and Environment) and sentence lengths (ranging from 4 to 30 words). The treebank is publicly available at http://metashare.upf.edu. 524 Marimon, Bel, and Padro´ Automatic Selection of HPSG-Parsed Sentences 2008), (iii) statistics about PoS sequences in a batch of parsed sentences (Reichart and Rappoport 2009), and (iv) ensemble parse algorithms (Reichart and Rappoport 2007; Sagae and Tsujii 2007; Baldridge and Osborne 2003). Here, we focus on ensemble approaches. Reichart and Rappoport (2007) selected high-quality constituency parses by using the level of agreement among 20 copies of the same parser, trained on different subsets of a training corpus. Experiments using training and test data for the same domain and in the parser-adaptation scenario showed improvements over several baselines. Sagae and Tsujii (2007) used an ensemble to select high-quality dependency parses. They compared the outputs of two statistical shift-reduce LR models and selected only identical parses, in their case to retrain the MaxEnt mo"
J14-3001,W06-2920,0,0.0285347,"e.html. 526 Marimon, Bel, and Padro´ Automatic Selection of HPSG-Parsed Sentences Figure 1 Derivation tree and dependency graph of Conceder licencias, cuando as´ı lo dispongan las ordenanzas [To grant licences, when so stipulated by ordinances]. format, also illustrated in Figure 1. In this target annotation, lexical elements are linked by asymmetrical dependency relations in which one of the elements is considered the head of the relation and the other one is its dependant. The conversion is a fully automatic and unambiguous process that produces the dependency structure in the CoNLL format (Buchholz and Marsi 2006). A deterministic conversion algorithm makes use of the identifiers of the phrase structure rules mentioned previously, in order to identify the heads, dependants, and some dependency types that are directly transferred onto the dependency structure (e.g., subject, specifier, and modifier). The identifiers of the lexical entries, which include the syntactic category of the subcategorized elements, enable the identification of the argument-related dependency functions.5 3.3 Dependency Parsing For dependency parsing, we use MaltParser (Nivre et al. 2007). To train it, we use manually disambiguat"
J14-3001,W97-1502,0,0.246513,"Missing"
J14-3001,W12-3602,0,0.0464821,"Missing"
J14-3001,I08-2097,0,0.058012,"Missing"
J14-3001,P07-1052,0,0.0137496,"making two annotators work on the same sentences. This makes the task even slower and more expensive. 2 The IULA Spanish LSP Treebank contains 43,000 annotated sentences, distributed among different domains (Law, Economy, Computing Science, Medicine, and Environment) and sentence lengths (ranging from 4 to 30 words). The treebank is publicly available at http://metashare.upf.edu. 524 Marimon, Bel, and Padro´ Automatic Selection of HPSG-Parsed Sentences 2008), (iii) statistics about PoS sequences in a batch of parsed sentences (Reichart and Rappoport 2009), and (iv) ensemble parse algorithms (Reichart and Rappoport 2007; Sagae and Tsujii 2007; Baldridge and Osborne 2003). Here, we focus on ensemble approaches. Reichart and Rappoport (2007) selected high-quality constituency parses by using the level of agreement among 20 copies of the same parser, trained on different subsets of a training corpus. Experiments using training and test data for the same domain and in the parser-adaptation scenario showed improvements over several baselines. Sagae and Tsujii (2007) used an ensemble to select high-quality dependency parses. They compared the outputs of two statistical shift-reduce LR models and selected only iden"
J14-3001,W09-1120,0,0.0182701,"rors, a common strategy is to control inter-annotator agreement by making two annotators work on the same sentences. This makes the task even slower and more expensive. 2 The IULA Spanish LSP Treebank contains 43,000 annotated sentences, distributed among different domains (Law, Economy, Computing Science, Medicine, and Environment) and sentence lengths (ranging from 4 to 30 words). The treebank is publicly available at http://metashare.upf.edu. 524 Marimon, Bel, and Padro´ Automatic Selection of HPSG-Parsed Sentences 2008), (iii) statistics about PoS sequences in a batch of parsed sentences (Reichart and Rappoport 2009), and (iv) ensemble parse algorithms (Reichart and Rappoport 2007; Sagae and Tsujii 2007; Baldridge and Osborne 2003). Here, we focus on ensemble approaches. Reichart and Rappoport (2007) selected high-quality constituency parses by using the level of agreement among 20 copies of the same parser, trained on different subsets of a training corpus. Experiments using training and test data for the same domain and in the parser-adaptation scenario showed improvements over several baselines. Sagae and Tsujii (2007) used an ensemble to select high-quality dependency parses. They compared the outputs"
J14-3001,D07-1111,0,0.0278777,"n the same sentences. This makes the task even slower and more expensive. 2 The IULA Spanish LSP Treebank contains 43,000 annotated sentences, distributed among different domains (Law, Economy, Computing Science, Medicine, and Environment) and sentence lengths (ranging from 4 to 30 words). The treebank is publicly available at http://metashare.upf.edu. 524 Marimon, Bel, and Padro´ Automatic Selection of HPSG-Parsed Sentences 2008), (iii) statistics about PoS sequences in a batch of parsed sentences (Reichart and Rappoport 2009), and (iv) ensemble parse algorithms (Reichart and Rappoport 2007; Sagae and Tsujii 2007; Baldridge and Osborne 2003). Here, we focus on ensemble approaches. Reichart and Rappoport (2007) selected high-quality constituency parses by using the level of agreement among 20 copies of the same parser, trained on different subsets of a training corpus. Experiments using training and test data for the same domain and in the parser-adaptation scenario showed improvements over several baselines. Sagae and Tsujii (2007) used an ensemble to select high-quality dependency parses. They compared the outputs of two statistical shift-reduce LR models and selected only identical parses, in their"
J14-3001,W06-1604,0,0.0855075,"Missing"
L16-1140,P09-1030,0,0.0757768,"Missing"
L16-1140,C94-1048,0,0.378969,"7 and, finally, conclusions and future work can be found in Section 8. 1 2. Related work Deriving new bilingual lexica from already existing ones in not new. Initial proposals typically used a pivot language to derive a new bilingual lexicon between two Source and Target languages, provided that the pairs Source/Pivot and Target/Pivot were already available. When using a pivot language to construct a bilingual dictionary, it is mandatory to discriminate inappropriate equivalences between words caused by translation ambiguities. A method to identify such incorrect translations was proposed by Tanaka & Umemura (1994) when constructing bilingual dictionaries intermediated by a third language. The method, known as one time inverse consultation (OTIC), was adapted by Lim et al. (2011) in the creation of multilingual lexicons from bilingual lists of words. More recently, different algorithms exploiting graph properties have been proposed to derive, enrich and/or validate lexical resources. Graph algorithms allow working with a larger number of lexicons. For instance the SenseUniformPaths algorithm (Soderland, 2009), based on graph sampling, uses probabilistic methods to infer lexical translations. The SenseUn"
L16-1353,P13-1040,0,0.0606994,"Missing"
L16-1353,P14-1023,0,0.0525432,"Missing"
L16-1353,N13-1056,0,0.10222,"characteristics. More interestingly, it is claimed that the relationship between vector spaces that represent different language word semantics can be captured by a linear transformation. Therefore, we propose to use word embedding vectors of translation word pairs to train a supervised classifier which can predict whether a new pair of words in two different languages can be under a translation relation, according to the regularities learned from a small training set. A previous attempt to use supervised classification for inducing bilingual lexica from non-parallel nor comparable corpora is Irvine and Callison-Burch (2013), although their approach is based on using non-distributional information (signals like temporal similarity, topical information, orthographical similarity, among others) to train the model. Moreover, the use of supervised methods has proved to be effective in statistical machine translation (SMT) too, a closely related task (Och & Ney, 2002). Our approach benefits from a supervised method and from the dense and small vectors produced by word2vec tool (Mikolov et al. 2013b) to build a binary classifier which can find the general relation between translation word pairs. Our classifier achieved"
L16-1353,D14-1177,0,0.0579608,"Missing"
L16-1353,W14-1618,0,0.0689534,"Missing"
L16-1353,W13-3523,0,0.0527046,"Missing"
L16-1353,S15-1021,1,0.871233,"Missing"
L16-1353,P02-1038,0,0.247906,"Missing"
L16-1353,P95-1050,0,0.396535,"Missing"
L16-1353,N03-1033,0,0.0143754,"either parallel nor comparable. In addition, the corpora that we used for the ranking task are WMT11 5 text data of English (59M) and Spanish (59M). Training set and test set are prepared in the same way. For the binary classification experiment, each source word can only be paired with one target word. To obtain a translation list (or positive instances called right translation) for training and testing, we randomly extracted a list of words for each PoS (only noun, verb and adjective), from the ES monolingual corpus. To PoS tag the Spanish and English corpora, we used Stanford PoS Tagger 6 (Toutanova et al., 2003). These randomly selected words were translated from source language (ES) to target language (EN and CH) using on-line Google Translator. Since not all the produced translations could be found in the target monolingual corpus, we removed from our datasets those words whose corresponding translation was not in the target corpus because we needed to obtain its word embedding. To build the no-translation set (called no translation), we randomly selected non-related source and target words from the monolingual corpus of each language and randomly combined them. The ratio was 5 negative instances f"
L16-1353,N09-2031,0,0.0655888,"Missing"
L16-1437,2005.eamt-1.3,0,0.024167,"MT2 is assigned a much lower score by UPF-Cobalt. Although all content words are matched they occur in different contexts and receive a high context penalty (0.90 for the main verb ”discussed” and 0.80 for the arguments ”government” and ”documents”). Thus, UPF-Cobalt is capable of distinguishing the use of equivalent constructions (active/passive alternation) from translation errors. The context penalty values calculated for each pair of aligned words can be used for locating translation errors. Examples of acceptable syntactic variation are frequently found in professional human translation (Ahrenberg, 2005). Translators often introduce optional changes to the original sentence in order to adhere to specific principles of target language use, resulting in the existence of various possible translations with a varying distance from the source sentence. If the available human reference contains optional changes with respect to the source, surface-level comparison is not informative, as the absence of such changes is not indicative of low MT quality. 4. Experiments The performance of evaluation systems is typically assessed by comparing the scores produced by the metrics with the results of manual MT"
L16-1437,J10-4006,0,0.0152224,"ng stage in order to avoid penalizing syntactic variation. 3.1.2. Distributional Similarity To get better lexical coverage, we integrate two additional levels to the MWA’s lexical similarity component. In addition to the Paraphrase Database, UPF-Cobalt employs WordNet synonyms (Miller and Fellbaum, 2007) and distributed word representations (Mikolov et al., 2013). WordNet and paraphrase databases are commonly used in MT evaluation for dealing with lexical variation. By contrast, to the best of our knowledge, distributional similarity has not yet been exploited. Distributional semantic models (Baroni and Lenci, 2010) have been shown to perform well across a variety of lexical similarity tasks. They are grounded on distributional hypothesis (Harris, 1954) that states that semantic similarity between two words can be modeled as a function of the degree of overlap between their contexts. In this framework, words are represented as vectors in which each entry is a measure of association between the word and a particular context. The similarity between two given words is then computed using some distance measure on the corresponding vectors. 2756 Using distributional similarity in combination with contextual i"
L16-1437,P14-1023,0,0.0105179,"correspondence between the words ”agreement” and ”consent” can be easily established with the help of common lexical similarity resources such as WordNet. This is not the case, however, with the words ”signalled” and ”given”, which can be considered semantically equivalent only given the equivalence of their contexts. Recently it has been proposed to represent words as dense vectors derived by various training methods inspired from neural-network language modeling (Mikolov et al., 2013). These representations, referred to as word embeddings, have been shown to outperform previous approaches (Baroni et al., 2014). We use dependency-based word embeddings developed by Levy and Goldberg (2014) and cosine similarity as a distance measure. The words that have cosine similarity higher than a threshold1 and at least one pair of exact matching content words in their contexts are considered candidates for alignment. 3.2. Scoring At the scoring stage we want to know if the word correspondences identified by the aligner are actually indicative of MT quality. UPF-Cobalt calculates a score for each pair of aligned words as a combination of their lexical similarity and a context penalty which measures the differenc"
L16-1437,W07-0718,0,0.112134,"is thus based on the defining properties of the translation and constitutes a powerful and intuitive instrument for assessing MT quality. Measuring absolute quality on a interval level scale, however, presents a problem of low inter-annotator agreement. The scale is arbitrary and no precise instructions are given to the annotators. As a result, different judges may assign different scores for the same sentence. To overcome this issue an alternative setting has been introduced, in which the judges are asked to rank different MTs of the same source sentences in terms of their relative quality (Callison-Burch et al., 2007). While this formulation of the task results in a higher inter-annotator agreement, it is less informative than absolute quality judgments. It has been shown that the performance of automatic evaluation systems varies significantly depending on the type of human judgments and the error metric (Denkowski and Lavie, 2010). Different types of human judgments pose different challenges to automatic evaluation systems. Ranking can be more difficult when very similar MTs have to be compared, in which case fine-grained distinctions between different kind of errors have to be made. On the other hand, i"
L16-1437,comelles-etal-2012-verta,0,0.345979,"Missing"
L16-1437,de-marneffe-etal-2006-generating,0,0.0203326,"Missing"
L16-1437,2010.amta-papers.20,0,0.236538,"annotators. As a result, different judges may assign different scores for the same sentence. To overcome this issue an alternative setting has been introduced, in which the judges are asked to rank different MTs of the same source sentences in terms of their relative quality (Callison-Burch et al., 2007). While this formulation of the task results in a higher inter-annotator agreement, it is less informative than absolute quality judgments. It has been shown that the performance of automatic evaluation systems varies significantly depending on the type of human judgments and the error metric (Denkowski and Lavie, 2010). Different types of human judgments pose different challenges to automatic evaluation systems. Ranking can be more difficult when very similar MTs have to be compared, in which case fine-grained distinctions between different kind of errors have to be made. On the other hand, in the ranking task the scores produced by a metric are not assessed directly. Ranking judgments provide little insight regarding how well the magnitude of the differences in quality between the MTs of different source sentences is reflected in automatic evaluation. MT can be evaluated at system or at sentence level. Sys"
L16-1437,W14-3348,0,0.613397,"are also present in the reference. This approach, however, is not reliable since the same source sentence can be correctly translated in many different ways. The fact that the MT output does not match one of the possible translation options is not necessarily indicative of low MT quality. Substantial work has focused on improving reference-based evaluation with various strategies: use of additional references (Albrecht and Hwa, 2008; Madnani and Dorr, 2013; Fomicheva et al., 2015a), integration of linguistic information (Pad´o et al., 2009; Gim´enez and M`arquez, 2010; Comelles et al., 2012; Denkowski and Lavie, 2014; Guzm´an et al., 2014) and use of machine learning techniques (Gupta et al., 2015; Herrera et al., 2015). Despite important achievements, automatic evaluation is still a poor substitute for manual quality assessment. The correlation between the metrics’ scores and human judgments of translation quality at sentence level continues to be low. The reason is that when comparing candidate and reference translations, the metrics are not able to distinguish acceptable linguistic variation from the differences that are indicative of MT errors. In this work we propose to use local context to discrimin"
L16-1437,N12-1017,0,0.0271912,"s based on surface-level similarity between the MT and a reference translation penalize acceptable differences induced by the use of semantically equivalent expressions that do not match in their surface forms. At the same time, the matches between the words that happen to have the same form but play totally different roles in the corresponding sentences incorrectly increase the evaluation score. The issue of acceptable variation has been addressed by using additional references. It has been shown that the performance of BLEU is improved when various human translations are used as benchmarks (Dreyer and Marcu, 2012). Having multiple human references is expensive. Albrecht and Hwa (2008) use pseudo-references as additional source of information. Data-driven (Owczarzak et al., 2006) and rule-based paraphrase generation (Fomicheva et al., 2015a) have also been explored. These approaches, however, fail to estimate the varying impact of different types of candidatereference mismatches on MT quality. An alternative strategy is to refine the comparison between the candidate MT and the available human translation. Meteor (Denkowski and Lavie, 2014) allows for stem, synonym and paraphrase matches, thus addressing"
L16-1437,W15-3046,1,0.676693,"ce. For example, the well known metric BLEU (Papineni et al., 2002) measures the number of word n-grams in the candidate translation that are also present in the reference. This approach, however, is not reliable since the same source sentence can be correctly translated in many different ways. The fact that the MT output does not match one of the possible translation options is not necessarily indicative of low MT quality. Substantial work has focused on improving reference-based evaluation with various strategies: use of additional references (Albrecht and Hwa, 2008; Madnani and Dorr, 2013; Fomicheva et al., 2015a), integration of linguistic information (Pad´o et al., 2009; Gim´enez and M`arquez, 2010; Comelles et al., 2012; Denkowski and Lavie, 2014; Guzm´an et al., 2014) and use of machine learning techniques (Gupta et al., 2015; Herrera et al., 2015). Despite important achievements, automatic evaluation is still a poor substitute for manual quality assessment. The correlation between the metrics’ scores and human judgments of translation quality at sentence level continues to be low. The reason is that when comparing candidate and reference translations, the metrics are not able to distinguish acce"
L16-1437,N13-1092,0,0.022911,"gnificant improvements (Thadani et al., 2012). The alignment module of UPF-Cobalt builds on an existing system Monolingual Word Aligner (MWA) which takes context information into account and has been shown to significantly outperform state-of-the-art results (Sultan et al., 2014). 3.1.1. Monolingual Word Aligner MWA makes alignment decisions based on lexical similarity and contextual evidence. The lexical similarity component identifies the word pairs that are possible candidates for alignment. Two levels of similarity are defined. In addition to the exact or lemma match, Paraphrase Database (Ganitkevitch et al., 2013) of lexical and phrasal paraphrases is employed to recognize semantically similar words. Context words are considered as evidence for alignment if they are lexically similar and have the same or equivalent syntactic relations with the words to be aligned. Syntactic equivalence is established through a mapping between different syntactic functions that instantiate the same semantic relation. Some examples of such functions are: possession modifier and noun compound modifier, indirect object and prepositional modifier, relative clause modifier and reduced non-finite verbal modifier, nominal subj"
L16-1437,D15-1124,0,0.147062,"Missing"
L16-1437,P14-1065,0,0.130305,"Missing"
L16-1437,P15-1078,0,0.0166416,"n be correctly translated in many different ways. The fact that the MT output does not match one of the possible translation options is not necessarily indicative of low MT quality. Substantial work has focused on improving reference-based evaluation with various strategies: use of additional references (Albrecht and Hwa, 2008; Madnani and Dorr, 2013; Fomicheva et al., 2015a), integration of linguistic information (Pad´o et al., 2009; Gim´enez and M`arquez, 2010; Comelles et al., 2012; Denkowski and Lavie, 2014; Guzm´an et al., 2014) and use of machine learning techniques (Gupta et al., 2015; Herrera et al., 2015). Despite important achievements, automatic evaluation is still a poor substitute for manual quality assessment. The correlation between the metrics’ scores and human judgments of translation quality at sentence level continues to be low. The reason is that when comparing candidate and reference translations, the metrics are not able to distinguish acceptable linguistic variation from the differences that are indicative of MT errors. In this work we propose to use local context to discriminate between acceptable and non-acceptable differences. Thus, variation between the MT and a human transla"
L16-1437,P14-2050,0,0.0211784,"stablished with the help of common lexical similarity resources such as WordNet. This is not the case, however, with the words ”signalled” and ”given”, which can be considered semantically equivalent only given the equivalence of their contexts. Recently it has been proposed to represent words as dense vectors derived by various training methods inspired from neural-network language modeling (Mikolov et al., 2013). These representations, referred to as word embeddings, have been shown to outperform previous approaches (Baroni et al., 2014). We use dependency-based word embeddings developed by Levy and Goldberg (2014) and cosine similarity as a distance measure. The words that have cosine similarity higher than a threshold1 and at least one pair of exact matching content words in their contexts are considered candidates for alignment. 3.2. Scoring At the scoring stage we want to know if the word correspondences identified by the aligner are actually indicative of MT quality. UPF-Cobalt calculates a score for each pair of aligned words as a combination of their lexical similarity and a context penalty which measures the difference in their syntactic contexts. 3.2.1. Lexical Similarity The values for differe"
L16-1437,W05-0904,0,0.141601,"ht and Hwa (2008) use pseudo-references as additional source of information. Data-driven (Owczarzak et al., 2006) and rule-based paraphrase generation (Fomicheva et al., 2015a) have also been explored. These approaches, however, fail to estimate the varying impact of different types of candidatereference mismatches on MT quality. An alternative strategy is to refine the comparison between the candidate MT and the available human translation. Meteor (Denkowski and Lavie, 2014) allows for stem, synonym and paraphrase matches, thus addressing the problem of acceptable variation at lexical level. Liu and Gildea (2005) propose a series of syntactic features based on the degree of overlap between the syntactic trees of candidate and reference translations. Translation quality is a complex object involving different aspects. A number of successful approaches, therefore, combine different types information. Thus, Gim´enez and M`arquez (2010) propose a combination of specialized similarity measures operating at various linguistic levels (lexical, syntactic and semantic). Guzm´an et al. (2014) further enrich this metric set with discourse level information, obtaining a marginal improvement. Our work follows this"
L16-1437,W15-3031,0,0.0932781,"Missing"
L16-1437,W15-3048,0,0.203839,"Missing"
L16-1437,W06-3112,0,0.0183364,"that do not match in their surface forms. At the same time, the matches between the words that happen to have the same form but play totally different roles in the corresponding sentences incorrectly increase the evaluation score. The issue of acceptable variation has been addressed by using additional references. It has been shown that the performance of BLEU is improved when various human translations are used as benchmarks (Dreyer and Marcu, 2012). Having multiple human references is expensive. Albrecht and Hwa (2008) use pseudo-references as additional source of information. Data-driven (Owczarzak et al., 2006) and rule-based paraphrase generation (Fomicheva et al., 2015a) have also been explored. These approaches, however, fail to estimate the varying impact of different types of candidatereference mismatches on MT quality. An alternative strategy is to refine the comparison between the candidate MT and the available human translation. Meteor (Denkowski and Lavie, 2014) allows for stem, synonym and paraphrase matches, thus addressing the problem of acceptable variation at lexical level. Liu and Gildea (2005) propose a series of syntactic features based on the degree of overlap between the syntactic"
L16-1437,P09-1034,0,0.326268,"Missing"
L16-1437,P02-1040,0,0.122783,"ings. Keywords: Machine Translation, Evaluation, Local Context, Alignment 1. Introduction Automatic evaluation of Machine Translation (MT) is based on the idea that the closer the MT output is to a human reference translation, the higher its quality. Thus, the task is typically approached by measuring some kind of similarity between the MT (also called candidate translation) and a reference translation. Most widely used evaluation systems follow a simple strategy of counting the number of matching words and word strings in the MT and a human reference. For example, the well known metric BLEU (Papineni et al., 2002) measures the number of word n-grams in the candidate translation that are also present in the reference. This approach, however, is not reliable since the same source sentence can be correctly translated in many different ways. The fact that the MT output does not match one of the possible translation options is not necessarily indicative of low MT quality. Substantial work has focused on improving reference-based evaluation with various strategies: use of additional references (Albrecht and Hwa, 2008; Madnani and Dorr, 2013; Fomicheva et al., 2015a), integration of linguistic information (Pa"
L16-1437,W15-3050,0,0.163894,"Missing"
L16-1437,Q14-1018,0,0.230827,"e difference of their syntactic contexts, if any. The number and the syntactic functions of the context words are taken into consideration. In this way, the metric can make fine-grained distinctions regarding the relative importance of the differences between the MT and the reference translation. Furthermore, we increase the coverage of the cases of acceptable differences. At lexical level distributed representations of words (Mikolov et al., 2013) are used in order to identify contextual synonyms. At syntactic level, we take advantage of the classes of equivalent dependency types proposed by Sultan et al. (2014). Using contextual information with the aforementioned enhancements helps to distinguish Machine Translations (MTs) that are different from the human translation and still essentially correct from those that share a high number of words with the reference but alter the meaning of the sentence due to translation errors. We conduct experiments with the data from two different evaluation tasks with various types of human judgments of MT quality provided. The metric achieves competitive results in varying evaluation settings, including the well known Metrics Task at the Association for Computation"
L16-1437,C12-2120,0,0.0305807,"MT and the human reference must be taken into consideration. Therefore, we have chosen to perform the evaluation in two stages. First, the MT output is aligned to the reference. Next, the MT is scored taking into account both the number of aligned words and their roles in the corresponding sentences. 3.1. Alignment In our setting, it is important to establish the relations between candidate and reference words correctly. Research in the area of monolingual alignment demonstrates that exploiting syntactic context to discriminate between possible alignments results in significant improvements (Thadani et al., 2012). The alignment module of UPF-Cobalt builds on an existing system Monolingual Word Aligner (MWA) which takes context information into account and has been shown to significantly outperform state-of-the-art results (Sultan et al., 2014). 3.1.1. Monolingual Word Aligner MWA makes alignment decisions based on lexical similarity and contextual evidence. The lexical similarity component identifies the word pairs that are possible candidates for alignment. Two levels of similarity are defined. In addition to the exact or lemma match, Paraphrase Database (Ganitkevitch et al., 2013) of lexical and phr"
L16-1437,W15-3053,0,0.106475,"Missing"
L16-1724,W13-0901,0,0.0897901,"normal (norms), and some unfrequent contexts (exploitations) among which are metaphors, that are used to express new insights with a rhetorical effect (Hanks, 2004). Such an effect can have different degrees of strength, which correspond to the degrees of metaphoricity of the metaphor: the stronger the rhetorical effect, the higher the degree of metaphoricity. The usefulness of considering metaphoricity when investigating and modelling metaphors has been stressed both from the theoretical point of view (Dunn, 2010; Hanks, 2004; Hanks, 2006; Nunberg, 1987) and in the field on NLP (Dunn, 2014; Dunn, 2013; Hovy et al., 2013; Mohler et al., 2015). (Shutova, 2015) in her vast review on metaphor processing systems suggests that real-world NLP applications should be concerned with the identification of metaphorical expressions with high degree of metaphoricity, i.e. those expressions that need to be interpreted differently from literal language and therefore processed with specific tools. At the opposite, they should not address low-metaphoricity expressions, since their meanings are already present in dictionaries and can be interpreted using standard word sense disambiguation techniques. From th"
L16-1724,P14-2121,0,0.335568,"e considered normal (norms), and some unfrequent contexts (exploitations) among which are metaphors, that are used to express new insights with a rhetorical effect (Hanks, 2004). Such an effect can have different degrees of strength, which correspond to the degrees of metaphoricity of the metaphor: the stronger the rhetorical effect, the higher the degree of metaphoricity. The usefulness of considering metaphoricity when investigating and modelling metaphors has been stressed both from the theoretical point of view (Dunn, 2010; Hanks, 2004; Hanks, 2006; Nunberg, 1987) and in the field on NLP (Dunn, 2014; Dunn, 2013; Hovy et al., 2013; Mohler et al., 2015). (Shutova, 2015) in her vast review on metaphor processing systems suggests that real-world NLP applications should be concerned with the identification of metaphorical expressions with high degree of metaphoricity, i.e. those expressions that need to be interpreted differently from literal language and therefore processed with specific tools. At the opposite, they should not address low-metaphoricity expressions, since their meanings are already present in dictionaries and can be interpreted using standard word sense disambiguation techniq"
L16-1724,W13-0907,0,0.118467,"ms), and some unfrequent contexts (exploitations) among which are metaphors, that are used to express new insights with a rhetorical effect (Hanks, 2004). Such an effect can have different degrees of strength, which correspond to the degrees of metaphoricity of the metaphor: the stronger the rhetorical effect, the higher the degree of metaphoricity. The usefulness of considering metaphoricity when investigating and modelling metaphors has been stressed both from the theoretical point of view (Dunn, 2010; Hanks, 2004; Hanks, 2006; Nunberg, 1987) and in the field on NLP (Dunn, 2014; Dunn, 2013; Hovy et al., 2013; Mohler et al., 2015). (Shutova, 2015) in her vast review on metaphor processing systems suggests that real-world NLP applications should be concerned with the identification of metaphorical expressions with high degree of metaphoricity, i.e. those expressions that need to be interpreted differently from literal language and therefore processed with specific tools. At the opposite, they should not address low-metaphoricity expressions, since their meanings are already present in dictionaries and can be interpreted using standard word sense disambiguation techniques. From this point of view, t"
L16-1724,P14-2050,0,0.0246293,"-steps methodology: firstly, a vector representation of each context in which a target verb occurs was created. In the second step, a clustering algorithm was employed in order to identify similar vector representations and, therefore, similar contexts. As for the realization of the first step, we initially extracted all the sentences in which a target verbs occurs in the British National Corpus. For each sentence we then selected the subject and object of the verb, and matched them with the corresponding vectorial representation, using the dependency based word embeddings (WE) introduced by (Levy and Goldberg, 2014). WE are low dimensional, dense and real-valued vectors which preserve syntactic and semantic information of words, and that have been proved to be efficient in several NLP tasks, such as detection of relational similarity (Mikolov et al., 2013b), word similarity tasks (Mikolov et al., 2013a) and contextual similiarity (Melamud et al., 2015). When both subject and object were available in the same sentence, the context vector was defined by averaging them (Melamud et al., 2015). Otherwise, if one of the two was not present, the context vector would be equivalent to the available one. In the se"
L16-1724,N10-1039,0,0.032836,"ation and interpretation, and are based on three main theoretical frameworks. 3 An example of a mild metaphor including ’butcher’ is ’Croatian and Bosnian fascists butchered Serbs’. The first one is the Conceptual Metaphor Theory (CMT) (Lakoff and Johnson, 1980), whereby a metaphor consists of a source-target mapping: metaphor modelling, thus, is performed discovering whether this mapping is present or not and finding the corresponding literal meaning (Shutova and Sun, 2013). The second theoretical framework is the Selectional Restrictions Hypothesis (Wilks, 1978), implemented for example in (Li and Sporleder, 2010). The basic idea here is that a metaphoric expression is characterized by the usage of a word that is not semantically related to the other words in the utterance, and that it is possible to detect metaphors through this semantic mismatching. Finally, the Abstractness Assumption (Turney et al., 2011) leverages the idea that metaphors occur when an abstract concept is explained using a more concrete one. Metaphor modelling, therefore, requires a measure of abstractness for target lexical items and their contexts. Independently of the different conceptual framework adopted, the majority of the s"
L16-1724,N15-1050,0,0.0305924,"a target verbs occurs in the British National Corpus. For each sentence we then selected the subject and object of the verb, and matched them with the corresponding vectorial representation, using the dependency based word embeddings (WE) introduced by (Levy and Goldberg, 2014). WE are low dimensional, dense and real-valued vectors which preserve syntactic and semantic information of words, and that have been proved to be efficient in several NLP tasks, such as detection of relational similarity (Mikolov et al., 2013b), word similarity tasks (Mikolov et al., 2013a) and contextual similiarity (Melamud et al., 2015). When both subject and object were available in the same sentence, the context vector was defined by averaging them (Melamud et al., 2015). Otherwise, if one of the two was not present, the context vector would be equivalent to the available one. In the second step, we identified groups of similar contexts of the verb by clustering the context vectors obtained in phase 1. We used the Birch algorithm for its reliable performances with large sets of data (Zhang et al., 1996) and because the final number of clusters does not have to be previously defined: this is in line with the fact that the n"
L16-1724,N13-1090,0,0.00510103,"exts. As for the realization of the first step, we initially extracted all the sentences in which a target verbs occurs in the British National Corpus. For each sentence we then selected the subject and object of the verb, and matched them with the corresponding vectorial representation, using the dependency based word embeddings (WE) introduced by (Levy and Goldberg, 2014). WE are low dimensional, dense and real-valued vectors which preserve syntactic and semantic information of words, and that have been proved to be efficient in several NLP tasks, such as detection of relational similarity (Mikolov et al., 2013b), word similarity tasks (Mikolov et al., 2013a) and contextual similiarity (Melamud et al., 2015). When both subject and object were available in the same sentence, the context vector was defined by averaging them (Melamud et al., 2015). Otherwise, if one of the two was not present, the context vector would be equivalent to the available one. In the second step, we identified groups of similar contexts of the verb by clustering the context vectors obtained in phase 1. We used the Birch algorithm for its reliable performances with large sets of data (Zhang et al., 1996) and because the final"
L16-1724,T87-1040,0,0.1298,"or few frequent contexts of use that are considered normal (norms), and some unfrequent contexts (exploitations) among which are metaphors, that are used to express new insights with a rhetorical effect (Hanks, 2004). Such an effect can have different degrees of strength, which correspond to the degrees of metaphoricity of the metaphor: the stronger the rhetorical effect, the higher the degree of metaphoricity. The usefulness of considering metaphoricity when investigating and modelling metaphors has been stressed both from the theoretical point of view (Dunn, 2010; Hanks, 2004; Hanks, 2006; Nunberg, 1987) and in the field on NLP (Dunn, 2014; Dunn, 2013; Hovy et al., 2013; Mohler et al., 2015). (Shutova, 2015) in her vast review on metaphor processing systems suggests that real-world NLP applications should be concerned with the identification of metaphorical expressions with high degree of metaphoricity, i.e. those expressions that need to be interpreted differently from literal language and therefore processed with specific tools. At the opposite, they should not address low-metaphoricity expressions, since their meanings are already present in dictionaries and can be interpreted using standa"
L16-1724,N13-1118,0,0.0231206,"Missing"
L16-1724,J15-4002,0,0.0718194,"rdingly to what said, the semantic properties of a given verb can be leveraged not only to determine the degree of metaphoricity of a specific input sentence including that verb (Dunn, 2014), but also to predict the upper-bound of metaphoricity of any expression in which it occurs. We believe that such an information could be highly useful for systems that perform metaphor detection, since it would allow to a priori exclude metaphoric expressions that, being created with low-POM verbs (e.g. ’take’), can only have low degrees of metaphoricity. In this way, it would be possible to realize what (Shutova, 2015) suggests, that is: to label as metaphoric only those expressions that, having high metaphoricity, are truly figurative, and that therefore need to be interpreted differently from literal expressions, while ignoring slightly metaphoric expressions. In this wok we introduce a method to define the POM of a verb based on its distributional behaviour. We follow (Hanks, 2006) and conjecture that verbs that occur with high frequency in many contexts (e.g. ’take a decision’, ’take a train’, etc.) lose the potential to be used in sentences with high degrees of metaphoricity, while verbs that have just"
L16-1724,D11-1063,0,0.0328876,"target mapping: metaphor modelling, thus, is performed discovering whether this mapping is present or not and finding the corresponding literal meaning (Shutova and Sun, 2013). The second theoretical framework is the Selectional Restrictions Hypothesis (Wilks, 1978), implemented for example in (Li and Sporleder, 2010). The basic idea here is that a metaphoric expression is characterized by the usage of a word that is not semantically related to the other words in the utterance, and that it is possible to detect metaphors through this semantic mismatching. Finally, the Abstractness Assumption (Turney et al., 2011) leverages the idea that metaphors occur when an abstract concept is explained using a more concrete one. Metaphor modelling, therefore, requires a measure of abstractness for target lexical items and their contexts. Independently of the different conceptual framework adopted, the majority of the systems in literature model metaphor as a discrete property, ignoring the fact that several degrees of metaphoricity are possible. To our knowledge, the only work that explicitly addresses metaphoricity is (Dunn, 2014), which introduces a computationallyderived scalar measurement of metaphoricity and"
L18-1406,W06-1615,0,0.132506,"o compensate this bias, we combined it with chi-squared selected ones. Thus, our system first ranks the best candidates in two separated lists, each using a different measure. Then, the two lists are joined into a new one by summing the AMI and chi-squared scores3. For instance, if a word is ranked 3rd by AMI and 5th by chisquared, in the joined list it will be the 8th. A single BoW of 600 features was used for all the aspect classifiers. As for domain adaptation methods, there are a number of different algorithms developed for compensating the degradation in performance. Daumé III (2007) and Blitzer et al. (2006) assumed the availability of some labelled For the classifiers, we trained SMO classifiers (as implemented by Weka, Hall et al., 2009). Texts were processed as follows. First, they were cleaned eliminating urls, hashtags, and rare characters. Second, texts were 2 3 ABSA 2016 presented an Out-of-Domain track only for French, but no participants registered for it. 2560 In case of tie, results are ordered alphabetically. tokenized and lemmatized using Freeling 4.0 (Padró & Stanilovsky, 2012). Stop words were eliminated before assessing the combined AMI+chi-squared rank explained before. Note that"
L18-1406,S14-2059,0,0.0721149,"Missing"
L18-1406,S14-2149,0,0.0742229,"Missing"
L18-1406,S14-2145,0,0.0665387,"Missing"
L18-1406,S14-2135,0,0.178637,"g of the classifiers would be necessary, and the classifiers would have to perform well in both domains. • • Note that for aspect identification, to retrieve a related word, although not exactly a corresponding analogue word, should be enough as the goal is to take into account words that refer to a particular aspect of a product. It could be different for polarity analysis where it is not the same to observe &apos;good&apos; than &apos;bad&apos;. But for aspect identification both, even if antonymous, refer to quality, for instance. For computing the offset, we used the 3COSMUL method as proposed by Levy et al. (2014). 3COSMUL was demonstrated to better balance the different aspects of similarity to prevent that similarity aspects in different scales can be more predominant in the calculation. The list of analogues proposed by 3COSMUL, which comes from all the corpus vocabulary, was filtered by discarding stopwords and forms not found in a spelling dictionary. Therefore, the list of features used for the out-of-domain classification experiment included the initial ones and the features that were ranked first by 3COSMUL that were actual words (preventing, for instance, forms such as tablespoonful) and were"
L18-1406,P07-1033,0,0.271252,"Missing"
L18-1406,S14-2148,0,0.0537413,"Missing"
L18-1406,S14-2076,0,0.0695523,"Missing"
L18-1406,P15-1027,0,0.036335,"Missing"
L18-1406,W14-1618,0,0.321017,"of the new domain to retrain, which, in practice, are not available. Daumé III (2007) proposed an approach for supervised adaptation by changing the selected features for ones relevant to the new domain and re-training the classifiers with an augmented list of features. Our method, explained in next section, proposed to augment the initial list of features by projecting them into the new domain. We formulated the problem as analogy questions. Word analogy questions have been used to demonstrate that vector space representations consistently encode linguistic regularities (Mikolov et al. 2013, Levy et al., 2014, Linzen, 2016, among others). These linguistic relations are referred as “syntactic”, including morphological relations such as verbal base forms and gerund forms, or “semantic” involving world knowledge such as currencies in different countries. Our task was closer to find semantic relations, as we intended to find words expressing specialization of taxonomic relations, for instance finding parts-of or properties-of. To our knowledge, this is the first time that word analogy method is applied to a domain shift problem. 3. Methodology Our proposal worked upon a basic text classification appro"
L18-1406,W16-2503,0,0.0572757,"Missing"
L18-1406,S14-2090,0,0.0715614,"Missing"
L18-1406,S14-2122,0,0.0718288,"Missing"
L18-1406,S14-2101,0,0.0656592,"Missing"
L18-1406,S14-2124,0,0.0604732,"Missing"
L18-1406,S14-2041,0,0.0658099,"Missing"
L18-1406,padro-stanilovsky-2012-freeling,0,0.10583,"Missing"
marimon-bel-2004-lexical,C96-1049,0,\N,Missing
marimon-bel-2004-lexical,C96-2180,0,\N,Missing
marimon-bel-2004-lexical,P01-1034,0,\N,Missing
marimon-bel-2004-lexical,P98-2195,0,\N,Missing
marimon-bel-2004-lexical,C98-2190,0,\N,Missing
marimon-bel-2004-lexical,P98-2144,0,\N,Missing
marimon-bel-2004-lexical,C98-2139,0,\N,Missing
marimon-etal-2012-iula,W09-3032,0,\N,Missing
marimon-etal-2012-iula,taule-etal-2008-ancora,0,\N,Missing
marimon-etal-2012-iula,branco-etal-2010-developing,0,\N,Missing
marimon-etal-2014-iula,W97-1502,0,\N,Missing
marimon-etal-2014-iula,W09-3032,0,\N,Missing
marimon-etal-2014-iula,W06-2920,0,\N,Missing
marimon-etal-2014-iula,marimon-etal-2012-iula,1,\N,Missing
marimon-etal-2014-iula,I13-1123,0,\N,Missing
marimon-etal-2014-iula,taule-etal-2008-ancora,0,\N,Missing
marimon-etal-2014-iula,branco-etal-2010-developing,0,\N,Missing
marimon-etal-2014-iula,I05-2035,0,\N,Missing
morell-etal-2012-iula2standoff,W07-1501,0,\N,Missing
morell-etal-2012-iula2standoff,ide-suderman-2006-integrating,0,\N,Missing
N07-2002,A97-1052,0,0.0843225,"ues to be used as features for training a Decision Tree (DT). Section 4 shortly introduces the methodology and data used in the experiments whose results are presented in section 5. And in section 6 we conclude by comparing with the published results for similar tasks and we sketch future research. 2 State of the art Most of the work on deep lexical information acquisition has been devoted to verbs. The existing acquisition systems learn very specialized linguistic information such as verb subcategorization frame2. The results for verb subcategorization are mostly around the 0.8 of precision. Briscoe & Carroll (1997) reported a type precision of 0,76 and a type recall of 0.43. Their results were improved by the work of Korhonen (2002) with a type precision of 0.87 and a recall of 0.68 using external resources to filter noise. Shulte im Walde (2002) reports a precision of 0.65 and a recall of 0.58. Chesley & Salmon-Alt (2006) report a precision of 0.86 and a recall of 0.54 for verb subcategorization acquisition for French. Lexical acquisition for nouns has been concerned mainly with ontological classes and has mainly worked on measuring semantic similarity on the basis of occurrence contexts. As for gramma"
N07-2002,chesley-salmon-alt-2006-automatic,0,0.0477142,"the art Most of the work on deep lexical information acquisition has been devoted to verbs. The existing acquisition systems learn very specialized linguistic information such as verb subcategorization frame2. The results for verb subcategorization are mostly around the 0.8 of precision. Briscoe & Carroll (1997) reported a type precision of 0,76 and a type recall of 0.43. Their results were improved by the work of Korhonen (2002) with a type precision of 0.87 and a recall of 0.68 using external resources to filter noise. Shulte im Walde (2002) reports a precision of 0.65 and a recall of 0.58. Chesley & Salmon-Alt (2006) report a precision of 0.86 and a recall of 0.54 for verb subcategorization acquisition for French. Lexical acquisition for nouns has been concerned mainly with ontological classes and has mainly worked on measuring semantic similarity on the basis of occurrence contexts. As for grammatical information, the work of Baldwin and Bond (2003) in acquisition of countability features for English nouns also tackles the very important problem of feature selection. Other work like Carroll and Fang’s (2004) and Baldwin’s (2005) have focused on grammatical information acquisition for HPSG based computati"
N07-2002,W05-1008,0,\N,Missing
N07-2002,P03-1059,0,\N,Missing
necsulescu-etal-2014-combining,D12-1042,0,\N,Missing
necsulescu-etal-2014-combining,W09-0205,0,\N,Missing
necsulescu-etal-2014-combining,C92-2082,0,\N,Missing
necsulescu-etal-2014-combining,P06-1040,0,\N,Missing
necsulescu-etal-2014-combining,P09-1113,0,\N,Missing
necsulescu-etal-2014-combining,J06-3003,0,\N,Missing
necsulescu-etal-2014-combining,P06-1038,0,\N,Missing
necsulescu-etal-2014-combining,P11-1055,0,\N,Missing
necsulescu-etal-2014-combining,J06-1005,0,\N,Missing
necsulescu-etal-2014-combining,W11-2501,0,\N,Missing
necsulescu-etal-2014-combining,W03-1210,0,\N,Missing
necsulescu-etal-2014-combining,C08-1114,0,\N,Missing
necsulescu-etal-2014-combining,P99-1008,0,\N,Missing
necsulescu-etal-2014-combining,P10-1013,0,\N,Missing
necsulescu-etal-2014-combining,N03-1011,0,\N,Missing
necsulescu-etal-2014-combining,P02-1006,0,\N,Missing
P13-2127,J08-4004,0,0.183848,"20 171 298 82 95 140 139 U 7 54 25 22 48 91 83 69 47 V 3 8 0 3 3 53 44 54 40 B 4 48 25 19 45 38 39 15 7 Table 3: Literal, Metonymic and Underspecified sense distributions, and underspecified senses broken down in Voting and Backoff Average observed agreement (Ao ) is the mean across examples for the proportion of matching senses assigned by the annotators. Krippendorff’s alpha is an aggregate measure that takes chance disagreement in consideration and accounts for the replicability of an annotation scheme. There are large differences in α across datasets. The scheme can only provide reliable (Artstein and Poesio, 2008) annotations (α &gt; 0.6) for one dot type2 . This indicates that not all dot types are equally easy to annotate, regardless of the kind of annotator. In spite of the number and type of annotators, the Location/Organization dot type gives fairly high agreement values for a semantic task, and this behavior is consistent across languages. The columns labelled L, M and U in Table 3 provide the sense distributions for each dot type. The preference for the underspecified sense varies greatly, from the very infrequent for English in Animal/Meat to the two Danish datasets where the underspecified sense"
P13-2127,J12-3005,0,0.0920653,"s, the results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations. 1 ´ Nuria Bel Universitat Pompeu Fabra Barcelona (Spain) nuria.bel@upf.edu Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between senses and the cases of underspecification, including a contrastive study between languages. Here we describe the annotation"
P13-2127,W09-3716,0,0.0271131,"the annotations. 1 ´ Nuria Bel Universitat Pompeu Fabra Barcelona (Spain) nuria.bel@upf.edu Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between senses and the cases of underspecification, including a contrastive study between languages. Here we describe the annotation process, its results in terms of inter-encoder agreement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy and, MACE an unsupervised system to choose the most likely sense from all"
P13-2127,gonzalez-agirre-etal-2012-multilingual,0,0.0239491,"responding dot type. In spite of a part of the annotation being made with a contrastive study in mind, no parallel text was used to avoid using translated text. For English and Danish we used freely available reference corpora (Ide and Macleod, 2001; Andersen et al., 2002) and, for Spanish, a corpus built from newswire and technical text (Vivaldi, 2009). For most of the English examples we used the words in Rumshisky (2007), except for Location/Organization. For Danish and Spanish we translated the words from English. We expanded the lists using each language’s wordnet (Pedersen et al., 2009; Gonzalez-Agirre et al., 2012) as thesaurus to make the total of occurrences reach 500 after we had removed homonyms and other forms of semantic variation outside of the purview of regular polysemy. For Location/Organization we have used highfrequency names of geopolitical locations from each of the corpora. Many of them are corpusspecific (e.g. Madrid is more frequent in the Spanish corpus) but a set of words is shared: Afghanistan, Africa, America, China, England, Europe,Germany, London. Every dot type has its particularities that we had to deal with. For instance, English has lexical alFigure 1: Screen capture for a Mec"
P13-2127,N13-1132,0,0.0505433,"Missing"
P13-2127,P09-3001,0,0.0176863,"ement, and the sense distributions obtained with two methods: majority voting with a theory-compliant backoff strategy, and MACE, an unsupervised system to choose the most likely sense from all the annotations. 1 ´ Nuria Bel Universitat Pompeu Fabra Barcelona (Spain) nuria.bel@upf.edu Introduction This article shows the annotation task of a corpus in English, Danish and Spanish for regular polysemy. Regular polysemy (Apresjan, 1974; Pustejovsky, 1995; Briscoe et al., 1995; Nunberg, 1995) has received a lot of attention in computational linguistics (Boleda et al., 2012; Rumshisky et al., 2007; Shutova, 2009). The lack of available senseannotated gold standards with underspecification is a limitation for NLP applications that rely on dot types1 (Rumshisky et al., 2007; Poibeau, 2006; Pustejovsky et al., 2009). Our goal is to obtain human-annotated corpus data to study regular polysemy and to detect it in an automatic manner. We have collected a corpus of annotated examples in English, Danish and Spanish to study the alternation between senses and the cases of underspecification, including a contrastive study between languages. Here we describe the annotation process, its results in terms of inter-"
P13-2127,D08-1027,0,0.254133,"Missing"
P13-2127,J03-2004,0,0.0295571,") (ANC): a) Manuel died in exile in 1932 in England. b) England was being kept busy with other concerns c) England was, after all, an important wine market In case a), England refers to the English territory (Location), whereas in b) it refers arguably to England as a political entity (Organization). The third case refers to both. The ability of certain words to switch between semantic types in a predictable manner is referred to as regular polysemy. Unlike other forms of meaning variation caused by metaphor or homonymy, regular polysemy is considered to be caused by metonymy (Apresjan, 1974; Lapata and Lascarides, 2003). Regular polysemy is different from other forms of polysemy in that both senses can be active at the same in a predicate, which we refer to as underspecification. Underspecified instances can be broken down in: 1. Contextually complex: England was, after all, an important wine market 2. Zeugmatic, in which two mutually exclusive readings are coordinated: England is conservative and rainy 3. Vague, in which no contextual element enforces a reading: The case of England is similar We present the result of an annotation task on regular polysemy for a series of semantic classes or dot types in Eng"
P13-2127,markert-nissim-2002-towards,0,0.152885,"locations from each of the corpora. Many of them are corpusspecific (e.g. Madrid is more frequent in the Spanish corpus) but a set of words is shared: Afghanistan, Africa, America, China, England, Europe,Germany, London. Every dot type has its particularities that we had to deal with. For instance, English has lexical alFigure 1: Screen capture for a Mechanical Turk annotation instance or HIT This annotation scheme is designed with the intention of capturing literal, metonymic and underspecified senses, and we use an inventory of three possible answers, instead of using Markert and Nissim’s (Markert and Nissim, 2002; Nissim and Markert, 2005) approach with fine-grained sense distinctions, which are potentially more difficult to annotate and resolve automatically. Markert and Nissim acknowledge a mixed sense they define as being literal and metonymic at the same time. For English we used Amazon Mechanical Turk (AMT) with five annotations per example by turkers certified as Classification Masters. Using AMT provides annotations very quickly, possibly at the expense of reliability, but it has been proven suitable for sense-disambiguation task (Snow et al., 2008). Moreover, it is not possible to obtain annot"
P13-2127,D12-1017,0,0.0680757,"Table 5 breaks down the five annotations that each example received by turkers in literal, metonymic and underspecified. The last two columns show the sense tag provided by voting or MACE. Example d) e) f) g) h) i) j) L 2 3 1 2 2 3 1 M 2 1 2 2 2 0 2 U 1 1 2 1 1 2 2 VOTING U L M U U L M Conclusions MACE L U U M M U U 9 Table 5: Annotation summary and sense tags for the examples in this section Further work After collecting annotated data, the natural next step is to attempt class-based word-sense disambiguation (WSD) to predict the senses in Tables 3 and 4 using a state-of-the-art system like Nastase et al. (2012). We will consider a sense-assignment method (voting or MACE) as more appropriate if it provides the sense tags that are easiest to learn by our WSD system. However, learnability is only one possible parameter for quality, and we also want to develop an expert-annotated gold standard to compare our data against. We also consider the possibility of developing a sense-assignment method that relies both on the theoretical assumption behind the voting scheme and the latent-variable approach used by MACE. Just by looking at the table it is not immediate which method is preferable to assign sense ta"
poch-etal-2012-towards,ide-etal-2000-xces,0,\N,Missing
poch-etal-2012-towards,bramantoro-etal-2010-towards,0,\N,Missing
poch-etal-2012-towards,W07-1501,0,\N,Missing
poch-etal-2012-towards,2011.eamt-1.40,1,\N,Missing
poch-etal-2012-towards,murakami-etal-2010-language,0,\N,Missing
poch-etal-2014-ranking,C90-1014,0,\N,Missing
poch-etal-2014-ranking,padro-stanilovsky-2012-freeling,0,\N,Missing
R11-1041,2005.eamt-1.4,0,0.0316461,"Missing"
R11-1041,P06-4018,0,0.0082967,"e verbs that contain each element. 3. For each Incyta SCF minimal unit, it assesses the similarity with each SRG unit comparing the two binary vectors using the Jaccard distance measure, especially suited for binary vectors and as in (Chan and Wu, 1999). 4. It chooses as the corresponding elements those that maximize similarity. Once the corresponding elements have been extracted, a new feature structure is constructed substituting Incyta units with those from SRG and the actual merging with the SRG lexicon is done. Since the SCFs have a graph structure, we used a unification mechanism (NLTK, Bird 2006) to merge both lexica, lemma by lemma, as in Necsulescu et al. (2011). Thus, we obtained, totally automatically, a new lexicon that contains SCF information from both lexica. 3 Evaluation and Results To evaluate the results of our automatic mapping algorithm, we used the resulting lexicon of Necsulescu et al (2011) work as our gold-standard. To create this lexicon, Necsulescu et al (2011) developed a manually built set of extraction rules that converted Incyta list-based SCF’s into SRG-like feature structures. Once both dictionaries were reliably converted into the same format, they were merge"
R11-1041,W99-0630,0,0.173601,"ally equivalent pieces of information and to substitute the parenthetical list by the attribute-value equivalent matrix. 1 These characteristics made it not advisable to use LMF where lemma and sense are the mandatory information for a lexical entry. 2 Decorated lists, parenthetical or otherwise marked, have been a quite common way of representing SCF information, i.e. COMLEX, VERBNET among others. 297 2.1 Semantic Preserving Mapping Our experiment to avoid manual intervention when converting the two lexica into a common format with a blind, semantic preserving method departs from the idea of Chan and Wu (1999) to compare information contained in the same entries of different lexica, looking for significant equivalences. However they were working only with part-ofspeech tags, while we handle complex, structured information. Note that we need to automatically learn correspondences for both, labels (such as the label of a noun phrase) and structures (e.g. the representation of a prepositional phrase that is fulfilled by a clause phrase in indicative mode). The basic requirement for the automatic mapping is to have a number of verbs encoded in both lexica to be compared. Then it is possible to assess t"
R11-1041,W10-1840,0,0.0298176,"na related to verbal complements, their role and categorical characteristics expressed as restrictions. SCFs in the SRG lexicon are formulated in terms of feature-attribute value pairs, so they have a graph structure. In the Incyta lexicon, SCFs are represented as a list of parenthesis with less structured internal information2. In both cases, a lemma can have more than one SCF, and it is indeed the most frequent case as we will see later. For moredetails about these two lexica, see Necsulescu et al. (2011). In order to approach current proposals for standard formats (Francopoulo et al. 2008; Ide & Bunt, 2010) that recommend graph-based and attributevalue formalisms, we choose to map Incyta information towards SRG format which was closer to the standard recommendations. The devised method was to find semantically equivalent pieces of information and to substitute the parenthetical list by the attribute-value equivalent matrix. 1 These characteristics made it not advisable to use LMF where lemma and sense are the mandatory information for a lexical entry. 2 Decorated lists, parenthetical or otherwise marked, have been a quite common way of representing SCF information, i.e. COMLEX, VERBNET among oth"
R11-1041,marimon-2010-spanish,0,0.0278279,"-resourced languages. Any cost reduction will have a high impact in the actual re-use of resources. Thus, our objective was to reduce human intervention in the first step by devising a blind, semantic preserving mapping algorithm that covers the extraction of the information and the conversion into a format that allows, later, the merging. In our experiments, we wanted to merge two subcategorization lexica developed for rule-based grammars: the Spanish working lexicon of the Incyta Machine Translation system (Alonso, 2005) and the Spanish working lexicon of the Spanish Resource Grammar, SRG, (Marimon, 2010) developed for the LKB framework (Copestake, 2002). Note that different senses under the same lemma are not distinguished in these lexica, and thus, are not addressed in the research reported here1. SRG and Incyta lexica encode the same phenomena related to verbal complements, their role and categorical characteristics expressed as restrictions. SCFs in the SRG lexicon are formulated in terms of feature-attribute value pairs, so they have a graph structure. In the Incyta lexicon, SCFs are represented as a list of parenthesis with less structured internal information2. In both cases, a lemma ca"
R11-1041,W09-4619,0,0.0277107,"n their syntactic subcategorization in combination with their meaning, as described by WordNet, Cyc (Lenat, 1995) and VerbNet (Kipper et al., 2000). In this context, a proposal such as the Lexical Markup Framework, LMF (Francopoulo et al. 2008) is an attempt to standardize the format of 296 Proceedings of Recent Advances in Natural Language Processing, pages 296–301, Hissar, Bulgaria, 12-14 September 2011. computational lexica as a way to avoid the complexities of merging lexica with different structures. But there is no particular facility to easy the mapping from non-standard into standard. Molinero et al (2009) build a morphological and syntactic lexicon for Spanish (Leffe) by merging four different lexica. They convert these sources into the Alexina format which is compatible with LMF in order to merge them. Nevertheless, both the mapping to this common format and the merging of the resources is done using manually developed rules that need a deep knowledge of the lexica to be merged. The research presented here is closely related to Necsulescu et al (2011), that presents a method to automatically merge lexica using graph unification mechanism. To do so, the lexica need to be represented as feature"
R11-1041,J00-4006,0,\N,Missing
R11-1041,monachini-etal-2006-unified,0,\N,Missing
R11-1041,L10-1000,0,\N,Missing
rehm-etal-2014-strategic,P07-2045,0,\N,Missing
rehm-etal-2014-strategic,piperidis-etal-2014-meta,1,\N,Missing
rehm-etal-2014-strategic,piperidis-2012-meta,1,\N,Missing
romeo-etal-2014-cascade,bel-etal-2012-automatic,1,\N,Missing
romeo-etal-2014-cascade,W00-0103,0,\N,Missing
romeo-etal-2014-cascade,P90-1034,0,\N,Missing
romeo-etal-2014-cascade,W13-5404,1,\N,Missing
romeo-etal-2014-choosing,W99-0503,0,\N,Missing
romeo-etal-2014-choosing,bel-etal-2012-automatic,1,\N,Missing
romeo-etal-2014-choosing,N07-2002,1,\N,Missing
romeo-etal-2014-choosing,J12-3005,0,\N,Missing
romeo-etal-2014-choosing,P13-4006,0,\N,Missing
romeo-etal-2014-choosing,J10-4006,1,\N,Missing
romeo-etal-2014-choosing,P03-1059,0,\N,Missing
romeo-etal-2014-choosing,J01-3003,0,\N,Missing
S15-1021,W11-2501,0,0.37889,"tion about lexical and relational similarities for the classifier to generalize and to gain recall. Therefore, as further validation, a second experiment is carried out, where the systems have to classify word pairs from a different domain than the domains in the training set. The objective is to assess the importance of the domain-aware training instances for the classification. The K&H dataset contains only instances from three domains and is imbalanced between the number of instances across domains and relation types. Therefore, our second experiment tests each method on the BLESS dataset (Baroni and Lenci, 2011), which spans 17 topical domains and includes five relation types, the three in K&H and (a) attributes of concepts, a relation holding between nouns and adjectives, and (b) actions performed by/to concepts a relation holding between nouns and verbs. In total, the BLESS dataset contains 14400 positive instances and an equal number of negative instances. This experiment measures the generalizability of each system and tests the capabilities of the systems for lexical-semantic relation types other than taxonomic relations. Domain-aware training instances To show the importance of the domain-aware"
S15-1021,P14-1023,0,0.0665731,"nstances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations. The graph encodes the distributional behavior of each word in the pair and consequently, patterns of co-occurrence expressing each target relation are extracted from it as relational information. The second approach uses word embeddings which have been shown to preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target seman"
S15-1021,P99-1008,0,0.0152874,"ties among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisitio"
S15-1021,W03-0415,0,0.0316758,"es of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial rep183 resentation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined with lexical similarity calculated based on the distributional information of words (Cederberg and Widdows, 2003; Snow et al., 2004; Turney, 2006a; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009). However, (Turney, 2006b; Turney, 2008a) showed that relational similarity cannot be improved using the distributional similarity of words. In contrast with the previous approaches that took into account lexical and relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and"
S15-1021,P06-1038,0,0.0330214,"ttern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisition of meaningful patterns of co-occurrence cueing a single target relation (Snow et al., 2004; Girju et al., 2006; Davidov and Rappoport, 2006). More recent work focuses on methods for the classification of word pairs as instances of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial rep183 resentation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relational similarity was combined"
S15-1021,J13-3008,1,0.128812,"Missing"
S15-1021,P14-1113,0,0.167986,"jective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, depending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The goal of this work is to classify word pairs as instances of lexical-semantic relations. Given a set of target semantic relations R = {r1 , . . . , rn }, and a set of word pairs W = {(x, y)1 , . . . , (x, y)n }, the task is to label each word pair (x, y)"
S15-1021,J06-1005,0,0.0169068,"ation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed for the automatic acquisition of meaningful patterns of co-occurrence cueing a single target relation (Snow et al., 2004; Girju et al., 2006; Davidov and Rappoport, 2006). More recent work focuses on methods for the classification of word pairs as instances of several relations at once, based on their relational similarity. This similarity is calculated using a vectorial rep183 resentation for each pair, created by relying on cooccurrence contexts (Turney, 2008b; S´eaghdha and Copestake, 2009; Mintz et al., 2009). These representations are very sparse due to the scarce contexts where the members of many word pairs co-occur. Moreover, many semantically related word pairs do not co-occur in corpus. For overcoming these issues, relat"
S15-1021,C92-2082,0,0.691702,"been shown to preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techn"
S15-1021,W09-0205,0,0.151917,"e to requiring co-occurrence, other works have classified the relation of a word pair using lexical similarity, i.e., the similarity of the concepts themselves. Given two word pairs, (w1 , w2 ) and (w3 , w4 ), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect inProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – even wh"
S15-1021,S12-1047,1,0.748516,"to 3 words] y [0 to 1 words]. Using the initial set of lexical patterns extracted from a corpus, additional patterns are generated by optionally generalizing each word to its part of speech. For N seed pairs, the most frequent kN patterns are retained. We follow Turney (2008b) and set k = 20. The patterns retained are then used as features to train an SVM classifier over the set of possible relation types. DSZhila & DSLevy Word embeddings have previously been shown to accurately measure relational similarity; Zhila et al. (2013) demonstrate state-ofthe-art performance on SemEval-2012 Task 2 (Jurgens et al., 2012) which measures word pair similarity within a particular semantic relation (i.e., which pairs are most prototypical of a semantic relation). This approach can easily be extended to the classification setting: Given a target word pair (x, y), the similarity is computed between (x, y) and each word pair (x, y)i of a target relation r. The average of these similarity measurements was taken 187 as the final score for each relation r.4 Finally, the word pair is classified as an instance of the relation with the highest associated score. Two types of embeddings are used, (a) the word embeddings prod"
S15-1021,D10-1108,0,0.00836648,"64.7 F 8.7 14.2 59.0 21.0 14.2 73.1 74.2 73.5 76.4 Wikipedia P R F 77.0 11.7 20.4 89.4 16.2 27.5 94.0 75.5 83.7 32.8 22.6 26.8 27.7 15.6 20.0 96.8 87.7 92.0 97.6 89.3 93.2 95.4 86.1 90.5 96.7 88.4 92.4 Table 3: Aggregated results obtained for the indomain setup with the K&H dataset. Detailed results are presented in the Appendix A. occur in text. Therefore, in the first experiment, we test whether the recall of classification systems is improved when the word pair representation encodes information about lexical and relational similarity. As an evaluation dataset, we expand on the dataset of Kozareva and Hovy (2010) (K&H), which was collected from hyponym-hypernym instances from WordNet (Miller, 1995) spanning three topical domains: animals, plants and vehicles. Because our systems are capable of classifying instances with more than one relation at once, we enhance this dataset with instances of two more relation types: co-hyponymy and meronymy. Co-hyponyms are extracted directly from the K&H dataset: two words are co-hyponyms if they have the same direct ancestor.5 To avoid including generic nouns, such as “migrator” in the “animal” domain, only leaf nodes are considered. The meronym instances are extra"
S15-1021,P14-2050,0,0.201948,"enses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, depending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The goal of this work is to classify word pairs as instances of lexical-semant"
S15-1021,W14-1618,0,0.410157,"enses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, depending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The goal of this work is to classify word pairs as instances of lexical-semant"
S15-1021,P14-5010,0,0.00321295,"Setup Corpora Many pattern-based systems increase the size of the input corpus in an attempt to overcome data sparsity and to achieve a better recall. Therefore, in our experiments, we train our systems using two corpora of different sizes: the British National Corpus (BNC), a 100 million-word corpus, and a Wikipedia dump created from 5 million pages and containing 1.5 billion words. The size difference allows us to measure the potential impact of increased word co-occurrence on recall. Both corpora were initially parsed with the Stanford dependency parser in the collapsed dependency format (Manning et al., 2014). Embbedings WECEoffset and WECEconcat are implemented based on a bag-of-words (BoW) (Mikolov et al., 2013a) and based on dependency relations (Dep) (Levy and Goldberg, 2014a). Evaluation We compare each system by reporting precision (P), recall (R) and F1 measure (F). 4.2 Comparison Systems The two proposed models are compared with two state-of-the-art systems and one baseline system. PAIR C LASS The PairClass algorithm (Turney, 2008b) provides a state-of-the-art pattern-based approach for extracting and classifying the relationship between word pairs and has performed well for many relation"
S15-1021,N13-1090,0,0.549126,"i, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c; Levy and Goldberg, 2014b). Nevertheless, only one work has applied word embeddings for classifying instances of a lexical semantic relation, specifically the relation hyponymyhypernymy (Fu et al., 2014). This relation is more complex than other semantic relations tested and therefore, it is reflected in more than one offset, depending on the domain of each instance. The present work uses a machine learning approach to discover meaningful information for the semantic relations encoded in the dimensions of the embeddings. 3 Task description The go"
S15-1021,W12-4104,0,0.0214934,"nd relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and to increase the classification recall. As a first approach, we use a graph to model the distributional behavior of words. Other researchers used graph-based approaches to model corpus information for the extraction of co-hyponyms (Widdows and Dorow, 2002), hypernyms (Navigli and Velardi, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov et al., 2013a; Mikolov et al., 2013b;"
S15-1021,P09-1113,0,0.136745,"lexical-semantic relationship between two words. Approaches to classifying the relationship between a word pair have typically relied on the assumption that contexts where word pairs co-occur 182 David Jurgens McGill University Montreal, Canada jurgens@cs.mcgill.ca Roberto Navigli Universit`a “La Sapienza” Rome, Italy navigli@di.uniroma1.it will yield information on the semantic relation (if any) between them. Given a set of example word pairs having some relation, relation-specific patterns may be automatically acquired from the contexts in which these example pairs co-occur (Turney, 2008b; Mintz et al., 2009). Comparing these relation-specific patterns with those seen with other word pairs measures relational similarity, i.e., how similar is the relation holding between two word pairs. However, any classification system based on patterns of co-occurrence is limited to only those words co-occurring in the data considered; due to the Zipfian distribution of words, even in a very large corpus there are always semantically related word pairs that do not co-occur. As a result, these patternbased approaches have a strict upper-bound limit on the number of instances that they can classify. As an alternat"
S15-1021,P10-1134,1,0.846022,"roaches that took into account lexical and relational information as a linear combination of lexical and relational similarity scores, the present work focuses on introducing word pair representations that merge and jointly represent types of information: lexical and relational. In this way, we aim to reduce vector sparseness and to increase the classification recall. As a first approach, we use a graph to model the distributional behavior of words. Other researchers used graph-based approaches to model corpus information for the extraction of co-hyponyms (Widdows and Dorow, 2002), hypernyms (Navigli and Velardi, 2010) or synonyms (Minkov and Cohen, 2012), or for inducing word senses (Di Marco and Navigli, 2013). Navigli and Velardi (2010) have the most similar representation to ours, creating a graph that models only definitional sentences. In contrast, our objective is to create a general representation of the whole corpus that can be used for classifying instances of several lexical semantic relations. The second approach presented in this paper, relies on word embeddings to create word pair representations. Extensive experiments have leveraged word embeddings to find general semantic relations (Mikolov"
S15-1021,E09-1071,0,0.0480814,"Missing"
S15-1021,S13-2056,0,0.0316207,"al basis function kernel (Platt, 1999) is trained using WEKA (Hall et al., 2009) to classify each word pair based on its representation provided by a graph-based representation model (Section 3.1) or a word embeddings representation model (Section 3.2) for N different lexical relations. The SVM classifier generates a distribution over relation labels and the highest weighted label is selected as the relation holding between the members of the word pair. 4 Experiments While several datasets have been created for detecting semantic relations between two words in context (Hendrickx et al., 2010; Segura-Bedmar et al., 2013), in our work we focus on the classification of word pairs as instances of lexical-semantic relations out of context. The performance of the GraCE and WECE systems is tested across two datasets, focusing on their ability to classify instances of specific lexical-semantic relations as well as to provide insights into the systems’ generalization capabilities. 4.1 Experimental Setup Corpora Many pattern-based systems increase the size of the input corpus in an attempt to overcome data sparsity and to achieve a better recall. Therefore, in our experiments, we train our systems using two corpora of"
S15-1021,P06-1040,0,0.189331,"2 ) and (w3 , w4 ), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect inProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations. Th"
S15-1021,J06-3003,0,0.259422,"2 ) and (w3 , w4 ), if w1 is lexically similar to w3 and w2 to w4 (i.e., are pair-wise similar) then the pairs are said to have the same semantic relation. These two sources of information are used as two independent units: relational similarity is calculated using co-occurrence information; lexical similarity is calculated using distributional information (Snow et al., 2004; S´eaghdha and Copestake, 2009; Herdadelen and Baroni, 2009), and ultimately these scores are combined. Experimental evidence has shown that relational similarity cannot necessarily be revealed through lexical similarity (Turney, 2006b; Turney, 2008a), and therefore, the issue of how to collect inProceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 182–192, Denver, Colorado, June 4–5, 2015. formation for word pairs that do not co-occur is still an open problem. We propose two new approaches to representing word pairs in order to accurately classify them as instances of lexical-semantic relations – even when the pair members do not co-occur. The first approach creates a word pair representation based on a graph representation of the corpus created with dependency relations. Th"
S15-1021,C08-1114,0,0.0743919,"ing the type of lexical-semantic relationship between two words. Approaches to classifying the relationship between a word pair have typically relied on the assumption that contexts where word pairs co-occur 182 David Jurgens McGill University Montreal, Canada jurgens@cs.mcgill.ca Roberto Navigli Universit`a “La Sapienza” Rome, Italy navigli@di.uniroma1.it will yield information on the semantic relation (if any) between them. Given a set of example word pairs having some relation, relation-specific patterns may be automatically acquired from the contexts in which these example pairs co-occur (Turney, 2008b; Mintz et al., 2009). Comparing these relation-specific patterns with those seen with other word pairs measures relational similarity, i.e., how similar is the relation holding between two word pairs. However, any classification system based on patterns of co-occurrence is limited to only those words co-occurring in the data considered; due to the Zipfian distribution of words, even in a very large corpus there are always semantically related word pairs that do not co-occur. As a result, these patternbased approaches have a strict upper-bound limit on the number of instances that they can cl"
S15-1021,C02-1114,0,0.394996,"preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, depending on the domain, author, and writing style, which may not match the originally identified patterns. Moreover, the identification of high-quality patterns is costly and time-consuming, and must be repeated for each new relation type, domain and language. To overcome these limitations, techniques have been developed"
S15-1021,N13-1120,0,0.0989502,"of the corpus created with dependency relations. The graph encodes the distributional behavior of each word in the pair and consequently, patterns of co-occurrence expressing each target relation are extracted from it as relational information. The second approach uses word embeddings which have been shown to preserve linear regularities among words and pairs of words, therefore, encoding lexical and relational similarities (Baroni et al., 2014), a necessary property for our task. In two experiments comparing with state-of-the-art pattern-based and embedding-based classifiers (Turney, 2008b; Zhila et al., 2013), we demonstrate that our approaches achieve higher accuracy with significantly increased recall. 2 Related work Initial approaches to the extraction of lexicalsemantic relations have relied on hand-crafted lexico-syntactic patterns to identify instances of semantic relations (Hearst, 1992; Widdows and Dorow, 2002; Berland and Charniak, 1999). These manually designed patterns are explicit constructions expressing a target semantic relation such as the pattern X is a Y for the relation of hypernymy. However, these approaches are limited because a relation may be expressed in many ways, dependin"
S15-1021,S10-1006,0,\N,Missing
S16-1002,L16-1465,1,0.768602,"Missing"
S16-1002,S15-2080,0,0.0503377,"Missing"
S16-1002,klinger-cimiano-2014-usage,0,0.0621363,"Missing"
S16-1002,P15-2128,0,0.0333833,"Missing"
S16-1002,S16-1003,0,0.0786167,"Missing"
S16-1002,S13-2052,0,0.0105895,"Missing"
S16-1002,piperidis-2012-meta,0,0.0160887,"Missing"
S16-1002,S14-2004,1,0.673256,"Missing"
S16-1002,S15-2082,1,0.813624,"Missing"
S16-1002,S14-2009,0,0.0111835,"Missing"
S16-1002,S15-2078,0,0.0105712,"Missing"
S16-1002,D13-1170,0,0.0173861,"Missing"
S16-1002,E12-2021,0,0.0937892,"Missing"
vazquez-bel-2012-classification,padro-etal-2010-freeling,0,\N,Missing
vazquez-bel-2012-classification,baccianella-etal-2010-sentiwordnet,0,\N,Missing
vazquez-bel-2012-classification,C00-1044,0,\N,Missing
vazquez-bel-2012-classification,H05-1044,0,\N,Missing
vazquez-bel-2012-classification,W02-2034,0,\N,Missing
vazquez-bel-2012-classification,W03-1014,0,\N,Missing
vazquez-bel-2012-classification,N10-1119,0,\N,Missing
vazquez-bel-2012-classification,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
villegas-bel-2002-dtd,bel-etal-2000-simple,1,\N,Missing
villegas-etal-2010-case,P07-2037,0,\N,Missing
villegas-etal-2010-case,atserias-etal-2006-freeling,0,\N,Missing
villegas-etal-2012-using,padro-etal-2010-freeling,0,\N,Missing
villegas-etal-2012-using,W02-2004,0,\N,Missing
W06-1001,francopoulo-etal-2006-lexical,1,\N,Missing
W06-1001,bertagna-etal-2004-content,1,\N,Missing
W07-1214,W05-1008,0,0.030259,"RED time_n_rel] The integration of these messy details allows us to release the analysis process from certain tasks that may be reliably dealt with by shallow external components. 3 Automatic Lexical Acquisition We have investigated Machine Learning (ML) methods applied to the acquisition of the information contained in the lexicon of the SRG. ML applied to lexical acquisition is a very active area of work linked to deep linguistic analysis due to the central role that lexical information has in lexicalized grammars and the costs of handcrafting them. Korhonen (2002), Carroll and Fang (2004), Baldwin (2005), Blunsom and Baldwin (2006), and Zhang and Kordoni (2006) are just a few examples of reported research work on deep lexical acquisition. The most successful systems of lexical acquisition are based on the linguistic idea that the contexts where words occur are associated to particular lexical types. Although the methods are different, most of the systems work upon the syntactic information on words as collected from a corpus, and they develop different techniques to decide whether this information is relevant for type assignment or it is noise, especially when there are just a few examples. I"
W07-1214,W06-1620,0,0.0117584,"The integration of these messy details allows us to release the analysis process from certain tasks that may be reliably dealt with by shallow external components. 3 Automatic Lexical Acquisition We have investigated Machine Learning (ML) methods applied to the acquisition of the information contained in the lexicon of the SRG. ML applied to lexical acquisition is a very active area of work linked to deep linguistic analysis due to the central role that lexical information has in lexicalized grammars and the costs of handcrafting them. Korhonen (2002), Carroll and Fang (2004), Baldwin (2005), Blunsom and Baldwin (2006), and Zhang and Kordoni (2006) are just a few examples of reported research work on deep lexical acquisition. The most successful systems of lexical acquisition are based on the linguistic idea that the contexts where words occur are associated to particular lexical types. Although the methods are different, most of the systems work upon the syntactic information on words as collected from a corpus, and they develop different techniques to decide whether this information is relevant for type assignment or it is noise, especially when there are just a few examples. In the LKB grammatical framew"
W07-1214,A00-1031,0,0.0441527,"itting. • Morpho-syntactic disambiguation. analysis and • Named entity detection and classification. • Date/number/currency/ratios/physical magnitude (speed, weight, temperature, density, etc.) recognition. • Chart-based shallow parsing. • WordNet-based sense annotation. • Dependency parsing. FreeLing also includes a guesser to deal with words which are not found in the lexicon by computing the probability of each possible PoS tag given the longest observed termination string for that word. Smoothing using probabilities of shorter termination strings is also performed. Details can be found in Brants (2000) and Samuelson (1993). Our system integrates the FreeLing tool by means of the LKB Simple PreProcessor Protocol (SPPP; http://wiki.delph-in.net/moin/LkbSppp), which assumes that a preprocessor runs as an external process to the LKB system, and uses the LKB inflectional rule component to convert the PoS tags delivered by the FreeLing tool into partial descriptions of feature structures. 2.1 The integration of PoS tags The integration of the morpho-syntactic analysis in the LKB system using the SPPP protocol means defining inflectional rules that propagate the morpho-syntactic information associ"
W07-1214,C02-1071,1,0.768431,"o much more varied corpus data of the Corpus Tècnic de l’IULA, which includes specialized corpus of written text in the areas of computer science, environment, law, medicine and economics, collected from several sources, such as legal texts, textbooks, research reports, user manuals, … In these texts sentence length may range up to 70 words. The rest of the paper describes the preprocessing strategy we have adopted and on our on-going research on lexical acquisition. 2 Pre-processing in the SRG Following previous experiments within the Advanced Linguistic Engineering Platform (ALEP) platform (Marimon, 2002), we have integrated a shallow processing tool, the FreeLing tool, as a pre-processing module of the grammar. 106 The FreeLing tool is an open-source4 language analysis tool suite (Atserias et al., 2006) perfoming the following functionalities (though disambiguation, named entity classification and the last three functionalities have not been integrated): • Text tokenization (including MWU and contraction splitting). • Sentence splitting. • Morpho-syntactic disambiguation. analysis and • Named entity detection and classification. • Date/number/currency/ratios/physical magnitude (speed, weight,"
W07-1214,marimon-bel-2004-lexical,1,0.843037,"that are fully lexicalized and never show morpho-syntactic variation, a través de (through) and a buenas horas (finally). The integration of multiword expressions All multiword expressions in FreeLing are stored in a file. The format of the file is one multiword per line, having three fields each: form, lemma and PoS.7 (2) shows two examples of multiword fixed 5 Actually, Spanish weak pronouns are considered pronominal affixes rather than pronominal clitics. 6 The use of underspecified default lexical entries in a highly lexicalized grammar, however, may increase ambiguity and overgeneration (Marimon and Bel, 2004). 7 FreeLing only handles continuous multiword expressions. 107 The multiword form field may admit lemmas in angle brackets, meaning that any form with that lemma will be a valid component for the multiword. Tags are specified directly or as a reference to the tag of some of the multiword components. (3) builds a multiword with both singular and plural forms (apartado(s) de correos (P.O Box)). The tag of the multiform is that of its first form ($1) which starts with NC and takes the values for number depending on whether the form is singular or plural. (3) &lt;apartado>_de_correos apartado_de _co"
W07-1214,P04-1057,0,0.0395878,"Missing"
W07-1214,zhang-kordoni-2006-automated,0,0.0140337,"etails allows us to release the analysis process from certain tasks that may be reliably dealt with by shallow external components. 3 Automatic Lexical Acquisition We have investigated Machine Learning (ML) methods applied to the acquisition of the information contained in the lexicon of the SRG. ML applied to lexical acquisition is a very active area of work linked to deep linguistic analysis due to the central role that lexical information has in lexicalized grammars and the costs of handcrafting them. Korhonen (2002), Carroll and Fang (2004), Baldwin (2005), Blunsom and Baldwin (2006), and Zhang and Kordoni (2006) are just a few examples of reported research work on deep lexical acquisition. The most successful systems of lexical acquisition are based on the linguistic idea that the contexts where words occur are associated to particular lexical types. Although the methods are different, most of the systems work upon the syntactic information on words as collected from a corpus, and they develop different techniques to decide whether this information is relevant for type assignment or it is noise, especially when there are just a few examples. In the LKB grammatical framework, lexical types are defined"
W07-1214,N07-2002,1,\N,Missing
W07-1214,W02-1502,0,\N,Missing
W07-1214,atserias-etal-2006-freeling,0,\N,Missing
W11-3302,marimon-2010-spanish,0,0.0456296,"Missing"
W11-3302,monachini-etal-2006-unified,0,0.0608071,"Missing"
W11-3302,P06-4018,0,0.0401756,"Missing"
W11-3302,W99-0630,0,0.206715,"ructures. Nevertheless, the most consuming part of the previous task was the extraction and mapping from the original format of a lexicon to a common graph structure. In this section, we present our proposal to automatically perform this mapping, which is the main contribution of this paper. In section 5.2 we will compare the results of the manual and the automatic extraction and mapping phase to assess the usability of our approach. Our experiment to avoid manual intervention when converting the two lexica into a common format with a blind, semantic preserving method departs from the idea of Chan and Wu (1999) to compare information contained in the same entries of different lexica, looking for consistent, significant equivalences validated by a significant number of cases in the whole lexica. However, they were only mapping part-of-speech tags, while we needed to handle complex, structured information. Thus, our main goal was to reduce human intervention especially including An example of the code of one SCF in Incyta lexicon is (1): (1) (($SUBJ N1 N0 (FCP 0 INT) (MD-0 IND) (MD-INT SUB)) ($DOBJ N1)) Therefore, the information that had to be discovered was the following:  The Incyta lexicon marks"
W11-3302,W10-1840,0,\N,Missing
W11-3305,ide-etal-2000-xces,0,0.0448155,"n observed that these proposals have not been widely used. Other relevant projects have adapted some of these proposals to its concrete needs. KYOTO project (ICT211423) needed particular aspects found on LAF, MAF and SynAF which are really difficult to combine. Thus, a new annotation framework was designed to be compatible with LAF and 4.2.1 Travelling Object 1: XCES Although most of the deployed tools don’t use an XML format, it was considered to be the best option due to its numerous advantages, such as XML schemas, transformations, complex path queries, etc. XCES is the XML version of CES (Ide et al., 2000) which is a part of EAGLES guidelines for corpus representation to work in natural language processing applications. XCES documents used in the factory make use of the “header” and the “text” tags proposed. Thanks to the header, TO1 can store metadata to annotate the origin of the document, its title, the date, some key words, the language and some annotations to keep track of the web services which have processed the document. The “text” part of the XML contains the data itself. Depending of the level of data annota9 36 www.xces.org tion, this part has different versions. The basic and PoS ve"
W11-3305,W10-1840,0,0.0593755,"Missing"
W11-4604,E09-1005,0,0.0130194,"which is related to the lack of strategies to capture meaning underspecification. 3 State of the art The computational study of systematic polysemy has been geared to the collapsing of senses (Vossen et al., 1999; Buitelaar, 1998; Tomuro, 2001) prior to Word Sense Disambiguation (WSD). The best performance in WSD is obtained by supervised methods that require a very large amount of annotated learning data. The other main approach is to use a lexical knowledge base such as WordNet and a PageRank algorithm to compute the most likely sense in the sense enumeration of the lexical knowledge base (Agirre and Soroa, 2009). WordNet does not include the Location/Organization alternation in geopolitical locations, so the task at hands falls outside the traditional scope of WSD. The field of Named Entity Recognition (NER) shows two different approaches to regular-polysemy based sense alternations. In their account, Johannessen et al. (2005) differentiate what they call the Form over Function and the Function over Form strategy. Some NER systems assign a constant value to a word type, enforcing what Finkel et al. (2005) call label consistency, namely Form over Function. The Function over Form strategy, however, ass"
W11-4604,P05-1045,0,0.00423742,"ithm to compute the most likely sense in the sense enumeration of the lexical knowledge base (Agirre and Soroa, 2009). WordNet does not include the Location/Organization alternation in geopolitical locations, so the task at hands falls outside the traditional scope of WSD. The field of Named Entity Recognition (NER) shows two different approaches to regular-polysemy based sense alternations. In their account, Johannessen et al. (2005) differentiate what they call the Form over Function and the Function over Form strategy. Some NER systems assign a constant value to a word type, enforcing what Finkel et al. (2005) call label consistency, namely Form over Function. The Function over Form strategy, however, assigns a semantic type to the analyzed word depending on how it behaves in each context and is analogous to the work exposed in this article. A class of nominals that shows regular polysemy and is well studied is the deverbal noun (destruction, examination), which has distinct grammatical features that can help pinpoint its reading as either process or result, as covered in theory by Grimshaw (1990) and computationally acknowledged by Peris et al. (2009). There is also recent work in the identificati"
W11-4604,H92-1045,0,0.0794082,"lgarrif et al, 2004), which has only been used to establish the nominal word space. No other external resources like FrameNet or WordNet have been used, following Markert and Nissim’s (2009) claim that grammatical features tend to be the most discriminating features. For similar remarks, cf. Peris (2009), Rumshisky (2007). The hypotheses that regular polysemy alternations are often determined at subphrasal level can contradict traditional WSD algorithms like Page Rank, which have a larger scope of analysis. Selection of metonymical senses falls outside of the One-sense-per-discourse approach (Gale et al., 1992), since such approach has been phrased reLexical and grammatical features Figure 1: word sketch for ""country"" 21 Hector Martinez Alonso, Nuria Bel and Bolette Sandford Pedersen ´ 5.2 Following Joanis et al. (2006), the occurrences have been characterized in order to assess the amount of semantic information that their distributional data can provide. The total size of the feature space is of 317 binary features, divided as follows: 1. NP-traits (6 features): which describe the internal structure of the NP where t appears. The features indicate the presence of an adjective in the NP, of a commo"
W11-4604,J03-2004,0,0.0527164,"Missing"
W11-4604,S10-1005,0,0.0215225,"ng on how it behaves in each context and is analogous to the work exposed in this article. A class of nominals that shows regular polysemy and is well studied is the deverbal noun (destruction, examination), which has distinct grammatical features that can help pinpoint its reading as either process or result, as covered in theory by Grimshaw (1990) and computationally acknowledged by Peris et al. (2009). There is also recent work in the identification of metonymy (Markert and Nissim, 2009) as well as other Generative-Lexicon based sensedisambiguation works, such as Rumshisky et al. (2007) or Pustejovsky et al. (2010). Disambiguation systems, however, are still coping with the need of a representation and recognition of underspecification (Pustejovsky, 2009). The SIMPLE lexicon (Lenci et al., 2000) is a GL-compliant lexicon for twelve European languages. It describes its lexical items in terms of their position within a type ontology as well as 19 Hector Martinez Alonso, Nuria Bel and Bolette Sandford Pedersen ´ a qualia structure. SIMPLE list the Geopolitical Location class as a class associated to a complex type <Location,Human_Group>, which expresses the dot-type ambiguity of words of this class. Words"
W11-4604,N01-1010,0,0.0157248,"which can be seen a kind of underspecification. In spite of the GL's computational perspective, Natural Language Processing (NLP) implementations that examine the actual computational feasibility of the GL are few. Moreover, there is no overt attempt to identify the possible three behaviors of a dot type, as the dot predication has not been computationally tackled, which is related to the lack of strategies to capture meaning underspecification. 3 State of the art The computational study of systematic polysemy has been geared to the collapsing of senses (Vossen et al., 1999; Buitelaar, 1998; Tomuro, 2001) prior to Word Sense Disambiguation (WSD). The best performance in WSD is obtained by supervised methods that require a very large amount of annotated learning data. The other main approach is to use a lexical knowledge base such as WordNet and a PageRank algorithm to compute the most likely sense in the sense enumeration of the lexical knowledge base (Agirre and Soroa, 2009). WordNet does not include the Location/Organization alternation in geopolitical locations, so the task at hands falls outside the traditional scope of WSD. The field of Named Entity Recognition (NER) shows two different a"
W11-4604,W99-0512,0,0.0491728,"ible senses as most salient, as in k), which can be seen a kind of underspecification. In spite of the GL's computational perspective, Natural Language Processing (NLP) implementations that examine the actual computational feasibility of the GL are few. Moreover, there is no overt attempt to identify the possible three behaviors of a dot type, as the dot predication has not been computationally tackled, which is related to the lack of strategies to capture meaning underspecification. 3 State of the art The computational study of systematic polysemy has been geared to the collapsing of senses (Vossen et al., 1999; Buitelaar, 1998; Tomuro, 2001) prior to Word Sense Disambiguation (WSD). The best performance in WSD is obtained by supervised methods that require a very large amount of annotated learning data. The other main approach is to use a lexical knowledge base such as WordNet and a PageRank algorithm to compute the most likely sense in the sense enumeration of the lexical knowledge base (Agirre and Soroa, 2009). WordNet does not include the Location/Organization alternation in geopolitical locations, so the task at hands falls outside the traditional scope of WSD. The field of Named Entity Recogni"
W11-4604,bel-etal-2000-simple,1,0.673715,"on, examination), which has distinct grammatical features that can help pinpoint its reading as either process or result, as covered in theory by Grimshaw (1990) and computationally acknowledged by Peris et al. (2009). There is also recent work in the identification of metonymy (Markert and Nissim, 2009) as well as other Generative-Lexicon based sensedisambiguation works, such as Rumshisky et al. (2007) or Pustejovsky et al. (2010). Disambiguation systems, however, are still coping with the need of a representation and recognition of underspecification (Pustejovsky, 2009). The SIMPLE lexicon (Lenci et al., 2000) is a GL-compliant lexicon for twelve European languages. It describes its lexical items in terms of their position within a type ontology as well as 19 Hector Martinez Alonso, Nuria Bel and Bolette Sandford Pedersen ´ a qualia structure. SIMPLE list the Geopolitical Location class as a class associated to a complex type <Location,Human_Group>, which expresses the dot-type ambiguity of words of this class. Words that are considered geopolitical locations can be proper (Africa, Boston, China) or common (city, nation, state, etc) nouns. 4 Experiment We propose a classification experiment that id"
W11-4604,C10-1006,1,\N,Missing
W13-5404,bel-etal-2012-automatic,1,0.749479,"rpus data to acquire information on the sense composition of complex types. In line with approaches that explore corpus-based definitions of fine-grained distinctions that emerge as abstractions over the combinatorial patterns of lexical items (Ježek and Lenci, 2007), we use a classification approach based strictly on distributional evidence available in a corpus to automatically identify complex types. As most approaches in lexical semantic classification do not distinguish among related senses of the same word, considering it either as part of a class or not (Hindle, 1990; Bullinaria, 2008; Bel et al., 2012), our goal is to outline a strategy which automatically accounts for those nouns that belong to multiple classes, specifically to pinpoint complex-type nouns using distributional evidence. In this context we discuss an experiment involving two complex types in English: LOCATION•ORGANIZATION (LOC•ORG) and EVENT•INFORMATION (EVT•INF). Our hypothesis is that complex-type nouns demonstrate characteristic and indicative lexico-syntactic traits of more than one class, which allow us to use lexico-syntactic patterns over corpus data to automatically identify nouns for which there is distributional ev"
W13-5404,S12-1023,0,0.400656,"anisms in GL predict a noun like church, represented below, occurs not only in contexts typical of class x: ORG (see (1a)) and of class y: LOC (see (1b)), but also in contexts which activate the relation R1(x,y), i.e. contexts where both ORG and LOC senses are simultaneously activated (see (1c)). (3) These properties distinguish dot objects from simple types, unified types or standard generalization on types (cf. Pustejovsky, 1995: 141 and ff.). Moreover, the possibility to have word 1 Utt and Padó (2011) consider the importance of this distinction, proposing an automatic polysemy classifier. Boleda et al. (2012) also put forth an approach for predicting regular sense alternations in corpus data. However, both methods are based on external rich language resources, which besides only being available for a very restricted set of languages, do not necessarily mirror language use, as noted in the latter work. senses that semantically compose these words either individually or simultaneously activated, depending on the selectional environment, presents a challenge to NLP systems that deal with identifying word senses in context. In fact, these follow a one-word, one-sense approach, designed to identify a s"
W13-5404,W00-0103,0,0.676049,"ble on complex types not only can reduce the search space in disambiguation tasks, and thus the number of decisions needed, but can also provide grounds to opt for the non-disambiguation of instances when relevant, for example in co-predication contexts like (1c). Moreover, knowledge of the entire sense potential of a given word is sometimes required for specific tasks (see for instance Rumshisky et al. (2007) and Lenci et al. (2010)). Thus, information on the sense composition of complex types can be crucial in NLP, as it allows for the reduction of the amount of lexical semantic processing (Buitelaar, 2000) in tasks such as Information Retrieval, semantic role annotation, high-quality Machine Translation and Summarization, as well as Question Answering. In this paper we evaluate the possibility to employ information from actual language use as encoded in corpus data to acquire information on the sense composition of complex types. In line with approaches that explore corpus-based definitions of fine-grained distinctions that emerge as abstractions over the combinatorial patterns of lexical items (Ježek and Lenci, 2007), we use a classification approach based strictly on distributional evidence a"
W13-5404,lenci-etal-2010-building,0,0.0257416,"b), however in (1c) a decision for a single sense would have to be made, despite the fact that both senses are simultaneously activated by the context. Having rich information available on complex types not only can reduce the search space in disambiguation tasks, and thus the number of decisions needed, but can also provide grounds to opt for the non-disambiguation of instances when relevant, for example in co-predication contexts like (1c). Moreover, knowledge of the entire sense potential of a given word is sometimes required for specific tasks (see for instance Rumshisky et al. (2007) and Lenci et al. (2010)). Thus, information on the sense composition of complex types can be crucial in NLP, as it allows for the reduction of the amount of lexical semantic processing (Buitelaar, 2000) in tasks such as Information Retrieval, semantic role annotation, high-quality Machine Translation and Summarization, as well as Question Answering. In this paper we evaluate the possibility to employ information from actual language use as encoded in corpus data to acquire information on the sense composition of complex types. In line with approaches that explore corpus-based definitions of fine-grained distinctions"
W13-5404,J01-3003,0,0.0332349,"othesis to identify complex-type nouns Considering the above characterization of dot types, we assume them to be members of more than one lexical class, more precisely members of each class corresponding to the senses they are composed of. As members of more than one class, complex types are expected to occur in indicatory contexts of more than one individual class. With this in mind, we evaluate the possibility to automatically identify complex types using a cue-based classification methodology. Based on the Distributional Hypothesis (Harris, 1954), cue-based lexical semantic classification (Merlo and Stevenson, 2001) builds on the assumption that lexical semantic classes are emergent properties of a number of words that recurrently co-occur in a number of particular contexts. Thereby, as proposed by Bybee and Hopper (2001) and Bybee (2010), we understand lexical semantic classes as generalizations that come about when there is a systematic co-distribution for a number of words in a number of contexts. Different contexts where a number of words tend to occur thus become linguistic cues of a particular semantic property that a set of words has in common. Using these cues to gather indicatory distributional"
W13-5404,W11-0128,0,0.144398,"the same time (Pustejovsky, 1995: 223), illustrated in (1c). The levels of representation and generative mechanisms in GL predict a noun like church, represented below, occurs not only in contexts typical of class x: ORG (see (1a)) and of class y: LOC (see (1b)), but also in contexts which activate the relation R1(x,y), i.e. contexts where both ORG and LOC senses are simultaneously activated (see (1c)). (3) These properties distinguish dot objects from simple types, unified types or standard generalization on types (cf. Pustejovsky, 1995: 141 and ff.). Moreover, the possibility to have word 1 Utt and Padó (2011) consider the importance of this distinction, proposing an automatic polysemy classifier. Boleda et al. (2012) also put forth an approach for predicting regular sense alternations in corpus data. However, both methods are based on external rich language resources, which besides only being available for a very restricted set of languages, do not necessarily mirror language use, as noted in the latter work. senses that semantically compose these words either individually or simultaneously activated, depending on the selectional environment, presents a challenge to NLP systems that deal with iden"
W13-5404,P90-1034,0,0.844576,"al language use as encoded in corpus data to acquire information on the sense composition of complex types. In line with approaches that explore corpus-based definitions of fine-grained distinctions that emerge as abstractions over the combinatorial patterns of lexical items (Ježek and Lenci, 2007), we use a classification approach based strictly on distributional evidence available in a corpus to automatically identify complex types. As most approaches in lexical semantic classification do not distinguish among related senses of the same word, considering it either as part of a class or not (Hindle, 1990; Bullinaria, 2008; Bel et al., 2012), our goal is to outline a strategy which automatically accounts for those nouns that belong to multiple classes, specifically to pinpoint complex-type nouns using distributional evidence. In this context we discuss an experiment involving two complex types in English: LOCATION•ORGANIZATION (LOC•ORG) and EVENT•INFORMATION (EVT•INF). Our hypothesis is that complex-type nouns demonstrate characteristic and indicative lexico-syntactic traits of more than one class, which allow us to use lexico-syntactic patterns over corpus data to automatically identify nouns"
W13-5404,Y10-1097,0,0.0561524,"Missing"
W13-5411,J12-3005,0,0.101032,"tion 5) and conclude with final observations and future work (Sections 6 and 7). 2 Related Work Natural Language Processing (NLP) tasks that exploit distributional information are based on the Distributional Hypothesis (Harris, 1954). However, Pustejovsky and Je˘zek (2008) claim that only using distributional data cannot explain the variation of linguistic meaning in language, while Markert and Nissim (2009) refer to the challenges of dealing with regular polysemy as the different senses of polysemous words present obstacles due to varied use in context. Along this line, the empirical work of Boleda et al. (2012) showed that the skewed sense distribution of many words makes it difficult to distinguish evidence of a class from noise, presenting a challenge to model the relations between senses. When their machinelearning experiments reached the upper bound set by the inter-encoder agreement in their gold standard, they concluded that in order to improve the modelling of polysemy there is a need to shift from a type to a token-based (word-in-context) model (Sch¨utze, 1998; Erk and Pad´o, 2008). Hence, we employ a token-based model in our experiments. In our approach, we propose an unsupervised task usin"
W13-5411,W13-0602,0,0.0770537,"was being kept busy with other concerns. (c) England is conservative and rainy. In this example, (1a) shows the literal sense of England as a location, while (1b) demonstrates the metonymic sense of England as an organization. Dot types also allow for both senses to be simultaneously active in a predicate, as in example (1c). ´ Nuria Bel Universitat Pompeu Fabra Roc Boronat, 138 Barcelona (Spain) nuria.bel@upf.edu All proper names representative of geopolitical entities, for instance, demonstrate this type of classwide sense alternation, which is defined as regular polysemy (Apresjan, 1974). Copestake (2013) emphasizes the relevance of distributional evidence in tasks regarding phenomena characteristic to regular polysemy, such as underspecification, because it incorporates frequency effects and is theory-neutral, requiring only that examples cluster in a way that mirrors their senses. Thus far, underspecification in dot types has been formalized in the linguistic theory of lexical semantics, but has not been explicitly studied using WSI. Kilgariff (1997) claims that word senses should be “construed as abstractions over clusters of word usages”. Following this claim, our strategy employs WSI, whi"
W13-5411,D08-1094,0,0.0463486,"Missing"
W13-5411,E12-1060,0,0.088197,"ristic to regular polysemy, such as underspecification, because it incorporates frequency effects and is theory-neutral, requiring only that examples cluster in a way that mirrors their senses. Thus far, underspecification in dot types has been formalized in the linguistic theory of lexical semantics, but has not been explicitly studied using WSI. Kilgariff (1997) claims that word senses should be “construed as abstractions over clusters of word usages”. Following this claim, our strategy employs WSI, which aims to automatically induce senses of words by clustering patterns found in a corpus (Lau et al., 2012; Jurgens, 2012). In this way, we hypothesize that dot-type nominals will generate semantically more consistent (i.e. more homogeneous, cf. Section 5) groupings if clustered into more than two induced senses. This paper is organized as follows: we discuss related work (Section 2); elaborate upon our use of WSI and methodology employed (Section 3 and Section 4), as well as present results obtained; we discuss our results (Section 5) and conclude with final observations and future work (Sections 6 and 7). 2 Related Work Natural Language Processing (NLP) tasks that exploit distributional informat"
W13-5411,S10-1011,0,0.0222412,"nd Markert (2005) and Nastase et al. (2012), but we make use of a much larger amount of data and thus should suffer from less sparsity. The related experiment by Rumshisky et al. (2007) uses verbal arguments as features, while we use only a five-word context window. 2.1 Word Sense Induction As stated above, our main goal is to use WSI to capture the sense alternation of dot types in context. WSI methods, based on the distributional information available in corpus data, employ unsupervised means to induce senses using contexts of indicated target words without relying on handcrafted resources (Manandhar et al., 2010). Distributional Semantic Models (DSM) provide the groundwork for WSI. A DSM, also known as a Word Space Model (Turney and Pantel, 2010), attempts to describe the meaning of words by characterizing their usage over distributional patterns, i.e. their context. Each word is represented by a numeric vector positioned in a space where vectors for words that appear in similar contexts are closer to each other. Sense induction is achieved by building a DSM over a large corpus and clustering the contexts into induced senses. In recent years, WSI has been used with success for different tasks such as:"
W13-5411,P13-2127,1,0.599797,"Missing"
W13-5411,D12-1017,0,0.0562275,"Missing"
W13-5411,W11-1104,0,0.0137045,"r WSI. A DSM, also known as a Word Space Model (Turney and Pantel, 2010), attempts to describe the meaning of words by characterizing their usage over distributional patterns, i.e. their context. Each word is represented by a numeric vector positioned in a space where vectors for words that appear in similar contexts are closer to each other. Sense induction is achieved by building a DSM over a large corpus and clustering the contexts into induced senses. In recent years, WSI has been used with success for different tasks such as: novel sense detection (Lau et al., 2012), community detection (Jurgens, 2011) and graded sense disambiguation (Jurgens, 2012), among others. Jurgens (2011) previously employed WSI to discover overlaps in the distributional behavior of words in order to identify multiple senses with success. However, that work was not inclusive to any specific phenomenon of polysemy. Our objective is to cluster dot-type nominals according to their distributional evidence in context, using WSI to characterize the behavior of these nouns. 3 Method We use WSI to computationally assess the predicational behavior of dot types. To do this, we employ a WSI system to induce senses from a large"
W13-5411,S12-1027,0,0.0505611,"polysemy, such as underspecification, because it incorporates frequency effects and is theory-neutral, requiring only that examples cluster in a way that mirrors their senses. Thus far, underspecification in dot types has been formalized in the linguistic theory of lexical semantics, but has not been explicitly studied using WSI. Kilgariff (1997) claims that word senses should be “construed as abstractions over clusters of word usages”. Following this claim, our strategy employs WSI, which aims to automatically induce senses of words by clustering patterns found in a corpus (Lau et al., 2012; Jurgens, 2012). In this way, we hypothesize that dot-type nominals will generate semantically more consistent (i.e. more homogeneous, cf. Section 5) groupings if clustered into more than two induced senses. This paper is organized as follows: we discuss related work (Section 2); elaborate upon our use of WSI and methodology employed (Section 3 and Section 4), as well as present results obtained; we discuss our results (Section 5) and conclude with final observations and future work (Sections 6 and 7). 2 Related Work Natural Language Processing (NLP) tasks that exploit distributional information are based on"
W13-5411,D07-1043,0,0.0105421,"ample. For each sentence in the test data, we isolated the placeholder to disambiguate and we calculated the representation of the sentence within the corresponding WSI model using the specified 5-word context window. Once the vector for the sentence was obtained, we assigned the sentence to the induced sense representing the highest cosine similarity for each model (cf. Table 2 in Section 4 for evaluation). 4 Results To determine the success of our task for each class, sense representation and k value, we consider the information-theoretic measures of homogeneity, completeness and V-measure (Rosenberg and Hirschberg, 2007). These three measures compare the output of the clustering with a gold standard (as described in Section 3.1) and provide a score that can be interpreted in a manner similar to precision, recall and F1, respectively. Homogeneity determines to which extent each cluster only contains members of a single class, while completeness determines if all members of a given class are assigned to the same cluster. Both the homogeneity and completeness scores are bounded by 0.0 and 1.0, with 1.0 corresponding to the most homogeneous or complete solution, and can be interpreted in a manner similar to preci"
W13-5411,J98-1004,0,0.404918,"Missing"
W13-5411,W11-2214,0,\N,Missing
W15-1510,P14-1023,0,0.118963,"imensional, dense and real-valued vectors which preserve word syntactic and semantic information in a Vector Space Model (VSM). WE have recently been proved to be efficient in several NLP tasks, such as detection of relational similarity (Mikolov et al., 2013d), word similarity tasks (Mikolov et al., 2013a) and automatic building of bilingual lexica (Mikolov et al., 2013b), in which this word repre70 Proceedings of NAACL-HLT 2015, pages 70–78, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics sentations outperformed others with state-of-the-art methods (Baroni et al., 2014). However, at the best of our knowledge, this is the first work in which WE are used to represent and account for regular polysemy. Our work departs from the assumption that lexical classes related by regular polysemy are limited and known and that since the class-related senses of a polysemous nouns can be considered as modulations of meaning of a single lemma (Copestake and Briscoe, 1995), they are to be represented by a single vector. As a first step, we will prove through a clustering task that nouns instantiating a particular sense alternation (e.g. animal/food) group together and separat"
W15-1510,J12-3005,0,0.115316,"ber of the class ANIMAL but also of FOOD, thus defining its senses in terms of lexical semantic classes. For some polysemous nouns one sense can be much more frequent than the other, thus causing asymmetry in sense predominance; this is the case of turkey, in which the food sense is clearly more frequent than the animal one (Copestake and Briscoe, 1995). Given its pervasiveness in natural language, regular polysemy has been extensively investigated in lexical semantics (Apresjan, 1974; Nunberg, 1992). However, only few works attempted to computationally model this phenomenon (Copestake, 2013; Boleda et al., 2012b). The vast majority of applications treat regular polysemy like other phenomena of lexical ambiguity, such as homography, not considering the relevant theoretical differences between those phenomena, for example that regular polysemy is predictable, while homography is not (Utt and Pad´o, 2011). Information on regular polysemy would be valuable for a task like Word Sense Disambiguation, since it would reduce the number of possible options when choosing the right sense of a word. More generally, every lexical resource would benefit from the capability to cope with the shifts of meaning produc"
W15-1510,S12-1023,0,0.260149,"Missing"
W15-1510,W13-0602,0,0.0132196,"considered a member of the class ANIMAL but also of FOOD, thus defining its senses in terms of lexical semantic classes. For some polysemous nouns one sense can be much more frequent than the other, thus causing asymmetry in sense predominance; this is the case of turkey, in which the food sense is clearly more frequent than the animal one (Copestake and Briscoe, 1995). Given its pervasiveness in natural language, regular polysemy has been extensively investigated in lexical semantics (Apresjan, 1974; Nunberg, 1992). However, only few works attempted to computationally model this phenomenon (Copestake, 2013; Boleda et al., 2012b). The vast majority of applications treat regular polysemy like other phenomena of lexical ambiguity, such as homography, not considering the relevant theoretical differences between those phenomena, for example that regular polysemy is predictable, while homography is not (Utt and Pad´o, 2011). Information on regular polysemy would be valuable for a task like Word Sense Disambiguation, since it would reduce the number of possible options when choosing the right sense of a word. More generally, every lexical resource would benefit from the capability to cope with the shi"
W15-1510,frontini-etal-2014-polysemy,0,0.0617974,"Missing"
W15-1510,W14-1503,0,0.0139157,"ice is in line with previous research (Levy and Goldberg, 2014) that proved how the embeddings created on input annotated with dependency relations better represent similarity (i.e. the paradigmatic relation existing between words, e.g. coffee and tea) compared with embedding created on linear contexts, which tend to encode more contextual information, or relatedness (e.g. coffee and cup). Second, the size of the window was 1 word either side of the target word: once again, the reason is that smaller context windows have been proved to improve the ability of the model to represent similarity (Kiela and Clark, 2014). Finally, consistently with the theoretic approach adopted, a single WE was created for each noun; thus, only one vector representation was available for the two senses of a disemous noun. 3.2 Clustering WE have proved to be a representation that preserves semantic information in a vectorial space. Therefore, since nouns belonging to the same lexical class are close in the semantic space, a clustering algorithm should be capable of discovering the portion of the space where all the members of a class are located and include them in a cluster, thus separating them from nouns of other classes."
W15-1510,P14-2050,0,0.0303848,"the fact that all the cases of polysemy in the dataset instantiated a single sense alternation (animal/food). 3.1 Word Embeddings WE of size 200 were trained using the word2vec toolkit1 with the CBOW architecture, which has been proved computationally efficient for large datasets (Mikolov et al., 2013b; Baroni et al., 2014). 1 https://code.google.com/p/word2vec/ 72 Given the results of some preliminary studies, three relevant choices were made in the training phase. Firstly, WE were trained on a parsed version of the British National Corpus (BNC). The choice is in line with previous research (Levy and Goldberg, 2014) that proved how the embeddings created on input annotated with dependency relations better represent similarity (i.e. the paradigmatic relation existing between words, e.g. coffee and tea) compared with embedding created on linear contexts, which tend to encode more contextual information, or relatedness (e.g. coffee and cup). Second, the size of the window was 1 word either side of the target word: once again, the reason is that smaller context windows have been proved to improve the ability of the model to represent similarity (Kiela and Clark, 2014). Finally, consistently with the theoreti"
W15-1510,P13-2127,1,0.854901,"r a given pair of lexical classes, defines the degree of membership of a noun to each class: polysemy is hence implicitly represented as an intermediate value on the continuum between two classes. We finally show that by exploiting the information provided by the sense index it is possible to accurately detect polysemous nouns in the dataset. 1 Introduction A major issue in lexical semantics is regular polysemy (also known as systematic or logical polysemy), the phenomenon whereby words belonging to a semantic class can predictably act as members of another class (Pustejovsky, 1991; Mart´ınez Alonso et al., 2013). For example, the word chicken can be considered a member of the class ANIMAL but also of FOOD, thus defining its senses in terms of lexical semantic classes. For some polysemous nouns one sense can be much more frequent than the other, thus causing asymmetry in sense predominance; this is the case of turkey, in which the food sense is clearly more frequent than the animal one (Copestake and Briscoe, 1995). Given its pervasiveness in natural language, regular polysemy has been extensively investigated in lexical semantics (Apresjan, 1974; Nunberg, 1992). However, only few works attempted to c"
W15-1510,N13-1090,0,0.1091,"not (Utt and Pad´o, 2011). Information on regular polysemy would be valuable for a task like Word Sense Disambiguation, since it would reduce the number of possible options when choosing the right sense of a word. More generally, every lexical resource would benefit from the capability to cope with the shifts of meaning produced by regular polysemy, and this, in turn, would lead to improvements in several NLP applications as such machine translation, textual entailment or text analytics. In this paper we present a method for polysemy detection and representation based on Word Embeddings (WE) (Mikolov et al., 2013a). WE are low dimensional, dense and real-valued vectors which preserve word syntactic and semantic information in a Vector Space Model (VSM). WE have recently been proved to be efficient in several NLP tasks, such as detection of relational similarity (Mikolov et al., 2013d), word similarity tasks (Mikolov et al., 2013a) and automatic building of bilingual lexica (Mikolov et al., 2013b), in which this word repre70 Proceedings of NAACL-HLT 2015, pages 70–78, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics sentations outperformed others with state-of-t"
W15-1510,J91-4003,0,0.560862,"resent a sense index that, for a given pair of lexical classes, defines the degree of membership of a noun to each class: polysemy is hence implicitly represented as an intermediate value on the continuum between two classes. We finally show that by exploiting the information provided by the sense index it is possible to accurately detect polysemous nouns in the dataset. 1 Introduction A major issue in lexical semantics is regular polysemy (also known as systematic or logical polysemy), the phenomenon whereby words belonging to a semantic class can predictably act as members of another class (Pustejovsky, 1991; Mart´ınez Alonso et al., 2013). For example, the word chicken can be considered a member of the class ANIMAL but also of FOOD, thus defining its senses in terms of lexical semantic classes. For some polysemous nouns one sense can be much more frequent than the other, thus causing asymmetry in sense predominance; this is the case of turkey, in which the food sense is clearly more frequent than the animal one (Copestake and Briscoe, 1995). Given its pervasiveness in natural language, regular polysemy has been extensively investigated in lexical semantics (Apresjan, 1974; Nunberg, 1992). Howeve"
W15-1510,W13-5404,1,0.905382,"Missing"
W15-1510,romeo-etal-2014-choosing,1,0.834277,", and therefore to accurately detect polysemes. The main contribution of the work is a novel method for the identification and representation of polysemous nouns, which accounts for the semantic of such nouns and explicitly represents it. 2 Motivation and Related Works In the field of NLP the information regarding lexical semantic classes has been proved to be crucial for several applications, such as information extraction, machine translation and question answering, and an increasing amount of research has been carried out in order to create models for the automatic classification of nouns (Romeo et al., 2014a; Bel et al., 2013; Schwartz et al., 2014). Despite the effects of regular polysemy on lexical classification, few work attempted to computationally model the phenomenon. The approaches in the literature for the representation of polysemous words are basically three. Polysemes can be simultaneously represented as members of several 71 classes (e.g. the polysemous word xy belongs to both the classes X and Y); as members of new, independent class which includes only words with the same hybrid distributional behaviour (xy belongs to the new class XY); on a continuum, thus assigning each word a p"
W15-1510,romeo-etal-2014-cascade,1,0.852651,", and therefore to accurately detect polysemes. The main contribution of the work is a novel method for the identification and representation of polysemous nouns, which accounts for the semantic of such nouns and explicitly represents it. 2 Motivation and Related Works In the field of NLP the information regarding lexical semantic classes has been proved to be crucial for several applications, such as information extraction, machine translation and question answering, and an increasing amount of research has been carried out in order to create models for the automatic classification of nouns (Romeo et al., 2014a; Bel et al., 2013; Schwartz et al., 2014). Despite the effects of regular polysemy on lexical classification, few work attempted to computationally model the phenomenon. The approaches in the literature for the representation of polysemous words are basically three. Polysemes can be simultaneously represented as members of several 71 classes (e.g. the polysemous word xy belongs to both the classes X and Y); as members of new, independent class which includes only words with the same hybrid distributional behaviour (xy belongs to the new class XY); on a continuum, thus assigning each word a p"
W15-1510,C14-1153,0,0.0538757,"Missing"
W15-1510,W11-0128,0,0.0726945,"Missing"
W15-3046,W07-0718,0,0.381788,"Missing"
W15-3046,W14-3348,0,0.725426,"m WMT13 and WMT14 Metrics tasks (Mach´acˇ ek and Bojar, 2013; Mach´acˇ ek and Bojar, 2014). To evaluate our metric’s performance at segment level, we use Kendall’s Tau correlation (τ ) with human rankings, as defined in (Mach´acˇ ek and Bojar, 2014). At system level, we use Pearson correlation coefficient (r). Table 1 presents the results averaged over all into-English translation directions. For the sake of comparison, we provide the results for the best performing metrics that participated in WMT13 and WMT14 Metrics tasks, as well as baseline metrics BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014). As shown in Table 1, our approach is competitive (UPF-Cobalt would have been ranked as the best performing metric on WMT13 data and as the second best on WMT14 data) and generalizes well Where (c) refers to the words that belong to the syntactic context of the reference word (r) (immediate neighbors in the dependency graph).6 If the context word is found in the set of aligned word pairs |A |and its counterpart in the candidate translation has the same or equivalent syntactic relation with the word (h), the weight w(ci ) equals to 0. Otherwise, the weight is defined according to the relative"
W15-3046,N13-1092,0,0.0696008,"Missing"
W15-3046,D14-1027,0,0.0473982,"Missing"
W15-3046,P14-2050,0,0.029081,"is employed to recognize semantically similar words.3 We enhance MWA with additional lexical similarity resources to maximize the coverage of the alignment. In addition to the paraphrase database, UPF-Cobalt employs WordNet synsets (Miller and Fellbaum, 2007) and distributional similarity (Turney and Pantel, 2010). WordNet is commonly used in MT evaluation and related fields for dealing with lexical variation. By contrast, to the best of our knowledge, distributional similarity has not yet been exploited for the evaluation task. We use publically available distributional similarity resource (Levy and Goldberg, 2014), which contains dependency-based word embeddings. To minimize the noise, we establish the following restrictions. To be considered candidates for alignment the words must have the cosine similarity higher than a threshold (based on data observation, we currently define it as 0.25). Also, they must have at least one pair of exact matching content words in their contexts. Contextual evidence is used to choose the best alignment candidates and is defined as the number of similar words in the contexts of the words to be aligned. At syntactic level, the context is constituted by the head and depen"
W15-3046,D08-1084,0,0.0158119,"e correlation (Meteor achieves 0.354 correlation on this dataset). Manual inspection of the results shows that this is primarily due to the fact that MWA does not support phrase-level alignments. This functionality is highly relevant for the evaluation task as it allows covering acceptable variation that involves multiword expressions. We plan to integrate phrasal alignments in the metric in the future. Distributional similarity. Removing this component implies a considerable decrease in the correlation. Qualitative analysis of the results shows 7 Stanford typed dependencies from Marneffe and Manning, (2008) are used for the description of syntactic relations. 376 nn Equivalent dep. types poss Ref: An Obama voter ’s cry of despair. Scores UPF-Cobalt Meteor prep of ≈ poss prep for ≈ nn 0.804 0.389 prep of ≈ poss appos 6= nn 0.646 0.393 prep of prep for Cand1: The cry of despair of a voter for Obama. appos prep of Cand2: The cry of despair of a voter Obama. Table 3: Example of candidate and reference translations with the corresponding Meteor and UPF-Cobalt scores scores to both candidate translations, due to the differences in word order and the presence of function words absent in the reference."
W15-3046,N06-1006,0,0.0780883,"Missing"
W15-3046,W14-3336,0,0.054627,"score for each pair of aligned word is defined as follows: • Distributional similarity: 0.5 CP (h, r) = 1 1 + e−CP (h,r) a(h, r) = LexSim(h, r) − P en(h, r) (3) Sentence-level score is then calculated as a weighted combination of precision and recall over the sum of the scores for aligned candidate and reference words. To obtain system-level scores, we computed the ratio of sentences in which each system was assigned the highest sentence-level score by our metric. P (1) 3 Experiments We conduct experiments with the data from WMT13 and WMT14 Metrics tasks (Mach´acˇ ek and Bojar, 2013; Mach´acˇ ek and Bojar, 2014). To evaluate our metric’s performance at segment level, we use Kendall’s Tau correlation (τ ) with human rankings, as defined in (Mach´acˇ ek and Bojar, 2014). At system level, we use Pearson correlation coefficient (r). Table 1 presents the results averaged over all into-English translation directions. For the sake of comparison, we provide the results for the best performing metrics that participated in WMT13 and WMT14 Metrics tasks, as well as baseline metrics BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014). As shown in Table 1, our approach is competitive (UPF-Cobalt w"
W15-3046,W13-2256,0,0.0440701,"Missing"
W15-3046,D13-1056,0,0.0229861,"Missing"
W15-3046,de-marneffe-etal-2006-generating,0,0.034264,"Missing"
W15-3046,P02-1040,0,0.0996957,"nduct experiments with the data from WMT13 and WMT14 Metrics tasks (Mach´acˇ ek and Bojar, 2013; Mach´acˇ ek and Bojar, 2014). To evaluate our metric’s performance at segment level, we use Kendall’s Tau correlation (τ ) with human rankings, as defined in (Mach´acˇ ek and Bojar, 2014). At system level, we use Pearson correlation coefficient (r). Table 1 presents the results averaged over all into-English translation directions. For the sake of comparison, we provide the results for the best performing metrics that participated in WMT13 and WMT14 Metrics tasks, as well as baseline metrics BLEU (Papineni et al., 2002) and Meteor (Denkowski and Lavie, 2014). As shown in Table 1, our approach is competitive (UPF-Cobalt would have been ranked as the best performing metric on WMT13 data and as the second best on WMT14 data) and generalizes well Where (c) refers to the words that belong to the syntactic context of the reference word (r) (immediate neighbors in the dependency graph).6 If the context word is found in the set of aligned word pairs |A |and its counterpart in the candidate translation has the same or equivalent syntactic relation with the word (h), the weight w(ci ) equals to 0. Otherwise, the weigh"
W15-3046,Q14-1018,0,0.479277,"s show that UPF-Cobalt achieves competitive results, both at segment and at system levels. On WMT14 data, our metric would have been ranked as second-best performing metric at segment level, and tied with the first best-performing metric at system level. The rest of this paper is organized as follows. Section 2 describes UPF-Cobalt. In Section 3 we present the experiments and analyze the results. Section 4 examines relevant pieces of related work. Finally, in Section 5 we give the conclusions and suggest directions for future work. 2 shown to significantly improve on state-of-the-art results (Sultan et al., 2014). MWA exploits lexical similarity and contextual evidence to make alignment decisions. Lexical similarity component identifies possible candidates for alignment. In addition to exact and lemma match, Paraphrase Database (Ganitkevitch et al., 2013) of lexical and phrasal paraphrases is employed to recognize semantically similar words.3 We enhance MWA with additional lexical similarity resources to maximize the coverage of the alignment. In addition to the paraphrase database, UPF-Cobalt employs WordNet synsets (Miller and Fellbaum, 2007) and distributional similarity (Turney and Pantel, 2010)."
W15-3046,C12-2120,0,0.090075,"ust be taken into consideration. Therefore, we follow a two-stage approach to evaluation. First, MT is aligned to the reference. Next, the candidate translation is scored taking into account both the number of aligned words and their roles in the corresponding sentences. 2.1 Monolingual Word Aligner We assume that using better candidate-reference alignment results in better MT evaluation. Research in the area of monolingual alignment demonstrates that exploiting syntactic context to discriminate between candidate pairs for alignment significantly improves the results (MacCartney et al., 2008; Thadani et al., 2012; Yao et al, 2013; Sultan et al., 2014). The alignment module of UPF-Cobalt builds on an existing system Monolingual Word Aligner (MWA)2 which takes context information into account and has been 3 MWA does not support phrase-level alignments, but the framework is flexible enough to integrate them in the future. 4 The dependencies are extracted with Stanford dependency parser (de Marneffe et al., 2006). 2 https://github.com/ma-sultan/ monolingual-word-aligner. 374 scoring. 2.2 extent than dropping a determiner or an adjunct. We define three groups of syntactic functions accordingly and establis"
W15-3046,W13-2202,0,\N,Missing
W16-2339,W15-3046,1,0.916508,"urpose and intended use of the MT, manual evaluation can be performed in a number of different ways. However, in any setting both adequacy and fluency shape human perception of the overall translation quality. By contrast, automatic reference-based metrics are largely focused on MT adequacy, as they do not evaluate the appropriateness of the translation in the context of the target language. Translation fluency is thus assessed only indirectly, through the comparison with the reference. However, the difference from a particular human translation does not imply that the MT output is disfluent (Fomicheva et al., 2015a). We propose to explicitly model translation fluency in reference-based MT evaluation. To this end, we develop a number of features representing translation fluency and integrate them with our reference-based metric UPF-Cobalt, which was originally presented at WMT15 (Fomicheva et al., 2015b). Along with the features based on the target Language Model (LM) probability of the MT output, which have been widely used in the related fields of speech recognition (Uhrik and Ward, 1997) and quality estimation (Specia et al., 2009), we design a more detailed representation of MT fluency that takes in"
W16-2339,P11-1022,0,0.0153542,"is task, referred to as confidence or quality estimation, is aimed at MT systems in use and therefore has no access to reference translations (Specia et al., 2010). Quality estimation can be performed at different levels of granularity. Sentence-level quality estimation (Specia et al., 2009; Blatz et al., 2004) is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of MT sentences annotated with quality labels. In the word-level variant of this task, each word in the MT output is to be judged as correct or incorrect (Luong et al., 2015; Bach et al., 2011), or labelled for a specific error type. Research in the field of quality estimation is focused on the design of features and the selection of appropriate learning schemes to predict translation quality, using source sentences, MT outputs, internal MT system information and source and target language corpora. In particular, features that measure the probability of the MT output with respect to a target LM, thus capturing translation fluency, have demonstrated highly competitive performance in a variety of settings (Shah et 3 UPF-Cobalt Review UPF-Cobalt1 is an alignment-based evaluation metric"
W16-2339,C04-1046,0,0.103666,"zm´an et al., 2014; Yu et al., 2015). However, none of the above approaches explicitly addresses the fluency of the MT output. Predicting MT quality with respect to the target language norms has been investigated in a different evaluation scenario, when human translations are not available as benchmark. This task, referred to as confidence or quality estimation, is aimed at MT systems in use and therefore has no access to reference translations (Specia et al., 2010). Quality estimation can be performed at different levels of granularity. Sentence-level quality estimation (Specia et al., 2009; Blatz et al., 2004) is addressed as a supervised machine learning task using a variety of algorithms to induce models from examples of MT sentences annotated with quality labels. In the word-level variant of this task, each word in the MT output is to be judged as correct or incorrect (Luong et al., 2015; Bach et al., 2011), or labelled for a specific error type. Research in the field of quality estimation is focused on the design of features and the selection of appropriate learning schemes to predict translation quality, using source sentences, MT outputs, internal MT system information and source and target l"
W16-2339,P14-1065,0,0.0258889,"Missing"
W16-2339,W05-0904,0,0.0601197,"luency information into reference-based evaluation. able from WMT15 Metrics Task and obtain very promising results, which rival the best-performing system submissions. We have also submitted the metric to the WMT16 Metrics Task. 2 Related Work The recent advances in the field of MT evaluation have been largely directed to improving the informativeness and accuracy of candidate-reference comparison. Meteor (Denkowski and Lavie, 2014) allows for stem, synonym and paraphrase matches, thus addressing the problem of acceptable linguistic variation at lexical level. Other metrics measure syntactic (Liu and Gildea, 2005), semantic (Lo et al., 2012) or even discourse similarity (Guzm´an et al., 2014) between candidate and reference translations. Further improvements have been recently achieved by combining these partial measurements using different strategies including machine learning techniques (Comelles et al., 2012; Gim´enez and M`arquez, 2010b; Guzm´an et al., 2014; Yu et al., 2015). However, none of the above approaches explicitly addresses the fluency of the MT output. Predicting MT quality with respect to the target language norms has been investigated in a different evaluation scenario, when human tra"
W16-2339,W12-3129,0,0.0342631,"-based evaluation. able from WMT15 Metrics Task and obtain very promising results, which rival the best-performing system submissions. We have also submitted the metric to the WMT16 Metrics Task. 2 Related Work The recent advances in the field of MT evaluation have been largely directed to improving the informativeness and accuracy of candidate-reference comparison. Meteor (Denkowski and Lavie, 2014) allows for stem, synonym and paraphrase matches, thus addressing the problem of acceptable linguistic variation at lexical level. Other metrics measure syntactic (Liu and Gildea, 2005), semantic (Lo et al., 2012) or even discourse similarity (Guzm´an et al., 2014) between candidate and reference translations. Further improvements have been recently achieved by combining these partial measurements using different strategies including machine learning techniques (Comelles et al., 2012; Gim´enez and M`arquez, 2010b; Guzm´an et al., 2014; Yu et al., 2015). However, none of the above approaches explicitly addresses the fluency of the MT output. Predicting MT quality with respect to the target language norms has been investigated in a different evaluation scenario, when human translations are not available"
W16-2339,E06-1032,0,0.139235,"Missing"
W16-2339,comelles-etal-2012-verta,0,0.0248089,"Missing"
W16-2339,W14-3336,0,0.0582917,"ly not indicative of an error and should be penalized less. Based on this observation, we introduce a separate set of features that compute the word-level measurements discussed above only for the words that are not aligned to the reference translation. This results in 49 additional features, grouped here for space reasons: In our work we focus on sentence-level metrics’ performance, which is assessed by converting metrics’ scores to ranks and comparing them to the human judgements with Kendall rank correlation coefficient (τ ). We use the WMT14 official Kendall’s Tau implementation (Mach´acˇ ek and Bojar, 2014). Following the standard practice at WMT and to make our work comparable to the official metrics submitted to the task, we exclude ties in human judgments both for training and for testing our system. Our model is a simple linear interpolation of the features presented in the previous sections. For tuning the weights, we use the learn-to-rank approach (Burges et al., 2005), which has been successfully applied in similar settings in previous work (Guzm´an et al., 2014; Stanojevic and Sima’an, 2015). We use a standard implementation of Logistic Regression algorithm from the Python toolkit scikit"
W16-2339,W14-3348,0,0.318909,"nificant improvement over the reference-based evaluation systems on the task of predicting human postediting effort. We follow this line of research by focusing specifically on integrating fluency information into reference-based evaluation. able from WMT15 Metrics Task and obtain very promising results, which rival the best-performing system submissions. We have also submitted the metric to the WMT16 Metrics Task. 2 Related Work The recent advances in the field of MT evaluation have been largely directed to improving the informativeness and accuracy of candidate-reference comparison. Meteor (Denkowski and Lavie, 2014) allows for stem, synonym and paraphrase matches, thus addressing the problem of acceptable linguistic variation at lexical level. Other metrics measure syntactic (Liu and Gildea, 2005), semantic (Lo et al., 2012) or even discourse similarity (Guzm´an et al., 2014) between candidate and reference translations. Further improvements have been recently achieved by combining these partial measurements using different strategies including machine learning techniques (Comelles et al., 2012; Gim´enez and M`arquez, 2010b; Guzm´an et al., 2014; Yu et al., 2015). However, none of the above approaches ex"
W16-2339,W15-3048,0,0.0157603,"tinguish between the quality of two alternative candidate translations. For example, it may well be the case that both MT outputs are very different from human reference, but one constitutes a valid alternative translation, while the other is totally unacceptable. Finally, Groups III and VI contain the results of the best-performing evaluation systems from the WMT15 Metrics Task, as well as the baseline BLEU metric (Papineni et al., 2002) and a strong competitor, Meteor (Denkowski and Lavie, 2014), which we reproduce here for the sake of comparison. DPMFComb (Yu et al., 2015) and RATATOUILLE (Marie and Apidianaki, 2015) use a learnt combination of the scores from different evaluation metrics, while BEER Treepel (Stanojevic and Sima’an, 2015) combines word matching, word order and syntax-level features. We note that the number and complexity of the metrics used in the above approaches is quite high. For instance, DPMFComb is based on 72 separate evaluation systems, including the resource-heavy linguistic Acknowledgments This work was partially funded by TUNER (TIN2015-65308-C5-5-R) and MINECO/FEDER, UE. Marina Fomicheva was supported by funding from the FI-DGR grant program of the Generalitat de Catalunya. Ir"
W16-2339,W12-3110,1,0.839561,"ge use. Conversely, due to the variability of linguistic expression, neither lexical nor syntactic differences from a particular human translation imply ill-formedness of the MT output. Sentence fluency can be described in terms of the frequencies of the words with respect to a target LM. Here, in addition to the LM-based features that have been shown to perform well for sentence-level quality estimation (Shah et al., 2013), we introduce more complex features derived from word-level n-gram statistics. Besides the word-based representation, we rely on Part-ofSpeech (PoS) tags. As suggested by (Felice and Specia, 2012), morphosyntactic information can be a good indicator of ill-formedness in MT outputs. First, we select 16 simple sentence-level features from previous work (Felice and Specia, 2012; Specia et al., 2010), summarized below.  7,     6,           5,    4, b(wi ) =      3,          2,    1, if wi−2 , wi−1 , wi exists in the model if wi−2 , wi−1 and wi−1 , wi both exist in the model if only wi−1 , wi exists in the model if only wi−2 , wi−1 and wi exist separately in the model if wi−1 and wi both exist in the model if only wi exists in the model if wi is an out-o"
W16-2339,P02-1040,0,0.106436,"pensive, and objective numerical measurements of translation quality. As a cost-effective alternative to manual evaluation, the main concern of automatic evaluation metrics is to accurately approximate human judgments. The vast majority of evaluation metrics are based on the idea that the closer the MT output is to a human reference translation, the higher its quality. The evaluation task, therefore, is typically approached by measuring some kind of similarity between the MT (also called candidate translation) and a reference translation. The most widely used evaluation metrics, such as BLEU (Papineni et al., 2002), follow a simple strategy of counting the number of matching words or word sequences in the candidate and reference 483 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 483–490, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics al., 2013). Both translation evaluation and quality estimation aim to evaluate MT quality. Surprisingly, there have been very few attempts at joining the insights from these two related tasks. A notable exception is the work by Specia and Gim´enez (2010), who explore the combination of a"
W16-2339,L16-1437,1,0.831323,"The most important feature of the metric is a syntactically informed context penalty aimed at penalizing the matches of similar words that play different roles in the candidate and reference sentences. The metric has achieved highly competitive results on the data from previous WMT tasks, showing that the context penalty allows to better discriminate between acceptable candidatereference differences and the differences incurred by MT errors (Fomicheva et al., 2015b). Below we briefly review the main components of the metric. For a detailed description of the metric the reader is referred to (Fomicheva and Bel, 2016). 3.1 Alignment The alignment module of UPF-Cobalt builds on an existing system – Monolingual Word Aligner (MWA), which has been shown to significantly outperform state-of-the-art results for monolingual alignment (Sultan et al., 2014). We increase the coverage of the aligner by comparing distributed word representations as an additional source of lexical similarity information, 1 The metric is freely available for download at https://github.com/amalinovskiy/ upf-cobalt. 484 which allows to detect cases of quasi-synonyms (Fomicheva and Bel, 2016). 3.2 pair. The sentence-level average can be ob"
W16-2339,2013.mtsummit-papers.21,1,0.747085,"cular word or expression may be similar in meaning to the one present in the reference (adequacy), but awkward or even erroneous if considered in the context of the norms of the target language use. Conversely, due to the variability of linguistic expression, neither lexical nor syntactic differences from a particular human translation imply ill-formedness of the MT output. Sentence fluency can be described in terms of the frequencies of the words with respect to a target LM. Here, in addition to the LM-based features that have been shown to perform well for sentence-level quality estimation (Shah et al., 2013), we introduce more complex features derived from word-level n-gram statistics. Besides the word-based representation, we rely on Part-ofSpeech (PoS) tags. As suggested by (Felice and Specia, 2012), morphosyntactic information can be a good indicator of ill-formedness in MT outputs. First, we select 16 simple sentence-level features from previous work (Felice and Specia, 2012; Specia et al., 2010), summarized below.  7,     6,           5,    4, b(wi ) =      3,          2,    1, if wi−2 , wi−1 , wi exists in the model if wi−2 , wi−1 and wi−1 , wi both exis"
W16-2339,2010.amta-papers.3,1,0.729752,"Missing"
W16-2339,2009.eamt-1.5,1,0.839016,"r human translation does not imply that the MT output is disfluent (Fomicheva et al., 2015a). We propose to explicitly model translation fluency in reference-based MT evaluation. To this end, we develop a number of features representing translation fluency and integrate them with our reference-based metric UPF-Cobalt, which was originally presented at WMT15 (Fomicheva et al., 2015b). Along with the features based on the target Language Model (LM) probability of the MT output, which have been widely used in the related fields of speech recognition (Uhrik and Ward, 1997) and quality estimation (Specia et al., 2009), we design a more detailed representation of MT fluency that takes into account the number of disfluent segments observed in the candidate translation. We test our approach with the data availThe vast majority of Machine Translation (MT) evaluation approaches are based on the idea that the closer the MT output is to a human reference translation, the higher its quality. While translation quality has two important aspects, adequacy and fluency, the existing referencebased metrics are largely focused on the former. In this work we combine our metric UPF-Cobalt, originally presented at the WMT15"
W16-2339,P15-4020,1,0.82252,"rpolation of the features presented in the previous sections. For tuning the weights, we use the learn-to-rank approach (Burges et al., 2005), which has been successfully applied in similar settings in previous work (Guzm´an et al., 2014; Stanojevic and Sima’an, 2015). We use a standard implementation of Logistic Regression algorithm from the Python toolkit scikit-learn5 . The model is trained on WMT14 dataset and tested on WMT15 dataset. For the extraction of word-level backoff behaviour values and sentence-level fluency features, we use Quest++6 , an open source tool for quality estimation (Specia et al., 2015). We employ the LM used to build the baseline system for WMT15 Quality Estimation Task (Bojar et al., 2015).7 This LM provided was trained on data from the WMT12 translation task (a combination of news and Europarl data) and thus matches the domain of the dataset we use in our experiments. PoS tagging was performed with TreeTagger (Schmid, 1999). • Summary statistics of the LM backoff behaviour (word and PoS-tag LM) • Summary statistics of the LM backoff behaviour for non-aligned words only (word and PoS tag LM) • Percentage and number of words with low backoff behaviour value (word and PoS ta"
W16-2339,W15-3050,0,0.216238,"Missing"
W16-2339,Q14-1018,0,0.0166703,"titive results on the data from previous WMT tasks, showing that the context penalty allows to better discriminate between acceptable candidatereference differences and the differences incurred by MT errors (Fomicheva et al., 2015b). Below we briefly review the main components of the metric. For a detailed description of the metric the reader is referred to (Fomicheva and Bel, 2016). 3.1 Alignment The alignment module of UPF-Cobalt builds on an existing system – Monolingual Word Aligner (MWA), which has been shown to significantly outperform state-of-the-art results for monolingual alignment (Sultan et al., 2014). We increase the coverage of the aligner by comparing distributed word representations as an additional source of lexical similarity information, 1 The metric is freely available for download at https://github.com/amalinovskiy/ upf-cobalt. 484 which allows to detect cases of quasi-synonyms (Fomicheva and Bel, 2016). 3.2 pair. The sentence-level average can be obtained in a straightforward way from the word-level values (we use it as a feature in the decomposed version of the metric below). Scoring UPF-Cobalt’s sentence-level score is a weighted combination of precision and recall over the sum"
W16-2339,W15-3053,0,0.32949,"e-reference comparison. Meteor (Denkowski and Lavie, 2014) allows for stem, synonym and paraphrase matches, thus addressing the problem of acceptable linguistic variation at lexical level. Other metrics measure syntactic (Liu and Gildea, 2005), semantic (Lo et al., 2012) or even discourse similarity (Guzm´an et al., 2014) between candidate and reference translations. Further improvements have been recently achieved by combining these partial measurements using different strategies including machine learning techniques (Comelles et al., 2012; Gim´enez and M`arquez, 2010b; Guzm´an et al., 2014; Yu et al., 2015). However, none of the above approaches explicitly addresses the fluency of the MT output. Predicting MT quality with respect to the target language norms has been investigated in a different evaluation scenario, when human translations are not available as benchmark. This task, referred to as confidence or quality estimation, is aimed at MT systems in use and therefore has no access to reference translations (Specia et al., 2010). Quality estimation can be performed at different levels of granularity. Sentence-level quality estimation (Specia et al., 2009; Blatz et al., 2004) is addressed as"
W17-1807,W16-2921,1,0.591679,"Missing"
W17-1807,W08-0606,0,0.0860993,"on markers and their scope, and the corresponding annotation guidelines.1 To the best of our knowledge, this is the first corpus of medical Spanish texts manually annotated for negation, although two test-sets of about 500 and 1000 sentences for evaluating particular negation detection systems already exist, as described later in the Related Work section. Because no standard negation annotation schema still exists, our annotation schema has taken into account the currently existing English corpora annotated for negation, trying to be comprehensive with current practices (Mutalik et al., 2001; Szarvas et al., 2008; Morante and Daelemans, 2012). After this introductory section, in Section 2 we briefly describe negation structures in Spanish, in Section 3 we describe the corpus design, in Section 4 we present the guidelines we have followed to identify and classify negation information and in Section 5 we provide details of tags and statistics of the resulting annotated corpus, then, in Section 6 we review existing related corpora on which we have designed our annotation schema and, finally, in Section 7 we conclude. This paper presents the IULA Spanish Clinical Record Corpus, a corpus of 3,194 sentences"
