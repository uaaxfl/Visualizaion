2020.readi-1.5,Text Simplification to Help Individuals with Low Vision Read More Fluently,2020,-1,-1,4,0,15661,lauren sauvan,Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI),0,"The objective of this work is to introduce text simplification as a potential reading aid to help improve the poor reading performance experienced by visually impaired individuals. As a first step, we explore what makes a text especially complex when read with low vision, by assessing the individual effect of three word properties (frequency, orthographic similarity and length) on reading speed in the presence of Central visual Field Loss (CFL). Individuals with bilateral CFL induced by macular diseases read pairs of French sentences displayed with the self-paced reading method. For each sentence pair, sentence n contained a target word matched with a synonym word of the same length included in sentence n+1. Reading time was recorded for each target word. Given the corpus we used, our results show that (1) word frequency has a significant effect on reading time (the more frequent the faster the reading speed) with larger amplitude (in the range of seconds) compared to normal vision; (2) word neighborhood size has a significant effect on reading time (the more neighbors the slower the reading speed), this effect being rather small in amplitude, but interestingly reversed compared to normal vision; (3) word length has no significant effect on reading time. Supporting the development of new and more effective assistive technology to help low vision is an important and timely issue, with massive potential implications for social and rehabilitation practices. The end goal of this project will be to use our findings to custom text simplification to this specific population and use it as an optimal and efficient reading aid."
2020.readi-1.13,Combining Expert Knowledge with Frequency Information to Infer {CEFR} Levels for Words,2020,-1,-1,2,0,15681,alice pintard,Proceedings of the 1st Workshop on Tools and Resources to Empower People with REAding DIfficulties (READI),0,"Traditional approaches to set goals in second language (L2) vocabulary acquisition relied either on word lists that were obtained from large L1 corpora or on collective knowledge and experience of L2 experts, teachers, and examiners. Both approaches are known to offer some advantages, but also to have some limitations. In this paper, we try to combine both sources of information, namely the official reference level description for French language and the FLElex lexical database. Our aim is to train a statistical model on the French RLD that would be able to turn the distributional information from FLElex into one of the six levels of the Common European Framework of Reference for languages (CEFR). We show that such approach yields a gain of 29{\%} in accuracy compared to the method currently used in the CEFRLex project. Besides, our experiments also offer deeper insights into the advantages and shortcomings of the two traditional sources of information (frequency vs. expert knowledge)."
2020.lrec-1.169,{A}lector: A Parallel Corpus of Simplified {F}rench Texts with Alignments of Misreadings by Poor and Dyslexic Readers,2020,0,1,4,0,15665,nuria gala,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we present a new parallel corpus addressed to researchers, teachers, and speech therapists interested in text simplification as a means of alleviating difficulties in children learning to read. The corpus is composed of excerpts drawn from 79 authentic literary (tales, stories) and scientific (documentary) texts commonly used in French schools for children aged between 7 to 9 years old. The excerpts were manually simplified at the lexical, morpho-syntactic, and discourse levels in order to propose a parallel corpus for reading tests and for the development of automatic text simplification tools. A sample of 21 poor-reading and dyslexic children with an average reading delay of 2.5 years read a portion of the corpus. The transcripts of readings errors were integrated into the corpus with the goal of identifying lexical difficulty in the target population. By means of statistical testing, we provide evidence that the manual simplifications significantly reduced reading errors, highlighting that the words targeted for simplification were not only well-chosen but also substituted with substantially easier alternatives. The entire corpus is available for consultation through a web interface and available on demand for research purposes."
2020.aacl-demo.1,{AM}esure: A Web Platform to Assist the Clear Writing of Administrative Texts,2020,-1,-1,1,1,15664,thomas franccois,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations,0,"This article presents the AMesure platform, which aims to assist writers of French administrative texts in simplifying their writing. This platform includes a readability formula specialized for administrative texts and it also uses various natural language processing (NLP) tools to analyze texts and highlight a number of linguistic phenomena considered difficult to read. Finally, based on the difficulties identified, it offers pieces of advice coming from official plain language guides to users. This paper describes the different components of the system and reports an evaluation of these components."
W18-7001,The Interface Between Readability and Automatic Text Simplification,2018,0,0,1,1,15664,thomas franccois,Proceedings of the 1st Workshop on Automatic Text Adaptation ({ATA}),0,None
W18-7004,Assisted Lexical Simplification for {F}rench Native Children with Reading Difficulties,2018,-1,-1,3,0,27613,firas hmida,Proceedings of the 1st Workshop on Automatic Text Adaptation ({ATA}),0,None
W18-0514,{NT}2{L}ex: A {CEFR}-Graded Lexical Resource for {D}utch as a Foreign Language Linked to Open {D}utch {W}ord{N}et,2018,0,1,2,1,16943,anais tack,Proceedings of the Thirteenth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"In this paper, we introduce NT2Lex, a novel lexical resource for Dutch as a foreign language (NT2) which includes frequency distributions of 17,743 words and expressions attested in expert-written textbook texts and readers graded along the scale of the Common European Framework of Reference (CEFR). In essence, the lexicon informs us about what kind of vocabulary should be understood when reading Dutch as a non-native reader at a particular proficiency level. The main novelty of the resource with respect to the previously developed CEFR-graded lexicons concerns the introduction of corpus-based evidence for L2 word sense complexity through the linkage to Open Dutch WordNet (Postma et al., 2016). The resource thus contains, on top of the lemmatised and part-of-speech tagged lexical entries, a total of 11,999 unique word senses and 8,934 distinct synsets."
L18-1140,{EFLL}ex: A Graded Lexical Resource for Learners of {E}nglish as a Foreign Language,2018,0,0,2,0,28844,luise durlich,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
C18-1218,{R}e{S}yf: a {F}rench lexicon with ranked synonyms,2018,0,0,2,0,5620,mokhtar billami,Proceedings of the 27th International Conference on Computational Linguistics,0,"In this article, we present ReSyf, a lexical resource of monolingual synonyms ranked according to their difficulty to be read and understood by native learners of French. The synonyms come from an existing lexical network and they have been semantically disambiguated and refined. A ranking algorithm, based on a wide range of linguistic features and validated through an evaluation campaign with human annotators, automatically sorts the synonyms corresponding to a given word sense by reading difficulty. ReSyf is freely available and will be integrated into a web platform for reading assistance. It can also be applied to perform lexical simplification of French texts."
W17-5018,Human and Automated {CEFR}-based Grading of Short Answers,2017,0,1,2,1,16943,anais tack,Proceedings of the 12th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper is concerned with the task of automatically assessing the written proficiency level of non-native (L2) learners of English. Drawing on previous research on automated L2 writing assessment following the Common European Framework of Reference for Languages (CEFR), we investigate the possibilities and difficulties of deriving the CEFR level from short answers to open-ended questions, which has not yet been subjected to numerous studies up to date. The object of our study is twofold: to examine the intricacy involved with both human and automated CEFR-based grading of short answers. On the one hand, we describe the compilation of a learner corpus of short answers graded with CEFR levels by three certified Cambridge examiners. We mainly observe that, although the shortness of the answers is reported as undermining a clear-cut evaluation, the length of the answer does not necessarily correlate with inter-examiner disagreement. On the other hand, we explore the development of a soft-voting system for the automated CEFR-based grading of short answers and draw tentative conclusions about its use in a computer-assisted testing (CAT) setting."
W16-6510,{S}we{LL}ex: Second language learners{'} productive vocabulary,2016,-1,-1,5,0,2653,elena volodina,Proceedings of the joint workshop on {NLP} for Computer Assisted Language Learning and {NLP} for Language Acquisition,0,None
L16-1032,{SVAL}ex: a {CEFR}-graded Lexical Resource for {S}wedish Foreign and Second Language Learners,2016,0,3,1,1,15664,thomas franccois,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The paper introduces SVALex, a lexical resource primarily aimed at learners and teachers of Swedish as a foreign and second language that describes the distribution of 15,681 words and expressions across the Common European Framework of Reference (CEFR). The resource is based on a corpus of coursebook texts, and thus describes receptive vocabulary learners are exposed to during reading activities, as opposed to productive vocabulary they use when speaking or writing. The paper describes the methodology applied to create the list and to estimate the frequency distribution. It also discusses some characteristics of the resulting resource and compares it to other lexical resources for Swedish. An interesting feature of this resource is the possibility to separate the wheat from the chaff, identifying the core vocabulary at each level, i.e. vocabulary shared by several coursebook writers at each level, from peripheral vocabulary which is used by the minority of the coursebook writers."
L16-1035,Evaluating Lexical Simplification and Vocabulary Knowledge for Learners of {F}rench: Possibilities of Using the {FLEL}ex Resource,2016,0,1,2,1,16943,anais tack,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This study examines two possibilities of using the FLELex graded lexicon for the automated assessment of text complexity in French as a foreign language learning. From the lexical frequency distributions described in FLELex, we derive a single level of difficulty for each word in a parallel corpus of original and simplified texts. We then use this data to automatically address the lexical complexity of texts in two ways. On the one hand, we evaluate the degree of lexical simplification in manually simplified texts with respect to their original version. Our results show a significant simplification effect, both in the case of French narratives simplified for non-native readers and in the case of simplified Wikipedia texts. On the other hand, we define a predictive model which identifies the number of words in a text that are expected to be known at a particular learning level. We assess the accuracy with which these predictions are able to capture actual word knowledge as reported by Dutch-speaking learners of French. Our study shows that although the predictions seem relatively accurate in general (87.4{\%} to 92.3{\%}), they do not yet seem to cover the learners{'} lack of knowledge very well."
L16-1613,Combining Manual and Automatic Prosodic Annotation for Expressive Speech Synthesis,2016,28,0,2,0,35330,sandrine brognaux,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Text-to-speech has long been centered on the production of an intelligible message of good quality. More recently, interest has shifted to the generation of more natural and expressive speech. A major issue of existing approaches is that they usually rely on a manual annotation in expressive styles, which tends to be rather subjective. A typical related issue is that the annotation is strongly influenced â and possibly biased â by the semantic content of the text (e.g. a shot or a fault may incite the annotator to tag that sequence as expressing a high degree of excitation, independently of its acoustic realization). This paper investigates the assumption that human annotation of basketball commentaries in excitation levels can be automatically improved on the basis of acoustic features. It presents two techniques for label correction exploiting a Gaussian mixture and a proportional-odds logistic regression. The automatically re-annotated corpus is then used to train HMM-based expressive speech synthesizers, the performance of which is assessed through subjective evaluations. The results indicate that the automatic correction of the annotation with Gaussian mixture helps to synthesize more contrasted excitation levels, while preserving naturalness."
C16-1094,Are Cohesive Features Relevant for Text Readability Evaluation?,2016,27,5,2,0,15684,amalia todirascu,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"This paper investigates the effectiveness of 65 cohesion-based variables that are commonly used in the literature as predictive features to assess text readability. We evaluate the efficiency of these variables across narrative and informative texts intended for an audience of L2 French learners. In our experiments, we use a French corpus that has been both manually and automatically annotated as regards to co-reference and anaphoric chains. The efficiency of the 65 variables for readability is analyzed through a correlational analysis and some modelling experiments."
2016.jeptalnrecital-long.17,Mod{\\`e}les adaptatifs pour pr{\\'e}dire automatiquement la comp{\\'e}tence lexicale d{'}un apprenant de fran{\\c{c}}ais langue {\\'e}trang{\\`e}re (Adaptive models for automatically predicting the lexical competence of {F}rench as a foreign language learners),2016,-1,-1,2,1,16943,anais tack,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Articles longs),0,"Cette {\'e}tude examine l{'}utilisation de m{\'e}thodes d{'}apprentissage incr{\'e}mental supervis{\'e} afin de pr{\'e}dire la comp{\'e}tence lexicale d{'}apprenants de fran{\c{c}}ais langue {\'e}trang{\`e}re (FLE). Les apprenants cibl{\'e}s sont des n{\'e}erlandophones ayant un niveau A2/B1 selon le Cadre europ{\'e}en commun de r{\'e}f{\'e}rence pour les langues (CECR). {\`A} l{'}instar des travaux r{\'e}cents portant sur la pr{\'e}diction de la ma{\^\i}trise lexicale {\`a} l{'}aide d{'}indices de complexit{\'e}, nous {\'e}laborons deux types de mod{\`e}les qui s{'}adaptent en fonction d{'}un retour d{'}exp{\'e}rience, r{\'e}v{\'e}lant les connaissances de l{'}apprenant. En particulier, nous d{\'e}finissons (i) un mod{\`e}le qui pr{\'e}dit la comp{\'e}tence lexicale de tous les apprenants du m{\^e}me niveau de ma{\^\i}trise et (ii) un mod{\`e}le qui pr{\'e}dit la comp{\'e}tence lexicale d{'}un apprenant individuel. Les mod{\`e}les obtenus sont ensuite {\'e}valu{\'e}s par rapport {\`a} un mod{\`e}le de r{\'e}f{\'e}rence d{\'e}terminant la comp{\'e}tence lexicale {\`a} partir d{'}un lexique sp{\'e}cialis{\'e} pour le FLE et s{'}av{\`e}rent gagner significativement en exactitude (9{\%}-17{\%})."
W14-3502,An analysis of a {F}rench as a Foreign Language Corpus for Readability Assessment,2014,64,4,1,1,15664,thomas franccois,Proceedings of the third workshop on {NLP} for computer-assisted language learning,0,"Readability aims to assess the difficulty of texts based on various linguistic predictors (the lexicon used, the complexity of sentences, the coherence of the text, etc.). It is an active field that has applications in a large number of NLP domains, among which machine translation, text simplification, text summarisation, or CALL (Computer-Assisted Language Learning). For CALL, readability tools could be used to help the retrieval of educational materials or to make CALL platforms more adaptive. However, developing a readability formula is a costly process that requires a large amount of texts annotated in terms of difficulty. The current mainstream method to gather such a large corpus of annotated texts is to get them from educational resources such as textbooks or simplified readers. In this paper, we describe the collection process of an annotated corpus of French as a foreign language texts with the purpose of training a readability model. We follow the mainstream approach, getting the texts from textbooks, but we are concerned with the limitations of such xe2x80x9cannotationxe2x80x9d approach, in particular, as regards the homogeneity of the difficulty annotations across textbook series. Their reliability is assessed using both a qualitative and a quantitative analysis. It appears that, for some educational levels, the hypothesis of the annotation homogeneity must be rejected. Various reasons for such findings are discussed and the paper concludes with recommandations for future similar attempts"
W14-1206,Syntactic Sentence Simplification for {F}rench,2014,30,10,4,1,38767,laetitia brouwers,Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations ({PITR}),0,This paper presents a method for the syntactic simplification of French texts. Syntactic simplification aims at making texts easier to understand by simplifying complex syntactic structures that hinder reading. Our approach is based on the study of two parallel corpora (encyclopaedia articles and tales). It aims to identify the linguistic phenomena involved in the manual simplification of French texts and organise them within a typology. We then propose a syntactic simplification system that relies on this typology to generate simplified sentences. The module starts by generating all possible variants before selecting the best subset. The evaluation shows that about 80% of the simplified sentences produced by our system are accurate.
francois-etal-2014-flelex,{FLEL}ex: a graded lexical resource for {F}rench foreign learners,2014,31,5,1,1,15664,thomas franccois,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper we present FLELex, the first graded lexicon for French as a foreign language (FFL) that reports word frequencies by difficulty level (according to the CEFR scale). It has been obtained from a tagged corpus of 777,000 words from available textbooks and simplified readers intended for FFL learners. Our goal is to freely provide this resource to the community to be used for a variety of purposes going from the assessment of the lexical difficulty of a text, to the selection of simpler words within text simplification systems, and also as a dictionary in assistive tools for writing."
pho-etal-2014-multiple,Multiple Choice Question Corpus Analysis for Distractor Characterization,2014,13,6,6,0,37952,vanminh pho,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we present a study of MCQ aiming to define criteria in order to automatically select distractors. We are aiming to show that distractor editing follows rules like syntactic and semantic homogeneity according to associated answer, and the possibility to automatically identify this homogeneity. Manual analysis shows that homogeneity rule is respected to edit distractors and automatic analysis shows the possibility to reproduce these criteria. These ones can be used in future works to automatically select distractors, with the combination of other criteria."
F14-2014,{AM}esure: a readability formula for administrative texts ({AMESURE}: une plateforme de lisibilit{\\'e} pour les textes administratifs) [in {F}rench],2014,-1,-1,1,1,15664,thomas franccois,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
F14-1009,A model to predict lexical complexity and to grade words (Un mod{\\`e}le pour pr{\\'e}dire la complexit{\\'e} lexicale et graduer les mots) [in {F}rench],2014,-1,-1,2,0,15665,nuria gala,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
R13-1013,Automatic extraction of contextual valence shifters.,2013,17,6,2,0,41300,noemi boubel,Proceedings of the International Conference Recent Advances in Natural Language Processing {RANLP} 2013,0,"In opinion mining, many linguistic structures, called contextual valence shifters, may modify the prior polarity of items. Some systems of sentiment analysis have tried to take these shifters into account, but few studies have focused on the identification of all these structures and their impact on polarized words. In this paper, we describe a method that automatically identifies contextual valence shifters. It relies on a chi-square test applied to the contingency table representing the distribution of a candidate shifter in a corpus of reviews of diverse opinions. The approach depends on two resources - a corpus of reviews and a lexicon of valence terms - to build a list of contextual valence shifters. We also introduce a set of rules used to classify the extracted contextual valence shifters according to their impact on polarized words. They make use of the Pearson residuals in contingency tables to filter candidate shifters and classify them. We show that the technique reaches an F-measure of either 0.56 or 0.66, depending on how the categories of shifters are defined."
W12-2207,Do {NLP} and machine learning improve traditional readability formulas?,2012,28,34,1,1,15664,thomas franccois,Proceedings of the First Workshop on Predicting and Improving Text Readability for target reader populations,0,"Readability formulas are methods used to match texts with the readers' reading level. Several methodological paradigms have previously been investigated in the field. The most popular paradigm dates several decades back and gave rise to well known readability formulas such as the Flesch formula (among several others). This paper compares this approach (henceforth classic) with an emerging paradigm which uses sophisticated NLP-enabled features and machine learning techniques. Our experiments, carried on a corpus of texts for French as a foreign language, yield four main results: (1) the new readability formula performed better than the classic formula; (2) non-classic features were slightly more informative than classic features; (3) modern machine learning algorithms did not improve the explanatory power of our readability model, but allowed to better classify new observations; and (4) combining classic and non-classic features resulted in a significant gain in performance."
F12-2016,Simplification syntaxique de phrases pour le fran{\\c{c}}ais (Syntactic Simplification for {F}rench Sentences) [in {F}rench],2012,0,0,4,1,38767,laetitia brouwers,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
D12-1043,An {``}{AI} readability{''} Formula for {F}rench as a Foreign Language,2012,53,48,1,1,15664,thomas franccois,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"This paper present a new readability formula for French as a foreign language (FFL), which relies on 46 textual features representative of the lexical, syntactic, and semantic levels as well as some of the specificities of the FFL context. We report comparisons between several techniques for feature selection and various learning algorithms. Our best model, based on support vector machines (SVM), significantly outperforms previous FFL formulas. We also found that semantic features behave poorly in our case, in contrast with some previous readability studies on English as a first language."
W11-0813,An N-gram Frequency Database Reference to Handle {MWE} Extraction in {NLP} Applications,2011,22,10,2,0,34336,patrick watrin,Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World,0,"The identification and extraction of Multiword Expressions (MWEs) currently deliver satisfactory results. However, the integration of these results into a wider application remains an issue. This is mainly due to the fact that the association measures (AMs) used to detect MWEs require a critical amount of data and that the MWE dictionaries cannot account for all the lexical and syntactic variations inherent in MWEs. In this study, we use an alternative technique to overcome these limitations. It consists in defining an n-gram frequency data-base that can be used to compute AMs on-the-fly, allowing the extraction procedure to efficiently process all the MWEs in a text, even if they have not been previously observed."
R11-1061,On the Contribution of {MWE}-based Features to a Readability Formula for {F}rench as a Foreign Language,2011,20,11,1,1,15664,thomas franccois,Proceedings of the International Conference Recent Advances in Natural Language Processing 2011,0,"This study aims to assess the usefulness of multi-word expressions (MWEs) as features for a readability formula that predicts the difficulty of texts for French as a foreign language. Using a MWE extractor combining a statistical approach with a linguistic filter, we define 11 predictors. These take into account the density and the probability of MWEs, but also their internal structure. Our experiments show that the predictive power of these 11 variables is low and that a simple approach based on the average probability of n-grams is more effective."
2011.jeptalnrecital-court.3,Quel apport des unit{\\'e}s polylexicales dans une formule de lisibilit{\\'e} pour le fran{\\c{c}}ais langue {\\'e}trang{\\`e}re (What is the contribution of multi-word expressions in a readability formula for the {F}rench as a foreign language),2011,-1,-1,1,1,15664,thomas franccois,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Cette {\'e}tude envisage l{'}emploi des unit{\'e}s polylexicales (UPs) comme pr{\'e}dicteurs dans une formule de lisibilit{\'e} pour le fran{\c{c}}ais langue {\'e}trang{\`e}re. {\`A} l{'}aide d{'}un extracteur d{'}UPs combinant une approche statistique {\`a} un filtre linguistique, nous d{\'e}finissons six variables qui prennent en compte la densit{\'e} et la probabilit{\'e} des UPs nominales, mais aussi leur structure interne. Nos exp{\'e}rimentations concluent {\`a} un faible pouvoir pr{\'e}dictif de ces six variables et r{\'e}v{\`e}lent qu{'}une simple approche bas{\'e}e sur la probabilit{\'e} moyenne des n-grammes des textes est plus efficace."
E09-3003,Combining a Statistical Language Model with Logistic Regression to Predict the Lexical and Syntactic Difficulty of Texts for {FFL},2009,29,18,1,1,15664,thomas franccois,Proceedings of the Student Research Workshop at {EACL} 2009,0,"Reading is known to be an essential task in language learning, but finding the appropriate text for every learner is far from easy. In this context, automatic procedures can support the teacher's work. Some tools exist for English, but at present there are none for French as a foreign language (FFL). In this paper, we present an original approach to assessing the readability of FFL texts using NLP techniques and extracts from FFL textbooks as our corpus. Two logistic regression models based on lexical and grammatical features are explored and give quite good predictions on new texts. The results shows a slight superiority for multinomial logistic regression over the proportional odds model."
2009.jeptalnrecital-recital.7,Mod{\\`e}les statistiques pour l{'}estimation automatique de la difficult{\\'e} de textes de {FLE},2009,-1,-1,1,1,15664,thomas franccois,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues,0,"La lecture constitue l{'}une des t{\^a}ches essentielles dans l{'}apprentissage d{'}une langue {\'e}trang{\`e}re. Toutefois, la d{\'e}couverte d{'}un texte portant sur un sujet pr{\'e}cis et qui soit adapt{\'e} au niveau de chaque apprenant est consommatrice de temps et pourrait {\^e}tre automatis{\'e}e. Des exp{\'e}riences montrent que, pour l{'}anglais, l{'}utilisation de classifieurs statistiques permet d{'}estimer automatiquement la difficult{\'e} d{'}un texte. Dans cet article, nous proposons une m{\'e}thodologie originale comparant, pour le fran{\c{c}}ais langue {\'e}trang{\`e}re (FLE), diverses techniques de classification (la r{\'e}gression logistique, le bagging et le boosting) sur deux corpus d{'}entra{\^\i}nement. Il ressort de cette analyse comparative une l{\'e}g{\`e}re sup{\'e}riorit{\'e} de la r{\'e}gression logistique multinomiale."
