2020.acl-main.552,P18-3015,0,0.0185503,"ix benchmark datasets. Notably, we obtain a state-of-the-art extractive result on CNN/DailyMail (44.41 in ROUGE-1) by only using the base version of BERT. Moreover, we seek to observe where the performance gain of our model comes from. 2 Related Work 2.1 Extractive Summarization Recent research work on extractive summarization spans a large range of approaches. These work usually instantiate their encoder-decoder framework by choosing RNN (Zhou et al., 2018), Transformer (Zhong et al., 2019b; Wang et al., 2019) or GNN (Wang et al., 2020) as encoder, non-auto-regressive (Narayan et al., 2018b; Arumae and Liu, 2018) or auto-regressive decoders (Jadhav and Rajan, 2018; Liu and Lapata, 2019). Despite the effectiveness, these models are essentially sentence-level extractors with individual scoring process favor the highest scoring sentence, which probably is not the optimal one to form summary1 . The application of RL provides a means of summary-level scoring and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can a"
2020.acl-main.552,D19-5402,0,0.382219,"ly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than considering the semantics 6197 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197–6208 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of the entire summary. This makes them more inclined to select highly generalized sentences while ignoring the coupling of multiple sentences. Narayan et al. (2018b); Bae et al. (2019) utilize reinforcement learning (RL) to achieve summarylevel scoring, but still limited to the architecture of sentence-level summarizers. To better understand the advantages and limitations of sentence-level and summary-level approaches, we conduct an analysis on six benchmark datasets (in Section 3) to explore the characteristics of these two methods. We find that there is indeed an inherent gap between the two approaches across these datasets, which motivates us to propose the following summary-level method. In this paper, we propose a novel summary-level framework (M ATCH S UM, Figure 1) a"
2020.acl-main.552,P18-1063,0,0.17681,"sed our codes, processed dataset, as well as generated summaries in https://github. com/maszhongming/MatchSum. 1 BERT one from the original text, model the relationship between the sentences, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivation. At the stage of selecting sentences to form a summary, it will skip the sentence that has trigram overlapping with the previously selected sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially"
2020.acl-main.552,P16-1046,0,0.147785,"ave driven the state-of-the-art extractive result on CNN/DailyMail to a new level (44.41 in ROUGE-1). Experiments on the other five datasets also show the effectiveness of the matching framework. We believe the power of this matching-based summarization framework has not been fully exploited. To encourage more instantiations in the future, we have released our codes, processed dataset, as well as generated summaries in https://github. com/maszhongming/MatchSum. 1 BERT one from the original text, model the relationship between the sentences, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivati"
2020.acl-main.552,N18-2097,0,0.10261,"ain more convicing explanations, we perform experiments on six divergent mainstream datasets as follows. Reddit XSum CNN/DM Wiki PubMed M-News Ext Sel Size 5 1, 2 15 5 1, 2 15 5 2, 3 20 5 3, 4, 5 16 7 6 7 10 9 9 Table 2: Details about the candidate summary for different datasets. Ext denotes the number of sentences after we prune the original document, Sel denotes the number of sentences to form a candidate summary and Size is the number of final candidate summaries. CNN/DailyMail (Hermann et al., 2015) is a commonly used news summarization dataset modified by Nallapati et al. (2016). PubMed (Cohan et al., 2018) is collected from scientific papers. We modify this dataset by using the introduction section as the document and the abstract section as the corresponding summary. WikiHow (Koupaee and Wang, 2018) is a diverse dataset extracted from an online knowledge base. XSum (Narayan et al., 2018a) is a one-sentence summary dataset to answer the question “What is the article about?”. Multi-News (Fabbri et al., 2019) is a multi-document news summarization dataset, we concatenate the source documents as a single input. Reddit (Kim et al., 2019) is a highly abstractive dataset collected from social media p"
2020.acl-main.552,N19-1423,0,0.0177712,"h as information retrieval (Mitra et al., 2017), question answering (Yih et al., 2013; Severyn and Moschitti, 2015), natural language inference (Wang and Jiang, 2016; Wang et al., 2017) and so on. One of the most conventional approaches to semantic text matching is to learn a vector representation for each text fragment, and then apply typical similarity metrics to compute the matching scores. Specific to extractive summarization, we propose a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary. Siamese BERT leverages the pre-trained BERT (Devlin et al., 2019) in a Siamese network structure (Bromley et al., 1994; Hoffer and Ailon, 2015; Reimers and Gurevych, 2019) to derive semantically meaningful text embeddings that can be compared using cosine-similarity. A good summary has the highest similarity among a set of candidate summaries. We evaluate the proposed matching framework and perform significance testing on a range of benchmark datasets. Our model outperforms strong baselines significantly in all cases and improve the state-of-the-art extractive result on CNN/DailyMail. Besides, we design experiments to observe the gains brought by our framew"
2020.acl-main.552,D18-1409,0,0.320551,"Missing"
2020.acl-main.552,P19-1102,0,0.0868974,"te summary and Size is the number of final candidate summaries. CNN/DailyMail (Hermann et al., 2015) is a commonly used news summarization dataset modified by Nallapati et al. (2016). PubMed (Cohan et al., 2018) is collected from scientific papers. We modify this dataset by using the introduction section as the document and the abstract section as the corresponding summary. WikiHow (Koupaee and Wang, 2018) is a diverse dataset extracted from an online knowledge base. XSum (Narayan et al., 2018a) is a one-sentence summary dataset to answer the question “What is the article about?”. Multi-News (Fabbri et al., 2019) is a multi-document news summarization dataset, we concatenate the source documents as a single input. Reddit (Kim et al., 2019) is a highly abstractive dataset collected from social media platform. We use the TIFU-long version of Reddit. 5.2 Implementation Details We use the base version of BERT to implement our models in all experiments. Adam optimizer (Kingma and Ba, 2014) with warming-up is used and our learning rate schedule follows Vaswani et al. (2017) as: lr = 2e−3 · min(step−0.5 , step · wm−1.5 ), (11) where each step is a batch size of 32 and wm denotes warmup steps of 10,000. We ch"
2020.acl-main.552,N10-1131,0,0.17166,"orts are still limited to auto-regressive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for conten"
2020.acl-main.552,W09-1802,0,0.140904,"Missing"
2020.acl-main.552,P18-1014,0,0.0825034,"d dataset, as well as generated summaries in https://github. com/maszhongming/MatchSum. 1 BERT one from the original text, model the relationship between the sentences, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivation. At the stage of selecting sentences to form a summary, it will skip the sentence that has trigram overlapping with the previously selected sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extracto"
2020.acl-main.552,N19-1260,0,0.0298981,"zation dataset modified by Nallapati et al. (2016). PubMed (Cohan et al., 2018) is collected from scientific papers. We modify this dataset by using the introduction section as the document and the abstract section as the corresponding summary. WikiHow (Koupaee and Wang, 2018) is a diverse dataset extracted from an online knowledge base. XSum (Narayan et al., 2018a) is a one-sentence summary dataset to answer the question “What is the article about?”. Multi-News (Fabbri et al., 2019) is a multi-document news summarization dataset, we concatenate the source documents as a single input. Reddit (Kim et al., 2019) is a highly abstractive dataset collected from social media platform. We use the TIFU-long version of Reddit. 5.2 Implementation Details We use the base version of BERT to implement our models in all experiments. Adam optimizer (Kingma and Ba, 2014) with warming-up is used and our learning rate schedule follows Vaswani et al. (2017) as: lr = 2e−3 · min(step−0.5 , step · wm−1.5 ), (11) where each step is a batch size of 32 and wm denotes warmup steps of 10,000. We choose γ1 = 0 and γ2 = 0.01. When γ1 &lt;0.05 and 0.005&lt;γ2 &lt;0.05 they have little effect on performance, otherwise they will cause per"
2020.acl-main.552,P19-1209,0,0.0413831,"nd Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our model can be viewed as an extract-then-match framework, which also employs a sentence extractor to prune unnecessary information. 3 gsen (C) = 1 X R(s, C∗ ), |C| (1) s∈C where s is the sentence in C and |C |represents the number of sentences. R(·) denotes the average ROUGE score2 . Thus, gsen (C) indicates the average overlaps between each sentence in C and the gold summary C ∗ . 2) Summary-Level Score: gsum (C) = R(C, C ∗ ), (2) wher"
2020.acl-main.552,N03-1020,0,0.495644,"Missing"
2020.acl-main.552,D19-1387,0,0.477529,"s, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivation. At the stage of selecting sentences to form a summary, it will skip the sentence that has trigram overlapping with the previously selected sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than considering the semantics 6197 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197–6208 c July 5 - 10"
2020.acl-main.552,2021.ccl-1.108,0,0.201929,"Missing"
2020.acl-main.552,N19-1397,0,0.0500472,"(Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our model can be viewed as an extract-then-match framework, which also employs a sentence extractor to prune unnecessary information. 3 gsen (C) = 1 X R(s, C∗ ), |C| (1) s∈C where s is the sentence in C and |C |represents the number of sentences. R(·) denotes the average ROUGE score2 . Thus, gsen (C) indicates the average overlaps between each sentence in C and the gold summary C ∗ . 2) Summary-Level Score: gsum (C) = R(C, C ∗ ), (2) where gsum (C) considers sentences in C as a whol"
2020.acl-main.552,D16-1031,0,0.0217077,"ication of RL provides a means of summary-level scoring and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. ("
2020.acl-main.552,K16-1028,0,0.105737,"Missing"
2020.acl-main.552,D18-1206,0,0.460092,"ed sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than considering the semantics 6197 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197–6208 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of the entire summary. This makes them more inclined to select highly generalized sentences while ignoring the coupling of multiple sentences. Narayan et al. (2018b); Bae et al. (2019) utilize reinforcement learning (RL) to achieve summarylevel scoring, but still limited to the architecture of sentence-level summarizers. To better understand the advantages and limitations of sentence-level and summary-level approaches, we conduct an analysis on six benchmark datasets (in Section 3) to explore the characteristics of these two methods. We find that there is indeed an inherent gap between the two approaches across these datasets, which motivates us to propose the following summary-level method. In this paper, we propose a novel summary-level framework (M A"
2020.acl-main.552,N18-1158,0,0.356783,"ed sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than considering the semantics 6197 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6197–6208 c July 5 - 10, 2020. 2020 Association for Computational Linguistics of the entire summary. This makes them more inclined to select highly generalized sentences while ignoring the coupling of multiple sentences. Narayan et al. (2018b); Bae et al. (2019) utilize reinforcement learning (RL) to achieve summarylevel scoring, but still limited to the architecture of sentence-level summarizers. To better understand the advantages and limitations of sentence-level and summary-level approaches, we conduct an analysis on six benchmark datasets (in Section 3) to explore the characteristics of these two methods. We find that there is indeed an inherent gap between the two approaches across these datasets, which motivates us to propose the following summary-level method. In this paper, we propose a novel summary-level framework (M A"
2020.acl-main.552,D19-1410,0,0.023607,"Moschitti, 2015), natural language inference (Wang and Jiang, 2016; Wang et al., 2017) and so on. One of the most conventional approaches to semantic text matching is to learn a vector representation for each text fragment, and then apply typical similarity metrics to compute the matching scores. Specific to extractive summarization, we propose a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary. Siamese BERT leverages the pre-trained BERT (Devlin et al., 2019) in a Siamese network structure (Bromley et al., 1994; Hoffer and Ailon, 2015; Reimers and Gurevych, 2019) to derive semantically meaningful text embeddings that can be compared using cosine-similarity. A good summary has the highest similarity among a set of candidate summaries. We evaluate the proposed matching framework and perform significance testing on a range of benchmark datasets. Our model outperforms strong baselines significantly in all cases and improve the state-of-the-art extractive result on CNN/DailyMail. Besides, we design experiments to observe the gains brought by our framework. We summarize our contributions as follows: 1) Instead of scoring and extracting sentences one by one"
2020.acl-main.552,2020.acl-main.553,1,0.763028,"ework has achieved superior performance compared with strong baselines on six benchmark datasets. Notably, we obtain a state-of-the-art extractive result on CNN/DailyMail (44.41 in ROUGE-1) by only using the base version of BERT. Moreover, we seek to observe where the performance gain of our model comes from. 2 Related Work 2.1 Extractive Summarization Recent research work on extractive summarization spans a large range of approaches. These work usually instantiate their encoder-decoder framework by choosing RNN (Zhou et al., 2018), Transformer (Zhong et al., 2019b; Wang et al., 2019) or GNN (Wang et al., 2020) as encoder, non-auto-regressive (Narayan et al., 2018b; Arumae and Liu, 2018) or auto-regressive decoders (Jadhav and Rajan, 2018; Liu and Lapata, 2019). Despite the effectiveness, these models are essentially sentence-level extractors with individual scoring process favor the highest scoring sentence, which probably is not the optimal one to form summary1 . The application of RL provides a means of summary-level scoring and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive or non-auto-regressive architectures. Besides, i"
2020.acl-main.552,N16-1170,0,0.0128706,"a novel summary-level framework (M ATCH S UM, Figure 1) and conceptualize extractive summarization as a semantic text matching problem. The principle idea is that a good summary should be more semantically similar as a whole to the source document than the unqualified summaries. Semantic text matching is an important research problem to estimate semantic similarity between a source and a target text fragment, which has been applied in many fields, such as information retrieval (Mitra et al., 2017), question answering (Yih et al., 2013; Severyn and Moschitti, 2015), natural language inference (Wang and Jiang, 2016; Wang et al., 2017) and so on. One of the most conventional approaches to semantic text matching is to learn a vector representation for each text fragment, and then apply typical similarity metrics to compute the matching scores. Specific to extractive summarization, we propose a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary. Siamese BERT leverages the pre-trained BERT (Devlin et al., 2019) in a Siamese network structure (Bromley et al., 1994; Hoffer and Ailon, 2015; Reimers and Gurevych, 2019) to derive semantically meaningful text"
2020.acl-main.552,D19-1324,0,0.0938029,"ximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our model can be viewed as an extract-then-match framework, which also employs a sentence extractor to prune unnecessary information. 3 gsen (C) = 1 X R(s, C∗ ), |C| (1) s∈C where s is the sentence in C and |C |represents the number of sentences. R(·) denotes the average ROUGE score2 . Thus, gsen (C) indicates the average overlaps between each sentence in C and the gold summary C ∗ . 2) Summary-Level Score: gsum (C) = R(C, C ∗ ), (2) where gsum (C) considers se"
2020.acl-main.552,P13-1171,0,0.0389533,"us to propose the following summary-level method. In this paper, we propose a novel summary-level framework (M ATCH S UM, Figure 1) and conceptualize extractive summarization as a semantic text matching problem. The principle idea is that a good summary should be more semantically similar as a whole to the source document than the unqualified summaries. Semantic text matching is an important research problem to estimate semantic similarity between a source and a target text fragment, which has been applied in many fields, such as information retrieval (Mitra et al., 2017), question answering (Yih et al., 2013; Severyn and Moschitti, 2015), natural language inference (Wang and Jiang, 2016; Wang et al., 2017) and so on. One of the most conventional approaches to semantic text matching is to learn a vector representation for each text fragment, and then apply typical similarity metrics to compute the matching scores. Specific to extractive summarization, we propose a Siamese-BERT architecture to compute the similarity between the source document and the candidate summary. Siamese BERT leverages the pre-trained BERT (Devlin et al., 2019) in a Siamese network structure (Bromley et al., 1994; Hoffer and"
2020.acl-main.552,D15-1228,0,0.0168694,"g and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) fo"
2020.acl-main.552,K19-1074,0,0.12965,"essive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our mod"
2020.acl-main.552,P19-1499,0,0.256957,"essive or non-auto-regressive architectures. Besides, in the non-neural approaches, the Integer Linear Programming (ILP) method can also be used for summarylevel scoring (Wan et al., 2015). In addition, there is some work to solve extractive summarization from a semantic perspective before this paper, such as concept coverage (Gillick 6198 1 We will quantify this phenomenon in Section 3. and Favre, 2009), reconstruction (Miao and Blunsom, 2016) and maximize semantic volume (Yogatama et al., 2015). 2.2 Two-stage Summarization Recent studies (Alyguliyev, 2009; Galanis and Androutsopoulos, 2010; Zhang et al., 2019a) have attempted to build two-stage document summarization systems. Specific to extractive summarization, the first stage is usually to extract some fragments of the original text, and the second stage is to select or modify on the basis of these fragments. Chen and Bansal (2018) and Bae et al. (2019) follow a hybrid extract-then-rewrite architecture, with policy-based RL to bridge the two networks together. Lebanoff et al. (2019); Xu and Durrett (2019); Mendes et al. (2019) focus on the extractthen-compress learning paradigm, which will first train an extractor for content selection. Our mod"
2020.acl-main.552,P19-1100,1,0.9257,"and summary-level methods. 3) Our proposed framework has achieved superior performance compared with strong baselines on six benchmark datasets. Notably, we obtain a state-of-the-art extractive result on CNN/DailyMail (44.41 in ROUGE-1) by only using the base version of BERT. Moreover, we seek to observe where the performance gain of our model comes from. 2 Related Work 2.1 Extractive Summarization Recent research work on extractive summarization spans a large range of approaches. These work usually instantiate their encoder-decoder framework by choosing RNN (Zhou et al., 2018), Transformer (Zhong et al., 2019b; Wang et al., 2019) or GNN (Wang et al., 2020) as encoder, non-auto-regressive (Narayan et al., 2018b; Arumae and Liu, 2018) or auto-regressive decoders (Jadhav and Rajan, 2018; Liu and Lapata, 2019). Despite the effectiveness, these models are essentially sentence-level extractors with individual scoring process favor the highest scoring sentence, which probably is not the optimal one to form summary1 . The application of RL provides a means of summary-level scoring and brings improvement (Narayan et al., 2018b; Bae et al., 2019). However, these efforts are still limited to auto-regressive"
2020.acl-main.552,D19-5410,1,0.904806,"Missing"
2020.acl-main.552,P18-1061,0,0.591782,"nerated summaries in https://github. com/maszhongming/MatchSum. 1 BERT one from the original text, model the relationship between the sentences, and then select several sentences to form a summary. Cheng and Lapata (2016); Nallapati et al. (2017) formulate the extractive summarization task as a sequence labeling problem and solve it with an encoder-decoder framework. These models make independent binary decisions for each sentence, resulting in high redundancy. A natural way to address the above problem is to introduce an auto-regressive decoder (Chen and Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018), allowing the scoring operations of different sentences to influence on each other. Trigram Blocking (Paulus et al., 2017; Liu and Lapata, 2019), as a more popular method recently, has the same motivation. At the stage of selecting sentences to form a summary, it will skip the sentence that has trigram overlapping with the previously selected sentences. Surprisingly, this simple method of removing duplication brings a remarkable performance improvement on CNN/DailyMail. The above systems of modeling the relationship between sentences are essentially sentence-level extractors, rather than cons"
2020.acl-main.553,P16-1046,0,0.589481,"l extension from a singledocument setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et"
2020.acl-main.553,D18-1409,0,0.600455,"Missing"
2020.acl-main.553,P16-1188,0,0.0356539,"1 Datasets CNN/DailyMail The CNN/DailyMail question answering dataset (Hermann et al., 2015; Nallapati et al., 2016) is the most widely used benchmark dataset for single-document summarization. The standard dataset split contains 287,227/13,368/11,490 examples for training, validation, and test. For the data prepossessing, we follow Liu and Lapata (2019b), which use the nonanonymized version as See et al. (2017), to get ground-truth labels. NYT50 NYT50 is also a single-document summarization dataset, which was collected from New York Times Annotated Corpus (Sandhaus, 2008) and preprocessed by Durrett et al. (2016). It contains 110,540 articles with summaries and is split into 100,834 and 9706 for training and test. Following Durrett et al. (2016), we use the last 4,000 examples from the training set as validation and filter test examples to 3,452. Multi-News The Multi-News dataset is a largescale multi-document summarization introduced by Fabbri et al. (2019). It contains 56,216 articlessummary pairs and each example consists of 2-10 source documents and a human-written summary. Following their experimental settings, we split the dataset into 44,972/5,622/5,622 for training, validation and test example"
2020.acl-main.553,P19-1102,0,0.41741,"ile d2 contains s21 and s22 . As a relay node, the relation of document-document, sentence-sentence, and sentencedocument can be built through the common word nodes. For example, sentence s11 , s12 and s21 share the same word w1 , which connects them across documents. 3.5 Multi-document Summarization For multi-document summarization, the documentlevel relation is crucial for better understanding the core topic and most important content of this cluster. However, most existing neural models ignore this hierarchical structure and concatenate documents to a single flat sequence(Liu et al., 2018; Fabbri et al., 2019). Others try to model this relation by attention-based full-connected graph or take advantage of similarity or discourse relations(Liu and Lapata, 2019a). Our framework can establish the document-level relationship in the same way as the sentence-level by just adding supernodes for documents(as Figure 3), which means it can be easily adapted from single-document to multi-document summarization. The heterogeneous graph is then extended to three types of nodes: V = Vw ∪ Vs ∪ Vd and Vd = {d1 , · · · , dl } and l is the number of source documents. We name it as H ETER D OC SUMG RAPH. As we can see"
2020.acl-main.553,D18-1443,0,0.112597,"Missing"
2020.acl-main.553,D18-1446,0,0.0861031,"Missing"
2020.acl-main.553,N03-1020,0,0.30361,"Missing"
2020.acl-main.553,D19-1488,0,0.0417672,"nd their associated learning methods (i.e. message passing (Gilmer et al., 2017), selfattention (Velickovic et al., 2017)) are originally designed for the homogeneous graph where the whole graph shares the same type of nodes. However, the graph in the real-world application usually comes with multiple types of nodes (Shi et al., 2016), namely the heterogeneous graph. To model these structures, recent works have made preliminary exploration. Tu et al. (2019) introduced a heterogeneous graph neural network to encode documents, entities and candidates together for multihop reading comprehension. Linmei et al. (2019) focused on semi-supervised short text classification and constructed a topic-entity heterogeneous neural graph. For summarization, Wei (2012) proposes a heterogeneous graph consisting of topic, word and sentence nodes and uses the markov chain model for the iterative update. Wang et al. (2019b) modify TextRank for their graph with keywords and sentences and thus put forward HeteroRank. Inspired by the success of the heterogeneous graph-based neural network on other NLP tasks, we introduce it to extractive text summarization to learn a better node representation. 3 Methodology Given a document"
2020.acl-main.553,P19-1500,0,0.0706958,"s. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are usually hard to capture s"
2020.acl-main.553,D19-1387,0,0.0591542,"s. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are usually hard to capture s"
2020.acl-main.553,N19-1173,0,0.0488689,"Missing"
2020.acl-main.553,D19-1300,0,0.145067,"ed as sentence-word-sentence relationships. H ETER SUMG RAPH directly selects sentences for the summary by node classification, while H ETER SUMG RAPH with trigram blocking further utilizes the n-gram blocking to reduce redundancy. 3 The detailed experimental results are attached in the Appendix Section. R-2 R-L L EAD -3 (See et al., 2017) O RACLE (Liu and Lapata, 2019b) 40.34 17.70 36.57 52.59 31.24 48.87 REFRESH (Narayan et al., 2018) LATENT (Zhang et al., 2018) BanditSum (Dong et al., 2018) NeuSUM (Zhou et al., 2018) JECS (Xu and Durrett, 2019) LSTM+PN (Zhong et al., 2019a) HER w/o Policy (Luo et al., 2019) HER w Policy (Luo et al., 2019) 40.00 41.05 41.50 41.59 41.70 41.85 41.70 42.30 18.20 18.77 18.70 19.01 18.50 18.93 18.30 18.90 36.60 37.54 37.60 37.98 37.90 38.13 37.10 37.60 Ext-BiLSTM Ext-Transformer HSG HSG + Tri-Blocking 41.59 41.33 42.31 42.95 19.03 18.83 19.51 19.76 38.04 37.65 38.74 39.23 Table 1: Performance (Rouge) of our proposed models against recently released summarization systems on CNN/DailyMail. 5 5.1 4.3 R-1 Results and Analysis Single-document Summarization We evaluate our single-document model on CNN/DailyMail and NYT50 and report the unigram, bigram and longest common sub"
2020.acl-main.553,W04-3252,0,0.84773,"s) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are usually hard to capture sentence-level long-distance dependency, especially in the case of the long document or multidocuments. One more intuitive way is to model the relations of sentences using the graph structure. Nevertheless, it is challenging to find an effective graph structure for summarization. Efforts have been made in various ways. Early traditional work makes use of inter-sentence cosine similarity to build the connectivity graph like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Recently, some works account for discourse inter-sentential relationships when building summarization graphs, such as the Approximate Discourse Graph (ADG) with sentence personalization features (Yasunaga et al., 2017) and Rhetorical Structure Theory (RST) graph (Xu et al., 2019). However, they usually rely on external tools and need to take account of the error propagation problem. A more straightforward way is to create a sentence-level fully-connected graph. To some extent, the Transformer encoder (Vaswani et al., 2017) used in recent work(Zhong et al., 2019a; Liu and Lapata, 2019b) can b"
2020.acl-main.553,N18-1158,0,0.504042,"edocument setting to multi-document via introducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al."
2020.acl-main.553,D14-1162,0,0.0827789,"Missing"
2020.acl-main.553,P17-1099,0,0.0994329,"he same update process as sentence nodes. Experiment We evaluate our models both on single- and multidocument summarization tasks. Below, we start our experiment with the description of the datasets. 4.1 Datasets CNN/DailyMail The CNN/DailyMail question answering dataset (Hermann et al., 2015; Nallapati et al., 2016) is the most widely used benchmark dataset for single-document summarization. The standard dataset split contains 287,227/13,368/11,490 examples for training, validation, and test. For the data prepossessing, we follow Liu and Lapata (2019b), which use the nonanonymized version as See et al. (2017), to get ground-truth labels. NYT50 NYT50 is also a single-document summarization dataset, which was collected from New York Times Annotated Corpus (Sandhaus, 2008) and preprocessed by Durrett et al. (2016). It contains 110,540 articles with summaries and is split into 100,834 and 9706 for training and test. Following Durrett et al. (2016), we use the last 4,000 examples from the training set as validation and filter test examples to 3,452. Multi-News The Multi-News dataset is a largescale multi-document summarization introduced by Fabbri et al. (2019). It contains 56,216 articlessummary pairs"
2020.acl-main.553,P19-1260,0,0.03723,"ogically, these works only use one type of nodes, which formulate each document as a homogeneous graph. Heterogeneous Graph for NLP Graph neural networks and their associated learning methods (i.e. message passing (Gilmer et al., 2017), selfattention (Velickovic et al., 2017)) are originally designed for the homogeneous graph where the whole graph shares the same type of nodes. However, the graph in the real-world application usually comes with multiple types of nodes (Shi et al., 2016), namely the heterogeneous graph. To model these structures, recent works have made preliminary exploration. Tu et al. (2019) introduced a heterogeneous graph neural network to encode documents, entities and candidates together for multihop reading comprehension. Linmei et al. (2019) focused on semi-supervised short text classification and constructed a topic-entity heterogeneous neural graph. For summarization, Wei (2012) proposes a heterogeneous graph consisting of topic, word and sentence nodes and uses the markov chain model for the iterative update. Wang et al. (2019b) modify TextRank for their graph with keywords and sentences and thus put forward HeteroRank. Inspired by the success of the heterogeneous graph-"
2020.acl-main.553,D19-5410,1,0.583487,"oducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are us"
2020.acl-main.553,P18-1061,0,0.592842,"t al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are usually hard to capture sentence-level long-distance dependency, especially in the case of the long document or multidocuments. One more intuitive way is to model the relations of sentences using the graph structure. Nevertheless, it is challenging to find an effective graph structure for summarization. Efforts have been made in various ways. Early traditional work makes use of inter-sentence cosine similarity to build the connectivity graph like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Recently, some works account for discourse"
2020.acl-main.553,D19-1324,0,0.321381,"Missing"
2020.acl-main.553,K17-1045,0,0.134843,"uments. One more intuitive way is to model the relations of sentences using the graph structure. Nevertheless, it is challenging to find an effective graph structure for summarization. Efforts have been made in various ways. Early traditional work makes use of inter-sentence cosine similarity to build the connectivity graph like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Recently, some works account for discourse inter-sentential relationships when building summarization graphs, such as the Approximate Discourse Graph (ADG) with sentence personalization features (Yasunaga et al., 2017) and Rhetorical Structure Theory (RST) graph (Xu et al., 2019). However, they usually rely on external tools and need to take account of the error propagation problem. A more straightforward way is to create a sentence-level fully-connected graph. To some extent, the Transformer encoder (Vaswani et al., 2017) used in recent work(Zhong et al., 2019a; Liu and Lapata, 2019b) can be classified into this type, which learns the pairwise interaction between sentences. Despite their success, how to construct an effective graph structure for summarization remains an open question. In this paper, we pro"
2020.acl-main.553,D18-1088,0,0.180792,"Missing"
2020.acl-main.553,2020.acl-main.552,1,0.763251,"use recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018) or Transformer encoders 2 Since our proposed model is orthogonal to the methods that using pre-trained models, we believe our model can be further boosted by taking the pre-trained models to initialize the node representations, which we reserve for the future. (Zhong et al., 2019b; Wang et al., 2019a) for the sentential encoding. Recently, pre-trained language models are also applied in summarization for contextual word representations (Zhong et al., 2019a; Liu and Lapata, 2019b; Xu et al., 2019; Zhong et al., 2020). Another intuitive structure for extractive summarization is the graph, which can better utilize the statistical or linguistic information between sentences. Early works focus on document graphs constructed with the content similarity among sentences, like LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004). Some recent works aim to incorporate a relational priori into the encoder by graph neural networks (GNNs) (Yasunaga et al., 2017; Xu et al., 2019). Methodologically, these works only use one type of nodes, which formulate each document as a homogeneous graph. Heterogen"
2020.acl-main.553,P19-1100,1,0.6356,"oducing document nodes. To our knowledge, we are the first one to introduce different types of nodes into graph-based neural networks for extractive document summarization and perform a comprehensive qualitative analysis to investigate their benefits. The code will be released on Github1 . 1 Introduction Extractive document summarization aims to extract relevant sentences from the original documents and reorganize them as the summary. Recent years have seen a resounding success in the use of deep neural networks on this task (Cheng and Lapata, 2016; Narayan et al., 2018; Arumae and Liu, 2018; Zhong et al., 2019a; Liu and Lapata, 2019b). These existing models mainly follow the encoder-decoder framework in which each sentence will be encoded by neural components with different forms. To effectively extract the summary-worthy sentences from a document, a core step is to model ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/brxx122/ HeterSUMGraph † the cross-sentence relations. Most current models capture cross-sentence relations with recurrent neural networks (RNNs) (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are us"
2020.coling-main.501,2020.emnlp-main.751,1,0.853475,"n, Abstractiveness, and Coverage. To encourage reproducible research, we make all our analysis code and data publicly available.1 1 Introduction Automatic metrics play a significant role in summarization evaluation, profoundly affecting the direction of system optimization. Due to its importance, evaluating the quality of evaluation metrics, also known as meta-evaluation has been a crucial step. Generally, there are two meta-evaluation strategies: (i) assessing how well each metric correlates with human judgments (Lin, 2004; Ng and Abrecht, 2015; Louis and Nenkova, 2013; Peyrard et al., 2017; Bhandari et al., 2020), which requires procuring manual annotations that are expensive and time-consuming, and (ii) measuring the correlation between different metrics (Peyrard, 2019), which is a human judgment-free method. In this work, we focus on the latter and ask two research questions: RQ1: How do automated metrics correlate when ranking summaries in different scoring ranges (low, average, and high)? We revisit the experiments of Peyrard (2019) which concludes that automated metrics strongly disagree for ranking high-scoring summaries. 2 We find that the scoring range has little effect on the correlation of m"
2020.coling-main.501,N18-1065,0,0.136678,"ormalized to be between 0 and 1. Thus, EoS is the average over all metrics of the maximum score that any summary received. A higher EoS score for a document implies that for that document, we can generate higher scoring extractive summaries according to many metrics. i )∩Voc(ri )| 2. Abstractiveness: We define abstracriveness of a document di with reference ri as 1−|Voc(d |Voc(ri )| where Voc(x) is the set of unique tokens of any text x. Abstractiveness measures the overlap in vocabularies of the document and its reference summary. 3. Coverage: We use the definition of Coverage as provided by Grusky et al. (2018) i.e. “the percentage 5704 Metric Bin MS R1 R2 RL JS2 Bin MS R1 R2 RL JS2 BScore L+M+T M+T T 0.75 0.62 0.39 0.72 0.61 0.4 0.7 0.59 0.36 0.71 0.6 0.39 0.64 0.5 0.19 L M T 0.49 0.51 0.39 0.38 0.45 0.4 0.32 0.41 0.36 0.36 0.43 0.39 0.27 0.37 0.19 MS L+M+T M+T T 0.69 0.53 0.23 0.68 0.55 0.24 0.68 0.52 0.21 0.67 0.59 0.37 L M T 0.35 0.4 0.23 0.29 0.39 0.24 0.33 0.39 0.21 0.28 0.4 0.37 R1 L+M+T M+T T 0.75 0.69 0.53 0.91 0.88 0.82 0.66 0.5 0.16 L M T 0.38 0.47 0.53 0.78 0.78 0.82 0.28 0.38 0.16 R2 L+M+T M+T T 0.77 0.72 0.56 0.86 0.76 0.56 L M T 0.4 0.52 0.56 0.69 0.78 0.56 RL L+M+T M+T T 0.67 0.53 0."
2020.coling-main.501,N06-1059,0,0.0755214,"nd associated highlights as summaries. We use the non-anonymized version. 2.2 Evaluation Metrics We examine six metrics that measure the semantic equivalence between two texts, in our case, between the system-generated summary and the reference summary. BERTScore (BScore) measures soft overlap between contextual BERT embeddings of tokens between the two texts3 (Zhang et al., 2020). MoverScore (MS) applies a distance measure to contextualized BERT and ELMo word embeddings4 (Zhao et al., 2019). JS divergence (JS-2) measures Jensen-Shannon divergence between the two text’s bigram distributions5 (Lin et al., 2006). ROUGE-1 (R1) and ROUGE-2 (R2) measure the overlap of unigrams and bigrams respectively6 (Lin, 2004). ROUGE-L measures the overlap of the longest common subsequence between two texts (Lin, 2004). We use the recall variant of all metrics except MoverScore which has no specific recall variant. 2.3 Correlation Measure Kendall’s τ is a measure of the rank correlation between any two measured quantities (in our case scores given by evaluation metrics) and is popular in meta-evaluating metrics at the summary level (Peyrard, 2019). We use the implementation given by Virtanen et al. (2020). 3 Summary"
2020.coling-main.501,J13-2002,0,0.0997494,"t inter-metric agreement - Ease of Summarization, Abstractiveness, and Coverage. To encourage reproducible research, we make all our analysis code and data publicly available.1 1 Introduction Automatic metrics play a significant role in summarization evaluation, profoundly affecting the direction of system optimization. Due to its importance, evaluating the quality of evaluation metrics, also known as meta-evaluation has been a crucial step. Generally, there are two meta-evaluation strategies: (i) assessing how well each metric correlates with human judgments (Lin, 2004; Ng and Abrecht, 2015; Louis and Nenkova, 2013; Peyrard et al., 2017; Bhandari et al., 2020), which requires procuring manual annotations that are expensive and time-consuming, and (ii) measuring the correlation between different metrics (Peyrard, 2019), which is a human judgment-free method. In this work, we focus on the latter and ask two research questions: RQ1: How do automated metrics correlate when ranking summaries in different scoring ranges (low, average, and high)? We revisit the experiments of Peyrard (2019) which concludes that automated metrics strongly disagree for ranking high-scoring summaries. 2 We find that the scoring r"
2020.coling-main.501,D18-1206,0,0.103006,"Missing"
2020.coling-main.501,D15-1222,0,0.0225378,"properties that impact inter-metric agreement - Ease of Summarization, Abstractiveness, and Coverage. To encourage reproducible research, we make all our analysis code and data publicly available.1 1 Introduction Automatic metrics play a significant role in summarization evaluation, profoundly affecting the direction of system optimization. Due to its importance, evaluating the quality of evaluation metrics, also known as meta-evaluation has been a crucial step. Generally, there are two meta-evaluation strategies: (i) assessing how well each metric correlates with human judgments (Lin, 2004; Ng and Abrecht, 2015; Louis and Nenkova, 2013; Peyrard et al., 2017; Bhandari et al., 2020), which requires procuring manual annotations that are expensive and time-consuming, and (ii) measuring the correlation between different metrics (Peyrard, 2019), which is a human judgment-free method. In this work, we focus on the latter and ask two research questions: RQ1: How do automated metrics correlate when ranking summaries in different scoring ranges (low, average, and high)? We revisit the experiments of Peyrard (2019) which concludes that automated metrics strongly disagree for ranking high-scoring summaries. 2 W"
2020.coling-main.501,C16-1024,0,0.017688,"ommon subsequence between two texts (Lin, 2004). We use the recall variant of all metrics except MoverScore which has no specific recall variant. 2.3 Correlation Measure Kendall’s τ is a measure of the rank correlation between any two measured quantities (in our case scores given by evaluation metrics) and is popular in meta-evaluating metrics at the summary level (Peyrard, 2019). We use the implementation given by Virtanen et al. (2020). 3 Summary Generation To simulate the full scoring range of summaries that are possible for a document, we follow Peyrard (2019) and use a genetic algorithm (Peyrard and Eckle-Kohler, 2016) to generate extractive summaries. We optimize for 5 metrics - ROUGE-1, ROUGE-2, ROUGE-L, BERTScore, and MoverScore, generating 100 summaries per metric for each of the nearly 11K documents in the CNNDM test set resulting in 500 summaries per document. After de-duplication, we are left with nearly 419 summaries per document on average. For the TAC dataset, we randomly sample 500 summaries for each document from the nearly 2000 output summaries provided by Peyrard (2019). 4 Experiment and Analysis 4.1 Width of Scoring Range In this experiment, we aim to re-examine the results in Peyrard (2019)"
2020.coling-main.501,W17-4510,0,0.0149298,"- Ease of Summarization, Abstractiveness, and Coverage. To encourage reproducible research, we make all our analysis code and data publicly available.1 1 Introduction Automatic metrics play a significant role in summarization evaluation, profoundly affecting the direction of system optimization. Due to its importance, evaluating the quality of evaluation metrics, also known as meta-evaluation has been a crucial step. Generally, there are two meta-evaluation strategies: (i) assessing how well each metric correlates with human judgments (Lin, 2004; Ng and Abrecht, 2015; Louis and Nenkova, 2013; Peyrard et al., 2017; Bhandari et al., 2020), which requires procuring manual annotations that are expensive and time-consuming, and (ii) measuring the correlation between different metrics (Peyrard, 2019), which is a human judgment-free method. In this work, we focus on the latter and ask two research questions: RQ1: How do automated metrics correlate when ranking summaries in different scoring ranges (low, average, and high)? We revisit the experiments of Peyrard (2019) which concludes that automated metrics strongly disagree for ranking high-scoring summaries. 2 We find that the scoring range has little effect"
2020.coling-main.501,P19-1502,0,0.366491,"significant role in summarization evaluation, profoundly affecting the direction of system optimization. Due to its importance, evaluating the quality of evaluation metrics, also known as meta-evaluation has been a crucial step. Generally, there are two meta-evaluation strategies: (i) assessing how well each metric correlates with human judgments (Lin, 2004; Ng and Abrecht, 2015; Louis and Nenkova, 2013; Peyrard et al., 2017; Bhandari et al., 2020), which requires procuring manual annotations that are expensive and time-consuming, and (ii) measuring the correlation between different metrics (Peyrard, 2019), which is a human judgment-free method. In this work, we focus on the latter and ask two research questions: RQ1: How do automated metrics correlate when ranking summaries in different scoring ranges (low, average, and high)? We revisit the experiments of Peyrard (2019) which concludes that automated metrics strongly disagree for ranking high-scoring summaries. 2 We find that the scoring range has little effect on the correlation of metrics. It is rather the width of the scoring range which affects inter-metric correlation. Specifically, we observe that metrics agree in ranking summaries from"
2020.coling-main.501,D19-1053,0,0.0128467,"et al., 2015) is a commonly used summarization dataset modified by Nallapati et al. (2016), which contains news articles and associated highlights as summaries. We use the non-anonymized version. 2.2 Evaluation Metrics We examine six metrics that measure the semantic equivalence between two texts, in our case, between the system-generated summary and the reference summary. BERTScore (BScore) measures soft overlap between contextual BERT embeddings of tokens between the two texts3 (Zhang et al., 2020). MoverScore (MS) applies a distance measure to contextualized BERT and ELMo word embeddings4 (Zhao et al., 2019). JS divergence (JS-2) measures Jensen-Shannon divergence between the two text’s bigram distributions5 (Lin et al., 2006). ROUGE-1 (R1) and ROUGE-2 (R2) measure the overlap of unigrams and bigrams respectively6 (Lin, 2004). ROUGE-L measures the overlap of the longest common subsequence between two texts (Lin, 2004). We use the recall variant of all metrics except MoverScore which has no specific recall variant. 2.3 Correlation Measure Kendall’s τ is a measure of the rank correlation between any two measured quantities (in our case scores given by evaluation metrics) and is popular in meta-eval"
2020.emnlp-main.489,N19-1078,0,0.0730511,"Missing"
2020.emnlp-main.751,W16-2302,0,0.117828,"Missing"
2020.emnlp-main.751,D19-1307,0,0.0261957,"Missing"
2020.emnlp-main.751,P18-1060,0,0.16456,"f automatic summarization systems. Specifically, we conduct four experiments analyzing the correspondence between various metrics and human evaluation. Somewhat surprisingly, we find that many of the previously attested properties of metrics found on the TAC dataset demonstrate different trends on our newly collected CNNDM dataset, as shown in Tab. 1. For example, MoverScore is the best performing metric for evaluating summaries on dataset TAC, but it is significantly worse than ROUGE-2 on our collected CNNDM set. Additionally, many previous works (Novikova et al., 2017; Peyrard et al., 2017; Chaganty et al., 2018) show that metrics have much lower correlations at comparing summaries than systems. For extractive summaries on CNNDM, however, most metrics are better at comparing summaries than systems. Calls for Future Research These observations demonstrate the limitations of our current bestperforming metrics, highlighting (1) the need for future meta-evaluation to (i) be across multiple datasets and (ii) evaluate metrics on different application scenarios, e.g. summary level vs. system level (2) the need for more systematic metaevaluation of summarization metrics that updates with our ever-evolving sys"
2020.emnlp-main.751,P18-1063,0,0.0542504,"018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9348 Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), twoStageRL (Zhang et al., 2019a), preSummAbs (Liu and Lapata, 2019b), preSummAbsext (Liu and Lapata, 2019b) BART (Lewis et al., 2019) and Semsim (Yoon et al., 2020) as abstractive systems. In total, we use 14 abstractive system outputs for each document in the CNNDM test set. 2.3 Evaluation Metrics We examine eight metrics that measure the agreement between two texts, in our case, between the system summary and reference summary."
2020.emnlp-main.751,P19-1264,0,0.0359635,"total, we use 14 abstractive system outputs for each document in the CNNDM test set. 2.3 Evaluation Metrics We examine eight metrics that measure the agreement between two texts, in our case, between the system summary and reference summary. BERTScore (BScore) measures soft overlap between contextual BERT embeddings of tokens between the two texts4 (Zhang et al., 2020). MoverScore (MScore) applies a distance measure to contextualized BERT and ELMo word embeddings5 (Zhao et al., 2019). Sentence Mover Similarity (SMS) applies minimum distance matching between text based on sentence embeddings (Clark et al., 2019). Word Mover Similarity (WMS) measures similarity using minimum distance matching between texts which are represented as a bag of word embeddings6 (Kusner et al., 2015). JS divergence (JS-2) measures Jensen-Shannon divergence between the two text’s bigram distributions7 (Lin et al., 2006). ROUGE-1 and ROUGE-2 measure overlap of unigrams and bigrams respectively8 (Lin, 2004). ROUGE-L measures overlap of the longest common subsequence between two texts (Lin, 2004). We use the recall variant of all metrics (since the Pyramid method of human evaluations is inherently recall based) except MScore wh"
2020.emnlp-main.751,N06-1059,0,0.13983,"contextual BERT embeddings of tokens between the two texts4 (Zhang et al., 2020). MoverScore (MScore) applies a distance measure to contextualized BERT and ELMo word embeddings5 (Zhao et al., 2019). Sentence Mover Similarity (SMS) applies minimum distance matching between text based on sentence embeddings (Clark et al., 2019). Word Mover Similarity (WMS) measures similarity using minimum distance matching between texts which are represented as a bag of word embeddings6 (Kusner et al., 2015). JS divergence (JS-2) measures Jensen-Shannon divergence between the two text’s bigram distributions7 (Lin et al., 2006). ROUGE-1 and ROUGE-2 measure overlap of unigrams and bigrams respectively8 (Lin, 2004). ROUGE-L measures overlap of the longest common subsequence between two texts (Lin, 2004). We use the recall variant of all metrics (since the Pyramid method of human evaluations is inherently recall based) except MScore which has no specific recall variant. 2.4 Correlation Measures Pearson Correlation is a measure of linear correlation between two variables and is popular in metaevaluating metrics at the system level (Lee Rodgers, 1988). We use the implementation given by Virtanen et al. (2020). William’s"
2020.emnlp-main.751,C04-1072,0,0.120342,"summarization use exclusively automatic evaluation metrics, such as ROUGE (Lin, 2004) , JS-2 (Louis and Nenkova, 2013), S3 (Peyrard et al., 2017), BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019) etc. Among these metrics, ROUGE is by far the most popular, and there is relatively little discussion of how ROUGE may deviate from human judgment and the potential for this deviation to change conclusions drawn regarding relative merit of baseline and proposed methods. To characterize the relative goodness of evaluation metrics, it is necessary to perform metaevaluation (Graham, 2015; Lin and Och, 2004), where a dataset annotated with human judgments (e.g. TAC1 2008 (Dang and Owczarzak, 2008)) is used to test the degree to which automatic metrics correlate therewith. However, the classic TAC meta-evaluation datasets are now 6-12 years old2 and it is not clear whether conclusions found there will hold with modern systems and summarization tasks. Two earlier works exemplify this disconnect: (1) Peyrard (2019) observed that the human-annotated summaries in the TAC dataset are mostly of lower quality than those produced by modern systems and that various automated evaluation metrics strongly dis"
2020.emnlp-main.751,D19-1387,0,0.0914071,"that contains news articles and associated highlights as summaries. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9348 Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), t"
2020.emnlp-main.751,J13-2002,0,0.630999,"not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive): https://github.com/neulab/REALSumm 1 Introduction In text summarization, manual evaluation, as exemplified by the Pyramid method (Nenkova and Passonneau, 2004), is the gold-standard in evaluation. However, due to time required and relatively high cost of annotation, the great majority of research papers on summarization use exclusively automatic evaluation metrics, such as ROUGE (Lin, 2004) , JS-2 (Louis and Nenkova, 2013), S3 (Peyrard et al., 2017), BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019) etc. Among these metrics, ROUGE is by far the most popular, and there is relatively little discussion of how ROUGE may deviate from human judgment and the potential for this deviation to change conclusions drawn regarding relative merit of baseline and proposed methods. To characterize the relative goodness of evaluation metrics, it is necessary to perform metaevaluation (Graham, 2015; Lin and Och, 2004), where a dataset annotated with human judgments (e.g. TAC1 2008 (Dang and Owczarzak, 2008)) is used"
2020.emnlp-main.751,P14-5010,0,0.00281181,"In the end, we obtained nearly 10.5 SCUs on average from each reference summary. System Evaluation During system evaluation the full set of SCUs is presented to crowd workers. Workers are paid similar to Shapira et al. (2019), scaling the rates for fewer SCUs and shorter summary texts. For abstractive systems, we pay $0.20 per summary and for extractive systems, we pay $0.15 per summary since extractive summaries are more readable and might precisely overlap with SCUs. We post-process system output summaries before presenting them to annotators by true-casing the text using Stanford CoreNLP (Manning et al., 2014) and replacing “unknown” tokens with a special symbol “2” (Chaganty et al., 2018). Tab. 2 depicts an example reference summary, system summary, SCUs extracted from the reference summary, and annotations obtained in evaluating the system summary. Annotation Scoring For robustness (Shapira et al., 2019), each system summary is evaluated by 4 crowd workers. Each worker annotates up to 16 SCUs by marking an SCU “present” if it can be 9 We contacted the authors of these systems to gather the corresponding outputs, including variants of the systems. 10 https://www.mturk.com/ 11 In our representative"
2020.emnlp-main.751,K16-1028,0,0.126832,"Missing"
2020.emnlp-main.751,N18-1158,0,0.024709,"CNNDM) (Hermann et al., 2015; Nallapati et al., 2016) is a commonly used summarization dataset that contains news articles and associated highlights as summaries. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018),"
2020.emnlp-main.751,N04-1019,0,0.601402,"ssessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both systemlevel and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive): https://github.com/neulab/REALSumm 1 Introduction In text summarization, manual evaluation, as exemplified by the Pyramid method (Nenkova and Passonneau, 2004), is the gold-standard in evaluation. However, due to time required and relatively high cost of annotation, the great majority of research papers on summarization use exclusively automatic evaluation metrics, such as ROUGE (Lin, 2004) , JS-2 (Louis and Nenkova, 2013), S3 (Peyrard et al., 2017), BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019) etc. Among these metrics, ROUGE is by far the most popular, and there is relatively little discussion of how ROUGE may deviate from human judgment and the potential for this deviation to change conclusions drawn regarding relative merit of b"
2020.emnlp-main.751,D15-1222,0,0.216412,"aper, our human evaluation methodology is based on the Pyramid (Nenkova and Passonneau, 2004) and LitePyramids (Shapira et al., 2019) techniques. Chaganty et al. (2018) also obtain human evaluations on system summaries on the CNNDM dataset, but with a focus on language quality of summaries. In comparison, our work is focused on evaluating content selection. Our work also covers more systems than their study (11 extractive + 14 abstractive vs. 4 abstractive). Meta-evaluation with Human Judgment The effectiveness of different automatic metrics ROUGE-2 (Lin, 2004), ROUGE-L (Lin, 2004), ROUGE-WE (Ng and Abrecht, 2015), JS-2 (Louis and Nenkova, 2013) and S3 (Peyrard et al., 2017) is commonly evaluated based on their correlation with human judgments (e.g., on the TAC2008 (Dang and Owczarzak, 2008) and TAC2009 (Dang and Owczarzak, 2009) datasets). As an important supplementary technique to metaevaluation, Graham (2015) advocate for the use of a significance test, William’s test (Williams, 1959), to measure the improved correlations of a metric with human scores and show that the popular variant of ROUGE (mean ROUGE-2 score) is sub-optimal. Unlike these works, instead of proposing a new metric, in this paper,"
2020.emnlp-main.751,D17-1238,0,0.0911357,"Missing"
2020.emnlp-main.751,P19-1502,0,0.0416069,"drawn regarding relative merit of baseline and proposed methods. To characterize the relative goodness of evaluation metrics, it is necessary to perform metaevaluation (Graham, 2015; Lin and Och, 2004), where a dataset annotated with human judgments (e.g. TAC1 2008 (Dang and Owczarzak, 2008)) is used to test the degree to which automatic metrics correlate therewith. However, the classic TAC meta-evaluation datasets are now 6-12 years old2 and it is not clear whether conclusions found there will hold with modern systems and summarization tasks. Two earlier works exemplify this disconnect: (1) Peyrard (2019) observed that the human-annotated summaries in the TAC dataset are mostly of lower quality than those produced by modern systems and that various automated evaluation metrics strongly disagree in the higher-scoring range in which current systems now operate. (2) Rankel et al. (2013) observed that the correlation between ROUGE and human judgments in the TAC dataset decreases when looking at the best systems only, even for systems from eight years ago, which are far from today’s state-of-the-art. Constrained by few existing human judgment datasets, it remains unknown how existing metrics behave"
2020.emnlp-main.751,W17-4510,0,0.452132,"n datasets and systems. We release a dataset of human judgments that are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive): https://github.com/neulab/REALSumm 1 Introduction In text summarization, manual evaluation, as exemplified by the Pyramid method (Nenkova and Passonneau, 2004), is the gold-standard in evaluation. However, due to time required and relatively high cost of annotation, the great majority of research papers on summarization use exclusively automatic evaluation metrics, such as ROUGE (Lin, 2004) , JS-2 (Louis and Nenkova, 2013), S3 (Peyrard et al., 2017), BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019) etc. Among these metrics, ROUGE is by far the most popular, and there is relatively little discussion of how ROUGE may deviate from human judgment and the potential for this deviation to change conclusions drawn regarding relative merit of baseline and proposed methods. To characterize the relative goodness of evaluation metrics, it is necessary to perform metaevaluation (Graham, 2015; Lin and Och, 2004), where a dataset annotated with human judgments (e.g. TAC1 2008 (Dang and Owczarzak, 2008)) is used to test the degree to which"
2020.emnlp-main.751,P13-2024,0,0.0136503,"wczarzak, 2008)) is used to test the degree to which automatic metrics correlate therewith. However, the classic TAC meta-evaluation datasets are now 6-12 years old2 and it is not clear whether conclusions found there will hold with modern systems and summarization tasks. Two earlier works exemplify this disconnect: (1) Peyrard (2019) observed that the human-annotated summaries in the TAC dataset are mostly of lower quality than those produced by modern systems and that various automated evaluation metrics strongly disagree in the higher-scoring range in which current systems now operate. (2) Rankel et al. (2013) observed that the correlation between ROUGE and human judgments in the TAC dataset decreases when looking at the best systems only, even for systems from eight years ago, which are far from today’s state-of-the-art. Constrained by few existing human judgment datasets, it remains unknown how existing metrics behave on current top-scoring summarization systems. In this paper, we ask the question: does the rapid progress of model development in summarization models require us to re-evaluate the evaluation process used for text summarization? To this end, we create and release a large benchmark f"
2020.emnlp-main.751,P17-1099,0,0.0716575,"er (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9348 Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), twoStageRL (Zhang et al., 2019a), preSummAbs (Liu and Lapata, 2019b), preSummAbsext (Liu and Lapata, 2019b) BART (Lewis et al., 2019) and Semsim (Yoon et al., 2020) as abstractive systems. In total, we use 14 abstractive system outputs for each document in the CNNDM test set. 2.3 Evaluation Metrics We examine eight metrics that measure the agreement between two texts, in our case, between the sy"
2020.emnlp-main.751,2020.acl-main.704,0,0.0851236,"Missing"
2020.emnlp-main.751,N19-1072,0,0.438136,"2) Metrics have much lower correlations when evaluating summaries than systems. (1) ROUGE metrics outperform all other metrics. (2) For extractive summaries, most metrics are better at evaluating summaries than systems. For abstractive summaries, some metrics are better at summary level, others are better at system level. Table 1: Summary of our experiments, observations on existing human judgments on the TAC, and contrasting observations on newly obtained human judgments on the CNNDM dataset. Please refer to Sec. 4 for more details. • Manual evaluations using the lightweight pyramids method (Shapira et al., 2019), which we use as a gold-standard to evaluate summarization systems as well as automated metrics. Using this benchmark, we perform an extensive analysis, which indicates the need to re-examine our assumptions about the evaluation of automatic summarization systems. Specifically, we conduct four experiments analyzing the correspondence between various metrics and human evaluation. Somewhat surprisingly, we find that many of the previously attested properties of metrics found on the TAC dataset demonstrate different trends on our newly collected CNNDM dataset, as shown in Tab. 1. For example, Mo"
2020.emnlp-main.751,D19-1053,0,0.150837,"t are collected from 25 top-scoring neural summarization systems (14 abstractive and 11 extractive): https://github.com/neulab/REALSumm 1 Introduction In text summarization, manual evaluation, as exemplified by the Pyramid method (Nenkova and Passonneau, 2004), is the gold-standard in evaluation. However, due to time required and relatively high cost of annotation, the great majority of research papers on summarization use exclusively automatic evaluation metrics, such as ROUGE (Lin, 2004) , JS-2 (Louis and Nenkova, 2013), S3 (Peyrard et al., 2017), BERTScore (Zhang et al., 2020), MoverScore (Zhao et al., 2019) etc. Among these metrics, ROUGE is by far the most popular, and there is relatively little discussion of how ROUGE may deviate from human judgment and the potential for this deviation to change conclusions drawn regarding relative merit of baseline and proposed methods. To characterize the relative goodness of evaluation metrics, it is necessary to perform metaevaluation (Graham, 2015; Lin and Och, 2004), where a dataset annotated with human judgments (e.g. TAC1 2008 (Dang and Owczarzak, 2008)) is used to test the degree to which automatic metrics correlate therewith. However, the classic TAC"
2020.emnlp-main.751,P19-1100,1,0.857875,"ies. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9348 Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), twoStageRL (Zhang et al., 2019a), preSummAbs (Liu and Lapata, 20"
2020.emnlp-main.751,N18-1156,0,0.0145532,"allapati et al., 2016) is a commonly used summarization dataset that contains news articles and associated highlights as summaries. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9"
2020.emnlp-main.751,K19-1074,0,0.0157723,"commonly used summarization dataset that contains news articles and associated highlights as summaries. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9348 Unilm-v1 (Dong et al., 2"
2020.emnlp-main.751,D18-1088,0,0.0187218,"d during the TAC-2008, TAC-2009 shared tasks. CNN/DailyMail (CNNDM) (Hermann et al., 2015; Nallapati et al., 2016) is a commonly used summarization dataset that contains news articles and associated highlights as summaries. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLr"
2020.emnlp-main.751,P19-1499,0,0.0165874,"commonly used summarization dataset that contains news articles and associated highlights as summaries. We use the version without entities anonymized. 2.2 Representative Systems We use the following representative top-scoring systems that either achieve state-of-the-art (SOTA) results or competitive performance, for which we could gather the outputs on the CNNDM dataset. Extractive summarization systems. We use CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al. (2018)), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum (Zhou et al., 2018), HIBERT (Zhang et al., 2019b), Bert-SumExt (Liu and Lapata, 2019a), CNN-TransformerBiClassifier (CTrans-SL; Zhong et al. (2019)), CNN-Transformer-Pointer (CTrans-PN; Zhong et al. (2019)), HeterGraph (Wang et al., 2020) and MatchSum (Zhong et al., 2020) as representatives of extractive systems, totaling 11 extractive system outputs for each document in the CNNDM test set. Abstractive summarization systems. We use pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), 9348 Unilm-v1 (Dong et al., 2"
2020.tacl-1.12,P18-1087,0,0.177759,"CFU (Section 3.4) generates the target vector rt through a self-attentive operation on Rt , and then learns the contribution of each context to obtain the ultimate context vector rc . Finally, the Output Layer (Section 3.5) composes the context vector and the target vector for predicting the target’s sentiment. 3.1 Problem Formulation et al. (2018). They argued that using one layer of attention to attend all context words may introduce noises and degrade classification accuracy. To alleviate this problem, Chen et al. (2017) proposed refining the attended words in an iterative manner, whereas Li et al. (2018) used a convolutional neural network to extract n-gram features whose contributions were decided by their relative positions to the target in the context sentence. To the best of our knowledge, no existing study has explicitly considered uncovering a sentence’s semantic segments and learning their contributions to a target’s sentiment. We address this problem with a novel target-guided structured attention network in this work. A sentence is a sequence of words S = {w1 , . . . , wi , . . . , wL }, where wi is the one-hot representation of a word and L is the length of the sequence. Given a tar"
2020.tacl-1.12,D17-1047,0,0.165622,"ome widely used for modeling the relatedness between every context word and the target for TDSA (Wang et al., 2016; Yang et al., 2017; Liu and Zhang, 2017; Ma et al., 2017). For example, Yang et al. (2017) assigned attention scores to each context word according to their relevance to the target, and combined all context words with their attention scores to constitute the context representation of the target for sentiment classification. The aforementioned attention-based methods used a single attention layer to capture targetrelated contexts. One drawback of this has been recently examined by Chen et al. (2017) and Li Figure 1: A motivating example, where darker shades denote higher contributions to the sentiment of the target [waiting]. (a) A TDSA model should be able to identify two salient segments from the sentence, and that the second one is more important for determining the target’s sentiment. (b) Existing attention-based models would attend important words individually and fail to determine their relatedness with the target. in this paper. The core components of TG-SAN include a Structured Context Extraction Unit (SCU) and a Context Fusion Unit (CFU). As opposed to using word-level attention"
2020.tacl-1.12,E17-2091,0,0.0826219,"a {jasonzhang, stacychen, chaohe, caneleung}@wisers.com, ppfliu@gmail.com Abstract The major challenge of TDSA lies in modeling the semantic relatedness between the target and its context sentence (Tang et al., 2016a; Chen et al., 2017). Most recent progress in this area benefits from the attention mechanism, which captures the relevance between the target and every other word in the sentence. Based on such word-level correlations, several models have already been proposed for constructing target-related sentence representations for sentiment prediction (Wang et al., 2016; Tang et al., 2016b; Liu and Zhang, 2017; Yang et al., 2017; Ma et al., 2017). One important underlying assumption in existing attention-based models is that words can be used as independent semantic units for modeling the context sentence when performing TDSA. This assumption neglects the fact that a sentence is oftentimes composed of multiple semantic segments, where each segment may contain multiple words expressing a certain meaning or sentiment collectively. Furthermore, different semantic segments may even contribute differently to the sentiment of a certain target. Figure 1 shows an example of a restaurant review, which conta"
2020.tacl-1.12,P14-2009,0,0.166388,"ts in the sentence for determining the target’s sentiment. Early work adopted rulebased methods or statistical methods to solve this problem (Ding et al., 2008; Zhao et al., 2010; Jiang et al., 2011). These methods relied either on handcrafted features, rules, or sentiment lexicons, all of which required massive manual efforts. In recent years, neural networks have achieved great success in various fields for their strong representation capability. They have also been proven effective in modeling the relatedness between the target and its contexts. Recursive neural networks were first used by Dong et al. (2014) and Nguyen and Shirai (2015) for TDSA. Specifically, the target was first converted into the root node of a parsing tree, and then it contexts were composed based on syntactic relations in the tree. As such approaches rely strongly on dependency parsing, they fall short when analyzing nonstandard texts such as comments and tweets, which are commonly used for sentiment analysis. Another line of work applied recurrent neural network (RNN) and its extensions to TDSA for their natural way of encoding sentences in a sequential fashion. For instance, Tang et al. (2016a) utilized two RNNs to individ"
2020.tacl-1.12,D15-1298,0,0.0409783,"Missing"
2020.tacl-1.12,D14-1162,0,0.0824701,"Missing"
2020.tacl-1.12,P11-1016,0,0.0462029,"sociation for Computational Linguistics, vol. 8, pp. 172–182, 2020. https://doi.org/10.1162/tacl a 00308 Action Editor: Walter Daelemans. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. 2 Related Work Given a target and its context sentence, the major challenge of TDSA lies in identifying targetrelated contexts in the sentence for determining the target’s sentiment. Early work adopted rulebased methods or statistical methods to solve this problem (Ding et al., 2008; Zhao et al., 2010; Jiang et al., 2011). These methods relied either on handcrafted features, rules, or sentiment lexicons, all of which required massive manual efforts. In recent years, neural networks have achieved great success in various fields for their strong representation capability. They have also been proven effective in modeling the relatedness between the target and its contexts. Recursive neural networks were first used by Dong et al. (2014) and Nguyen and Shirai (2015) for TDSA. Specifically, the target was first converted into the root node of a parsing tree, and then it contexts were composed based on syntactic rela"
2020.tacl-1.12,S14-2076,0,0.0371571,"s (in italic) have similar contexts, their sentimental orientations are totally different. It is therefore necessary to consider the target itself along with its contexts to predict its sentiment. In the output layer, the context vector rc and the target vector rt are concatenated, and transformed via a non-linear function. The transformed vector is further used in conjunction with rc to build the final feature vector rct : Compared Models To demonstrate the ability of the proposed model, we compare it with three baseline approaches, four attention-based models, and the state-of-the-art. SVM (Kiritchenko et al., 2014): This was a topperforming system in SemEval 2014. It utilized various types of handcrafted features to build a SVM classifier. AdaRNN (Dong et al., 2014): This utilized a recursive neural network based on dependency (16) where f (·) denotes a non-linear activation function, and the ReLU function is adopted in this paper. A softmax layer is then applied to convert the feature vector into a probability distribution: q (y |rct ) = softmax(Wq rct + bq ) X where yi is the true sentiment label, q i is the predicted probability of the true label, θ is the set of parameters of TG-SAN, λ1 and λ2 are r"
2020.tacl-1.12,C16-1311,0,0.297041,"Limited, HKSAR, China {jasonzhang, stacychen, chaohe, caneleung}@wisers.com, ppfliu@gmail.com Abstract The major challenge of TDSA lies in modeling the semantic relatedness between the target and its context sentence (Tang et al., 2016a; Chen et al., 2017). Most recent progress in this area benefits from the attention mechanism, which captures the relevance between the target and every other word in the sentence. Based on such word-level correlations, several models have already been proposed for constructing target-related sentence representations for sentiment prediction (Wang et al., 2016; Tang et al., 2016b; Liu and Zhang, 2017; Yang et al., 2017; Ma et al., 2017). One important underlying assumption in existing attention-based models is that words can be used as independent semantic units for modeling the context sentence when performing TDSA. This assumption neglects the fact that a sentence is oftentimes composed of multiple semantic segments, where each segment may contain multiple words expressing a certain meaning or sentiment collectively. Furthermore, different semantic segments may even contribute differently to the sentiment of a certain target. Figure 1 shows an example of a restaura"
2020.tacl-1.12,D16-1021,0,0.0722839,"contexts were composed based on syntactic relations in the tree. As such approaches rely strongly on dependency parsing, they fall short when analyzing nonstandard texts such as comments and tweets, which are commonly used for sentiment analysis. Another line of work applied recurrent neural network (RNN) and its extensions to TDSA for their natural way of encoding sentences in a sequential fashion. For instance, Tang et al. (2016a) utilized two RNNs to individually capture the left and the right contexts of the target, and then combined the two contexts for sentiment prediction. Zhang et al. (2016) elaborated on this idea by using a gate to leverage the contributions of the two contexts for sentiment prediction. However, such RNN-based methods place more emphasis on the words near the target while ignoring the distant ones, regardless of whether they are target-related. Recently, attention mechanisms have become widely used for modeling the relatedness between every context word and the target for TDSA (Wang et al., 2016; Yang et al., 2017; Liu and Zhang, 2017; Ma et al., 2017). For example, Yang et al. (2017) assigned attention scores to each context word according to their relevance t"
2020.tacl-1.12,D10-1006,0,0.0558197,"nsactions of the Association for Computational Linguistics, vol. 8, pp. 172–182, 2020. https://doi.org/10.1162/tacl a 00308 Action Editor: Walter Daelemans. Submission batch: 8/2019; Revision batch: 12/2019; Published 4/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. 2 Related Work Given a target and its context sentence, the major challenge of TDSA lies in identifying targetrelated contexts in the sentence for determining the target’s sentiment. Early work adopted rulebased methods or statistical methods to solve this problem (Ding et al., 2008; Zhao et al., 2010; Jiang et al., 2011). These methods relied either on handcrafted features, rules, or sentiment lexicons, all of which required massive manual efforts. In recent years, neural networks have achieved great success in various fields for their strong representation capability. They have also been proven effective in modeling the relatedness between the target and its contexts. Recursive neural networks were first used by Dong et al. (2014) and Nguyen and Shirai (2015) for TDSA. Specifically, the target was first converted into the root node of a parsing tree, and then it contexts were composed ba"
2020.tacl-1.12,D16-1058,0,\N,Missing
2021.acl-demo.34,N19-1078,0,0.0266042,"s. A B OARD 4.1 Experimental Setup Attribute Definition We define attributes following Fu et al. (2020a) and three of them are used below: entity length, sentence length and label of entity. Collection of Systems Outputs Currently, we collect system outputs by either implementing them by ourselves or collecting from other researchers 12 http://www.statmt.org/wmt20/ metrics-task.html Analysis using ExplainaBoard Fig. 2 illustrates different types of results driven by four functionality buttons13 over the top-3 NER systems: LUKE (Yamada et al., 2020), FLERT (Schweter and Akbik, 2020) and FLAIR (Akbik et al., 2019). Box A breaks down the performance of the top-1 system over different attributes.14 We can intuitively observe that even the state-of-the-art system does worse on longer entities. Users can further print error cases in the longer entity bucket by clicking the corresponding bin. Box B shows the 1st system’s (LUKE) performance minus the 2nd system’s (FLERT) performance. We can see that although LUKE surpasses FLERT holistically, it performs worse when dealing with PERSON entities. Box C identifies samples that all systems mispredict. Further analysis of these samples uncovers challenging patter"
2021.acl-demo.34,P05-1001,0,0.0441952,"andard in NLP to release the system outputs that E XPLAINA B OARD needs. 283 classification (Pang et al., 2002), topic identification (Wang and Manning, 2012), and intention detection (Chen et al., 2013). Task Text Pair Classification Prediction of a class given two texts, such as the natural language inference task (Bowman et al., 2015). Sequence Labeling Prediction of a label for each token in a sequence. The E XPLAINA B OARD currently includes four concrete tasks: named entity recognition (Tjong Kim Sang and De Meulder, 2003), part-of-speech tagging (Toutanova et al., 2003), text chunking (Ando and Zhang, 2005), and Chinese word segmentation (Chen et al., 2015). 8 40 2 Topic 4 18 2 Intention 1 3 2 Text-Span Classification Aspect Sentiment 4 20 4 Text Pair Classification NLI 2 6 7 NER 3 74 9 Text Classification Text-Span Classification Prediction of a predefined class from the input of a text and a span, such as aspect-based sentiment classification task (Pappas and Popescu-Belis, 2014). We collect topperform system outputs from (Dai et al., 2021). Data Model Attr. Sentiment Sequence Labeling Structure Pred. Text Generation POS 3 14 4 Chunking 3 14 9 CWS 7 64 7 Semantic Parsing 4 12 4 Summarization 2"
2021.acl-demo.34,D13-1160,0,0.0107544,"t classification task (Pappas and Popescu-Belis, 2014). We collect topperform system outputs from (Dai et al., 2021). Data Model Attr. Sentiment Sequence Labeling Structure Pred. Text Generation POS 3 14 4 Chunking 3 14 9 CWS 7 64 7 Semantic Parsing 4 12 4 Summarization 2 36 7 Translation 4 60 9 Table 2: Brief descriptions of tasks, datasets and systems that E XPLAINA B OARD currently supports. “Attr.” denotes Attribute. “Pred.” denotes “Prediction”. Structure Prediction Prediction of a syntactic or semantic structure from text, where E X PLAINA B OARD currently covers semantic parsing tasks (Berant et al., 2013; Yu et al., 2018). (Fu et al., 2020b; Schweter and Akbik, 2020; Yamada et al., 2020). Using these methods, we have gathered 74 models on six NER datasets with system output information. Text Generation E XPLAINA B OARD also considers text generation tasks, and currently mainly focuses on conditional text generation, for example, text summarization (Rush et al., 2015; Liu and Lapata, 2019) and machine translation . System outputs on text summarization are expanded based on the previous work’s collection (Bhandari et al., 2020) as well as recently state-of-the-art systems (Liu and Liu, 2021) wh"
2021.acl-demo.34,2020.emnlp-main.751,1,0.728601,"text, where E X PLAINA B OARD currently covers semantic parsing tasks (Berant et al., 2013; Yu et al., 2018). (Fu et al., 2020b; Schweter and Akbik, 2020; Yamada et al., 2020). Using these methods, we have gathered 74 models on six NER datasets with system output information. Text Generation E XPLAINA B OARD also considers text generation tasks, and currently mainly focuses on conditional text generation, for example, text summarization (Rush et al., 2015; Liu and Lapata, 2019) and machine translation . System outputs on text summarization are expanded based on the previous work’s collection (Bhandari et al., 2020) as well as recently state-of-the-art systems (Liu and Liu, 2021) while outputs from machine translation are collected from the WMT20.12 4.2 4 Case Study Here, we briefly showcase the actual E XPLAIN interface through a case study on analyzing state-of-the-art NER systems. A B OARD 4.1 Experimental Setup Attribute Definition We define attributes following Fu et al. (2020a) and three of them are used below: entity length, sentence length and label of entity. Collection of Systems Outputs Currently, we collect system outputs by either implementing them by ourselves or collecting from other resea"
2021.acl-demo.34,D15-1075,0,0.0257375,"low, and show high-level statistics in Tab. 2. Text Classification Prediction of one or multiple pre-defined label(s) for a given input text. The current interface includes datasets for sentiment 11 265 of these models are implemented by us, as unfortunately it is currently not standard in NLP to release the system outputs that E XPLAINA B OARD needs. 283 classification (Pang et al., 2002), topic identification (Wang and Manning, 2012), and intention detection (Chen et al., 2013). Task Text Pair Classification Prediction of a class given two texts, such as the natural language inference task (Bowman et al., 2015). Sequence Labeling Prediction of a label for each token in a sequence. The E XPLAINA B OARD currently includes four concrete tasks: named entity recognition (Tjong Kim Sang and De Meulder, 2003), part-of-speech tagging (Toutanova et al., 2003), text chunking (Ando and Zhang, 2005), and Chinese word segmentation (Chen et al., 2015). 8 40 2 Topic 4 18 2 Intention 1 3 2 Text-Span Classification Aspect Sentiment 4 20 4 Text Pair Classification NLI 2 6 7 NER 3 74 9 Text Classification Text-Span Classification Prediction of a predefined class from the input of a text and a span, such as aspect-base"
2021.acl-demo.34,D15-1141,1,0.793763,"AINA B OARD needs. 283 classification (Pang et al., 2002), topic identification (Wang and Manning, 2012), and intention detection (Chen et al., 2013). Task Text Pair Classification Prediction of a class given two texts, such as the natural language inference task (Bowman et al., 2015). Sequence Labeling Prediction of a label for each token in a sequence. The E XPLAINA B OARD currently includes four concrete tasks: named entity recognition (Tjong Kim Sang and De Meulder, 2003), part-of-speech tagging (Toutanova et al., 2003), text chunking (Ando and Zhang, 2005), and Chinese word segmentation (Chen et al., 2015). 8 40 2 Topic 4 18 2 Intention 1 3 2 Text-Span Classification Aspect Sentiment 4 20 4 Text Pair Classification NLI 2 6 7 NER 3 74 9 Text Classification Text-Span Classification Prediction of a predefined class from the input of a text and a span, such as aspect-based sentiment classification task (Pappas and Popescu-Belis, 2014). We collect topperform system outputs from (Dai et al., 2021). Data Model Attr. Sentiment Sequence Labeling Structure Pred. Text Generation POS 3 14 4 Chunking 3 14 9 CWS 7 64 7 Semantic Parsing 4 12 4 Summarization 2 36 7 Translation 4 60 9 Table 2: Brief description"
2021.acl-demo.34,N13-1124,0,0.0164703,"Missing"
2021.acl-demo.34,2021.naacl-main.146,1,0.797971,"Missing"
2021.acl-demo.34,I11-1153,0,0.0410927,"be useful to identify challenging samples or even annotation errors. 2. In single-system analysis, users can choose particular buckets in the performance histogram9 and see corresponding error samples in that bucket (e.g. which long entities does the current system mispredict?). 3. In pairwise analysis, users can select a bucket, and the unique errors (e.g. system A succeeds while B fails and vice versa) of two models will be displayed. F5: System Combination: Is there potential complementarity between different systems? System combination (Ting and Witten, 1997; González-Rubio et al., 2011; Duh et al., 2011) is a technique to improve performance by combining the output from multiple existing systems. In E X PLAINA B OARD, users can choose multiple systems and obtain combined results calculated by voting over multiple base systems.10 In practice, for NER task, we use the recently proposed S PAN N ER (Fu 9 Each bin of the performance histogram is clickable, returning an error case table. 10 With the system combination button of Explainaboard, we observed the-state-of-the art performance of some tasks (e.g., NER, Chunking) can be further improved. et al., 2021) as a combiner, and for text summarizat"
2021.acl-demo.34,2020.emnlp-main.393,0,0.283313,"ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 280–289, August 1st - August 6th, 2021. ©2021 Association for Computational Linguistics they provide a standardized evaluation setup, often on multiple tasks, that eases reproducible model comparison across organizations. However, at the same time, due to the prestige imbued by a top, or high, place on a leaderboard they also can result in a singular focus on raising evaluation numbers at the cost of deeper scientific understanding of model properties (Ethayarajh and Jurafsky, 2020). In particular, we argue that, among others, the following are three major limitations of the existing leaderboard paradigm: • Interpretability: Most existing leaderboards commonly use a single number to summarize system performance holistically. This is conducive to system ranking but at the same time, the results are opaque, making the strengths and weaknesses of systems less interpretable. • Interactivity: Existing leaderboards are static and non-interactive, which limits the ability of users to dig deeper into the results. Thus, (1) they usually do not flexibly support more complex evalua"
2021.acl-demo.34,2021.acl-long.558,1,0.819882,"Missing"
2021.acl-demo.34,2020.emnlp-main.489,1,0.776427,"iety of NLP tasks (as illustrated in Tab. 1). Many of these functionalities are grounded in existing research on evaluation and fine-grained diagnostics. In this paper, we describe E XPLAINA B OARD (see Fig.1), a software package and hosted leaderboard that satisfies all of the above desiderata. It also serves as a prototype implementation of 2.1 Interpretability some desirable features that may be included in future leaderboards, even independent of the pro- Interpretable evaluation (Popovi´c and Ney, 2011; vided software itself. We have deployed E XPLAIN - Stymne, 2011; Neubig et al., 2019; Fu et al., 2020a), is a research area that considers methods that break A B OARD for 9 different tasks and 41 different down the holistic performance of each system into datasets, and demonstrate how it can be easily different interpretable groups. For example, in a adapted to new tasks of interest. We expect that E XPLAINA B OARD will benefit 5 http://explainaboard.nlpedia.ai/ different steps of the research process: leaderboard/xtreme/ 6 (i) System Developement: E XPLAINA B OARD https://sites.research.google/xtreme/ 7 Since the first release of E XPLAINA B OARD, we have provides more detailed information r"
2021.acl-demo.34,2020.emnlp-main.457,1,0.724348,"iety of NLP tasks (as illustrated in Tab. 1). Many of these functionalities are grounded in existing research on evaluation and fine-grained diagnostics. In this paper, we describe E XPLAINA B OARD (see Fig.1), a software package and hosted leaderboard that satisfies all of the above desiderata. It also serves as a prototype implementation of 2.1 Interpretability some desirable features that may be included in future leaderboards, even independent of the pro- Interpretable evaluation (Popovi´c and Ney, 2011; vided software itself. We have deployed E XPLAIN - Stymne, 2011; Neubig et al., 2019; Fu et al., 2020a), is a research area that considers methods that break A B OARD for 9 different tasks and 41 different down the holistic performance of each system into datasets, and demonstrate how it can be easily different interpretable groups. For example, in a adapted to new tasks of interest. We expect that E XPLAINA B OARD will benefit 5 http://explainaboard.nlpedia.ai/ different steps of the research process: leaderboard/xtreme/ 6 (i) System Developement: E XPLAINA B OARD https://sites.research.google/xtreme/ 7 Since the first release of E XPLAINA B OARD, we have provides more detailed information r"
2021.acl-demo.34,P11-1127,0,0.320367,"common error cases, which can be useful to identify challenging samples or even annotation errors. 2. In single-system analysis, users can choose particular buckets in the performance histogram9 and see corresponding error samples in that bucket (e.g. which long entities does the current system mispredict?). 3. In pairwise analysis, users can select a bucket, and the unique errors (e.g. system A succeeds while B fails and vice versa) of two models will be displayed. F5: System Combination: Is there potential complementarity between different systems? System combination (Ting and Witten, 1997; González-Rubio et al., 2011; Duh et al., 2011) is a technique to improve performance by combining the output from multiple existing systems. In E X PLAINA B OARD, users can choose multiple systems and obtain combined results calculated by voting over multiple base systems.10 In practice, for NER task, we use the recently proposed S PAN N ER (Fu 9 Each bin of the performance histogram is clickable, returning an error case table. 10 With the system combination button of Explainaboard, we observed the-state-of-the art performance of some tasks (e.g., NER, Chunking) can be further improved. et al., 2021) as a combiner, and"
2021.acl-demo.34,2021.acl-short.135,1,0.72831,"(Berant et al., 2013; Yu et al., 2018). (Fu et al., 2020b; Schweter and Akbik, 2020; Yamada et al., 2020). Using these methods, we have gathered 74 models on six NER datasets with system output information. Text Generation E XPLAINA B OARD also considers text generation tasks, and currently mainly focuses on conditional text generation, for example, text summarization (Rush et al., 2015; Liu and Lapata, 2019) and machine translation . System outputs on text summarization are expanded based on the previous work’s collection (Bhandari et al., 2020) as well as recently state-of-the-art systems (Liu and Liu, 2021) while outputs from machine translation are collected from the WMT20.12 4.2 4 Case Study Here, we briefly showcase the actual E XPLAIN interface through a case study on analyzing state-of-the-art NER systems. A B OARD 4.1 Experimental Setup Attribute Definition We define attributes following Fu et al. (2020a) and three of them are used below: entity length, sentence length and label of entity. Collection of Systems Outputs Currently, we collect system outputs by either implementing them by ourselves or collecting from other researchers 12 http://www.statmt.org/wmt20/ metrics-task.html Analysis"
2021.acl-demo.34,2021.naacl-main.113,1,0.869877,"om multiple existing systems. In E X PLAINA B OARD, users can choose multiple systems and obtain combined results calculated by voting over multiple base systems.10 In practice, for NER task, we use the recently proposed S PAN N ER (Fu 9 Each bin of the performance histogram is clickable, returning an error case table. 10 With the system combination button of Explainaboard, we observed the-state-of-the art performance of some tasks (e.g., NER, Chunking) can be further improved. et al., 2021) as a combiner, and for text summarization we employed R EFACTOR, a state-of-the-art ensemble approach (Liu et al., 2021). Regarding the other tasks, we adopt the majority voting method for system combination. 2.3 Reliability The experimental conclusions obtained from the evaluation metrics are not necessarily statistically reliable, especially when the experimental results can be affected by many factors. E XPLAIN A B OARD also makes a step towards more reliable interpretable evaluation. F6: Confidence Analysis: To what extent can we trust the results of our system? E XPLAIN A B OARD can perform confidence analysis over both holistic and fine-grained performance metrics. As shown in Tab. 1, for each bucket, the"
2021.acl-demo.34,N19-4007,1,0.846656,"licable to a wide variety of NLP tasks (as illustrated in Tab. 1). Many of these functionalities are grounded in existing research on evaluation and fine-grained diagnostics. In this paper, we describe E XPLAINA B OARD (see Fig.1), a software package and hosted leaderboard that satisfies all of the above desiderata. It also serves as a prototype implementation of 2.1 Interpretability some desirable features that may be included in future leaderboards, even independent of the pro- Interpretable evaluation (Popovi´c and Ney, 2011; vided software itself. We have deployed E XPLAIN - Stymne, 2011; Neubig et al., 2019; Fu et al., 2020a), is a research area that considers methods that break A B OARD for 9 different tasks and 41 different down the holistic performance of each system into datasets, and demonstrate how it can be easily different interpretable groups. For example, in a adapted to new tasks of interest. We expect that E XPLAINA B OARD will benefit 5 http://explainaboard.nlpedia.ai/ different steps of the research process: leaderboard/xtreme/ 6 (i) System Developement: E XPLAINA B OARD https://sites.research.google/xtreme/ 7 Since the first release of E XPLAINA B OARD, we have provides more detai"
2021.acl-demo.34,D15-1044,0,0.00960657,"B OARD currently supports. “Attr.” denotes Attribute. “Pred.” denotes “Prediction”. Structure Prediction Prediction of a syntactic or semantic structure from text, where E X PLAINA B OARD currently covers semantic parsing tasks (Berant et al., 2013; Yu et al., 2018). (Fu et al., 2020b; Schweter and Akbik, 2020; Yamada et al., 2020). Using these methods, we have gathered 74 models on six NER datasets with system output information. Text Generation E XPLAINA B OARD also considers text generation tasks, and currently mainly focuses on conditional text generation, for example, text summarization (Rush et al., 2015; Liu and Lapata, 2019) and machine translation . System outputs on text summarization are expanded based on the previous work’s collection (Bhandari et al., 2020) as well as recently state-of-the-art systems (Liu and Liu, 2021) while outputs from machine translation are collected from the WMT20.12 4.2 4 Case Study Here, we briefly showcase the actual E XPLAIN interface through a case study on analyzing state-of-the-art NER systems. A B OARD 4.1 Experimental Setup Attribute Definition We define attributes following Fu et al. (2020a) and three of them are used below: entity length, sentence len"
2021.acl-demo.34,2021.ccl-1.54,0,0.0296761,"Missing"
2021.acl-demo.34,P11-4010,0,0.0375117,"s that are applicable to a wide variety of NLP tasks (as illustrated in Tab. 1). Many of these functionalities are grounded in existing research on evaluation and fine-grained diagnostics. In this paper, we describe E XPLAINA B OARD (see Fig.1), a software package and hosted leaderboard that satisfies all of the above desiderata. It also serves as a prototype implementation of 2.1 Interpretability some desirable features that may be included in future leaderboards, even independent of the pro- Interpretable evaluation (Popovi´c and Ney, 2011; vided software itself. We have deployed E XPLAIN - Stymne, 2011; Neubig et al., 2019; Fu et al., 2020a), is a research area that considers methods that break A B OARD for 9 different tasks and 41 different down the holistic performance of each system into datasets, and demonstrate how it can be easily different interpretable groups. For example, in a adapted to new tasks of interest. We expect that E XPLAINA B OARD will benefit 5 http://explainaboard.nlpedia.ai/ different steps of the research process: leaderboard/xtreme/ 6 (i) System Developement: E XPLAINA B OARD https://sites.research.google/xtreme/ 7 Since the first release of E XPLAINA B OARD, we hav"
2021.acl-demo.34,2020.emnlp-demos.15,0,0.0830454,"Missing"
2021.acl-demo.34,W02-1011,0,0.0280969,"Missing"
2021.acl-demo.34,D14-1052,0,0.0634385,"Missing"
2021.acl-demo.34,N03-1033,0,0.0247044,"as unfortunately it is currently not standard in NLP to release the system outputs that E XPLAINA B OARD needs. 283 classification (Pang et al., 2002), topic identification (Wang and Manning, 2012), and intention detection (Chen et al., 2013). Task Text Pair Classification Prediction of a class given two texts, such as the natural language inference task (Bowman et al., 2015). Sequence Labeling Prediction of a label for each token in a sequence. The E XPLAINA B OARD currently includes four concrete tasks: named entity recognition (Tjong Kim Sang and De Meulder, 2003), part-of-speech tagging (Toutanova et al., 2003), text chunking (Ando and Zhang, 2005), and Chinese word segmentation (Chen et al., 2015). 8 40 2 Topic 4 18 2 Intention 1 3 2 Text-Span Classification Aspect Sentiment 4 20 4 Text Pair Classification NLI 2 6 7 NER 3 74 9 Text Classification Text-Span Classification Prediction of a predefined class from the input of a text and a span, such as aspect-based sentiment classification task (Pappas and Popescu-Belis, 2014). We collect topperform system outputs from (Dai et al., 2021). Data Model Attr. Sentiment Sequence Labeling Structure Pred. Text Generation POS 3 14 4 Chunking 3 14 9 CWS 7 64 7 S"
2021.acl-demo.34,J11-4002,0,0.102992,"Missing"
2021.acl-demo.34,D19-3002,0,0.055649,"Missing"
2021.acl-demo.34,D16-1264,0,0.0175544,"tion of the E XPLAINA B OARD concept. Compared to vanilla leaderboards, E XPLAINA B OARD allows users to perform interpretable (single-system , pairwise analysis, data bias), interactive (system combination, fine-grained/common error analysis), and reliable analysis (confidence interval, calibration) on systems in which they are interested. “Comb.” denotes “combination” and “Errs” represents “errors”. “PER, LOC, ORG” refer to different labels. This is true both for classical tasks such as machine translation (Sutskever et al., 2014; Wu et al., 2016), as well as for new tasks (Lu et al., 2016; Rajpurkar et al., 2016), domains (Beltagy et al., 2019), and languages (Conneau and Lample, 2019). One way this progress is quantified is through leaderboards, which report and update performance numbers 1 Introduction of state-of-the-art systems on one or more tasks. Some prototypical leaderboards include GLUE and Natural language processing (NLP) research has been and is making astounding strides forward. SuperGLUE for natural language understanding (Wang et al., 2018, 2019), XTREME and XGLUE 1 E XPLAINA B OARD keeps updated and is recently up(Hu et al., 2020; Liang et al., 2020) for multilingual graded by support"
2021.acl-demo.34,W18-5446,0,0.0728221,"Missing"
2021.acl-demo.34,P12-2018,0,0.141691,"Missing"
2021.acl-demo.34,2020.emnlp-main.523,0,0.187757,"outputs from (Dai et al., 2021). Data Model Attr. Sentiment Sequence Labeling Structure Pred. Text Generation POS 3 14 4 Chunking 3 14 9 CWS 7 64 7 Semantic Parsing 4 12 4 Summarization 2 36 7 Translation 4 60 9 Table 2: Brief descriptions of tasks, datasets and systems that E XPLAINA B OARD currently supports. “Attr.” denotes Attribute. “Pred.” denotes “Prediction”. Structure Prediction Prediction of a syntactic or semantic structure from text, where E X PLAINA B OARD currently covers semantic parsing tasks (Berant et al., 2013; Yu et al., 2018). (Fu et al., 2020b; Schweter and Akbik, 2020; Yamada et al., 2020). Using these methods, we have gathered 74 models on six NER datasets with system output information. Text Generation E XPLAINA B OARD also considers text generation tasks, and currently mainly focuses on conditional text generation, for example, text summarization (Rush et al., 2015; Liu and Lapata, 2019) and machine translation . System outputs on text summarization are expanded based on the previous work’s collection (Bhandari et al., 2020) as well as recently state-of-the-art systems (Liu and Liu, 2021) while outputs from machine translation are collected from the WMT20.12 4.2 4 Case Study"
2021.acl-demo.34,2021.eacl-main.324,1,0.883418,"ble interpretable evaluation. F6: Confidence Analysis: To what extent can we trust the results of our system? E XPLAIN A B OARD can perform confidence analysis over both holistic and fine-grained performance metrics. As shown in Tab. 1, for each bucket, there is an error bar whose width reflects how reliable the performance value is. We claim this is an important feature for fine-grained analysis since the numbers of test samples in each bucket are imbalanced, and with the confidence interval, one could know how much uncertainty there is. In practice, we use bootstrapping method (Efron, 1992; Ye et al., 2021) to calculate the confidence interval. F7: Calibration Analysis: How well is the confidence of prediction calibrated with its correctness? One commonly-cited issue with modern neural predictors is that their probability estimates are not accurate (i.e. they are poorly calibrated), often being over-confident in the correctness of their predictions (Guo et al., 2017). We also incorporate this feature into E XPLAINA B OARD, allowing users to evaluate how well-calibrated their systems of interest are. 3 Tasks, Datasets and Systems We have already added to E XPLAINA B OARD 12 NLP tasks, 50 datasets"
2021.acl-demo.34,D18-1425,0,0.0226747,"(Pappas and Popescu-Belis, 2014). We collect topperform system outputs from (Dai et al., 2021). Data Model Attr. Sentiment Sequence Labeling Structure Pred. Text Generation POS 3 14 4 Chunking 3 14 9 CWS 7 64 7 Semantic Parsing 4 12 4 Summarization 2 36 7 Translation 4 60 9 Table 2: Brief descriptions of tasks, datasets and systems that E XPLAINA B OARD currently supports. “Attr.” denotes Attribute. “Pred.” denotes “Prediction”. Structure Prediction Prediction of a syntactic or semantic structure from text, where E X PLAINA B OARD currently covers semantic parsing tasks (Berant et al., 2013; Yu et al., 2018). (Fu et al., 2020b; Schweter and Akbik, 2020; Yamada et al., 2020). Using these methods, we have gathered 74 models on six NER datasets with system output information. Text Generation E XPLAINA B OARD also considers text generation tasks, and currently mainly focuses on conditional text generation, for example, text summarization (Rush et al., 2015; Liu and Lapata, 2019) and machine translation . System outputs on text summarization are expanded based on the previous work’s collection (Bhandari et al., 2020) as well as recently state-of-the-art systems (Liu and Liu, 2021) while outputs from m"
2021.acl-demo.34,D19-5410,1,0.812592,"archers to interact with it, interpreting systems and datasets that they are interested in from different perspectives. (ii) We also release our back-end code for different NLP tasks so that researchers could flexibly use them to process their own system outputs, which can assist their research projects. Example Use-cases To show the practical utility of E XPLAINA B OARD, we first present examples of how it has been used as an analysis tool in existing published research papers. Fu et al. (2020b) (Tab.4) utilize single-system analysis with the attribute of label consistency for NER task while Zhong et al. (2019) (Tab.4-5) use it for Contributing to ExplainaBoard The commutext summarization with attributes of density nity can contribute to E XPLAINA B OARD in sevand compression. Fig.4 and Tab.3 in Fu et al. eral ways: (i) Submit system outputs of their im(2020a) leverage the data bias analysis and pairplemented models. (ii) Add more informative atwise system diagnostics to interpret top-performing tributes for different NLP tasks. (iii) Add new NER systems while Tab.4 in Fu et al. (2020c) use datasets or benchmarks for existing or new tasks. single and pairwise system analysis to investigate what’s ne"
2021.acl-long.558,C18-1139,0,0.107383,"T0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its ar"
2021.acl-long.558,Q17-1010,0,0.0119914,"coder. √ indicates the embedding/structure is utilized in the current S EQ L AB system. For example, “sq0” denotes a model that uses Flair, GloVe, LSTM, and CRF as the character-, word-level embedding, sentence-level encoder, and decoder, respectively. “–” indicates not applicable.8 Regarding S EQ L AB-base systems, following (Fu et al., 2020b), their designs are diverse in four components: (1) character/subword-sensitive representation: ELMo (Peters et al., 2018), Flair (Akbik et al., 2018, 2019), BERT 9 (Devlin et al., 2018) 2) word representation: GloVe (Pennington et al., 2014), fastText (Bojanowski et al., 2017); (3) sentence-level encoders: LSTM (Hochreiter and Schmidhuber, 1997), CNN (Kalchbrenner et al., 2014b; Chen et al., 2019); (4) decoders: CRF (Lample et al., 2016; Collobert et al., 2011). We keep the testing result from the model with the best performance on the development set, terminating training when the performance of the development set is not improved in 20 epochs. 5.2 Baselines We extensively explore six system combination methods as competitors, which involves supervised and unsupervised fashions. 5.2.1 Voting-based Approaches Voting, as an unsupervised method, has been commonly use"
2021.acl-long.558,W17-4418,0,0.0607179,"Missing"
2021.acl-long.558,N19-1423,0,0.0160104,"Missing"
2021.acl-long.558,2020.emnlp-main.393,0,0.427901,"Missing"
2021.acl-long.558,W03-0425,0,0.477173,"Missing"
2021.acl-long.558,2020.emnlp-main.489,1,0.612955,"Missing"
2021.acl-long.558,2020.emnlp-main.457,1,0.472097,"stem diagnosis. sqi represents different S EQ L AB systems. Each value in heatmap entry (i, j) represents the performance gap between S EQ L AB and S PAN N ER (F 1sq − F 1span ) on j-th bucket. The green area indicates S EQ L AB performs better while the red area implies S PAN N ER is better. eCon, sLen, eLen, and oDen represent different attributes. 3.3 Exp-II: Analysis of Complementarity The holistic results in Tab. 1 make it hard for us to interpret the relative advantages of NER systems with different structural biases. To address this problem, we follow the interpretable evaluation idea (Fu et al., 2020a,c) that proposes to breakdown the holistic performance into different buckets from different perspectives and use a performance heatmap to illustrate relative advantages between two systems, i.e., system-pair diagnosis. Setup As a comparison, we replicate five topscoring S EQ L AB-based NER systems, which are sq1 : 92.41, sq2 : 92.01, sq3 : 92.46, sq4 : 92.11, sq5 : 91.99. Notably, to make a fair comparison, all five S EQ L ABs are with closed performance comparing to the above S PAN N ERs. Although we will detail configurations of these systems later (to reduce content redundancy) in §5.1 T"
2021.acl-long.558,C18-1161,0,0.0172363,"n. Here, we only consider the English (EN) dataset collected from the Reuters Corpus. CoNLL-2002 3 (Sang, 2002) contains annotated corpus in Dutch (NL) collected from De Morgen news, and Spanish (ES) collected from Spanish EFE News Agency. We evaluate both languages. OntoNotes 5.0 4 (Weischedel et al., 2013) is a large corpus consisting of three different languages: English, Chinese, and Arabic, involving six genres: newswire (NW), broadcast news (BN), broadcast conversation (BC), magazine (MZ), web data (WB), and telephone conversation (TC). Following previous works (Durrett and Klein, 2014; Ghaddar and Langlais, 2018), we utilize different domains in English to test the robustness of proposed models. 1 http://www.clips.uantwerpen.be/ conll2000/chunking/conlleval.txt 2 https://www.clips.uantwerpen.be/ conll2003/ner/ 3 https://www.clips.uantwerpen.be/ conll2002/ner/ 4 https://catalog.ldc.upenn.edu/ LDC2013T19 7184 WNUT-2016 5 and WNUT-2017 6 (Strauss et al., 2016; Derczynski et al., 2017) are social media data from Twitter, which were public as a shared task at WNUT-2016 (W16) and WNUT-2017 (W17). 3 Span Prediction for NE Recognition Although this is not the first work that formulates NER as a span predictio"
2021.acl-long.558,P11-1127,0,0.709198,"Missing"
2021.acl-long.558,P08-1067,0,0.15142,"Missing"
2021.acl-long.558,2020.acl-main.192,0,0.0476544,"Missing"
2021.acl-long.558,P14-1062,0,0.659847,"oMK/71n/I="">AAAC0XicjVHLSsNAFD2Nr7a+qi7dBIsoLkriRpcFNy4rtQ9oa0nSaRuaF5OJUEpB3PoDbvWnxD/QpX/gnWkKahG9IcmZc+85M3euHXluLAzjNaMtLa+srmVz+fWNza3tws5uPQ4T7rCaE3ohb9pWzDw3YDXhCo81I84s3/ZYwx5dyHzjlvHYDYNrMY5Yx7cGgdt3HUsQddP2LTHk/qQ6jqdds1soGiVDhb4IzBQUy7mPI1BUwsIL2ughhIMEPhgCCMIeLMT0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgHdHJZ4&lt;/latexit> The rapid evolution of neural architectures (Kalchbrenner et al., 2014a; Kim, 2014; Hochreiter and Schmidhuber, 1997) and large pre-trained models (Devlin et al., 2019; Lewis et al., 2020) not only drive the state-of-the-art performance of many NLP tasks (Devlin et al., 2019; Liu and Lapata, 2019) to a new level but also change the way † This work is done when Jinlan visited CMU remotely. Corresponding author. SeqLab ... Sysm Sys1 Sys2 Figure 1: ONE span prediction model (S PAN NER) finishes TWO things: (1) named entity recognition (2) combination of different NER systems. Introduction ∗ SpanNer &lt;latexit sha1_base64=""CvJCMPVwdVTi+QwayXJ4kUndUbo="">AAAC0XicjVHLSsN"
2021.acl-long.558,P16-1101,0,0.143103,"VDhb4IzBQUy7mPI1BUwsIL2ughhIMEPhgCCMIeLMT0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explor"
2021.acl-long.558,2020.emnlp-main.514,0,0.122214,"NGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its architectural bias has not been fully understood so far. For example, what are the complementary advantages compared with S EQ L AB frameworks and how to make full use of them? Motivated by thi"
2021.acl-long.558,D14-1181,0,0.00548704,"FD2Nr7a+qi7dBIsoLkriRpcFNy4rtQ9oa0nSaRuaF5OJUEpB3PoDbvWnxD/QpX/gnWkKahG9IcmZc+85M3euHXluLAzjNaMtLa+srmVz+fWNza3tws5uPQ4T7rCaE3ohb9pWzDw3YDXhCo81I84s3/ZYwx5dyHzjlvHYDYNrMY5Yx7cGgdt3HUsQddP2LTHk/qQ6jqdds1soGiVDhb4IzBQUy7mPI1BUwsIL2ughhIMEPhgCCMIeLMT0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgHdHJZ4&lt;/latexit> The rapid evolution of neural architectures (Kalchbrenner et al., 2014a; Kim, 2014; Hochreiter and Schmidhuber, 1997) and large pre-trained models (Devlin et al., 2019; Lewis et al., 2020) not only drive the state-of-the-art performance of many NLP tasks (Devlin et al., 2019; Liu and Lapata, 2019) to a new level but also change the way † This work is done when Jinlan visited CMU remotely. Corresponding author. SeqLab ... Sysm Sys1 Sys2 Figure 1: ONE span prediction model (S PAN NER) finishes TWO things: (1) named entity recognition (2) combination of different NER systems. Introduction ∗ SpanNer &lt;latexit sha1_base64=""CvJCMPVwdVTi+QwayXJ4kUndUbo="">AAAC0XicjVHLSsNAFD3GV+uz6tJ"
2021.acl-long.558,N16-1030,0,0.718952,"sIL2ughhIMEPhgCCMIeLMT0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the und"
2021.acl-long.558,2020.acl-main.703,0,0.015985,"La+srmVz+fWNza3tws5uPQ4T7rCaE3ohb9pWzDw3YDXhCo81I84s3/ZYwx5dyHzjlvHYDYNrMY5Yx7cGgdt3HUsQddP2LTHk/qQ6jqdds1soGiVDhb4IzBQUy7mPI1BUwsIL2ughhIMEPhgCCMIeLMT0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgHdHJZ4&lt;/latexit> The rapid evolution of neural architectures (Kalchbrenner et al., 2014a; Kim, 2014; Hochreiter and Schmidhuber, 1997) and large pre-trained models (Devlin et al., 2019; Lewis et al., 2020) not only drive the state-of-the-art performance of many NLP tasks (Devlin et al., 2019; Liu and Lapata, 2019) to a new level but also change the way † This work is done when Jinlan visited CMU remotely. Corresponding author. SeqLab ... Sysm Sys1 Sys2 Figure 1: ONE span prediction model (S PAN NER) finishes TWO things: (1) named entity recognition (2) combination of different NER systems. Introduction ∗ SpanNer &lt;latexit sha1_base64=""CvJCMPVwdVTi+QwayXJ4kUndUbo="">AAAC0XicjVHLSsNAFD3GV+uz6tJNUBRxUZJudCm4cVmp1UJ9MImjDubFzEQsRRC3/oBb/SZB/AO79AfEO9MIPhCdkOTMufecmXtvkEVCac97HnAGh4ZHRkvlsfGJyanpyszsr"
2021.acl-long.558,2020.acl-main.519,0,0.190268,"5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its architectural bias has not been fully understood so far. For example, what are the complementary advantages compared with S EQ L AB frameworks and how to make full use of t"
2021.acl-long.558,2020.acl-main.752,0,0.0151927,"CgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its architectural bias has not been fully understood so far. For example, what are the complementary a"
2021.acl-long.558,2021.acl-demo.34,1,0.751707,"vantages of systems based on different paradigms. We then reveal that span prediction, simultaneously, can serve as a system combiner to re-recognize named entities from different systems’ outputs. We experimentally implement 154 systems on 11 datasets, covering three languages, comprehensive results show the effectiveness of span prediction models that both serve as base NER systems and system combiners. We make all code and datasets available: https:// github.com/neulab/spanner, as well as an online system demo: http://spanner. sh. Our model also has been deployed into the E XPLAINA B OARD (Liu et al., 2021) platform, which allows users to flexibly perform the system combination of top-scoring systems in an interactive way: http://explainaboard. nlpedia.ai/leaderboard/task-ner/. 1 &lt;latexit sha1_base64=""DZv0jzXJT2tUzVZoBGtnbFfy+Jg="">AAAC1XicjVHLSsNAFD2Nr1pfUZdugkXoqiTd6LLgxpVUtA+opSTTaQ1NkzCZFEvpTtz6A271l8Q/0L/wzpiCWkQnJDlz7j1n5t7rxYGfSNt+zRlLyyura/n1wsbm1vaOubvXSKJUMF5nURCJlucmPPBDXpe+DHgrFtwdeQFvesNTFW+OuUj8KLySk5h3Ru4g9Ps+cyVRXdO8lvxWJmx6GbvhORezrlm0y7Ze1iJwMlBEtmqR+YJr9BCBIcUIHCEk4QAuEnracGAjJq6DKXGCkK/jHDMUSJtSFqcMl9ghfQe0a2dsSHvlmWg1o1MCegUpLRyRJqI8QVidZul4qp0V+5v3VHuqu03o72VeI2Ilboj9SzfP/K9"
2021.acl-long.558,D19-1387,0,0.0209121,"GiVDhb4IzBQUy7mPI1BUwsIL2ughhIMEPhgCCMIeLMT0tGDCQERcBxPiOCFX5RmmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgHdHJZ4&lt;/latexit> The rapid evolution of neural architectures (Kalchbrenner et al., 2014a; Kim, 2014; Hochreiter and Schmidhuber, 1997) and large pre-trained models (Devlin et al., 2019; Lewis et al., 2020) not only drive the state-of-the-art performance of many NLP tasks (Devlin et al., 2019; Liu and Lapata, 2019) to a new level but also change the way † This work is done when Jinlan visited CMU remotely. Corresponding author. SeqLab ... Sysm Sys1 Sys2 Figure 1: ONE span prediction model (S PAN NER) finishes TWO things: (1) named entity recognition (2) combination of different NER systems. Introduction ∗ SpanNer &lt;latexit sha1_base64=""CvJCMPVwdVTi+QwayXJ4kUndUbo="">AAAC0XicjVHLSsNAFD3GV+uz6tJNUBRxUZJudCm4cVmp1UJ9MImjDubFzEQsRRC3/oBb/SZB/AO79AfEO9MIPhCdkOTMufecmXtvkEVCac97HnAGh4ZHRkvlsfGJyanpyszsrkpzGfJmmEapbAVM8UgkvKmFjngrk5zFQcT3gvNNE9+74FKJNNnRnYwfxOw0ESciZJqow/2Y6TMZdxsddXVUO6oselXPLvcn8AuwuFF+Xe5dPr7"
2021.acl-long.558,N16-1133,0,0.0621377,"Missing"
2021.acl-long.558,D14-1162,0,0.0874003,"nction to get the probability w.r.t label y. score(si , y) P(y|si ) = X , score(si , y0 ) S PAN N ER as NER System Overall, the span prediction-based framework for NER consists of three major modules: token representation layer, span representation layer, and span prediction layer. 3.1.1 Token Representation Layer Given a sentence X = {x1 , · · · , xn } with n tokens, the token representation hi is as follows: u1 , · · · , un = E MB(x1 , · · · , xn ), h1 , · · · , hn = B I LSTM(u1 , · · · , un ), (1) (2) where E MB(·) is the pre-trained embeddings, such as non-contextualized embeddings GloVe (Pennington et al., 2014) or contextualized pre-trained embeddings BERT (Devlin et al., 2018). B I LSTM is the bidirectional LSTM (Hochreiter and Schmidhuber, 1997). 3.1.2 Span Representation Layer First, we enumerate all the possible m spans S = {s1 , · · · , si , · · · , sm } for sentence X = {x1 , · · · , xn } and then re-assign a label y ∈ Y for each span s. For example, for sentence: “London1 is2 beautiful3 ”, the possible span’s (start, end) indices are {(1, 1), (2, 2), (3, 3), (1, 2), (2, 3), (1, 3)}, and the labels of these spans are all “O” except (1, 1) (London) is “LOC”. We use bi and ei to denote the start"
2021.acl-long.558,N18-1202,0,0.250669,"mmyJM2oSpGFRaxI/oOaNVK2YDW0jNWaod28ejlpNRxSJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its architectural bias has"
2021.acl-long.558,W03-0419,0,0.446523,"Missing"
2021.acl-long.558,W02-2024,0,0.279681,"t sequence and Y = {y1 , y2 , . . . , yT } is the output label (e.g., “B-PER”, “I-LOC”, “O”) sequence. The goal of this task is to accurately predict entities by assigning output label yt for each token xt . We take the F1-score1 as the evaluation metric for the NER task. 2.2 Datasets To make a comprehensive evaluation, in this paper, we use multiple NER datasets that cover different domains and languages. CoNLL-2003 2 (Sang and De Meulder, 2003) covers two different languages: English and German. Here, we only consider the English (EN) dataset collected from the Reuters Corpus. CoNLL-2002 3 (Sang, 2002) contains annotated corpus in Dutch (NL) collected from De Morgen news, and Spanish (ES) collected from Spanish EFE News Agency. We evaluate both languages. OntoNotes 5.0 4 (Weischedel et al., 2013) is a large corpus consisting of three different languages: English, Chinese, and Arabic, involving six genres: newswire (NW), broadcast news (BN), broadcast conversation (BC), magazine (MZ), web data (WB), and telephone conversation (TC). Following previous works (Durrett and Klein, 2014; Ghaddar and Langlais, 2018), we utilize different domains in English to test the robustness of proposed models."
2021.acl-long.558,P17-1076,0,0.0510281,"Missing"
2021.acl-long.558,P17-2060,0,0.0575205,"Missing"
2021.acl-long.558,P11-1125,0,0.0332102,"Missing"
2021.acl-long.558,W03-0433,0,0.512573,"l bias of the span prediction framework: it can not only be used as a base system for named entity recognition but also serve as a meta-system to combine multiple NER systems’ outputs. In other words, the span prediction model play two roles showing in Fig. 1: (i) as a base NER system; and (ii) as a system combiner of multiple base systems. We claim that compared with traditional ensemble learning of the NER task, S PAN N ER combiners are advantageous in the following aspects: 1. Most of the existing NER combiners rely on heavy feature engineering and external knowledge (Florian et al., 2003; Wu et al., 2003; Saha and Ekbal, 2013). Instead, the S PAN N ER models we proposed for system combination train in an end-to-end fashion. 2. Combining complementarities of different paradigms: most previous works perform NER system combination solely focusing on the sequence labeling framework. It is still an understudied topic how systems from different frameworks help each other. 3. No extra training overhead and flexibility of use: Existing ensemble learning algorithms are expensive, which usually need to collect training samples by k-fold cross-validation for system combiner (Speck and Ngomo, 2014), redu"
2021.acl-long.558,P19-1138,0,0.017933,"SJqQ6jhhuZuu8olyluxv3hPlKc82pr+devnECgyJ/Us3r/yvTvYi0Me56sGlniLFyO6c1CVRtyJPrn/pSpBDRJzEPcpzwo5Szu9ZV5pY9S7v1lL5N1UpWbl20toE7/KUNGDz5zgXQf20ZBol84omfYJZZLGPAxzTPM9QxiUqqJE3xyOe8KxVtbF2p93PSrVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its architectural bias has not been fully understood so far. For e"
2021.acl-long.558,2020.emnlp-main.523,0,0.405558,"Missing"
2021.acl-long.558,2020.acl-main.577,0,0.0647271,"rVMqtnDt9AePgFrq5a0&lt;/latexit> how researchers formulate the task. For example, recent years have seen frequent paradigm shifts for the task of named entity recognition (NER) from token-level tagging, which conceptualize NER as a sequence labeling (S EQ L AB) task (Chiu and Nichols, 2015; Huang et al., 2015; Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018; Peters et al., 2018; Devlin et al., 2018; Xia et al., 2019; Luo et al., 2020; Lin et al., 2020; Fu et al., 2021), to span-level prediction (S PAN N ER) (Li et al., 2020; Mengge et al., 2020; Jiang et al., 2020; Ouchi et al., 2020; Yu et al., 2020), which regards NER either as question answering (Li et al., 2020; Mengge et al., 2020), span classification (Jiang et al., 2020; Ouchi et al., 2020; Yamada et al., 2020), and dependency parsing tasks (Yu et al., 2020). However, despite the success of span predictionbased systems, as a relatively newly-explored framework, the understanding of its architectural bias has not been fully understood so far. For example, what are the complementary advantages compared with S EQ L AB frameworks and how to make full use of them? Motivated by this, in this paper, we make two scientific contributions. We"
2021.acl-long.59,S17-2091,0,0.0524295,"Missing"
2021.acl-long.59,D19-1371,0,0.022721,"n identification, saliency classification, and relation extraction) in a sequence, treating coreference resolution as an external black box. While word and span representations are shared across all tasks and updated to minimize multi-task loss, the model trains each task on gold input. Figure 2 summarizes the baseline model’s end-to-end architecture, and highlights the places where we propose improvements for our CitationIE model. Feature Extraction The model extracts features from raw text in two stages. First, contextualized word embeddings are obtained for each section by running SciBERT (Beltagy et al., 2019) on that section of text (up to 512 tokens). Then, the embeddings from all words over all sections are passed through a bidirectional LSTM (Graves et al., 2005) to contextualize each word’s representation with those from other sections. Mention Identification The baseline model treats this named entity recognition task as an IOBES sequence tagging problem (Reimers and Gurevych, 2017). The tagger takes the SciBERTBiLSTM (Beltagy et al., 2019; Graves et al., 2005) word embeddings (as shown in the Figure 2), feeds them through two feedforward networks (not shown in Figure 2), and produces tag pot"
2021.acl-long.59,D14-1150,0,0.180367,"and relation extraction. For each task, we consider two types of citation graph information, either separately or together: (1) structural information from the graph network topology and (2) textual information from the content of citing and cited documents. 4.1 Structural Information The structure of the citation graph can contextualize a document within the greater body of work. Prior works in scientific information extraction have predominantly used the citation graph only to analyze the content of citing papers, such as CiteTextRank (Das Gollapalli and Caragea, 2014) and Citation TF-IDF (Caragea et al., 2014), which is described in detail in §4.2.2. However, the citation graph can be used to discover relationships between non-adjacent documents in the citation graph; prior works struggle to capture these relationships. 722 Stage 1 Stage 2 Output Figure 4: Feedforward architecture in each task (with CitationIE-specific parameters shown in light blue). Arnold and Cohen (2009) are the only prior work, to our knowledge, to explicitly use the citation graph’s structure for scientific IE. They predict key entities related to a paper via random walks on a combined knowledge-and-citation-graph consisting"
2021.acl-long.59,W18-2501,0,0.0186975,"g end-to-end and per-task metrics. All metrics, except where stated otherwise, are the same as described by Jain et al. (2020). Mention Identification We evaluate mention identification with the average F1 score of classifying entities of each span type. Salient Entity Classification Similar to Jain et al. (2020) we evaluate this task at the mention level and cluster level. We evaluate both metrics on gold standard entity recognition inputs. Baselines 5.1.3 Training Details We build our proposed CitationIE methods on top of the SciREX repository7 (Jain et al., 2020) in the AllenNLP framework (Gardner et al., 2018). For each task, we first train that component in isolation from the rest of the system to minimize 7 724 https://github.com/allenai/SciREX Model F1 P size10 (66 samples) and inter-model variation. (3) Incorporating graph embeddings and citances simultaneously is no better than using either. (4) Our reimplemented baseline differs from the results reported by Jain et al. (2020) despite using their published code to train their model. This may be because we use a batch size of 4 (due to compute limits) while they reported a batch size of 50. R Salient Mention Evaluation Baseline (reported) Basel"
2021.acl-long.59,I11-1001,0,0.02721,"have experimented with such models on the Switchboard task […] ” for the scientific community to read this many papers in a time-critical situation, and make accurate judgements to help separate signal from the noise. To this end, how can machines help researchers quickly identify relevant papers? One step in this direction is to automatically extract and organize scientific information (e.g. important concepts and their relations) from a collection of research articles, which could help researchers identify new methods or materials for a given task. Scientific information extraction (SciIE) (Gupta and Manning, 2011; Yogatama et al., 2011), which aims to extract structured information from scientific articles, has seen growing interest recently, as reflected in the rapid evolution of systems and datasets (Luan et al., 2018; G´abor et al., 2018; Jain et al., 2020). Existing works on SciIE revolve around extraction solely based on the content of different parts of an individual paper, such as the abstract or conclusion (Augenstein et al., 2017; Luan et al., 2019). However, scientific papers do not exist in a vacuum — they are part of a larger ecosystem of papers, related to each other through different con"
2021.acl-long.59,P19-1513,0,0.105876,"Most work on scientific information extraction has used annotated datasets of scientific abstracts, such as those provided for SemEval 2017 and SemEval 2018 shared tasks (Augenstein et al., 2017; G´abor et al., 2018), the SciERC dataset (Luan et al., 2018), and the BioCreative V Chemical Disease Relation dataset (Wei et al., 2016). We focus on the task of open-domain documentlevel relation extraction from long, full-text documents. This is in contrast to the above methods that only use paper abstracts. Our setting is also different from works that consider a fixed set of candidate relations (Hou et al., 2019; Kardas et al., 2020) or those that only consider IE tasks other than relation extraction, such as entity recognition (Verspoor et al., 2011). We base our task definition and baseline models on the recently released SciREX dataset (Jain et al., 2020), which contains 438 annotated papers,3 all related to machine learning research. Each document consists of sections D = {S1 , . . . , SN }, where each section contains a sequence of words Si = {wi,1 , . . . , wi,Ni }. Each document comes with annotations of entities, coreference clusters, cluster-level saliency labels, and 4-ary document-level re"
2021.acl-long.59,2020.acl-main.670,0,0.0286505,"Missing"
2021.acl-long.59,2020.emnlp-main.692,0,0.0162033,"ntific information extraction has used annotated datasets of scientific abstracts, such as those provided for SemEval 2017 and SemEval 2018 shared tasks (Augenstein et al., 2017; G´abor et al., 2018), the SciERC dataset (Luan et al., 2018), and the BioCreative V Chemical Disease Relation dataset (Wei et al., 2016). We focus on the task of open-domain documentlevel relation extraction from long, full-text documents. This is in contrast to the above methods that only use paper abstracts. Our setting is also different from works that consider a fixed set of candidate relations (Hou et al., 2019; Kardas et al., 2020) or those that only consider IE tasks other than relation extraction, such as entity recognition (Verspoor et al., 2011). We base our task definition and baseline models on the recently released SciREX dataset (Jain et al., 2020), which contains 438 annotated papers,3 all related to machine learning research. Each document consists of sections D = {S1 , . . . , SN }, where each section contains a sequence of words Si = {wi,1 , . . . , wi,Ni }. Each document comes with annotations of entities, coreference clusters, cluster-level saliency labels, and 4-ary document-level relations. We break down"
2021.acl-long.59,W04-3250,0,0.057712,"metric. 5.1.2 For each task, we compare against Jain et al. (2020), whose architecture our system is built on. No other model to our knowledge performs all the tasks we consider on full documents. For the 4-ary relation extraction task, we also compare against the DocTAET model (Hou et al., 2019), which is considered as state-of-the-art for full-text scientific relation extraction (Jain et al., 2020; Hou et al., 2019). Significance To improve the rigor of our evaluation, we run significance tests for each of our proposed methods against its associated baseline, via paired bootstrap sampling (Koehn, 2004). In experiments where we trained multiple models with different seeds, we perform a hierarchical bootstrap procedure where we first sample a seed for each model and then sample a randomized test set. Metrics The ultimate product of our work is an end-to-end document-level relation extraction system, but we also measure each component of our system in isolation, giving end-to-end and per-task metrics. All metrics, except where stated otherwise, are the same as described by Jain et al. (2020). Mention Identification We evaluate mention identification with the average F1 score of classifying ent"
2021.acl-long.59,2020.acl-main.447,0,0.152798,"eraged over all sections and passed through another feedforward network which returns a binary prediction. 3 Citation-aware SciIE Dataset Although citation network information has been shown to be effective in other tasks, few works have recently tried using it in SciIE systems. One potential reason is the lack of a suitable dataset. Thus, as a first contribution of this paper, we address this bottleneck by constructing a SciIE dataset that is annotated with citation graph information.4 Specifically, we combine the rich annotations of SciREX with a source of citation graph information, S2ORC (Lo et al., 2020). For each paper, S2ORC includes parsed metadata about which other papers cite this paper, which other papers are 4 We have released code to construct this dataset: https: //github.com/viswavi/ScigraphIE 721 Input Document Title/Abstract Feature Extraction SciBERT BiLSTM Mention Identification Salient Entity Classification Span Embedding CRF Intro. (Part 1) FF “ASR"" FF FF Salient FF FF ✓ Metric: “WER” Relation: { Task: Method: Dataset: Metric: } ASR, RNN, Switchboard, WER, Task: Span Embedding Intro. (Part 2) “ASR"" FF FF B-METHOD L -METHOD O Recurrent nets with ... sigmoid activations “RNN” “S"
2021.acl-long.59,D18-1360,0,0.131786,"To this end, how can machines help researchers quickly identify relevant papers? One step in this direction is to automatically extract and organize scientific information (e.g. important concepts and their relations) from a collection of research articles, which could help researchers identify new methods or materials for a given task. Scientific information extraction (SciIE) (Gupta and Manning, 2011; Yogatama et al., 2011), which aims to extract structured information from scientific articles, has seen growing interest recently, as reflected in the rapid evolution of systems and datasets (Luan et al., 2018; G´abor et al., 2018; Jain et al., 2020). Existing works on SciIE revolve around extraction solely based on the content of different parts of an individual paper, such as the abstract or conclusion (Augenstein et al., 2017; Luan et al., 2019). However, scientific papers do not exist in a vacuum — they are part of a larger ecosystem of papers, related to each other through different conceptual relations. In this paper, we claim a better under719 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Languag"
2021.acl-long.59,N19-1308,0,0.0245759,"search articles, which could help researchers identify new methods or materials for a given task. Scientific information extraction (SciIE) (Gupta and Manning, 2011; Yogatama et al., 2011), which aims to extract structured information from scientific articles, has seen growing interest recently, as reflected in the rapid evolution of systems and datasets (Luan et al., 2018; G´abor et al., 2018; Jain et al., 2020). Existing works on SciIE revolve around extraction solely based on the content of different parts of an individual paper, such as the abstract or conclusion (Augenstein et al., 2017; Luan et al., 2019). However, scientific papers do not exist in a vacuum — they are part of a larger ecosystem of papers, related to each other through different conceptual relations. In this paper, we claim a better under719 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 719–731 August 1–6, 2021. ©2021 Association for Computational Linguistics standing of a research article relies not only on its content but also on its relations with associated works, using both the content of related pape"
2021.acl-long.59,2020.emnlp-main.687,0,0.0349169,"es with annotations of entities, coreference clusters, cluster-level saliency labels, and 4-ary document-level relations. We break down the end-to-end information extraction process as a sequence of these four related tasks, with each task taking the output of the preceding tasks as input. Mention Identification For each span of text within a section, this task aims to recognize if the span describes a Task, Dataset, Method, or Metric entity, if any. Coreference This task requires clustering all entity mentions in a document such that, in each cluster, every mention refers to the same entity (Varkel and Globerson, 2020). The SciREX dataset 2 Our proposed method actually makes correct predictions on both these samples, where the baseline model fails on both. 3 The dataset contains 306 documents for training, 66 for validation, and 66 for testing. 720 includes coreference annotations for each Task, Dataset, Method, and Metric mention. Salient Entity Classification Given a cluster of mentions corresponding to the same entity, the model must predict whether the entity is key to the work described in a paper. We follow the definition from the SciREX dataset (Jain et al., 2020), where an entity in a paper is deeme"
2021.acl-long.59,D11-1055,0,0.0748359,"Missing"
2021.acl-short.135,2021.naacl-main.384,1,0.776364,"R-1/2/L) as the main evaluation metrics for our experiments. We also evaluate our model on the recently developed semantic similarity metrics, namely, BERTScore (Zhang et al., 2020b) and MoverScore (Zhao et al., 2019). 3.3 R-1 ROUGE-1 3 System 4 8 12 16 Number of Test Candidates Figure 2: Test performance with different numbers of candidate summaries on CNNDM. Origin denotes the original performance of the baseline model. 3.5 Results on CNNDM dataset The results on CNNDM dataset are shown in Tab. 1. We use the pretrained BART5 as the base generation model (Origin). We use BART, Pegasus, GSum (Dou et al., 2021) and ProphetNet (Qi et al., 2020) for comparison. Notably, the Max oracle which always selects the best candidate has much better performance than the original outputs, suggesting that using a diverse sampling strategy can further exploit the potential power of the pretrained abstractive system. Apart from ROUGE, we also present the evaluation results on semantic similarity metrics. Our method is able to outperform the baseline model on all metrics, demonstrating its improvement is beyond exploiting the potential artifacts of ROUGE. While the scale of improvement is harder to interpret with th"
2021.acl-short.135,N18-1033,0,0.0182781,"un1065 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 1065–1072 August 1–6, 2021. ©2021 Association for Computational Linguistics stable and sensitive to hyper-parameters. Minimum risk training, as an alternative, has also been used in the language generation tasks (Shen et al., 2016; Wieting et al., 2019). However, the accuracy of the estimated loss is restricted by the number of sampled outputs. Other methods (Wiseman and Rush, 2016; Norouzi et al., 2016; Edunov et al., 2018) aim to extend the framework of MLE to incorporate sentence-level scores into the objective functions. While these methods can mitigate the limitations of MLE training, the relation between the evaluation metrics and the objective functions used in their methods can be indirect and implicit. Among this background, in this work we generalize the paradigm of contrastive learning (Chopra et al., 2005) to introduce an approach for abstractive summarization which achieves the goal of directly optimizing the model with the corresponding evaluation metrics, thereby mitigating the gaps between trainin"
2021.acl-short.135,2020.findings-emnlp.287,0,0.50301,"roach for abstractive summarization which achieves the goal of directly optimizing the model with the corresponding evaluation metrics, thereby mitigating the gaps between training and test stages in MLE training. While some related work (Lee et al., 2021; Pan et al., 2021) have proposed to introduce a contrastive loss as an augmentation of MLE training for conditional text generation tasks, we instead choose to disentangle the functions of contrastive loss and MLE loss by introducing them at different stages in our proposed framework. Specifically, inspired by the recent work of Zhong et al. (2020); Liu et al. (2021b) on text summarization, we propose to use a two-stage model for abstractive summarization, where a Seq2Seq model is first trained to generate candidate summaries with MLE loss, and then a parameterized evaluation model is trained to rank the generated candidates with contrastive learning. By optimizing the generation model and evaluation model at separate stages, we are able to train these two modules with supervised learning, bypassing the challenging and intricate optimization process of the RL-based methods. Our main contribution in this work is to approach metric-orient"
2021.acl-short.135,2020.acl-main.703,0,0.287532,"while empirically powerful framework for abstractive summarization, S IM CLS, which can bridge the gap between the learning objective and evaluation metrics resulting from the currently dominated sequence-to-sequence learning framework by formulating text generation as a reference-free evaluation problem (i.e., quality estimation) assisted by contrastive learning. Experimental results show that, with minor modification over existing topscoring systems, SimCLS can improve the performance of existing top-performing models by a large margin. Particularly, 2.51 absolute improvement against BART (Lewis et al., 2020) and 2.50 over PEGASUS (Zhang et al., 2020a) w.r.t ROUGE-1 on the CNN/DailyMail dataset, driving the state-of-the-art performance to a new level. We have open-sourced our codes and results: https://github. com/yixinL7/SimCLS. Results of our proposed models have been deployed into E X PLAINA B OARD (Liu et al., 2021a) platform, which allows researchers to understand our systems in a more fine-grained way. 1 Figure 1: SimCLS framework for two-stage abstractive summarization, where Doc, S, Ref represent the document, generated summary and reference respectively. At the first stage, a Seq2Seq gene"
2021.acl-short.135,D19-1623,0,0.0182698,"e function and the evaluation metrics, as the objective function is based on local, token-level predictions while the evaluation metrics (e.g. ROUGE (Lin, 2004)) would compare the holistic similarity between the gold references and system outputs. Furthermore, during the test stage the model needs to generate outputs autoregressivelly, which means the errors made in the previous steps will accumulate. This gap between the training and test has been referred to as the exposure bias in the previous work (Bengio et al., 2015; Ranzato et al., 2016). A main line of approaches (Paulus et al., 2018; Li et al., 2019) proposes to use the paradigm of Reinforcement Learning (RL) to mitigate the aforementioned gaps. While RL training makes it possible to train the model with rewards based on global predictions and closely related to the evaluation metrics, it introduces the common challenges of deep RL. Specifically, RL-based training suffers from the noise gradient estimation (Greensmith et al., 2004) problem, which often makes the training un1065 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
2021.acl-short.135,W04-1013,0,0.129112,"is et al., 2020; Zhang et al., 2020a) have shown promising potentials in the summarization task, they share the widely acknowledged challenges of Seq2Seq model training. Specifically, Seq2Seq models are usually trained under the framework of Maximum Likelihood Estimation (MLE) and in practice they are commonly trained with the teacher-forcing (Williams and ∗ Corresponding author. Zipser, 1989) algorithm. This introduces a gap between the objective function and the evaluation metrics, as the objective function is based on local, token-level predictions while the evaluation metrics (e.g. ROUGE (Lin, 2004)) would compare the holistic similarity between the gold references and system outputs. Furthermore, during the test stage the model needs to generate outputs autoregressivelly, which means the errors made in the previous steps will accumulate. This gap between the training and test has been referred to as the exposure bias in the previous work (Bengio et al., 2015; Ranzato et al., 2016). A main line of approaches (Paulus et al., 2018; Li et al., 2019) proposes to use the paradigm of Reinforcement Learning (RL) to mitigate the aforementioned gaps. While RL training makes it possible to train t"
2021.acl-short.135,2021.acl-demo.34,1,0.881677,"ty estimation) assisted by contrastive learning. Experimental results show that, with minor modification over existing topscoring systems, SimCLS can improve the performance of existing top-performing models by a large margin. Particularly, 2.51 absolute improvement against BART (Lewis et al., 2020) and 2.50 over PEGASUS (Zhang et al., 2020a) w.r.t ROUGE-1 on the CNN/DailyMail dataset, driving the state-of-the-art performance to a new level. We have open-sourced our codes and results: https://github. com/yixinL7/SimCLS. Results of our proposed models have been deployed into E X PLAINA B OARD (Liu et al., 2021a) platform, which allows researchers to understand our systems in a more fine-grained way. 1 Figure 1: SimCLS framework for two-stage abstractive summarization, where Doc, S, Ref represent the document, generated summary and reference respectively. At the first stage, a Seq2Seq generator (BART) is used to generate candidate summaries. At the second stage, a scoring model (RoBERTa) is used to predict the performance of the candidate summaries based on the source document. The scoring model is trained with contrastive learning, where the training examples are provided by the Seq2Seq model. Intr"
2021.acl-short.135,2021.ccl-1.108,0,0.0951945,"Missing"
2021.acl-short.135,2021.naacl-main.113,1,0.799301,"ty estimation) assisted by contrastive learning. Experimental results show that, with minor modification over existing topscoring systems, SimCLS can improve the performance of existing top-performing models by a large margin. Particularly, 2.51 absolute improvement against BART (Lewis et al., 2020) and 2.50 over PEGASUS (Zhang et al., 2020a) w.r.t ROUGE-1 on the CNN/DailyMail dataset, driving the state-of-the-art performance to a new level. We have open-sourced our codes and results: https://github. com/yixinL7/SimCLS. Results of our proposed models have been deployed into E X PLAINA B OARD (Liu et al., 2021a) platform, which allows researchers to understand our systems in a more fine-grained way. 1 Figure 1: SimCLS framework for two-stage abstractive summarization, where Doc, S, Ref represent the document, generated summary and reference respectively. At the first stage, a Seq2Seq generator (BART) is used to generate candidate summaries. At the second stage, a scoring model (RoBERTa) is used to predict the performance of the candidate summaries based on the source document. The scoring model is trained with contrastive learning, where the training examples are provided by the Seq2Seq model. Intr"
2021.acl-short.135,2020.acl-main.670,0,0.0613704,"Missing"
2021.acl-short.135,K16-1028,0,0.0703323,"Missing"
2021.acl-short.135,D18-1206,0,0.0345906,") − h(D, S)) i + XX i j>i max(0, h(D, S˜j ) − h(D, S˜i ) + λij ), (2) where S˜1 , · · · , S˜n is descendingly sorted by ˆ Here, λij = (j −i)∗λ is the correspondM (S˜i , S). ing margin that we defined following Zhong et al. (2020), and λ is a hyper-parameter.1 M can be any automated evaluation metrics or human judgments and here we use ROUGE (Lin, 2004). Experiments 3.1 Datasets We use two datasets for our experiments. The dataset statistics are listed in Appendix A. CNNDM CNN/DailyMail2 (Hermann et al., 2015; Nallapati et al., 2016) dataset is a large scale news articles dataset. XSum XSum3 (Narayan et al., 2018) dataset is a highly abstractive dataset containing online articles from the British Broadcasting Corporation (BBC). 3.2 Training Details As it is insensitive, we fix it to 0.01 in our experiments. https://cs.nyu.edu/˜kcho/DMQA/ 3 https://github.com/EdinburghNLP/XSum 4 https://github.com/huggingface/ transformers 2 BS MS BART* Pegasus* Prophet* GSum* 44.16 44.17 44.20 45.94 21.28 21.47 21.17 22.32 40.90 41.11 41.30 42.48 - - Origin Min Max Random 44.39 33.17 54.36 43.98 21.21 11.67 28.73 20.06 41.28 30.77 50.77 40.94 64.67 58.09 70.77 64.65 58.67 55.75 61.67 58.60 SimCLS 46.67† 22.15† 43.54† 6"
2021.acl-short.135,2021.acl-long.21,0,0.0425488,"the objective functions. While these methods can mitigate the limitations of MLE training, the relation between the evaluation metrics and the objective functions used in their methods can be indirect and implicit. Among this background, in this work we generalize the paradigm of contrastive learning (Chopra et al., 2005) to introduce an approach for abstractive summarization which achieves the goal of directly optimizing the model with the corresponding evaluation metrics, thereby mitigating the gaps between training and test stages in MLE training. While some related work (Lee et al., 2021; Pan et al., 2021) have proposed to introduce a contrastive loss as an augmentation of MLE training for conditional text generation tasks, we instead choose to disentangle the functions of contrastive loss and MLE loss by introducing them at different stages in our proposed framework. Specifically, inspired by the recent work of Zhong et al. (2020); Liu et al. (2021b) on text summarization, we propose to use a two-stage model for abstractive summarization, where a Seq2Seq model is first trained to generate candidate summaries with MLE loss, and then a parameterized evaluation model is trained to rank the genera"
2021.acl-short.135,2020.findings-emnlp.217,0,0.0161525,"trics for our experiments. We also evaluate our model on the recently developed semantic similarity metrics, namely, BERTScore (Zhang et al., 2020b) and MoverScore (Zhao et al., 2019). 3.3 R-1 ROUGE-1 3 System 4 8 12 16 Number of Test Candidates Figure 2: Test performance with different numbers of candidate summaries on CNNDM. Origin denotes the original performance of the baseline model. 3.5 Results on CNNDM dataset The results on CNNDM dataset are shown in Tab. 1. We use the pretrained BART5 as the base generation model (Origin). We use BART, Pegasus, GSum (Dou et al., 2021) and ProphetNet (Qi et al., 2020) for comparison. Notably, the Max oracle which always selects the best candidate has much better performance than the original outputs, suggesting that using a diverse sampling strategy can further exploit the potential power of the pretrained abstractive system. Apart from ROUGE, we also present the evaluation results on semantic similarity metrics. Our method is able to outperform the baseline model on all metrics, demonstrating its improvement is beyond exploiting the potential artifacts of ROUGE. While the scale of improvement is harder to interpret with these metrics, we note that the imp"
2021.acl-short.135,P16-1159,0,0.0245419,"rics, it introduces the common challenges of deep RL. Specifically, RL-based training suffers from the noise gradient estimation (Greensmith et al., 2004) problem, which often makes the training un1065 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 1065–1072 August 1–6, 2021. ©2021 Association for Computational Linguistics stable and sensitive to hyper-parameters. Minimum risk training, as an alternative, has also been used in the language generation tasks (Shen et al., 2016; Wieting et al., 2019). However, the accuracy of the estimated loss is restricted by the number of sampled outputs. Other methods (Wiseman and Rush, 2016; Norouzi et al., 2016; Edunov et al., 2018) aim to extend the framework of MLE to incorporate sentence-level scores into the objective functions. While these methods can mitigate the limitations of MLE training, the relation between the evaluation metrics and the objective functions used in their methods can be indirect and implicit. Among this background, in this work we generalize the paradigm of contrastive learning (Chopra et al., 2005)"
2021.acl-short.135,D16-1137,0,0.0697122,"Missing"
2021.acl-short.135,2020.emnlp-main.294,0,0.073682,"Missing"
2021.acl-short.135,P19-1427,0,0.0194788,"the common challenges of deep RL. Specifically, RL-based training suffers from the noise gradient estimation (Greensmith et al., 2004) problem, which often makes the training un1065 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 1065–1072 August 1–6, 2021. ©2021 Association for Computational Linguistics stable and sensitive to hyper-parameters. Minimum risk training, as an alternative, has also been used in the language generation tasks (Shen et al., 2016; Wieting et al., 2019). However, the accuracy of the estimated loss is restricted by the number of sampled outputs. Other methods (Wiseman and Rush, 2016; Norouzi et al., 2016; Edunov et al., 2018) aim to extend the framework of MLE to incorporate sentence-level scores into the objective functions. While these methods can mitigate the limitations of MLE training, the relation between the evaluation metrics and the objective functions used in their methods can be indirect and implicit. Among this background, in this work we generalize the paradigm of contrastive learning (Chopra et al., 2005) to introduce an approac"
2021.acl-short.135,D19-1053,0,0.0285551,"model and the evaluation model in our two-stage framework are trained separately, we use pre-trained state-of-the-art abstractive summarization systems as our generation model. Specifically, we use BART (Lewis et al., 2020) and Pegasus (Zhang et al., 2020a) as they are popular and have been comprehensively evaluated. 3.4 R-2 Table 1: Results on CNNDM. BS denotes BERTScore, MS deWe use ROUGE-1/2/L (R-1/2/L) as the main evaluation metrics for our experiments. We also evaluate our model on the recently developed semantic similarity metrics, namely, BERTScore (Zhang et al., 2020b) and MoverScore (Zhao et al., 2019). 3.3 R-1 ROUGE-1 3 System 4 8 12 16 Number of Test Candidates Figure 2: Test performance with different numbers of candidate summaries on CNNDM. Origin denotes the original performance of the baseline model. 3.5 Results on CNNDM dataset The results on CNNDM dataset are shown in Tab. 1. We use the pretrained BART5 as the base generation model (Origin). We use BART, Pegasus, GSum (Dou et al., 2021) and ProphetNet (Qi et al., 2020) for comparison. Notably, the Max oracle which always selects the best candidate has much better performance than the original outputs, suggesting that using a diverse"
2021.acl-short.135,2020.acl-main.552,1,0.923631,"roduce an approach for abstractive summarization which achieves the goal of directly optimizing the model with the corresponding evaluation metrics, thereby mitigating the gaps between training and test stages in MLE training. While some related work (Lee et al., 2021; Pan et al., 2021) have proposed to introduce a contrastive loss as an augmentation of MLE training for conditional text generation tasks, we instead choose to disentangle the functions of contrastive loss and MLE loss by introducing them at different stages in our proposed framework. Specifically, inspired by the recent work of Zhong et al. (2020); Liu et al. (2021b) on text summarization, we propose to use a two-stage model for abstractive summarization, where a Seq2Seq model is first trained to generate candidate summaries with MLE loss, and then a parameterized evaluation model is trained to rank the generated candidates with contrastive learning. By optimizing the generation model and evaluation model at separate stages, we are able to train these two modules with supervised learning, bypassing the challenging and intricate optimization process of the RL-based methods. Our main contribution in this work is to approach metric-orient"
2021.eacl-main.324,C18-1139,0,0.0126384,"length), (ii) and calculate performance (e.g., F1 score) for each bucket. Therefore, data-wise (ΦDts ), the input of performance prediction function in Eq. 4 (gf ine (·)) can be featurized as different types of (i) buckets (ii) aspects (iii) datasets. Additionally, we take (iv) different types of models as input. We present brief descriptions of the above four types of features. 1. Models: We choose 12 models for the NER task and 8 models for the Chinese Word Segmentation (CWS) task. The models are built by choosing the different character encoder (e.g., ELMo (Peters et al., 2018) and Flair (Akbik et al., 2018; Akbik et al.)), word embedding (e.g., GloVe (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013b)), sentence-level encoder (e.g., LSTM (Hochreiter and Schmidhuber, 1997) and CNN (Kalchbrenner et al., 2014)), and decoder (e.g., MLP and CRF (Lample et al., 2016; Collobert et al., 2011)). 2. Datasets: We consider 6 (5) datasets for the NER (CWS) task, detailed in appendix. 3. Attributes: We consider the interpretable evaluation aspects proposed in works (Fu et al., 2020a). We consider 9 attributes for the NER task and 8 attributes for the CWS task in this paper (e.g, entity length and"
2021.eacl-main.324,D08-1078,0,0.0310694,"learning system’s performance based on features of the underlying problem, dataset, or learning algorithm. While this topic is still relatively unexplored in the NLP context, there are a few examples of predicting performance as: (i) a function of training or model parameters for determining the number of training iterations (Kolachina et al., 2012) or value of hyperparameters (Rosenfeld et al., 2019) and identifying and terminating bad training runs (Domhan et al., 2015). (ii) a function of dataset characteristics to illustrate which factors are significant predictors of system performance (Birch et al., 2008; Turchi et al., 2008), or find a subset of representative experiments to run in order to obtain plausible predictions (Xia et al., 2020). In this paper, we ask two research questions with respect to performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However,"
2021.eacl-main.324,P18-1128,0,0.020658,"nsor can be conceived as a superposition of low-rank components and a sparse component, Robust PCA attempts to recover the low-rank and sparse components. The sparse components can be considered as the gross, but sparse noise in the dataset. 4 Statistical Preliminaries Before going into our second contribution to establishing reliability of performance prediction, we describe two relevant concepts from statistics. 4.1 Confidence Interval (CI) The confidence interval (CI) is a range of possible values for an unknown parameter associated with a confidence level of γ (Nakagawa and Cuthill, 2007; Dror et al., 2018) that the actual parameter can fall into the suggested range. Specifically, suppose that we are interested in estimating the underlying true parameter of ω. Given an observed parameter estimate of ω ˆ , obtained from the data, we aim to compute an interval with a confidence level γ that ω lies in an interval CI. Commonly, there are two approaches to calculate confidence intervals, depending on our knowledge about the distribution of the statistics of interest. When an analytical form exists and we have reasonable assumptions on the distribution, we can employ the normal theory or use Student’s"
2021.eacl-main.324,2020.emnlp-main.489,1,0.832238,"Missing"
2021.eacl-main.324,P14-1062,0,0.0564615,"of (i) buckets (ii) aspects (iii) datasets. Additionally, we take (iv) different types of models as input. We present brief descriptions of the above four types of features. 1. Models: We choose 12 models for the NER task and 8 models for the Chinese Word Segmentation (CWS) task. The models are built by choosing the different character encoder (e.g., ELMo (Peters et al., 2018) and Flair (Akbik et al., 2018; Akbik et al.)), word embedding (e.g., GloVe (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013b)), sentence-level encoder (e.g., LSTM (Hochreiter and Schmidhuber, 1997) and CNN (Kalchbrenner et al., 2014)), and decoder (e.g., MLP and CRF (Lample et al., 2016; Collobert et al., 2011)). 2. Datasets: We consider 6 (5) datasets for the NER (CWS) task, detailed in appendix. 3. Attributes: We consider the interpretable evaluation aspects proposed in works (Fu et al., 2020a). We consider 9 attributes for the NER task and 8 attributes for the CWS task in this paper (e.g, entity length and sentence length). 4. Buckets: The test entities (words) of the NER (CWS) task are partitioned into four buckets according to their attribute value. We compute the F1 score for the entities. 3 Parameterized Regression"
2021.eacl-main.324,W04-3250,0,0.279377,"h replacement from a distribution that approximates it, thereby allowing us to make inferences about the statistics of interest and construct confidence intervals. Common methods to construct the CI with bootstrap include the percentile method, where after specifying a confidence level γ, we take the range of points that cover the middle γ proportion of bootstrap sampling distribution Yˆ as the desired confidence interval, represented by (QYˆ ((1−γ)/2) , QYˆ ((1+γ)/2) ), where Q denotes the quantile. Works on establishing confidence for results in NLP tasks using this bootstrap method include Koehn (2004) and Li et al. (2017). acc(Bm ) = Model Calibration (MC) Calibration (Gleser, 1996), also known as reliability, refers to the ability of a model to make good probabilistic predictions. For a discrete distribution over events, a model is said to be wellcalibrated if for those events that the model assigns a probability of p, the long-run proportion that the event actually occurs turns out to be p. For example, if a weather forecast model predicts that there is a 0.1 probability of rain at 7 a.m., then when observed on a large number of random trials at 7 a.m., the model is well-calibrated if 0."
2021.eacl-main.324,P12-1003,0,0.0210698,"prediction model. Gray and red lines represent corresponding confidence intervals. Numbers in each bar indicate the number of test samples in each length bucket. //github.com/neulab/Reliable-NLPPP 1 3206 Introduction Performance prediction (P2 ) aims to predict a machine learning system’s performance based on features of the underlying problem, dataset, or learning algorithm. While this topic is still relatively unexplored in the NLP context, there are a few examples of predicting performance as: (i) a function of training or model parameters for determining the number of training iterations (Kolachina et al., 2012) or value of hyperparameters (Rosenfeld et al., 2019) and identifying and terminating bad training runs (Domhan et al., 2015). (ii) a function of dataset characteristics to illustrate which factors are significant predictors of system performance (Birch et al., 2008; Turchi et al., 2008), or find a subset of representative experiments to run in order to obtain plausible predictions (Xia et al., 2020). In this paper, we ask two research questions with respect to performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance pr"
2021.eacl-main.324,P02-1040,0,0.112252,"significant predictors of system performance (Birch et al., 2008; Turchi et al., 2008), or find a subset of representative experiments to run in order to obtain plausible predictions (Xia et al., 2020). In this paper, we ask two research questions with respect to performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However, from a perspective of understanding the workings of our models, work on model analysis has demonstrated the need for more fine-grained analysis over a wide variety of tasks (Kummerfeld et al., 2012; Kummerfeld and Klein, 2013; Karpathy et al., 2015; Neubig et al., 2019; Fu et al., 2020a,b,c). These methods calculate separate accuracy scores for different types of examples (e.g. accuracies for entity recognition by entity length). Our first contribution is to examine experimental settings where we predict these fine-grained evaluation scores (§2.3), and"
2021.eacl-main.324,D14-1162,0,0.0900421,"h bucket. Therefore, data-wise (ΦDts ), the input of performance prediction function in Eq. 4 (gf ine (·)) can be featurized as different types of (i) buckets (ii) aspects (iii) datasets. Additionally, we take (iv) different types of models as input. We present brief descriptions of the above four types of features. 1. Models: We choose 12 models for the NER task and 8 models for the Chinese Word Segmentation (CWS) task. The models are built by choosing the different character encoder (e.g., ELMo (Peters et al., 2018) and Flair (Akbik et al., 2018; Akbik et al.)), word embedding (e.g., GloVe (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013b)), sentence-level encoder (e.g., LSTM (Hochreiter and Schmidhuber, 1997) and CNN (Kalchbrenner et al., 2014)), and decoder (e.g., MLP and CRF (Lample et al., 2016; Collobert et al., 2011)). 2. Datasets: We consider 6 (5) datasets for the NER (CWS) task, detailed in appendix. 3. Attributes: We consider the interpretable evaluation aspects proposed in works (Fu et al., 2020a). We consider 9 attributes for the NER task and 8 attributes for the CWS task in this paper (e.g, entity length and sentence length). 4. Buckets: The test entities (words) of the NER (CWS"
2021.eacl-main.324,N18-1202,0,0.0350715,"n a certain aspect (e.g., entity length), (ii) and calculate performance (e.g., F1 score) for each bucket. Therefore, data-wise (ΦDts ), the input of performance prediction function in Eq. 4 (gf ine (·)) can be featurized as different types of (i) buckets (ii) aspects (iii) datasets. Additionally, we take (iv) different types of models as input. We present brief descriptions of the above four types of features. 1. Models: We choose 12 models for the NER task and 8 models for the Chinese Word Segmentation (CWS) task. The models are built by choosing the different character encoder (e.g., ELMo (Peters et al., 2018) and Flair (Akbik et al., 2018; Akbik et al.)), word embedding (e.g., GloVe (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013b)), sentence-level encoder (e.g., LSTM (Hochreiter and Schmidhuber, 1997) and CNN (Kalchbrenner et al., 2014)), and decoder (e.g., MLP and CRF (Lample et al., 2016; Collobert et al., 2011)). 2. Datasets: We consider 6 (5) datasets for the NER (CWS) task, detailed in appendix. 3. Attributes: We consider the interpretable evaluation aspects proposed in works (Fu et al., 2020a). We consider 9 attributes for the NER task and 8 attributes for the CWS task in this"
2021.eacl-main.324,D12-1096,0,0.0266754,"questions with respect to performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However, from a perspective of understanding the workings of our models, work on model analysis has demonstrated the need for more fine-grained analysis over a wide variety of tasks (Kummerfeld et al., 2012; Kummerfeld and Klein, 2013; Karpathy et al., 2015; Neubig et al., 2019; Fu et al., 2020a,b,c). These methods calculate separate accuracy scores for different types of examples (e.g. accuracies for entity recognition by entity length). Our first contribution is to examine experimental settings where we predict these fine-grained evaluation scores (§2.3), and also propose performance prediction methods particularly suited to this fine-grained evaluation setting (§3). Our second contribution is the development of methods for estimating the reliability of performance predictions. While allowing"
2021.eacl-main.324,W09-1119,0,0.0978428,"istics to illustrate which factors are significant predictors of system performance (Birch et al., 2008; Turchi et al., 2008), or find a subset of representative experiments to run in order to obtain plausible predictions (Xia et al., 2020). In this paper, we ask two research questions with respect to performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However, from a perspective of understanding the workings of our models, work on model analysis has demonstrated the need for more fine-grained analysis over a wide variety of tasks (Kummerfeld et al., 2012; Kummerfeld and Klein, 2013; Karpathy et al., 2015; Neubig et al., 2019; Fu et al., 2020a,b,c). These methods calculate separate accuracy scores for different types of examples (e.g. accuracies for entity recognition by entity length). Our first contribution is to examine experimental settings where we predict these fine"
2021.eacl-main.324,D13-1027,0,0.0148,"performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However, from a perspective of understanding the workings of our models, work on model analysis has demonstrated the need for more fine-grained analysis over a wide variety of tasks (Kummerfeld et al., 2012; Kummerfeld and Klein, 2013; Karpathy et al., 2015; Neubig et al., 2019; Fu et al., 2020a,b,c). These methods calculate separate accuracy scores for different types of examples (e.g. accuracies for entity recognition by entity length). Our first contribution is to examine experimental settings where we predict these fine-grained evaluation scores (§2.3), and also propose performance prediction methods particularly suited to this fine-grained evaluation setting (§3). Our second contribution is the development of methods for estimating the reliability of performance predictions. While allowing estimation 3703 Proceedings"
2021.eacl-main.324,N16-1030,0,0.0188411,"take (iv) different types of models as input. We present brief descriptions of the above four types of features. 1. Models: We choose 12 models for the NER task and 8 models for the Chinese Word Segmentation (CWS) task. The models are built by choosing the different character encoder (e.g., ELMo (Peters et al., 2018) and Flair (Akbik et al., 2018; Akbik et al.)), word embedding (e.g., GloVe (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013b)), sentence-level encoder (e.g., LSTM (Hochreiter and Schmidhuber, 1997) and CNN (Kalchbrenner et al., 2014)), and decoder (e.g., MLP and CRF (Lample et al., 2016; Collobert et al., 2011)). 2. Datasets: We consider 6 (5) datasets for the NER (CWS) task, detailed in appendix. 3. Attributes: We consider the interpretable evaluation aspects proposed in works (Fu et al., 2020a). We consider 9 attributes for the NER task and 8 attributes for the CWS task in this paper (e.g, entity length and sentence length). 4. Buckets: The test entities (words) of the NER (CWS) task are partitioned into four buckets according to their attribute value. We compute the F1 score for the entities. 3 Parameterized Regression Functions The performance prediction model takes in a"
2021.eacl-main.324,P17-1064,0,0.0115779,"m a distribution that approximates it, thereby allowing us to make inferences about the statistics of interest and construct confidence intervals. Common methods to construct the CI with bootstrap include the percentile method, where after specifying a confidence level γ, we take the range of points that cover the middle γ proportion of bootstrap sampling distribution Yˆ as the desired confidence interval, represented by (QYˆ ((1−γ)/2) , QYˆ ((1+γ)/2) ), where Q denotes the quantile. Works on establishing confidence for results in NLP tasks using this bootstrap method include Koehn (2004) and Li et al. (2017). acc(Bm ) = Model Calibration (MC) Calibration (Gleser, 1996), also known as reliability, refers to the ability of a model to make good probabilistic predictions. For a discrete distribution over events, a model is said to be wellcalibrated if for those events that the model assigns a probability of p, the long-run proportion that the event actually occurs turns out to be p. For example, if a weather forecast model predicts that there is a 0.1 probability of rain at 7 a.m., then when observed on a large number of random trials at 7 a.m., the model is well-calibrated if 0.1 of them actually do"
2021.eacl-main.324,N19-4007,1,0.8592,"on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However, from a perspective of understanding the workings of our models, work on model analysis has demonstrated the need for more fine-grained analysis over a wide variety of tasks (Kummerfeld et al., 2012; Kummerfeld and Klein, 2013; Karpathy et al., 2015; Neubig et al., 2019; Fu et al., 2020a,b,c). These methods calculate separate accuracy scores for different types of examples (e.g. accuracies for entity recognition by entity length). Our first contribution is to examine experimental settings where we predict these fine-grained evaluation scores (§2.3), and also propose performance prediction methods particularly suited to this fine-grained evaluation setting (§3). Our second contribution is the development of methods for estimating the reliability of performance predictions. While allowing estimation 3703 Proceedings of the 16th Conference of the European Chapt"
2021.eacl-main.324,2020.emnlp-main.213,0,0.0132722,"for reliability analysis of the predicted confidence interval, which could also be explored on other scenarios, e.g., density forecasting (Diebold et al., 1997). Another potentially valuable research topic is to build connections with the probability integral transform (Angus, 1994), which is a typical method of calibration evaluation in financial risks, and our proposed calibration method. Calibration for automated evaluation metrics: From a broader point of view, the role of existing learnable automatic evaluation metrics for text generation, such as BLEURT (Sellam et al., 2020) and COMET (Rei et al., 2020), is similar to a performance prediction model (i.e., both take features of input data as input and then output an evaluation score). Reliability analysis of these metrics is also an important topic since they determine the direction of model optimization. Implications and Future Directions In this work, we not1 only widen the applicability of performance prediction, extending it to finegrained evaluation scenarios, but also establish a 1 set of reliability analysis mechanisms to improve its practicality. In closing, we highlight some potential future directions: We sincerely thank all reviewe"
2021.eacl-main.324,2021.eacl-main.115,0,0.0252345,"Missing"
2021.eacl-main.324,2020.acl-main.704,0,0.0333875,"dence: Our work provides an idea for reliability analysis of the predicted confidence interval, which could also be explored on other scenarios, e.g., density forecasting (Diebold et al., 1997). Another potentially valuable research topic is to build connections with the probability integral transform (Angus, 1994), which is a typical method of calibration evaluation in financial risks, and our proposed calibration method. Calibration for automated evaluation metrics: From a broader point of view, the role of existing learnable automatic evaluation metrics for text generation, such as BLEURT (Sellam et al., 2020) and COMET (Rei et al., 2020), is similar to a performance prediction model (i.e., both take features of input data as input and then output an evaluation score). Reliability analysis of these metrics is also an important topic since they determine the direction of model optimization. Implications and Future Directions In this work, we not1 only widen the applicability of performance prediction, extending it to finegrained evaluation scenarios, but also establish a 1 set of reliability analysis mechanisms to improve its practicality. In closing, we highlight some potential future directions: W"
2021.eacl-main.324,W08-0305,0,0.138819,"Missing"
2021.eacl-main.324,2020.acl-main.278,0,0.023207,"nts, a model is said to be wellcalibrated if for those events that the model assigns a probability of p, the long-run proportion that the event actually occurs turns out to be p. For example, if a weather forecast model predicts that there is a 0.1 probability of rain at 7 a.m., then when observed on a large number of random trials at 7 a.m., the model is well-calibrated if 0.1 of them actually do result in rain. Similarly, for a classification model matching the probability a model assigns to a predicted label (i.e., confidence) and the correctness measure of the prediction (i.e., accuracy) (Wang et al., 2020) is desired. Nonetheless, it is common that a model could have a high predictive accuracy, but poor calibration if the model systematically over- or under-estimates its confidence in the predictions it makes. One way to quantify miscalibration is to use Expected Calibration Error (ECE; Nae (2015)), which aims to quantitatively characterize the difference in expectation between confidence and accuracy. To calculate ECE, the predictions should first be partitioned into M buckets based on the confidence of the predictions, where N represents the total number of prediction samples and |Bm |is the"
2021.eacl-main.324,2020.acl-main.764,1,0.887359,"tively unexplored in the NLP context, there are a few examples of predicting performance as: (i) a function of training or model parameters for determining the number of training iterations (Kolachina et al., 2012) or value of hyperparameters (Rosenfeld et al., 2019) and identifying and terminating bad training runs (Domhan et al., 2015). (ii) a function of dataset characteristics to illustrate which factors are significant predictors of system performance (Birch et al., 2008; Turchi et al., 2008), or find a subset of representative experiments to run in order to obtain plausible predictions (Xia et al., 2020). In this paper, we ask two research questions with respect to performance prediction: can we predict performance on a more fine-grained level, and can we quantify the reliability of performance predictions? With respect to the first contribution, previous P2 methods have almost entirely focused on predicting holistic measures of accuracy such as entity F1 (Ratinov and Roth, 2009) or BLEU score (Papineni et al., 2002) over the entire dataset (§2.2). However, from a perspective of understanding the workings of our models, work on model analysis has demonstrated the need for more fine-grained an"
2021.eacl-main.324,P19-1579,1,0.786512,"ettings) so that they can know which training setting can lead to bad performance without running. Dodge et al. (2020) estimate validation performance as a function of computation budget to conduct more robust model comparisons. Why Performance Prediction matters for NLP tasks Firstly, for some NLP tasks with few resources, it is challenging to build and test systems for all languages or domains. For example, the task of Machine Translation (MT) for low resource languages is hard due to the lack of the large parallel corpora, preventing us from measuring system performance in these scenarios (Xia et al., 2019, 2020). Therefore, performance prediction is useful in that it can efficiently and comprehensively give insights about the workings of models over a wide variety of task settings. Secondly, performance prediction can be used to alleviate the data 3704 sparsity problem in fine-grained evaluation, which plays an important role in current NLP task evaluation (Fu et al., 2020a). In this paper, we consider two performance prediction scenarios, a holistic evaluation setting that most previous works have explored, and a novel setting of predicting fine-grained evaluation metrics. Below, we briefly d"
2021.emnlp-main.802,2020.findings-emnlp.195,0,0.0856569,"Missing"
2021.emnlp-main.802,2020.emnlp-main.630,1,0.933863,"l., 2020), i.e. they We retain the XNLI (Conneau et al., 2018), UDplace the semantically most related text pairs (e.g. a POS (Nivre et al., 2018), WikiANN-NER (Pan question and its answer) closest together in repreet al., 2017), XQuAD (Artetxe et al., 2020a), sentation space, regardless of their language idenMLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark tities. The second analogously frames entity linket al., 2020), and Tatoeba (Artetxe and Schwenk, ing as retrieving from a multilingual pool of entity 2019) tasks from XTREME (see Appendix C). descriptions, given an entity mention in context (Botha et al., 2020). For both, we report perfor3.2 New Tasks mance as mean average precision at 20 (mAP@20). 3.2.1 Multilingual Causal Reasoning LAReQA Language Agnostic Retrieval Question XCOPA The Cross-lingual Choice of Plausible Answering (Roy et al., 2020) is a sentence retrieval Alternatives (Ponti et al., 2020) dataset asks models task. Each query has target answers in multiple lanto decide which of two sentences causally follows guages, and models are expected to rank all correct 10218 Table 3: C HECK L IST templates and generated tests for different capabilities in English, Hebrew, Arabic, and Bengali."
2021.emnlp-main.802,2021.naacl-main.280,0,0.0801691,"Missing"
2021.emnlp-main.802,2020.tacl-1.30,1,0.79452,"i.e. RemBERT for retrieval and mT5 for retrieval and tagging. but remain well below performance on English. On POS tagging (Figure 1d), scores remain largely the same; performance is lower for some languages with non-Latin scripts and low-resource languages. We show the scores for the remaining tasks in Appendix B. The remaining gap to English performance on these tasks is partially an artefact of the evaluation setup: zero-shot cross-lingual transfer from English favors English representations whereas models fine-tuned on in-language monolingual data perform more similarly across languages (Clark et al., 2020; Hu et al., 2020). ilar to the downstream setting but does not significantly improve performance on other tasks. Finetuning on automatically translated task-specific data yields strong gains and is used by most recent models to achieve the best performance (Hu et al., 2020; Ouyang et al., 2020; Luo et al., 2020). Nevertheless, key challenges such as how to learn robust cross-lingual syntactic and semantic processing capabilities during pre-training remain. 3 XTREME-R In order to encourage the NLP community to tackle challenging research directions in pursuit of betOverall, representations fro"
2021.emnlp-main.802,2020.acl-main.747,0,0.251756,"aset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models. XTREME XTREME - R Analysis tools 40 9 Classification, structured prediction, QA, retrieval — Leaderboard Static 50 − 2 + 3 = 10 +language-agnostic retrieval M ULTI C HECK L IST, Explainaboard Interactive, +metadata # of languages # of tasks Task categories Table 1: Overview of XTREME and XTREME - R. have been introduced, consolidating existing multilingual tasks and covering tens of languages. When XTREME was released, the gap between the best-performing baseline, XLM-R Large (Conneau et al., 2020), and human-level performance was roughly 25. This has since shrunk to less than 12 points, a much smaller but still substantial gap compared to the difference from human-level performance observed in English transfer learning (Wang et al., 2019a), which has recently been closed entirely on some evaluation suites (He et al., 2021). In order to examine the nature of this progress, we first perform an analysis of state-of-the-art mul1 Introduction tilingual models on XTREME. We observe that progress has not been uniform, but concentrated on Most research in natural language processing cross-ling"
2021.emnlp-main.802,D18-1269,0,0.022892,"ll be used to search over only English candidates. However, practical settings often violate this assumption, e.g. the answer to a question may be available in any number of languages, possibly different from the query language. Models that cannot compare the appropriateness of retrieval results across languages are thus ineffective in such real-world scenarios. XTREME - R includes two new related crosslingual retrieval tasks. The first seeks to measure the extent to which cross-lingual representations 3.1 Retained Tasks are “strongly aligned” (Roy et al., 2020), i.e. they We retain the XNLI (Conneau et al., 2018), UDplace the semantically most related text pairs (e.g. a POS (Nivre et al., 2018), WikiANN-NER (Pan question and its answer) closest together in repreet al., 2017), XQuAD (Artetxe et al., 2020a), sentation space, regardless of their language idenMLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark tities. The second analogously frames entity linket al., 2020), and Tatoeba (Artetxe and Schwenk, ing as retrieving from a multilingual pool of entity 2019) tasks from XTREME (see Appendix C). descriptions, given an entity mention in context (Botha et al., 2020). For both, we report perfor3.2 New Tasks m"
2021.emnlp-main.802,N19-1423,0,0.0292115,"rk (Hu et al., 2020; Lauscher et al., 2020; Hedderich et al., 2020) demonstrates the benefits of fine-tuning on in-language data, we believe the zero-shot scenario remains the most effective way to evaluate the amount of a priori multilingual knowledge a pre-trained model captures. Due to variation in cross-lingual evaluation (Keung et al., 2020), we recommend researchers to use the validation set of a single target language for development (Artetxe et al., 2020b). 4.1 Baselines We employ established pre-trained multilingual and models using translations as baselines. mBERT Multilingual BERT (Devlin et al., 2019) has been pretrained on the Wikipedias of 104 languages using MLM. XLM-R XLM-R Large (Conneau et al., 2020) uses the same MLM objective with a larger model, and was trained on a magnitude more web data from 100 languages. mT5 Multilingual T5 (Xue et al., 2021) is an encoder-decoder transformer that frames NLP tasks in a “text-to-text” format. It was pre-trained with MLM on a large multilingual web corpus covering 101 languages. We employ the largest mT5-XXL variant with 13B parameters. Translate-train To evaluate the impact of MT, we fine-tune mBERT on translations of English training data fro"
2021.emnlp-main.802,2020.emnlp-main.489,1,0.872791,"zing a model’s performance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabilities in a massively multilingual setting. b) We extend the multi-dataset evaluation framework E XPLAINA B OARD (Fu et al., 2020; Liu et al., 2021) to additional tasks and the multilingual setting. This framework allows us to break down performance based on language and task-specific attributes, which enables a more nuanced diagnosis of a model’s behaviour. We also make several logistic improvements to improve XTREME - R’s utility as a leaderboard. To make it easier to choose the best model for a use case, each submission is required to provide metadata such as the number of parameters and amount of pre-training data, which we make available via an interactive leaderboard. We also introduce task and language-specific s"
2021.emnlp-main.802,N18-1108,0,0.0260624,"tr, uk, ur, vi, wo, yo, zh.2 XTREME - R is similarly typologically and genealogically diverse as XTREME while covering a larger number of languages (see Appendix D). 3.4 Diagnostic and evaluation suite To increase the language coverage of low-resource languages in XTREME - R and to enable us to systematically evaluate a model’s cross-lingual generalization ability, we augment XTREME - R with a massively multilingual diagnostic and evaluation suite. Challenge sets and diagnostic suites in NLP (Wang et al., 2019a,b; Belinkov and Glass, 2019) are mostly limited to English, with a few exceptions (Gulordava et al., 2018). As challenge sets are generally created with a human in the loop, the main challenge for creating a large multilingual diagnostic suite is to scale the annotation or translation effort to many languages and to deal with each language’s idiosyncrasies. M ULTI C HECK L IST To address this, we build on the C HECK L IST (Ribeiro et al., 2020) framework, which facilitates creating parameterized tests for models. C HECK L IST enables the creation of test cases using templates, which test for specific behavioral capabilities of a model with regard to a downstream task. Importantly, by relying on te"
2021.emnlp-main.802,2020.emnlp-main.204,0,0.0263038,"erent attribute values. We define new taskspecific attributes for the four task types as well as task-independent attributes (see Appendix K). Metadata We additionally would like to enable practitioners to rank submissions based on other information. To this end, we ask each submission to XTREME - R for relevant metadata such as the number of parameters, the amount of pre-training data, etc. We will show this information in an interactive leaderboard (see Appendix H for the metadata of current XTREME submissions). 4 Experiments glish. While recent work (Hu et al., 2020; Lauscher et al., 2020; Hedderich et al., 2020) demonstrates the benefits of fine-tuning on in-language data, we believe the zero-shot scenario remains the most effective way to evaluate the amount of a priori multilingual knowledge a pre-trained model captures. Due to variation in cross-lingual evaluation (Keung et al., 2020), we recommend researchers to use the validation set of a single target language for development (Artetxe et al., 2020b). 4.1 Baselines We employ established pre-trained multilingual and models using translations as baselines. mBERT Multilingual BERT (Devlin et al., 2019) has been pretrained on the Wikipedias of 104 l"
2021.emnlp-main.802,2020.emnlp-main.479,1,0.848683,"Missing"
2021.emnlp-main.802,2020.acl-main.560,0,0.0412131,"been closed entirely on some evaluation suites (He et al., 2021). In order to examine the nature of this progress, we first perform an analysis of state-of-the-art mul1 Introduction tilingual models on XTREME. We observe that progress has not been uniform, but concentrated on Most research in natural language processing cross-lingual retrieval tasks where fine-tuning on (NLP) to date has focused on developing methods other tasks and pre-training with parallel data lead that work well for English and a small set of other to large gains. On other task categories improvehigh-resource languages (Joshi et al., 2020). In ments are more modest. Models still generally contrast, methods for other languages can be vastly perform poorly on languages with limited data and more beneficial as they enable access to language non-Latin scripts. Fine-tuning on additional transtechnology for more than three billion speakers of low-resource languages and prevent the NLP com- lated data generally leads to the best performance. munity from overfitting to English. Motivated by Based on this analysis, we propose XTREME - R these benefits, the area of multilingual NLP has (XTREME Revisited), a new benchmark with the attract"
2021.emnlp-main.802,2020.findings-emnlp.445,0,0.0706199,"Missing"
2021.emnlp-main.802,2020.emnlp-main.40,0,0.031752,"n to XTREME - R for relevant metadata such as the number of parameters, the amount of pre-training data, etc. We will show this information in an interactive leaderboard (see Appendix H for the metadata of current XTREME submissions). 4 Experiments glish. While recent work (Hu et al., 2020; Lauscher et al., 2020; Hedderich et al., 2020) demonstrates the benefits of fine-tuning on in-language data, we believe the zero-shot scenario remains the most effective way to evaluate the amount of a priori multilingual knowledge a pre-trained model captures. Due to variation in cross-lingual evaluation (Keung et al., 2020), we recommend researchers to use the validation set of a single target language for development (Artetxe et al., 2020b). 4.1 Baselines We employ established pre-trained multilingual and models using translations as baselines. mBERT Multilingual BERT (Devlin et al., 2019) has been pretrained on the Wikipedias of 104 languages using MLM. XLM-R XLM-R Large (Conneau et al., 2020) uses the same MLM objective with a larger model, and was trained on a magnitude more web data from 100 languages. mT5 Multilingual T5 (Xue et al., 2021) is an encoder-decoder transformer that frames NLP tasks in a “text-"
2021.emnlp-main.802,2020.acl-main.329,0,0.0428374,"Missing"
2021.emnlp-main.802,2020.emnlp-main.363,0,0.0253632,"Missing"
2021.emnlp-main.802,2020.acl-main.653,0,0.0342518,"of retrieval results across languages are thus ineffective in such real-world scenarios. XTREME - R includes two new related crosslingual retrieval tasks. The first seeks to measure the extent to which cross-lingual representations 3.1 Retained Tasks are “strongly aligned” (Roy et al., 2020), i.e. they We retain the XNLI (Conneau et al., 2018), UDplace the semantically most related text pairs (e.g. a POS (Nivre et al., 2018), WikiANN-NER (Pan question and its answer) closest together in repreet al., 2017), XQuAD (Artetxe et al., 2020a), sentation space, regardless of their language idenMLQA (Lewis et al., 2020), TyDiQA-GoldP (Clark tities. The second analogously frames entity linket al., 2020), and Tatoeba (Artetxe and Schwenk, ing as retrieving from a multilingual pool of entity 2019) tasks from XTREME (see Appendix C). descriptions, given an entity mention in context (Botha et al., 2020). For both, we report perfor3.2 New Tasks mance as mean average precision at 20 (mAP@20). 3.2.1 Multilingual Causal Reasoning LAReQA Language Agnostic Retrieval Question XCOPA The Cross-lingual Choice of Plausible Answering (Roy et al., 2020) is a sentence retrieval Alternatives (Ponti et al., 2020) dataset asks mo"
2021.emnlp-main.802,2020.acl-main.465,0,0.0160657,"s by covering 50 typologically diverse languages and 10 challenging, diverse tasks. To make retrieval more difficult, we introduce two new tasks that focus on “language-agnostic” retrieval (Roy et al., 2020), where targets must be retrieved from a large multilingual candidate pool. We additionally establish new state-of-the-art mT5 (Xue et al., 2021) and translate-train baselines for our tasks. XTREME - R aims to move away from a single aggregate metric summarizing a model’s performance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabilities in a massively multilingual setting. b) We extend the multi-dataset evaluation framework E XPLAINA B OARD (Fu et al., 2020; Liu et al., 2021) to additional tasks and the multilingual setting. This framework allows us to break down performance based on lan"
2021.emnlp-main.802,E17-2002,0,0.063781,"Missing"
2021.emnlp-main.802,2021.acl-demo.34,1,0.889858,"rformance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabilities in a massively multilingual setting. b) We extend the multi-dataset evaluation framework E XPLAINA B OARD (Fu et al., 2020; Liu et al., 2021) to additional tasks and the multilingual setting. This framework allows us to break down performance based on language and task-specific attributes, which enables a more nuanced diagnosis of a model’s behaviour. We also make several logistic improvements to improve XTREME - R’s utility as a leaderboard. To make it easier to choose the best model for a use case, each submission is required to provide metadata such as the number of parameters and amount of pre-training data, which we make available via an interactive leaderboard. We also introduce task and language-specific sub-leaderboards to"
2021.emnlp-main.802,2021.ccl-1.108,0,0.0472179,"Missing"
2021.emnlp-main.802,2021.emnlp-main.3,0,0.0855395,"Missing"
2021.emnlp-main.802,P17-1178,0,0.0484182,"Missing"
2021.emnlp-main.802,P02-1040,0,0.111792,"Missing"
2021.emnlp-main.802,2020.emnlp-main.617,1,0.881246,"Missing"
2021.emnlp-main.802,2020.aacl-main.56,0,0.0322166,"tic processing capabilities during pre-training remain. 3 XTREME-R In order to encourage the NLP community to tackle challenging research directions in pursuit of betOverall, representations from token-level MLM pre-training are of limited use for cross-lingual sen- ter cross-lingual model generalization, we propose XTREME - R (XTREME Revisited). XTREME - R tence retrieval, as evidenced by the comparatively poor performance of the mBERT and XLM-R mod- shares its predecessor’s core design principles for creating an accessible benchmark to evaluate crossels. Fine-tuning on sentence-level tasks (Phang et al., 2020; Fang et al., 2021) can mitigate this. lingual transfer but makes some key changes. The strong performance of recent models such as First, XTREME - R focuses on the tasks that have VECO and ERNIE-M on the retrieval tasks can proven to be hardest for current multilingual modbe attributed to a combination of parallel data and els. To this end, it drops XTREME’s PAWS-X and new pre-training objectives that make use of it. Pre- BUCC tasks since recent advances have left less training on parallel data improves performance on room for further improvement, and they cover only retrieval by making the"
2021.emnlp-main.802,2020.emnlp-main.185,0,0.0332192,"Missing"
2021.emnlp-main.802,P19-1015,0,0.0348481,"Missing"
2021.emnlp-main.802,D16-1264,0,0.111238,"Missing"
2021.emnlp-main.802,2020.emnlp-main.477,1,0.864987,"Missing"
2021.emnlp-main.802,D19-1454,0,0.0537933,"Missing"
2021.emnlp-main.802,P18-1072,1,0.884249,"Missing"
2021.emnlp-main.802,P19-1355,0,0.0118816,"forts to include training data in multiple languages. Biases in multilingual models 7.4 Environmental concerns XTREME - R aims to enable efficient evaluation of multilingual models. To this end, we created a new dataset, Mewsli-X, that captures the essence of multilingual entity linking against a diverse knowledge base but is computationally cheaper to evaluate than the large-scale Mewsli-9 (Botha et al., 2020). Nevertheless, the models that perform best on benchmarks like XTREME - R are generally large-scale Transformer models pre-trained on large amounts of data, which comes at a high cost (Strubell et al., 2019). We thus particularly encourage the development of efficient methods to adapt existing models to new languages (Pfeiffer et al., 2020) rather than training multilingual models entirely from scratch. Acknowledgements We thank Marco Tulio Ribeiro for advice on C HECK L IST. We are grateful to Laura Rimell and Jon Clark for valuable feedback on drafts of this paper, and to Dan Gillick for feedback on the Mewsli-X dataset design. We thank Hila Gonen, Bidisha Samantha, and Partha Talukdar for advice on Arabic, Bengali, and Hebrew C HECK L IST examples. References Antonios Anastasopoulos and Graham"
2021.emnlp-main.802,N18-1101,0,0.0932773,"Missing"
2021.emnlp-main.802,2020.acl-main.442,0,0.188983,"c” retrieval (Roy et al., 2020), where targets must be retrieved from a large multilingual candidate pool. We additionally establish new state-of-the-art mT5 (Xue et al., 2021) and translate-train baselines for our tasks. XTREME - R aims to move away from a single aggregate metric summarizing a model’s performance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabilities in a massively multilingual setting. b) We extend the multi-dataset evaluation framework E XPLAINA B OARD (Fu et al., 2020; Liu et al., 2021) to additional tasks and the multilingual setting. This framework allows us to break down performance based on language and task-specific attributes, which enables a more nuanced diagnosis of a model’s behaviour. We also make several logistic improvements to improve XTREME - R’s utility as a leade"
2021.emnlp-main.802,2021.naacl-main.41,1,0.926532,"and XGLUE (Liang et al., 2020) multilingual, diverse, and accessible. It expands on 10215 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10215–10245 c November 7–11, 2021. 2021 Association for Computational Linguistics by covering 50 typologically diverse languages and 10 challenging, diverse tasks. To make retrieval more difficult, we introduce two new tasks that focus on “language-agnostic” retrieval (Roy et al., 2020), where targets must be retrieved from a large multilingual candidate pool. We additionally establish new state-of-the-art mT5 (Xue et al., 2021) and translate-train baselines for our tasks. XTREME - R aims to move away from a single aggregate metric summarizing a model’s performance and towards a more nuanced evaluation and comparison of multilingual models (Ethayarajh and Jurafsky, 2020; Linzen, 2020). To this end, we introduce an extensible multilingual diagnostic and evaluation suite that consists of two main components: a) M ULTI C HECK L IST, a test suite (Ribeiro et al., 2020) for probing question answering capabilities in 50 languages. This test suite is the first of its kind and enables direct evaluation of finegrained capabil"
2021.findings-acl.303,N19-1260,0,0.0163815,"Pittsburgh, PA {ptejaswi, drn, pliu3}@andrew.cmu.edu Abstract Kryscinski et al., 2020) which are used for tuning hyperparameters and comparing models. While the reliability of these metrics has been explored extensively (Peyrard, 2019; Bhandari et al., 2020; Fabbri et al., 2020), few studies have focused on the underlying characteristics of different datasets, and how these impact model performance and metric reliability. Datasets like CNN/DailyMail (Nallapati et al., 2016), Gigaword (Rush et al., 2015), XSum (Narayan et al., 2018a), and many more (Wang and Ling, 2016; Koupaee and Wang, 2018; Kim et al., 2019; Ganesan et al., 2010) were collected by scraping a large collection of web-pages. And for all the benefits this approach offers (seemingly infinite samples, diverse subjects, etc) there are some caveats: State-of-the-art summarization systems are trained and evaluated on massive datasets scraped from the web. Despite their prevalence, we know very little about the underlying characteristics (data noise, summarization complexity, etc.) of these datasets, and how these affect system performance and the reliability of automatic metrics like ROUGE. In this study, we manually analyse 600 samples"
2021.findings-acl.303,D19-1051,0,0.0315089,"Missing"
2021.findings-acl.303,2020.emnlp-main.750,0,0.0203043,"list popular automatic metrics explored in this work. Except for the last two, all outputs from every model is scored on the following metrics. ROUGE-1/2/L measure overlap of unigrams, bigrams and longest common subsequence. respectively5 (Lin, 2004). BERTScore (BS) measures soft overlap between contextual BERT embeddings of tokens between the two texts6 (Zhang et al., 2020). MoverScore (MS) applies a distance measure to contextualized BERT and ELMo word embeddings7 (Zhao et al., 2019). FactCC is introduced to measure the fact consistency between the generated summaries and source documents (Kryscinski et al., 2020). Due to issues with the setup and training procedure, this metric was only used in the CNN/DM analysis. Human Pyramid (HP) provides a robust technique for evaluating content selection by exhaustively obtaining a set of Semantic Content Units (SCUs) from a set of references, and then scoring system summaries on the number of SCUs that can be inferred (Nenkova and Passonneau, 2004). We use the scores shared by Bhandari et al. (2020) for the first 100 samples of CNN/DM subset. 3.3 Model Performance For each dataset, we group the samples by their labels. For all samples in a subset, the model res"
2021.findings-acl.303,2020.acl-main.703,0,0.0430544,"Missing"
2021.findings-acl.303,D19-1387,0,0.0775823,"y of metrics is dependent on sample complexity. (3) Faithful summaries often receive low scores because of the poor diversity of references. We release the code, annotated data and model outputs.1 1 Data Noise We have no idea about the noise in the dataset. In the context of text summarization, noise could be an incomplete or irrelevant reference. At the moment, its quantity and impact on the performance is unknown. Introduction The past few years have witnessed major breakthroughs and improvements in automatic summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018; Liu and Lapata, 2019; Liu, 2019; Dou et al., 2020; Yuan et al., 2021; Liu et al., 2021). Apart from the improvements in the summarization model architectures (Zhang et al., 2019; Zhong et al., 2020), this growth has been aided by large-scale datasets (Nallapati et al., 2016; Narayan et al., 2018a; Sharma et al., 2019) and automatic evaluation metrics (Lin, 2004; Zhao et al., 2019; ∗ This author was the primary contributor. Corresponding author. 1 https://github.com/priyamtejaswin/howwelldoyouknow † Summarization Complexity What do we really know about the nature of samples in the dataset? Gigaword is a headline g"
2021.findings-acl.303,2021.naacl-main.113,1,0.763923,"often receive low scores because of the poor diversity of references. We release the code, annotated data and model outputs.1 1 Data Noise We have no idea about the noise in the dataset. In the context of text summarization, noise could be an incomplete or irrelevant reference. At the moment, its quantity and impact on the performance is unknown. Introduction The past few years have witnessed major breakthroughs and improvements in automatic summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018; Liu and Lapata, 2019; Liu, 2019; Dou et al., 2020; Yuan et al., 2021; Liu et al., 2021). Apart from the improvements in the summarization model architectures (Zhang et al., 2019; Zhong et al., 2020), this growth has been aided by large-scale datasets (Nallapati et al., 2016; Narayan et al., 2018a; Sharma et al., 2019) and automatic evaluation metrics (Lin, 2004; Zhao et al., 2019; ∗ This author was the primary contributor. Corresponding author. 1 https://github.com/priyamtejaswin/howwelldoyouknow † Summarization Complexity What do we really know about the nature of samples in the dataset? Gigaword is a headline generation dataset with short sources and references. Does this impl"
2021.findings-acl.303,D18-1206,0,0.301345,"ruv Naik Pengfei Liu † Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA {ptejaswi, drn, pliu3}@andrew.cmu.edu Abstract Kryscinski et al., 2020) which are used for tuning hyperparameters and comparing models. While the reliability of these metrics has been explored extensively (Peyrard, 2019; Bhandari et al., 2020; Fabbri et al., 2020), few studies have focused on the underlying characteristics of different datasets, and how these impact model performance and metric reliability. Datasets like CNN/DailyMail (Nallapati et al., 2016), Gigaword (Rush et al., 2015), XSum (Narayan et al., 2018a), and many more (Wang and Ling, 2016; Koupaee and Wang, 2018; Kim et al., 2019; Ganesan et al., 2010) were collected by scraping a large collection of web-pages. And for all the benefits this approach offers (seemingly infinite samples, diverse subjects, etc) there are some caveats: State-of-the-art summarization systems are trained and evaluated on massive datasets scraped from the web. Despite their prevalence, we know very little about the underlying characteristics (data noise, summarization complexity, etc.) of these datasets, and how these affect system performance and the reliability"
2021.findings-acl.303,N18-1158,0,0.382045,"ruv Naik Pengfei Liu † Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA {ptejaswi, drn, pliu3}@andrew.cmu.edu Abstract Kryscinski et al., 2020) which are used for tuning hyperparameters and comparing models. While the reliability of these metrics has been explored extensively (Peyrard, 2019; Bhandari et al., 2020; Fabbri et al., 2020), few studies have focused on the underlying characteristics of different datasets, and how these impact model performance and metric reliability. Datasets like CNN/DailyMail (Nallapati et al., 2016), Gigaword (Rush et al., 2015), XSum (Narayan et al., 2018a), and many more (Wang and Ling, 2016; Koupaee and Wang, 2018; Kim et al., 2019; Ganesan et al., 2010) were collected by scraping a large collection of web-pages. And for all the benefits this approach offers (seemingly infinite samples, diverse subjects, etc) there are some caveats: State-of-the-art summarization systems are trained and evaluated on massive datasets scraped from the web. Despite their prevalence, we know very little about the underlying characteristics (data noise, summarization complexity, etc.) of these datasets, and how these affect system performance and the reliability"
2021.findings-acl.303,N04-1019,0,0.157121,". MoverScore (MS) applies a distance measure to contextualized BERT and ELMo word embeddings7 (Zhao et al., 2019). FactCC is introduced to measure the fact consistency between the generated summaries and source documents (Kryscinski et al., 2020). Due to issues with the setup and training procedure, this metric was only used in the CNN/DM analysis. Human Pyramid (HP) provides a robust technique for evaluating content selection by exhaustively obtaining a set of Semantic Content Units (SCUs) from a set of references, and then scoring system summaries on the number of SCUs that can be inferred (Nenkova and Passonneau, 2004). We use the scores shared by Bhandari et al. (2020) for the first 100 samples of CNN/DM subset. 3.3 Model Performance For each dataset, we group the samples by their labels. For all samples in a subset, the model response is scored using a metric. The mean of these sample scores returns a single subset-model-metric score, which is then averaged across all models in the subset, leaving us with a single subset-metric score. This is repeated for all (subset × metric) pairs. The results are captured in Figures 2, 3 and 4 for Gigaword, CNN/DM and XSum respectively. The last column in each group is"
2021.findings-acl.303,P02-1040,0,0.11208,"ces, some outputs would have found better matches, and thus, higher scores! In fact, we see that BERTScore (a more “semantically” oriented metric) is extremely competitive across all 3441 categories in all three datasets (Figures 2, 3, 4), suggesting the generations are similar to the references. These results lead us to believe that token-based summarization metrics might also suffer from a “summarization-ese” effect: the metrics could be biased towards simpler, more “extractive” references. Recently, Freitag et al. (2020) also arrived at the same conclusion for machine translation and BLEU (Papineni et al., 2002). In the next section, we continue to explore the reliability of these metrics. 4 (a) Gigaword Does the reliability of metrics change with data properties? (Q2 b) (b) CNN/DM For each document di , i ∈ {1 . . . n} in a dataset D, we have J system outputs, where the outputs can come from different systems. Let sij , j ∈ {1 . . . J} be the j th summary of the ith document, mi be a specific metric (including human judgment). sum Km 1 m2 n  1X = K [m1 (si1 ) . . . m1 (siJ )], n i=1   [m2 (si1 ) . . . m2 (siJ )] . (1) Correlation is calculated for each document, among the different system outputs"
2021.findings-acl.303,P19-1502,0,0.133571,"dels. (2) Whether samples have different levels of difficulty. Armed with this, we ask the following questions. Q2 a. How do these properties impact model performance? Specifically, we’d like to know (1) If, and how, the performance varies across the different types of samples discovered from Q1. (2) If the performance is consistent across metrics. Q2 b. If the reliability of metrics changes with these properties? This is motivated (in part) from prior metric-analysis studies, where researchers have explored inter-metric agreement and alignment with human-judgement under different conditions (Peyrard, 2019; Bhandari et al., 2020). Here we are more interested in knowing if the metrics are more correlated with human judgement for simpler samples, than complex ones. Large-scale automatic intrinsic dataset evaluation has been explored with some promising results (Bommasani and Cardie, 2020). However, these methods rely on heuristics like content-value, density and compression (Grusky et al., 2018). We are interested in a more fine-grained, interpretable analysis that can only come from manual inspection, much like the analysis by Chen et al. (2016) and by Yatskar (2019). To that end, we first defin"
2021.findings-acl.303,2020.findings-emnlp.217,0,0.0206739,"Missing"
2021.findings-acl.303,D15-1044,0,0.312829,"sets? Priyam Tejaswin ∗ Dhruv Naik Pengfei Liu † Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA {ptejaswi, drn, pliu3}@andrew.cmu.edu Abstract Kryscinski et al., 2020) which are used for tuning hyperparameters and comparing models. While the reliability of these metrics has been explored extensively (Peyrard, 2019; Bhandari et al., 2020; Fabbri et al., 2020), few studies have focused on the underlying characteristics of different datasets, and how these impact model performance and metric reliability. Datasets like CNN/DailyMail (Nallapati et al., 2016), Gigaword (Rush et al., 2015), XSum (Narayan et al., 2018a), and many more (Wang and Ling, 2016; Koupaee and Wang, 2018; Kim et al., 2019; Ganesan et al., 2010) were collected by scraping a large collection of web-pages. And for all the benefits this approach offers (seemingly infinite samples, diverse subjects, etc) there are some caveats: State-of-the-art summarization systems are trained and evaluated on massive datasets scraped from the web. Despite their prevalence, we know very little about the underlying characteristics (data noise, summarization complexity, etc.) of these datasets, and how these affect system perf"
2021.findings-acl.303,K16-1028,0,0.0427532,"Missing"
2021.findings-acl.303,P17-1099,0,0.662867,"eir collection process. (2) The performance of models and reliability of metrics is dependent on sample complexity. (3) Faithful summaries often receive low scores because of the poor diversity of references. We release the code, annotated data and model outputs.1 1 Data Noise We have no idea about the noise in the dataset. In the context of text summarization, noise could be an incomplete or irrelevant reference. At the moment, its quantity and impact on the performance is unknown. Introduction The past few years have witnessed major breakthroughs and improvements in automatic summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018; Liu and Lapata, 2019; Liu, 2019; Dou et al., 2020; Yuan et al., 2021; Liu et al., 2021). Apart from the improvements in the summarization model architectures (Zhang et al., 2019; Zhong et al., 2020), this growth has been aided by large-scale datasets (Nallapati et al., 2016; Narayan et al., 2018a; Sharma et al., 2019) and automatic evaluation metrics (Lin, 2004; Zhao et al., 2019; ∗ This author was the primary contributor. Corresponding author. 1 https://github.com/priyamtejaswin/howwelldoyouknow † Summarization Complexity What do we really k"
2021.findings-acl.303,P19-1212,0,0.0202302,"se could be an incomplete or irrelevant reference. At the moment, its quantity and impact on the performance is unknown. Introduction The past few years have witnessed major breakthroughs and improvements in automatic summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018; Liu and Lapata, 2019; Liu, 2019; Dou et al., 2020; Yuan et al., 2021; Liu et al., 2021). Apart from the improvements in the summarization model architectures (Zhang et al., 2019; Zhong et al., 2020), this growth has been aided by large-scale datasets (Nallapati et al., 2016; Narayan et al., 2018a; Sharma et al., 2019) and automatic evaluation metrics (Lin, 2004; Zhao et al., 2019; ∗ This author was the primary contributor. Corresponding author. 1 https://github.com/priyamtejaswin/howwelldoyouknow † Summarization Complexity What do we really know about the nature of samples in the dataset? Gigaword is a headline generation dataset with short sources and references. Does this imply a higher volume of simpler (i.e. more extractive) samples? The degree of summarization complexity, and its impact on model performance is unknown. Exploring these open questions is critical for two reasons: (1) Information about t"
2021.findings-acl.303,P19-1207,0,0.0475972,"Missing"
2021.findings-acl.303,D19-1053,0,0.0813916,"its quantity and impact on the performance is unknown. Introduction The past few years have witnessed major breakthroughs and improvements in automatic summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018; Liu and Lapata, 2019; Liu, 2019; Dou et al., 2020; Yuan et al., 2021; Liu et al., 2021). Apart from the improvements in the summarization model architectures (Zhang et al., 2019; Zhong et al., 2020), this growth has been aided by large-scale datasets (Nallapati et al., 2016; Narayan et al., 2018a; Sharma et al., 2019) and automatic evaluation metrics (Lin, 2004; Zhao et al., 2019; ∗ This author was the primary contributor. Corresponding author. 1 https://github.com/priyamtejaswin/howwelldoyouknow † Summarization Complexity What do we really know about the nature of samples in the dataset? Gigaword is a headline generation dataset with short sources and references. Does this imply a higher volume of simpler (i.e. more extractive) samples? The degree of summarization complexity, and its impact on model performance is unknown. Exploring these open questions is critical for two reasons: (1) Information about the noise could lead to more informed data collection and prepro"
2021.findings-acl.303,P19-1100,1,0.897536,"Missing"
2021.findings-acl.303,D19-5410,1,0.923666,"pectively. Disagreements were discussed between all authors before arriving at a consensus for the final label. 2.2.1 Motivation and Advantages To the best of our knowledge, summarization datasets have not been manually analysed in this manner. A review of the most relevant summarization dataset analysis research shows that the most common form of intrinsic evaluation is to use surface-level heuristics. Most studies only cover a part of our typology, while almost all studies ignore the noise present in datasets. Coverage , Density, Redundancy Grusky et al. (2018); Bommasani and Cardie (2020); Zhong et al. (2019b) use similar forms of token-level coverage between the source and the reference to measure the extractiveness of the summary. In it’s simplest form, this is a ratio of the number of overlapping tokens and reference length. In our definition of Extractive, we first set a meaninful, well-defined criterion, and then manually check for extractive references, while allowing for some relaxations. Content Compression In most papers (Grusky et al., 2018; Zhong et al., 2019b; Bommasani and Cardie, 2020), the summarization complexity is defined by a compression ratio (usually the normalized word-count"
2021.findings-acl.303,N16-1007,0,0.0212686,"ogies Institute, Carnegie Mellon University, Pittsburgh, PA {ptejaswi, drn, pliu3}@andrew.cmu.edu Abstract Kryscinski et al., 2020) which are used for tuning hyperparameters and comparing models. While the reliability of these metrics has been explored extensively (Peyrard, 2019; Bhandari et al., 2020; Fabbri et al., 2020), few studies have focused on the underlying characteristics of different datasets, and how these impact model performance and metric reliability. Datasets like CNN/DailyMail (Nallapati et al., 2016), Gigaword (Rush et al., 2015), XSum (Narayan et al., 2018a), and many more (Wang and Ling, 2016; Koupaee and Wang, 2018; Kim et al., 2019; Ganesan et al., 2010) were collected by scraping a large collection of web-pages. And for all the benefits this approach offers (seemingly infinite samples, diverse subjects, etc) there are some caveats: State-of-the-art summarization systems are trained and evaluated on massive datasets scraped from the web. Despite their prevalence, we know very little about the underlying characteristics (data noise, summarization complexity, etc.) of these datasets, and how these affect system performance and the reliability of automatic metrics like ROUGE. In th"
2021.findings-acl.303,N19-1241,0,0.018394,"oise could lead to more informed data collection and preprocessing methods: in a recent study, Kryscinski et al. (2019) quantified HTML artefacts in popular summarization datasets, and proposed ways to detect and remove them. (2) Awareness about the complexity could better explain model performance, metrics, and even lead to new model architectures. In the tasks of machine comprehension 3436 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3436–3449 August 1–6, 2021. ©2021 Association for Computational Linguistics and question answering, Chen et al. (2016) and Yatskar (2019) manually inspected random samples and drew insights which led to new state-of-the-art models. Such analysis could also help researchers choose datasets and metrics more carefully. In this study, we perform intrinsic and modelcentric evaluation of three popular summarization datasets (Gigaword, CNN/DM and XSum). We are interested in answering the following questions: Q1. What are the underlying intrinsic properties of summarization datasets? We are interested in (1) Identifying and quantifying the different types of “noise” that could occur and could penalize models. (2) Whether samples have d"
2021.findings-emnlp.179,2020.emnlp-main.506,0,0.0333661,"Summarization Yiran Chen∗, Pengfei Liu]∗, Xipeng Qiu† Shanghai Key Laboratory of Intelligent Information Processing, Fudan University School of Computer Science, Fudan University 2005 Songhu Road, Shanghai, China ]Carnegie Mellon University {yrchen19,xpqiu}@fudan.edu.cn {pliu3}@cs.cmu.edu Abstract supported by the source document (Cao et al., 2018a). With the continuous upgrading of the summaAmong this background, a large body of recent rization systems driven by deep neural networks (Wang et al., 2020a; Kryscinski et al., 2020; works, researchers have higher requirements Durmus et al., 2020; Cao et al., 2020) are trying to on the quality of the generated summaries, search for new automated metrics that can assess which should be not only fluent and informative but also factually correct. As a rethe factuality of generated summaries due to the sult, the field of factual evaluation has defact that existing metrics (e.g., ROUGE) are not veloped rapidly recently. Despite its initial correlated well with factual consistency (Maynez progress in evaluating generated summaries, et al., 2020; Goyal and Durrett, 2020). the meta-evaluation methodologies of factualGenerally, the process of designing these eva"
2021.findings-emnlp.179,2020.findings-emnlp.322,0,0.424136,"2020a; Kryscinski et al., 2020; works, researchers have higher requirements Durmus et al., 2020; Cao et al., 2020) are trying to on the quality of the generated summaries, search for new automated metrics that can assess which should be not only fluent and informative but also factually correct. As a rethe factuality of generated summaries due to the sult, the field of factual evaluation has defact that existing metrics (e.g., ROUGE) are not veloped rapidly recently. Despite its initial correlated well with factual consistency (Maynez progress in evaluating generated summaries, et al., 2020; Goyal and Durrett, 2020). the meta-evaluation methodologies of factualGenerally, the process of designing these evaluity metrics are limited in their opacity, leading ation metrics w.r.t factuality is commonly formuto the insufficient understanding of factuality metrics’ relative advantages and their applicalated into different forms of NLP tasks, ranging bility. In this paper, we present an adversarfrom text entailment (Falke et al., 2019; Kryscinski ial meta-evaluation methodology that allows et al., 2020) at sentence level or more fine-grained us to (i) diagnose the fine-grained strengths level (Goyal and Durrett,"
2021.findings-emnlp.179,2021.naacl-main.114,0,0.151983,"or Automated Metrics Metaevaluation aims to evaluate the reliability of automated metrics based on their correlation with human judgments (Graham, 2015; Peyrard, 2019; Bhandari et al., 2020). Most existing works perform meta-evaluation on metrics that measure semantic equivalence, such as ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020). Yuan et al. (2021) more recently propose BARTScore and meta evaluate it on multiple evaluation perspectives. By contrast, in this paper, we focus on the evaluation of factuality metrics using our constructed diagnostic test sets. Concurrent with our work, Goyal and Durrett (2021b); Pagnoni et al. (2021b) also look into the error patterns of existing factuality checkers.1 Factuality in Text Summarization Recent studies on factuality of text generation revolve around metric design and system optimization. Regarding the metric perspective, researchers formulate the design of automated metrics w.r.t factuality as different problems: text entailment over sequential (Kryscinski et al., 2020; Goyal and Durrett, 2021a) or tree (Goyal and Durrett, 2020, 2021a) structures; 1 question answering (Wang et al., 2020a; Durmus We encourage readers to read these works as well to et a"
2021.findings-emnlp.179,D15-1013,0,0.0221422,"et al., 2018) question answering (Jia and Liang, 2017), machine translation (Burlot and Yvon, 2017) and language model (Marvin and Linzen, 2018) to examine system drawbacks. More recently, Gardner et al. (2020) introduces the concept of “contrast set” and proposes to use it to measure the generalization of different NLP systems. Instead of adversarially evaluate an NLP system, we perform an adversarial metaevaluation of evaluation metrics. Meta-evaluation for Automated Metrics Metaevaluation aims to evaluate the reliability of automated metrics based on their correlation with human judgments (Graham, 2015; Peyrard, 2019; Bhandari et al., 2020). Most existing works perform meta-evaluation on metrics that measure semantic equivalence, such as ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020). Yuan et al. (2021) more recently propose BARTScore and meta evaluate it on multiple evaluation perspectives. By contrast, in this paper, we focus on the evaluation of factuality metrics using our constructed diagnostic test sets. Concurrent with our work, Goyal and Durrett (2021b); Pagnoni et al. (2021b) also look into the error patterns of existing factuality checkers.1 Factuality in Text Summarization"
2021.findings-emnlp.179,2020.acl-main.761,0,0.0211237,"Missing"
2021.findings-emnlp.179,D17-1215,0,0.0279614,"rmation extraction and dependency parsing (Cao et al., 2018b; Zhu et al., 2020). Chen et al. (2020) explore how factuality metrics are influenced by domain shift and conclude that out-of-domain systems can even surpass in-domain systems in terms of factuality and factuality checkers like FactCC is limited in predictive power of positive samples. Adversarial Evaluation of NLP Systems Adversarial evaluation has been extensively explored in many NLP tasks recently. The adversarial challenge sets have been introduced into tasks of natural language inference (Naik et al., 2018) question answering (Jia and Liang, 2017), machine translation (Burlot and Yvon, 2017) and language model (Marvin and Linzen, 2018) to examine system drawbacks. More recently, Gardner et al. (2020) introduces the concept of “contrast set” and proposes to use it to measure the generalization of different NLP systems. Instead of adversarially evaluate an NLP system, we perform an adversarial metaevaluation of evaluation metrics. Meta-evaluation for Automated Metrics Metaevaluation aims to evaluate the reliability of automated metrics based on their correlation with human judgments (Graham, 2015; Peyrard, 2019; Bhandari et al., 2020). M"
2021.findings-emnlp.179,2020.acl-main.450,0,0.0912437,"Missing"
2021.findings-emnlp.179,P19-1100,1,0.834272,"logies of factuality metrics are limited in nostic test datasets, trained factuality models their opacity–they are opaque to their results, which available: https://github.com/zide05/ are usually holistic scores (e.g., accuracy) and not AdvFact. interpretable. Specifically, different from traditional non-learnable metrics like ROUGE, whose 1 Introduction scores are relatively straightforward to interpret, With the rapid development of neural networks in e.g., lower ROUGE-2 Recall implies fewer bitext summarization (Liu and Lapata, 2019; Liu, grams from reference summaries are covered by 2019; Zhong et al., 2019; Zhang et al., 2019; Lewis generated summaries, there are diverse factors that et al., 2019; Zhong et al., 2020; Liu and Liu, 2021), could lead to lower score of factuality metrics (e.g., especially the use of contextualized pre-trained entity replacement, number inference). However, models (Devlin et al., 2019; Lewis et al., 2019), most of existing meta-evaluation strategies fail to the state-of-the-art performance, measured by auto- tell (i) which types of factual errors the metric evalmated metrics such as ROUGE (Lin, 2004) and uated at hand are better at identifying, (ii) on which BERTSco"
2021.findings-emnlp.179,P19-1085,0,0.0444187,"servations. 2083 3 3.1 Preliminaries Definition of Factuality Although researchers have slightly different definitions of factuality (Maynez et al., 2020; Kryscinski et al., 2020). In this paper, we consider factuality as how well generated summaries are supported by source documents without using any external knowledge. A factual error happens when generated summaries contain salient facts (Kryscinski et al., 2020) that can not be inferred from source documents. The summary sentences that need to be verified are also called claims below to keep consistent with the field of fact verification (Zhou et al., 2019; Schuster et al., 2019; Liu et al., 2020). Models Type Train data M NLI B ERT M NLI ROBERTA M NLI E LECTRA DAE FACT CC F EQA NLI-S NLI-S NLI-S NLI-A NLI-S QA MNLI MNLI MNLI PARANMT-G CNNDM-G QA2D, SQuA Table 1: The model types and training data of factuality metrics. NLI-A and NLI-S represent NLI-based metrics defining facts as dependency arcs and span respectively. PARANMT-G and CNNDM-G mean the automatically generated training data from PARANMT (Wieting and Gimpel, 2018) and CNN/DailyMail (Nallapati et al., 2016) (referred to as CNNDM in the rest of the paper). 3.2 Factuality Metrics There"
2021.findings-emnlp.179,P18-1042,0,0.0147892,"e summary sentences that need to be verified are also called claims below to keep consistent with the field of fact verification (Zhou et al., 2019; Schuster et al., 2019; Liu et al., 2020). Models Type Train data M NLI B ERT M NLI ROBERTA M NLI E LECTRA DAE FACT CC F EQA NLI-S NLI-S NLI-S NLI-A NLI-S QA MNLI MNLI MNLI PARANMT-G CNNDM-G QA2D, SQuA Table 1: The model types and training data of factuality metrics. NLI-A and NLI-S represent NLI-based metrics defining facts as dependency arcs and span respectively. PARANMT-G and CNNDM-G mean the automatically generated training data from PARANMT (Wieting and Gimpel, 2018) and CNN/DailyMail (Nallapati et al., 2016) (referred to as CNNDM in the rest of the paper). 3.2 Factuality Metrics There are two major task formulations of factuality metrics: natural language inference (NLI) and question answering (QA). Model types and training data are summarized in Tab. 1. NLI transferred models Following Falke et al. (2019), we train different factuality checkers (M NLI B ERT, M NLI ROBERTA and M NLI E LEC TRA ) based on BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and ELECTRA (Clark et al., 2019) on MNLI dataset (Williams et al., 2018). The neutral class sample"
2021.findings-emnlp.179,N18-1101,0,0.0175059,"ing data from PARANMT (Wieting and Gimpel, 2018) and CNN/DailyMail (Nallapati et al., 2016) (referred to as CNNDM in the rest of the paper). 3.2 Factuality Metrics There are two major task formulations of factuality metrics: natural language inference (NLI) and question answering (QA). Model types and training data are summarized in Tab. 1. NLI transferred models Following Falke et al. (2019), we train different factuality checkers (M NLI B ERT, M NLI ROBERTA and M NLI E LEC TRA ) based on BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and ELECTRA (Clark et al., 2019) on MNLI dataset (Williams et al., 2018). The neutral class samples are deleted in the dataset for fair comparison following Goyal and Durrett (2020). 3.2.2 QA-based Metrics The basic idea behind QA-based metrics is whether similar answers can be replied when we ask the same question to a generated summary S and its source document D (Durmus et al., 2020; Wang et al., 2020b). In practice, we use the recently proposed F EQA (Durmus et al., 2020). FEQA It first generates questions based on summary, and answers the questions based on source document and summary separately. Mismatching answers indicate an inconsistency between document"
2021.naacl-main.113,P18-1063,0,0.0166366,"Dou et al., 2020) enhances the performance of BART using additional guidance information, which achieves the current state-of-the-art performance on the CNNDM dataset. PEGASUS (Zhang et al., 2020) achieves competitive performance on various summarization datasets and is the current state-of-the-art on the XSum dataset. To make a comprehensive evaluation of our proposed model, we additionally collect 19 top-scoring systems as base systems on CNNDM.6 In details, for §5.7 we use the following systems: pointer-generator+coverage (See et al., 2017), REFRESH (Narayan et al., 2018b), fastAbsRL-rank (Chen and Bansal, 2018), CNN-LSTM-BiClassifier (Kedzie et al., 2018), CNN-Transformer-BiClassifier (Zhong et al., 2019), CNN-Transformer-Pointer (Zhong et al., 2019), BERT-Transformer-Pointer (Zhong et al., 2019), Bottom-Up (Gehrmann et al., 2018), NeuSum (Zhou et al., 2018), BanditSum (Dong et al., 2018), twoStageRL (Zhang et al., 2019a), preSummAbs (Liu and Lapata, 2019), preSummAbsext (Liu and Lapata, 2019), HeterGraph (Wang et al., 2020), MatchSum (Zhong et al., 2020), Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), T5 (Raffel et al., 2020). Refactor as Meta Learner Both pre-trained Refactor and fine"
2021.naacl-main.113,I11-1153,0,0.490166,"Missing"
2021.naacl-main.113,D15-1011,0,0.0251401,"Missing"
2021.naacl-main.113,2020.acl-main.228,0,0.0354727,"commonly have flexible choices in model archi- tem. By contrast, Hong et al. (2015) focus on sumtectures (Rush et al., 2015; Kedzie et al., 2018), maries generated from different systems and use decoding strategies (Paulus et al., 2018) (e.g. beam a non-neural system combination method to make search) and etc. As a result, even on the same their complementary advantages. Few works exdataset, different selection biases of these choices plore if the complementarity existing in different will lead to diverse system outputs (Kedzie et al., scenarios could be utilized in a unified framework. 2018; Hossain et al., 2020). (ii) Base-Meta Learning Gap: parameterized To combine complementarity of system’s output models between two learning stages are relatively under different setups, researchers have made some independent. For example, Zhou et al. (2017) and preliminary efforts on two-stage learning (Collins Huang et al. (2020) adapt the seq2seq (Sutskever and Koo, 2005; Huang, 2008; González-Rubio et al., 2014) framework as the meta model for com∗ Corresponding author. bination, which takes the outputs of multiple base 1437 Proceedings of the 2021 Conference of the North American Chapter of the Association for"
2021.naacl-main.113,P08-1067,0,0.251165,"works exdataset, different selection biases of these choices plore if the complementarity existing in different will lead to diverse system outputs (Kedzie et al., scenarios could be utilized in a unified framework. 2018; Hossain et al., 2020). (ii) Base-Meta Learning Gap: parameterized To combine complementarity of system’s output models between two learning stages are relatively under different setups, researchers have made some independent. For example, Zhou et al. (2017) and preliminary efforts on two-stage learning (Collins Huang et al. (2020) adapt the seq2seq (Sutskever and Koo, 2005; Huang, 2008; González-Rubio et al., 2014) framework as the meta model for com∗ Corresponding author. bination, which takes the outputs of multiple base 1437 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1437–1448 June 6–11, 2021. ©2021 Association for Computational Linguistics systems as a part of the inputs for machine trans- imental results demonstrate that there exists comlation. As a result, there is no parameter sharing plementarity introduced by decoding algorithms between the meta model and base"
2021.naacl-main.113,2020.acl-main.124,0,0.0162561,"ems by the Ci ∈C view that a summary can be generated by selecting the best combination of document sentences. where D and Ci denote document and summary Therefore, both base and meta systems aim to select representations respectively, which are calculated 1439 by a BERT (Devlin et al., 2019) model. S CORE(·) is a function that measures the similarity between a document and candidate summary. S CORE(D, C) = 2 R(D, C) · P(D, C) R(D, C) + P(D, C) 40000 Count Contextualized Similarity Function To instantiate S CORE(·), we follow the forms as mentioned in Zhang et al. (2019b); Zhao et al. (2019); Gao et al. (2020), which have shown superior performance on measuring semantic similarity between documents and summaries. Specifically, S CORE(·) is defined based on the greedy matching algorithm, which matches every word in one text sequence to the most similar word in another text sequence and vise versa. Given the document embedding matrix D = hd1 , · · · , dk i and the candidate embedding matrix C = hc1 , · · · , cl i encoded by BERT, S CORE(·) can be calculated as: data type meta-train meta-test pre-train 50000 30000 20000 10000 0 0 60 ROUGE-1 80 100 document as possible. Formally, L= XX i wi is the weig"
2021.naacl-main.113,P06-2034,0,0.0197134,"ypes of gaps can be alleviated by promoting communication between Reranking Reranking is a technique to improve the two stages in §4 , and therefore present a new performance by reranking the output of an existparadigm where the base and meta learners are ing system, which has been widely used across parameterized with shared parameters; different NLP tasks, such as constituency parsing (Collins and Koo, 2005; Huang, 2008), depen(3) We have made comprehensive experiments (twenty-two top-scoring systems, four datasets). dency parsing (Zhou et al., 2016; Do and Rehbein, 2020), semantic parsing (Ge and Mooney, 2006; In addition to achieving state-of-the-art results on CNN/DailyMail dataset (§5) by a significant mar- Yin and Neubig, 2019), machine translation (Shen et al., 2004; Mizumoto and Matsumoto, 2016). gin, the efficacy of the proposed Refactor opens up a thought-provoking direction for performance Comparing reranking and stacking, both of them improvement: instead of pursuing a purely end-to- involve two-stage learning and the first stage would end system, a promising exploration is to incorpo- provide multiple candidate outputs as the input for rate different types of inductive biases stage-wise"
2021.naacl-main.113,D18-1443,0,0.0210223,"mance on various summarization datasets and is the current state-of-the-art on the XSum dataset. To make a comprehensive evaluation of our proposed model, we additionally collect 19 top-scoring systems as base systems on CNNDM.6 In details, for §5.7 we use the following systems: pointer-generator+coverage (See et al., 2017), REFRESH (Narayan et al., 2018b), fastAbsRL-rank (Chen and Bansal, 2018), CNN-LSTM-BiClassifier (Kedzie et al., 2018), CNN-Transformer-BiClassifier (Zhong et al., 2019), CNN-Transformer-Pointer (Zhong et al., 2019), BERT-Transformer-Pointer (Zhong et al., 2019), Bottom-Up (Gehrmann et al., 2018), NeuSum (Zhou et al., 2018), BanditSum (Dong et al., 2018), twoStageRL (Zhang et al., 2019a), preSummAbs (Liu and Lapata, 2019), preSummAbsext (Liu and Lapata, 2019), HeterGraph (Wang et al., 2020), MatchSum (Zhong et al., 2020), Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), T5 (Raffel et al., 2020). Refactor as Meta Learner Both pre-trained Refactor and fine-tuned Refactor can be used as a meta system to select the best candidate when we have multiple system summaries. In this work, we explore the following settings: (1) Single System: It considers re-ranking candidate summarie"
2021.naacl-main.113,P11-1127,0,0.669686,"Missing"
2021.naacl-main.113,N18-1065,0,0.0414483,"Missing"
2021.naacl-main.113,D18-1208,0,0.135927,"), which aims to rerank different outputs of one system. Although these methods each play a role in different scenarios, they suffer from following potential limitations: (i) Ad-hoc Methods: most existing methods are designed for a specific scenario. For example, Li et al. (2015) and Narayan et al. (2018b) resort to 1 Introduction reranking techniques to select summary-worthy In neural text summarization, system designers sentences that are usually generated from one syscommonly have flexible choices in model archi- tem. By contrast, Hong et al. (2015) focus on sumtectures (Rush et al., 2015; Kedzie et al., 2018), maries generated from different systems and use decoding strategies (Paulus et al., 2018) (e.g. beam a non-neural system combination method to make search) and etc. As a result, even on the same their complementary advantages. Few works exdataset, different selection biases of these choices plore if the complementarity existing in different will lead to diverse system outputs (Kedzie et al., scenarios could be utilized in a unified framework. 2018; Hossain et al., 2020). (ii) Base-Meta Learning Gap: parameterized To combine complementarity of system’s output models between two learning stage"
2021.naacl-main.113,2020.acl-main.703,0,0.0603674,"istics. Len is the length of tokens. 4.4.1 Refactor as Base Learner The pre-trained Refactor can not only be fine-tuned for a better selection of candidate summaries, but also be regarded as a base system, providing one system output. This feature of Refactor maximizes parameter sharing across the two training stages. WikiHow5 (Koupaee and Wang, 2018) is a largescale dataset constructed from the articles using online WikiHow knowledge base. 4.4.2 Below, we mainly use BART, GSum and PEGASUS as the base systems since they have achieved state-of-the-art performance on at least one dataset. BART (Lewis et al., 2020) is a large pre-trained sequence-to-sequence model that achieves strong performance on the abstractive summarization. GSum (Dou et al., 2020) enhances the performance of BART using additional guidance information, which achieves the current state-of-the-art performance on the CNNDM dataset. PEGASUS (Zhang et al., 2020) achieves competitive performance on various summarization datasets and is the current state-of-the-art on the XSum dataset. To make a comprehensive evaluation of our proposed model, we additionally collect 19 top-scoring systems as base systems on CNNDM.6 In details, for §5.7 we"
2021.naacl-main.113,N15-1145,0,0.0180778,"11; Mizumoto and Matsumoto, 2016), consisting of (i) a base-stage: first generates different outputs under different setups, and (ii) a meta-stage: then aggregates them in diverse ways, exemplified by stacking that uses a high-level model to combine multiple low-level models (Ting and Witten, 1997), or reranking (Collins and Koo, 2005), which aims to rerank different outputs of one system. Although these methods each play a role in different scenarios, they suffer from following potential limitations: (i) Ad-hoc Methods: most existing methods are designed for a specific scenario. For example, Li et al. (2015) and Narayan et al. (2018b) resort to 1 Introduction reranking techniques to select summary-worthy In neural text summarization, system designers sentences that are usually generated from one syscommonly have flexible choices in model archi- tem. By contrast, Hong et al. (2015) focus on sumtectures (Rush et al., 2015; Kedzie et al., 2018), maries generated from different systems and use decoding strategies (Paulus et al., 2018) (e.g. beam a non-neural system combination method to make search) and etc. As a result, even on the same their complementary advantages. Few works exdataset, different"
2021.naacl-main.113,W04-1013,0,0.0766683,"(dot(di , d √ , wi = P (8) ˆ0 )/ d) exp(dot(dj , d 40 Figure 2: ROUGE-1 distributions of the candidates in pretraining stage training set (pre-train), fine-tuning stage training set (meta-train) and fine-tuning stage test set (meta-test) on XSum dataset. (5) where the weighted recall R, precision P are defined as follows:1 P wi maxj cos(di , cj ) P R(D, C) = i + 1, (6) wi i P j maxi cos(di , cj ) + 1, (7) P(D, C) = l 20 max(0, SCORE(D, Cj ) j>i (9) − SCORE(D, Ci ) + (j − i) ∗ λc ) where Ci and Cj denote the i-th and j-th sample of the candidate list which is descendingly sorted by the ROUGE (Lin, 2004) scores between the reference summary Cˆ and candidates. That is, ˆ > ROUGE(Cj , C) ˆ for i < j. ROUGE(Ci , C) λc is the corresponding margin set to 0.01. 4.3 Fine-tuned Refactor In order to fit the distributions of the specific types of input, we then fine-tune Refactor using the outputs generated by the base systems. Specifically, fine-tuning is also based on Eq. 9 where the candidate summaries C are generated by the base systems under different application scenarios. Why does Pre-train and Fine-tune matter? We elaborate on the proposed two-step training using a real case. Fig. 2 depicts the"
2021.naacl-main.113,P17-1099,0,0.062894,"achieves strong performance on the abstractive summarization. GSum (Dou et al., 2020) enhances the performance of BART using additional guidance information, which achieves the current state-of-the-art performance on the CNNDM dataset. PEGASUS (Zhang et al., 2020) achieves competitive performance on various summarization datasets and is the current state-of-the-art on the XSum dataset. To make a comprehensive evaluation of our proposed model, we additionally collect 19 top-scoring systems as base systems on CNNDM.6 In details, for §5.7 we use the following systems: pointer-generator+coverage (See et al., 2017), REFRESH (Narayan et al., 2018b), fastAbsRL-rank (Chen and Bansal, 2018), CNN-LSTM-BiClassifier (Kedzie et al., 2018), CNN-Transformer-BiClassifier (Zhong et al., 2019), CNN-Transformer-Pointer (Zhong et al., 2019), BERT-Transformer-Pointer (Zhong et al., 2019), Bottom-Up (Gehrmann et al., 2018), NeuSum (Zhou et al., 2018), BanditSum (Dong et al., 2018), twoStageRL (Zhang et al., 2019a), preSummAbs (Liu and Lapata, 2019), preSummAbsext (Liu and Lapata, 2019), HeterGraph (Wang et al., 2020), MatchSum (Zhong et al., 2020), Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), T5 (Raffel e"
2021.naacl-main.113,D19-1387,0,0.105169,"on of our proposed model, we additionally collect 19 top-scoring systems as base systems on CNNDM.6 In details, for §5.7 we use the following systems: pointer-generator+coverage (See et al., 2017), REFRESH (Narayan et al., 2018b), fastAbsRL-rank (Chen and Bansal, 2018), CNN-LSTM-BiClassifier (Kedzie et al., 2018), CNN-Transformer-BiClassifier (Zhong et al., 2019), CNN-Transformer-Pointer (Zhong et al., 2019), BERT-Transformer-Pointer (Zhong et al., 2019), Bottom-Up (Gehrmann et al., 2018), NeuSum (Zhou et al., 2018), BanditSum (Dong et al., 2018), twoStageRL (Zhang et al., 2019a), preSummAbs (Liu and Lapata, 2019), preSummAbsext (Liu and Lapata, 2019), HeterGraph (Wang et al., 2020), MatchSum (Zhong et al., 2020), Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), T5 (Raffel et al., 2020). Refactor as Meta Learner Both pre-trained Refactor and fine-tuned Refactor can be used as a meta system to select the best candidate when we have multiple system summaries. In this work, we explore the following settings: (1) Single System: It considers re-ranking candidate summaries generated from a single abstractive system using beam search. (2) Multi-system Summary-level: It is tasked to select the best"
2021.naacl-main.113,N16-1133,0,0.355961,"ss of the Refactor model sheds light on insight for performance improvement. Our system can be directly used by other researchers as an offthe-shelf tool to achieve further performance improvements. We open-source all the code and provide a convenient interface to use it: https://github.com/yixinL7/ Refactoring-Summarization. Figure 1: Illustration of two-stage learning. “Doc, Hypo, Ref” represent “input document, generated hypothesis, gold reference” respectively. “Hypo’” represents texts generated during test phase. ΘBase and ΘMeta represent learnable parameters in two stages. et al., 2011; Mizumoto and Matsumoto, 2016), consisting of (i) a base-stage: first generates different outputs under different setups, and (ii) a meta-stage: then aggregates them in diverse ways, exemplified by stacking that uses a high-level model to combine multiple low-level models (Ting and Witten, 1997), or reranking (Collins and Koo, 2005), which aims to rerank different outputs of one system. Although these methods each play a role in different scenarios, they suffer from following potential limitations: (i) Ad-hoc Methods: most existing methods are designed for a specific scenario. For example, Li et al. (2015) and Narayan et a"
2021.naacl-main.113,K16-1028,0,0.0395664,"st candidate summary from the results of different systems. (3) Multi-system Sentence-level: We also take a step towards the fine-grained fusion of summaries from extractive and abstractive systems. Specifically, here candidate summaries are generated by combining the results of different systems at the sentence level. 5 Experiments 5.1 Datasets We mainly experiment on four datasets, whose statistics are shown in Tab. 1. CNNDM2 (Hermann et al., 2015) is a widely used dataset containing news articles and the associated highlights which are used as the reference summaries. We follow the work of Nallapati et al. (2016) for data preprocessing. XSum3 (Narayan et al., 2018a) contains online articles collected from BBC with highly abstractive one-sentence summaries. PubMed4 (Cohan et al., 2018) contains scientific papers collected from PubMed.com. 2 https://cs.nyu.edu/~kcho/DMQA/ https://github.com/EdinburghNLP/XSum 4 https://github.com/acohan/ long-summarization 3 5.2 Base Systems 5 https://github.com/mahnazkoupaee/ WikiHow-Dataset 6 Since CNNDM is the most popular dataset, we can collect more existing systems on it. 1441 5.3 Baseline Systems System Neural system combinator: We use BERTScore (Zhang et al., 201"
2021.naacl-main.113,D18-1206,0,0.532691,"umoto, 2016), consisting of (i) a base-stage: first generates different outputs under different setups, and (ii) a meta-stage: then aggregates them in diverse ways, exemplified by stacking that uses a high-level model to combine multiple low-level models (Ting and Witten, 1997), or reranking (Collins and Koo, 2005), which aims to rerank different outputs of one system. Although these methods each play a role in different scenarios, they suffer from following potential limitations: (i) Ad-hoc Methods: most existing methods are designed for a specific scenario. For example, Li et al. (2015) and Narayan et al. (2018b) resort to 1 Introduction reranking techniques to select summary-worthy In neural text summarization, system designers sentences that are usually generated from one syscommonly have flexible choices in model archi- tem. By contrast, Hong et al. (2015) focus on sumtectures (Rush et al., 2015; Kedzie et al., 2018), maries generated from different systems and use decoding strategies (Paulus et al., 2018) (e.g. beam a non-neural system combination method to make search) and etc. As a result, even on the same their complementary advantages. Few works exdataset, different selection biases of these"
2021.naacl-main.113,N18-1158,0,0.333037,"umoto, 2016), consisting of (i) a base-stage: first generates different outputs under different setups, and (ii) a meta-stage: then aggregates them in diverse ways, exemplified by stacking that uses a high-level model to combine multiple low-level models (Ting and Witten, 1997), or reranking (Collins and Koo, 2005), which aims to rerank different outputs of one system. Although these methods each play a role in different scenarios, they suffer from following potential limitations: (i) Ad-hoc Methods: most existing methods are designed for a specific scenario. For example, Li et al. (2015) and Narayan et al. (2018b) resort to 1 Introduction reranking techniques to select summary-worthy In neural text summarization, system designers sentences that are usually generated from one syscommonly have flexible choices in model archi- tem. By contrast, Hong et al. (2015) focus on sumtectures (Rush et al., 2015; Kedzie et al., 2018), maries generated from different systems and use decoding strategies (Paulus et al., 2018) (e.g. beam a non-neural system combination method to make search) and etc. As a result, even on the same their complementary advantages. Few works exdataset, different selection biases of these"
2021.naacl-main.113,D15-1044,0,0.0694135,"llins and Koo, 2005), which aims to rerank different outputs of one system. Although these methods each play a role in different scenarios, they suffer from following potential limitations: (i) Ad-hoc Methods: most existing methods are designed for a specific scenario. For example, Li et al. (2015) and Narayan et al. (2018b) resort to 1 Introduction reranking techniques to select summary-worthy In neural text summarization, system designers sentences that are usually generated from one syscommonly have flexible choices in model archi- tem. By contrast, Hong et al. (2015) focus on sumtectures (Rush et al., 2015; Kedzie et al., 2018), maries generated from different systems and use decoding strategies (Paulus et al., 2018) (e.g. beam a non-neural system combination method to make search) and etc. As a result, even on the same their complementary advantages. Few works exdataset, different selection biases of these choices plore if the complementarity existing in different will lead to diverse system outputs (Kedzie et al., scenarios could be utilized in a unified framework. 2018; Hossain et al., 2020). (ii) Base-Meta Learning Gap: parameterized To combine complementarity of system’s output models betw"
2021.naacl-main.113,N04-1023,0,0.353684,"Missing"
2021.naacl-main.113,2020.acl-main.553,1,0.830542,"as base systems on CNNDM.6 In details, for §5.7 we use the following systems: pointer-generator+coverage (See et al., 2017), REFRESH (Narayan et al., 2018b), fastAbsRL-rank (Chen and Bansal, 2018), CNN-LSTM-BiClassifier (Kedzie et al., 2018), CNN-Transformer-BiClassifier (Zhong et al., 2019), CNN-Transformer-Pointer (Zhong et al., 2019), BERT-Transformer-Pointer (Zhong et al., 2019), Bottom-Up (Gehrmann et al., 2018), NeuSum (Zhou et al., 2018), BanditSum (Dong et al., 2018), twoStageRL (Zhang et al., 2019a), preSummAbs (Liu and Lapata, 2019), preSummAbsext (Liu and Lapata, 2019), HeterGraph (Wang et al., 2020), MatchSum (Zhong et al., 2020), Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), T5 (Raffel et al., 2020). Refactor as Meta Learner Both pre-trained Refactor and fine-tuned Refactor can be used as a meta system to select the best candidate when we have multiple system summaries. In this work, we explore the following settings: (1) Single System: It considers re-ranking candidate summaries generated from a single abstractive system using beam search. (2) Multi-system Summary-level: It is tasked to select the best candidate summary from the results of different systems. (3) Multi-sys"
2021.naacl-main.113,P11-1125,0,0.0136618,"s model to combine lower-level models to achieve allows them to share a set of parameters, thereby alleviating the “Base-Meta learning gap”. Besides, greater predictive accuracy (Ting and Witten, 1997). In NLP research, this method has been widely we propose a pretrain-then-finetune paradigm for explored in machine translation (MT) task. TraRefactor that mitigates the “Train-Test distribution ditionally, it is used to improve the performance gap”. In practice, our proposed Refactor can be of statistical MT systems (González-Rubio et al., applied to different scenarios. For example, as a 2011; Watanabe and Sumita, 2011; Duh et al., meta system, it can be used for multiple system 2011; Mizumoto and Matsumoto, 2016). Some combination or single system re-ranking. Our contributions can be briefly summarized as: recent work (Zhou et al., 2017; Huang et al., 2020) also extends this method to neural MT where the (1) We dissect two major factors that influence meta model and base systems are all neural models. the performance of two-stage learning when leverThere is a handful of works about system combiaging the complementarity among different sysnation for summarization (Hong et al., 2015), in tems: (i) Base-Meta"
2021.naacl-main.113,2020.emnlp-demos.6,0,0.0986447,"Missing"
2021.naacl-main.113,P19-1447,0,0.0248755,"ges in §4 , and therefore present a new performance by reranking the output of an existparadigm where the base and meta learners are ing system, which has been widely used across parameterized with shared parameters; different NLP tasks, such as constituency parsing (Collins and Koo, 2005; Huang, 2008), depen(3) We have made comprehensive experiments (twenty-two top-scoring systems, four datasets). dency parsing (Zhou et al., 2016; Do and Rehbein, 2020), semantic parsing (Ge and Mooney, 2006; In addition to achieving state-of-the-art results on CNN/DailyMail dataset (§5) by a significant mar- Yin and Neubig, 2019), machine translation (Shen et al., 2004; Mizumoto and Matsumoto, 2016). gin, the efficacy of the proposed Refactor opens up a thought-provoking direction for performance Comparing reranking and stacking, both of them improvement: instead of pursuing a purely end-to- involve two-stage learning and the first stage would end system, a promising exploration is to incorpo- provide multiple candidate outputs as the input for rate different types of inductive biases stage-wisely the second stage. However, they differ in the way with the same parameterized function. Our exper- how multiple candidate"
2021.naacl-main.113,K19-1074,0,0.228525,"unifies the goal of the base and meta systems by the Ci ∈C view that a summary can be generated by selecting the best combination of document sentences. where D and Ci denote document and summary Therefore, both base and meta systems aim to select representations respectively, which are calculated 1439 by a BERT (Devlin et al., 2019) model. S CORE(·) is a function that measures the similarity between a document and candidate summary. S CORE(D, C) = 2 R(D, C) · P(D, C) R(D, C) + P(D, C) 40000 Count Contextualized Similarity Function To instantiate S CORE(·), we follow the forms as mentioned in Zhang et al. (2019b); Zhao et al. (2019); Gao et al. (2020), which have shown superior performance on measuring semantic similarity between documents and summaries. Specifically, S CORE(·) is defined based on the greedy matching algorithm, which matches every word in one text sequence to the most similar word in another text sequence and vise versa. Given the document embedding matrix D = hd1 , · · · , dk i and the candidate embedding matrix C = hc1 , · · · , cl i encoded by BERT, S CORE(·) can be calculated as: data type meta-train meta-test pre-train 50000 30000 20000 10000 0 0 60 ROUGE-1 80 100 document as p"
2021.naacl-main.113,P19-1100,1,0.877347,"ieves the current state-of-the-art performance on the CNNDM dataset. PEGASUS (Zhang et al., 2020) achieves competitive performance on various summarization datasets and is the current state-of-the-art on the XSum dataset. To make a comprehensive evaluation of our proposed model, we additionally collect 19 top-scoring systems as base systems on CNNDM.6 In details, for §5.7 we use the following systems: pointer-generator+coverage (See et al., 2017), REFRESH (Narayan et al., 2018b), fastAbsRL-rank (Chen and Bansal, 2018), CNN-LSTM-BiClassifier (Kedzie et al., 2018), CNN-Transformer-BiClassifier (Zhong et al., 2019), CNN-Transformer-Pointer (Zhong et al., 2019), BERT-Transformer-Pointer (Zhong et al., 2019), Bottom-Up (Gehrmann et al., 2018), NeuSum (Zhou et al., 2018), BanditSum (Dong et al., 2018), twoStageRL (Zhang et al., 2019a), preSummAbs (Liu and Lapata, 2019), preSummAbsext (Liu and Lapata, 2019), HeterGraph (Wang et al., 2020), MatchSum (Zhong et al., 2020), Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), T5 (Raffel et al., 2020). Refactor as Meta Learner Both pre-trained Refactor and fine-tuned Refactor can be used as a meta system to select the best candidate when we have multiple"
2021.naacl-main.113,N18-1156,0,0.0234814,"atasets and is the current state-of-the-art on the XSum dataset. To make a comprehensive evaluation of our proposed model, we additionally collect 19 top-scoring systems as base systems on CNNDM.6 In details, for §5.7 we use the following systems: pointer-generator+coverage (See et al., 2017), REFRESH (Narayan et al., 2018b), fastAbsRL-rank (Chen and Bansal, 2018), CNN-LSTM-BiClassifier (Kedzie et al., 2018), CNN-Transformer-BiClassifier (Zhong et al., 2019), CNN-Transformer-Pointer (Zhong et al., 2019), BERT-Transformer-Pointer (Zhong et al., 2019), Bottom-Up (Gehrmann et al., 2018), NeuSum (Zhou et al., 2018), BanditSum (Dong et al., 2018), twoStageRL (Zhang et al., 2019a), preSummAbs (Liu and Lapata, 2019), preSummAbsext (Liu and Lapata, 2019), HeterGraph (Wang et al., 2020), MatchSum (Zhong et al., 2020), Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), T5 (Raffel et al., 2020). Refactor as Meta Learner Both pre-trained Refactor and fine-tuned Refactor can be used as a meta system to select the best candidate when we have multiple system summaries. In this work, we explore the following settings: (1) Single System: It considers re-ranking candidate summaries generated from a single ab"
2021.naacl-main.113,P16-1132,0,0.0209612,"pervised text summarization systems. (2) We show these two types of gaps can be alleviated by promoting communication between Reranking Reranking is a technique to improve the two stages in §4 , and therefore present a new performance by reranking the output of an existparadigm where the base and meta learners are ing system, which has been widely used across parameterized with shared parameters; different NLP tasks, such as constituency parsing (Collins and Koo, 2005; Huang, 2008), depen(3) We have made comprehensive experiments (twenty-two top-scoring systems, four datasets). dency parsing (Zhou et al., 2016; Do and Rehbein, 2020), semantic parsing (Ge and Mooney, 2006; In addition to achieving state-of-the-art results on CNN/DailyMail dataset (§5) by a significant mar- Yin and Neubig, 2019), machine translation (Shen et al., 2004; Mizumoto and Matsumoto, 2016). gin, the efficacy of the proposed Refactor opens up a thought-provoking direction for performance Comparing reranking and stacking, both of them improvement: instead of pursuing a purely end-to- involve two-stage learning and the first stage would end system, a promising exploration is to incorpo- provide multiple candidate outputs as the"
2021.naacl-main.113,P17-2060,0,0.150782,".g. beam a non-neural system combination method to make search) and etc. As a result, even on the same their complementary advantages. Few works exdataset, different selection biases of these choices plore if the complementarity existing in different will lead to diverse system outputs (Kedzie et al., scenarios could be utilized in a unified framework. 2018; Hossain et al., 2020). (ii) Base-Meta Learning Gap: parameterized To combine complementarity of system’s output models between two learning stages are relatively under different setups, researchers have made some independent. For example, Zhou et al. (2017) and preliminary efforts on two-stage learning (Collins Huang et al. (2020) adapt the seq2seq (Sutskever and Koo, 2005; Huang, 2008; González-Rubio et al., 2014) framework as the meta model for com∗ Corresponding author. bination, which takes the outputs of multiple base 1437 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1437–1448 June 6–11, 2021. ©2021 Association for Computational Linguistics systems as a part of the inputs for machine trans- imental results demonstrate that there exists c"
2021.naacl-main.113,D19-1053,0,0.0868285,"e base and meta systems by the Ci ∈C view that a summary can be generated by selecting the best combination of document sentences. where D and Ci denote document and summary Therefore, both base and meta systems aim to select representations respectively, which are calculated 1439 by a BERT (Devlin et al., 2019) model. S CORE(·) is a function that measures the similarity between a document and candidate summary. S CORE(D, C) = 2 R(D, C) · P(D, C) R(D, C) + P(D, C) 40000 Count Contextualized Similarity Function To instantiate S CORE(·), we follow the forms as mentioned in Zhang et al. (2019b); Zhao et al. (2019); Gao et al. (2020), which have shown superior performance on measuring semantic similarity between documents and summaries. Specifically, S CORE(·) is defined based on the greedy matching algorithm, which matches every word in one text sequence to the most similar word in another text sequence and vise versa. Given the document embedding matrix D = hd1 , · · · , dk i and the candidate embedding matrix C = hc1 , · · · , cl i encoded by BERT, S CORE(·) can be calculated as: data type meta-train meta-test pre-train 50000 30000 20000 10000 0 0 60 ROUGE-1 80 100 document as possible. Formally, L="
2021.naacl-main.115,D14-1162,0,0.081962,"as achieved larger improvement by average, which can be further enhanced by introducing contextualized pre-trained models (e.g. BERT). 3) Incorporating larger-context information with some aggregators also can lead to performance drop on some datasets (e.g, using graph aggregaSettings and Hyper-parameters We adopt CNN-LSTM-CRF as a prototype and augment it with larger-context information by four categories of aggregators: bow, seq, graph, and cPre-seq. We use Word2Vec (Mikolov et al., 2013) (trained on simplified Chinese Wikipedia dump) as noncontextualized embeddings for CWS task, and GloVe (Pennington et al., 2014) for NER, Chunk, and POS tasks. The window size (the number of sentence) k of larger-context aggregators will be explored with a range of k = {1, 2, 3, 4, 5, 6, 10} for seq, bow, and cPre-seq. We chose the best performance that the larger-context aggregator achieved with window 1466 4 The settings of window size k are listed in the appendix. CWS Emb. Agg. NER CITYU NCC SXU PKU CN03 92.26 +0.42 -0.61 +0.34 94.94 +0.03 -0.02 +0.18 94.35 +0.04 +0.33 +0.08 90.46 -0.39 +1.47 -0.14 Chunk POS BC BN MZ WB NW TC 75.38 +1.66 +0.17 +0.65 86.89 +0.32 +0.42 -0.50 85.42 +1.51 -0.16 +1.49 62.09 +3.49 +4.84 +"
2021.naacl-main.115,N18-1202,0,0.0996018,"apid development of deep neural models has and Hovy, 2016; Lample et al., 2016) (RNNs) or shown impressive performances on sequence tag- graph topology by graph neural networks (Kipf ging tasks that aim to assign labels to each token and Welling, 2016; Schlichtkrull et al., 2018). of an input sequence (Sang and De Meulder, 2003; Understanding the discrepancies of these aggreLample et al., 2016; Ma and Hovy, 2016). More gators can help us reach a more generalized conrecently, the use of unsupervised pre-trained modclusion about the effectiveness of larger-context els (Akbik et al., 2018, 2019; Peters et al., 2018; training. To this end, we study larger-context aggreDevlin et al., 2018) (especially contextualized vergators with three different structural priors (defined sion) has driven state-of-the-art performance to a in Sec. 3.2) and comprehensively evaluate their ∗ Corresponding author efficacy. 1463 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1463–1475 June 6–11, 2021. ©2021 Association for Computational Linguistics Q2: Can the larger-context training easily play to its strengths with the help"
2021.naacl-main.115,W03-0419,0,0.170672,"Missing"
2021.naacl-main.115,D17-1283,0,0.0391961,"Missing"
2021.naacl-main.115,D19-1585,0,0.0360579,"Missing"
2021.naacl-main.115,C18-1327,0,0.0153608,"haddar and Langlais, 2018) that utilize different domains of this dataset, which also paves the way for our fine-grained analysis. Chinese Word Segmentation (CWS) We use four mainstream datasets from SIGHAN2005 and SIGHAN2008, in which CITYU is traditional Chinese, while PKU, NCC, and SXU are simplified ones. Chunking (Chunk) CoNLL-2000 (CN00) is a benchmark dataset for text chunking. Part-of-Speech (POS) We use the Penn Treebank (PTB) III dataset for POS tagging.2 2.3 Neural Tagging Models Despite the emergence of a bunch of architectural explorations (Ma and Hovy, 2016; Lample et al., 2016; Yang et al., 2018; Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2018) for sequence tagging, two general frameworks can be summarized: (i) cEnc-wEnc-CRF consists of the wordlevel encoder, sentence-level encoder, and CRF 2 It’s hard to cover all datasets for all tasks. For Chunk and POS tasks, we adopt the two most popular benchmark datasets. 1464 layer (Lafferty et al., 2001); (ii) ContPre-MLP is composed of a contextualized pre-trained layer, followed by an MLP or CRF layer. In this paper, we take both frameworks as study objects for our three research questions first, 3 and instantiate them as two"
2021.naacl-main.115,2021.eacl-main.324,1,0.680233,"Missing"
2021.naacl-main.115,2020.acl-main.306,0,0.0200424,"Missing"
2021.naacl-main.146,K19-1015,0,0.0148424,"ency tree, pre-trained modone or more aspects in a sentence, the task calls for detecting the sentiment polarities for all aspects. els (PTMs) (Qiu et al., 2020), such as BERT (Devlin et al., 2019), have also been used to enhance Take the sentence “great food but the service was dreadful” for example, the task is to predict the sen- the performance of the ALSC task (Sun et al., timents towards the underlined aspects, which ex- 2019a; Tang et al., 2020; Phan and Ogunbona, pects to get polarity positive for aspect food and po- 2020; Wang et al., 2020). From the view of interpretability of PTMs, Chen et al. (2019); Hewitt larity negative for aspect service. Generally, ABSA and Manning (2019); Wu et al. (2020) try to use ∗ Equal contribution. probing methods to detect syntactic information in † Corresponding author. 1 PTMs. Empirical results reveal that PTMs capture Our code will be released at https://github.com/ ROGERDJQ/RoBERTaABSA. some kind of dependency tree structures implicitly. 1816 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1816–1829 June 6–11, 2021. ©2021 Association for Computational Li"
2021.naacl-main.146,C96-1058,0,0.701005,"ERTa) to replace the token xi , which returns a representation Hθ (x{xi })i for the masked xi ; secondly, it further masks the token xj , which returns a representation Hθ (x{xi , xj })i with both xi , xj being masked. The impact value f (xi , xj ) is calculated by the Euclidean distance as follows, f(xi ,xj) = ||Hθ (x{xi })i −Hθ (x{xi ,xj})i ||2 (3) By repeating this process between every two tokens in the sentence, we can get an impact matrix 3.1.1 BERT and RoBERTa M ∈ RT ×T and Mi,j = f (xi , xj ). The tree deBERT (Devlin et al., 2019) and RoBERTa (Liu coding algorithm, such as Eisner (Eisner, 1996) et al., 2019) both take Transformers (Vaswani et al., and Chu–Liu/Edmonds’ algorithm (Chu and Liu, 2017) as backbone architecture. Generally, they 1965; Edmonds, 1967), is then used to extract the 1818 dependency tree from the matrix M. The Perturbed Masking can exert on any layer of BERT or RoBERTa. Dataset Split Positive Negative Neutral Rest14 Train Test 2164 728 807 196 637 196 3.2 Laptop14 Train Test 994 341 870 128 464 169 Twitter Train Test 1561 173 1560 173 3127 346 ALSC Models Based on Trees In this subsection, we introduce three representative tree-based ALSC models. Each of the mod"
2021.naacl-main.146,2021.ccl-1.108,0,0.104678,"Missing"
2021.naacl-main.146,D14-1162,0,0.0844928,"Missing"
2021.naacl-main.146,2020.acl-main.293,0,0.363911,"eriments also show that the pure RoBERTa-based the topological structure of the dependency tree model can outperform or approximate to the (Dong et al., 2014; Zhang et al., 2019a; Huang and previous SOTA performances on six datasets Carley, 2019; Sun et al., 2019b; Zheng et al., 2020; across four languages since it implicitly inTang et al., 2020); The second one is to use the treecorporates the task-oriented syntactic informabased distance, which counts the number of edges tion.1 in a shortest path between two tokens in the dependency tree (He et al., 2018; Zhang et al., 2019b; 1 Introduction Phan and Ogunbona, 2020); The third one is to Aspect-based sentiment analysis (ABSA) aims to simultaneously use both the topological structure do the fine-grained sentiment analysis towards asand the tree-based distance. pects (Pontiki et al., 2014, 2016). Specifically, for Except for the dependency tree, pre-trained modone or more aspects in a sentence, the task calls for detecting the sentiment polarities for all aspects. els (PTMs) (Qiu et al., 2020), such as BERT (Devlin et al., 2019), have also been used to enhance Take the sentence “great food but the service was dreadful” for example, the task is to predict th"
2021.naacl-main.146,P11-1016,0,0.0710812,"19,txsun19,xpqiu}@fudan.edu.cn pliu3@cs.cmu.edu Abstract contains aspect extraction (AE) and aspect-level sentiment classification (ALSC). We only focus on Aspect-Based Sentiment Analysis (ABSA), the ALSC task. aiming at predicting the polarities for aspects, Early works of ALSC mainly rely on manuis a fine-grained task in the field of sentiment ally designed syntactic features, which is laboranalysis. Previous work showed syntactic information, e.g. dependency trees, can effecintensive yet insufficient. In order to avoid detively improve the ABSA performance. Resigning hand-crafted features (Jiang et al., 2011; cently, pre-trained models (PTMs) also have Kiritchenko et al., 2014), various neural network shown their effectiveness on ABSA. Theremodels have been proposed in ALSC (Dong et al., fore, the question naturally arises whether 2014; Vo and Zhang, 2015; Wang et al., 2016; Chen PTMs contain sufficient syntactic information et al., 2017; He et al., 2018; Zhang et al., 2019b; for ABSA so that we can obtain a good Wang et al., 2020). Since the dependency tree can ABSA model only based on PTMs. In this paper, we firstly compare the induced trees help the aspects find their contextual words, most fr"
2021.naacl-main.146,S14-2076,0,0.194596,"s aspect extraction (AE) and aspect-level sentiment classification (ALSC). We only focus on Aspect-Based Sentiment Analysis (ABSA), the ALSC task. aiming at predicting the polarities for aspects, Early works of ALSC mainly rely on manuis a fine-grained task in the field of sentiment ally designed syntactic features, which is laboranalysis. Previous work showed syntactic information, e.g. dependency trees, can effecintensive yet insufficient. In order to avoid detively improve the ABSA performance. Resigning hand-crafted features (Jiang et al., 2011; cently, pre-trained models (PTMs) also have Kiritchenko et al., 2014), various neural network shown their effectiveness on ABSA. Theremodels have been proposed in ALSC (Dong et al., fore, the question naturally arises whether 2014; Vo and Zhang, 2015; Wang et al., 2016; Chen PTMs contain sufficient syntactic information et al., 2017; He et al., 2018; Zhang et al., 2019b; for ABSA so that we can obtain a good Wang et al., 2020). Since the dependency tree can ABSA model only based on PTMs. In this paper, we firstly compare the induced trees help the aspects find their contextual words, most from PTMs and the dependency parsing trees of the recently proposed State"
2021.naacl-main.146,S14-2004,0,0.560513,"sets Carley, 2019; Sun et al., 2019b; Zheng et al., 2020; across four languages since it implicitly inTang et al., 2020); The second one is to use the treecorporates the task-oriented syntactic informabased distance, which counts the number of edges tion.1 in a shortest path between two tokens in the dependency tree (He et al., 2018; Zhang et al., 2019b; 1 Introduction Phan and Ogunbona, 2020); The third one is to Aspect-based sentiment analysis (ABSA) aims to simultaneously use both the topological structure do the fine-grained sentiment analysis towards asand the tree-based distance. pects (Pontiki et al., 2014, 2016). Specifically, for Except for the dependency tree, pre-trained modone or more aspects in a sentence, the task calls for detecting the sentiment polarities for all aspects. els (PTMs) (Qiu et al., 2020), such as BERT (Devlin et al., 2019), have also been used to enhance Take the sentence “great food but the service was dreadful” for example, the task is to predict the sen- the performance of the ALSC task (Sun et al., timents towards the underlined aspects, which ex- 2019a; Tang et al., 2020; Phan and Ogunbona, pects to get polarity positive for aspect food and po- 2020; Wang et al., 20"
2021.naacl-main.146,P18-1087,0,0.0604417,"Missing"
2021.naacl-main.146,E17-2091,0,0.0219867,"nguistics: Human Language Technologies, pages 1816–1829 June 6–11, 2021. ©2021 Association for Computational Linguistics Therefore, two following questions arise naturally. 2 Related Work ALSC without Dependencies Vo and Zhang (2015) propose the early neural network model which does not rely on the dependency tree. Along this line, diverse neural network models have been proposed. Tang et al. (2016a) use the long short term memory (LSTM) network to enhance the interactions between aspects and context words. In order to model relations of aspects and their contextual words, Wang et al. (2016); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based Q2: Will PTMs adapt the implicitly entailed tree structure to the ALSC task during the fine- neural network models. Other model structures such as convolutional neural network (CNN) (Li tuning? Therefore, in this paper, we not only use the trees induced from the off-the-shelf PTMs to en- et al., 2018; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memhance ALSC models, but also use the trees induced from the fine-tuned PTMs (In short FT-PTMs) ory neural network (Tang e"
2021.naacl-main.146,D16-1103,0,0.0421316,"Missing"
2021.naacl-main.146,N19-1035,1,0.948195,"inion words (Wang et al., 2020; Sun et al., the parser-provided tree. The further analy2019b; Zhang et al., 2019b). Generally, these sis experiments reveal that the FT-RoBERTa dependency tree based ALSC models are impleInduced Tree is more sentiment-word-oriented mented in three methods. The first one is to use and could benefit the ABSA task. The experiments also show that the pure RoBERTa-based the topological structure of the dependency tree model can outperform or approximate to the (Dong et al., 2014; Zhang et al., 2019a; Huang and previous SOTA performances on six datasets Carley, 2019; Sun et al., 2019b; Zheng et al., 2020; across four languages since it implicitly inTang et al., 2020); The second one is to use the treecorporates the task-oriented syntactic informabased distance, which counts the number of edges tion.1 in a shortest path between two tokens in the dependency tree (He et al., 2018; Zhang et al., 2019b; 1 Introduction Phan and Ogunbona, 2020); The third one is to Aspect-based sentiment analysis (ABSA) aims to simultaneously use both the topological structure do the fine-grained sentiment analysis towards asand the tree-based distance. pects (Pontiki et al., 2014, 2016). Specif"
2021.naacl-main.146,2020.acl-main.295,0,0.704181,"ctic information, e.g. dependency trees, can effecintensive yet insufficient. In order to avoid detively improve the ABSA performance. Resigning hand-crafted features (Jiang et al., 2011; cently, pre-trained models (PTMs) also have Kiritchenko et al., 2014), various neural network shown their effectiveness on ABSA. Theremodels have been proposed in ALSC (Dong et al., fore, the question naturally arises whether 2014; Vo and Zhang, 2015; Wang et al., 2016; Chen PTMs contain sufficient syntactic information et al., 2017; He et al., 2018; Zhang et al., 2019b; for ABSA so that we can obtain a good Wang et al., 2020). Since the dependency tree can ABSA model only based on PTMs. In this paper, we firstly compare the induced trees help the aspects find their contextual words, most from PTMs and the dependency parsing trees of the recently proposed State-of-the-art (SOTA) on several popular models for the ABSA ALSC models utilize the dependency tree to astask, showing that the induced tree from finesist in modeling connections between aspects and tuned RoBERTa (FT-RoBERTa) outperforms their opinion words (Wang et al., 2020; Sun et al., the parser-provided tree. The further analy2019b; Zhang et al., 2019b). G"
2021.naacl-main.146,P18-1088,0,0.041026,"he LSTM-based Q2: Will PTMs adapt the implicitly entailed tree structure to the ALSC task during the fine- neural network models. Other model structures such as convolutional neural network (CNN) (Li tuning? Therefore, in this paper, we not only use the trees induced from the off-the-shelf PTMs to en- et al., 2018; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memhance ALSC models, but also use the trees induced from the fine-tuned PTMs (In short FT-PTMs) ory neural network (Tang et al., 2016b; Chen et al., which are fine-tuned on the ALSC datasets. Ex- 2017; Wang et al., 2018), attention neural network (Tang et al., 2019) have also been applied in ALSC. periments show that trees induced from FT-PTMs ALSC with Dependencies Early works of ALSC can help tree-based ALSC models achieve better performance than their counterparts before fine- mainly employ traditional text classification methods focusing on machine learning algorithms and tuning. Besides, models with trees induced from the ALSC fine-tuned RoBERTa can even outper- manually designed features, which took syntactic structures into consideration from the very beginform trees from the dependency parser. ning. K"
2021.naacl-main.146,D16-1058,0,0.450812,"of ALSC mainly rely on manuis a fine-grained task in the field of sentiment ally designed syntactic features, which is laboranalysis. Previous work showed syntactic information, e.g. dependency trees, can effecintensive yet insufficient. In order to avoid detively improve the ABSA performance. Resigning hand-crafted features (Jiang et al., 2011; cently, pre-trained models (PTMs) also have Kiritchenko et al., 2014), various neural network shown their effectiveness on ABSA. Theremodels have been proposed in ALSC (Dong et al., fore, the question naturally arises whether 2014; Vo and Zhang, 2015; Wang et al., 2016; Chen PTMs contain sufficient syntactic information et al., 2017; He et al., 2018; Zhang et al., 2019b; for ABSA so that we can obtain a good Wang et al., 2020). Since the dependency tree can ABSA model only based on PTMs. In this paper, we firstly compare the induced trees help the aspects find their contextual words, most from PTMs and the dependency parsing trees of the recently proposed State-of-the-art (SOTA) on several popular models for the ABSA ALSC models utilize the dependency tree to astask, showing that the induced tree from finesist in modeling connections between aspects and tun"
2021.naacl-main.146,2020.acl-main.383,0,0.320473,"ment polarities for all aspects. els (PTMs) (Qiu et al., 2020), such as BERT (Devlin et al., 2019), have also been used to enhance Take the sentence “great food but the service was dreadful” for example, the task is to predict the sen- the performance of the ALSC task (Sun et al., timents towards the underlined aspects, which ex- 2019a; Tang et al., 2020; Phan and Ogunbona, pects to get polarity positive for aspect food and po- 2020; Wang et al., 2020). From the view of interpretability of PTMs, Chen et al. (2019); Hewitt larity negative for aspect service. Generally, ABSA and Manning (2019); Wu et al. (2020) try to use ∗ Equal contribution. probing methods to detect syntactic information in † Corresponding author. 1 PTMs. Empirical results reveal that PTMs capture Our code will be released at https://github.com/ ROGERDJQ/RoBERTaABSA. some kind of dependency tree structures implicitly. 1816 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1816–1829 June 6–11, 2021. ©2021 Association for Computational Linguistics Therefore, two following questions arise naturally. 2 Related Work ALSC without Depende"
2021.naacl-main.146,2020.emnlp-main.183,0,0.215931,"Missing"
2021.naacl-main.146,P18-1234,0,0.0140387,"short term memory (LSTM) network to enhance the interactions between aspects and context words. In order to model relations of aspects and their contextual words, Wang et al. (2016); Liu and Zhang (2017); Ma et al. (2017); Tay et al. (2018) incorporate the attention mechanism into the LSTM-based Q2: Will PTMs adapt the implicitly entailed tree structure to the ALSC task during the fine- neural network models. Other model structures such as convolutional neural network (CNN) (Li tuning? Therefore, in this paper, we not only use the trees induced from the off-the-shelf PTMs to en- et al., 2018; Xue and Li, 2018), gated neural network (Zhang et al., 2016; Xue and Li, 2018), memhance ALSC models, but also use the trees induced from the fine-tuned PTMs (In short FT-PTMs) ory neural network (Tang et al., 2016b; Chen et al., which are fine-tuned on the ALSC datasets. Ex- 2017; Wang et al., 2018), attention neural network (Tang et al., 2019) have also been applied in ALSC. periments show that trees induced from FT-PTMs ALSC with Dependencies Early works of ALSC can help tree-based ALSC models achieve better performance than their counterparts before fine- mainly employ traditional text classification metho"
2021.naacl-main.146,D19-1464,0,0.473499,"Missing"
2021.naacl-main.384,N18-2097,0,0.0172867,"18a) is an abstractive dataset that contains one-sentence summaries of online articles from BBC. CNN/DM (Hermann et al., 2015; Nallapati et al., 2016) is a widely-used summarization dataset consisting of news articles and associated highlights as summaries. We use its non-anonymized version. WikiHow (Koupaee and Wang, 2018) is extracted from an online knowledge base and requires high level of abstraction. New York Times (NYT) (Sandhaus, 2008) is a dataset that consists of news articles and their associated summaries.3 We follow Kedzie et al. (2018) to preprocess and split the dataset. PubMed (Cohan et al., 2018) is relatively extractive and is collected from scientific papers. 4.2 Guide R-1 R-2 R-L - 43.25 41.72 41.58 20.24 19.39 18.99 39.63 38.76 38.56 BertAbs + Sentence Auto. Oracle 43.78 55.18 20.66 32.54 40.66 52.06 BertAbs + Keyword Auto. Oracle 42.21 45.08 19.36 22.22 39.23 42.07 BertAbs + Relation Auto. Oracle 41.40 45.96 18.66 23.09 38.40 42.92 BertAbs + Retrieve Auto. Oracle 40.88 43.69 18.24 20.53 37.99 40.71 ∗ BertExt (Base) BertAbs∗ BertAbs (Ours) Ours Table 3: Results (ROUGE; Lin (2004)) on CNN/DM. “Auto” and “oracle” denote using automatically predicted and oracle-extracted guidance at"
2021.naacl-main.384,D18-1208,0,0.0280179,"we extract for extractive summarization. XSum (Narayan et al., 2018a) is an abstractive dataset that contains one-sentence summaries of online articles from BBC. CNN/DM (Hermann et al., 2015; Nallapati et al., 2016) is a widely-used summarization dataset consisting of news articles and associated highlights as summaries. We use its non-anonymized version. WikiHow (Koupaee and Wang, 2018) is extracted from an online knowledge base and requires high level of abstraction. New York Times (NYT) (Sandhaus, 2008) is a dataset that consists of news articles and their associated summaries.3 We follow Kedzie et al. (2018) to preprocess and split the dataset. PubMed (Cohan et al., 2018) is relatively extractive and is collected from scientific papers. 4.2 Guide R-1 R-2 R-L - 43.25 41.72 41.58 20.24 19.39 18.99 39.63 38.76 38.56 BertAbs + Sentence Auto. Oracle 43.78 55.18 20.66 32.54 40.66 52.06 BertAbs + Keyword Auto. Oracle 42.21 45.08 19.36 22.22 39.23 42.07 BertAbs + Relation Auto. Oracle 41.40 45.96 18.66 23.09 38.40 42.92 BertAbs + Retrieve Auto. Oracle 40.88 43.69 18.24 20.53 37.99 40.71 ∗ BertExt (Base) BertAbs∗ BertAbs (Ours) Ours Table 3: Results (ROUGE; Lin (2004)) on CNN/DM. “Auto” and “oracle” denot"
2021.naacl-main.384,D16-1140,1,0.776214,"methods for guided neural abstractive summarization: methods that provide various types of guidance signals that 1) constrain the summary so that the output content will deviate less from 1 Introduction the source document; 2) allow for controllability Modern techniques for text summarization gener- through provision of user-specified inputs. ally can be categorized as either extractive methThere have been some previous methods for ods (Nallapati et al., 2017; Narayan et al., 2018b; guiding neural abstractive summarization models. Zhou et al., 2018), which identify the most suit- For example, Kikuchi et al. (2016) specify the able words or sentences from the input document length of abstractive summaries, Li et al. (2018) and concatenate them to form a summary, or ab- provide models with keywords to prevent the model stractive methods (Rush et al., 2015; Chopra et al., from missing key information, and Cao et al. 2016; Nallapati et al., 2016; Paulus et al., 2018), (2018) propose models that retrieve and reference which generate summaries freely and are able to relevant summaries from the training set. While produce novel words and sentences. Compared these methods have demonstrated improvements with ex"
2021.naacl-main.384,D15-1044,0,0.0697521,"lity Modern techniques for text summarization gener- through provision of user-specified inputs. ally can be categorized as either extractive methThere have been some previous methods for ods (Nallapati et al., 2017; Narayan et al., 2018b; guiding neural abstractive summarization models. Zhou et al., 2018), which identify the most suit- For example, Kikuchi et al. (2016) specify the able words or sentences from the input document length of abstractive summaries, Li et al. (2018) and concatenate them to form a summary, or ab- provide models with keywords to prevent the model stractive methods (Rush et al., 2015; Chopra et al., from missing key information, and Cao et al. 2016; Nallapati et al., 2016; Paulus et al., 2018), (2018) propose models that retrieve and reference which generate summaries freely and are able to relevant summaries from the training set. While produce novel words and sentences. Compared these methods have demonstrated improvements with extractive algorithms, abstractive algorithms in summarization quality and controllability, each are more flexible, making them more likely to pro- focuses on one particular type of guidance – it reduce fluent and coherent summaries. However, the"
2021.naacl-main.384,D19-1387,0,0.410408,"attention to them at test time. With this in mind, we describe the four varieties of guidance signal we experiment with, along with their automatic and oracle extraction methods. Highlighted Sentences. The success of extractive approaches have demonstrated that we can extract a subset of sentences {xi1 , · · · , xin } from the source document and concatenate them to form a summary. Inspired by this, we explicitly inform our model which subset of source sentences should be highlighted using extractive models. We perform oracle extraction using a greedy search algorithm (Nallapati et al., 2017; Liu and Lapata, 2019) to find a set of sentences in the source document that have the highest ROUGE scores with the reference (detailed in Appendix) and treat these as our guidance g. At test time, we use pretrained extractive summarization models (BertExt (Liu and Lapata, 2019) or MatchSum (Zhong et al., 2020) in our experiments) to perform automatic prediction. cur in an actual summary, which could distract the model from focusing on the desired aspects of the input. Therefore, we also try to feed our model with a set of individual keywords {w1 , . . . , wn } from the source document. For oracle extraction, we f"
2021.naacl-main.384,P17-1099,0,0.123549,"t summaries. 2 Background and Related Work Neural abstractive summarization typically takes a source document x consisting of multiple sentences x1 , · · · , x|x |, runs them through an encoder to generate representations, and passes them to a decoder that outputs the summary y one target word at a time. Model parameters θ are trained to maximize the conditional likelihood of the outputs in a parallel training corpus hX , Yi: X arg max log p(yi |xi ; θ). θ hxi ,yi i∈hX ,Yi Several techniques have been proposed to improve the model architecture. For example, models of copying (Gu et al., 2016; See et al., 2017; Gehrmann et al., 2018) allow words to be copied directly from the input to the output, and models of coverage discourage the model from generating repetitive words (See et al., 2017). We evaluate our methods on 6 popular summarization benchmarks. Our best model, using highlighted sentences as guidance, can achieve stateof-the-art performance on 4 out of the 6 datasets, Guidance can be defined as some variety of sigincluding 1.28/0.79/1.13 ROUGE-1/2/L improve- nal g that is fed into the model in addition to the source document x: ments over previous state-of-the-art model on the X widely-used"
2021.naacl-main.384,D18-1444,0,0.0619156,"an also result in problems. First, it can result In this paper, we propose a general and exten1 sible guided summarization framework that can Code is available at https://github.com/ neulab/guided_summarization. take different kinds of external guidance as in4830 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4830–4842 June 6–11, 2021. ©2021 Association for Computational Linguistics Guidance Form Work Tokens Triples Sentences Summaries Kikuchi et al. (2016) Cao et al. (2018) Li et al. (2018) Liu et al. (2018a) Liu et al. (2018b) Fan et al. (2018) Zhu et al. (2020) Jin et al. (2020) Saito et al. (2020) 3 (length tokens) 7 3 (keywords) 7 3 (length tokens) 3 (length, entity, style tokens) 7 7 3 (keywords) 7 7 7 7 7 7 3 (relations) 3 (relations) 7 7 7 7 3 (highlighted sents.) 7 7 7 7 3 (highlighted sents.) 7 3 (retrieved sums.) 7 7 7 7 7 7 7 Ours 3 (keywords) 3 (relations) 3 (highlighted sents.) 3 (retrieved sums.) Table 1: A comparison of different guided neural abstractive summarization models. Previous works have tried to provide guidance in different forms, including tokens, triples, sentences an"
2021.naacl-main.384,W04-3252,0,0.124575,"led in Appendix) and treat these as our guidance g. At test time, we use pretrained extractive summarization models (BertExt (Liu and Lapata, 2019) or MatchSum (Zhong et al., 2020) in our experiments) to perform automatic prediction. cur in an actual summary, which could distract the model from focusing on the desired aspects of the input. Therefore, we also try to feed our model with a set of individual keywords {w1 , . . . , wn } from the source document. For oracle extraction, we first use the greedy search algorithm mentioned above to select a subset of input sentences, then use TextRank (Mihalcea and Tarau, 2004) to extract keywords from these sentences. We also filter the keywords that are not in the target summary. The remaining keywords are then fed to our models. For automatic prediction, we use another neural model (BertAbs (Liu and Lapata, 2019) in the experiments) to predict the keywords in the target summary. Relations. Relations are typically represented in the form of relational triples, with each triple containing a subject, a relation, and an object. For example, Barack Obama was born in Hawaii will create a triple (Barack Obama, was born in, Hawaii). For oracle extraction, we first use St"
2021.naacl-main.384,2020.acl-main.552,1,0.945898,"ences {xi1 , · · · , xin } from the source document and concatenate them to form a summary. Inspired by this, we explicitly inform our model which subset of source sentences should be highlighted using extractive models. We perform oracle extraction using a greedy search algorithm (Nallapati et al., 2017; Liu and Lapata, 2019) to find a set of sentences in the source document that have the highest ROUGE scores with the reference (detailed in Appendix) and treat these as our guidance g. At test time, we use pretrained extractive summarization models (BertExt (Liu and Lapata, 2019) or MatchSum (Zhong et al., 2020) in our experiments) to perform automatic prediction. cur in an actual summary, which could distract the model from focusing on the desired aspects of the input. Therefore, we also try to feed our model with a set of individual keywords {w1 , . . . , wn } from the source document. For oracle extraction, we first use the greedy search algorithm mentioned above to select a subset of input sentences, then use TextRank (Mihalcea and Tarau, 2004) to extract keywords from these sentences. We also filter the keywords that are not in the target summary. The remaining keywords are then fed to our model"
2021.naacl-main.384,K16-1028,0,0.432985,"ied inputs. ally can be categorized as either extractive methThere have been some previous methods for ods (Nallapati et al., 2017; Narayan et al., 2018b; guiding neural abstractive summarization models. Zhou et al., 2018), which identify the most suit- For example, Kikuchi et al. (2016) specify the able words or sentences from the input document length of abstractive summaries, Li et al. (2018) and concatenate them to form a summary, or ab- provide models with keywords to prevent the model stractive methods (Rush et al., 2015; Chopra et al., from missing key information, and Cao et al. 2016; Nallapati et al., 2016; Paulus et al., 2018), (2018) propose models that retrieve and reference which generate summaries freely and are able to relevant summaries from the training set. While produce novel words and sentences. Compared these methods have demonstrated improvements with extractive algorithms, abstractive algorithms in summarization quality and controllability, each are more flexible, making them more likely to pro- focuses on one particular type of guidance – it reduce fluent and coherent summaries. However, the mains unclear which is better and whether they are unconstrained nature of abstractive su"
2021.naacl-main.384,D18-1206,0,0.236389,"hard to pick in advance which aspects of the original content an abstractive system may touch upon. To address the issues, we propose methods for guided neural abstractive summarization: methods that provide various types of guidance signals that 1) constrain the summary so that the output content will deviate less from 1 Introduction the source document; 2) allow for controllability Modern techniques for text summarization gener- through provision of user-specified inputs. ally can be categorized as either extractive methThere have been some previous methods for ods (Nallapati et al., 2017; Narayan et al., 2018b; guiding neural abstractive summarization models. Zhou et al., 2018), which identify the most suit- For example, Kikuchi et al. (2016) specify the able words or sentences from the input document length of abstractive summaries, Li et al. (2018) and concatenate them to form a summary, or ab- provide models with keywords to prevent the model stractive methods (Rush et al., 2015; Chopra et al., from missing key information, and Cao et al. 2016; Nallapati et al., 2016; Paulus et al., 2018), (2018) propose models that retrieve and reference which generate summaries freely and are able to relevant"
2021.naacl-main.384,P18-1061,0,0.020999,"ctive system may touch upon. To address the issues, we propose methods for guided neural abstractive summarization: methods that provide various types of guidance signals that 1) constrain the summary so that the output content will deviate less from 1 Introduction the source document; 2) allow for controllability Modern techniques for text summarization gener- through provision of user-specified inputs. ally can be categorized as either extractive methThere have been some previous methods for ods (Nallapati et al., 2017; Narayan et al., 2018b; guiding neural abstractive summarization models. Zhou et al., 2018), which identify the most suit- For example, Kikuchi et al. (2016) specify the able words or sentences from the input document length of abstractive summaries, Li et al. (2018) and concatenate them to form a summary, or ab- provide models with keywords to prevent the model stractive methods (Rush et al., 2015; Chopra et al., from missing key information, and Cao et al. 2016; Nallapati et al., 2016; Paulus et al., 2018), (2018) propose models that retrieve and reference which generate summaries freely and are able to relevant summaries from the training set. While produce novel words and senten"
D15-1141,J96-1002,0,0.251174,"ments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system"
D15-1141,P15-1168,1,0.526007,"urian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters. Thus, the whole GRNN has an ability to simulate the design of the sophisticated features in traditional methods. Despite of their success, a limitation of them is that their performances are easily affected by the size of the context window. Intuitively, many words are difficult to segment based on the local information only. For example, the segmentation of the follow"
D15-1141,I05-3017,0,0.518308,"Missing"
D15-1141,D15-1280,1,0.0484058,"ent problem (Hochreiter and Schmidhuber, 1997). The LSTM provides a solution by incorporating memory units that allow the network to learn when to forget previous information and when to update the memory cells given new information. Thus, it is a natural choice to apply LSTM neural network to word segmentation task since the LSTM neural network can learn from data with long range temporal dependencies (memory) due to the considerable time lag between the inputs and their corresponding outputs. In addition, the LSTM has been applied successfully in many NLP tasks, such as text classification (Liu et al., 2015) and machine translation (Sutskever et al., 2014). The core of the LSTM model is a memory cell c encoding memory at every time step of what inputs have been observed up to this step (see Figure 2) . The behavior of the cell is controlled by three “gates”, namely input gate i, forget gate f and output gate o. The operations on gates are defined as element-wise multiplications, thus gate can either scale the input value if the gate is non-zero vector or omit input if the gate is zero vector. The output of output gate will be fed into the next time step t + 1 as previous hidden state and input of"
D15-1141,P14-1028,0,0.750867,"asks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters. Thus, the whole GRNN has an ability to simulate the design of the sophisticated features in traditional methods. Despite of their success, a limitation of them is that their performances are easily affected by the size of the cont"
D15-1141,C04-1081,0,0.829695,"tation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information in memory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineer"
D15-1141,P13-1045,0,0.0413166,"o all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013"
D15-1141,D13-1061,0,0.443356,"d by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015) proposed a gated recursive neural network (GRNN) to explicitly model the combinations of the characters for Chinese word segmentation task. Each neuron in GRNN can be regarded as a different combination of the input characters"
D15-1141,D11-1090,0,0.0275565,"Missing"
D15-1141,I05-3027,0,0.0366579,"Missing"
D15-1141,P10-1040,0,0.0273478,"equence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort in feature engineering (Collobert ∗ Corresponding author. et al., 2011; Socher et al., 2013; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-theart systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging, also he proposed a perceptron style algorithm to speed up the training process with negligible loss in performance. Pei et al. (2014) models tag-tag interactions, tagcharacter interactions and character-character interactions based on Zheng et al. (2013). Chen et al. (2015)"
D15-1141,O03-4002,0,0.931359,"word segmentation, which adopts the long short-term memory (LSTM) neural network to keep the previous important information in memory cell and avoids the limit of window size of local context. Experiments on PKU, MSRA and CTB6 benchmark datasets show that our model outperforms the previous neural network models and state-of-the-art methods. 1 Introduction Word segmentation is a fundamental task for Chinese language processing. In recent years, Chinese word segmentation (CWS) has undergone great development. The popular method is to regard word segmentation task as a sequence labeling problem (Xue, 2003; Peng et al., 2004). The goal of sequence labeling is to assign labels to all elements in a sequence, which can be handled with supervised learning algorithms such as Maximum Entropy (ME) (Berger et al., 1996) and Conditional Random Fields (CRF) (Lafferty et al., 2001). However, the ability of these models is restricted by the design of features, and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have increasingly used for NLP tasks for their ability to minimize the effort"
D15-1141,P12-1083,0,0.049662,"Missing"
D15-1141,P07-1106,0,0.105943,"ir methods. models P +Pre-train (Zheng et al., 2013) (Pei et al., 2014) LSTM +bigram LSTM +Pre-train+bigram (Pei et al., 2014) LSTM PKU R MSRA P R F F P CTB6 R F 93.5 92.2 92.8 94.2 93.7 93.9 93.9* 93.4* 93.7* 94.4 93.6 94.0 95.2 94.6 94.9 94.2* 93.7* 94.0* 96.3 95.6 96.0 96.7 96.5 96.6 95.9 95.5 95.7 96.3 95.9 96.1 97.1 97.1 97.1 95.6 95.3 95.5 - 95.2 - 97.2 96.6 96.4 96.5 97.5 97.3 97.4 96.2 95.8 96.0 Table 5: Performances on three test sets with pre-trained and bigram character embeddings. The results with * symbol are from our implementations of their methods. Models (Tseng et al., 2005) (Zhang and Clark, 2007) (Sun and Xu, 2011) (Zhang et al., 2013) This work PKU MSRA CTB6 95.0 96.4 95.1 97.2 95.7 96.1 97.4 96.5 97.4 96.0 a lot. On PKU dataset, it takes about 3 days to train the model (last row of Table 5) using CPU (Intel(R) Xeon(R) CPU E5-2665 @ 2.40GHz) only. All implementation is based on Python. 6 Related Work Table 6: Comparison of our model with state-ofthe-art methods on three test sets. with previous neural models with pre-trained embedding and bigram embeddings. Table 6 lists the performances of our model as well as previous state-of-the-art systems. (Zhang and Clark, 2007) is a word-base"
D15-1141,D13-1031,0,0.061144,"Missing"
D15-1168,P10-2050,0,0.00837811,"ned word embeddings. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work A line of previous research in fine-grained opinion mining focused on detecting opinion (subjective) 1 https://github.com/ppfliu/opinion-target expressions, e.g., (Wilson et al., 2005; Breck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type"
D15-1168,H05-1045,0,0.00883991,". In Section 4, we briefly describe the pre-trained word embeddings. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work A line of previous research in fine-grained opinion mining focused on detecting opinion (subjective) 1 https://github.com/ppfliu/opinion-target expressions, e.g., (Wilson et al., 2005; Breck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam a"
D15-1168,S15-1002,0,0.00849171,"He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose semantic word vectors based on syntactic parse trees, and use the vectors to identify the sentiments of the phrases and sentences. Le and Zuidema (2015) extended recursive neural networks with LSTM to compute a parent vector in parse trees by combining information of both output and LSTM memory cells from its two children. Most relevant to our work is the recent work of Irsoy and Cardie (2014), where they apply deep Elman-type RNN to extract opinion expressions and show that deep RNN outperforms CRF, semiCRF and shallow RNN. They used word embeddings from Google without fine-tuning them. Although inspired, our work differs from the work of Irsoy and Cardie (2014) in many ways. (i) We experiment with not only Elman-type, but also with a Jordan"
D15-1168,D14-1080,0,0.716206,"uite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012). The state-of-the-art model for opinion target extraction is also based on a CRF (Pontiki et al., 2014). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand. An alternative approach of deep learning automatically learns latent features as distributed vectors and have recently been shown to outperform CRFs on similar tasks. For example, Irsoy and Cardie (2014) apply deep recurrent neural networks (RNNs) to extract opinion expressions from sentences and show that RNNs outperform CRFs. Socher et al. (2013) propose recursive neural networks for a semantic compositional task to identify the sentiments of phrases and sentences hierarchically using the syntactic parse trees. Meanwhile, recent advances in word embed1433 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1433–1443, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. ding induction methods (Collobert and Weston"
D15-1168,S14-2004,0,0.779477,"om row) identification tasks. On the other hand, characterizing intensity and sentiment of an opinionated expression can be regarded as a semantic compositional problem, where the task is to aggregate vector representations of tokens in a meaningful way and later use them for sentiment classification (Socher et al., 2013). Conditional random fields (CRFs) (Lafferty et al., 2001) have been quite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012). The state-of-the-art model for opinion target extraction is also based on a CRF (Pontiki et al., 2014). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand. An alternative approach of deep learning automatically learns latent features as distributed vectors and have recently been shown to outperform CRFs on similar tasks. For example, Irsoy and Cardie (2014) apply deep recurrent neural networks (RNNs) to extract opinion expressions from sentences and show that RNNs outperform CRFs. Socher et al. (2013) propose recursive neural networks for a semantic composit"
D15-1168,P13-1161,0,0.00970649,"common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 2011; Yang and Cardie, 2012; Yang and Cardie, 2013), as well as topic modeling techniques using Latent Dirichlet Allocation (LDA) model and its variants (Titov and McDonald, 2008; Lin and He, 2009; Moghaddam and Ester, 2012). Conventional RNNs (e.g., Elman type) and LSTM have been successfully applied to various sequence prediction tasks, such as language modeling (Mikolov et al., 2010; Sundermeyer et al., 2012), speech recognition (Graves and Jaitly, 2014; Sak et al., 2014) and spoken language understanding (Mesnil et al., 2013). For sentiment analysis, Socher et al. (2013) propose to use recursive neural networks to hierarchically compose se"
D15-1168,D13-1170,0,0.0225568,"opinion targets can be formulated as a token-level sequence tagging problem, where the task is to label each word in a sentence using the conventional BIO tagging scheme. For example, Table 1 shows a sentence tagged with BIO scheme for opinion target (middle row) and for opinion expression (bottom row) identification tasks. On the other hand, characterizing intensity and sentiment of an opinionated expression can be regarded as a semantic compositional problem, where the task is to aggregate vector representations of tokens in a meaningful way and later use them for sentiment classification (Socher et al., 2013). Conditional random fields (CRFs) (Lafferty et al., 2001) have been quite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012). The state-of-the-art model for opinion target extraction is also based on a CRF (Pontiki et al., 2014). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand. An alternative approach of deep learning automatically learns latent features as distributed vectors and have"
D15-1168,S14-2038,0,0.184408,"Missing"
D15-1168,P10-1040,0,0.0135739,"works for a semantic compositional task to identify the sentiments of phrases and sentences hierarchically using the syntactic parse trees. Meanwhile, recent advances in word embed1433 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1433–1443, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. ding induction methods (Collobert and Weston, 2008; Mikolov et al., 2013b) have benefited researchers in two ways: (i) they have contributed to significant gains when used as extra word features in existing NLP systems (Turian et al., 2010; Lebret and Lebret, 2013), and (ii) they have enabled more effective training of RNNs by providing compact input representations of the words (Mesnil et al., 2013; Irsoy and Cardie, 2014). Motivated by the recent success of deep learning, in this paper we propose a general class of models based on RNN architecture and word embeddings, that can be successfully applied to finegrained opinion mining tasks without any taskspecific feature engineering effort. We experiment with several important RNN architectures including Elman-RNN, Jordan-RNN, long short term memory (LSTM) and their variations."
D15-1168,H05-1044,0,0.049066,"e second best on the Restaurant dataset in SemEval-2014. We make our source code available.1 In the remainder of this paper, after discussing related work in Section 2, we present our RNN models in Section 3. In Section 4, we briefly describe the pre-trained word embeddings. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work A line of previous research in fine-grained opinion mining focused on detecting opinion (subjective) 1 https://github.com/ppfliu/opinion-target expressions, e.g., (Wilson et al., 2005; Breck et al., 2007). The common approach was to formulate the problem as a sequence tagging task and use a CRF model. Later approaches extended this to jointly identify opinion holders (Choi et al., 2005), and intensity and polarity (Choi and Cardie, 2010). Extracting aspect terms or opinion targets have been actively investigated in the past. Typical approaches include association mining to find frequent item sets (i.e., co-occurring words) as candidate aspects (Hu and Liu, 2004), classificationbased methods such as hidden Markov model (Jin et al., 2009) and CRF (Shariaty and Moghaddam, 201"
D15-1168,D12-1122,0,0.0452663,"shows a sentence tagged with BIO scheme for opinion target (middle row) and for opinion expression (bottom row) identification tasks. On the other hand, characterizing intensity and sentiment of an opinionated expression can be regarded as a semantic compositional problem, where the task is to aggregate vector representations of tokens in a meaningful way and later use them for sentiment classification (Socher et al., 2013). Conditional random fields (CRFs) (Lafferty et al., 2001) have been quite successful for different fine-grained opinion mining tasks, e.g., opinion expression extraction (Yang and Cardie, 2012). The state-of-the-art model for opinion target extraction is also based on a CRF (Pontiki et al., 2014). However, the success of CRFs depends heavily on the use of an appropriate feature set and feature function expansion, which often requires a lot of engineering effort for each task in hand. An alternative approach of deep learning automatically learns latent features as distributed vectors and have recently been shown to outperform CRFs on similar tasks. For example, Irsoy and Cardie (2014) apply deep recurrent neural networks (RNNs) to extract opinion expressions from sentences and show t"
D15-1168,E14-1051,0,\N,Missing
D15-1280,D15-1141,1,0.0532292,"Missing"
D15-1280,P14-1062,0,0.475887,"s on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., ∗ 2015) and machine translation (Sutskever et al., 2014). LSTM is an extension of the recurrent neural network (RNN) (Elman, 1990), which can c"
D15-1280,D14-1181,0,0.0823907,"Missing"
D15-1280,C02-1150,0,0.0317821,"level, and the last dataset is document-level. The detailed statistics about the four datasets are listed in Table 1. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • QC The TREC questions dataset2 involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information (Li and Roth, 2002). • IMDB The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. (16) where L is the average length of the corpus. Thus, the slowest group is activated at least twice. 5 Training In each of the experiments, the hidden layer at the last moment has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes given the input sentence. The network is trained to minimise the cross-entropy of the predicted and true distribution"
D15-1280,P11-1015,0,0.0556068,"isted in Table 1. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • QC The TREC questions dataset2 involves six different question types, e.g. whether the question is about a location, about a person or about some numeric information (Li and Roth, 2002). • IMDB The IMDB dataset3 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. (16) where L is the average length of the corpus. Thus, the slowest group is activated at least twice. 5 Training In each of the experiments, the hidden layer at the last moment has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes given the input sentence. The network is trained to minimise the cross-entropy of the predicted and true distributions; the objective includes an L2 regularization term over the parameters. The network is trained with"
D15-1280,D11-1014,0,0.43637,"ence models are sensitive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013). Recursive neural network (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural netw"
D15-1280,D12-1110,0,0.868221,"ive to word order, but they have a bias towards the latest input words. This gives the RNN excellent performance at language modelling, but it is suboptimal for modeling the whole sentence, especially for the long texts. Le and Mikolov (2014) proposed a Paragraph Vector (PV) to learn continuous distributed vector representations for pieces of texts, which can be regarded as a long-term memory of sentences as opposed to the short-memory in RNN. Topological models Topological models compose the sentence representation following a given topological structure over the words (Socher et al., 2011a; Socher et al., 2012; Socher et al., 2013). Recursive neural network (RecNN) adopts a more general structure to encode sentence (Pollack, 1990; Socher et al., 2013). At every node in the tree the contexts at the left and right children of the node are combined by a classical layer. The weights of the layer are shared across all nodes in the tree. The layer computed at the top node gives a representation for the sentence. However, RecNN depends on external constituency parse trees provided by an external topological structure, such as parse tree. Convolutional models Convolutional neural network (CNN) is also used"
D15-1280,D13-1170,0,0.44174,"n model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term memory neural network (LSTM) (Hochreiter and Schmidhuber, 1997) has been applied successfully in many NLP tasks, such as spoken language understanding (Yao et al., 2014), sequence labeling (Chen et al., ∗ 2015) and machine translation (Sutskever et al., 2014). LSTM is an e"
D15-1280,P10-1040,0,0.0289762,"and documents. In this paper, we propose a multi-timescale long short-term memory (MT-LSTM) neural network to model long texts. MTLSTM partitions the hidden states of the standard LSTM into several groups. Each group is activated at different time periods. Thus, MT-LSTM can model very long documents as well as short sentences. Experiments on four benchmark datasets show that our model outperforms the other neural models in text classification task. 1 Introduction Distributed representations of words have been widely used in many natural language processing (NLP) tasks (Collobert et al., 2011; Turian et al., 2010; Mikolov et al., 2013b; Bengio et al., 2003). Following this success, it is rising a substantial interest to learn the distributed representations of the continuous words, such as phrases, sentences, paragraphs and documents (Mitchell and Lapata, 2010; Socher et al., 2013; Mikolov et al., 2013b; Le and Mikolov, 2014; Kalchbrenner et al., 2014). The primary role of these models is to represent the variable-length sentence or document as a fixed-length vector. A good representation of the variable-length text should fully capture the semantics of natural language. Recently, the long short-term"
D15-1280,P12-2018,0,0.0615111,"d update rule (Duchi et al., 2011). The back propagation of the error propagation is similar to LSTM as well. The only difference is that the error propagates only from groups that were executed at time step t. The error of nonactivated groups gets copied back in time (similarly to copying the activations of nodes not activated at the time step t during the corresponding forward pass), where it is added to the backpropagated error. 6 6.2 Competitor Models We compare our model with the following models: • NB-SVM and MNB. Naive Bayes SVM and Multinomial Naive Bayes with uni and bigram features (Wang and Manning, 2012). • NBOW The NBOW sums the word vectors and applies a non-linearity followed by a softmax classification layer. • RAE Recursive Autoencoders with pretrained word vectors from Wikipedia (Socher et al., 2011b). • MV-RNN Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012). Experiments In this section, we investigate the empirical performances of our proposed MT-LSTM model on four benchmark datasets for sentence and document classification and then compare it to other competitor models. 1 http://nlp.stanford.edu/sentiment. http://cogcomp.cs.illinois.edu/Data/ QA/QC/. 3 ht"
D15-1280,D14-1179,0,\N,Missing
D16-1012,P07-1056,0,0.0643455,"Missing"
D16-1012,D14-1082,0,0.0202468,"More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-task learning is an approach to learn multiple related tasks simultaneously to significantly improve performan"
D16-1012,P15-1166,0,0.00687272,"sentation for input words and solved different traditional NLP tasks within one framework. However, only one lookup table is shared, and the other lookup tables and layers are task-specific. Liu et al. (2015b) developed a multi-task DNN for learning representations across multiple tasks. Their multi-task DNN approach combines tasks of query classification and ranking for web search. But the input of the model is bag-of-word representation, which loses the information of word order. More recently, several multi-task encoder125 decoder networks were also proposed for neural machine translation (Dong et al., 2015; Luong et al., 2015; Firat et al., 2016), which can make use of cross-lingual information. Unlike these works, in this paper we design two neural architectures with shared memory for multitask learning, which can store useful information across the tasks. Our architectures are relatively loosely coupled, and therefore more flexible to expand. With the help of shared memory, we can obtain better task-specific sentence representation by utilizing the knowledge obtained by other related tasks. 7 Conclusion and Future Work In this paper, we introduce two deep architectures for multi-task learning"
D16-1012,N16-1101,0,0.0452605,"Missing"
D16-1012,P14-1062,0,0.112434,"insufficient training data. In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Correspo"
D16-1012,D14-1181,0,0.0121801,"Missing"
D16-1012,D15-1280,1,0.855459,"opose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-task learning is an"
D16-1012,N15-1092,0,0.0816419,"opose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-task learning is an"
D16-1012,P16-1098,1,0.518839,"y on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-task learning is an approach to learn multiple related tasks simultaneously"
D16-1012,P11-1015,0,0.00422292,"veral related tasks. 5.1 Datasets The used multi-task datasets are briefly described as follows. The detailed statistics are listed in Table 1. Movie Reviews The movie reviews dataset consists of four sub-datasets about movie reviews. • SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • SUBJ The movie reviews with labels of subjective or objective (Pang and Lee, 2004). • IMDB The IMDB dataset2 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. 1 http://nlp.stanford.edu/sentiment. http://ai.stanford.edu/˜amaas/data/ sentiment/ 2 Model LSTM Single Task ME-LSTM ARC-I ARC-II Multi-task MT-CNN MT-DNN NBOW RAE (Socher et al., 2011) MV-RNN (Socher et al., 2012) RNTN (Socher et al., 2013) DCNN (Kalchbrenner et al., 2014) CNN-multichannel (Kim, 2014) Tree-LSTM (Tai et al., 2015) SST-1 45.9 46.4 48.6 49.5 46.7 44.5 42.4 43.2 44.4 45.7 48.5 47.4 50.6 SST-2 85.8 85.5 87.0 87.8 86.1 84.0 80.5 82.4 82.9 85.4 86.8 88.1 86.9 SUBJ 91.6 91.0 93.8 95.0 92.2 90.1 91.3 93."
D16-1012,P04-1035,0,0.0123024,"irical performances of our proposed architectures on two multitask datasets. Each dataset contains several related tasks. 5.1 Datasets The used multi-task datasets are briefly described as follows. The detailed statistics are listed in Table 1. Movie Reviews The movie reviews dataset consists of four sub-datasets about movie reviews. • SST-1 The movie reviews with five classes in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also from the Stanford Sentiment Treebank. • SUBJ The movie reviews with labels of subjective or objective (Pang and Lee, 2004). • IMDB The IMDB dataset2 consists of 100,000 movie reviews with binary classes (Maas et al., 2011). One key aspect of this dataset is that each movie review has several sentences. 1 http://nlp.stanford.edu/sentiment. http://ai.stanford.edu/˜amaas/data/ sentiment/ 2 Model LSTM Single Task ME-LSTM ARC-I ARC-II Multi-task MT-CNN MT-DNN NBOW RAE (Socher et al., 2011) MV-RNN (Socher et al., 2012) RNTN (Socher et al., 2013) DCNN (Kalchbrenner et al., 2014) CNN-multichannel (Kim, 2014) Tree-LSTM (Tai et al., 2015) SST-1 45.9 46.4 48.6 49.5 46.7 44.5 42.4 43.2 44.4 45.7 48.5 47.4 50.6 SST-2 85.8 85."
D16-1012,D14-1162,0,0.111743,"08) with convolutional layer, in which lookup-tables are shared partially while other layers are task-specific. 3 https://www.cs.jhu.edu/˜mdredze/ datasets/sentiment/ 123 • MT-DNN: The model is proposed by Liu et al. (2015b) with bag-of-words input and multilayer perceptrons, in which a hidden layer is shared. 5.3 Hyperparameters and Training The networks are trained with backpropagation and the gradient-based optimization is performed using the Adagrad update rule (Duchi et al., 2011). The word embeddings for all of the models are initialized with the 100d GloVe vectors (840B token version, (Pennington et al., 2014)) and fine-tuned during training to improve the performance. The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. The mini-batch size is set to 16. For each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.1, 0.01], l2 regularization [0.0, 5E−5, 1E−5]. For datasets without development set, we use 10-fold cross-validation (CV) instead. The final hyper-parameters are set as Table 2. 5.4 5.6 Multi-task Learning of Movie Reviews We fir"
D16-1012,D11-1014,0,0.0811253,"Missing"
D16-1012,D12-1110,0,0.0638676,"Missing"
D16-1012,D13-1170,0,0.356173,"In this paper, we propose two deep architectures which can be trained jointly on multiple related tasks. More specifically, we augment neural model with an external memory, which is shared by several tasks. Experiments on two groups of text classification tasks show that our proposed architectures can improve the performance of a task with the help of other related tasks. 1 Introduction Neural network based models have been shown to achieved impressive results on various NLP tasks rivaling or in some cases surpassing traditional models, such as text classification (Kalchbrenner et al., 2014; Socher et al., 2013; Liu et al., 2015a), semantic matching (Hu et al., 2014; Liu et al., 2016a), parser (Chen and Manning, 2014) and machine translation (Bahdanau et al., 2014). Usually, due to the large number of parameters these neural models need a large-scale corpus. It is hard to train a deep neural model that generalizes well with size-limited data, while building the large scale resources for some NLP tasks is also a challenge. To overcome this problem, these models often involve an unsupervised pre-training phase. The final model is fine-tuned on specific task with respect ∗ Corresponding author. Multi-t"
D16-1012,P15-1150,0,0.020304,"Missing"
D16-1012,P10-1040,0,0.0282603,"Missing"
D16-1176,D15-1075,0,0.0679935,"rately, then they are concatenated and fed to a MLP. • Attention LSTMs: An attentive LSTM to encode two sentences into a semantic space, which used in (Hermann et al., 2015; Rockt¨aschel et al., 2015). • Word-by-word Attention LSTMs: An improvement of attention LSTM by introducing wordby-word attention mechanism, which used in (Hermann et al., 2015; Rockt¨aschel et al., 2015). 6.3 Experiment-I: Recognizing Textual Entailment Recognizing textual entailment (RTE) is a task to determine the semantic relationship between two sentences. We use the Stanford Natural Language Inference Corpus (SNLI) (Bowman et al., 2015). This corpus contains 570K sentence pairs, and all of the sentences and labels stem from human annotators. SNLI is two orders of magnitude larger than all other existing RTE corpora. Therefore, the massive scale of SNLI allows us to train powerful neural networks such as our proposed architecture in this paper. 6.3.1 Results Table 2 shows the evaluation results on SNLI. The 3rd column of the table gives the number of parameters of different models without the word embeddings. Our proposed two C-LSTMs models with four stacked blocks outperform all the competitor models, which indicates that ou"
D16-1176,D15-1181,0,0.0294076,"teractions, such as ARCI(Hu et al., 2014), CNTN(Qiu and Huang, 2015) and so on. These models first encode two sequences with some basic (Neural Bag-of-words, BOW) or advanced (RNN, CNN) components of neural networks separately, and then compute the matching score based on the distributed vectors of two sentences. In this paradigm, two sentences have no interaction until arriving final phase. Semi-interaction Models Some improved methods focus on utilizing multi-granularity representation (word, phrase and sentence level), such as MultiGranCNN (Yin and Sch¨utze, 2015) and MultiPerspective CNN (He et al., 2015). Another kind of models use soft attention mechanism to obtain the representation of one sentence by depending on representation of another sentence, such as ABCNN (Yin et al., 2015), Attention LSTM(Rockt¨aschel et al., 2015; Hermann et al., 2015). These models can alleviate the weak interaction problem, but are still insufficient to model the contextualized interaction on the word as well as phrase level. Strong Interaction Models These models directly build an interaction space between two sentences and model the interaction at different positions, such as ARC-II (Hu et al., 2014), MV-LSTM"
D16-1176,P14-1062,0,0.0777921,"hitecture to model the strong interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architectures. 1 Introduction Distributed representations of words or sentences have been widely used in many natural language processing (NLP) tasks, such as text classification (Kalchbrenner et al., 2014; Liu et al., 2015), question answering and machine translation (Sutskever et al., 2014) and so on. Among these tasks, a common problem is modelling the relevance/similarity of the sentence pair, which is also called text semantic matching. Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses (Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016). ∗ Corresponding author. According to the phases of interaction between two sentences, previous models can be classified into three categories. Weak interaction Model"
D16-1176,D15-1280,1,0.746506,"ng interaction of sentence pair with two coupled-LSTMs. Specifically, we introduce two coupled ways to model the interdependences of two LSTMs, coupling the local contextualized interactions of two sentences. We then aggregate these interactions and use a dynamic pooling to select the most informative features. Experiments on two very large datasets demonstrate the efficacy of our proposed architectures. 1 Introduction Distributed representations of words or sentences have been widely used in many natural language processing (NLP) tasks, such as text classification (Kalchbrenner et al., 2014; Liu et al., 2015), question answering and machine translation (Sutskever et al., 2014) and so on. Among these tasks, a common problem is modelling the relevance/similarity of the sentence pair, which is also called text semantic matching. Recently, deep learning based models is rising a substantial interest in text semantic matching and have achieved some great progresses (Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016). ∗ Corresponding author. According to the phases of interaction between two sentences, previous models can be classified into three categories. Weak interaction Models Some early works"
D16-1176,P16-1098,1,0.558516,"t attention mechanism to obtain the representation of one sentence by depending on representation of another sentence, such as ABCNN (Yin et al., 2015), Attention LSTM(Rockt¨aschel et al., 2015; Hermann et al., 2015). These models can alleviate the weak interaction problem, but are still insufficient to model the contextualized interaction on the word as well as phrase level. Strong Interaction Models These models directly build an interaction space between two sentences and model the interaction at different positions, such as ARC-II (Hu et al., 2014), MV-LSTM (Wan et al., 2016) and DF-LSTMs(Liu et al., 2016). These models can easily capture the difference between semantic capacity of two sentences. In this paper, we propose a new deep neural network architecture to model the strong interactions 1703 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1703–1712, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of two sentences. Different with modelling two sentences with separated LSTMs, we utilize two interdependent LSTMs, called coupled-LSTMs, to fully affect each other at different time steps. The output of coupled-LST"
D16-1176,D14-1162,0,0.0836116,"stochastic gradient descent with the diagonal variant of AdaGrad (Duchi et al., 2011). To prevent exploding gradients, we perform gradient clipping by scaling the gradient when the norm exceeds a threshold (Graves, 2013). 6 Experiment In this section, we investigate the empirical performances of our proposed model on two different text matching tasks: classification task (recognizing textual entailment) and ranking task (matching of question and answer). 6.1 Hyperparameters and Training The word embeddings for all of the models are initialized with the 100d GloVe vectors (840B token version, (Pennington et al., 2014)) and fine-tuned during training to improve the performance. The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. For each task, we take the hyperparameters which achieve the best performance on the development set via an small grid search over combinations of the initial learning rate [0.05, 0.0005, 0.0001], l2 regularization [0.0, 5E−5, 1E−5, 1E−6] and the threshold value of gradient norm [5, 10, 100]. The final hyperparameters are set as Table 1. 6.2 Competitor Methods • Neural bag-of-words (NBOW): Each sequence as the sum of the embeddings of"
D16-1176,N15-1091,0,0.0461014,"Missing"
D17-1124,balahur-etal-2010-sentiment,0,0.11879,"Missing"
D17-1124,P16-1139,0,0.0146598,"0.01, 0.001], l2 regularization [0.0, 5E−5, 1E−5] The final hyper-parameters are as follows. The initial learning rate is 0.1. The regularization weight of the parameters is 1E−5. For all the sentences from the five datasets, we parse them with constituency parser (Klein and Manning, 2003) to obtain the trees for our and some competitor models. 5.2 We give some descriptions about the setting of our models and several baseline models. • CharLSTM: Character level LSTM. • TLSTM: Vanilla tree-based LSTM, proposed by Tai et al. (2015). • Cont-TLSTM: Context-dependent tree-based LSTM, introduced by Bowman et al. (2016). • iTLSTM-Lo: Proposed model with Look-Up idiomatic interpreter. • iTLSTM-Mo: Proposed model with Morphology-Sensitive interpreter. #Sentences Table 4: Key statistics for the idioms and sentences in iSent dataset. O(Original) denotes the idioms in dev/test sets are in original forms and have appeared in training set. M(Morphology) and L(Lexical) represent the morphology and lexical idiom variations respectively and they are unseen in training set. 600 500 400 300 200 100 0 5.3 0 5 10 15 20 25 30 35 40 45 50 55 Sentence length Figure 2: The distribution of the number of reviews over different"
D17-1124,W06-3506,0,0.0445032,"sentences such as the “Verb Phrases” and “Adverb Phrases”. For example, the sentence “More often than not, this mixed bag hit its mark” has a positive sentiment. Cont-TLSTM pays much more attention to the word “not” without realizing that it belongs to the collocation “more often than not”, which expresses neutral emotion. In comparison, our model regards this collocation as a whole with neutral sentiment, which is crucial for the final prediction. 6 Related Work Previous work related to idioms focused on their identification, which falls in two kinds of paradigms: idiom type classification (Gedigian et al., 2006; Shutova et al., 2010) and idiom token classification (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). Different with these work, we integrate idioms understanding into a real-world task and consider different peculiarities of idioms in an end-to-end trainable framework. Recently, there are some work exploring the compositionality of various types of phrases (Kartsaklis et al., 2012; Muraoka et al., 2014; Hermann, 2014; Hashimoto and Tsuruoka, 2016). Compared with these work, we focus on how to properly model idioms under the con"
D17-1124,P16-1020,0,0.0323403,"Missing"
D17-1124,P14-1062,0,0.225926,"semantic matching (Liu et al., 2016a,b), and machine translation (Cho et al., 2014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge"
D17-1124,C12-2054,0,0.0320652,"tion. 6 Related Work Previous work related to idioms focused on their identification, which falls in two kinds of paradigms: idiom type classification (Gedigian et al., 2006; Shutova et al., 2010) and idiom token classification (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). Different with these work, we integrate idioms understanding into a real-world task and consider different peculiarities of idioms in an end-to-end trainable framework. Recently, there are some work exploring the compositionality of various types of phrases (Kartsaklis et al., 2012; Muraoka et al., 2014; Hermann, 2014; Hashimoto and Tsuruoka, 2016). Compared with these work, we focus on how to properly model idioms under the context of sentence representations. More recently, Zhu et al. (2016) propose a DAG-structured LSTM to incorporate external semantics including non-compositional or holistically learned semantics. Its key characteristic is that a DAG needs be built in advance, which merges some detected n-grams as the noncompositional phrases based on external knowledge. Different from this work, we focus on how to integrate detection and understanding of idioms int"
D17-1124,P16-1160,0,0.0248845,"al memory M , which is a table and stores idiomatic information for each idiom as depicted in the top subfigure in Figure 1-(c). Formally, the idiomatic meaning for a phrase can be obtained as: h(i) = M[k] (6) where k denotes the index of the corresponding phrase p. Morphology-Sensitive Model Since most idioms enjoy certain flexibility in morphology, lexicon, syntax, the above model suffers from the problem of idiom variations. To remedy this, inspired by the compositional view of idioms (Nunberg et al., 1994) and recent success of characterbased models (Kim et al., 2016; Lample et al., 2016; Chung et al., 2016), we propose to use CharLSTM to directly encode the meaning of a phrase in an idiomatic space and generate an idiomatic representation, which is not contaminated by its literal meaning and sensitive to different variations. Formally, for each non-leaf node i and its corresponding phrase pi in a constituency tree, we apply charLSTM to phrase pi as depicted in the bottom subfigure in Figure 1-(c) and utilize the emitted hidden states rj to represent the idiomatic meaning of the phrase. rj = Char-LSTM(rj−1 , cj−1 , xj ) (7) where j = 1, 2, · · · , m and m represents the length of the input phrase"
D17-1124,W06-1203,0,0.281998,"eiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. Mo"
D17-1124,J09-1005,0,0.188751,"onvolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. More concretely, we introduce a neural idiom d"
D17-1124,D14-1181,0,0.0266757,"Missing"
D17-1124,P03-1054,0,0.0175655,"he lurch → in the big lurch on the same page → on different pages Table 3: Idiom variations at morphological and lexical level. Add. and Sub. refer to lexical addition and substitution respectively. Train Idiom Sent. 1247 9752 O 124 720 Dev M 21 200 L 40 100 O 124 1403 Test M 21 400 L 40 200 tions of the initial learning rate [0.1, 0.01, 0.001], l2 regularization [0.0, 5E−5, 1E−5] The final hyper-parameters are as follows. The initial learning rate is 0.1. The regularization weight of the parameters is 1E−5. For all the sentences from the five datasets, we parse them with constituency parser (Klein and Manning, 2003) to obtain the trees for our and some competitor models. 5.2 We give some descriptions about the setting of our models and several baseline models. • CharLSTM: Character level LSTM. • TLSTM: Vanilla tree-based LSTM, proposed by Tai et al. (2015). • Cont-TLSTM: Context-dependent tree-based LSTM, introduced by Bowman et al. (2016). • iTLSTM-Lo: Proposed model with Look-Up idiomatic interpreter. • iTLSTM-Mo: Proposed model with Morphology-Sensitive interpreter. #Sentences Table 4: Key statistics for the idioms and sentences in iSent dataset. O(Original) denotes the idioms in dev/test sets are in"
D17-1124,N16-1030,0,0.0328092,"rieved from an external memory M , which is a table and stores idiomatic information for each idiom as depicted in the top subfigure in Figure 1-(c). Formally, the idiomatic meaning for a phrase can be obtained as: h(i) = M[k] (6) where k denotes the index of the corresponding phrase p. Morphology-Sensitive Model Since most idioms enjoy certain flexibility in morphology, lexicon, syntax, the above model suffers from the problem of idiom variations. To remedy this, inspired by the compositional view of idioms (Nunberg et al., 1994) and recent success of characterbased models (Kim et al., 2016; Lample et al., 2016; Chung et al., 2016), we propose to use CharLSTM to directly encode the meaning of a phrase in an idiomatic space and generate an idiomatic representation, which is not contaminated by its literal meaning and sensitive to different variations. Formally, for each non-leaf node i and its corresponding phrase pi in a constituency tree, we apply charLSTM to phrase pi as depicted in the bottom subfigure in Figure 1-(c) and utilize the emitted hidden states rj to represent the idiomatic meaning of the phrase. rj = Char-LSTM(rj−1 , cj−1 , xj ) (7) where j = 1, 2, · · · , m and m represents the lengt"
D17-1124,D09-1033,0,0.0840053,"; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. More concretely, we introd"
D17-1124,C10-1113,0,0.0214074,"“Verb Phrases” and “Adverb Phrases”. For example, the sentence “More often than not, this mixed bag hit its mark” has a positive sentiment. Cont-TLSTM pays much more attention to the word “not” without realizing that it belongs to the collocation “more often than not”, which expresses neutral emotion. In comparison, our model regards this collocation as a whole with neutral sentiment, which is crucial for the final prediction. 6 Related Work Previous work related to idioms focused on their identification, which falls in two kinds of paradigms: idiom type classification (Gedigian et al., 2006; Shutova et al., 2010) and idiom token classification (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). Different with these work, we integrate idioms understanding into a real-world task and consider different peculiarities of idioms in an end-to-end trainable framework. Recently, there are some work exploring the compositionality of various types of phrases (Kartsaklis et al., 2012; Muraoka et al., 2014; Hermann, 2014; Hashimoto and Tsuruoka, 2016). Compared with these work, we focus on how to properly model idioms under the context of sentence repres"
D17-1124,P16-1098,1,0.837358,"ty of a phrase literally or idiomatically. To better evaluate our models, we also construct an idiom-enriched sentiment classification dataset with considerable scale and abundant peculiarities of idioms. The qualitative and quantitative experimental analyses demonstrate the efficacy of our models. The newly-introduced datasets are publicly available at http: //nlp.fudan.edu.cn/data/ 1 Introduction Currently, neural network models have achieved great success for many natural language processing (NLP) tasks , such as text classification (Zhao et al., 2015; Liu et al., 2017), semantic matching (Liu et al., 2016a,b), and machine translation (Cho et al., 2014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recu"
D17-1124,D12-1110,0,0.149592,"Missing"
D17-1124,P17-1001,1,0.859383,"Missing"
D17-1124,D16-1176,1,0.851595,"ty of a phrase literally or idiomatically. To better evaluate our models, we also construct an idiom-enriched sentiment classification dataset with considerable scale and abundant peculiarities of idioms. The qualitative and quantitative experimental analyses demonstrate the efficacy of our models. The newly-introduced datasets are publicly available at http: //nlp.fudan.edu.cn/data/ 1 Introduction Currently, neural network models have achieved great success for many natural language processing (NLP) tasks , such as text classification (Zhao et al., 2015; Liu et al., 2017), semantic matching (Liu et al., 2016a,b), and machine translation (Cho et al., 2014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recu"
D17-1124,Y14-1010,0,0.0550271,"Missing"
D17-1124,P04-1035,0,0.00622438,"lso evaluate our models on these datasets to make a comparison with many recent proposed models. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also derived from the Stanford Sentiment Treebank. • MR The movie reviews with two classes 2 (Pang and Lee, 2005). • SUBJ Subjectivity data set where the goal is to classify each instance (snippet) as being subjective or objective. (Pang and Lee, 2004) The detailed statistics about these four datasets are listed in Table 2. 4.2 Reasons for a New Dataset Differing from previous work, which evaluating idiom detection as a standalone task, we want to integrate idiom understanding into sentiment classification task. However, most of existing sentiment datasets do not cover enough idioms or related linguistic phenomenon. To better evaluate our models on idiom understanding task, we proposed an idiom-enriched sentiment classification dataset, in which each sentence contains at least one idiom. Additionally, considering most idioms have certain fl"
D17-1124,P05-1015,0,0.016112,"lassification We list four kinds of datasets which are most commonly used for sentiment classification in NLP community. Additionally, we also evaluate our models on these datasets to make a comparison with many recent proposed models. Each dataset is briefly described as follows. • SST-1 The movie reviews with five classes (negative, somewhat negative, neutral, somewhat positive, positive) in the Stanford Sentiment Treebank1 (Socher et al., 2013). • SST-2 The movie reviews with binary classes. It is also derived from the Stanford Sentiment Treebank. • MR The movie reviews with two classes 2 (Pang and Lee, 2005). • SUBJ Subjectivity data set where the goal is to classify each instance (snippet) as being subjective or objective. (Pang and Lee, 2004) The detailed statistics about these four datasets are listed in Table 2. 4.2 Reasons for a New Dataset Differing from previous work, which evaluating idiom detection as a standalone task, we want to integrate idiom understanding into sentiment classification task. However, most of existing sentiment datasets do not cover enough idioms or related linguistic phenomenon. To better evaluate our models on idiom understanding task, we proposed an idiom-enriched"
D17-1124,D14-1216,0,0.0930799,"networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. More concretely, we introduce a neural idiom detector for each ph"
D17-1124,D14-1162,0,0.101534,": The distribution of the number of reviews over different lengths. 5.1 Experimental Settings Loss Function Given a sentence and its label, the output of neural network is the probabilities of the different classes. The parameters of the network are trained to minimise the cross-entropy of the predicted and true label distributions. To minimize the objective, we use stochastic gradient descent with the diagonal variant of AdaGrad (Duchi et al., 2011). Initialization and Hyperparameters In all of our experiments, the word embeddings for all of the models are initialized with the GloVe vectors (Pennington et al., 2014). The other parameters are initialized by randomly sampling from uniform distribution in [−0.1, 0.1]. For each task, we take the hyperparameters which achieve the best performance on the development set via a small grid search over combinaCompetitor Models Evaluation over Mainstream Datasets The experimental results are shown in Table 5. We can see Cont-TLSTM outperforms TLSTM on all four tasks, showing the importance of contextsensitive composition. Besides, both iTLSTM-Lo and iTLSTM-Mo achieve better results than TLSTM and Cont-LSTM, which indicates the effectiveness of our introduced module"
D17-1124,D11-1014,0,0.0732044,"Missing"
D17-1124,D13-1170,0,0.390752,"lation (Cho et al., 2014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neura"
D17-1124,P15-1150,0,0.377141,"014). The key factor of these neural models is how to compose a phrase or sentence representation from its constitutive words. Typically, a shared compositional function is used to compose word vectors recursively until obtaining the representation of the phrase or sentence. The form ∗ Corresponding author. of compositional function involves many kinds of neural networks, such as recurrent neural networks (Hochreiter and Schmidhuber, 1997; Chung et al., 2014), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic r"
D17-1124,N16-1106,0,0.0186106,"ation (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). Different with these work, we integrate idioms understanding into a real-world task and consider different peculiarities of idioms in an end-to-end trainable framework. Recently, there are some work exploring the compositionality of various types of phrases (Kartsaklis et al., 2012; Muraoka et al., 2014; Hermann, 2014; Hashimoto and Tsuruoka, 2016). Compared with these work, we focus on how to properly model idioms under the context of sentence representations. More recently, Zhu et al. (2016) propose a DAG-structured LSTM to incorporate external semantics including non-compositional or holistically learned semantics. Its key characteristic is that a DAG needs be built in advance, which merges some detected n-grams as the noncompositional phrases based on external knowledge. Different from this work, we focus on how to integrate detection and understanding of idioms into a unified end-to-end model, in which an idiomatic detector is introduced to adaptively control the semantic compositionality. Particularly, in the whole process no extra information is given to tell which phrases s"
D17-1124,W14-1007,0,0.0191394,"ntation. • We integrate idioms understanding into a real-world NLP task instead of evaluating idiom detection as a standalone task. • We construct a new real-world dataset covering abundant idioms with original and variational forms. The elaborate qualitative and quantitative experimental analyses show the effectiveness of our models. 2 Linguistic Interpretation of Idioms Recently, idioms have raised eyebrows among linguists, psycholinguists, and lexicographers due to their pervasiveness in daily discourse and their fascinating properties in linguistics literature (Villavicencio et al., 2005; Salton et al., 2014). As peculiar linguistic constructions (Villavicencio et al., 2005; Salton et al., 2014), idioms have three following properties: Invisibility Idioms always disguise themselves as normal multi-words in sentences. It makes end-to-end training hard since we should detect idioms first, and then understand them. Idiomaticity Idioms are semantically opaque, whose meanings cannot be derived from their constituent words. Existing compositional distributed approaches fail due to the hypothesis that the meaning of any phrase can be composed of the meanings of its constituents. Flexibility While structu"
D17-1124,P16-1019,0,0.0635211,"et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). However, these methods show an obvious defect in representing idiomatic phrases, whose semantics are not literal compositions of the individual words. For example, “pulling my leg” is idiomatic, and its meaning cannot be directly derived from a literal combination of its contained words. Due to its importance, some previous work focuses on automatic identification of idioms (Katz and Giesbrecht, 2006; Li and Sporleder, 2009; Fazly et al., 2009; Peng et al., 2014; Salton et al., 2016). However, challenge remains to take idioms into account to improve neural based semantic representations of phrases or sentences. Motivated by the literal-first psycholinguistic hypothesis proposed by Bobrow and Bell (1973), in this paper, we propose an end-to-end neural model for idiom-aware distributed semantic representation, in which we adopt a neural architecture of recursive network (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015) to learn the compositional semantics over a constituent tree. More concretely, we introduce a neural idiom detector for each phrase in a sentence to"
D19-5410,D18-1443,0,0.026602,"xample, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ stru"
D19-5410,N18-1065,0,0.0832705,"Missing"
D19-5410,P17-1044,0,0.0353276,"ng of the characteristics for the datasets and the increasing development of above three learning methods. Architecture Designs Architecturally speaking, most of existing extractive summarization systems consists of three major modules: sentence encoder, document encoder and decoder. In this paper, our architectural choices vary with two types of document encoders: LSTM2 (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017) while we keep the sentence encoder (convolutional neural networks) and decoder (sequence labeling) unchanged3 . The base 2 We use the implementation of He et al. (2017). Since they do not show significant influence on our explored experiments. 3 81 model in all experiments refers to Transformer equipped with sequence labelling. extracted, the representation of the sentence consists of two components: position representation4 , which indicates the position of the sentence in the document; content representation, which contains the semantic information of the sentence. Therefore, we define the position and content information of the sentence as constituent factors, aiming to explore how the selected sentences in the test set relate to the training set in terms"
D19-5410,N18-1150,0,0.271935,"riors residing in datasets and model designs, analyzing how different properties of datasets influence the choices of model structure design and training methods. Finally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from abov"
D19-5410,P18-1014,0,0.381999,"and model designs, analyzing how different properties of datasets influence the choices of model structure design and training methods. Finally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite"
D19-5410,D19-1327,0,0.230067,"al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ structures, training schemas, and pre-trained knowledge. Some observations could instruct future researchers ∗ These three authors contributed equally. Corresponding author. 1 Concurrent with our work, (Jung et al., 2019) makes a similar analysis on datasets biases and presents three factors † which matter for the text summarization task. 80 Proceedings of the 2nd Workshop on New Frontiers in Summarization, pages 80–89 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics Factors of Datasets Measures Model designs Constituent[4.1] Positional coverage rate [4.1.1] Content coverage rate [4.1.2] Architecture designs [6.2] Pre-trained strategies [6.2] Style [4.2] Density [4.2.1] Compression [4.2.2] Training schemas [6.1] Table 1: Organization structure of this paper: four measures pr"
D19-5410,P18-1063,0,0.309888,"typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in"
D19-5410,P16-1046,0,0.11915,"en a dataset D, different learning methods are trying to explain the data in diverse ways, which show different generalization behaviours. Existing learning methods for extractive summarization systems vary in architectures designs, pre-trained strategies and training schemas. Neural Extractive Summarization Recently, neural network-based models have achieved great success in extractive summarization. (Celikyilmaz et al., 2018; Jadhav and Rajan, 2018; Liu, 2019). Existing works on text summarization can roughly fall into one of three classes: exploring networks’ structures with suitable bias (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018); introducing new training schemas (Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018) and incorporating large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). Instead of exploring the possibility for a new state-of-the-art along one of above three lines, in this paper, we aim to bridge the gap between the lack of understanding of the characteristics for the datasets and the increasing development of above three learning methods. Architecture Designs Architecturally speaking, most of existing"
D19-5410,N18-2097,0,0.106766,"Missing"
D19-5410,P18-1176,0,0.0310446,"e same dataset. So the results of compression in Table 4 are in line with our expectations, how the model represents long documents to get good performance in text summarization task remains a challenge (Celikyilmaz et al., 2018). Unlike the exploration of density, we attempt to understand how the model extracts sentences when faced with different compression samples. We utilize an attribution technique called Integrated Gradients (IG) (Sundararajan et al., 2017) to separate the position and content information of each sentence. The setting of input x and baseline x’ in this paper is close to Mudrakarta et al. (2018)8 , but it is worth noting that our base model adds positional embedding to each sentence, so input x and baseline x’ both have positional information. Table 5: Experiment aboutP DENSITY , Pct denotes the percentage of ψ(si , S) to si ∈D ψ(si , S). The first three sentences contain more salient information in samples with higher density. In order to comprehend this correlation, we conduct the following experiment. Given an article and summary pair, we assign a score ψ(si , S) to each sentence in article to indicate how much sailent information is contained in the sentence. ψ(si , S) = LCS(si ,"
D19-5410,K16-1028,0,0.0983355,"Missing"
D19-5410,P18-1061,0,0.359477,"nally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of exist"
D19-5410,N18-1158,0,0.292258,"nalysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ structures, training schemas, and pre-trained knowledge. Some observations could instruct f"
D19-5410,N18-1202,0,0.207059,"ing significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ structures, training schemas, and pre-trained knowledge. Some observations could instruct future researchers ∗ These three authors contributed equally. Corresponding author. 1 Concurrent with our work, (Jung et al"
D19-5410,P17-1099,0,0.102021,"nnection between priors residing in datasets and model designs, analyzing how different properties of datasets influence the choices of model structure design and training methods. Finally, by taking a typical dataset as an example, we rethink the process of the model design based on the experience of the above analysis. We demonstrate that when we have a deep understanding of the characteristics of datasets, a simple approach can bring significant improvements to the existing stateof-the-art model. 1 Introduction Neural network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems"
D19-5410,P19-1100,1,0.717017,"l network-based models have achieved great success on summarization tasks (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). Current studies on summarization either explore the possibility of optimization in terms of networks’ structures (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018), the improvement in terms of training schemas (Wang et al., 2019; Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), or the information fusion with large pre-trained knowledge (Peters et al., 2018; Devlin et al., 2018; Liu, 2019; Dong et al., 2019). More recently, Zhong et al. (2019) conducts a comprehensive analysis on why existing summarization systems perform so well from above three aspects. Despite their success, a relatively missing topic1 Main Contributions 1) For the summarization task itself, we diagnose the weakness of existing learning methods in terms of networks’ structures, training schemas, and pre-trained knowledge. Some observations could instruct future researchers ∗ These three authors contributed equally. Corresponding author. 1 Concurrent with our work, (Jung et al., 2019) makes a similar analysis on datasets biases and presents three factors † which"
N19-1133,D15-1075,0,0.0809655,"the Transformer paper (Vaswani et al., 2017), the maximum dependency path length of RNN and Transformer are O(n), O(1), respectively. Star-Transformer can pass the message from one node to another node via the relay node so that the maximum dependency path length is also O(1), with a constant two comparing to Transformer. Compare with the standard Transformer, all positions are processed in parallel, pair-wise connec1318 Dataset Train Dev. Test |V | H DIM #head head DIM Masked Summation 10k 10k 10k - 100 10 10 SST (Socher et al., 2013) 8k 1k 2k 20k 300 6 50 1400 200 400 8k∼28k 300 6 50 SNLI (Bowman et al., 2015) 550k 10k 10k 34k 600 6 100 PTB POS (Marcus et al., 1993) 38k 5k 5k 44k 300 6 50 CoNLL03 (Sang and Meulder, 2003) 15k 3k 3k 25k 300 6 50 OntoNotes NER (Pradhan et al., 2012) 94k 14k 8k 63k 300 6 50 MTL-16 † (Liu et al., 2017) Apparel Baby Books Camera DVD Electronics Health IMDB Kitchen Magazines MR Music Software Sports Toys Video Table 1: An overall of datasets and its hyper-parameters, “H DIM, #head, head DIM” indicates the dimension of hidden states, the number of heads in the Multi-head attention, the dimension of each head, respectively. MTL-16† consists of 16 datasets, each of them has"
N19-1133,P16-1139,0,0.0131918,"ould be 4.5 times fast than the standard Transformer on average. 5.3 Natural Language Inference Natural Language Inference (NLI) asks the model to identify the semantic relationship between a premise sentence and a corresponding hypothesis sentence. In this paper, we use the Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) for evaluation. Since we want to study how the model encodes the sentence as a vector representation, we set Star-Transformer as a sentence vector-based model and compared it with sentence vector-based models. In this experiment, we follow the previous work (Bowman et al., 2016) to use concat(r1 , r2 , kr1 − r2 k, r1 − r2 ) as the classification feature. The r1 , r2 are representations of premise and hypothesis sentence, it is calculated by sT + max(HT ) which is same with the classification task. See Appendix for the detail of hyper-parameters. As shown in Tab-4, the Star-Transformer outperforms most typical baselines (DiSAN, SPINN) and achieves comparable results compared with the state-of-the-art model. Notably, our model beats standard Transformer by a large margin, which is easy to overfit although we have made a careful hyper-parameters’ searching for Transform"
N19-1133,D16-1053,0,0.0339991,"Missing"
N19-1133,D17-1070,0,0.0417947,"Missing"
N19-1133,P19-1285,0,0.119448,"the virtual relay node. Red edges and blue edges are ring and radical connections, respectively. Recently, the fully-connected attention-based models, like Transformer (Vaswani et al., 2017), become popular in natural language processing (NLP) applications, notably machine translation (Vaswani et al., 2017) and language modeling (Radford et al., 2018). Some recent work also suggest that Transformer can be an alternative to recurrent neural networks (RNNs) and convolutional neural networks (CNNs) in many NLP tasks, such as GPT (Radford et al., 2018), BERT (Devlin et al., 2018), Transformer-XL (Dai et al., 2019) and Universal Transformer (Dehghani et al., 2018). More specifically, there are two limitations of the Transformer. First, the computation and memCorresponding Author. work done at NYU Shanghai, now with AWS Shanghai AI Lab. ‡ h8 h3 h6 Introduction ∗ h1 h2 ory overhead of the Transformer are quadratic to the sequence length. This is especially problematic with long sentences. Transformer-XL (Dai et al., 2019) provides a solution which achieves the acceleration and performance improvement, but it is specifically designed for the language modeling task. Second, studies indicate that Transformer"
N19-1133,D17-1206,0,0.0235026,"tasets. 5 Experiments We evaluate Star-Transformer on one simulation task to probe its behavior when challenged with long-range dependency problem, and three real tasks (Text Classification, Natural Language Inference, and Sequence Labelling). All experiments are ran on a NVIDIA Titan X card. Datasets used in this paper are listed in the Tab-1. We use the Adam (Kingma and Ba, 2014) as our optimizer. On the real task, we set the embedding size to 300 and initialized with GloVe (Pennington et al., 2014). And the symbol “Ours + Char” means an additional character-level pre-trained embedding JMT (Hashimoto et al., 2017) is used. Therefore, the total size of embedding should be 400 which as a result of the concatenation of GloVe and JMT. We also fix the embedding layer of the Star-Transformer in all experiments. Since semi- or unsupervised model is also a feasible solution to improve the model in a parallel direction, such as the ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), we exclude these models in the comparison and focus on the relevant architectures. 5.1 Masked Summation In this section, we introduce a simulation task on the synthetic data to probe the efficiency and nonlocal/long-range dep"
N19-1133,P14-1062,0,0.0176737,"is task, 1 https://github.com/dmlc/dgl and https: //github.com/fastnlp/fastNLP we verify that both Transformer and StarTransformer are good at handling long-range dependencies compared to the LSTM and BiLSTM. 2 Related Work Recently, neural networks have proved very successful in learning text representation and have achieved state-of-the-art results in many different tasks. Modelling Local Compositionality A popular approach is to represent each word as a lowdimensional vector and then learn the local semantic composition functions over the given sentence structures. For example, Kim (2014); Kalchbrenner et al. (2014) used CNNs to capture the semantic representation of sentences, whereas Cho et al. (2014) used RNNs. These methods are biased for learning local compositional functions and are hard to capture the long-term dependencies in a text sequence. In order to augment the ability to model the nonlocal compositionality, a class of improved methods utilizes various self-attention mechanisms to aggregate the weighted information of each word, which can be used to get sentence-level representations for classification tasks (Yang et al., 2016; Lin et al., 2017; Shen et al., 2018a). Another class of improved"
N19-1133,D14-1181,0,0.00706571,"ncies. In this task, 1 https://github.com/dmlc/dgl and https: //github.com/fastnlp/fastNLP we verify that both Transformer and StarTransformer are good at handling long-range dependencies compared to the LSTM and BiLSTM. 2 Related Work Recently, neural networks have proved very successful in learning text representation and have achieved state-of-the-art results in many different tasks. Modelling Local Compositionality A popular approach is to represent each word as a lowdimensional vector and then learn the local semantic composition functions over the given sentence structures. For example, Kim (2014); Kalchbrenner et al. (2014) used CNNs to capture the semantic representation of sentences, whereas Cho et al. (2014) used RNNs. These methods are biased for learning local compositional functions and are hard to capture the long-term dependencies in a text sequence. In order to augment the ability to model the nonlocal compositionality, a class of improved methods utilizes various self-attention mechanisms to aggregate the weighted information of each word, which can be used to get sentence-level representations for classification tasks (Yang et al., 2016; Lin et al., 2017; Shen et al., 2018a"
N19-1133,Q16-1026,0,0.0739745,"Missing"
N19-1133,D15-1180,0,0.0386201,"Missing"
N19-1133,D15-1278,0,0.0412974,"Missing"
N19-1133,N18-1202,0,0.0153189,". We use the Adam (Kingma and Ba, 2014) as our optimizer. On the real task, we set the embedding size to 300 and initialized with GloVe (Pennington et al., 2014). And the symbol “Ours + Char” means an additional character-level pre-trained embedding JMT (Hashimoto et al., 2017) is used. Therefore, the total size of embedding should be 400 which as a result of the concatenation of GloVe and JMT. We also fix the embedding layer of the Star-Transformer in all experiments. Since semi- or unsupervised model is also a feasible solution to improve the model in a parallel direction, such as the ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), we exclude these models in the comparison and focus on the relevant architectures. 5.1 Masked Summation In this section, we introduce a simulation task on the synthetic data to probe the efficiency and nonlocal/long-range dependencies of LSTM, Transformer, and the Star-Transformer. As mentioned in (Vaswani et al., 2017), the maximum path length of long-range dependencies of LSTM and Transformer are O(n) and O(1), where n is the sequence length. The maximum dependency path length of Star-Transformer is O(1) with a constant two via the relay node. To validate the"
N19-1133,W12-4501,0,0.0543652,"Missing"
N19-1133,P17-1001,1,0.927262,"t the maximum dependency path length is also O(1), with a constant two comparing to Transformer. Compare with the standard Transformer, all positions are processed in parallel, pair-wise connec1318 Dataset Train Dev. Test |V | H DIM #head head DIM Masked Summation 10k 10k 10k - 100 10 10 SST (Socher et al., 2013) 8k 1k 2k 20k 300 6 50 1400 200 400 8k∼28k 300 6 50 SNLI (Bowman et al., 2015) 550k 10k 10k 34k 600 6 100 PTB POS (Marcus et al., 1993) 38k 5k 5k 44k 300 6 50 CoNLL03 (Sang and Meulder, 2003) 15k 3k 3k 25k 300 6 50 OntoNotes NER (Pradhan et al., 2012) 94k 14k 8k 63k 300 6 50 MTL-16 † (Liu et al., 2017) Apparel Baby Books Camera DVD Electronics Health IMDB Kitchen Magazines MR Music Software Sports Toys Video Table 1: An overall of datasets and its hyper-parameters, “H DIM, #head, head DIM” indicates the dimension of hidden states, the number of heads in the Multi-head attention, the dimension of each head, respectively. MTL-16† consists of 16 datasets, each of them has 1400/200/400 samples in train/dev/test. tions are replaced with a “gather and dispatch” mechanism. As a result, we accelerate the Transformer 10 times on the simulation task and 4.5 times on real tasks. The model also preserv"
N19-1133,P16-1101,0,0.0496375,"Missing"
N19-1133,J93-2004,0,0.0685742,"m dependency path length of RNN and Transformer are O(n), O(1), respectively. Star-Transformer can pass the message from one node to another node via the relay node so that the maximum dependency path length is also O(1), with a constant two comparing to Transformer. Compare with the standard Transformer, all positions are processed in parallel, pair-wise connec1318 Dataset Train Dev. Test |V | H DIM #head head DIM Masked Summation 10k 10k 10k - 100 10 10 SST (Socher et al., 2013) 8k 1k 2k 20k 300 6 50 1400 200 400 8k∼28k 300 6 50 SNLI (Bowman et al., 2015) 550k 10k 10k 34k 600 6 100 PTB POS (Marcus et al., 1993) 38k 5k 5k 44k 300 6 50 CoNLL03 (Sang and Meulder, 2003) 15k 3k 3k 25k 300 6 50 OntoNotes NER (Pradhan et al., 2012) 94k 14k 8k 63k 300 6 50 MTL-16 † (Liu et al., 2017) Apparel Baby Books Camera DVD Electronics Health IMDB Kitchen Magazines MR Music Software Sports Toys Video Table 1: An overall of datasets and its hyper-parameters, “H DIM, #head, head DIM” indicates the dimension of hidden states, the number of heads in the Multi-head attention, the dimension of each head, respectively. MTL-16† consists of 16 datasets, each of them has 1400/200/400 samples in train/dev/test. tions are replace"
N19-1133,P16-2022,0,0.0596285,"Missing"
N19-1133,D13-1170,0,0.051355,"h analysis of the path length of dependencies in these models. As discussed in the Transformer paper (Vaswani et al., 2017), the maximum dependency path length of RNN and Transformer are O(n), O(1), respectively. Star-Transformer can pass the message from one node to another node via the relay node so that the maximum dependency path length is also O(1), with a constant two comparing to Transformer. Compare with the standard Transformer, all positions are processed in parallel, pair-wise connec1318 Dataset Train Dev. Test |V | H DIM #head head DIM Masked Summation 10k 10k 10k - 100 10 10 SST (Socher et al., 2013) 8k 1k 2k 20k 300 6 50 1400 200 400 8k∼28k 300 6 50 SNLI (Bowman et al., 2015) 550k 10k 10k 34k 600 6 100 PTB POS (Marcus et al., 1993) 38k 5k 5k 44k 300 6 50 CoNLL03 (Sang and Meulder, 2003) 15k 3k 3k 25k 300 6 50 OntoNotes NER (Pradhan et al., 2012) 94k 14k 8k 63k 300 6 50 MTL-16 † (Liu et al., 2017) Apparel Baby Books Camera DVD Electronics Health IMDB Kitchen Magazines MR Music Software Sports Toys Video Table 1: An overall of datasets and its hyper-parameters, “H DIM, #head, head DIM” indicates the dimension of hidden states, the number of heads in the Multi-head attention, the dimension"
N19-1133,P15-1150,0,0.120509,"Missing"
N19-1133,Q16-1016,0,0.0310563,"Missing"
N19-1133,N16-1174,0,0.324121,"the given sentence structures. For example, Kim (2014); Kalchbrenner et al. (2014) used CNNs to capture the semantic representation of sentences, whereas Cho et al. (2014) used RNNs. These methods are biased for learning local compositional functions and are hard to capture the long-term dependencies in a text sequence. In order to augment the ability to model the nonlocal compositionality, a class of improved methods utilizes various self-attention mechanisms to aggregate the weighted information of each word, which can be used to get sentence-level representations for classification tasks (Yang et al., 2016; Lin et al., 2017; Shen et al., 2018a). Another class of improved methods augments neural networks with a re-reading ability or global state while processing each word (Cheng et al., 2016; Zhang et al., 2018). Modelling Non-Local Compositionality There are two kinds of methods to model the non-local semantic compositions in a text sequence directly. One class of models incorporate syntactic tree into the network structure for learning sentence representations (Tai et al., 2015; Zhu et al., 2015). Another type of models learns the dependencies between words based entirely on self-attention wit"
N19-1133,W17-5308,0,0.0338431,"Missing"
N19-1133,D14-1162,0,0.0876774,"t sequences. Besides the acceleration, the Star-Transformer achieves significant improvement on some modestly sized datasets. 5 Experiments We evaluate Star-Transformer on one simulation task to probe its behavior when challenged with long-range dependency problem, and three real tasks (Text Classification, Natural Language Inference, and Sequence Labelling). All experiments are ran on a NVIDIA Titan X card. Datasets used in this paper are listed in the Tab-1. We use the Adam (Kingma and Ba, 2014) as our optimizer. On the real task, we set the embedding size to 300 and initialized with GloVe (Pennington et al., 2014). And the symbol “Ours + Char” means an additional character-level pre-trained embedding JMT (Hashimoto et al., 2017) is used. Therefore, the total size of embedding should be 400 which as a result of the concatenation of GloVe and JMT. We also fix the embedding layer of the Star-Transformer in all experiments. Since semi- or unsupervised model is also a feasible solution to improve the model in a parallel direction, such as the ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018), we exclude these models in the comparison and focus on the relevant architectures. 5.1 Masked Summation In t"
N19-1133,P18-1030,0,0.121072,"d for learning local compositional functions and are hard to capture the long-term dependencies in a text sequence. In order to augment the ability to model the nonlocal compositionality, a class of improved methods utilizes various self-attention mechanisms to aggregate the weighted information of each word, which can be used to get sentence-level representations for classification tasks (Yang et al., 2016; Lin et al., 2017; Shen et al., 2018a). Another class of improved methods augments neural networks with a re-reading ability or global state while processing each word (Cheng et al., 2016; Zhang et al., 2018). Modelling Non-Local Compositionality There are two kinds of methods to model the non-local semantic compositions in a text sequence directly. One class of models incorporate syntactic tree into the network structure for learning sentence representations (Tai et al., 2015; Zhu et al., 2015). Another type of models learns the dependencies between words based entirely on self-attention without any recurrent or convolutional layers, such as Transformer (Vaswani et al., 2017), which has achieved state-of-the-art results on a machine translation task. The success of Transformer has raised a large"
P16-1098,D15-1181,0,0.0282344,"nterest in text semantic matching and has achieved some great progresses (Hu et al., 2014; Qiu and Huang, 2015; Wan et al., 2016). According to their interaction ways, previous models can be classified into three categories: Weak interaction Models Some early works focus on sentence level interactions, such as ARCI(Hu et al., 2014), CNTN(Qiu and Huang, 2015) ∗ Corresponding author Strong Interaction Models Some models build the interaction at different granularity (word, phrase and sentence level), such as ARC-II (Hu et al., 2014), MultiGranCNN (Yin and Sch¨utze, 2015), Multi-Perspective CNN (He et al., 2015), MV-LSTM (Wan et al., 2016), MatchPyramid (Pang et al., 2016). The final matching score depends on these different levels of interactions. In this paper, we adopt a deep fusion strategy to model the strong interactions of two sentences. Given two texts x1:m and y1:n , we define a matching vector hi,j to represent the interaction of the subsequences x1:i and y1:j . hi,j depends on the matching vectors hs,t on previous interactions 1 ≤ s &lt; i and 1 ≤ t &lt; j. Thus, text matching can be regarded as modelling the interaction of two texts in a recursive matching way. Following this idea, we propose d"
P16-1098,D15-1075,0,0.0196488,"-by-word Attention LSTMs: An improved strategy of attention LSTMs, which introduces word-by-word attention mechanism and is proposed by (Rockt¨aschel et al., 2015). k 100 Train 77.9 Test 75.1 100 83.7 80.9 −0.4 −0.2 0 0.2 0.4 −0.2 100 84.8 77.6 running 100 83.2 82.3 pet 100 83.7 83.5 being 100 85.2 84.6 0.6 0.8 A young family enjoys feeling ocean waves lap at their feet by another Experiment-I: Recognizing Textual Entailment Recognizing textual entailment (RTE) is a task to determine the semantic relationship between two sentences. We use the Stanford Natural Language Inference Corpus (SNLI) (Bowman et al., 2015). This corpus contains 570K sentence pairs, and all of the sentences and labels stem from human annotators. SNLI is two orders of magnitude larger than all other existing RTE corpora. Therefore, the massive scale of SNLI allows us to train powerful neural networks such as our proposed architecture in this paper. Results Table 2 shows the evaluation results on SNLI. The 2nd column of the table gives the number of hidden states. From experimental results, we have several experimental findings. The results of DF-LSTMs outperform all the competitor models with the same number of hidden states whil"
P16-1098,D16-1053,0,0.0572037,"Missing"
P16-1098,D15-1280,1,0.535996,"ed, the input gate controls how much each unit is updated, and the output gate controls the exposure of the internal memory state. The update of each LSTM unit can be written precisely as (ht , ct ) = LSTM(ht−1 , ct−1 , xt ). (5) Here, the function LSTM(·, ·, ·) is a shorthand for Eq. (2-4). LSTM can map the input sequence of arbitrary length to a fixed-sized vector, and has been successfully applied to a wide range of NLP tasks, such as machine translation (Sutskever et al., 2014), language modelling (Sutskever et al., 2011), text matching (Rockt¨aschel et al., 2015) and text classification (Liu et al., 2015). 4 Deep Fusion LSTMs for Recursively Semantic Matching To deal with two sentences, one straightforward method is to model them with two separate LSTMs. However, this method is difficult to model local interactions of two sentences. Following the recursive matching strategy, we propose a neural model of deep fusion LSTMs (DF-LSTMs), which consists of two interdependent LSTMs to capture the inter- and intrainteractions between two sequences. Figure 2 gives an illustration of DF-LSTMs unit. To facilitate our model, we firstly give some definitions. Given two sequences X = x1 , x2 , · · · , xn an"
P16-1098,D14-1162,0,0.0877353,"Missing"
P16-1098,N15-1091,0,0.0620213,"Missing"
P16-1098,N16-1036,0,\N,Missing
P16-1163,D15-1262,0,0.816312,"on becomes much more challenging when such connectives are missing. In fact, such implicit discourse relations Intuitively, (good, wrong) and (good, ruined), seem to be the most informative word pairs, and it is likely that they will trigger a contrast relation. Therefore, we can see that another main disadvantage of using word pairs is the lack of contextual information, and using n-gram pairs will again suffer from data sparsity problem. Recently, the distributed word representations (Bengio et al., 2006; Mikolov et al., 2013) have shown an advantage when dealing with data sparsity problem (Braud and Denis, 2015), and many deep learning based models are generating substantial interests in text semantic matching and have achieved some significant progresses (Hu 1726 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1726–1735, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Psyllium's not a good crop. You get a rain at the wrong time and the crop is ruined. Bidirectional LSTM Pooling Layer MLP f Gated Relevance Network Bilinear Tensor Single Layer Network Figure 1: The processing framework of the proposed approach. et al.,"
P16-1163,Q15-1024,0,0.374564,"for testing and the other sections for validation (Prasad et al., 2008). For comparison with the previous work (Pitler et al., 2009; Zhou et al., 2010; Park 1729 Table 1: The unbalanced sample distribution of PDTB. Relation Comparison Contingency Expansion Temporal Train 1894 3281 6792 665 Dev 401 628 1253 93 Table 2: Hyperparameters for our model in the experiment. Word Embedding size Initial learning rate Minibatch size Pooling Size Number of tensor slice Test 146 276 556 68 segments representation. The rest of the method is the same as Word-NTN. and Cardie, 2012; Rutherford and Xue, 2014; Ji and Eisenstein, 2015), we train four binary classifiers to identify each of the top level relations, the EntRel relations are merged with Expansion relations. For each classifier, we use an equal number of positive and negative samples as training data, because each of the relations except Expansion is infrequent (Pitler et al., 2009) as what shows in Table 1. The negative samples were chosen randomly from training sections 2-20. 3.2 3.2.1 • Word+GRN: We use the gated relevance network proposed in this paper to capture the semantic interaction scores between every word embedding pair of the two text segments. The"
P16-1163,P13-2013,0,0.594359,"ng@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held between them using only word pairs. Consider the following sentence pair with a casual relation as an example: Word pairs, which are one of the most easily accessible features between two text segments, have been proven to be very useful for detecting the discourse relations"
P16-1163,W12-1614,0,0.186442,"an implicit marker. Since explicit relations are easy to identify (Pitler et al., 2008), existing methods achieved good performance on the relations with explicit maker. In recent years, researchers mainly focused on implicit relations. For easily comparing with other methods, in this work, we also use PDTB as the training and testing corpus. As we mentioned above, various approaches have been proposed to do the task. Pitler et al. (2009) proposed to train four binary classifiers using word pairs as well as other rich linguistic features to automatically identify the top-level PDTB relations. Park and Cardie (2012) achieved a higher performance by optimizing the feature set. McKeown and Biran (2013) aims at solving the data sparsity problem, and they extended the work of Pitler et al. (2009) by aggregating word pairs. Rutherford and Xue (2014) used Brown clusters and coreferential patterns as new features and improved the baseline a lot. Braud and Denis (2015) compared different word representations for implicit relation classification. The word pairs feature have been studied by all of the work above, showing its importance on discourse relation. We follow their work, and incorporate word embedding to"
P16-1163,P09-1077,0,0.173917,"e with Gated Relevance Network Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China jfchen14,qz,pfliu14,xpqiu,xjhuang@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held between them using only w"
P16-1163,prasad-etal-2008-penn,0,0.92201,"ur model is trained end to end by BackPropagation and Adagrad. The main contribution of this paper can be summarized as follows: • We use word embeddings to replace the original words in the text segments to overcome data sparsity problem. In order to preserve the contextual information, we further encode the text segment to its positional representation through a recurrent neural network. • To deal with the semantic gap problem, we adopt a gated relevance network to capture the semantic interaction between the intermediate representations of the text segments. • Experimental results on PDTB (Prasad et al., 2008) show that the proposed method can achieve better performance in recognizing discourse level relations in all of the relations than the previous methods. 2 The Proposed Method The architecture of our proposed method is shown in figure 1. In the following of this section, we will illustrate the details of the proposed framework. 2.1 Embedding Layer To model the sentences with neural model, we firstly need to transform the one-hot representation of word into the distributed representation. All words of two text segments X and Y will be mapped into low dimensional vector representations, which ar"
P16-1163,E14-1068,0,0.467992,"e Network Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China jfchen14,qz,pfliu14,xpqiu,xjhuang@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held between them using only word pairs. Consider the fol"
P16-1163,N03-1030,0,0.090673,"on via a Deep Architecture with Gated Relevance Network Jifan Chen, Qi Zhang, Pengfei Liu, Xipeng Qiu, Xuanjing Huang Shanghai Key Laboratory of Data Science School of Computer Science, Fudan University 825 Zhangheng Road, Shanghai, China jfchen14,qz,pfliu14,xpqiu,xjhuang@fudan.edu.cn Abstract outnumber explicit relations in naturally occurring text, and identify those relations have been shown to be the performance bottleneck of an end-to-end discourse parser (Lin et al., 2014). Most of the existing researches used rich linguistic features and supervised learning methods to achieve the task (Soricut and Marcu, 2003; Pitler et al., 2009; Rutherford and Xue, 2014). Among their works, word pairs are heavily used as an important feature, since word pairs like (warm,cold) might directly trigger a contrast relation. However, because of the data sparsity problem (McKeown and Biran, 2013) and the lack of metrics to measure the semantic relation between those pairs, which is so-called the semantic gap problem (Zhao and Grosky, 2002), the classifiers based on word pairs in the previous studies did not work well. Moreover, some text segment pairs are more complicated, it is hard to determine the relation held betw"
P16-1163,P10-1040,0,0.0338307,"d+NTN: We use the neural tensor defined in (8) to capture the semantic interaction scores between every word embedding pair, the rest of the method is the same as our proposed method. • LSTM+NTN: We use two single LSTM to generate the positional text segments representation. The rest of the method is the same as Word-NTN. • BLSTM+NTN: We use two single bidirectional LSTM to generate the positional text nw = 50 ρ = 0.01 m = 32 (p, q) = (3, 3) r=2 3.2.2 Parameter Setting For the initialization of the word embeddings used in our model, we use the 50-dimensional pre-trained embeddings provided by Turian et al. (2010), and the embeddings are fixed during training. We only preserve the top 10,000 words according to its frequency of occurrence in the training data, all the text segments are padded to have the same length of 50, the intermediate representations of LSTM are also set to 50. The other parameters are initialized by randomly sampling from uniform distribution in [-0.1,0.1]. For other hyperparameters of our proposed model, we take those hyperparameters that achieved best performance on the development set, and keep the same parameters for other competitors. The final hyper-parameters are show in Ta"
P16-1163,K15-2001,0,0.0985963,"ation could help to determine which part of the two sentence should be focused when identifying their relation. 4 Related Work Discourse relations, which link clauses in text, are used to represent the overall text structure. Many downstream NLP tasks such as text summarization, question answering, and textual entailment can benefit from the task. Along with the increasing requirement, many works have been constructed to automatically identify these relations from different aspects (Pitler et al., 2008; Pitler et al., 2009; Zhou et al., 2010; McKeown and Biran, 2013; Rutherford and Xue, 2014; Xue et al., 2015). For training and comparing the performance of different methods, the Penn Discourse Treebank (PDTB) 2.0, which is large annotated discourse corpuses, were released in 2008 (Prasad et al., 2008). The annotation methodology of it follows the lexically grounded, predicate-argument approach. In PDTB, the discourse relations were predefined by Webber (2004). PDTB-styled discourse relations hold in only a local contextual window, and these relations are organized hierarchically. Also, every relation in PDTB has either an explicit or an implicit marker. Since explicit relations are easy to identify"
P16-1163,C10-2172,0,0.697668,"τ for parameter θτ,i . 3 3.1 Experiment Dataset The dataset we used in this work is Penn Discourse Treebank 2.0 (Prasad et al., 2008), which is one of the largest available annotated corpora of discourse relations. It contains 40,600 relations, which are manually annotated from the same 2,312 Wall Street Journal (WSJ) articles as the Penn Treebank. We follow the recommended section partition of PDTB 2.0, which is to use sections 2-20 for training, sections 21-22 for testing and the other sections for validation (Prasad et al., 2008). For comparison with the previous work (Pitler et al., 2009; Zhou et al., 2010; Park 1729 Table 1: The unbalanced sample distribution of PDTB. Relation Comparison Contingency Expansion Temporal Train 1894 3281 6792 665 Dev 401 628 1253 93 Table 2: Hyperparameters for our model in the experiment. Word Embedding size Initial learning rate Minibatch size Pooling Size Number of tensor slice Test 146 276 556 68 segments representation. The rest of the method is the same as Word-NTN. and Cardie, 2012; Rutherford and Xue, 2014; Ji and Eisenstein, 2015), we train four binary classifiers to identify each of the top level relations, the EntRel relations are merged with Expansion"
P16-1163,miltsakaki-etal-2004-penn,0,\N,Missing
P16-1163,C08-2022,0,\N,Missing
P17-1001,P07-1056,0,0.397818,"Missing"
P17-1001,P14-1062,0,0.00708944,"text sequence x = {x1 , x2 , · · · , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi . The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017). ˆ = softmax(WhT + b) y (5) ˆ is prediction probabilities, W is the where y weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi , yi ), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. Long Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhub"
P17-1001,P16-1098,1,0.203704,"opose an adversarial multi-task framework, in which the shared and private feature spaces are inIntroduction Multi-task learning is an effective approach to improve the performance of a single task with the help of other related tasks. Recently, neuralbased models for multi-task learning have become very popular, ranging from computer vision (Misra et al., 2016; Zhang et al., 2014) to natural language processing (Collobert and Weston, 2008; Luong et al., 2015), since they provide a convenient way of combining information from multiple tasks. However, most existing work on multi-task learning (Liu et al., 2016c,b) attempts to divide the features of different tasks into private and shared spaces, merely based on whether parameters of 1 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1–10 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1001 forget gate ft , an output gate ot , a memory cell ct and a hidden state ht . d is the number of the LSTM units. The elements of the gating vectors it , ft and ot are in [0, 1]. The LSTM is precisely specified as follows. herently disjoint b"
P17-1001,D15-1280,1,0.52053,"urrent Models for Text Classification Text Classification with LSTM Given a text sequence x = {x1 , x2 , · · · , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi . The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017). ˆ = softmax(WhT + b) y (5) ˆ is prediction probabilities, W is the where y weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi , yi ), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. L"
P17-1001,D16-1012,1,0.450341,"Missing"
P17-1001,N15-1092,0,0.0605154,"urrent Models for Text Classification Text Classification with LSTM Given a text sequence x = {x1 , x2 , · · · , xT }, we first use a lookup layer to get the vector representation (embeddings) xi of the each word xi . The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017). ˆ = softmax(WhT + b) y (5) ˆ is prediction probabilities, W is the where y weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi , yi ), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. L"
P17-1001,P11-1015,0,0.0560746,"Missing"
P17-1001,P05-1015,0,0.33778,"Missing"
P17-1001,D14-1162,0,0.098307,"Missing"
P17-1001,D13-1170,0,0.00557314,"lookup layer to get the vector representation (embeddings) xi of the each word xi . The output at the last moment hT can be regarded as the representation of the whole sequence, which has a fully connected layer followed by a softmax non-linear layer that predicts the probability distribution over classes. There are many neural sentence models, which can be used for text modelling, involving recurrent neural networks (Sutskever et al., 2014; Chung et al., 2014; Liu et al., 2015a), convolutional neural networks (Collobert et al., 2011; Kalchbrenner et al., 2014), and recursive neural networks (Socher et al., 2013). Here we adopt recurrent neural network with long short-term memory (LSTM) due to their superior performance in various NLP tasks (Liu et al., 2016a; Lin et al., 2017). ˆ = softmax(WhT + b) y (5) ˆ is prediction probabilities, W is the where y weight which needs to be learned, b is a bias term. Given a corpus with N training samples (xi , yi ), the parameters of the network are trained to minimise the cross-entropy of the predicted and true distributions. Long Short-term Memory Long short-term memory network (LSTM) (Hochreiter and Schmidhuber, 1997) is a type of recurrent neural network (RNN)"
P19-1100,P18-3015,0,0.612259,"ering problem? Understanding the above questions can not only help us to choose suitable architectures in different application scenarios, but motivate us to move forward to more powerful frameworks. External Transferable Knowledge and Learning schemas Clearly, the improvement in accuracy and performance is not merely because of the shift from feature engineering to structure engineering, but the flexible ways to incorporate external knowledge (Mikolov et al., 2013; Peters et al., 2018a; Devlin et al., 2018) and learning schemas to introduce extra instructive constraints (Paulus et al., 2017; Arumae and Liu, 2018). For this part, we make some first steps toward answers to the following questions: 1) Which type of pre-trained models (supervised or unsupervised pre-training) is more friendly to the summarization task? 2) When architectures are explored exhaustively, can we push the state-of-the-art results to a new level by introducing external transferable knowledge or changing another learning schema? To make a comprehensive study of above an1049 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1049–1058 c Florence, Italy, July 28 - August 2, 2019. 2019 Ass"
P19-1100,N18-1150,0,0.0446013,"ow neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Additionally, we find an effective way to improve current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally"
P19-1100,P18-1063,0,0.35113,"mprove current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can generate grammatically and coherent summaries (Nallapati et al., 2017). and seek to ∗ These two authors contributed equally. Corresponding author. 1 https://github.com"
P19-1100,P16-1046,0,0.224131,"able knowledge influence extractive summarization and a more popular neural architecture, Transformer. Besides, we come to inconsistent conclusions when analyzing the auto-regressive decoder. More importantly, our paper also shows how existing systems can be improved, and we have achieved a state-of-the-art performance on CNN/DailyMail. Extractive Summarization Most of recent work attempt to explore different neural components or their combinations to build an end-to-end learning model. Specifically, these work instantiate their encoder-decoder framework by choosing recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018) as encoder, auto-regressive decoder (Chen and 1050 Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018) or non auto-regressive decoder (Isonuma et al., 2017; Narayan et al., 2018; Arumae and Liu, 2018) as decoder, based on pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014). However, how to use Transformer in extractive summarization is still a missing issue. In addition, some work uses reinforcement learning technique (Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), which can provide more direct optim"
P19-1100,D18-1409,0,0.714632,"Missing"
P19-1100,D18-1443,0,0.128821,"ks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can generate grammatically and coherent summaries (Nallapati et al., 2017). and seek to ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/fastnlp/fastNLP † Archi"
P19-1100,N18-1065,0,0.119485,"Missing"
P19-1100,D17-1223,0,0.0153301,"y, our paper also shows how existing systems can be improved, and we have achieved a state-of-the-art performance on CNN/DailyMail. Extractive Summarization Most of recent work attempt to explore different neural components or their combinations to build an end-to-end learning model. Specifically, these work instantiate their encoder-decoder framework by choosing recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018) as encoder, auto-regressive decoder (Chen and 1050 Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018) or non auto-regressive decoder (Isonuma et al., 2017; Narayan et al., 2018; Arumae and Liu, 2018) as decoder, based on pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014). However, how to use Transformer in extractive summarization is still a missing issue. In addition, some work uses reinforcement learning technique (Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), which can provide more direct optimization goals. Although above work improves the performance of summarization system from different perspectives, yet a comprehensive study remains missing. 3 A Testbed for Text Summarization To analyze neu"
P19-1100,P18-1014,0,0.461649,"rization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Additionally, we find an effective way to improve current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can gener"
P19-1100,D18-1208,0,0.308389,"g a right direction. 2 Related Work The work is connected to the following threads of work of NLP research. Task-oriented Neural Networks Interpreting Without knowing the internal working mechanism of the neural network, it is easy for us to get into a hobble when the performance of a task has reached the bottleneck. More recently, Peters et al. (2018b) investigate how different learning frameworks influence the properties of learned contextualized representations. Different from this work, in this paper, we focus on dissecting the neural models for text summarization. A similar work to us is Kedzie et al. (2018), which studies how deep learning models perform context selection in terms of several typical summarization architectures, and domains. Compared with this work, we make a more comprehensive study and give more different analytic aspects. For example, we additionally investigate how transferable knowledge influence extractive summarization and a more popular neural architecture, Transformer. Besides, we come to inconsistent conclusions when analyzing the auto-regressive decoder. More importantly, our paper also shows how existing systems can be improved, and we have achieved a state-of-the-art"
P19-1100,D14-1181,0,0.0110666,"Missing"
P19-1100,P16-1098,1,0.838798,"te the LSTM-based structure and the Transformer structure, both of which have proven to be effective and achieved the state-of-the-art results in many other NLP tasks. Notably, to let the model make the best of its structural bias, stacking deep layers is allowed. LSTM Layer Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning long-term dependencies, which has proven to be effective in a wide range of NLP tasks, such as text classification (Liu et al., 2017, 2016b), semantic matching (Rockt¨aschel et al., 2015; Liu et al., 2016a), text summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). Transformer Layer Transformer (Vaswani et al., 2017) is essentially a feed-forward selfattention architecture, which achieves pairwise interaction by attention mechanism. Recently, Transformer has achieved great success in many other NLP tasks (Vaswani et al., 2017; Dai et al., 2018), and it is appealing to know how this neural module performs on text summarization task. 3.2.3 Decoder Decoder is used to extract a subset of sentences from the original document based on contextualized representations: s1"
P19-1100,P17-1001,1,0.855402,"zed representations s1 , · · · , sn . To achieve this goal, we investigate the LSTM-based structure and the Transformer structure, both of which have proven to be effective and achieved the state-of-the-art results in many other NLP tasks. Notably, to let the model make the best of its structural bias, stacking deep layers is allowed. LSTM Layer Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning long-term dependencies, which has proven to be effective in a wide range of NLP tasks, such as text classification (Liu et al., 2017, 2016b), semantic matching (Rockt¨aschel et al., 2015; Liu et al., 2016a), text summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). Transformer Layer Transformer (Vaswani et al., 2017) is essentially a feed-forward selfattention architecture, which achieves pairwise interaction by attention mechanism. Recently, Transformer has achieved great success in many other NLP tasks (Vaswani et al., 2017; Dai et al., 2018), and it is appealing to know how this neural module performs on text summarization task. 3.2.3 Decoder Decoder is used to extract a subset of sentence"
P19-1100,K16-1028,0,0.137143,"Missing"
P19-1100,N18-1158,0,0.164021,"s how existing systems can be improved, and we have achieved a state-of-the-art performance on CNN/DailyMail. Extractive Summarization Most of recent work attempt to explore different neural components or their combinations to build an end-to-end learning model. Specifically, these work instantiate their encoder-decoder framework by choosing recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al., 2017; Zhou et al., 2018) as encoder, auto-regressive decoder (Chen and 1050 Bansal, 2018; Jadhav and Rajan, 2018; Zhou et al., 2018) or non auto-regressive decoder (Isonuma et al., 2017; Narayan et al., 2018; Arumae and Liu, 2018) as decoder, based on pre-trained word representations (Mikolov et al., 2013; Pennington et al., 2014). However, how to use Transformer in extractive summarization is still a missing issue. In addition, some work uses reinforcement learning technique (Narayan et al., 2018; Wu and Hu, 2018; Chen and Bansal, 2018), which can provide more direct optimization goals. Although above work improves the performance of summarization system from different perspectives, yet a comprehensive study remains missing. 3 A Testbed for Text Summarization To analyze neural summarization syst"
P19-1100,D14-1162,0,0.0931397,"Enc. represent decoder and encoder respectively. Sup. denotes supervised learning and NEWS. means supervised pre-training knowledge. alytical perspectives, we first build a testbed for summarization system, in which training and testing environment will be constructed. In the training environment, we design different summarization models to analyze how they influence the performance. Specifically, these models differ in the types of architectures (Encoders: CNN, LSTM, Transformer (Vaswani et al., 2017); Decoders: auto-regressive2 , non auto-regressive), external transferable knowledge (GloVe (Pennington et al., 2014), BERT (Devlin et al., 2018), N EWSROOM (Grusky et al., 2018)) and different learning schemas (supervised learning and reinforcement learning). To peer into the internal working mechanism of above testing cases, we provide sufficient evaluation scenarios in the testing environment. Concretely, we present a multi-domain test, sentence shuffling test, and analyze models by different metrics: repetition, sentence length, and position bias, which we additionally developed to provide a better understanding of the characteristics of different datasets. Empirically, our main observations are summariz"
P19-1100,N18-1202,0,0.151942,"for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can generate grammatically and coherent summaries (Nallapati et al., 2017). and seek to ∗ These two authors contributed equally. Corresponding author. 1 https://github.com/fastnlp/fastNLP † Architectures Architecturally, the better performance usually comes at the cost of our understanding of the system. To date, we know little about the functionality of each neural component and the differences between them (Peters et al., 2018b), which raises the following typical questions: 1) How does the choice of different neural architectures (CNN, RNN, Transformer) influence the performance of the summarization system? 2) Which part of components matters for specific dataset? 3) Do current models suffer from the over-engineering problem? Understanding the above questions can not only help us to choose suitable architectures in different application scenarios, but motivate us to move forward to more powerful frameworks. External Transferable Knowledge and Learning schemas Clearly, the improvement in accuracy and performance is"
P19-1100,D15-1044,0,0.131669,"sformer structure, both of which have proven to be effective and achieved the state-of-the-art results in many other NLP tasks. Notably, to let the model make the best of its structural bias, stacking deep layers is allowed. LSTM Layer Long short-term memory network (LSTM) was proposed by (Hochreiter and Schmidhuber, 1997) to specifically address this issue of learning long-term dependencies, which has proven to be effective in a wide range of NLP tasks, such as text classification (Liu et al., 2017, 2016b), semantic matching (Rockt¨aschel et al., 2015; Liu et al., 2016a), text summarization (Rush et al., 2015) and machine translation (Sutskever et al., 2014). Transformer Layer Transformer (Vaswani et al., 2017) is essentially a feed-forward selfattention architecture, which achieves pairwise interaction by attention mechanism. Recently, Transformer has achieved great success in many other NLP tasks (Vaswani et al., 2017; Dai et al., 2018), and it is appealing to know how this neural module performs on text summarization task. 3.2.3 Decoder Decoder is used to extract a subset of sentences from the original document based on contextualized representations: s1 , · · · , sn . Most existing architecture"
P19-1100,P17-1099,0,0.722919,"etter understand how neural extractive summarization systems could benefit from different types of model architectures, transferable knowledge and learning schemas. Additionally, we find an effective way to improve current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization sinc"
P19-1100,P18-1061,0,0.289883,"effective way to improve current frameworks and achieve the state-ofthe-art result on CNN/DailyMail by a large margin based on our observations and analyses. Hopefully, our work could provide more clues for future research on extractive summarization. Source code will be available on Github1 . 1 Introduction Recent years has seen remarkable success in the use of deep neural networks for text summarization (See et al., 2017; Celikyilmaz et al., 2018; Jadhav and Rajan, 2018). So far, most research utilizing the neural network for text summarization has revolved around architecture engineering (Zhou et al., 2018; Chen and Bansal, 2018; Gehrmann et al., 2018). Despite their success, it remains poorly understood why they perform well and what their shortcomings are, which limits our ability to design better architectures. The rapid development of neural architectures calls for a detailed empirical study of analyzing and understanding existing models. In this paper, we primarily focus on extractive summarization since they are computationally efficient, and can generate grammatically and coherent summaries (Nallapati et al., 2017). and seek to ∗ These two authors contributed equally. Corresponding autho"
P19-1100,W04-1013,0,\N,Missing
P19-1100,D18-1179,0,\N,Missing
P19-1406,D17-1098,0,0.0195216,"b), missing value reconstruction (e.g. for damaged or historical documents) (Berglund et al., 2015), acrostic poetry generation (Liu et al., 2018a), and text representation learning (Devlin et al., 2018). Text infilling is an under-explored challenging task in the field of text generation. Recently, sequence generative models like sequenceto-sequence (seq2seq) models (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) are widely used in text generation tasks, including neural machine translation (Wu et al., 2016; Vaswani et al., 2017), image captioning (Anderson et al., 2017), abstractive summarization (See et al., 2017), and dialogue generation (Mei et al., 2017). Unfortunately, given a well-trained2 neural seq2seq model or unconditional neural language model (Mikolov et al., 2010), it is a daunting task to directly apply it to text infilling task. As shown in Figure 1, we observe that the infilled words should be conditioned on past and future information around the missing part, which is contrary to the popular learning paradigm, namely, each output symbol is conditioned on all previous outputs during inference by using unidirectional Beam Search (BS) (Och and"
P19-1406,E17-1059,0,0.0163322,"to add an additional regularization term that directly narrow the distance between yˆjemb and its nearest word embedding in Wemb , which is used in Cheng et al. (2018) for seq2seq adversarial attacks, but no obvious improvement was found. Given the loss L(x, y ∗ ), yˆjemb is updated with ∇yˆemb L(x, y ∗ ) by one-step gradient descent: j yˆjemb = yˆjemb − α · ∇yˆemb LN LL (x, y ∗ ). j 3 yk ∈V (5) Instead of updating yˆjemb by na¨ıvely SGD algorithm, we experimentally find that Nesterov (Sutskever et al., 2013) optimizer performed better than other optimizers to update yˆjemb . As discussed in Dong et al. (2017b), this momentum based optimizer can stabilize update directions and escape from poor local maxima during the iterations for adversarial attack. In P-step, we aim to project the yˆjemb into a valid infilled word yˆj . A na¨ıve way is to find the word whose word embedding in Wemb is nearest to yˆjemb based on the distance metric function dist(·)3 . However, due to its high dimensionality, Through experiments we find that using Euclidean distance as metric function dist(·) perform slightly better than Cosine distance for our method. (6) Set yˆj = arg min LN LL (x, y ∗ ) y ˆj ∈S end for Update y"
P19-1406,I17-1099,0,0.0272979,"Missing"
P19-1406,D15-1166,0,0.124517,"Missing"
P19-1406,J04-4002,0,0.458918,"., 2017), abstractive summarization (See et al., 2017), and dialogue generation (Mei et al., 2017). Unfortunately, given a well-trained2 neural seq2seq model or unconditional neural language model (Mikolov et al., 2010), it is a daunting task to directly apply it to text infilling task. As shown in Figure 1, we observe that the infilled words should be conditioned on past and future information around the missing part, which is contrary to the popular learning paradigm, namely, each output symbol is conditioned on all previous outputs during inference by using unidirectional Beam Search (BS) (Och and Ney, 2004). To solve the issues above, one family of methods for text infilling is “trained to fill in blanks” (Berglund et al., 2015; Fedus et al., 2018; Zhu et al., 2019), which requires large amounts of data in fill-in-the-blank format to train a new model that takes the output template as a conditional in∗ Correspondence to Jiancheng Lv. Our code and data are available https://github.com/dayihengliu/ Text-Infilling-Gradient-Search 1 at 2 Here “well-trained” means that ones focus on popular model settings and data sets, and follow standard training protocols. 4146 Proceedings of the 57th Annual Meeti"
P19-1406,P02-1040,0,0.104398,"13 BLEU Template Seq2Seq-f Seq2Seq-b Seq2Seq-f+b BiRNN-BiBS BiRNN-GSN Mask-Seq2Seq Mask-Self-attn TIGS (ours) 0.503 0.781 0.779 0.812 0.867 0.879 0.860 0.878 0.883 0.692 0.897 0.896 0.905 0.896 0.904 0.900 0.914 0.911 0.127 0.623 0.616 0.658 0.715 0.751 0.750 0.778 0.774 0.432 0.881 0.872 0.887 0.869 0.884 0.856 0.882 0.889 0.009 0.682 0.683 0.703 0.740 0.736 0.754 0.778 0.769 0.182 0.879 0.864 0.884 0.856 0.882 0.835 0.870 0.878 Dialog Poetry APRC Table 1: BLEU and NLL results. 5.3 Metrics Following Sun et al. (2017), we compare methods on standard sentence-level metric BLEU scores (4-gram) (Papineni et al., 2002) which considers the correspondence between the ground truth and the complete sentences. However, such a metric also has some deficiencies in text infilling tasks. For example, given two complete sentences with only one word different, the sentence level statistics of them may be quite similar, whereas a human can clearly tell which one is most natural. Moreover, given a template, there may be several reasonable ways to fill in the blanks. For example, this book, highly recomgiven a template, “i mend it”, it is reasonable to fill the word “love” or “like” in the blank. However, since there is"
P19-1406,P17-1099,0,0.0508645,"or historical documents) (Berglund et al., 2015), acrostic poetry generation (Liu et al., 2018a), and text representation learning (Devlin et al., 2018). Text infilling is an under-explored challenging task in the field of text generation. Recently, sequence generative models like sequenceto-sequence (seq2seq) models (Sutskever et al., 2014; Bahdanau et al., 2014; Gehring et al., 2017; Vaswani et al., 2017) are widely used in text generation tasks, including neural machine translation (Wu et al., 2016; Vaswani et al., 2017), image captioning (Anderson et al., 2017), abstractive summarization (See et al., 2017), and dialogue generation (Mei et al., 2017). Unfortunately, given a well-trained2 neural seq2seq model or unconditional neural language model (Mikolov et al., 2010), it is a daunting task to directly apply it to text infilling task. As shown in Figure 1, we observe that the infilled words should be conditioned on past and future information around the missing part, which is contrary to the popular learning paradigm, namely, each output symbol is conditioned on all previous outputs during inference by using unidirectional Beam Search (BS) (Och and Ney, 2004). To solve the issues above, one fam"
P19-1406,1983.tc-1.13,0,0.300668,"Missing"
S14-2092,W03-0430,0,0.130714,"(?|x) word sequence ? label sequence Predict (b) CRF model for sequence labeling Figure 1: The Architecture, the MaxEnt and CRF Models of the SeemGo System. where θk is a weight parameter to be estimated for the corresponding feature function fk (x, y), and Z(x) is a normalizing factor over all classes to ensure a proper probability. K is the total number of feature functions. 3.1.2 Conditional Random Fields CRF is an extension to the MaxEnt model for handling sequence data. The linear-chain CRF is a special case of CRF that obeys the Markov property between its neighbouring labels. Following McCallum and Li (2003), Formula 2 defines the linear-chain CRF: y = {yt }Tt=1 , x = {xt }Tt=1 are label sequence and observation sequence respectively, and there are K arbitrary feature functions {fk }1≤k≤K and the corresponding weight parameters {θk }1≤k≤K . Z(x) is a normalizing factor over all label sequences. T X K X 1 P (y|x) = exp θk fk (yt , yt−1 , x, t) Z(x) t=1 ! (2) k=1 In the labeling phase, the Viterbi decoding algorithm is applied to find the best label sequence y∗ for the observation sequence x. 3.2 Background Subtask 1: Aspect Term Extraction The datasets (Laptops and Restaurants) are provided in XML"
S14-2092,W02-1011,0,0.0215103,"in the sentences. The task of aspect category detection is essentially a text classification problem, for which many techniques exist. Joachims (1998) explored the use of Support Vector Machines (SVM) for text categorization and obtained good performance due to their ability to generalize well in highdimensional feature spaces. Nigam et al. (1999) proposed the MaxEnt model for document classification by estimating the conditional distribution of the class variable give the document, and showed that MaxEnt is significantly better that Naive Bayes on some datasets. For polarity classification, Pang et al. (2002) conducted experiments on movie reviews and showed that standard machine learning techniques (e.g., Naive Bayes, SVM and MaxEnt) outperform human-produced baselines. 3 Test sentence: Training Set ?1 label …… ? ? word count Train ?? label MaxEnt P(?|?) ? label Predict (a) MaxEnt model for label classification Test sentence: Training Set Transform ?1 label sequence …… ? ? word sequence ?? label sequence I’ve been to several places for Dim Sum and this has got to be the WORST. ? ?1 word sequence Train CRF P(?|x) word sequence ? label sequence Predict (b) CRF model for sequence labeling Figure 1:"
S14-2092,H05-1045,0,0.0380045,"nizes noun phrases from sentences, while NER extracts a set of entities such as Person, Place, and Organization. Both NPC and NER are sequential learning problems and they are typically modelled by sequence models such as Hidden Markov Model (HMM) and CRF (Finkel et al., 2005). For the task of aspect term extraction, some related papers also model it with sequence models. Jin et al. (2009) proposed an HMM-based framework to extract product entities and associated opinion orientations by integrating linguistic features such as part-of-speech tag, lexical patterns and surrounding words/phrases. Choi et al. (2005) proposed a hybrid approach using both CRF and extraction patterns to identify sources of opinions in text. Jakob and Gurevych (2010) described a Introduction In this paper, we present the SeemGo system developed for the task of Aspect Based Sentiment Analysis in SemEval-2014. The task consists of four subtasks: (1) aspect term extraction (identify particular aspects of a given entity, e.g., laptop, restaurant, etc.); (2) aspect category detection (detect the category of a given sentence, e.g., food, service for a restaurant, etc.), (3) aspect term polarity, and (4) aspect category polarity. T"
S14-2092,N03-1028,0,0.0191999,"sed features and context features. The other three subtasks are solved by the Maximum Entropy model, with the occurrence counts of unigram and bigram words of each sentence as features. The subtask of aspect category detection obtains the best result when applying the Boosting method on the Maximum Entropy model, with the precision of 0.869 for Restaurants. The Maximum Entropy model also shows good performance in the subtasks of both aspect term and aspect category polarity classification. 1 2 Related Work The subtask of aspect term extraction is quite similar with Noun Phrase Chunking (NPC) (Sha and Pereira, 2003) and Named Entity Recognition (NER) (Finkel et al., 2005). NPC recognizes noun phrases from sentences, while NER extracts a set of entities such as Person, Place, and Organization. Both NPC and NER are sequential learning problems and they are typically modelled by sequence models such as Hidden Markov Model (HMM) and CRF (Finkel et al., 2005). For the task of aspect term extraction, some related papers also model it with sequence models. Jin et al. (2009) proposed an HMM-based framework to extract product entities and associated opinion orientations by integrating linguistic features such as"
S14-2092,P05-1045,0,0.333592,"are solved by the Maximum Entropy model, with the occurrence counts of unigram and bigram words of each sentence as features. The subtask of aspect category detection obtains the best result when applying the Boosting method on the Maximum Entropy model, with the precision of 0.869 for Restaurants. The Maximum Entropy model also shows good performance in the subtasks of both aspect term and aspect category polarity classification. 1 2 Related Work The subtask of aspect term extraction is quite similar with Noun Phrase Chunking (NPC) (Sha and Pereira, 2003) and Named Entity Recognition (NER) (Finkel et al., 2005). NPC recognizes noun phrases from sentences, while NER extracts a set of entities such as Person, Place, and Organization. Both NPC and NER are sequential learning problems and they are typically modelled by sequence models such as Hidden Markov Model (HMM) and CRF (Finkel et al., 2005). For the task of aspect term extraction, some related papers also model it with sequence models. Jin et al. (2009) proposed an HMM-based framework to extract product entities and associated opinion orientations by integrating linguistic features such as part-of-speech tag, lexical patterns and surrounding word"
S14-2092,D10-1101,0,0.0247749,"NER are sequential learning problems and they are typically modelled by sequence models such as Hidden Markov Model (HMM) and CRF (Finkel et al., 2005). For the task of aspect term extraction, some related papers also model it with sequence models. Jin et al. (2009) proposed an HMM-based framework to extract product entities and associated opinion orientations by integrating linguistic features such as part-of-speech tag, lexical patterns and surrounding words/phrases. Choi et al. (2005) proposed a hybrid approach using both CRF and extraction patterns to identify sources of opinions in text. Jakob and Gurevych (2010) described a Introduction In this paper, we present the SeemGo system developed for the task of Aspect Based Sentiment Analysis in SemEval-2014. The task consists of four subtasks: (1) aspect term extraction (identify particular aspects of a given entity, e.g., laptop, restaurant, etc.); (2) aspect category detection (detect the category of a given sentence, e.g., food, service for a restaurant, etc.), (3) aspect term polarity, and (4) aspect category polarity. The polarity of each aspect term or aspect category includes positive, negative, neutral or conflict (i.e., both positive and negative"
