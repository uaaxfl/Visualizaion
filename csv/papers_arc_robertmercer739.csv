2020.lrec-1.380,A Lexicon-Based Approach for Detecting Hedges in Informal Text,2020,-1,-1,3,0,17424,jumayel islam,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Hedging is a commonly used strategy in conversational management to show the speaker{'}s lack of commitment to what they communicate, which may signal problems between the speakers. Our project is interested in examining the presence of hedging words and phrases in identifying the tension between an interviewer and interviewee during a survivor interview. While there have been studies on hedging detection in the natural language processing literature, all existing work has focused on structured texts and formal communications. Our project thus investigated a corpus of eight unstructured conversational interviews about the Rwanda Genocide and identified hedging patterns in the interviewees{'} responses. Our work produced three manually constructed lists of hedge words, booster words, and hedging phrases. Leveraging these lexicons, we developed a rule-based algorithm that detects sentence-level hedges in informal conversations such as survivor interviews. Our work also produced a dataset of 3000 sentences having the categories Hedge and Non-hedge annotated by three researchers. With experiments on this annotated dataset, we verify the efficacy of our proposed algorithm. Our work contributes to the further development of tools that identify hedges from informal conversations and discussions."
2020.lrec-1.516,Multilingual Corpus Creation for Multilingual Semantic Similarity Task,2020,-1,-1,3,0,17692,mahtab ahmed,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In natural language processing, the performance of a semantic similarity task relies heavily on the availability of a large corpus. Various monolingual corpora are available (mainly English); but multilingual resources are very limited. In this work, we describe a semi-automated framework to create a multilingual corpus which can be used for the multilingual semantic similarity task. The similar sentence pairs are obtained by crawling bilingual websites, whereas the dissimilar sentence pairs are selected by applying topic modeling and an Open-AI GPT model on the similar sentence pairs. We focus on websites in the government, insurance, and banking domains to collect English-French and English-Spanish sentence pairs; however, this corpus creation approach can be applied to any other industry vertical provided that a bilingual website exists. We also show experimental results for multilingual semantic similarity to verify the quality of the corpus and demonstrate its usage."
2020.argmining-1.10,Use of Claim Graphing and Argumentation Schemes in Biomedical Literature: A Manual Approach to Analysis,2020,-1,-1,2,0,22331,eli moser,Proceedings of the 7th Workshop on Argument Mining,0,"Argumentation in an experimental life science paper consists of a main claim being supported with reasoned argumentative steps based on the data garnered from the experiments that were carried out. In this paper we report on an investigation of the large scale argumentation structure found when examining five biochemistry journal publications. One outcome of this investigation of biochemistry articles suggests that argumentation schemes originally designed for genetic research articles may transfer to experimental biomedical literature in general. Our use of these argumentation schemes shows that claims depend not only on experimental data but also on other claims. The tendency for claims to use other claims as their supporting evidence in addition to the experimental data led to two novel models that have provided a better understanding of the large scale argumentation structure of a complete biochemistry paper. First, the claim graph displays the claims within a paper, their interactions, and their evidence. Second, another aspect of this argumentation network is further illustrated by the Model of Informational Hierarchy (MIH) which visualizes at a meta-level the flow of reasoning provided by the authors of the paper and also connects the main claim to the paper{'}s title. Together, these models, which have been produced by a manual examination of the biochemistry articles, would be likely candidates for a computational method that analyzes the large scale argumentation structure."
W19-5018,Incorporating Figure Captions and Descriptive Text in {M}e{SH} Term Indexing,2019,0,0,2,0,21970,xindi wang,Proceedings of the 18th BioNLP Workshop and Shared Task,0,"The goal of text classification is to automatically assign categories to documents. Deep learning automatically learns effective features from data instead of adopting human-designed features. In this paper, we focus specifically on biomedical document classification using a deep learning approach. We present a novel multichannel TextCNN model for MeSH term indexing. Beyond the normal use of the text from the abstract and title for model training, we also consider figure and table captions, as well as paragraphs associated with the figures and tables. We demonstrate that these latter text sources are important feature sources for our method. A new dataset consisting of these text segments curated from 257,590 full text articles together with the articles{'} MEDLINE/PubMed MeSH terms is publicly available."
W19-4514,Annotation of Rhetorical Moves in Biochemistry Articles,2019,0,1,2,0,24139,mohammed alliheedi,Proceedings of the 6th Workshop on Argument Mining,0,"This paper focuses on the real world application of scientific writing and on determining rhetorical moves, an important step in establishing the argument structure of biomedical articles. Using the observation that the structure of scholarly writing in laboratory-based experimental sciences closely follows laboratory procedures, we examine most closely the Methods section of the texts and adopt an approach of identifying rhetorical moves that are procedure-oriented. We also propose a verb-centric frame semantics with an effective set of semantic roles in order to support the analysis. These components are designed to support a computational model that extends a promising proposal of appropriate rhetorical moves for this domain, but one which is merely descriptive. Our work also contributes to the understanding of argument-related annotation schemes. In particular, we conduct a detailed study with human annotators to confirm that our selection of semantic roles is effective in determining the underlying rhetorical structure of existing biomedical articles in an extensive dataset. The annotated dataset that we produce provides the important knowledge needed for our ultimate goal of analyzing biochemistry articles."
P19-1030,You Only Need Attention to Traverse Trees,2019,0,1,3,0,17692,mahtab ahmed,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"In recent NLP research, a topic of interest is universal sentence encoding, sentence representations that can be used in any supervised task. At the word sequence level, fully attention-based models suffer from two problems: a quadratic increase in memory consumption with respect to the sentence length and an inability to capture and use syntactic information. Recursive neural nets can extract very good syntactic information by traversing a tree structure. To this end, we propose Tree Transformer, a model that captures phrase level syntax for constituency trees as well as word-level dependencies for dependency trees by doing recursive traversal only with attention. Evaluation of this model on four tasks gets noteworthy results compared to the standard transformer and LSTM-based models as well as tree-structured LSTMs. Ablation studies to find whether positional information is inherently encoded in the trees and which type of attention is suitable for doing the recursive traversal are provided."
N19-1137,Multi-Channel Convolutional Neural Network for {T}witter Emotion and Sentiment Recognition,2019,0,2,2,0,17424,jumayel islam,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"The advent of micro-blogging sites has paved the way for researchers to collect and analyze huge volumes of data in recent years. Twitter, being one of the leading social networking sites worldwide, provides a great opportunity to its users for expressing their states of mind via short messages which are called tweets. The urgency of identifying emotions and sentiments conveyed through tweets has led to several research works. It provides a great way to understand human psychology and impose a challenge to researchers to analyze their content easily. In this paper, we propose a novel use of a multi-channel convolutional neural architecture which can effectively use different emotion and sentiment indicators such as hashtags, emoticons and emojis that are present in the tweets and improve the performance of emotion and sentiment identification. We also investigate the incorporation of different lexical features in the neural network model and its effect on the emotion and sentiment identification task. We analyze our model on some standard datasets and compare its effectiveness with existing techniques."
W15-2706,Identification and Disambiguation of Lexical Cues of Rhetorical Relations across Different Text Genres,2015,34,2,3,0,36907,taraneh khazaei,"Proceedings of the First Workshop on Linking Computational Models of Lexical, Sentential and Discourse-level Semantics",0,"Lexical cues are linguistic expressions that can signal the presence of a rhetorical relation. However, such cues can be ambiguous as they may signal more than one relation or may not always function as a relation indicator. In this study, we first conduct a corpus-based analysis to derive a set of n-grams as potential lexical cues. These cues are then utilized in graph-based probabilistic models to determine the syntactic context in which the cue is signaling the presence of a particular relation. Evaluation results are reported for various cues of the CIRCUMSTANCE relation, confirming the value of syntactic features for the task of cue disambiguation in the context of Rhetorical Structure Theory. Moreover, using a graph to encode syntactic information is shown to be a more generalizable and effective approach compared to the direct usage of syntactic features."
W14-2624,The Use of Text Similarity and Sentiment Analysis to Examine Rationales in the Large-Scale Online Deliberations,2014,19,2,3,0,38656,wanting mao,"Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"To overcome the increasingly time consuming and potentially challenging identification of key points and the associated rationales in large-scale online deliberations, we propose a computational linguistics method that has the potential of facilitating this process of reading and evaluating the text. Our approach is novel in how we determine the sentiment of a rationale at the sentence level and in that it includes a text similarity measure and sentence-level sentiment analysis to achieve this goal."
W14-2103,An automated method to build a corpus of rhetorically-classified sentences in biomedical texts,2014,16,11,2,0,38701,hospice houngbo,Proceedings of the First Workshop on Argumentation Mining,0,"The rhetorical classification of sentences in biomedical texts is an important task in the recognition of the components of a scientific argument. Generating supervised machine learned models to do this recognition requires corpora annotated for the rhetorical categories Introduction (or Background), Method, Result, Discussion (or Conclusion). Currently, a few, small annotated corpora exist. We use a straightforward feature of co-referring text using the word xe2x80x9cthisxe2x80x9d to build a selfannotating corpus extracted from a large biomedical research paper dataset. The corpus is annotated for all of the rhetorical categories except Introduction without involving domain experts. In a 10-fold cross-validation, we report an overall F"
W14-2113,Titles That Announce Argumentative Claims in Biomedical Research Articles,2014,13,2,3,0,38712,heather graves,Proceedings of the First Workshop on Argumentation Mining,0,"In the experimental sciences authors use the scientific article to express their findings by making an argumentative claim. While past studies have located the claim in the Abstract, the Introduction, and in the Discussion section, in this paper we focus on the article title as a potential source of the claim. Our investigation has suggested that titles which contain a tensed verb almost certainly announce the argument claim while titles which do not contain a tensed verb have varied announcements. Another observation that we have confirmed in our dataset is that the frequency of verbs in titles of experimental research articles has increased over time."
W14-2114,Extracting Higher Order Relations From Biomedical Text,2014,4,4,2,0,38715,syeed faiz,Proceedings of the First Workshop on Argumentation Mining,0,"Argumentation in a scientific article is composed of unexpressed and explicit statements of old and new knowledge combined into a logically coherent textual argument. Discourse relations, linguistic coherence relations that connect discourse segments, help to communicate an argumentxe2x80x99s logical steps. A biomedical relation exhibits a relationship between biomedical entities. In this paper, we are primarily concerned with the extraction of connections between biomedical relations, a connection that we call a higher order relation. We combine two methods, namely biomedical relation extraction and discourse relation parsing, to extract such higher order relations from biomedical research articles. Finding and extracting these relations can assist in automatically understanding the scientific arguments expressed by the author in the text."
W14-2117,Extracting Imperatives from {W}ikipedia Article for Deletion Discussions,2014,9,3,2,0,38720,fiona mao,Proceedings of the First Workshop on Argumentation Mining,0,"Wikipedia contains millions of articles, collaboratively produced. If an article is controversial, an online xe2x80x9cArticle for Deletionxe2x80x9d (AfD) discussion is held to determine whether the article should be deleted. It is open to any user to participate and make a comment or argue an opinion. Some of these comments and arguments can be counter-arguments, attacks in Dungxe2x80x99s (1995) argumentation terminology. Here, we consider the extraction of one type of attack, the directive speech act formed as an imperative."
J12-2009,Book Review: The Structure of Scientific Articles: Applications to Citation Indexing and Summarization by Simone Teufel,2012,0,0,1,1,17425,robert mercer,Computational Linguistics,0,None
C12-1074,Method Mention Extraction from Scientific Research Papers,2012,23,5,2,0,38701,hospice houngbo,Proceedings of {COLING} 2012,0,"Scientific publications contain many references to method terminologies used during scientific experiments. New terms are constantly created within the research community, especially in the biomedical domain where thousands of papers are published each week. In this study we report our attempt to automatically extract such method terminologies from scientific research papers, using rule-based and machine learning techniques. We first used some linguistic features to extract fine-grained method sentences from a large biomedical corpus and then applied well established methodologies to extract the method terminologies. We focus the present study on the extraction of method phrases that contain an explicit mention of method keywords such as (algorithm, technique, analysis, approach and method) and other less explicit method terms such as Multiplex Ligation dependent Probe Amplification. Our initial results show an average F-score of 91.89 for the rule-based system and 78.26 for the Conditional Random Field-based machine learning system."
C12-1087,A Machine Learning Approach for Phenotype Name Recognition,2012,38,7,2,0,43767,maryam khordad,Proceedings of {COLING} 2012,0,"Extracting biomedical named entities is one of the major challenges in automatic processing of biomedical literature. This paper proposes a machine learning approach for finding phenotype names in text. Features are included in a machine learning infrastructure to implement the rules found in our previously developed rule-based system. The system also uses two available resources: MetaMap and HPO. As we are not aware of any available corpus for phenotype names, a corpus has been constructed. Since manual tagging of the corpus was not possible for us, we started tagging only HPO phenotypes in the corpus and then using a semi-supervised learning method, the tagging process improved. The evaluation results (F-Score 92.25) suggest that the system achieved good performance and it outperforms the rule-based system."
W04-3113,A Design Methodology for a Biomedical Literature Indexing Tool Using the Rhetoric of Science,2004,23,25,1,1,17425,robert mercer,"{HLT}-{NAACL} 2004 Workshop: Linking Biological Literature, Ontologies and Databases",0,"Literature indexing tools provide researchers with a means to navigate through the network of scholarly scientific articles in a subject domain. We propose that more effective indexing tools may be designed using the links between articles provided by citations. With the explosion in the amount of scientific literature and with the advent of artifacts requiring more sophisticated indexing, a means to provide more information about the citation relation in order to give more intelligent control to the navigation process is warranted. In order to navigate a citation index in this more-sophisticated manner, the citation index must provide not only the citation-link information, but also must indicate the function of the citation. The design methodology of an indexing tool for scholarly biomedical literature which uses the rhetorical context surrounding the citation to provide the citation function is presented. In particular, we discuss how the scientific method is reflected in scientific writing and how this knowledge can be used to decide the purpose of a citation."
J01-2007,Book Reviews: Natural Language Processing and Knowledge Representation: Language for Knowledge and Knowledge for Language,2001,0,0,1,1,17425,robert mercer,Computational Linguistics,0,None
H94-1028,The {C}andide System for Machine Translation,1994,10,122,7,0,54340,adam berger,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"We present an overview of Candide, a system for automatic translation of French text to English text. Candide uses methods of information theory and statistics to develop a probability model of the translation process. This model, which is made to accord as closely as possible with a large body of French and English sentence pairs, is then used to generate English translations of previously unseen French sentences. This paper provides a tutorial in these methods, discussions of the training and operation of the system, and a summary of test results."
P93-1005,Towards History-based Grammars: Using Richer Models for Probabilistic Parsing,1993,12,145,5,1,49916,ezra black,31st Annual Meeting of the Association for Computational Linguistics,1,"We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
J93-2003,The Mathematics of Statistical Machine Translation: Parameter Estimation,1993,15,3573,4,0.555556,56351,peter brown,Computational Linguistics,0,"We describe a series of five statistical models of the translation process and give algorithms for estimating the parameters of these models given a set of pairs of sentences that are translations of one another. We define a concept of word-by-word alignment between such pairs of sentences. For any given pair of such sentences each of our models assigns a probability to each of the possible word-by-word alignments. We give an algorithm for seeking the most probable of these alignments. Although the algorithm is suboptimal, the alignment thus obtained accounts well for the word-by-word relationships in the pair of sentences. We have a great deal of data in French and English from the proceedings of the Canadian Parliament. Accordingly, we have restricted our work to these two languages; but we feel that because our algorithms have minimal linguistic content they would work well on other pairs of languages. We also feel, again because of the minimal linguistic content of our algorithms, that it is reasonable to argue that word-by-word alignments are inherent in any sufficiently large bilingual corpus."
J93-1001,Introduction to the Special Issue on Computational Linguistics Using Large Corpora,1993,0,0,2,0,3453,kenneth church,Computational Linguistics,0,None
H93-1039,But Dictionaries Are Data Too,1993,3,58,6,0.555556,56351,peter brown,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"Although empiricist approaches to machine translation depend vitally on data in the form of large bilingual corpora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum likelihood estimates of the parameters of a probabilistic model from this combined data and we show how these parameters are affected by inclusion of the dictionary for some sample words."
J92-4003,Class-Based \\textit{n}-gram Models of Natural Language,1992,12,2311,5,0.6,56351,peter brown,Computational Linguistics,0,"We address the problem of predicting a word from previous words in a sample of text. In particular, we discuss n-gram models based on classes of words. We also discuss several statistical algorithms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics."
J92-1002,An Estimate of an Upper Bound for the Entropy of {E}nglish,1992,9,234,5,0.6,56351,peter brown,Computational Linguistics,0,"We present an estimate of an upper bound of 1.75 bits for the entropy of characters in printed English, obtained by constructing a word trigram model and then computing the cross-entropy between this model and a balanced sample of English text. We suggest the well-known and widely available Brown Corpus of printed English as a standard against which to measure progress in language modeling and offer our bound as the first of what we hope will be a series of steadily decreasing bounds."
H92-1023,Decision Tree Models Applied to the Labeling of Text with Parts-of-Speech,1992,6,48,4,1,49916,ezra black,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,We describe work which uses decision trees to estimate marginal probabilities in a maximum entropy model for predicting the part-of-speech of a word given the context in which it appears. Two experiments are presented which exhibit improvements over the usual hidden Markov model approach.
H92-1026,Towards History-based Grammars: Using Richer Models for Probabilistic Parsing,1992,12,91,5,1,49916,ezra black,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"We describe a generative probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Tree-bank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error."
H92-1053,Dividing and Conquering Long Sentences in a Translation System,1992,3,7,4,0.6,56351,peter brown,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"The time required for our translation system to handle a sentence of length l is a rapidly growing function of l. We describe here a method for analyzing a sentence into a series of pieces that can be translated sequentially. We show that for sentences with ten or fewer words, it is possible to decrease the translation time by 40% with almost no effect on translation accuracy. We argue that for longer sentences, the effect should be more dramatic."
1992.tmi-1.8,"Analysis, statistical transfer, and synthesis in machine translation",1992,-1,-1,5,0.6,56351,peter brown,Proceedings of the Fourth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
W91-0220,Presuppositions and Default Reasoning: A Study in Lexical Pragmatics,1991,9,5,1,1,17425,robert mercer,Lexical Semantics and Knowledge Representation,0,"Explaining how the meaning of words relate to the meaning of the utterance in which they are used is of utmost importance. The most common approaches view the meaning of an utterance as a composition of the meanings of it parts, which of course include the words used to construct the utterance. This approach is successful for entailments. However, similar approaches to explain the presuppositional behaviour of utterances have for the most part failed. In this paper we describe the application of Default Logic to the representation and the generation of natural language presuppositions. The view is taken that the presuppositions of an utterance are conjectures made by the hearer based upon the assumption that the speaker is following Grice's maxims of cooperative conversation. These conjectures represent information implicitly contained in the utterance which cannot be generated by classical techniques. The compositional framework is maintained. The difference is that functional units rather than predetermined semantic units are inherited by the meaning structure. The function's meaning changes depending on the contents of the meaning structure. Hence, we view the study of these functional units as lexical pragmatics rather than lexical semantics. Default Logic is one formal method for performing default reasoning in the area of Artificial Intelligence called Knowledge Representation. Default reasoning attempts to fill with conjectures the gaps left by classical forms of reasoning. We suggest that the use of non-classical inferencing techniques such as default reasoning will prove fruitful in the realm of lexical reasoning."
P91-1022,Aligning Sentences in Parallel Corpora,1991,8,423,3,1,56351,peter brown,29th Annual Meeting of the Association for Computational Linguistics,1,"In this paper we describe a statistical technique for aligning sentences with their translations in two parallel corpora. In addition to certain anchor points that are available in our data, the only information about the sentences that we use for calculating alignments is the number of tokens that they contain. Because we make no use of the lexical details of the sentence, the alignment computation is fast and therefore practical for application to very large collections of text. We have used this technique to align several million sentences in the English-French Hansard corpora and have achieved an accuracy in excess of 99% in a random selected set of 1000 sentence pairs that we checked by hand. We show that even without the benefit of anchor points the correlation between the lengths of aligned sentences is strong enough that we should expect to achieve an accuracy of between 96% and 97%. Thus, the technique may be applicable to a wider variety of texts than we have yet tried."
P91-1034,Word-Sense Disambiguation Using Statistical Methods,1991,9,361,4,1,56351,peter brown,29th Annual Meeting of the Association for Computational Linguistics,1,"We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the translation of that instance in another language. When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent."
J91-3005,Erratum to: A Statistical Approach to Machine Translation,1991,0,0,4,1,56351,peter brown,Computational Linguistics,0,"In Section 6 of A statistical approach to machine translation (Computational Linguistics 16(2), 79-85), we reported the results of two experiments in which we estimated parameters of a statistical model of translation from English to French. In the first experiment, the English and French vocabularies each consisted of 9,000 common words, and the model parameters were estimated from 40,000 pairs of sentences 25 words or less in length. Words outside the 9,000-word vocabularies in these sentences were mapped to special unknown words. In the second experiment, the vocabularies were limited to 1,000 common English words and 1,700 common French words, and the model parameters were estimated from 117,000 pairs of sentences 10 words or less in length that were completely covered by the respective vocabularies. In Figures 4, 5, and 6 of the paper, we erroneously presented parameter estimates from the 1,000-word experiment, while claiming in the text that they were from the 9,000-word experiment. The parameter estimates for these two experiments differ considerably because of the restriction of the training corpus in the 1,000-word experiment to short, covered sentences. For example, the probability that hear is translated as bravo"
H91-1025,A Statistical Approach to Sense Disambiguation in Machine Translation,1991,4,77,4,1,56351,peter brown,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the word's translations.
J90-2002,A Statistical Approach to Machine Translation,1990,8,1411,7,1,56351,peter brown,Computational Linguistics,0,"In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results."
C88-2086,Solving Some Persistent Presupposition Problems,1988,8,5,1,1,17425,robert mercer,{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"/Soames 1979/ provides some counterexamples to the theory of natural language presuppositions that is presented in /Gazdar 1979/. /Soames 1982/ provides a theory which explains these counterexamples. /Mercer 1987/ rejects the solution found in /Soames 1982/ leaving these counterexamples unexplained. By reappraising these insightful counterexamples, the inferential theory for natural language presuppositions described in /Mercer 1987, 1988/ gives a simple and straightforward explanation for the presuppositional nature of these sentences."
1988.tmi-1.19,A statistical approach to {F}rench/{E}nglish translation,1988,8,30,6,0,57932,brown,Proceedings of the Second Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"In this paper we will outline an approach to automatic translation that utilizes techniques of statistical information extraction from large data bases. These self-organizing techniques have proven successful in the field of automatic speech recognition [1,2,3]. Statistical approaches have also been used recently in lexicography [4] and natural language processing [3,5,6]. The idea of automatic translation by statistical (information theoretic) methods was proposed many years ago by Warren Weaver [7]."
