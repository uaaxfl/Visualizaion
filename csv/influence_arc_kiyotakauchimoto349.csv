2002.tmi-papers.14,W99-0606,0,0.0689983,"Missing"
2002.tmi-papers.14,A00-2020,0,0.0829102,"achine translation. We describe the modality corpus in Section 2, the method of corpus correction in Section 3, and our experiments on corpus correction in Section 4. 2 Modality Corpus for Machine Translation In this section, we describe the modality corpus. A part of the modality corpus is shown in Figure 1. It is composed of a Japanese-English bilingual corpus; each English sentence can include two types of tags: 1 There is no previous paper on error correction in corpora. In terms of error detection in corpora, there has been research using boosting or anomaly detection (Abney et al. 1999; Eskin 2000). , kono kodomo wa aa ieba kou iu kara koniku-rashii This child always talks back to me, and this &lt;v>is&lt;/v> why I &lt;vj>hate&lt;/vj> him. d kare ga aa okubyou da to wa omowanakatta I &lt;v>did not think&lt;/v> he was so timid. c aa isogashikute wa yasumu hima mo nai hazu da Such a busy man as he &lt;v>cannot have&lt;/v> any spare time. Figure 1: Part of the modality corpus • The English main verb phrase is tagged with &lt;v>. • The English verb phrase corresponding to the Japanese main verb phrase is tagged with &lt;vj>. The symbols at the beginning of each Japanese sentence, such as “c” and “d”, indicate a category"
2002.tmi-papers.14,1999.tmi-1.7,1,0.874862,"Missing"
2002.tmi-papers.14,P94-1013,0,0.00968044,"f each category/feature pair as calculated from p˜(a, b) are the same as those from p(a, b) (this corresponds to Equation (1).) These estimated values are not so sparse. We can thus use the above assumption for calculating p(a, b). Furthermore, we maximize the entropy of the distribution of p˜(a, b) to obtain one solution of p˜(a, b), because using only Equation 1 produces several solutions for p˜(a, b). Maximizing the entropy has the effect of making the distribution more uniform and is considered to be a good solution for data sparseness problems. • Method based on the decision-list method (Yarowsky 1994) In this method, the probability of each category is calculated using one of the features, f j (∈ F,  ≤ j ≤ k). The probability that produces category a in context b is given by the following equation: p(a|b) = p(a|f max ), (3) such that f max is defined by f max = argmaxf j ∈F maxai ∈A p˜(ai |f j ), (4) where p˜(ai |f j ) is the occurrence rate of category a i when the context has feature f j. In this paper, we used the following items as features, which are the context when the probabilities are calculated; 26 (= 5 + 10 + 10 + 1) features appear in each English sentence: • the strings of 1-"
2005.mtsummit-papers.10,J97-2004,0,0.152501,"d (&c&&)). (3) Posstra expresses the possibility estimated by and Aaug as alignment results. using the traditional Chinese characters. 5.1.2 Algorithm for broadening coverage Let WJ ( WC ) denote the list of words Bilingual Dictionary j ∈ WJ (c ∈ WC ) that are still not aligned. In this A translation dictionary can help to identify the translation relations. Let C j denote the Chinese phase, we only consider one to one alignment. ~ ~ For j (∈ WJ ) , we estimate the possibility of j translation set of j . We can estimate the possibility of j being aligned with &c&& using the following formula (Ker and Chang, 1997). Possdic ( j ,&c&&) = max Sim(c' ,&c&&). c '∈C j being aligned with c~ (∈ WC ) as follows. ~ For an alignment candidate ( j , c~ ) , we estimate its likelihood by taking the established alignments into account. Here we consider four established alignments: the two alignments that are the nearest ~ to j on the left and right and the two alignments that are the nearest to c~ on the left and right. Null0 , Null0 ) and First, add ( (4) Possdic expresses the possibility estimated by using a translation dictionary. An automatically built Japanese-Chinese dictionary is used here, which was built fro"
2005.mtsummit-papers.10,maekawa-etal-2000-spontaneous,1,0.906267,"Missing"
2005.mtsummit-papers.10,P01-1067,0,0.0308519,"Missing"
2005.mtsummit-papers.10,C94-2209,0,0.0409686,"Annotation on Chinese Sentences For Chinese morphological analysis, we used the analyser developed by Peking University, where the research on definition of Chinese words and the criteria of word segmentation has been conducted for over ten years. The achievements include a grammatical knowledge base of contemporary Chinese, an automatic morphological analyser, and an annotated People’s Daily Corpus. Since the definition and tagset are widely used in Chinese language processing, we also took the criteria as the basis of our guidelines. A morphological analyzer developed by Peking University (Zhou and Yu, 1994) was applied for automatic annotation of the Chinese sentences and then the automatically tagged sentences were revised by humans. An annotated sentence is illustrated in Figure 3, which is the Chinese sentence in Ex. 1 in Section 2. The interface of the tool is shown in Figure 4 and Figure 5. S-ID: 950104141-008 这些/r 俄军/j 士兵/n 均/d 为/v 十九/m 岁/q 左右/m 的/u 年青人/n ，/w 他们/r 甚至/d 连/p 回答/v 问题/n 的/u 气力/n 也/d 没有/v 。/w Figure 3 An annotated Chinese sentence 4.3 Tool for Manual Revision We developed a tool to assist annotators in revision. The tool has both Japanese and Chinese versions. Here, we introduc"
2005.mtsummit-papers.10,W04-2208,1,\N,Missing
2005.mtsummit-papers.31,P02-1040,0,0.147985,"Missing"
bond-etal-2008-boot,isahara-etal-2008-development,1,\N,Missing
bond-etal-2008-boot,W04-2209,0,\N,Missing
bond-etal-2008-boot,kanzaki-etal-2008-extraction,1,\N,Missing
bond-etal-2008-boot,2001.mtsummit-papers.10,1,\N,Missing
bond-etal-2008-boot,W07-0734,0,\N,Missing
bond-etal-2008-boot,W06-0601,0,\N,Missing
bond-etal-2008-boot,kaji-watanabe-2006-automatic,0,\N,Missing
C00-1074,W96-0102,0,0.0512099,"Missing"
C00-1074,C94-1027,0,0.0855273,"Missing"
C00-1074,J94-2001,0,\N,Missing
C00-1074,J95-4004,0,\N,Missing
C00-1082,J96-1002,0,0.00692912,"of information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree learning method. As shown in Figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes. In Figure 2, for example, the value of the feature `the major POS of the far left morpheme&apos; is `Noun.&apos; 3.2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers (Berger et al., 1996; Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998; Uchimoto et al., 1999). In our maximum-entropy experiment we used Ristad&apos;s system (Ristad, 1998). The analysis is performed by calculating the probability of inserting or not inserting a partition mark, from the output of the system. Whichever probability is higher is selected as the desired answer. In the maximum-entropy method, we use the same four types of morphological information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, as in the decision-tree method. However, it does not consider a combinati"
C00-1082,W98-1118,0,0.0131898,"tic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree learning method. As shown in Figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes. In Figure 2, for example, the value of the feature `the major POS of the far left morpheme&apos; is `Noun.&apos; 3.2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers (Berger et al., 1996; Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998; Uchimoto et al., 1999). In our maximum-entropy experiment we used Ristad&apos;s system (Ristad, 1998). The analysis is performed by calculating the probability of inserting or not inserting a partition mark, from the output of the system. Whichever probability is higher is selected as the desired answer. In the maximum-entropy method, we use the same four types of morphological information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, as in the decision-tree method. However, it does not consider a combination of features. Unlike the decision-tree method, as a result w"
C00-1082,W95-0107,0,0.0509382,"hashi, for example, made 146 rules for bunsetsu identi cation (Kurohashi, 1998). In an attempt to reduce the number of manhours, we used machine-learning methods for bunsetsu identi cation. Because it was not clear which machine-learning method would be the one most appropriate for bunsetsu identi cation, so we tried a variety of them. In this paper we report experiments comparing four machine-learning methods (decision tree, maximum entropy, example-based, and decision list methods) and our new methods using category-exclusive rules. 1 Bunsetsu identi cation is a problem similar to chunking (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999) in other languages. 2 Bunsetsu identi cation problem We conducted experiments on the following supervised learning methods for identifying bunsetsu:  Decision tree method  Maximum entropy method  Example-based method (use of similarity)  Decision list (use of probability and frequency)  Method 1 (use of exclusive rules)  Method 2 (use of exclusive rules with the highest similarity). In general, bunsetsu identi cation is done after morphological and before syntactic analysis. Morphological analysis corresponds to part-of-speech tagging in English. Japanese synta"
C00-1082,W96-0213,0,0.0136307,"major POS, (ii) minor POS, (iii) semantic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree learning method. As shown in Figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes. In Figure 2, for example, the value of the feature `the major POS of the far left morpheme&apos; is `Noun.&apos; 3.2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers (Berger et al., 1996; Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998; Uchimoto et al., 1999). In our maximum-entropy experiment we used Ristad&apos;s system (Ristad, 1998). The analysis is performed by calculating the probability of inserting or not inserting a partition mark, from the output of the system. Whichever probability is higher is selected as the desired answer. In the maximum-entropy method, we use the same four types of morphological information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, as in the decision-tree method. However, it does not consider a combination of features. Unl"
C00-1082,W97-0301,0,0.0125915,"or POS, (iii) semantic information, and (iv) word, mentioned in the previous section were also used as features with the decision-tree learning method. As shown in Figure 3, the number of features is 12 (2 + 4 + 4 + 2) because we do not use (iii) semantic information and (iv) word information from the two outside morphemes. In Figure 2, for example, the value of the feature `the major POS of the far left morpheme&apos; is `Noun.&apos; 3.2 Maximum-entropy method The maximum-entropy method is useful with sparse data conditions and has been used by many researchers (Berger et al., 1996; Ratnaparkhi, 1996; Ratnaparkhi, 1997; Borthwick et al., 1998; Uchimoto et al., 1999). In our maximum-entropy experiment we used Ristad&apos;s system (Ristad, 1998). The analysis is performed by calculating the probability of inserting or not inserting a partition mark, from the output of the system. Whichever probability is higher is selected as the desired answer. In the maximum-entropy method, we use the same four types of morphological information, (i) major POS, (ii) minor POS, (iii) semantic information, and (iv) word, as in the decision-tree method. However, it does not consider a combination of features. Unlike the decision-tr"
C00-1082,E99-1023,0,0.0134246,"46 rules for bunsetsu identi cation (Kurohashi, 1998). In an attempt to reduce the number of manhours, we used machine-learning methods for bunsetsu identi cation. Because it was not clear which machine-learning method would be the one most appropriate for bunsetsu identi cation, so we tried a variety of them. In this paper we report experiments comparing four machine-learning methods (decision tree, maximum entropy, example-based, and decision list methods) and our new methods using category-exclusive rules. 1 Bunsetsu identi cation is a problem similar to chunking (Ramshaw and Marcus, 1995; Sang and Veenstra, 1999) in other languages. 2 Bunsetsu identi cation problem We conducted experiments on the following supervised learning methods for identifying bunsetsu:  Decision tree method  Maximum entropy method  Example-based method (use of similarity)  Decision list (use of probability and frequency)  Method 1 (use of exclusive rules)  Method 2 (use of exclusive rules with the highest similarity). In general, bunsetsu identi cation is done after morphological and before syntactic analysis. Morphological analysis corresponds to part-of-speech tagging in English. Japanese syntactic structures are usuall"
C00-1082,E99-1026,1,0.925638,"t for analyzing Japanese sentences. In experiments comparing the four previously available machinelearning methods (decision tree, maximum-entropy method, example-based approach and decision list) and two new methods using category-exclusive rules, the new method using the category-exclusive rules with the highest similarity performed best. 1 Introduction This paper is about machine learning methods for identifying bunsetsus, which correspond to English phrasal units such as noun phrases and prepositional phrases. Since Japanese syntactic analysis is usually done after bunsetsu identi cation (Uchimoto et al., 1999), identifying bunsetsu is important for analyzing Japanese sentences. The conventional studies on bunsetsu identi cation1 have used hand-made rules (Kameda, 1995; Kurohashi, 1998), but bunsetsu identi cation is not an easy task. Conventional studies used many hand-made rules developed at the cost of many man-hours. Kurohashi, for example, made 146 rules for bunsetsu identi cation (Kurohashi, 1998). In an attempt to reduce the number of manhours, we used machine-learning methods for bunsetsu identi cation. Because it was not clear which machine-learning method would be the one most appropriate"
C00-1082,P94-1013,0,0.0437241,"| Symbol Punctuation 2 2 Figure 5: Example of levels of similarity but are expanded by combining all the features, and are stored in a one-dimensional list. A priority order is de ned in a certain way and all of the rules are arranged in this order. The decision-list method searches for rules from the top of the list and analyzes a particular problem by using only the rst applicable rule. In this study we used in the decision-list method the same 152 types of patterns that were used in the maximum-entropy method. To determine the priority order of the rules, we referred to Yarowsky&apos;s method (Yarowsky, 1994) and Nishiokayama&apos;s method (Nishiokayama et al., 1998) and used the probability and frequency of each rule as measures of this priority order. When multiple rules had the same probability, the rules were arranged in order of their frequency. Suppose, for example, that Pattern A Noun: Normal Noun; Particle: Case-Particle: none: wo; Verb: Normal Form: 217; Symbol: Punctuation&quot; occurs 13 times in a learning set and that ten of the occurrences include the inserted partition mark. Suppose also that Pattern B Noun; Particle; Verb; Symbol&quot; occurs 123 times in a learning set and that 90 of the occur"
C00-2109,W98-1512,0,0.0643188,"Missing"
C00-2109,W98-1510,0,\N,Missing
C00-2109,E99-1026,1,\N,Missing
C00-2109,P97-1003,0,\N,Missing
C00-2109,P98-1083,0,\N,Missing
C00-2109,C98-1080,0,\N,Missing
C00-2109,P95-1037,0,\N,Missing
C00-2109,W98-1511,0,\N,Missing
C00-2126,J96-1002,0,\N,Missing
C00-2126,P99-1018,0,\N,Missing
C02-1064,C00-1007,0,0.0708941,"ndidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkilde, 2000) to express the candidate-text sentences. Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000). They consider dependency information when deriving trees by using XTAG grammar, but they assume that the input contains dependency information. Our system generates candidate-text sentences without relying on dependency information in the input, and our model estimates the dependencies between keywords. Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). The input of these models is semantic attributes. His models are similar to ours if the semantic attributes are replaced with keywords. However, his models need a training corpus in which certain words"
C02-1064,J96-1002,0,0.00733679,"n bunsetsus. 4. posterior dependency bigram model We assume that ki depends only on the headword, ws , and the word on its right, ws+1 , in the bunsetsu that is modiﬁed by the bunsetsu including ki (see Fig. 3). Text-Generation Model We next describe the model represented by Eq. (4); that is, a keyword-production model, a morpheme model that estimates how likely a string is to be a morpheme, and a dependency model. The goal of this model is to select optimal sets of morphemes and dependencies that can generate natural sentences. We implemented these models within an maximum entropy framework (Berger et al., 1996; Ristad, 1997; Ristad, 1998). 4.1 Keyword-Production Models This section describes ﬁve keyword-production models which are represented by P (K|M, D, T ) in Eq. (4). In these models, we deﬁne the set of headwords whose frequency in the corpus is over a certain threshold as a set of keywords, KS, and we restrict the bunsetsus to those generated by the generation rules represented in form (5). We assume that all keywords are independent and that ki corresponds to word wj (1 ≤ j ≤ m) when text is given as a series of words w1 . . . wm . 1. trigram model We assume that ki depends only on the two a"
C02-1064,J90-2002,0,0.0214748,"uction Text generation is an important technique used for applications like machine translation, summarization, and human/computer dialogue. In recent years, many corpora have become available, and have been used to generate natural surface sentences. For example, corpora have been used to generate sentences for language model estimation in statistical machine translation. In such translation, given a source language text, S, the translated text, T , in the target language that maximizes the probability P (T |S) is selected as the most appropriate translation, Tbest , which is represented as (Brown et al., 1990) Tbest = argmaxT P (T |S) = argmaxT (P (S|T ) × P (T )) . (1) In this equation, P (S|T ) represents the model used to replace words or phrases in a source language with those in the target language. It is called a translation model. P (T ) represents a language model that is used to reorder translated words or phrases into a natural order in Hitoshi Isahara† ‡ New York University 715 Broadway, 7th floor New York, NY 10003, USA sekine@cs.nyu.edu the target language. The input of the language model is a “bag of words,” and the goal of the model is basically to reorder the words. At this point, t"
C02-1064,W01-0812,0,0.0111773,"erate text from semantic attributes (Ratnaparkhi, 2000). The input of these models is semantic attributes. His models are similar to ours if the semantic attributes are replaced with keywords. However, his models need a training corpus in which certain words are replaced with semantic attributes. Although our model also needs a training corpus, the corpus can be automatically created by using a morphological analyzer and a dependency analyzer, both of which are readily available. Humphreys et al. proposed using models developed for sentence-structure analysis to rank candidate-text sentences (Humphreys et al., 2001). As well as models developed for sentence-structure analysis, we also use those developed for morphological analysis and found that these models contribute to the generation of appropriate text. Berger and Laﬀerty proposed a language model for information retrieval (Berger and Lafferty, 1999). Their concept is similar to that of our model, which can be regarded as a model that translates keywords into text, while their model can be regarded as one that translates query words into documents. However, the purpose of their model is diﬀerent: their goal is to retrieve text that already exists whi"
C02-1064,P95-1034,0,0.051106,"ed. In this section, we describe the diﬀerences between our method and several previous methods. Japanese words are often followed by postpositional particles, such as “ga” and “wo”, to indicate the subject and object of a sentence. There are no corresponding words in English. Instead, English words are preceded by articles, “the” and “a,” to distinguish definite and indeﬁnite nouns, and so on, and in this case there are no corresponding words in Japanese. Knight et al. proposed a way to compensate for missing information caused by a lack of language-dependent knowledge, or a “knowledge gap” (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998a; Langkilde and Knight, 1998b). They use semantic expressions as input, whereas we use keywords. Also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkil"
C02-1064,P98-1116,0,0.053813,"e diﬀerences between our method and several previous methods. Japanese words are often followed by postpositional particles, such as “ga” and “wo”, to indicate the subject and object of a sentence. There are no corresponding words in English. Instead, English words are preceded by articles, “the” and “a,” to distinguish definite and indeﬁnite nouns, and so on, and in this case there are no corresponding words in Japanese. Knight et al. proposed a way to compensate for missing information caused by a lack of language-dependent knowledge, or a “knowledge gap” (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998a; Langkilde and Knight, 1998b). They use semantic expressions as input, whereas we use keywords. Also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkilde, 2000) to express the can"
C02-1064,W98-1426,0,0.0486915,"e diﬀerences between our method and several previous methods. Japanese words are often followed by postpositional particles, such as “ga” and “wo”, to indicate the subject and object of a sentence. There are no corresponding words in English. Instead, English words are preceded by articles, “the” and “a,” to distinguish definite and indeﬁnite nouns, and so on, and in this case there are no corresponding words in Japanese. Knight et al. proposed a way to compensate for missing information caused by a lack of language-dependent knowledge, or a “knowledge gap” (Knight and Hatzivassiloglou, 1995; Langkilde and Knight, 1998a; Langkilde and Knight, 1998b). They use semantic expressions as input, whereas we use keywords. Also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkilde, 2000) to express the can"
C02-1064,A00-2023,0,0.031131,"u, 1995; Langkilde and Knight, 1998a; Langkilde and Knight, 1998b). They use semantic expressions as input, whereas we use keywords. Also, they construct candidate-text sentences or word lattices by applying rules, and apply their language model, an n-gram model, to select the most appropriate surface text. While we cannot use their rules to generate candidate-text sentences when given keywords, we can apply their language model to our system to generate surface-text sentences from candidate-text sentences in the form of dependency trees. We can also apply the formalism proposed by Langkilde (Langkilde, 2000) to express the candidate-text sentences. Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000). They consider dependency information when deriving trees by using XTAG grammar, but they assume that the input contains dependency information. Our system generates candidate-text sentences without relying on dependency information in the input, and our model estimates the dependencies between keywords. Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). The input of these models is semant"
C02-1064,A00-2026,0,0.0407408,"y the formalism proposed by Langkilde (Langkilde, 2000) to express the candidate-text sentences. Bangalore and Rambow proposed a method to generate candidate-text sentences in the form of trees (Bangalore and Rambow, 2000). They consider dependency information when deriving trees by using XTAG grammar, but they assume that the input contains dependency information. Our system generates candidate-text sentences without relying on dependency information in the input, and our model estimates the dependencies between keywords. Ratnaparkhi proposed models to generate text from semantic attributes (Ratnaparkhi, 2000). The input of these models is semantic attributes. His models are similar to ours if the semantic attributes are replaced with keywords. However, his models need a training corpus in which certain words are replaced with semantic attributes. Although our model also needs a training corpus, the corpus can be automatically created by using a morphological analyzer and a dependency analyzer, both of which are readily available. Humphreys et al. proposed using models developed for sentence-structure analysis to rank candidate-text sentences (Humphreys et al., 2001). As well as models developed fo"
C02-1064,E99-1026,1,0.849331,"appropriate. We used headwords that were found ﬁve times or more in the newspaper articles appearing from January 1st to 16th in the Kyoto University text corpus and also found in those appearing on January 1st as the set of headwords, KS. For headwords that were not in KS, we added their major part-of-speech categories to the set. We trained our keyword-production models by using 1,129 sentences (containing 10,201 headwords) from newspaper articles appearing on January 1st. We used a morpheme model and a dependency model identical to those proposed by Uchimoto et al. (Uchimoto et al., 2001; Uchimoto et al., 1999; Uchimoto et al., 2000b). To train the models, we used 8,835 sentences from newspaper articles appearing from January 1st to 9th in 1995. Generation rules were acquired from newspaper articles appearing from January 1st to 16th. The total number of sentences was 18,435. First, we evaluated the outputs generated when the rightmost two keywords, such as “連 覇 and 達成,” on each line of Table 1 were input. Table 2 shows the results. KM1 through KM5 stand for the ﬁve keyword-production models described in Section 4.1, and MM and DM stand for the morpheme and the dependency models, respectively. The"
C02-1064,C00-2126,1,0.918068,"eadwords that were found ﬁve times or more in the newspaper articles appearing from January 1st to 16th in the Kyoto University text corpus and also found in those appearing on January 1st as the set of headwords, KS. For headwords that were not in KS, we added their major part-of-speech categories to the set. We trained our keyword-production models by using 1,129 sentences (containing 10,201 headwords) from newspaper articles appearing on January 1st. We used a morpheme model and a dependency model identical to those proposed by Uchimoto et al. (Uchimoto et al., 2001; Uchimoto et al., 1999; Uchimoto et al., 2000b). To train the models, we used 8,835 sentences from newspaper articles appearing from January 1st to 9th in 1995. Generation rules were acquired from newspaper articles appearing from January 1st to 16th. The total number of sentences was 18,435. First, we evaluated the outputs generated when the rightmost two keywords, such as “連 覇 and 達成,” on each line of Table 1 were input. Table 2 shows the results. KM1 through KM5 stand for the ﬁve keyword-production models described in Section 4.1, and MM and DM stand for the morpheme and the dependency models, respectively. The symbol + indicates a co"
C02-1064,2000.iwpt-1.43,1,0.881498,"eadwords that were found ﬁve times or more in the newspaper articles appearing from January 1st to 16th in the Kyoto University text corpus and also found in those appearing on January 1st as the set of headwords, KS. For headwords that were not in KS, we added their major part-of-speech categories to the set. We trained our keyword-production models by using 1,129 sentences (containing 10,201 headwords) from newspaper articles appearing on January 1st. We used a morpheme model and a dependency model identical to those proposed by Uchimoto et al. (Uchimoto et al., 2001; Uchimoto et al., 1999; Uchimoto et al., 2000b). To train the models, we used 8,835 sentences from newspaper articles appearing from January 1st to 9th in 1995. Generation rules were acquired from newspaper articles appearing from January 1st to 16th. The total number of sentences was 18,435. First, we evaluated the outputs generated when the rightmost two keywords, such as “連 覇 and 達成,” on each line of Table 1 were input. Table 2 shows the results. KM1 through KM5 stand for the ﬁve keyword-production models described in Section 4.1, and MM and DM stand for the morpheme and the dependency models, respectively. The symbol + indicates a co"
C02-1064,W01-0512,1,0.834673,"priate, it is judged as appropriate. We used headwords that were found ﬁve times or more in the newspaper articles appearing from January 1st to 16th in the Kyoto University text corpus and also found in those appearing on January 1st as the set of headwords, KS. For headwords that were not in KS, we added their major part-of-speech categories to the set. We trained our keyword-production models by using 1,129 sentences (containing 10,201 headwords) from newspaper articles appearing on January 1st. We used a morpheme model and a dependency model identical to those proposed by Uchimoto et al. (Uchimoto et al., 2001; Uchimoto et al., 1999; Uchimoto et al., 2000b). To train the models, we used 8,835 sentences from newspaper articles appearing from January 1st to 9th in 1995. Generation rules were acquired from newspaper articles appearing from January 1st to 16th. The total number of sentences was 18,435. First, we evaluated the outputs generated when the rightmost two keywords, such as “連 覇 and 達成,” on each line of Table 1 were input. Table 2 shows the results. KM1 through KM5 stand for the ﬁve keyword-production models described in Section 4.1, and MM and DM stand for the morpheme and the dependency mod"
C02-1064,C98-1112,0,\N,Missing
C02-2019,maekawa-etal-2000-spontaneous,1,0.820306,"ethod to tag a spontaneous speech corpus. Their method uses a model that can not only consult a dictionary but can also identify unknown words by learning certain characteristics. To learn these characteristics, we focused on such information as whether or not a string is found in a dictionary and what types of characters are used in a string. The model estimates how likely a string is to be a morpheme. This model is independent of the domain of corpora; in this paper we demonstrate that this is true by applying our model to the spontaneous speech corpus, Corpus of Spontaneous Japanese (CSJ) (Maekawa et al., 2000). We also show that a dictionary developed for a corpus on a certain domain is helpful for improving accuracy in analyzing a corpus on another domain. 2 A Morpheme Model This section describes a model which estimates how likely a string is to be a morpheme. We implemented this model within an M.E. framework. Given a tokenized test corpus, the problem of Japanese morphological analysis can be reduced to the problem of assigning one of two tags to each string in a sentence. A string is tagged with a 1 or a 0 to indicate whether or not it is a morpheme. When a string is a morpheme, a grammatical"
C02-2019,C96-2202,0,0.317545,"Japanese sentence analysis. A morpheme is a minimal grammatical unit, such as a word or a suﬃx, and morphological analysis is the process of segmenting a given sentence into a row of morphemes and assigning to each morpheme grammatical attributes such as part-of-speech (POS) and inﬂection type. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus. Two statistical approaches have been applied to this problem. One is to ﬁnd unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (M.E.) model (Uchimoto et al., 2001). We used their method to tag a spontaneous speech corpus. Their method uses a model that can not only consult a dictionary but can also identify unknown words by learning certain characteristics. To learn these characteristics, we focused on such information as whether or not a string is found in a dictionary and what typ"
C02-2019,P99-1036,0,0.261165,"ess of segmenting a given sentence into a row of morphemes and assigning to each morpheme grammatical attributes such as part-of-speech (POS) and inﬂection type. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus. Two statistical approaches have been applied to this problem. One is to ﬁnd unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (M.E.) model (Uchimoto et al., 2001). We used their method to tag a spontaneous speech corpus. Their method uses a model that can not only consult a dictionary but can also identify unknown words by learning certain characteristics. To learn these characteristics, we focused on such information as whether or not a string is found in a dictionary and what types of characters are used in a string. The model estimates how likely a string is to be a morpheme. This model is independent"
C02-2019,W01-0512,1,0.81887,"and inﬂection type. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus. Two statistical approaches have been applied to this problem. One is to ﬁnd unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (M.E.) model (Uchimoto et al., 2001). We used their method to tag a spontaneous speech corpus. Their method uses a model that can not only consult a dictionary but can also identify unknown words by learning certain characteristics. To learn these characteristics, we focused on such information as whether or not a string is found in a dictionary and what types of characters are used in a string. The model estimates how likely a string is to be a morpheme. This model is independent of the domain of corpora; in this paper we demonstrate that this is true by applying our model to the spontaneous speech corpus, Corpus of Spontaneous"
C02-2019,J96-1002,0,\N,Missing
C04-1159,P96-1025,0,0.0898547,"ependency is represented by a probability estimated by a dependency probability model. Given sentence S, let us assume that it is uniquely divided into n bunsetsus, b1 , . . . , bn , and that it is represented as an ordered set of bunsetsus, B = {b1 , . . . , bn }. Let D be an ordered set of dependencies in the sentence and let Di be a dependency whose modiﬁer is bunsetsu bi (i = 1, . . . , n − 1). Let us also assume that D = {D1 , . . . , Dn−1 }. Statistical dependency structure analysis ﬁnds dependencies that maximize probability P (D|S) given sentence S. The conventional statistical model (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et al., 1998; Uchimoto et al., 1999) uses only the relationship between two bunsetsus to estimate the probability of dependency, whereas the model in this study (Uchimoto et al., 2000) takes into account not only the relationship between two bunsetsus but also the relationship between the left bunsetsu and all the bunsetsus to its right. This model uses more information than the conventional model. We implemented this model within a maximum entropy modeling framework. The features used in the model were basically attributes of bunsetsus, such as character st"
C04-1159,W98-1511,0,0.65108,"is a big diﬀerence between a written text corpus and a spontaneous speech corpus: In spontaneous speech, especially when it is long, sentence boundaries are often ambiguous. In the CSJ, therefore, sentence boundaries were deﬁned based on clauses whose boundaries were automatically detected by using surface information (Maruyama et al., 2003), and they were detected manually (Takanashi et al., 2003). Our deﬁnition of sentence boundaries follows the deﬁnition used in the CSJ. Almost all previous research on Japanese dependency structure analysis dealt with dependency structures in written text (Fujio and Matsumoto, 1998; Haruno et al., 1998; Uchimoto et al., 1999; Uchimoto et al., 2000; Kudo and Matsumoto, 2000). Although Matsubara and colleagues did investigate dependency structures in spontaneous speech (Matsubara et al., 2002), the target speech was dialogues where the utterances were short and sentence boundaries could be easily deﬁned based on turn-taking data. In contrast, we investigated dependency structures in spontaneous and long speeches in the CSJ. The biggest problem in dependency structure analysis with spontaneous and long speeches is that sentence boundaries are ambiguous. Therefore, sentence"
C04-1159,P98-1083,0,0.545732,"a written text corpus and a spontaneous speech corpus: In spontaneous speech, especially when it is long, sentence boundaries are often ambiguous. In the CSJ, therefore, sentence boundaries were deﬁned based on clauses whose boundaries were automatically detected by using surface information (Maruyama et al., 2003), and they were detected manually (Takanashi et al., 2003). Our deﬁnition of sentence boundaries follows the deﬁnition used in the CSJ. Almost all previous research on Japanese dependency structure analysis dealt with dependency structures in written text (Fujio and Matsumoto, 1998; Haruno et al., 1998; Uchimoto et al., 1999; Uchimoto et al., 2000; Kudo and Matsumoto, 2000). Although Matsubara and colleagues did investigate dependency structures in spontaneous speech (Matsubara et al., 2002), the target speech was dialogues where the utterances were short and sentence boundaries could be easily deﬁned based on turn-taking data. In contrast, we investigated dependency structures in spontaneous and long speeches in the CSJ. The biggest problem in dependency structure analysis with spontaneous and long speeches is that sentence boundaries are ambiguous. Therefore, sentence boundaries should be"
C04-1159,W00-1303,0,0.079418,"us speech, especially when it is long, sentence boundaries are often ambiguous. In the CSJ, therefore, sentence boundaries were deﬁned based on clauses whose boundaries were automatically detected by using surface information (Maruyama et al., 2003), and they were detected manually (Takanashi et al., 2003). Our deﬁnition of sentence boundaries follows the deﬁnition used in the CSJ. Almost all previous research on Japanese dependency structure analysis dealt with dependency structures in written text (Fujio and Matsumoto, 1998; Haruno et al., 1998; Uchimoto et al., 1999; Uchimoto et al., 2000; Kudo and Matsumoto, 2000). Although Matsubara and colleagues did investigate dependency structures in spontaneous speech (Matsubara et al., 2002), the target speech was dialogues where the utterances were short and sentence boundaries could be easily deﬁned based on turn-taking data. In contrast, we investigated dependency structures in spontaneous and long speeches in the CSJ. The biggest problem in dependency structure analysis with spontaneous and long speeches is that sentence boundaries are ambiguous. Therefore, sentence boundaries should be detected before or during dependency structure analysis in order to obta"
C04-1159,N01-1025,0,0.0382378,"e extracted as sentence boundary candidates. So, an output sequence is selected from all possible conversion patterns generated using two words to the left and two words to the right of each sentence boundary candidate. To perform this operation, we used a beam search with a width of 10 because a number of conversion patterns can be generated with such a search. 3.4 Sentence Boundary Detection Based on Machine Learning (Method 2) We use Support Vector Machine (SVM) as a machine learning model and we approached the problem of sentence boundary detection as a text chunking task. We used YamCha (Kudo and Matsumoto, 2001) as a text chunker, which is based on SVM and uses polynomial kernel functions. To determine the appropriate chunk label for a target word, YamCha uses two words to the right and two words to the left of the We used the IOE labeling scheme for proper chunking, and the following parameters for YamCha. • Degree of polynomial kernel: 3rd • Analysis direction: Left to right • Multi-class method: Pairwise 4 Experiments and Discussion In our experiments, we used the transcriptions of 188 talks in the CSJ. We used 10 talks for testing. Dependency structure analysis results were evaluated for closed-"
C04-1159,maekawa-etal-2000-spontaneous,1,0.878846,"Missing"
C04-1159,C02-1136,0,0.184882,"were deﬁned based on clauses whose boundaries were automatically detected by using surface information (Maruyama et al., 2003), and they were detected manually (Takanashi et al., 2003). Our deﬁnition of sentence boundaries follows the deﬁnition used in the CSJ. Almost all previous research on Japanese dependency structure analysis dealt with dependency structures in written text (Fujio and Matsumoto, 1998; Haruno et al., 1998; Uchimoto et al., 1999; Uchimoto et al., 2000; Kudo and Matsumoto, 2000). Although Matsubara and colleagues did investigate dependency structures in spontaneous speech (Matsubara et al., 2002), the target speech was dialogues where the utterances were short and sentence boundaries could be easily deﬁned based on turn-taking data. In contrast, we investigated dependency structures in spontaneous and long speeches in the CSJ. The biggest problem in dependency structure analysis with spontaneous and long speeches is that sentence boundaries are ambiguous. Therefore, sentence boundaries should be detected before or during dependency structure analysis in order to obtain the dependency structure of each sentence. In this paper, we ﬁrst describe the problems with dependency structure ana"
C04-1159,E99-1026,1,0.947104,"and a spontaneous speech corpus: In spontaneous speech, especially when it is long, sentence boundaries are often ambiguous. In the CSJ, therefore, sentence boundaries were deﬁned based on clauses whose boundaries were automatically detected by using surface information (Maruyama et al., 2003), and they were detected manually (Takanashi et al., 2003). Our deﬁnition of sentence boundaries follows the deﬁnition used in the CSJ. Almost all previous research on Japanese dependency structure analysis dealt with dependency structures in written text (Fujio and Matsumoto, 1998; Haruno et al., 1998; Uchimoto et al., 1999; Uchimoto et al., 2000; Kudo and Matsumoto, 2000). Although Matsubara and colleagues did investigate dependency structures in spontaneous speech (Matsubara et al., 2002), the target speech was dialogues where the utterances were short and sentence boundaries could be easily deﬁned based on turn-taking data. In contrast, we investigated dependency structures in spontaneous and long speeches in the CSJ. The biggest problem in dependency structure analysis with spontaneous and long speeches is that sentence boundaries are ambiguous. Therefore, sentence boundaries should be detected before or dur"
C04-1159,2000.iwpt-1.43,1,0.944091,"ch corpus: In spontaneous speech, especially when it is long, sentence boundaries are often ambiguous. In the CSJ, therefore, sentence boundaries were deﬁned based on clauses whose boundaries were automatically detected by using surface information (Maruyama et al., 2003), and they were detected manually (Takanashi et al., 2003). Our deﬁnition of sentence boundaries follows the deﬁnition used in the CSJ. Almost all previous research on Japanese dependency structure analysis dealt with dependency structures in written text (Fujio and Matsumoto, 1998; Haruno et al., 1998; Uchimoto et al., 1999; Uchimoto et al., 2000; Kudo and Matsumoto, 2000). Although Matsubara and colleagues did investigate dependency structures in spontaneous speech (Matsubara et al., 2002), the target speech was dialogues where the utterances were short and sentence boundaries could be easily deﬁned based on turn-taking data. In contrast, we investigated dependency structures in spontaneous and long speeches in the CSJ. The biggest problem in dependency structure analysis with spontaneous and long speeches is that sentence boundaries are ambiguous. Therefore, sentence boundaries should be detected before or during dependency structur"
C04-1159,A97-1004,0,\N,Missing
C04-1159,C98-1080,0,\N,Missing
C08-2030,kozawa-etal-2008-automatic,1,0.880303,"Missing"
C94-2169,1993.iwpt-1.11,1,0.734317,"roposes a novel example retrieval method for avoiding ftfll retrieval of examples. The proposed method has the following three features, 1) it generates retrieval queries from similarities, 2) efficient example retrieval through the tree structure of a thesaurus, 3) binary search along subsumption ordering of retrieval queries. Example retrieval time drastically decreases with the method. 1 2 Introduction Since a nmdel of machine translation (MT) called Translation by Analogy was first proposed in Nagao (1984), nmch work has been undertaken in exampleba~sed NLP (e.g. Sato and Nagao (1990) and Kurohashi and Nagao (1993)). The basic idea of examplebased approach to NLP is to accomplish some task in NLP by imitating a similar previous example, instead of using rules written by h u m a n writers. Major processing steps of example-based approach are: 1) collect examples and the results of performing the task in a database, 2) given an input, retrieve similar examples from the database, 3) adapt the results of tile similar examples to the current input and obtain the output. Compared with the traditional rule-based approach, example-based approach has advantages like: 1) it is easier to m a i n t a i n the implem"
C94-2169,C90-3044,1,0.740067,"the database. This paper proposes a novel example retrieval method for avoiding ftfll retrieval of examples. The proposed method has the following three features, 1) it generates retrieval queries from similarities, 2) efficient example retrieval through the tree structure of a thesaurus, 3) binary search along subsumption ordering of retrieval queries. Example retrieval time drastically decreases with the method. 1 2 Introduction Since a nmdel of machine translation (MT) called Translation by Analogy was first proposed in Nagao (1984), nmch work has been undertaken in exampleba~sed NLP (e.g. Sato and Nagao (1990) and Kurohashi and Nagao (1993)). The basic idea of examplebased approach to NLP is to accomplish some task in NLP by imitating a similar previous example, instead of using rules written by h u m a n writers. Major processing steps of example-based approach are: 1) collect examples and the results of performing the task in a database, 2) given an input, retrieve similar examples from the database, 3) adapt the results of tile similar examples to the current input and obtain the output. Compared with the traditional rule-based approach, example-based approach has advantages like: 1) it is easie"
C94-2169,1988.tmi-1.13,0,0.0817155,"Missing"
D09-1060,W08-2102,0,0.197319,"orating the subtree-based features. Table 2 shows the performance of the systems that were compared, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003), CO2006 refers to the parser of Corston-Oliver et al. (2006), Hall2006 refers to the parser of Hall et al. (2006), Wang2007 refers to the parser of Wang et al. (2007), Z&C 2008 refers to the combination graph-based and transition-based system of Zhang and Clark (2008), KOO08-dep1c/KOO08dep2c refers to a graph-based system with first/second-order cluster-based features by Koo et al. (2008), and Carreras2008 refers to the paper of Carreras et al. (2008). The results showed that Ord2s performed better than the first five systems. The second-order system of Koo et al. (2008) performed better than our systems. The reason may be that the MSTParser only uses sibling interactions for second-order, while Koo et al. (2008) uses both sibling and grandparent interactions, and uses cluster-based features. Carreras et al. (2008) reported a very high accuracy using information of constituent structure of the TAG grammar formalism. In our systems, we did not use such knowledge. Our subtree-based features could be combined 4.1.1 Main results of English dat"
D09-1060,N06-1021,0,0.0749069,"Missing"
D09-1060,A00-1031,0,0.0251006,"or testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008). For the unannotated data, we used the PFR corpus10 , which has approximately 15 million words whose segmentation and POS tags are given. We used its original segmentation though there are differences in segmentation policy between CTB and this corpus. As for POS tags, we discarded the original POS tags and assigned CTB style POS tags using a TNT-based tagger (Brants, 2000) trained on the training data. We used the Basic Parser to process all the sentences of the PFR corpus. We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all punctuation tokens) with the correct HEAD. And we also evaluated on complete dependency analysis. The results are shown in Table 1, where Ord1/Ord2 refers to a first-/second-order MSTParser with basic features, Ord1s/Ord2s refers to a first-/second-order MSTParser with basic+subtree-based features, and the improvements by the subtree-based features over the basic features are"
D09-1060,W06-2920,0,0.0596271,"features for the parsing models. 3.1 Subtrees extraction To ease explanation, we transform the dependency structure into a more tree-like structure as shown in Figure 2, the sentence is the same as the one in Figure 1. ROOT ROOT I ate the fish with a fork ate . Figure 1: Example for dependency structure I fish with . the fork 2.1 Parsing approach For dependency parsing, there are two main types of parsing models (Nivre and McDonald, 2008): graph-based model and transition-based model, which achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007). Our subtree-based features can be applied in both of the two parsing models. In this paper, as the base parsing system, we employ the graph-based MST parsing model proposed by McDonald et al. (2005) and McDonald and Pereira (2006), which uses the idea of Maximum Spanning Trees of a graph and large margin structured learning algorithms. The details a I ate the fish with a fork . Figure 2: Example for dependency structure in tree-format Our task is to extract subtrees from dependency trees. If a subtree contains two nodes, we call it a bigram-subtree. If a subtree contains"
D09-1060,D07-1101,0,0.315431,"forms of heads. Specifically, for any feature related to word form, we remove this feature if the word is not one of the Top-N most frequent words in the training data. We used N=1000 for the experiments in this paper. This method can reduce the size of the feature sets. In this paper, we only used bigram-subtrees and the limited form of trigram-subtrees, though in theory we can use k-gram-subtrees, which are limited in the same way as our trigram subtrees, in (k-1)th-order MST parsing models mentioned in McDonald and Pereira (2006) or use grandparenttype trigram-subtrees in parsing models of Carreras (2007). Although the higher-order MST parsing models will be slow with exact inference, requiring O(nk ) time (McDonald and Pereira, 2006), it might be possible to use higher-order kgram subtrees with approximated parsing model in the future. Of course, our method can also be easily extended to the labeled dependency case. d+1 … (a) …h … d1 … d2 … (b) Figure 4: Word pairs and triple for feature representation in Figure 5, where h is “ate” and d is “with”. We can generate the features for the pairs linked by dashed-lines, such as h − d, h − d+1 and so on. Then we have the temporary bigram-subtrees “a"
D09-1060,I08-1012,1,0.908673,"he BLLIP corpus. For Chinese, we used the Chinese Treebank 7 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 8 (CTB) version 4.09 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008). For the unannotated data, we used the PFR corpus10 , which has approximately 15 million words whose segmentation and POS tags are given. We used its original segmentation though there are differences in segmentation policy between CTB and this corpus. As for POS tags, we discarded the original POS tags and assigned CTB style POS tags using a TNT-based tagger (Brants, 2000) trained on the training data. We used the Basic Parser to process all the sentences of the PFR corpus. We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of token"
D09-1060,P06-2041,0,0.0127447,"r MST parsing models. For baseline systems, we used the first- and second-order basic features, which were the same as the features used by McDonald and Pereira (2006), and we used the default settings of MSTParser throughout the paper: iters=10; training-k=1; decode-type=proj. We implemented our systems based on the MSTParser by incorporating the subtree-based features. Table 2 shows the performance of the systems that were compared, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003), CO2006 refers to the parser of Corston-Oliver et al. (2006), Hall2006 refers to the parser of Hall et al. (2006), Wang2007 refers to the parser of Wang et al. (2007), Z&C 2008 refers to the combination graph-based and transition-based system of Zhang and Clark (2008), KOO08-dep1c/KOO08dep2c refers to a graph-based system with first/second-order cluster-based features by Koo et al. (2008), and Carreras2008 refers to the paper of Carreras et al. (2008). The results showed that Ord2s performed better than the first five systems. The second-order system of Koo et al. (2008) performed better than our systems. The reason may be that the MSTParser only uses sibling interactions for second-order, while Koo et a"
D09-1060,C08-1054,0,0.0118978,"rds because their numbers were very small. The Better and Worse curves showed that our approach always provided better results. The results indicated that the improvements apparently became larger when the sentences had more unknown words for the Chinese data. And for the English data, the graph also showed the similar trend, although the improvements for the sentences have three and four unknown words were slightly less than the others. 4.2.2 Coordinating conjunctions We analyzed our new parsers’ behavior for coordinating conjunction structures, which is a very difficult problem for parsing (Kawahara and Kurohashi, 2008). Here, we compared the Ord2 system with the Ord2s system. Figures 9 and 10 show how the subtree-based features affect accuracy as a function of the number of conjunctions, where the x axis refers to the number of conjunctions in one sentence and the y axis shows the percentages of the three classes. The figures indicated that the subtree-based features improved the coordinating conjunction problem. In the trigram-subtree list, many subtrees are related to coordinating conjunctions, such as “utilities:1:3 and:2:3 businesses:3:0” and “pull:1:0 and:2:1 protect:3:1”. These subtrees can provide ad"
D09-1060,P08-1068,0,0.550631,"ive idea to improve dependency parsing performance. In this paper, we present an approach that extracts subtrees from dependency trees in autoparsed data to improve dependency parsing. The 570 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 570–579, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP glish and Chinese data. We show that this simple approach greatly improves the accuracy and that the use of richer structures (i.e, word triples) indeed gives additional improvement. We also demonstrate that our approach and other improvement techniques (Koo et al., 2008; Nivre and McDonald, 2008) are complementary and that we can achieve very high accuracies when we combine our method with other improvement techniques. Specifically, we achieve the best accuracy for the Chinese data. The rest of this paper is as follows: Section 2 introduces the background of dependency parsing. Section 3 proposes an approach for extracting subtrees and represents the subtree-based features for dependency parsers. Section 4 explains the experimental results and Section 5 discusses related work. Finally, in section 6 we draw conclusions. 2 Dependency parsing Dependency parsing"
D09-1060,J93-2004,0,0.0327919,"m-subtree is formed by the word forms of h, d1, and d2. Then we retrieve the subtree in Lst to get its set ID. In addition, we consider the triples of “h-NULL”6 , d1, and d2, which means that we only check the words of sibling nodes without checking the head word. Then, we generate second-order subtree-based features, consisting of indicator functions for set IDs of the retrieved trigram-subtrees. 6 h-NULL is a dummy token 573 In order to evaluate the effectiveness of the subtree-based features, we conducted experiments on English data and Chinese Data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”7 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). To match previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10way jackknifing to generate tags for"
D09-1060,P06-1043,0,0.179445,"Missing"
D09-1060,D08-1059,0,0.606476,"d Pereira (2006), and we used the default settings of MSTParser throughout the paper: iters=10; training-k=1; decode-type=proj. We implemented our systems based on the MSTParser by incorporating the subtree-based features. Table 2 shows the performance of the systems that were compared, where Y&M2003 refers to the parser of Yamada and Matsumoto (2003), CO2006 refers to the parser of Corston-Oliver et al. (2006), Hall2006 refers to the parser of Hall et al. (2006), Wang2007 refers to the parser of Wang et al. (2007), Z&C 2008 refers to the combination graph-based and transition-based system of Zhang and Clark (2008), KOO08-dep1c/KOO08dep2c refers to a graph-based system with first/second-order cluster-based features by Koo et al. (2008), and Carreras2008 refers to the paper of Carreras et al. (2008). The results showed that Ord2s performed better than the first five systems. The second-order system of Koo et al. (2008) performed better than our systems. The reason may be that the MSTParser only uses sibling interactions for second-order, while Koo et al. (2008) uses both sibling and grandparent interactions, and uses cluster-based features. Carreras et al. (2008) reported a very high accuracy using infor"
D09-1060,E06-1011,0,0.206252,"explains the experimental results and Section 5 discusses related work. Finally, in section 6 we draw conclusions. 2 Dependency parsing Dependency parsing assigns head-dependent relations between the words in a sentence. A simple example is shown in Figure 1, where an arc between two words indicates a dependency relation between them. For example, the arc between “ate” and “fish” indicates that “ate” is the head of “fish” and “fish” is the dependent. The arc between “ROOT” and “ate” indicates that “ate” is the ROOT of the sentence. of parsing model were presented in McDonald et al. (2005) and McDonald and Pereira (2006). 2.2 Baseline Parser In the MST parsing model, there are two well-used modes: the first-order and the second-order. The first-order model uses first-order features that are defined over single graph edges and the secondorder model adds second-order features that are defined on adjacent edges. For the parsing of unannotated data, we use the first-order MST parsing model, because we need to parse a large number of sentences and the parser must be fast. We call this parser the Baseline Parser. 3 Our approach In this section, we describe our approach of extracting subtrees from unannotated data."
D09-1060,W08-2127,0,0.0175142,"cy parsing results for English, for our parsers and previous work To demonstrate that our approach and other work are complementary, we thus implemented a system using all the techniques we had at hand that used subtree- and cluster-based features and applied the integrating method of Nivre and McDonald (2008). We used the word clustering tool12 , which was used by Koo et al. (2008), to produce word clusters on the BLLIP corpus. The cluster-based features were the same as the features used by Koo et al. (2008). For the integrating method, we used the transition MaxEnt-based parser of Zhao and Kit (2008) because it was faster than the MaltParser. The results are shown in the bottom part of Table 2, where Ord1c/Ord2c refers to a first-/second-order MSTParser with cluster-based features, Ord1i/Ordli refers to a first/second-order MSTParser with integrating-based features, Ord1sc/Ord2sc refers to a first-/secondorder MSTParser with subtree-based+clusterbased features, and Ord1sci/Ord2sci refers to a first-/second-order MSTParser with subtreebased+cluster-based+integrating-based features. Ord1c/Ord2c was worse than KOO08-dep1c/dep2c, but Ord1sci outperformed KOO08-dep1c Chinese UAS 86.38 87.68(+1"
D09-1060,P05-1012,0,0.843284,"endency parsers. Section 4 explains the experimental results and Section 5 discusses related work. Finally, in section 6 we draw conclusions. 2 Dependency parsing Dependency parsing assigns head-dependent relations between the words in a sentence. A simple example is shown in Figure 1, where an arc between two words indicates a dependency relation between them. For example, the arc between “ate” and “fish” indicates that “ate” is the head of “fish” and “fish” is the dependent. The arc between “ROOT” and “ate” indicates that “ate” is the ROOT of the sentence. of parsing model were presented in McDonald et al. (2005) and McDonald and Pereira (2006). 2.2 Baseline Parser In the MST parsing model, there are two well-used modes: the first-order and the second-order. The first-order model uses first-order features that are defined over single graph edges and the secondorder model adds second-order features that are defined on adjacent edges. For the parsing of unannotated data, we use the first-order MST parsing model, because we need to parse a large number of sentences and the parser must be fast. We call this parser the Baseline Parser. 3 Our approach In this section, we describe our approach of extracting"
D09-1060,2006.iwslt-evaluation.9,0,0.0600461,"onstrate the effectiveness of our proposed approach, we present the experimental results on the English Penn Treebank and the Chinese Penn Treebank. These results show that our approach significantly outperforms baseline systems. And, it achieves the best accuracy for the Chinese data and an accuracy which is competitive with the best known systems for the English data. 1 Introduction Dependency parsing, which attempts to build dependency links between words in a sentence, has experienced a surge of interest in recent times, owing to its usefulness in such applications as machine translation (Nakazawa et al., 2006) and question answering (Cui et al., 2005). To obtain dependency parsers with high accuracy, supervised techniques require a large amount of handannotated data. While hand-annotated data are very expensive, large-scale unannotated data can be obtained easily. Therefore, the use of largescale unannotated data in training is an attractive idea to improve dependency parsing performance. In this paper, we present an approach that extracts subtrees from dependency trees in autoparsed data to improve dependency parsing. The 570 Proceedings of the 2009 Conference on Empirical Methods in Natural Langu"
D09-1060,P08-1108,0,0.351178,"e dependency parsing performance. In this paper, we present an approach that extracts subtrees from dependency trees in autoparsed data to improve dependency parsing. The 570 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 570–579, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP glish and Chinese data. We show that this simple approach greatly improves the accuracy and that the use of richer structures (i.e, word triples) indeed gives additional improvement. We also demonstrate that our approach and other improvement techniques (Koo et al., 2008; Nivre and McDonald, 2008) are complementary and that we can achieve very high accuracies when we combine our method with other improvement techniques. Specifically, we achieve the best accuracy for the Chinese data. The rest of this paper is as follows: Section 2 introduces the background of dependency parsing. Section 3 proposes an approach for extracting subtrees and represents the subtree-based features for dependency parsers. Section 4 explains the experimental results and Section 5 discusses related work. Finally, in section 6 we draw conclusions. 2 Dependency parsing Dependency parsing assigns head-dependent rel"
D09-1060,H94-1048,0,0.0447416,"the x axis refers to the number of conjunctions in one sentence and the y axis shows the percentages of the three classes. The figures indicated that the subtree-based features improved the coordinating conjunction problem. In the trigram-subtree list, many subtrees are related to coordinating conjunctions, such as “utilities:1:3 and:2:3 businesses:3:0” and “pull:1:0 and:2:1 protect:3:1”. These subtrees can provide additional information for parsing models. 4.2.3 PP attachment We analyzed our new parsers’ behavior for preposition-phrase attachment, which is also a difficult task for parsing (Ratnaparkhi et al., 1994). We compared the Ord2 system with the Ord2s system. Figures 11 and 12 show how the subtreebased features affect accuracy as a function of the number of prepositions, where the x axis refers to the number of prepositions in one sentence and the 577 y axis shows the percentages of the three classes. The figures indicated that the subtree-based features improved preposition-phrase attachment. 5 Related work Our approach is to incorporate unannotated data into parsing models for dependency parsing. Several previous studies relevant to our approach have been conducted. Chen et al. (2008) previousl"
D09-1060,W96-0213,0,0.611923,"te the effectiveness of the subtree-based features, we conducted experiments on English data and Chinese Data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”7 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). To match previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10way jackknifing to generate tags for the training set. For the unannotated data, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ text.8 We used the MXPOST tagger trained on training data to assign part-of-speech tags and used the Basic Parser to process the sentences of the BLLIP corpus. For Chinese, we used the Chinese Treebank 7 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We ensured that the text used for extracting subtrees"
D09-1060,D07-1111,0,0.0905789,"Missing"
D09-1060,E03-1008,0,0.0906958,"Missing"
D09-1060,W07-2201,0,0.202814,"Missing"
D09-1060,W03-3023,0,0.921881,"f “h-NULL”6 , d1, and d2, which means that we only check the words of sibling nodes without checking the head word. Then, we generate second-order subtree-based features, consisting of indicator functions for set IDs of the retrieved trigram-subtrees. 6 h-NULL is a dummy token 573 In order to evaluate the effectiveness of the subtree-based features, we conducted experiments on English data and Chinese Data. For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool “Penn2Malt”7 to convert the data into dependency structures using a standard set of head rules (Yamada and Matsumoto, 2003). To match previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23). Following the work of Koo et al. (2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10way jackknifing to generate tags for the training set. For the unannotated data, we used the BLLIP corpus (Charniak et al., 2000) that contains about 43 million words of WSJ text.8 We used the"
D09-1060,C08-1132,0,0.5151,"r Chinese, we used the Chinese Treebank 7 http://w3.msi.vxu.se/˜nivre/research/Penn2Malt.html We ensured that the text used for extracting subtrees did not include the sentences of the Penn Treebank. 8 (CTB) version 4.09 in the experiments. We also used the “Penn2Malt” tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development. We used gold standard segmentation and part-of-speech tags in the CTB. The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008). For the unannotated data, we used the PFR corpus10 , which has approximately 15 million words whose segmentation and POS tags are given. We used its original segmentation though there are differences in segmentation policy between CTB and this corpus. As for POS tags, we discarded the original POS tags and assigned CTB style POS tags using a TNT-based tagger (Brants, 2000) trained on the training data. We used the Basic Parser to process all the sentences of the PFR corpus. We measured the parser quality by the unlabeled attachment score (UAS), i.e., the percentage of tokens (excluding all p"
D09-1060,D07-1096,0,\N,Missing
D09-1069,P02-1051,0,0.0149479,"nt of Chinese phonemes, they are the same as the transliteration models in the literature used for machine transliteration from English to other languages without relying on targetlanguage phonemes (Karimi et al., 2007; Malik, 2006; Oh et al., 2006; Sherif and Kondrak, 2007; Yoon et al., 2007). Note that M (EG , φ) is the same transliteration model as the one proposed by Li et al. (2004). 3.2 Hybrid Transliteration Models The hybrid transliteration models in each class are defined by discrete mixture between the probability distribution of the two basic transliteration models, as in Eq. (10) (Al-Onaizan and Knight, 2002; Oh et al., 2006), where 0 &lt; α &lt; 1. We denote a hybrid transliteration model between two basic transliteration models M (x1 , y) and M (x2 , y) as M (x1 + x2 , y, α), where y ∈ Y = {φ, CP , JCP }, x1 = x2 , and x1 , x2 ∈ X = {EG , EP , EGP }. In this paper, we define three types of hybrid transliteration models in each class: M (EG + EP , y, α), M (EG + EGP , y, α), and M (EP + EGP , y, α). PM (EG ,φ) (CG |EG ) = P (CG |EG ) (7) PM (x1 +x2 ,y,α) (CG |EG ) PM (EP ,φ) (CG |EG )  P (EP |EG ) × P (CG |EP ) = (8) = α × PM (x1 ,y) (CG |EG ) + (1 − α) × PM (x2 ,y) (CG |EG ) ∀EP PM (EGP ,φ) (CG |EG"
D09-1069,W97-0301,0,0.0345717,"iple1 (egi , cpi , cgi−1 ) egi = “r”, cpi−1 = “GE”, cgi−1 = “ò:B” cgi = “/:B” f6 triple2 (egi−1 , cgi−1 , cpi−1 ) egi−1 = “g”, cpii−1 = “GE L”, cgi−1 = “ò:B” cgi = “/:B” they can be simplified into a series of products in Eqs. (11)–(13). The maximum entropy model is used to estimate the probabilities in Eqs. (11)–(13) (Berger et al., 1996). Generally, a conditional maximum entropy model is an exponential model that gives the conditional probability, as described in Eq. (14), where λi is the parameter to be estimated and fi (a, b) is a feature function corresponding to λi (Berger et al., 1996; Ratnaparkhi, 1997):  i+k P (EP |EG ) ≈ P (epi |epi−1 (11) i−k , egi−k ) Table 4: Context predicates and their descriptions Category N- GRAM PAIR T RIPLE Context predicates gram1 (uj ) gram2 (uj ) gram3 (uj ) pair11 (uj , vk ) pair12 (uj , vk ) pair22 (uj , vk ) triple1 (uj , vk , wl ) triple2 (uj , vk , wl ) Description uj uj+1 j uj+2 j u j , vk uj , vkk+1 k+1 uj+1 j , vk u j , vk , wl uj , vk , wll+1 i P (CP |EG , EP )  i+k P (cpi |cpi−1 ≈ i−k , eg, epi−k ) (12) P (CG |EG , EP , CP )  i−1 P (cgi |cgi−k , eg, ep, cpi+k ≈ i−k ) (13) (e.g., pair12 (uj , uj+1 ) = trigram(uj ) = uj+2 j ). Table 3 represents"
D09-1069,J96-1002,0,0.0696923,"n Table 2, where i = 2 i+2 f1 gram3 (egi ) egi = “ree” cgi = “/:B” cpi−1 = “G”, cgi−1 = “ò:B” cgi = “/:B” f2 pair11 (cpi−1 , cgi−1 ) i f3 pair12 (cgi−1 , cpi−1 ) cpi−1 = “GE L”, cgi−1 = “ò:B” cgi = “/:B” i egi−1 = “gr”, epii−1 = “G R” cgi = “/:B” f4 pair22 (cpi−1 , cgi−2 ) f5 triple1 (egi , cpi , cgi−1 ) egi = “r”, cpi−1 = “GE”, cgi−1 = “ò:B” cgi = “/:B” f6 triple2 (egi−1 , cgi−1 , cpi−1 ) egi−1 = “g”, cpii−1 = “GE L”, cgi−1 = “ò:B” cgi = “/:B” they can be simplified into a series of products in Eqs. (11)–(13). The maximum entropy model is used to estimate the probabilities in Eqs. (11)–(13) (Berger et al., 1996). Generally, a conditional maximum entropy model is an exponential model that gives the conditional probability, as described in Eq. (14), where λi is the parameter to be estimated and fi (a, b) is a feature function corresponding to λi (Berger et al., 1996; Ratnaparkhi, 1997):  i+k P (EP |EG ) ≈ P (epi |epi−1 (11) i−k , egi−k ) Table 4: Context predicates and their descriptions Category N- GRAM PAIR T RIPLE Context predicates gram1 (uj ) gram2 (uj ) gram3 (uj ) pair11 (uj , vk ) pair12 (uj , vk ) pair22 (uj , vk ) triple1 (uj , vk , wl ) triple2 (uj , vk , wl ) Description uj uj+1 j uj+2 j u"
D09-1069,N09-1034,0,0.0434359,"Missing"
D09-1069,P07-1119,0,0.0524423,"ee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). The three basic transliteration models in MS are identical as those in MJ , except for the Chinese phoneme-to-grapheme conversion method. They only depend on Chinese phonemes in Chinese phoneme-to-grapheme conversion represented as P (CG |CP ) in Eqs. (4)–(6). els are independent of Chinese phonemes, they are the same as the transliteration models in the literature used for machine transliteration from English to other languages without relying on targetlanguage phonemes (Karimi et al., 2007; Malik, 2006; Oh et al., 2006; Sherif and Kondrak, 2007; Yoon et al., 2007). Note that M (EG , φ) is the same transliteration model as the one proposed by Li et al. (2004). 3.2 Hybrid Transliteration Models The hybrid transliteration models in each class are defined by discrete mixture between the probability distribution of the two basic transliteration models, as in Eq. (10) (Al-Onaizan and Knight, 2002; Oh et al., 2006), where 0 &lt; α &lt; 1. We denote a hybrid transliteration model between two basic transliteration models M (x1 , y) and M (x2 , y) as M (x1 + x2 , y, α), where y ∈ Y = {φ, CP , JCP }, x1 = x2 , and x1 , x2 ∈ X = {EG , EP , EGP }. In"
D09-1069,W03-1508,0,0.493219,"mes and their corresponding English graphemes and phonemes. Experiments showed that Chinese phonemes in our proposed model can contribute to the performance improvement in English-to-Chinese transliteration. 1 Introduction 1.1 Motivation Transliteration, i.e., phonetic translation, is commonly used to translate proper names and technical terms across languages. A variety of Englishto-Chinese machine transliteration models has been proposed in the last decade (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Li et al., 2004; Li et al., 2007; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). They can be categorized into those based on Chinese phonemes (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003) and those that don’t rely on Chinese phonemes (Li et al., 2004; Li et al., 2007). Inspired by the success of English grapheme-tophoneme research in speech synthesis, many researchers have proposed phoneme-based English1.2 Our Approach Previous approaches using Chinese phonemes have relied only on Chinese phonemes in Chinese phoneme-to-grapheme conversion. However, the simple use of Chinese phonemes doesn"
D09-1069,P98-2220,0,0.760576,"int use of Chinese phonemes and their corresponding English graphemes and phonemes. Experiments showed that Chinese phonemes in our proposed model can contribute to the performance improvement in English-to-Chinese transliteration. 1 Introduction 1.1 Motivation Transliteration, i.e., phonetic translation, is commonly used to translate proper names and technical terms across languages. A variety of Englishto-Chinese machine transliteration models has been proposed in the last decade (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Li et al., 2004; Li et al., 2007; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). They can be categorized into those based on Chinese phonemes (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003) and those that don’t rely on Chinese phonemes (Li et al., 2004; Li et al., 2007). Inspired by the success of English grapheme-tophoneme research in speech synthesis, many researchers have proposed phoneme-based English1.2 Our Approach Previous approaches using Chinese phonemes have relied only on Chinese phonemes in Chinese phoneme-to-grapheme conversion. However, the simple u"
D09-1069,P07-1082,0,0.0300719,"al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). The three basic transliteration models in MS are identical as those in MJ , except for the Chinese phoneme-to-grapheme conversion method. They only depend on Chinese phonemes in Chinese phoneme-to-grapheme conversion represented as P (CG |CP ) in Eqs. (4)–(6). els are independent of Chinese phonemes, they are the same as the transliteration models in the literature used for machine transliteration from English to other languages without relying on targetlanguage phonemes (Karimi et al., 2007; Malik, 2006; Oh et al., 2006; Sherif and Kondrak, 2007; Yoon et al., 2007). Note that M (EG , φ) is the same transliteration model as the one proposed by Li et al. (2004). 3.2 Hybrid Transliteration Models The hybrid transliteration models in each class are defined by discrete mixture between the probability distribution of the two basic transliteration models, as in Eq. (10) (Al-Onaizan and Knight, 2002; Oh et al., 2006), where 0 &lt; α &lt; 1. We denote a hybrid transliteration model between two basic transliteration models M (x1 , y) and M (x2 , y) as M (x1 + x2 , y, α), where y ∈ Y = {φ, CP ,"
D09-1069,W03-0317,0,0.251343,"onventional models. Our proposed model relies on the joint use of Chinese phonemes and their corresponding English graphemes and phonemes. Experiments showed that Chinese phonemes in our proposed model can contribute to the performance improvement in English-to-Chinese transliteration. 1 Introduction 1.1 Motivation Transliteration, i.e., phonetic translation, is commonly used to translate proper names and technical terms across languages. A variety of Englishto-Chinese machine transliteration models has been proposed in the last decade (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Li et al., 2004; Li et al., 2007; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). They can be categorized into those based on Chinese phonemes (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003) and those that don’t rely on Chinese phonemes (Li et al., 2004; Li et al., 2007). Inspired by the success of English grapheme-tophoneme research in speech synthesis, many researchers have proposed phoneme-based English1.2 Our Approach Previous approaches using Chinese phonemes have relied only on Chinese phonemes in Chi"
D09-1069,P07-1015,0,0.0218881,"d Verspoor, 1998; Virga and Khudanpur, 2003). The three basic transliteration models in MS are identical as those in MJ , except for the Chinese phoneme-to-grapheme conversion method. They only depend on Chinese phonemes in Chinese phoneme-to-grapheme conversion represented as P (CG |CP ) in Eqs. (4)–(6). els are independent of Chinese phonemes, they are the same as the transliteration models in the literature used for machine transliteration from English to other languages without relying on targetlanguage phonemes (Karimi et al., 2007; Malik, 2006; Oh et al., 2006; Sherif and Kondrak, 2007; Yoon et al., 2007). Note that M (EG , φ) is the same transliteration model as the one proposed by Li et al. (2004). 3.2 Hybrid Transliteration Models The hybrid transliteration models in each class are defined by discrete mixture between the probability distribution of the two basic transliteration models, as in Eq. (10) (Al-Onaizan and Knight, 2002; Oh et al., 2006), where 0 &lt; α &lt; 1. We denote a hybrid transliteration model between two basic transliteration models M (x1 , y) and M (x2 , y) as M (x1 + x2 , y, α), where y ∈ Y = {φ, CP , JCP }, x1 = x2 , and x1 , x2 ∈ X = {EG , EP , EGP }. In this paper, we defi"
D09-1069,P04-1021,0,0.192847,".jp Abstract to-Chinese transliteration models. In these approaches, Chinese phonemes are generated from English graphemes or phonemes, and then the Chinese phonemes are converted into Chinese graphemes (or characters), where Chinese Pinyin strings1 are used for representing a syllable-level Chinese phoneme sequence. Despite its high accuracy in generating Chinese phonemes from English, this approach has severely suffered from errors in Chinese phoneme-to-grapheme conversion, mainly caused by Chinese homophone confusion – one Chinese Pinyin string can correspond to several Chinese characters (Li et al., 2004). For example, the Pinyin string “LI” corresponds to such different Chinese characters as (, :, and /. For this reason, it has been reported that English-toChinese transliteration without Chinese phonemes outperforms that with Chinese phonemes (Li et al., 2004). Then “Can Chinese phonemes improve English-to-Chinese transliteration, if we can reduce the errors in Chinese phoneme-to-grapheme conversion?” Our research starts from this question. Inspired by the success of English grapheme-to-phoneme research in speech synthesis, many researchers have proposed phoneme-based English-to-Chinese trans"
D09-1069,P07-1016,0,0.439781,"relies on the joint use of Chinese phonemes and their corresponding English graphemes and phonemes. Experiments showed that Chinese phonemes in our proposed model can contribute to the performance improvement in English-to-Chinese transliteration. 1 Introduction 1.1 Motivation Transliteration, i.e., phonetic translation, is commonly used to translate proper names and technical terms across languages. A variety of Englishto-Chinese machine transliteration models has been proposed in the last decade (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Li et al., 2004; Li et al., 2007; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). They can be categorized into those based on Chinese phonemes (Meng et al., 2001; Gao et al., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003) and those that don’t rely on Chinese phonemes (Li et al., 2004; Li et al., 2007). Inspired by the success of English grapheme-tophoneme research in speech synthesis, many researchers have proposed phoneme-based English1.2 Our Approach Previous approaches using Chinese phonemes have relied only on Chinese phonemes in Chinese phoneme-to-grapheme conversio"
D09-1069,P06-1143,0,0.0302869,"., 2004; Jiang et al., 2007; Lee and Chang, 2003; Wan and Verspoor, 1998; Virga and Khudanpur, 2003). The three basic transliteration models in MS are identical as those in MJ , except for the Chinese phoneme-to-grapheme conversion method. They only depend on Chinese phonemes in Chinese phoneme-to-grapheme conversion represented as P (CG |CP ) in Eqs. (4)–(6). els are independent of Chinese phonemes, they are the same as the transliteration models in the literature used for machine transliteration from English to other languages without relying on targetlanguage phonemes (Karimi et al., 2007; Malik, 2006; Oh et al., 2006; Sherif and Kondrak, 2007; Yoon et al., 2007). Note that M (EG , φ) is the same transliteration model as the one proposed by Li et al. (2004). 3.2 Hybrid Transliteration Models The hybrid transliteration models in each class are defined by discrete mixture between the probability distribution of the two basic transliteration models, as in Eq. (10) (Al-Onaizan and Knight, 2002; Oh et al., 2006), where 0 &lt; α &lt; 1. We denote a hybrid transliteration model between two basic transliteration models M (x1 , y) and M (x2 , y) as M (x1 + x2 , y, α), where y ∈ Y = {φ, CP , JCP }, x1 ="
D09-1069,H89-2027,0,\N,Missing
D09-1069,C98-2215,0,\N,Missing
D09-1069,W09-3502,0,\N,Missing
D09-1069,Y06-1050,0,\N,Missing
E99-1026,W98-1512,0,0.0661847,"Missing"
E99-1026,J96-1002,0,0.112055,". Because of the statistical property, we can incorporate a beam search, an effective way of limiting the search space in a backward analysis. 0 : otherwise. Here ""has(h,z)"" is a binary function which returns true if the history h has an attribute x. We focus on attributes on a bunsetsu itself and those between bunsetsus. Section 3 will mention these attributes. Given a set of features and some training data, the maximum entropy estimation process produces a model in which every feature gi has associated with it a parameter ai. This allows us to compute the conditional probability as follows (Berger et al., 1996): P(flh) Y I iz~(h) a [ '(n'l) - ~,i I 2 The Probability Model Given a tokenization of a test corpus, the problem of dependency structure analysis in Japanese can be reduced to the problem of assigning one of two tags to each relationship which consists of two bunsetsus. A relationship could be tagged as ""0"" or ""1"" to indicate whether or not there is a dependency between the bunsetsus, respectively. The two tags form the space of ""futures"" for a maximum entropy formulation of our dependency problem between bunsetsus. A maximum entropy solution to this, or any other similar problem allows the c"
E99-1026,P96-1025,0,0.102655,"Missing"
E99-1026,W98-1511,0,0.439654,"consistent rules or assign consistent scores. • As syntactic characteristics differ across different domains, the rules have to be changed when the target domain changes. It is costly to create a new hand-made rule for each domain. At/other approach is a fully automatic corpusbased approach. This approach has the potential to overcome the problems of the rule-based approach. It automatically learns the likelihoods of dependencies from a tagged corpus and calculates the best dependencies for an input sentence. We take this approach. This approach is taken by some other systems (Collins, 1996; Fujio and Matsumoto, 1998; Haruno et ah, 1998). The parser proposed by Ratnaparkhi (Ratnaparkhi, 1997) is considered to be one of the most accurate parsers in English. Its probability estimation is based on the maximum entropy models. We also use the maximum entropy model. This model learns the weights of given features from a training corpus. The weights are calculated based on the frequencies of the features in the training data. The set of features is defined by a human. In our model, we use features of bunsetsu, such as character strings, parts of speech, and inflection types of bunsetsu, as well as information be"
E99-1026,P98-1083,0,0.271188,"eriments and the results. Then we describe some interesting statistics that we found in our experiments. Finally, we compare our work with some related systems. 3.1 R e s u l t s o f E x p e r i m e n t s The features used in our experiments are listed in Tables 1 and 2. Each row in Table 1 contains a feature type, feature values, and an experimental result that will be explained later. Each feature consists of a type and a value. The features are basically some attributes of a bunsetsu itself or those between bunsetsus. We call them 'basic features.' The list is expanded from tIaruno's list (Haruno et al., 1998). The features in the list are classified into five categories that are related to the ""Head"" part of the anterior bunsetsu (category ""a""), the '~rype"" part of the anterior bunsetsu (category ""b""), the ""Head"" part of the posterior bunsetsu (category ""c""), the '~l~ype"" part of the posterior bunsetsu (category ""d""), and the features between bunsetsus (category ""e"") respectively. The term ""Head"" basically means a rightmost content word in a bunsetsu, and the term ""Type"" basically means a function word following a ""Head"" word or an inflection type of a ""Head"" word. The terms are defined in the fol"
E99-1026,W97-0301,0,0.0805237,"Missing"
E99-1026,C98-1080,0,\N,Missing
I05-2015,J97-2004,0,0.0332403,"he annotated data to improve the performance of automatic word alignment. We will also investigate a method to automatically identify phrase alignments from the annotated word alignment and a method to automatically discover the syntactic structures on the Chinese side from the annotated phrase alignments. Annotation of word alignment Since automatic word alignment techniques cannot reach as high a level as the morphological analyses, we adopt a practical method of using multiple aligners. One aligner is a lexical knowledge-based approach, which was implemented by us based on the work of Ker (Ker and Chang, 1997). Another aligner is the well-known GIZA++ toolkit, which is a statistics-based approach. For GIZA++, two directions were adopted: the Chinese sentences were used as source sentences and the Japanese sentences as target sentences, and vice versa. The results produced by the lexical knowledgebased aligner, C → J of GIZA++, and J → C of GIZA++ were selected in a majority decision. If an alignment result was produced by two or three aligners at the same time, the result was accepted. Otherwise, was abandoned. In this way, we aimed to utilize the results of each aligner and maintain high precision"
I05-2015,maekawa-etal-2000-spontaneous,1,0.909793,"Missing"
I05-2015,P01-1067,0,0.0246433,"Missing"
I05-2015,C94-2209,0,0.0395746,"Annotation on Chinese Sentences For Chinese morphological analysis, we used the analyser developed by Peking University, where the research on definition of Chinese words and the criteria of word segmentation has been conducted for over ten years. The achievements include a grammatical knowledge base of contemporary Chinese, an automatic morphological analyser, and an annotated People’s Daily Corpus. Since the definition and tagset are widely used in Chinese language processing, we also took the criteria as the basis of our guidelines. A morphological analyzer developed by Peking University (Zhou and Yu, 1994) was applied for automatic annotation of the Chinese sentences and then the automatically tagged sentences were revised by humans. An annotated sentence is illustrated in Figure 3, which is the Chinese sentence in Ex. 1 in Section 2. The interface of the tool is shown in Figure 4 and Figure 5. S-ID: 950104141-008 这些/r 俄军/j 士兵/n 均/d 为/v 十九/m 岁/q 左右/m 的/u 年青人/n ，/w 他们/r 甚至/d 连/p 回答/v 问题/n 的/u 气力/n 也/d 没有/v 。/w Figure 3 An annotated Chinese sentence 4.3 Tool for Manual Revision We developed a tool to assist annotators in revision. The tool has both Japanese and Chinese versions. Here, we introduc"
I05-2015,W04-2208,1,\N,Missing
I05-6009,izumi-etal-2004-overview,1,0.818468,"Missing"
I08-1012,A00-1031,0,0.0519108,"2 in our experiments. We used the same rules for conversion and created the same data split as (Wang et al., 2007): ﬁles 1-270 and 400-931 as training, 271-300 as testing and ﬁles 301-325 as development. We used the gold standard segmentation and POS tags in the CTB. For unlabeled data, we used the PFR corpus 3 . It includes the documents from People’s Daily at 1998 (12 months). There are about 290 thousand sentences and 15 million words in the PFR corpus. To simplify, we used its segmentation. And we discarded the POS tags because PFR and CTB used different POS sets. We used the package TNT (Brants, 2000), a very efﬁcient statistical part-of-speech tagger, to train a POS tagger4 on training data of the CTB. We measured the quality of the parser by the unlabeled attachment score (UAS), i.e., the percentage of tokens with correct HEAD. We reported two types of scores: “UAS without p” is the UAS score without all punctuation tokens and “UAS with p” is the one with all punctuation tokens. 4.1 Experimental results In the experiments, we trained the parsers on training data and tuned the parameters on development data. In the following sessions, “baseline” refers to Basic Parser (the model with basi"
I08-1012,D07-1097,0,0.013766,"case frames. And we represent additional information as the features for learning models while they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input string): • Left-Arc(LA): Add an arc f"
I08-1012,N06-1023,1,0.793343,"successful instance of parsing with self-training by using a re-ranker. As Figure 1 suggests, the dependency parser performs bad for parsing the words with long distances. In our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser. (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. They obtain consistent improvements by penalizing dependencies between two words that are farther apart in the string. The study most relevant to ours is done by (Kawahara and Kurohashi, 2006). They present an integrated probabilistic model for Japanese parsing. They also use partial information after current parser parses the sentences. Our work differs in that we consider general dependency relations while they only consider case frames. And we represent additional information as the features for learning models while they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature sp"
I08-1012,P06-1043,0,0.0775317,"mance. Our study is relative to incorporating unlabeled data into a model for parsing. There are several other studies relevant to ours as described below. A simple method is self-training in which the existing model ﬁrst labels unlabeled data and then the newly labeled data is then treated as hand annotated data for training a new model. But it seems that selftraining is not so effective. (Steedman et al., 2003) reports minor improvement by using self-training for syntactic parsing on small labeled data. The reason may be that errors in the original model would be ampliﬁed in the new model. (McClosky et al., 2006) presents a successful instance of parsing with self-training by using a re-ranker. As Figure 1 suggests, the dependency parser performs bad for parsing the words with long distances. In our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser. (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. They obtain consistent improvements by penalizing dependencies between two words that are farther apart in the string. The study most relevant to ours"
I08-1012,D07-1013,0,0.091155,"Missing"
I08-1012,E06-1011,0,0.0577863,"Missing"
I08-1012,W06-2932,0,0.0408924,"onent for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input string): • Left-Arc(LA): Add an arc from NEXT to TOP; pop the stack. • Right-Arc(RA): Add an arc from TOP to NEXT; push NEXT onto the stack. • Reduce(RE): Pop the stack. •"
I08-1012,W06-2933,0,0.0192393,"elations while they only consider case frames. And we represent additional information as the features for learning models while they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input stri"
I08-1012,W03-3017,0,0.0749943,"t parser parses the sentences. Our work differs in that we consider general dependency relations while they only consider case frames. And we represent additional information as the features for learning models while they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (w"
I08-1012,P06-1072,0,0.0195353,"model. But it seems that selftraining is not so effective. (Steedman et al., 2003) reports minor improvement by using self-training for syntactic parsing on small labeled data. The reason may be that errors in the original model would be ampliﬁed in the new model. (McClosky et al., 2006) presents a successful instance of parsing with self-training by using a re-ranker. As Figure 1 suggests, the dependency parser performs bad for parsing the words with long distances. In our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser. (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependency grammar induction models by EM from unlabeled data. They obtain consistent improvements by penalizing dependencies between two words that are farther apart in the string. The study most relevant to ours is done by (Kawahara and Kurohashi, 2006). They present an integrated probabilistic model for Japanese parsing. They also use partial information after current parser parses the sentences. Our work differs in that we consider general dependency relations while they only consider case frames. And we represent additional information as"
I08-1012,E03-1008,0,0.0855318,"ta for dependency parsing. We use a parser to parse the sentences in unlabeled data. Then another parser makes use of the information on short dependency relations in the newly parsed data to improve performance. Our study is relative to incorporating unlabeled data into a model for parsing. There are several other studies relevant to ours as described below. A simple method is self-training in which the existing model ﬁrst labels unlabeled data and then the newly labeled data is then treated as hand annotated data for training a new model. But it seems that selftraining is not so effective. (Steedman et al., 2003) reports minor improvement by using self-training for syntactic parsing on small labeled data. The reason may be that errors in the original model would be ampliﬁed in the new model. (McClosky et al., 2006) presents a successful instance of parsing with self-training by using a re-ranker. As Figure 1 suggests, the dependency parser performs bad for parsing the words with long distances. In our approach, we choose partial reliable information which comes from short dependency relations for the dependency parser. (Smith and Eisner, 2006) presents an approach to improve the accuracy of a dependen"
I08-1012,W05-1516,0,0.0438281,"Missing"
I08-1012,P06-1054,0,0.0376974,"Missing"
I08-1012,N07-3002,0,0.0308211,"Missing"
I08-1012,W03-3023,0,0.171228,"le they use the case frames as one component for a probabilistic model. 3 Our Approach In this section, we describe our approach of exploiting reliable features from unlabeled data, which is parsed by a basic parser. We then train another parser based on new feature space. 3.1 Training a basic parser In this paper, we implement a deterministic parser based on the model described by (Nivre, 2003). This model is simple and works very well in the shared-tasks of CoNLL2006(Nivre et al., 2006) and CoNLL2007(Hall et al., 2007). In fact, our approach 90 can also be applied to other parsers, such as (Yamada and Matsumoto, 2003)’s parser, (McDonald et al., 2006)’s parser, and so on. 3.1.1 The parser The parser predicts unlabeled directed dependencies between words in sentences. The algorithm (Nivre, 2003) makes a dependency parsing tree in one left-to-right pass over the input, and uses a stack to store the processed tokens. The behaviors of the parser are deﬁned by four elementary actions (where TOP is the token on top of the stack and NEXT is the next token in the original input string): • Left-Arc(LA): Add an arc from NEXT to TOP; pop the stack. • Right-Arc(RA): Add an arc from TOP to NEXT; push NEXT onto the stac"
I08-1012,D07-1096,0,\N,Missing
I08-2097,P05-1001,0,0.0205575,"machine in spite of the time complexity of O(n3 ). If greater efﬁciency is required, it is possible to apply a pre-ﬁlter that removes long sentences (e.g., longer than 30 words), which are seldom selected by the reliability detector. In addition, our method does not depend on a particular parser, and can be applied to other state-of-theart parsers, such as Malt Parser (Nivre et al., 2006), which is a feature-rich linear-time parser. In general, it is very difﬁcult to improve the accuracy of the best performing systems by using unlabeled data. There are only a few successful studies, such as (Ando and Zhang, 2005) for chunking and (McClosky et al., 2006a; McClosky et al., 2006b) on constituency parsing. We succeeded in boosting the accuracy of the second-order MST parser, which is 713 a state-of-the-art dependency parser, in the CoNLL 2007 domain adaptation task. This was a difﬁcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data (Dredze et al., 2007). The key factor in our success was the extraction of only reliable information from unlabeled data. However, that improvement was not satisfactory. In order to achieve more gains, it is necessary to exp"
I08-2097,J94-4001,0,0.034451,"Missing"
I08-2097,N06-1020,0,0.118392,"of O(n3 ). If greater efﬁciency is required, it is possible to apply a pre-ﬁlter that removes long sentences (e.g., longer than 30 words), which are seldom selected by the reliability detector. In addition, our method does not depend on a particular parser, and can be applied to other state-of-theart parsers, such as Malt Parser (Nivre et al., 2006), which is a feature-rich linear-time parser. In general, it is very difﬁcult to improve the accuracy of the best performing systems by using unlabeled data. There are only a few successful studies, such as (Ando and Zhang, 2005) for chunking and (McClosky et al., 2006a; McClosky et al., 2006b) on constituency parsing. We succeeded in boosting the accuracy of the second-order MST parser, which is 713 a state-of-the-art dependency parser, in the CoNLL 2007 domain adaptation task. This was a difﬁcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data (Dredze et al., 2007). The key factor in our success was the extraction of only reliable information from unlabeled data. However, that improvement was not satisfactory. In order to achieve more gains, it is necessary to exploit a much larger number of unlabeled d"
I08-2097,P06-1043,0,0.0739276,"of O(n3 ). If greater efﬁciency is required, it is possible to apply a pre-ﬁlter that removes long sentences (e.g., longer than 30 words), which are seldom selected by the reliability detector. In addition, our method does not depend on a particular parser, and can be applied to other state-of-theart parsers, such as Malt Parser (Nivre et al., 2006), which is a feature-rich linear-time parser. In general, it is very difﬁcult to improve the accuracy of the best performing systems by using unlabeled data. There are only a few successful studies, such as (Ando and Zhang, 2005) for chunking and (McClosky et al., 2006a; McClosky et al., 2006b) on constituency parsing. We succeeded in boosting the accuracy of the second-order MST parser, which is 713 a state-of-the-art dependency parser, in the CoNLL 2007 domain adaptation task. This was a difﬁcult challenge as many participants in the task failed to obtain any meaningful gains from unlabeled data (Dredze et al., 2007). The key factor in our success was the extraction of only reliable information from unlabeled data. However, that improvement was not satisfactory. In order to achieve more gains, it is necessary to exploit a much larger number of unlabeled d"
I08-2097,D07-1013,0,0.0883378,"Missing"
I08-2097,W06-2932,0,0.0182422,"for them. We prepared the following three labeled data sets to train the base dependency parser and the reliability detector. PTB base train: training set for the base parser: 14,862 sentences PTB rel train: training set for reliability detector: 2,500 sentences4 BIO rel dev: development set for reliability detector: 200 sentences (= labeled BIO data) PTB base train is used to train the base dependency parser, and PTB rel train is used to train our reliability detector. BIO rel dev is used for tuning the parameters of the reliability detector. 4.1 Base Dependency Parser We used the MSTParser (McDonald et al., 2006), which achieved top results in the CoNLL 2006 (CoNLL-X) shared task, as a base dependency parser. To enable second-order features, the parameter order was set to 2. The other parameters were set to default. We used PTB base train (14,862 sentences) to train this parser. 4.2 Algorithm to Detect Reliable Parses We built a binary classiﬁer for detecting reliable sentences from a set of automatic parses produced by 4 1,215 labeled PTB sentences are left as another development set for the reliability detector, but they are not used in this paper. the base dependency parser. We used support vector"
I08-2097,1991.mtsummit-papers.9,0,0.0246341,", does not deal with domain adaptation of a tagger but focuses on that of a parser. 4 Learning Reliability of Parses Our approach assesses automatic parses of a single parser in order to select only reliable parses from them. We compare automatic parses and their goldstandard ones, and regard accurate parses as positive examples and the remainder as negative examples. Based on these examples, we build a binary classiﬁer that classiﬁes each sentence as reliable or not. To precisely detect reliable parses, we make use of several linguistic features inspired by the notion of controlled language (Mitamura et al., 1991). That is to say, the reliability of parses is judged based on the degree of sentence difﬁculty. Before describing our base dependency parser and the algorithm for detecting reliable parses, we ﬁrst explain the data sets used for them. We prepared the following three labeled data sets to train the base dependency parser and the reliability detector. PTB base train: training set for the base parser: 14,862 sentences PTB rel train: training set for reliability detector: 2,500 sentences4 BIO rel dev: development set for reliability detector: 200 sentences (= labeled BIO data) PTB base train is us"
I08-2097,P02-1063,0,0.0680386,"Missing"
I08-2097,D07-1119,0,0.0265757,"Missing"
I08-2097,W96-0213,0,0.0379723,"Missing"
I08-2097,P07-1052,0,0.0982428,"Missing"
I08-2097,P05-1022,0,0.171,"Missing"
I08-2097,P07-1078,0,0.185469,"Missing"
I08-2097,J05-1003,0,0.0434159,"Missing"
I08-2097,P07-1033,0,0.0327808,"Missing"
I08-2097,D07-1097,0,0.0282794,"Missing"
I08-2097,D07-1111,0,0.296028,"le parser, because speed and efﬁciency are important when processing a massive volume of text. The resulting highly reliable parses would be useful to automatically construct dictionaries and knowledge bases, such as case frames (Kawahara and Kurohashi, 2006). Furthermore, we incorporate the reliable parses we obtained into the dependency parser to achieve domain adaptation. The CoNLL 2007 shared task tackled domain adaptation of dependency parsers for the ﬁrst time (Nivre et al., 2007). Sagae and Tsujii applied an ensemble method to the domain adaptation track and achieved the highest score (Sagae and Tsujii, 2007). They ﬁrst parsed in-domain unlabeled sentences using two parsers trained on out-of-domain labeled data. Then, they extracted identical parses that were produced by the two parsers and added them to the original (out-of-domain) training set to train a domain-adapted model. Dredze et al. yielded the second highest score1 in the domain adaptation track (Dredze et al., 2007). However, their results were obtained without adaptation. They concluded that it is very difﬁcult to substantially improve the target domain performance over that of a state-of-the-art parser. To conﬁrm this, we parsed the t"
I08-2097,W07-2201,0,0.0274237,"Missing"
I08-2097,N07-1049,0,\N,Missing
I08-2097,W06-2933,0,\N,Missing
I08-2097,N06-2033,0,\N,Missing
I08-2097,D07-1112,0,\N,Missing
I08-2097,D07-1096,0,\N,Missing
isahara-etal-2008-development,vossen-etal-2008-kyoto,1,\N,Missing
isahara-etal-2008-development,kanzaki-etal-2008-extraction,1,\N,Missing
isahara-etal-2008-development,kaji-watanabe-2006-automatic,0,\N,Missing
izumi-etal-2004-overview,W00-0708,0,\N,Missing
kawahara-uchimoto-2008-method,W98-1505,0,\N,Missing
kawahara-uchimoto-2008-method,H05-1059,0,\N,Missing
kawahara-uchimoto-2008-method,C94-1042,0,\N,Missing
kawahara-uchimoto-2008-method,W07-1424,0,\N,Missing
kawahara-uchimoto-2008-method,W02-0907,0,\N,Missing
kawahara-uchimoto-2008-method,A97-1052,0,\N,Missing
kawahara-uchimoto-2008-method,J93-2002,0,\N,Missing
kawahara-uchimoto-2008-method,P03-1007,0,\N,Missing
kawahara-uchimoto-2008-method,P98-1013,0,\N,Missing
kawahara-uchimoto-2008-method,C98-1013,0,\N,Missing
kawahara-uchimoto-2008-method,P93-1032,0,\N,Missing
kawahara-uchimoto-2008-method,W06-2932,0,\N,Missing
kawahara-uchimoto-2008-method,P98-1071,0,\N,Missing
kawahara-uchimoto-2008-method,C98-1068,0,\N,Missing
kawahara-uchimoto-2008-method,P87-1027,0,\N,Missing
kawahara-uchimoto-2008-method,P99-1051,0,\N,Missing
kawahara-uchimoto-2008-method,korhonen-etal-2006-large,0,\N,Missing
kawahara-uchimoto-2008-method,kawahara-kurohashi-2006-case,1,\N,Missing
kawahara-uchimoto-2008-method,W97-0123,0,\N,Missing
kozawa-etal-2008-automatic,A00-2018,0,\N,Missing
kozawa-etal-2008-automatic,C92-2083,0,\N,Missing
kozawa-etal-2008-automatic,Y04-1012,0,\N,Missing
kozawa-etal-2008-automatic,P07-1112,0,\N,Missing
kozawa-etal-2008-automatic,bouillon-etal-2002-acquisition,0,\N,Missing
kozawa-etal-2008-automatic,C96-1079,0,\N,Missing
kozawa-etal-2010-collection,kozawa-etal-2008-automatic,1,\N,Missing
kozawa-etal-2010-collection,quasthoff-etal-2006-corpus,0,\N,Missing
kozawa-etal-2010-collection,biemann-etal-2004-web,0,\N,Missing
kozawa-etal-2010-collection,tohyama-etal-2008-construction-metadata,1,\N,Missing
kozawa-etal-2010-collection,dalli-etal-2004-web,0,\N,Missing
L16-1350,W14-7001,1,0.377751,"her unit belonging to the same paper in the translated data are extracted. Therefore, there is no sentence pairs sharing the same paper across the training, development, development-test and test sets. This is a practical setting of the machine translation for scientific papers in the future where the input sentences are not in the training data. Application: Workshop on Asian Translation (WAT) 4.1. Overview of WAT The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages hosted by JST, NICT and Kyoto University. The first workshop was held in 2014 (Nakazawa et al., 2014) where the ASPEC was centered as the official dataset for the scientific paper translation subtasks. ASPEC was again used in the workshop in 2015 (Nakazawa et al., 2015) to observe the contiguous development of machine translation technologies together with the newly added dataset. WAT will keep growing as the leader of the machine translation technology development in Asia. WAT is working toward the practical use of machine translation among all Asian countries. WAT tries to understand the essence of machine translation and the problems to be solved by collecting and sharing the knowledge acq"
L16-1350,W15-5001,1,0.860841,"development-test and test sets. This is a practical setting of the machine translation for scientific papers in the future where the input sentences are not in the training data. Application: Workshop on Asian Translation (WAT) 4.1. Overview of WAT The Workshop on Asian Translation (WAT) is a new open evaluation campaign focusing on Asian languages hosted by JST, NICT and Kyoto University. The first workshop was held in 2014 (Nakazawa et al., 2014) where the ASPEC was centered as the official dataset for the scientific paper translation subtasks. ASPEC was again used in the workshop in 2015 (Nakazawa et al., 2015) to observe the contiguous development of machine translation technologies together with the newly added dataset. WAT will keep growing as the leader of the machine translation technology development in Asia. WAT is working toward the practical use of machine translation among all Asian countries. WAT tries to understand the essence of machine translation and the problems to be solved by collecting and sharing the knowledge acquired in the workshop. WAT is unique in the following points: As described in Section 2., ASPEC-JC includes only 8 scientific fields. The distribution of the fields is s"
L16-1350,2007.mtsummit-papers.63,1,0.812108,"Missing"
L18-1456,L18-1456,1,0.0513221,"Missing"
L18-1456,tohyama-etal-2008-construction-metadata,1,0.468776,"Missing"
N07-1005,2001.mtsummit-papers.3,0,0.0334423,"ere, the term Qj corresponds to the rate of accomplishment of the sub-goal having the i-th ID, and λQj is a weight for the rate of accomplishment. The  term Qj corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ is a j weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λ is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et 37 Automatic Estimation of Rate of Accomplishment of Sub-goals The rate of accomplishment of sub-goals is estimated by determ"
N07-1005,P04-1079,0,0.130077,"ine Translation Based on Rate of Accomplishment of Sub-goals Kiyotaka Uchimoto and Katsunori Kotani and Yujie Zhang and Hitoshi Isahara National Institute of Information and Communications Technology 3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan {uchimoto,yujie,isahara}@nict.go.jp, kat@khn.nict.go.jp Abstract issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate. We propose a method f"
N07-1005,W05-0909,0,0.126772,"nt of Sub-goals Kiyotaka Uchimoto and Katsunori Kotani and Yujie Zhang and Hitoshi Isahara National Institute of Information and Communications Technology 3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan {uchimoto,yujie,isahara}@nict.go.jp, kat@khn.nict.go.jp Abstract issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate. We propose a method for automatically evaluating the quality of ea"
N07-1005,2005.iwslt-1.26,0,0.0249315,"Missing"
N07-1005,1995.mtsummit-1.35,1,0.83169,"nguistic phenomena that are difficult to automatically translate. Recently, MT evaluation campaigns such 34 as the International Workshop on Spoken Language Translation 1 , NIST Machine Translation Evaluation 2 , and HTRDP Evaluation 3 were organized to support the improvement of MT techniques. The data used in the evaluation campaigns were arbitrarily collected from newspaper articles or travel conversation data for fair evaluation. They are classified as the former type of data mentioned above. On the other hand, the data provided by NTT (Ikehara et al., 1994) and that constructed by JEIDA (Isahara, 1995) are classified as the latter type. Almost all the data mentioned above consist of only parallel translations in two languages. Data with information for evaluating MT results, such as JEIDA’s are rarely found. In this paper, we call data that consist of parallel translations collected for MT evaluation and that the information for MT evaluation is assigned to, a test set. The most characteristic information assigned to the JEIDA test set is the yes/no question for assessing the translation results. For example, a yes/no question such as “Is ‘for’ translated into an expression representing a c"
N07-1005,W04-3250,0,0.0286266,"ach question. The third procedure is combining a measure based on the questions and conventional measures. We also present a method for automatically generating sub-goals in the form of yes/no questions and estimating the rate of accomplishment of the sub-goals. Promising results are shown. 1 Introduction In machine translation (MT) research, appropriately evaluating the quality of MT results is an important MT systems can be ranked if a set of MT results for each system and their reference translations are given. Usually, about 300 or more sentences are used to automatically rank MT systems (Koehn, 2004). However, the quality of a sentence translated by an MT system is difficult to evaluate. For example, the results of five MTs into Japanese of the sentence “The percentage of stomach cancer among the workers appears to be the highest for any asbestos workers.” are shown in Table 1. A conventional automatic evaluation method ranks the fifth MT result first although its human subjective evaluation is the lowest. This is because conventional methods are based on the similarity between a translated sentence and its reference translation, and they give the translated sentence a high score when the"
N07-1005,2003.mtsummit-papers.32,0,0.19761,"Zealand. The personal pronoun “they” is omitted in a translation like “nyuujiilando de wa eigo wo hanasu”? The answer is yes if the pattern [karera wa|sore ra wa] is not included in a translation. Otherwise, the answer is no. ear regression model as follows using the rate of accomplishment of the sub-goals and the similarities between a given translation and its reference translation. The best-fitted line for the observed data is calculated by the method of least-squares (Draper and Smith, 1981). A = m  i=1 +  Qj =   Qj = λSi × Si n  (1)  (λQj × Qj + λQ × Qj ) + λ j=1 al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S∗, SU∗, W-1.2), were used to calculate each similarity Si . Therefore, the value of m in Eq. (1) was 23. Japanese word segmentation was performed by using JUMAN 4 in our experiments. As you can see, the definition of our new measure is based on a combination of an evaluation measure focusing on local information and that focusing on global information. j 1 : if subgoal is accomplished 0 : otherwise 3.2 (2) 1 : if subgoal is unaccomplished (3) 0 : otherwise Here, the term Qj corresponds to the rate of accomplish"
N07-1005,C04-1072,0,0.0727122,"ate of Accomplishment of Sub-goals Kiyotaka Uchimoto and Katsunori Kotani and Yujie Zhang and Hitoshi Isahara National Institute of Information and Communications Technology 3-5, Hikari-dai, Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan {uchimoto,yujie,isahara}@nict.go.jp, kat@khn.nict.go.jp Abstract issue. In recent years, many researchers have tried to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate. We propose a method for automatically ev"
N07-1005,W04-1013,0,0.00357986,"ey” is omitted in a translation like “nyuujiilando de wa eigo wo hanasu”? The answer is yes if the pattern [karera wa|sore ra wa] is not included in a translation. Otherwise, the answer is no. ear regression model as follows using the rate of accomplishment of the sub-goals and the similarities between a given translation and its reference translation. The best-fitted line for the observed data is calculated by the method of least-squares (Draper and Smith, 1981). A = m  i=1 +  Qj =   Qj = λSi × Si n  (1)  (λQj × Qj + λQ × Qj ) + λ j=1 al., 2000), PER (Leusch et al., 2003), and ROUGE (Lin, 2004) with n-gram lengths of 1, 2, 3, and 4 and 4 variants (LCS, S∗, SU∗, W-1.2), were used to calculate each similarity Si . Therefore, the value of m in Eq. (1) was 23. Japanese word segmentation was performed by using JUMAN 4 in our experiments. As you can see, the definition of our new measure is based on a combination of an evaluation measure focusing on local information and that focusing on global information. j 1 : if subgoal is accomplished 0 : otherwise 3.2 (2) 1 : if subgoal is unaccomplished (3) 0 : otherwise Here, the term Qj corresponds to the rate of accomplishment of the sub-goal ha"
N07-1005,niessen-etal-2000-evaluation,0,0.0410402,"ed (3) 0 : otherwise Here, the term Qj corresponds to the rate of accomplishment of the sub-goal having the i-th ID, and λQj is a weight for the rate of accomplishment. The  term Qj corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ is a j weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λ is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et 37 Automatic Estimation of Rate of Accomplishment of Sub-goals The rate of accomplishment of sub-goals is"
N07-1005,P03-1021,0,0.0239749,"ed to automatically evaluate the quality of MT and improve the performance of automatic MT evaluations (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005) because improving the performance of automatic MT evaluation is expected to enable us to use and improve MT systems efficiently. For example, Och reported that the quality of MT results was improved by using automatic MT evaluation measures for the parameter tuning of an MT system (Och, 2003). This report shows that the quality of MT results improves as the performance of automatic MT evaluation improves. The quality of a sentence translated by a machine translation (MT) system is difficult to evaluate. We propose a method for automatically evaluating the quality of each translation. In general, when translating a given sentence, one or more conditions should be satisfied to maintain a high translation quality. In EnglishJapanese translation, for example, prepositions and infinitives must be appropriately translated. We show several procedures that enable evaluating the quality of"
N07-1005,P02-1040,0,0.0808433,"responds to the rate of accomplishment of the sub-goal having the i-th ID, and λQj is a weight for the rate of accomplishment. The  term Qj corresponds to the rate of unaccomplishment of the sub-goal having the i-th ID, and λQ is a j weight for the rate of unaccomplishment. The value n indicates the number of types of sub-goals. The term λ is constant. The term Si indicates a similarity between a translated sentence and its reference translation, and λSi is a weight for the similarity. Many methods for calculating the similarity have been proposed (Niessen et al., 2000; Akiba et al., 2001; Papineni et al., 2002; NIST, 2002; Leusch et al., 2003; Turian et al., 2003; Babych and Hartley, 2004; Lin and Och, 2004; Banerjee and Lavie, 2005; Gime´nez et al., 2005). In our research, 23 scores, namely BLEU (Papineni et al., 2002) with maximum n-gram lengths of 1, 2, 3, and 4, NIST (NIST, 2002) with maximum n-gram lengths of 1, 2, 3, 4, and 5, GTM (Turian et al., 2003) with exponents of 1.0, 2.0, and 3.0, METEOR (exact) (Banerjee and Lavie, 2005), WER (Niessen et 37 Automatic Estimation of Rate of Accomplishment of Sub-goals The rate of accomplishment of sub-goals is estimated by determining the answer to eac"
N07-1005,C04-1046,0,\N,Missing
P00-1042,W98-1120,0,\N,Missing
P00-1042,M98-1018,0,\N,Missing
P00-1042,W98-1118,0,\N,Missing
P00-1042,W96-0213,0,\N,Missing
P00-1042,M95-1012,0,\N,Missing
P00-1042,E99-1026,1,\N,Missing
P00-1042,J96-1002,0,\N,Missing
P00-1042,J95-4004,0,\N,Missing
P00-1042,M98-1006,0,\N,Missing
P00-1042,M95-1013,0,\N,Missing
P00-1042,A97-1029,0,\N,Missing
P03-1061,J96-1002,0,0.0122742,"a sentence. A string is tagged with a 1 or a 0 to indicate whether it is a morpheme. When a string is a morpheme, a grammatical attribute is assigned to it. A tag designated as a 1 is thus assigned one of a number, n, of grammatical attributes assigned to morphemes, and the problem becomes to assign an attribute (from 0 to n) to every string in a given sentence. We define a model that estimates the likelihood that a given string is a morpheme and has a grammatical attribute i(1 ≤ i ≤ n) as a morpheme model. We implemented this model within an ME modeling framework (Jaynes, 1957; Jaynes, 1979; Berger et al., 1996). The model is represented by Eq. (1): pλ (a|b) = exp  i,j λi,j gi,j (a, b) Zλ (b)  (1) Word 形態 (form) Short word Pronunciation POS Others ケータイ(keitai) Noun Word 形態素解析 (morphological 素 (element) ソ (so) Suffix 解析 (analysis) カイセキ(kaiseki) Noun に ニ (ni) PPP case marker について Verb KA-GYO, ADF, euphonic change PPP conjunctive Prefix Long word Pronunciation POS ケー タ イ ソ カ イ セ(keitaisokaiseki) Noun analysis) キ (about) ニツイテ (nitsuite) オハナシシタシ (ohanashiitasi) Verb つい (relate) ツイ (tsui) て お テ オ (te) (o) 話し (talk) いたし(do) ます ハナシ イタシ マス (hanashi) Verb SA-GYO, ADF (itashi) Verb SA-GYO, ADF (masu) AUX end"
P03-1061,maekawa-etal-2000-spontaneous,1,0.907195,"Missing"
P03-1061,C96-2202,0,0.498087,"tioned in Section 1, tagging the whole of the CSJ manually would be difficult. Therefore, we are taking a semi-automatic approach. This section describes major problems in tagging a large spontaneous speech corpus with high precision in a semiautomatic way, and our solutions to those problems. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus. Two statistical approaches have been applied to this problem. One is to find unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (ME) model (Uchimoto et al., 2001). Their method uses a model that estimates how likely a string is to be a morpheme as its probability, and thus it has a potential to overcome the unknown word problem. Therefore, we use their method for morphological analysis of the CSJ. However, Uchimoto et al. reported that the accuracy of automatic word segmentation and"
P03-1061,P99-1036,0,0.44756,"s section describes major problems in tagging a large spontaneous speech corpus with high precision in a semiautomatic way, and our solutions to those problems. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus. Two statistical approaches have been applied to this problem. One is to find unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (ME) model (Uchimoto et al., 2001). Their method uses a model that estimates how likely a string is to be a morpheme as its probability, and thus it has a potential to overcome the unknown word problem. Therefore, we use their method for morphological analysis of the CSJ. However, Uchimoto et al. reported that the accuracy of automatic word segmentation and POS tagging was 94 points in F-measure (Uchimoto et al., 2002). That is much lower than the accuracy obtained by manual taggin"
P03-1061,W01-0512,1,0.862115,"ns to those problems. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus. Two statistical approaches have been applied to this problem. One is to find unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (ME) model (Uchimoto et al., 2001). Their method uses a model that estimates how likely a string is to be a morpheme as its probability, and thus it has a potential to overcome the unknown word problem. Therefore, we use their method for morphological analysis of the CSJ. However, Uchimoto et al. reported that the accuracy of automatic word segmentation and POS tagging was 94 points in F-measure (Uchimoto et al., 2002). That is much lower than the accuracy obtained by manual tagging. Several problems led to this inaccuracy. In the following, we describe these problems and our solutions to them. • Fillers and disfluencies Fille"
P03-1061,C02-2019,1,0.560951,"that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). Uchimoto et al. used both approaches. They proposed a morphological analysis method based on a maximum entropy (ME) model (Uchimoto et al., 2001). Their method uses a model that estimates how likely a string is to be a morpheme as its probability, and thus it has a potential to overcome the unknown word problem. Therefore, we use their method for morphological analysis of the CSJ. However, Uchimoto et al. reported that the accuracy of automatic word segmentation and POS tagging was 94 points in F-measure (Uchimoto et al., 2002). That is much lower than the accuracy obtained by manual tagging. Several problems led to this inaccuracy. In the following, we describe these problems and our solutions to them. • Fillers and disfluencies Fillers and disfluencies are characteristic expressions often used in spoken language, but they are randomly inserted into text, so detecting their segmentation is difficult. In the CSJ, they are tagged manually. Therefore, we first delete fillers and disfluencies and then put them back in their original place after analyzing a text. • Accuracy for unknown words The morpheme model that will"
P03-1061,W99-0606,0,\N,Missing
P03-1061,A00-2020,0,\N,Missing
P06-2042,N01-1025,0,0.0438954,"ed to have no modifiee. In our experiments, we defined their dependencies as follows. ¯ The rightmost bunsetsu in a quotation or an inserted clause depends on the rightmost one in the sentence. ¯ If a sentence boundary is included in a quotation or an inserted clause, the bunsetsu to the immediate left of the boundary depends on the rightmost bunsetsu in the quotation or the inserted clause. ¯ Other bunsetsus that have no modifiee depend on the next one. 3.2 Detection of Quotations and Inserted Clauses We regard the problem of clause boundary detection as a text chunking task. We used YamCha (Kudo and Matsumoto, 2001) as a text chunker, which is based on Support Vector Machine (SVM). We used the chunk labels consisting of three tags which correspond to sentence boundaries, boundaries of quotations, and boundaries of inserted clauses, respectively. The tag for sentence 326 Table 1: Tag categories used for chunking Tag B E I O S Explanation of tag Beginning of a clause End of a clause Interior of a clause (except B and E) Exterior of a clause Clause consisting of one bunsetsu boundaries can be either E (the rightmost bunsetsu in a sentence) or I (the others). The tags for the boundaries of quotations and ins"
P06-2042,maekawa-etal-2000-spontaneous,1,0.746056,"a sentence is represented by dependency relationships between bunsetsus in the CSJ. For example, the sentence “彼は ゆっくり歩いている” (He is walking slowly) can be divided into three bunsetsus, “彼は, kare-wa” (he), “ゆっくり, yukkuri” (slowly), and “歩いて いる, arui-te-iru” (is walking). In this sentence, the first and second bunsetsus depend on the third one. The dependency structure is described as follows. 彼は─────┐ (he) │ ゆっくり─┤ (slowly) 歩いている (is walking) 1 Introduction The “Spontaneous Speech: Corpus and Processing Technology” project sponsored the construction of the Corpus of Spontaneous Japanese (CSJ) (Maekawa et al., 2000). The CSJ is the biggest spontaneous speech corpus in the world, consisting of roughly 7M words with the total speech length of 700 hours, and is a collection of monologues such as academic presentations and simulated public speeches. The CSJ includes transcriptions of the speeches as well as audio recordings of them. Approximately one tenth of the In this paper, we first describe the problems with dependency structure analysis of spontaneous speech. We focus on ambiguous clause boundaries as the biggest problem and present a solution. 2 Problems with Dependency Structure Analysis in Spontaneo"
P06-2042,2000.iwpt-1.43,1,0.73658,"propose a method for improving dependency structure analysis based on automatic detection of quotations and inserted clauses. 3 Dependency Structure Analysis and Detection of Quotations and Inserted Clauses The outline of the proposed processes is shown in Figure 1. Here, we use “clause” to describe a quotation and an inserted clause. Inserted Clauses In spontaneous speech, speakers insert clauses in the middle of other clauses. This occurs when speakers change their speech plans while produc325 3.1 Dependency Structure Analysis In this research, we use the method proposed by Uchimoto et al. (Uchimoto et al., 2000) to analyze dependency structures. This method is a twostep procedure, and the first step is preparation of a dependency matrix in which each element represents the likelihood that one bunsetsu depends on another. The second step of the analysis is finding an optimal set of dependencies for the entire sentence. The likelihood of dependency is represented by a probability, using a dependency probability model. The model in this study (Uchimoto et al., 2000) takes into account not only the relationship between two bunsetsus but also the relationship between the left bunsetsu and all the bunsetsu"
P06-2042,C04-1159,1,0.824005,"en written text and spontaneous speech, and consequently, problems peculiar to spontaneous speech arise in de324 Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 324–330, c Sydney, July 2006. 2006 Association for Computational Linguistics pendency structure analysis, such as ambiguous clause boundaries, independent bunsetsus, crossed dependencies, self-corrections, and inversions. In this study, we address the problem of ambiguous clause boundaries in dependency structure analysis in spontaneous speech. We treated the other problems in the same way as Shitaoka et al. (Shitaoka et al., 2004). For example, inversions are represented as dependency relationships going in the direction from right to left in the CSJ, and their direction was changed to that from left to right in our experiments. In this paper, therefore, all the dependency relationships were assumed to go in the direction from left to right (Uchimoto et al., 2006). There are several types of clause boundaries such as sentence boundaries, boundaries of quotations and inserted clauses. In the CSJ, clause boundaries were automatically detected by using surface information (Maruyama et al., 2003), and sentence boundaries w"
P07-2052,A00-2018,0,0.248206,"Missing"
P07-2052,N06-1021,0,0.0246711,"Missing"
P07-2052,P05-1067,0,0.0124204,"rd on just phrasal and part-of-speech categories (Bikel, 2004). This paper describes our investigation into the effectiveness of lexicalization in dependency parsing 205 instead of phrase-structure parsing. Usual dependency parsing cannot utilize phrase categories, and thus relies on word information like parts of speech and lexicalized words. Therefore, we want to know the performance of dependency parsers that have minimal or low lexicalization. Dependency trees have been used in a variety of NLP applications, such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). For such applications, a fast, efﬁcient and accurate dependency parser is required to obtain dependency trees from a large corpus. From this point of view, minimally lexicalized parsers have advantages over fully lexicalized ones in parsing speed and memory consumption. We examined the change in performance of dependency parsing by varying the degree of lexicalization. The degree of lexicalization is speciﬁed by giving a list of words to be lexicalized, which appear in a training corpus. For minimal lexicalization, we used a short list that consists of only high-frequency words, and for maxi"
P07-2052,C04-1040,0,0.0395821,"Missing"
P07-2052,P03-1054,0,0.0584778,"Missing"
P07-2052,N01-1025,0,0.156268,"Missing"
P07-2052,P05-1010,0,0.103473,"Missing"
P07-2052,E06-1011,0,0.0326959,"Missing"
P07-2052,P05-1012,0,0.0511429,"Missing"
P07-2052,C04-1010,0,0.0287052,"Missing"
P07-2052,P06-1055,0,0.088056,"Missing"
P07-2052,N03-1027,0,0.0436103,"Missing"
P07-2052,H05-1059,0,0.0238108,"Missing"
P07-2052,W03-3023,0,0.193547,"Missing"
P07-2052,J04-4004,0,\N,Missing
P07-2052,J03-4003,0,\N,Missing
P07-2052,P04-1054,0,\N,Missing
P07-2055,W04-3230,0,0.229669,"thod for Word Segmentation and POS Tagging In this paper, we present a hybrid method for word segmentation and POS tagging. The target languages are those in which word boundaries are ambiguous, such as Chinese and Japanese. In the method, word-based and character-based processing is combined, and word segmentation and POS tagging are conducted simultaneously. Experimental results on multiple corpora show that the integrated method has high accuracy. Many methods have been studied for Chinese and Japanese word segmentation, which include wordbased methods and character-based methods. Nakagawa (2004) studied a method which combines a word-based method and a character-based method. Given an input sentence in the method, a lattice is constructed first using a word dictionary, which consists of word-level nodes for all the known words in the sentence. These nodes have POS tags. Then, character-level nodes for all the characters in the sentence are added into the lattice (Figure 1). These nodes have position-of-character (POC) tags which indicate word-internal positions of the characters (Xue, 2003). There are four POC tags, B, I, E and S, each of which respectively indicates the beginning of"
P07-2055,C04-1067,1,0.94491,"Hybrid Method for Word Segmentation and POS Tagging In this paper, we present a hybrid method for word segmentation and POS tagging. The target languages are those in which word boundaries are ambiguous, such as Chinese and Japanese. In the method, word-based and character-based processing is combined, and word segmentation and POS tagging are conducted simultaneously. Experimental results on multiple corpora show that the integrated method has high accuracy. Many methods have been studied for Chinese and Japanese word segmentation, which include wordbased methods and character-based methods. Nakagawa (2004) studied a method which combines a word-based method and a character-based method. Given an input sentence in the method, a lattice is constructed first using a word dictionary, which consists of word-level nodes for all the known words in the sentence. These nodes have POS tags. Then, character-level nodes for all the characters in the sentence are added into the lattice (Figure 1). These nodes have position-of-character (POC) tags which indicate word-internal positions of the characters (Xue, 2003). There are four POC tags, B, I, E and S, each of which respectively indicates the beginning of"
P07-2055,W04-3236,0,0.501975,"daries and POS tags are used at the same time with this method. The following sections describe the hybrid method and results of experiments on Chinese and Japanese corpora. 217 Proceedings of the ACL 2007 Demo and Poster Sessions, pages 217–220, c Prague, June 2007. 2007 Association for Computational Linguistics Figure 1: Word Segmentation and Known Word POS Tagging using Word and Character-based Processing ging of known words, and information of partsof-speech of unknown words can be used for word segmentation. There are also two other methods capable of conducting unknown word POS tagging (Ng and Low, 2004): Word-based Post-Processing Method This method receives results of word segmentation and known word POS tagging, and predicts POS tags of unknown words using words as units (Figure 2, B). This approach is the same as the approach widely used in English POS tagging. In the method, the process of unknown word POS tagging is separated from word segmentation and known word POS tagging, and information of parts-of-speech of unknown words cannot be used for word segmentation. In later experiments, maximum entropy models were used deterministically to predict POS tags of unknown words. As features f"
P07-2055,O03-4002,0,0.0979197,"Japanese word segmentation, which include wordbased methods and character-based methods. Nakagawa (2004) studied a method which combines a word-based method and a character-based method. Given an input sentence in the method, a lattice is constructed first using a word dictionary, which consists of word-level nodes for all the known words in the sentence. These nodes have POS tags. Then, character-level nodes for all the characters in the sentence are added into the lattice (Figure 1). These nodes have position-of-character (POC) tags which indicate word-internal positions of the characters (Xue, 2003). There are four POC tags, B, I, E and S, each of which respectively indicates the beginning of a word, the middle of a word, the end of a word, and a single character word. In the method, the word-level nodes are used to identify known words, and the character-level nodes are used to identify unknown words, because generally wordlevel information is precise and appropriate for processing known words, and character-level information is robust and appropriate for processing unknown words. Extended hidden Markov models are used to choose the best path among all the possible candidates in the lat"
P09-1049,P07-1000,0,0.131789,"ived from (T IGER , S IBERIAN TIGER) in Figure 4. English and Japanese terms are used for building bilingual instance dictionary DBI for hyponymyrelation acquisition, where DBI is composed of translation pairs between English and Japanese hyponymy-relation candidates5 . Wikipedia infobox, a special kind of template, that describes a tabular summary of an article subject expressed by attribute-value pairs. An attribute type coupled with the infobox name to which it belongs provides the semantic properties of its value that enable us to easily understand what the attribute value means (Auer and Lehmann, 2007; Wu and Weld, 2007). For example, infobox template City Japan in Wikipedia article Kyoto contains several attribute-value pairs such as “Mayor=Daisaku Kadokawa” as attribute=its value. What Daisaku Kadokawa, the attribute value of mayor in the example, represents is hard to understand alone if we lack knowledge, but its attribute type, mayor, gives a clue–Daisaku Kadokawa is a mayor related to Kyoto. These semantic properties enable us to discover semantic evidence for hyponymy relations. We extract triples (infobox name, attribute type, attribute value) from the Wikipedia infoboxes and encod"
P09-1049,I08-2126,1,0.731186,"Missing"
P09-1049,sumida-etal-2008-boosting,1,0.924703,"roblems, especially when the student also has a certain level of confidence in his opinion on a class label but disagrees with the teacher: rT &gt; θ and clS = clT . In that case, the teacher does nothing Wikipedia Articles in J Hyponymy-relation candidate extraction Translation dictionary Candidates in J Bilingual instance dictionary Classifier in E Labeled instances Unlabeled instances in E Newly labeled instances for E Unlabeled instances in J Newly labeled instances for J Classifier in J Labeled instances Bilingual Co-Training Figure 3: System architecture 3.1 Candidate Extraction We follow Sumida et al. (2008) to extract hyponymy-relation candidates from English and Japanese Wikipedia. A layout structure is chosen 434 used in Sumida et al. (2008) but LF1 –LF5 and SF1 –SF2 are the same as their feature set. Let us provide an overview of the feature sets used in Sumida et al. (2008). See Sumida et al. (2008) for more details. Lexical features LF1 –LF5 are used to recognize the lexical evidence encoded in hyper and hypo for hyponymy relations. For example, (hyper,hypo) is often a proper hyponymy relation if hyper and hypo share the same head morpheme or word. In LF1 and LF2 , such information is provi"
P09-1049,S07-1003,0,0.059436,"Missing"
P09-1049,D07-1073,1,0.830981,"in bilingual co-training cooperate in doing the same type of tasks. Bilingual resources have been used for monolingual tasks including verb classification and noun phrase semantic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods for the classification of semantic relations between simple nominals in English sentences. However, the previous work seldom considered the bilingual aspect of semantic relations in the acquisition of monolingual semantic relations. 6 Conclusion We proposed a bilingual co-training approach and applied it to hyponymy-relation acquisition from Wikipedia. Experiments showed that bilingual co-training is effectiv"
P09-1049,P02-1044,0,0.0476847,"Missing"
P09-1049,P02-1027,0,0.0325283,"ote that F1 of INIT in Table 2 was 72.2 in English and 76.6 in Japanese.) 5 bilingual bootstrapping. However, the two classifiers in bilingual bootstrapping were for a bilingual task but did different tasks from the monolingual viewpoint. A classifier in each language is for word sense disambiguation, where a class label (or word sense) is different based on the languages. On the contrary, classifiers in bilingual co-training cooperate in doing the same type of tasks. Bilingual resources have been used for monolingual tasks including verb classification and noun phrase semantic interpolation (Merlo et al., 2002; Girju, 2006). However, unlike ours, their focus was limited to bilingual features for one monolingual classifier based on supervised learning. Recently, there has been increased interest in semantic relation acquisition from corpora. Some regarded Wikipedia as the corpora and applied hand-crafted or machine-learned rules to acquire semantic relations (Herbelot and Copestake, 2006; Kazama and Torisawa, 2007; Ruiz-casado et al., 2005; Nastase and Strube, 2008; Sumida et al., 2008; Suchanek et al., 2007). Several researchers who participated in SemEval-07 (Girju et al., 2007) proposed methods f"
P09-1049,J04-1001,0,\N,Missing
P09-1058,J96-2001,0,0.0305502,"of unknown words (with characterlevel nodes) as well as those of known words (with word-level nodes). We can directly estimate the statistics of known words from an annotated corpus where a sentence is already segmented into words and assigned POS tags. If we select the correct path yt that corresponds to the annotated sentence, it will only consist of word-level nodes that do not allow learning for unknown words. We therefore need to choose character-level nodes as correct nodes instead of word-level nodes for some words. We expect that those words could reflect unknown words in the future. Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words. Their idea has proven effective for estimating the statistics of unknown words in previous studies (Ratnaparkhi, 1996; Nagata, 1999; Nakagawa, 2004). We adopt Baayen and Sproat’s approach as the baseline policy in our word-character hybrid model. In the baseline policy, we first count the frequencies of words3 in the training corpus. We then collect infrequent words that appear less than or equal to r times.4 If these infrequent words are in the correct path, we use character-level node"
P09-1058,J95-4004,0,0.503119,"Missing"
P09-1058,P99-1036,0,0.028539,"s. If we select the correct path yt that corresponds to the annotated sentence, it will only consist of word-level nodes that do not allow learning for unknown words. We therefore need to choose character-level nodes as correct nodes instead of word-level nodes for some words. We expect that those words could reflect unknown words in the future. Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words. Their idea has proven effective for estimating the statistics of unknown words in previous studies (Ratnaparkhi, 1996; Nagata, 1999; Nakagawa, 2004). We adopt Baayen and Sproat’s approach as the baseline policy in our word-character hybrid model. In the baseline policy, we first count the frequencies of words3 in the training corpus. We then collect infrequent words that appear less than or equal to r times.4 If these infrequent words are in the correct path, we use character-level nodes to represent them, and hence the characteristics of unknown words can be learned. For example, in Figure 1 we select the character-level nodes of the word “ ” (Chongming) as the correct nodes. As a result, the correct path yt can contain"
P09-1058,W02-1001,0,0.0750953,"lgorithm has two main search steps: forward and backward. For the forward search, we use Viterbi-style decoding to find the best partial path and its score up to each node in the lattice. For the backward search, we use A∗ style decoding to generate the top k-best paths. A complete path is found when the backward search reaches the beginning node of the lattice, and the algorithm terminates when the number of generated paths equals k. In summary, we use k-best MIRA to iteratively update w(i) . The final weight vector w is the average of the weight vectors after each iteration. As reported in (Collins, 2002; McDonald et al., 2005), parameter averaging can effectively avoid overfitting. For inference, we can use Viterbi-style decoding to search for the most likely path y∗ for a given sentence x where: Input: Training set S = {(xt , yt )}Tt=1 Output: Model weight vector w 1: w(0) = 0; v = 0; i = 0 2: for iter = 1 to N do 3: for t = 1 to T do 4: w(i+1) = update w(i) according to (xt , yt ) 5: v = v + w(i+1) 6: i=i+1 7: end for 8: end for 9: w = v/(N × T ) within a few iterations (McDonald, 2006). Algorithm 1 outlines the generic online learning algorithm (McDonald, 2006) used in our framework. 4.2"
P09-1058,P07-2055,1,0.292461,"taka Uchimoto‡ and Jun’ichi Kazama‡ Yiou Wang‡ and Kentaro Torisawa‡ and Hitoshi Isahara†‡ † Graduate School of Engineering, Kobe University 1-1 Rokkodai-cho, Nada-ku, Kobe 657-8501 Japan ‡ National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan {canasai,uchimoto,kazama,wangyiou,torisawa,isahara}@nict.go.jp tem’s word dictionary1 . The word boundaries and the POS tags of unknown words, which are very difficult to identify, cause numerous errors. The word-character hybrid model proposed by Nakagawa and Uchimoto (Nakagawa, 2004; Nakagawa and Uchimoto, 2007) shows promising properties for solving this problem. However, it suffers from structural complexity. Nakagawa (2004) described a training method based on a word-based Markov model and a character-based maximum entropy model that can be completed in a reasonable time. However, this training method is limited by the generatively-trained Markov model in which informative features are hard to exploit. In this paper, we overcome such limitations concerning both efficiency and effectiveness. We propose a new framework for training the wordcharacter hybrid model based on the Margin Infused Relaxed A"
P09-1058,C04-1067,0,0.417122,"gkrai†‡ and Kiyotaka Uchimoto‡ and Jun’ichi Kazama‡ Yiou Wang‡ and Kentaro Torisawa‡ and Hitoshi Isahara†‡ † Graduate School of Engineering, Kobe University 1-1 Rokkodai-cho, Nada-ku, Kobe 657-8501 Japan ‡ National Institute of Information and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto 619-0289 Japan {canasai,uchimoto,kazama,wangyiou,torisawa,isahara}@nict.go.jp tem’s word dictionary1 . The word boundaries and the POS tags of unknown words, which are very difficult to identify, cause numerous errors. The word-character hybrid model proposed by Nakagawa and Uchimoto (Nakagawa, 2004; Nakagawa and Uchimoto, 2007) shows promising properties for solving this problem. However, it suffers from structural complexity. Nakagawa (2004) described a training method based on a word-based Markov model and a character-based maximum entropy model that can be completed in a reasonable time. However, this training method is limited by the generatively-trained Markov model in which informative features are hard to exploit. In this paper, we overcome such limitations concerning both efficiency and effectiveness. We propose a new framework for training the wordcharacter hybrid model based o"
P09-1058,W04-3236,0,0.900606,"our approach on the Penn Chinese Treebank, and show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature. 1 Introduction In Chinese, word segmentation and part-of-speech (POS) tagging are indispensable steps for higherlevel NLP tasks. Word segmentation and POS tagging results are required as inputs to other NLP tasks, such as phrase chunking, dependency parsing, and machine translation. Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion (Ng and Low, 2004; Nakagawa and Uchimoto, 2007; Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). In joint word segmentation and the POS tagging process, one serious problem is caused by unknown words, which are defined as words that are not found in a training corpus or in a sys1 A system’s word dictionary usually consists of a word list, and each word in the list has its own POS category. In this paper, we constructed the system’s word dictionary from a training corpus. 513 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513–521, c Suntec, Singapore, 2"
P09-1058,W96-0213,0,0.439508,"nd assigned POS tags. If we select the correct path yt that corresponds to the annotated sentence, it will only consist of word-level nodes that do not allow learning for unknown words. We therefore need to choose character-level nodes as correct nodes instead of word-level nodes for some words. We expect that those words could reflect unknown words in the future. Baayen and Sproat (1996) proposed that the characteristics of infrequent words in a training corpus resemble those of unknown words. Their idea has proven effective for estimating the statistics of unknown words in previous studies (Ratnaparkhi, 1996; Nagata, 1999; Nakagawa, 2004). We adopt Baayen and Sproat’s approach as the baseline policy in our word-character hybrid model. In the baseline policy, we first count the frequencies of words3 in the training corpus. We then collect infrequent words that appear less than or equal to r times.4 If these infrequent words are in the correct path, we use character-level nodes to represent them, and hence the characteristics of unknown words can be learned. For example, in Figure 1 we select the character-level nodes of the word “ ” (Chongming) as the correct nodes. As a result, the correct path y"
P09-1058,W03-1719,0,0.0612588,"nese Treebank (CTB) (Xia et al., 2000) in experiments. However, versions of CTB and experimental settings vary across different studies. In this paper, we used CTB 5.0 (LDC2005T01) as our main corpus, defined the training, development and test sets according to (Jiang et al., 2008a; Jiang et al., 2008b), and designed our experiments to explore the impact of the training corpus size on our approach. Table 5 provides the statistics of our experimental settings on the small and large training data. The out-of-vocabulary (OOV) is defined as tokens in the test set that are not in the training set (Sproat and Emerson, 2003). Note that the development set was only used for evaluating the trained model to obtain the optimal values of tunable parameters. 5.4 Impact of policies for correct path selection Table 6 shows the results of our word-character hybrid model using the error-driven and baseline policies. The third and fourth columns indicate the numbers of known and artificial unknown words in the training phase. The total number of words is the same, but the different policies yield different balances between the known and artificial unknown words for learning the hybrid model. Optimal balances were selected u"
P09-1058,P08-1102,0,0.671136,"Missing"
P09-1058,W01-0512,1,0.806822,"cond is a discriminative online learning algorithm based on MIRA that enables us to incorporate arbitrary features to our hybrid model. Based on extensive comparisons, we showed that our approach is superior to the existing approaches reported in the literature. In future work, we plan to apply our framework to other Asian languages, including Thai and Japanese. 7 Related work In this section, we discuss related approaches based on several aspects of learning algorithms and search space representation methods. Maximum entropy models are widely used for word segmentation and POS tagging tasks (Uchimoto et al., 2001; Ng and Low, 2004; Nakagawa, 2004; Nakagawa and Uchimoto, 2007) since they only need moderate training times while they provide reasonable performance. Conditional random fields (CRFs) (Lafferty et al., 2001) further improve the performance (Kudo et al., 2004; Shi and Wang, 2007) by performing whole-sequence normalization to avoid label-bias and length-bias problems. However, CRF-based algorithms typically require longer training times, and we observed an infeasible convergence time for our hybrid model. Online learning has recently gained popularity for many NLP tasks since it performs compa"
P09-1058,C08-1049,0,0.775572,"uperior performance compared to the state-ofthe-art approaches reported in the literature. 1 Introduction In Chinese, word segmentation and part-of-speech (POS) tagging are indispensable steps for higherlevel NLP tasks. Word segmentation and POS tagging results are required as inputs to other NLP tasks, such as phrase chunking, dependency parsing, and machine translation. Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion (Ng and Low, 2004; Nakagawa and Uchimoto, 2007; Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). In joint word segmentation and the POS tagging process, one serious problem is caused by unknown words, which are defined as words that are not found in a training corpus or in a sys1 A system’s word dictionary usually consists of a word list, and each word in the list has its own POS category. In this paper, we constructed the system’s word dictionary from a training corpus. 513 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513–521, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Figure 1: Lattice used in word-charac"
P09-1058,W04-3230,0,0.04645,"n future work, we plan to apply our framework to other Asian languages, including Thai and Japanese. 7 Related work In this section, we discuss related approaches based on several aspects of learning algorithms and search space representation methods. Maximum entropy models are widely used for word segmentation and POS tagging tasks (Uchimoto et al., 2001; Ng and Low, 2004; Nakagawa, 2004; Nakagawa and Uchimoto, 2007) since they only need moderate training times while they provide reasonable performance. Conditional random fields (CRFs) (Lafferty et al., 2001) further improve the performance (Kudo et al., 2004; Shi and Wang, 2007) by performing whole-sequence normalization to avoid label-bias and length-bias problems. However, CRF-based algorithms typically require longer training times, and we observed an infeasible convergence time for our hybrid model. Online learning has recently gained popularity for many NLP tasks since it performs comparably or better than batch learning using shorter training times (McDonald, 2006). For example, a perceptron algorithm is used for joint Chinese word segmentation and POS tagging (Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). Another potent"
P09-1058,xia-etal-2000-developing,0,0.388731,"2005; McDonald, 2006). We describe k-best decoding for our hybrid model and design its loss function and the features appropriate for our task. In our word-character hybrid model, allowing the model to learn the characteristics of both known and unknown words is crucial to achieve optimal performance. Here, we describe our strategies that yield good balance for learning these two characteristics. We propose an errordriven policy that delivers this balance by acquiring examples of unknown words from particular errors in a training corpus. We conducted our experiments on Penn Chinese Treebank (Xia et al., 2000) and compared our approach with the best previous approaches reported in the literature. Experimental results indicate that our approach can achieve state-of-the-art performance. Abstract In this paper, we present a discriminative word-character hybrid model for joint Chinese word segmentation and POS tagging. Our word-character hybrid model offers high performance since it can handle both known and unknown words. We describe our strategies that yield good balance for learning the characteristics of known and unknown words and propose an errordriven policy that delivers such balance by acquiri"
P09-1058,P08-1101,0,0.620356,"show that it achieves superior performance compared to the state-ofthe-art approaches reported in the literature. 1 Introduction In Chinese, word segmentation and part-of-speech (POS) tagging are indispensable steps for higherlevel NLP tasks. Word segmentation and POS tagging results are required as inputs to other NLP tasks, such as phrase chunking, dependency parsing, and machine translation. Word segmentation and POS tagging in a joint process have received much attention in recent research and have shown improvements over a pipelined fashion (Ng and Low, 2004; Nakagawa and Uchimoto, 2007; Zhang and Clark, 2008; Jiang et al., 2008a; Jiang et al., 2008b). In joint word segmentation and the POS tagging process, one serious problem is caused by unknown words, which are defined as words that are not found in a training corpus or in a sys1 A system’s word dictionary usually consists of a word list, and each word in the list has its own POS category. In this paper, we constructed the system’s word dictionary from a training corpus. 513 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 513–521, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Figure 1: Lattice"
P09-1058,H05-1066,0,0.00424798,"o main search steps: forward and backward. For the forward search, we use Viterbi-style decoding to find the best partial path and its score up to each node in the lattice. For the backward search, we use A∗ style decoding to generate the top k-best paths. A complete path is found when the backward search reaches the beginning node of the lattice, and the algorithm terminates when the number of generated paths equals k. In summary, we use k-best MIRA to iteratively update w(i) . The final weight vector w is the average of the weight vectors after each iteration. As reported in (Collins, 2002; McDonald et al., 2005), parameter averaging can effectively avoid overfitting. For inference, we can use Viterbi-style decoding to search for the most likely path y∗ for a given sentence x where: Input: Training set S = {(xt , yt )}Tt=1 Output: Model weight vector w 1: w(0) = 0; v = 0; i = 0 2: for iter = 1 to N do 3: for t = 1 to T do 4: w(i+1) = update w(i) according to (xt , yt ) 5: v = v + w(i+1) 6: i=i+1 7: end for 8: end for 9: w = v/(N × T ) within a few iterations (McDonald, 2006). Algorithm 1 outlines the generic online learning algorithm (McDonald, 2006) used in our framework. 4.2 k-best MIRA We focus on"
P09-1058,C94-1032,0,0.27228,"hybrid model since it quickly converges 3 We consider a word and its POS tag a single entry. In our experiments, the optimal threshold value r is selected by evaluating the performance of joint word segmentation and POS tagging on the development set. 4 515 Algorithm 1 Generic Online Learning Algorithm above quadratic programming (QP) problem can be solved using Hildreth’s algorithm (Yair Censor, 1997). Replacing Eq. (2) into line 4 of Algorithm 1, we obtain k-best MIRA. The next question is how to efficiently generate bestk (xt ; w(i) ). In this paper, we apply a dynamic programming search (Nagata, 1994) to kbest MIRA. The algorithm has two main search steps: forward and backward. For the forward search, we use Viterbi-style decoding to find the best partial path and its score up to each node in the lattice. For the backward search, we use A∗ style decoding to generate the top k-best paths. A complete path is found when the backward search reaches the beginning node of the lattice, and the algorithm terminates when the number of generated paths equals k. In summary, we use k-best MIRA to iteratively update w(i) . The final weight vector w is the average of the weight vectors after each iterat"
P09-1058,W03-1726,0,\N,Missing
S01-1033,W00-0730,0,0.0142784,"kernel functions. We have used the following polynomial function exclusively. K(x,y) =(x·y+l)d (9) C and d are constants set by experimentation. For all of the experiments reported in this paper, C was fixed as 1 and d was fixed as 2. A set of Xi that satisfies O:i &gt; 0 is called a support vector (SVs) 4 . The summation portion of Equation (4) was calculated using only the examples that were support vectors. Support vector machine methods are capable of handling data consisting of two categories. In general, data consisting of more than two categories is handled by using the pair-wise method (Kudoh and Matsumoto, 2000). In this method, for data consisting of N categories, pairs of two different categories (N (N1)/2 pairs) are constructed. The better cate1rn Figure 1, the circles in the broken lines indicate support vectors. gory is determined by using a 2-category classifier (in this paper, a support vector machine 5 was used as the 2-category classifier), and the correct category is finally determined by ""voting"" on the N(N-1)/2 pairs that result from analysis using the 2-category classifier. The support vector machine method is, in fact, performed by combining the support vector machine and pair-wise meth"
S01-1038,W01-1415,1,0.879344,"ources of information related to the input sentence and examples. Since we want to avoid making complicated rules, we use machine learning models to calculate the similarity. Instead of all examples in the TM, English headwords are used as classes in machine learning models. Therefore, examples having the same English headword are put into the same class and are considered to have the same similarity. 1 A description on how to use ""diff"" can be found in (Murata and Isahara, 2001). 2 Work on using machine learning methods for the tra!lslation of tenses, aspects, and modalities can be found'in {Murata et al., 2001a). 156 Classes identified by machine learning models are basically English headwords in TM, and they are detected manually. For example, English headwords of the examples in Figure 1 are ""feel constrained"", ""constraint"", and ""refrain"", respectively. When English headwords are verbs, they are represented by their basic forms. English words obtained when a Japanese headword is looked up in a Japanese-English dictionary are also used as classes. For the training data, we use not only examples in the TM but also other data collected from bilingual dictionaries or a parallel corpus. The collected"
tohyama-etal-2008-construction-metadata,kozawa-etal-2008-automatic,1,\N,Missing
uchimoto-den-2008-word,P05-1012,0,\N,Missing
uchimoto-den-2008-word,C04-1010,0,\N,Missing
uchimoto-den-2008-word,uchimoto-etal-2006-dependency,1,\N,Missing
uchimoto-den-2008-word,maekawa-etal-2000-spontaneous,0,\N,Missing
uchimoto-etal-2006-automatic,P02-1028,0,\N,Missing
uchimoto-etal-2006-automatic,2005.mtsummit-papers.31,1,\N,Missing
uchimoto-etal-2006-dependency,E99-1026,1,\N,Missing
uchimoto-etal-2006-dependency,C02-1136,0,\N,Missing
uchimoto-etal-2006-dependency,P98-1083,0,\N,Missing
uchimoto-etal-2006-dependency,C98-1080,0,\N,Missing
uchimoto-etal-2006-dependency,C04-1159,1,\N,Missing
uchimoto-etal-2006-dependency,W98-1511,0,\N,Missing
uchimoto-etal-2006-dependency,maekawa-etal-2000-spontaneous,1,\N,Missing
uchimoto-etal-2006-dependency,W00-1303,0,\N,Missing
W01-0512,J96-1002,0,0.030443,") : verb (1) ( ) = > & =1 : 0 : otherwise h; x g h; f ; 00 x ; f : Here has(h,x)&quot; is a binary function that returns true if the history h has feature x. In our experiments, we focused on such information as whether or not a string is found in a dictionary, the length of the string, what types of characters are used in the string, and the part-of-speech of the adjacent morpheme. Given a set of features and some training data, the M.E. estimation process produces a model in which every feature gi has an associated parameter i . This enables us to compute the conditional probability as follows (Berger et al., 1996): P (f jh) = Z (h) = Q g (h;f ) i i i Z (h) gi i (h;f ) : f i XY (2) (3) The M.E. estimation process guarantees that for every feature gi , the expected value of gi according to the M.E. model will equal the empirical expectation of gi in the training corpus. In other words, X~ h;f P (h; f ) 1 gi (h; f ) = X~ h P (h) 1 X f PM:E: (f jh) 1 gi (h; f ): (4) Here P~ is an empirical probability and PM:E: is the probability assigned by the model. We de ne part-of-speech and bunsetsu boundaries as grammatical attributes. Here a bunsetsu is a phrasal unit consisting of one or more morphemes. When the"
W01-0512,J95-4004,0,0.149518,"Missing"
W01-0512,A92-1018,0,0.0221582,"Missing"
W01-0512,P97-1030,0,0.0242452,"Missing"
W01-0512,P97-1031,0,0.0336342,"Missing"
W01-0512,C96-2202,0,0.211023,"Missing"
W01-0512,C94-1032,0,0.0153311,"pheme and has the grammatical attribute i(1  i  n). We call it a morpheme model. This model is represented by Eq. (2), in which f can be one of (n + 1) tags from 0 to n. A given sentence is divided into morphemes, and a grammatical attribute is assigned to each morpheme so as to maximize the sentence probability estimated by our morpheme model. Sentence probability is de ned as the product of the probabilities estimated for a particular division of morphemes in a sentence. We use the Viterbi algorithm to nd the optimal set of morphemes in a sentence and we use the method proposed by Nagata (Nagata, 1994) to search for the Nbest sets. 3 Experiments and Discussion 3.1 Experimental Conditions The part-of-speech categories that we used follow those of JUMAN (Kurohashi and Nagao, 1999). There are 53 categories covering all possible combinations of major and minor categories as de ned in JUMAN. The number of grammatical attributes is 106 if we include the detection of whether or not the left side of a morpheme is a bunsetsu boundary. We do not identify in ection types probabilistically since 1 Not only morphemes but also bunsetsus can be identi ed by considering the information related to their bun"
W01-0512,P99-1036,0,0.582214,"menting a given sentence into a row of morphemes and assigning to each morpheme grammatical attributes such as a partof-speech (POS) and an in ection type. One of the most important problems in morphological analysis is that posed by unknown words, which are words found in neither a dictionary nor a training corpus, and there have been two statistical approaches to this problem. One is to acquire unknown words from corpora and put them into a dictionary (e.g., (Mori and Nagao, 1996)), and the other is to estimate a model that can identify unknown words correctly (e.g., (Kashioka et al., 1997; Nagata, 1999)). We would like to be able to make good use of both approaches. If words acquired by the former method could be added to a dictionary and a model developed by the latter method could consult the amended dictionary, then the model could be the best statistical model which has the potential to overcome the unknown word problem. Mori and Nagao proposed a statistical model that can consult a dictionary (Mori and Nagao, 1998). In their model the probability that a string of letters or characters is y Hitoshi Isahara sekine@cs.nyu.edu a morpheme is augmented when the string is found in a dictionary"
W01-0512,W96-0213,0,0.0502922,"Missing"
W01-0512,C94-1027,0,0.0244018,"Missing"
W01-0512,P94-1025,0,0.048906,"Missing"
W01-0512,E99-1026,1,0.87637,"Missing"
W01-0512,P98-1081,0,0.0207993,"Missing"
W01-1415,1999.tmi-1.7,1,\N,Missing
W01-1415,A97-1015,0,\N,Missing
W01-1415,C00-1082,1,\N,Missing
W01-1415,W00-0730,0,\N,Missing
W02-1036,W98-1118,0,0.0604978,"Missing"
W02-1036,P98-1029,0,0.137114,"Missing"
W02-1036,W99-0623,0,0.0380812,"Missing"
W02-1036,A00-2005,0,0.0321499,"Missing"
W02-1036,C00-2102,1,0.899727,"Missing"
W02-1036,W98-1120,0,0.0805802,"Missing"
W02-1036,A00-2007,0,0.0360044,"Missing"
W02-1036,P00-1042,1,0.906528,"Missing"
W02-1036,P98-1081,0,0.0733585,"Missing"
W02-1036,P94-1013,0,0.124199,"Missing"
W02-1036,C98-1078,0,\N,Missing
W02-1036,C98-1029,0,\N,Missing
W04-2208,C00-1007,0,0.0337696,"h expressions corresponding to a Japanese expression is 1.3 as shown in Table 2. Even when there are two or more possible English expressions, an appropriate English expression can be chosen by selecting a Japanese expression by referring to dependencies in extracted translation pairs. Therefore, in many cases, English sentences can be generated just by reordering the selected expressions. The English word order was estimated manually in this experiment. However, we can automatically estimate English word order by using a language model or an English surface sentence generator such as FERGUS (Bangalore and Rambow, 2000). Unnatural or ungrammatical parallel translations are sometimes generated in the above steps. However, comprehensible translations can be generated as shown in Figure 4. The biggest advantage of this framework is that comprehensible target sentences can be generated basically by referring only to source sentences. Although it is costly to search and select appropriate translation pairs, we believe that human labor can be reduced by developing a human interface. For example, when we use a Japanese text generation system from keywords (Uchimoto et al., 2002), users should only select appropriat"
W04-2208,P01-1030,0,0.0114299,"o a given sentence can be semiautomatically generated. In this paper we show that the framework can be achieved by using our aligned parallel treebank corpus. 1 ‡ New York University 715 Broadway, 7th floor 3-5 Hikari-dai, Seika-cho, Soraku-gun, New York, NY 10003, USA Kyoto 619-0289, Japan {sudo,sekine}@cs.nyu.edu {uchimoto,yujie,murata,isahara}@nict.go.jp Abstract pora and do not have bilingual or multilinNational Institute of Information and Communications Technology Introduction Recently, accurate machine translation systems can be constructed by using parallel corpora (Och and Ney, 2000; Germann et al., 2001). However, almost all existing machine translation systems do not consider the problem of translating a given sentence into a natural sentence reﬂecting its contextual information in the target language. One of the main reasons for this is that we had many problems that had to be solved by one-sentence to one-sentence machine translation before we could solve the contextual problem. Another reason is that it was diﬃcult to simply investigate the inﬂuence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-to-one, and are usually"
W04-2208,2002.tmi-papers.9,0,0.015857,"and Japanese-English machine translation. We can directly compare various methods of machine translation by using this corpus. It can be summarized as follows in terms of the characteristics of the corpus. One-sentence to one-sentence translation can be simply used for the evaluation of various methods of machine translation. Morphological and syntactic information can be used for the evaluation of methods that actively use morphological and syntactic information, such as methods for examplebased machine translation (Nagao, 1981; Watanabe et al., 2003), or transfer-based machine translation (Imamura, 2002). Phrasal alignment is used for the evaluation of automatically acquired translation knowledge (Yamamoto and Matsumoto, 2003). An actual comparison and evaluation is our future work. 3.2 Analysis of Translation One-sentence to one-sentence translation reﬂects contextual information. Therefore, it is suitable to investigate the inﬂuence of the context on the translation. For example, we can investigate the diﬀerence in the use of demonstratives and pronouns between English and Japanese. We can also investigate the diﬀerence in the use of anaphora. Morphological and syntactic information and phr"
W04-2208,J93-2004,0,0.0235999,"ng a given sentence into a natural sentence reﬂecting its contextual information in the target language. One of the main reasons for this is that we had many problems that had to be solved by one-sentence to one-sentence machine translation before we could solve the contextual problem. Another reason is that it was diﬃcult to simply investigate the inﬂuence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-to-one, and are usually one-to-many or many-to-many. On the other hand, high-quality treebanks such as the Penn Treebank (Marcus et al., 1993) and the Kyoto University text corpus (Kurohashi and Nagao, 1997) have contributed to improving the accuracies of fundamental techniques for natural language processing such as morphological analysis and syntactic structure analysis. However, almost all of these highquality treebanks are based on monolingual corgual information. There are few high-quality bilingual or multilingual treebank corpora because parallel corpora have mainly been actively used for machine translation between related languages such as English and French, therefore their syntactic structures are not required so much for"
W04-2208,P00-1056,0,0.126608,"ntence is similar to a given sentence can be semiautomatically generated. In this paper we show that the framework can be achieved by using our aligned parallel treebank corpus. 1 ‡ New York University 715 Broadway, 7th floor 3-5 Hikari-dai, Seika-cho, Soraku-gun, New York, NY 10003, USA Kyoto 619-0289, Japan {sudo,sekine}@cs.nyu.edu {uchimoto,yujie,murata,isahara}@nict.go.jp Abstract pora and do not have bilingual or multilinNational Institute of Information and Communications Technology Introduction Recently, accurate machine translation systems can be constructed by using parallel corpora (Och and Ney, 2000; Germann et al., 2001). However, almost all existing machine translation systems do not consider the problem of translating a given sentence into a natural sentence reﬂecting its contextual information in the target language. One of the main reasons for this is that we had many problems that had to be solved by one-sentence to one-sentence machine translation before we could solve the contextual problem. Another reason is that it was diﬃcult to simply investigate the inﬂuence of the context on the translation because sentence correspondences of the existing bilingual documents are rarely one-"
W04-2208,C02-1064,1,0.816552,"tence generator such as FERGUS (Bangalore and Rambow, 2000). Unnatural or ungrammatical parallel translations are sometimes generated in the above steps. However, comprehensible translations can be generated as shown in Figure 4. The biggest advantage of this framework is that comprehensible target sentences can be generated basically by referring only to source sentences. Although it is costly to search and select appropriate translation pairs, we believe that human labor can be reduced by developing a human interface. For example, when we use a Japanese text generation system from keywords (Uchimoto et al., 2002), users should only select appropriate keywords. We are investigating whether or not we can generate similar parallel translations to all of the Japanese sentences appearing on January 17, 1995. So far, we found that we can generate similar parallel translations to 691 out of 840 sentences (the average number of bunsetsus is about 10.3) including the 102 sentences described in Section 3.3. We found that we could not generate similar parallel translations to 149 out of 840 sentences. In the proposed framework of similar parallel translation generation, the language appearing in a corpus corresp"
W04-2208,P01-1067,0,0.102382,"erefore their syntactic structures are not required so much for aligning words or phrases. However, syntactic structures are necessary for machine translation between languages whose syntactic structures are diﬀerent from each other, such as in Japanese-English, Japanese-Chinese, and Chinese-English machine translations, because it is more diﬃcult to automatically align words or phrases between two unrelated languages than between two related languages. Actually, it has been reported that syntactic structures contribute to improving the accuracy of word alignment between Japanese and English (Yamada and Knight, 2001). Therefore, if we had a high-quality parallel treebank corpus, the accuracies of machine translation between languages whose syntactic structures are diﬀerent from each other would improve. Furthermore, if the parallel treebank corpus had word or phrase alignment, the accuracy of automatic word or phrase alignment would increase by using the parallel treebank corpus as training data. However, so far, there is no aligned parallel treebank corpus whose domain is not restricted. For example, the Japanese Electronics Industry Development Association’s (JEIDA’s) bilingual corpus (Isahara and Harun"
W04-2208,A00-2018,0,\N,Missing
W06-2404,N01-1025,0,0.0932704,"Missing"
W06-2404,W03-0429,0,0.0467726,"Missing"
W06-2404,W99-0502,0,0.0173432,"Missing"
W06-2404,W04-0405,0,0.196948,"Missing"
W06-2404,E99-1023,0,0.0382977,"Missing"
W06-2404,J96-2004,0,\N,Missing
W09-1209,burchardt-etal-2006-salsa,0,0.0198371,"Missing"
W09-1209,I08-1012,1,0.714872,"ods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the l"
W09-1209,D07-1097,0,0.04916,"Missing"
W09-1209,kawahara-etal-2002-construction,0,0.011934,"hnology (NICT) and City University of Hong Kong (CityU) for the joint learning task of CoNLL-2009 shared task (Hajiˇc et al., 2009)1 . The system is basically a pipeline of syntactic parser and semantic parser. We use a syntactic parser that uses very rich features and integrates graph- and transition-based methods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their metho"
W09-1209,P08-1068,0,0.10612,"c parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT i"
W09-1209,E06-1011,0,0.013162,"rser and semantic parser. We use a syntactic parser that uses very rich features and integrates graph- and transition-based methods. As for the semantic parser, a group of well selected feature templates are used with n-best syntactic features. 1 Our thanks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use"
W09-1209,W06-2932,0,0.0235357,"atures for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT information (Hajiˇc et al., 2009). FEAT is a set of morphological-features, e.g. more detailed part of speech, number, gender, etc. We try to align different types of morphological-features. For example, 2 http://mstparser.sourceforge.net Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 61–66, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics we can obtain a sequence of gender tags of all words from a head h to its d"
W09-1209,P08-1108,0,0.0780057,"ks give to the following corpus providers, (Taul´e et al., 2008; Palmer and Xue, 2009; Hajiˇc et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006) and (Kawahara et al., 2002). 61 Basically, we build our syntactic dependency parsers based on the MSTParser, a freely available implementation2 , whose details are presented in the paper of McDonald and Pereira (2006). Moreover, we exploit rich features for the parsers. We represent features by following the work of Chen et al. (2008) and Koo et al. (2008) and use features based on dependency relations predicted by transition-based parsers (Nivre and McDonald, 2008). Chen et al. (2008) and Koo et al. (2008) proposed the methods to obtain new features from large-scale unlabeled data. In our system, we perform their methods on training data because the closed challenge does not allow to use unlabeled data. In this paper, we call these new additional features rich features. 2.1 Basic Features Firstly, we use all the features presented by McDonald et al. (2006), if they are available in data. Then we add new features for the languages having FEAT information (Hajiˇc et al., 2009). FEAT is a set of morphological-features, e.g. more detailed part of speech, nu"
W09-1209,W03-3017,0,0.0110636,"graph-based and transition-based parsers. Here, we represent features based on dependency relations predicted by transition-based parsers for graphbased parser. Based on the results on development data, we choose the MaltParser for Catalan, Czech, German, and Spanish, and choose another MaxEntbased parser for Chinese, English, and Japanese. 2.4.1 A Transition-based Parser: MaltParser For Catalan, Czech, German, and Spanish, we use the MaltParser, a freely available implementa4 http://www.cs.berkeley.edu/˜pliang/software/browncluster-1.2.zip tion5 , whose details are presented in the paper of Nivre (2003). More information about the parser can be available in the paper (Nivre, 2003). Due to computational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use"
W09-1209,W08-2121,0,0.0249233,"Missing"
W09-1209,taule-etal-2008-ancora,0,0.0239121,"Missing"
W09-1209,W08-2127,1,0.761054,"nted in the paper of Nivre (2003). More information about the parser can be available in the paper (Nivre, 2003). Due to computational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use the similar feature notations of that work. We use the same greedy feature selection of Zhao et al. (2009) to determine an optimal feature template set for each language. Full feature sets for the three languages can be found at website, http://bcmi.sjtu.edu.cn/˜zhaohai. 2.4.3 Feature Representation For training data, we use 2-way jackknifing to generate predicted dependency parsing trees by two transition-based parsers. Following the features of Nivre and McDonald (2008), we define features for a head h and its dependent d with label l as shown in table 2, where GT ran refers t"
W09-1209,W09-1208,1,0.895734,"omputational cost, we do not select new feature templates for the MaltParser. Following the features settings of Hall et al. (2007), we use their Czech feature file and Catalan feature file. To simply, we apply Czech feature file for German too, and apply Catalan feature file for Spanish. 2.4.2 Another Transition-based Parser: MaxEnt-based Parser In three highly projective language, Chinese, English and Japanese, we use the maximum entropy syntactic dependency parser as in Zhao and Kit (2008). We still use the similar feature notations of that work. We use the same greedy feature selection of Zhao et al. (2009) to determine an optimal feature template set for each language. Full feature sets for the three languages can be found at website, http://bcmi.sjtu.edu.cn/˜zhaohai. 2.4.3 Feature Representation For training data, we use 2-way jackknifing to generate predicted dependency parsing trees by two transition-based parsers. Following the features of Nivre and McDonald (2008), we define features for a head h and its dependent d with label l as shown in table 2, where GT ran refers to dependency parsing trees generated by the MaltParser or MaxEnt-base Parser and ∗ refers to any label. All features are"
W09-3401,vossen-etal-2008-kyoto,1,\N,Missing
W09-3401,C04-1053,0,\N,Missing
W09-3401,W04-2209,0,\N,Missing
W09-3401,W04-2208,1,\N,Missing
W09-3401,W07-1522,0,\N,Missing
W09-3401,I08-2108,1,\N,Missing
W09-3401,bond-etal-2008-boot,1,\N,Missing
W09-3401,francopoulo-etal-2006-lexical,0,\N,Missing
W09-3401,1991.mtsummit-papers.16,0,\N,Missing
W09-3506,J96-1002,0,0.0799941,"). The difference between the two models lies in whether or not a machine transliteration process depends on target-language phonemes. TM-G directly converts source-language graphemes into targetlanguage graphemes, while TM-GP first transforms source language graphemes into targetlanguage phonemes and then target-language phonemes coupled with their corresponding source-language graphemes are converted into target-language graphemes. We used three different machine learning algorithms (conditional random fields (CRFs), margin infused relaxed algorithm (MIRA), and maximum entropy model (MEM)) (Berger et al., 1996; Crammer and Singer, 2003; Lafferty et al., 2001) for building multiple machine transliteration engines. We PT M −G (T |S) = P (TG |S) (1) PT M −GP (T |S)  P (TP |S) × P (TG |TP , S) = (2) ∀TP En S C l i n t o n Ch TP KE L I N D U N Ja TG 克:B 林:B 林:I TP KU TG ク:B R リ:B リ:I TM-G Clinton 克林顿 I 林:I 顿:B 顿:I 顿:I N T O N ン:B ト:B ト:I ン:B TM-GP Clinton クリントン Clinton Clinton KELINDUN KURINTON 克林顿 クリントン Figure 1: Illustration of the two transliteration models 36 Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 36–39, c Suntec, Singapore, 7 August 2009. 2009 ACL and AFNLP Figure"
W09-3506,P04-1021,0,0.429966,"Missing"
W09-3506,W03-0430,0,0.0152691,"produces 30-best transliterations for a given source-language word. 3.1 Maximum Entropy Model Machine transliteration based on the maximum entropy model was described in detail in Oh et al. (2006) along with comprehensive evaluation of its performance. We used the same way as that proposed by Oh et al. (2006), thus its full description is not presented here. 3.2 Conditional Random Fields (CRFs) CRFs, a statistical sequence modeling framework, was first introduced by Lafferty et al. (2001). CRFs has been used for sequential labeling problems such as text chunking and named entity recognition (McCallum and Li, 2003). CRF++1 was used in our experiment. TM-G TM-GP 3.3 Margin Infused Relaxed Algorithm Source-language transliteration unit Grapheme Syllable ME-G, CRF-G MIRA-G ME-GP N/A Table 1: Design strategy for multiple transliteration engines The Margin Infused Relaxed Algorithm (MIRA) has been introduced by Crammer and Singer (2003) for large-margin multi-class classification. Kruengkrai et al. (2008) proposed a discriminative model for joint Chinese segmentation and POS tagging, where MIRA was used as their machine learning algorithm. We used the same model for our machine transliteration, exactly joint"
W09-3506,2007.mtsummit-papers.47,1,0.670215,"ative model for joint Chinese segmentation and POS tagging, where MIRA was used as their machine learning algorithm. We used the same model for our machine transliteration, exactly joint syllabication2 and transliteration. 4.2 Combining Methodology We combined the outputs of multiple transliteration engines by means of a re-ranking function, g(x). Let X be a set of transliterations generated by multiple transliteration engines for sourcelanguage word s and ref be a reference transliteration of s. A re-ranking function is defined as Eq. (3), where it ranks ref in X higher and the others lower (Oh and Isahara, 2007). 3.4 Features We used the following features within the ±3 context window3 for the above mentioned three mag(x) : X → {r : r is ordering of x ∈ X} (3) 1 Available at http://crfpp.sourceforge.net/ A syllable in English is defined as a sequence of English grapheme corresponding to one target-language grapheme. 3 The unit of context window is source-language grapheme or syllable. 2 We designed two types of re-ranking functions by using the rank of each individual engine and machine learning algorithm. 37 • grank (x), gF score (x), Rank1 i (x) , P (T |S) 4.2.1 Re-ranking Based on the Rank of In"
W09-3506,P09-1058,1,\N,Missing
W09-3506,W09-3501,0,\N,Missing
W09-3506,W09-3502,0,\N,Missing
W98-0202,H94-1070,0,0.0548158,"research has been limited to generating abstracts or extracting some topics. However, they are immature and still have many problems. No one, yet, has established a way for the user to tell a news reader what he/she requires. 3 Information Reader Retrieval and News There is much on-going research in information retrieval. In document retrieval, the key technology is the utilization of keywords, titles, and user defined &quot;key words&quot; (Jacobs, 1992). Full text search is now very fast using some programming techniques. T R E C (Text Retrieval Conference) by A R P A includes this kind of approach (Harman, 1994). One of the targets of the summarization and information extraction domains is to plug information into some templates. MUC (Message Understanding Conference) by ARPA is in~colved in doing this kind of work (ARPA, 1993). However, these approaches are not suitable for information retrieval from the network news on the Internet. Therefore, there have been many proposals for network news readers. For example, &quot;Galaxy of News&quot; retrieves sets of information related to one another by adopting a stochastic method to produce a hierarchy of keywords and it presents the results of the search visually,"
wang-etal-2010-adapting,C02-1148,0,\N,Missing
wang-etal-2010-adapting,P09-1058,1,\N,Missing
wang-etal-2010-adapting,W08-0336,0,\N,Missing
wang-etal-2010-adapting,N03-1017,0,\N,Missing
wang-etal-2010-adapting,P08-1115,0,\N,Missing
wang-etal-2010-adapting,I08-1033,0,\N,Missing
wang-etal-2010-adapting,D08-1076,0,\N,Missing
Y05-1014,W00-1303,0,0.012695,"nai (do not), shinakatta (did not). • Feature Set 2 This set consisted of all of the morphemes in each of the input sentences, e.g., kyou (today), watashi (I), wa (topic-marker particle), hashiru (run). 1 This corpus was made in our previous studies (Murata et al., 2002b; Murata et al., 2005). We found that support vector machines were more accurate than other kinds of machine learning methods such as the decision-list method and maximum entropy method (Murata et al., 2001). In addition, the use of support vector machines has been found to be effective in many studies (Taira and Haruno, 2001; Kudo and Matsumoto, 2000; Nakagawa et al., 2001; Murata et al., 2002a). Therefore, we used support vector machines in our translation systems. The detailed parameter settings we used are described in our previous paper (Murata et al., 2001). 2 Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation. Table 1: Occurrence rates of correct categories for tense, aspect, and modality. Category Occurrence rate present 0.65 (516/800) past 0.45 (356/800) prefect 0.32 (259/800) “can” 0.11 (90/800) “will” 0.11 (87/800) progressive 0.10 (82/800) imperative 0.09 (74/800) “should” 0.07 ("
Y05-1014,W01-1415,1,0.931958,"(one category) 5. participial constructions (one category) 6. verb ellipses (one category) 7. interjections or greeting sentences (one category) We used 800 sentences extracted from a corpus1 containing 40,198 sentences for the evaluation. We calculated the accuracy rates of six translation systems on the market and our new translation systems and examined the error patterns in the results. The six translation systems were the latest of leading translation system companies as of October 2003. Our systems for translating tense, aspect, and modality are based on support vector machines (SVMs) (Murata et al., 2001).2 They translate Japanese tense, aspect, and modality expressions into English. They detect categories of tense, aspect, and modality previously defined from English expressions. The categories are detected as a categorization problem by SVMs (Cristianini and Shawe-Taylor, 2000; Kudoh, 2000). However, an SVM can handle only two categories at a time. Therefore, we used a pairwise method in addition to the SVM to handle more than two categories (Moreira and Mayoraz, 1998). As training sentences, we used the sentences remaining after eliminating the 800 evaluation sentences from the 40,198-sente"
Y05-1014,2002.tmi-papers.14,1,0.747648,"Missing"
zhang-etal-2008-word,W04-2208,1,\N,Missing
zhang-etal-2008-word,I05-2015,1,\N,Missing
zhang-etal-2008-word,2005.mtsummit-papers.10,1,\N,Missing
zhang-etal-2008-word,maekawa-etal-2000-spontaneous,1,\N,Missing
